issue,id,reporter,closed_by,created_at,updated_at,closed_at,state,assignee,milestone,comments,label_name,url,title,body
mwaskom/seaborn#909,150578639,gepcel,,2016-04-23 17:40:00,2016-05-21 11:35:55,None,open,,,7,,https://api.github.com/repos/mwaskom/seaborn/issues/909,"b'Got ValueError from clustermap, says ""Valid methods when the raw observations are omitted are \'single\', \'complete\', \'weighted\', and \'average\'""'","b'When I run this:\r\n```python\r\nimport seaborn as sns\r\nsns.clustermap(spe_norm, method=\'ward\')\r\n```\r\n`spe_norm` is from [this file](https://gist.githubusercontent.com/gepcel/4c8c657796a481a78a89754298c408ac/raw/e48d3dcdfaa19f65212106dd64f7e3ecd271f640/spe-norm.csv). I got an error:\r\n```python\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in calculated_linkage(self)\r\n    563         try:\r\n--> 564             return self._calculate_linkage_fastcluster()\r\n    565         except ImportError:\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in _calculate_linkage_fastcluster(self)\r\n    543     def _calculate_linkage_fastcluster(self):\r\n--> 544         import fastcluster\r\n    545         # Fastcluster has a memory-saving vectorized version, but only\r\n\r\nImportError: No module named \'fastcluster\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-224973069d28> in <module>()\r\n      1 import seaborn as sns\r\n      2 rcParams[\'font.family\'] = \'Noto Sans S Chinese\'\r\n----> 3 sns.clustermap(spe_norm, method=\'ward\')\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs)\r\n   1167                         row_cluster=row_cluster, col_cluster=col_cluster,\r\n   1168                         row_linkage=row_linkage, col_linkage=col_linkage,\r\n-> 1169                         **kwargs)\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in plot(self, metric, method, colorbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, **kws)\r\n   1006         colorbar_kws = {} if colorbar_kws is None else colorbar_kws\r\n   1007         self.plot_dendrograms(row_cluster, col_cluster, metric, method,\r\n-> 1008                               row_linkage=row_linkage, col_linkage=col_linkage)\r\n   1009         try:\r\n   1010             xind = self.dendrogram_col.reordered_ind\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in plot_dendrograms(self, row_cluster, col_cluster, metric, method, row_linkage, col_linkage)\r\n    931             self.dendrogram_row = dendrogram(\r\n    932                 self.data2d, metric=metric, method=method, label=False, axis=0,\r\n--> 933                 ax=self.ax_row_dendrogram, rotate=True, linkage=row_linkage)\r\n    934         else:\r\n    935             self.ax_row_dendrogram.set_xticks([])\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in dendrogram(data, linkage, axis, label, metric, method, rotate, ax)\r\n    682     plotter = _DendrogramPlotter(data, linkage=linkage, axis=axis,\r\n    683                                  metric=metric, method=method,\r\n--> 684                                  label=label, rotate=rotate)\r\n    685     if ax is None:\r\n    686         ax = plt.gca()\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in __init__(self, data, linkage, metric, method, axis, label, rotate)\r\n    497 \r\n    498         if linkage is None:\r\n--> 499             self.linkage = self.calculated_linkage\r\n    500         else:\r\n    501             self.linkage = linkage\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in calculated_linkage(self)\r\n    564             return self._calculate_linkage_fastcluster()\r\n    565         except ImportError:\r\n--> 566             return self._calculate_linkage_scipy()\r\n    567 \r\n    568     def calculate_dendrogram(self):\r\n\r\nC:\\anaconda\\lib\\site-packages\\seaborn\\matrix.py in _calculate_linkage_scipy(self)\r\n    537 \r\n    538         pairwise_dists = distance.pdist(self.array, metric=self.metric)\r\n--> 539         linkage = hierarchy.linkage(pairwise_dists, method=self.method)\r\n    540         del pairwise_dists\r\n    541         return linkage\r\n\r\nC:\\anaconda\\lib\\site-packages\\scipy\\cluster\\hierarchy.py in linkage(y, method, metric)\r\n    626         d = distance.num_obs_y(y)\r\n    627         if method not in _cpy_non_euclid_methods:\r\n--> 628             raise ValueError(""Valid methods when the raw observations are ""\r\n    629                              ""omitted are \'single\', \'complete\', \'weighted\', ""\r\n    630                              ""and \'average\'."")\r\n\r\nValueError: Valid methods when the raw observations are omitted are \'single\', \'complete\', \'weighted\', and \'average\'.\r\n```\r\n\r\nI know I should have installed fastcluster, and it\'s sweat to do a scipy cluster if fastcluster is not valid. But shouldn\'t [the code line 544-545 of matrix.py](https://github.com/mwaskom/seaborn/blob/master/seaborn/matrix.py#L544-L545) be like this:?\r\n```python\r\nlinkage = hierarchy.linkage(pairwise_dists, method=self.method, metric=self.metric)\r\n```\r\nother than\r\n```python\r\npairwise_dists = distance.pdist(self.array, metric=self.metric)\r\nlinkage = hierarchy.linkage(pairwise_dists, method=self.method)\r\n```\r\nI\'m not familiar with cluster algorithms. But I can do this directly in scipy:\r\n```python\r\nfrom scipy.cluster import hierarchy\r\nhierarchy.linkage(spe_norm, method=\'ward\')\r\n```\r\nOr, is this actually the wrong way?'"
mwaskom/seaborn#930,156030078,olgabot,,2016-05-20 19:03:16,2016-05-20 21:33:31,None,open,,,4,,https://api.github.com/repos/mwaskom/seaborn/issues/930,b'Use one-step linkage calculation with both scipy and fastcluster',"b'Trying to solve https://github.com/mwaskom/seaborn/issues/909, do linkage calculation in one step rather than two because of a weird compatibility issue with `scipy` 0.17 (https://github.com/scipy/scipy/issues/6180)'"
ContinuumIO/anaconda-issues#804,156872068,mrclary,,2016-05-25 23:24:42,2016-05-30 19:26:11,None,open,,,3,,https://api.github.com/repos/ContinuumIO/anaconda-issues/issues/804,b'Possible issue with linear algebra pack used for numpy compilation',"b""I have recently discovered an issue with Anaconda installations of numpy >=1.9.3.\r\nI believe it is related to the linear algebra library used to compile numpy since my MacPorts install of numpy 1.10.4 does not produce the error.\r\nThe behavior is detailed below.\r\n\r\nWith Anaconda numpy version <= 1.9.2 and all MacPorts numpy versions, polyfit behaved correctly, as follows:\r\n\r\n    In [1]: np.polyfit([1, 10], [np.nan, np.nan], 1)\r\n    Out[1]: array([ nan,  nan])\r\n\r\nBut for Anaconda numpy version >=1.9.3, polyfit raises the following error:\r\n\r\n    In [1]: np.polyfit([1, 10], [np.nan, np.nan], 1)\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-3-bf0d5d071f08> in <module>()\r\n    ----> 1 np.polyfit([1, 10], [np.nan, np.nan], 1)\r\n    \r\n    /Users/rclary/anaconda/lib/python2.7/site-packages/numpy/lib/polynomial.pyc in polyfit(x, y, deg, rcond, full, w, cov)\r\n        580     scale = NX.sqrt((lhs*lhs).sum(axis=0))\r\n        581     lhs /= scale\r\n    --> 582     c, resids, rank, s = lstsq(lhs, rhs, rcond)\r\n        583     c = (c.T/scale).T  # broadcast scale coefficients\r\n        584 \r\n    \r\n    /Users/rclary/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc in lstsq(a, b, rcond)\r\n       1865         work = zeros((lwork,), t)\r\n       1866         results = lapack_routine(m, n, n_rhs, a, m, bstar, ldb, s, rcond,\r\n    -> 1867                                  0, work, lwork, iwork, 0)\r\n       1868     if results['info'] > 0:\r\n       1869         raise LinAlgError('SVD did not converge in Linear Least Squares')\r\n    \r\n    ValueError: On entry to DGELSD parameter number 6 had an illegal value\r\n\r\nDoes anyone have any insight on this issue?"""
biocore/scikit-bio#1259,128871057,jairideout,jairideout,2016-01-26 16:29:19,2016-04-27 16:57:44,2016-02-03 01:25:46,closed,mortonjt,0.4.2,7,bug;CI / testing;severity: moderate,https://api.github.com/repos/biocore/scikit-bio/issues/1259,b'ANCOM unit test failures with scipy 0.17.0',"b'The following ANCOM unit tests fail with scipy 0.17.0 on Python 2 and 3:\r\n\r\n```shell\r\n$ python skbio/stats/tests/test_composition.py\r\n.F.....................F................\r\n======================================================================\r\nFAIL: test_ancom_alternative_test (__main__.AncomTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""skbio/stats/tests/test_composition.py"", line 642, in test_ancom_alternative_test\r\n    assert_data_frame_almost_equal(result, exp)\r\n  File ""/Users/jairideout/dev/scikit-bio/skbio/util/_testing.py"", line 1205, in assert_data_frame_almost_equal\r\n    check_exact=False)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 1049, in assert_frame_equal\r\n    obj=\'DataFrame.iloc[:, {0}]\'.format(i))\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 946, in assert_series_equal\r\n    check_less_precise, obj=\'{0}\'.format(obj))\r\n  File ""pandas/src/testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3809)\r\n  File ""pandas/src/testing.pyx"", line 147, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2685)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 819, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (100.0 %)\r\n[left]:  [0, 0, 0, 0, 0, 0, 0]\r\n[right]: [6, 6, 2, 2, 2, 2, 2]\r\n\r\n======================================================================\r\nFAIL: test_ancom_normal_data (__main__.AncomTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""skbio/stats/tests/test_composition.py"", line 655, in test_ancom_normal_data\r\n    assert_data_frame_almost_equal(result, exp)\r\n  File ""/Users/jairideout/dev/scikit-bio/skbio/util/_testing.py"", line 1205, in assert_data_frame_almost_equal\r\n    check_exact=False)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 1049, in assert_frame_equal\r\n    obj=\'DataFrame.iloc[:, {0}]\'.format(i))\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 946, in assert_series_equal\r\n    check_less_precise, obj=\'{0}\'.format(obj))\r\n  File ""pandas/src/testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3809)\r\n  File ""pandas/src/testing.pyx"", line 147, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2685)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 819, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (22.22222 %)\r\n[left]:  [8, 7, 3, 3, 7, 3, 3, 3, 3]\r\n[right]: [8, 8, 3, 3, 8, 3, 3, 3, 3]\r\n\r\n----------------------------------------------------------------------\r\nRan 40 tests in 0.435s\r\n\r\nFAILED (failures=2)\r\n```\r\n\r\nThe tests pass using the previous scipy release (0.16.1, available via `conda` and `pip`). Note that scipy 0.17.0 is currently only available via `pip`.\r\n\r\nThe tests are failing because something in `scipy.stats.mannwhitneyu` changed between scipy 0.16.1 and 0.17.0. Changing `test_ancom_alternative_test` to pass `alternative=\'less\'` fixes the test. However, making the same change to `test_ancom_normal_data` doesn\'t fix the failure.\r\n\r\nSee #1258 for original discussion.'"
BlueBrain/NeuroM#220,127493391,lidakanari,juanchopanza,2016-01-19 17:08:28,2016-01-21 12:17:02,2016-01-21 12:17:02,closed,,,11,,https://api.github.com/repos/BlueBrain/NeuroM/issues/220,b'Basic statistical tests to compare two datasets ',"b""This is useful to built more generic comparison tests in examples. Please suggest other names for the function if the proposed one doesn't make sense."""
biocore/scikit-bio#1259,128871057,jairideout,jairideout,2016-01-26 16:29:19,2016-04-27 16:57:44,2016-02-03 01:25:46,closed,mortonjt,0.4.2,7,bug;CI / testing;severity: moderate,https://api.github.com/repos/biocore/scikit-bio/issues/1259,b'ANCOM unit test failures with scipy 0.17.0',"b'The following ANCOM unit tests fail with scipy 0.17.0 on Python 2 and 3:\r\n\r\n```shell\r\n$ python skbio/stats/tests/test_composition.py\r\n.F.....................F................\r\n======================================================================\r\nFAIL: test_ancom_alternative_test (__main__.AncomTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""skbio/stats/tests/test_composition.py"", line 642, in test_ancom_alternative_test\r\n    assert_data_frame_almost_equal(result, exp)\r\n  File ""/Users/jairideout/dev/scikit-bio/skbio/util/_testing.py"", line 1205, in assert_data_frame_almost_equal\r\n    check_exact=False)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 1049, in assert_frame_equal\r\n    obj=\'DataFrame.iloc[:, {0}]\'.format(i))\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 946, in assert_series_equal\r\n    check_less_precise, obj=\'{0}\'.format(obj))\r\n  File ""pandas/src/testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3809)\r\n  File ""pandas/src/testing.pyx"", line 147, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2685)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 819, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (100.0 %)\r\n[left]:  [0, 0, 0, 0, 0, 0, 0]\r\n[right]: [6, 6, 2, 2, 2, 2, 2]\r\n\r\n======================================================================\r\nFAIL: test_ancom_normal_data (__main__.AncomTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""skbio/stats/tests/test_composition.py"", line 655, in test_ancom_normal_data\r\n    assert_data_frame_almost_equal(result, exp)\r\n  File ""/Users/jairideout/dev/scikit-bio/skbio/util/_testing.py"", line 1205, in assert_data_frame_almost_equal\r\n    check_exact=False)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 1049, in assert_frame_equal\r\n    obj=\'DataFrame.iloc[:, {0}]\'.format(i))\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 946, in assert_series_equal\r\n    check_less_precise, obj=\'{0}\'.format(obj))\r\n  File ""pandas/src/testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3809)\r\n  File ""pandas/src/testing.pyx"", line 147, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2685)\r\n  File ""/Users/jairideout/miniconda/envs/scikit-bio/lib/python3.5/site-packages/pandas/util/testing.py"", line 819, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (22.22222 %)\r\n[left]:  [8, 7, 3, 3, 7, 3, 3, 3, 3]\r\n[right]: [8, 8, 3, 3, 8, 3, 3, 3, 3]\r\n\r\n----------------------------------------------------------------------\r\nRan 40 tests in 0.435s\r\n\r\nFAILED (failures=2)\r\n```\r\n\r\nThe tests pass using the previous scipy release (0.16.1, available via `conda` and `pip`). Note that scipy 0.17.0 is currently only available via `pip`.\r\n\r\nThe tests are failing because something in `scipy.stats.mannwhitneyu` changed between scipy 0.16.1 and 0.17.0. Changing `test_ancom_alternative_test` to pass `alternative=\'less\'` fixes the test. However, making the same change to `test_ancom_normal_data` doesn\'t fix the failure.\r\n\r\nSee #1258 for original discussion.'"
EconForge/dolo#88,146419323,albop,albop,2016-04-06 20:09:38,2016-05-10 11:19:08,2016-05-10 11:19:08,closed,,,3,,https://api.github.com/repos/EconForge/dolo/issues/88,b'Cleanup the qz code',b'It still relies on some old ctypes wrapper by Sven Schreiber.\r\nThe reordering routines in lapack seem complicated to wrap because of the need to send callback routines (selectg). The medium run equilibrium could be:\r\n- get rid of all ctypes\r\n- call the non-reordering routine in scipy\r\n- numbaify qzswitch\r\n- use qzswitch to write a new reodering function with several options\r\n\r\nAn alternative for the last point would be to use the current code which reorders based on a threshold value.'
numpy/numpy#7509,145990811,ev-br,charris,2016-04-05 13:33:53,2016-04-05 23:44:43,2016-04-05 23:44:43,closed,,,4,,https://api.github.com/repos/numpy/numpy/issues/7509,b'a possible MA regression',"b""scipy tests started failing today with numpy from travis-dev-wheels. \r\nThe failure is in mstats: https://travis-ci.org/scipy/scipy/builds/120882377\r\n\r\nThe failing test does this:\r\n\r\n```\r\nIn [1]: from scipy.stats import mstats\r\n\r\nIn [2]: %paste\r\n        x = [1, 3, 5, 7, 9]\r\n        y = [2, 4, 6, 8, 10]\r\n## -- End pasted text --\r\n\r\nIn [3]: mstats.kruskal(x, y)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-5edb46bc91da> in <module>()\r\n----> 1 mstats.kruskal(x, y)\r\n\r\n/home/br/virtualenvs/np/local/lib/python2.7/site-packages/scipy/stats/mstats_basic.pyc in kruskal(*args)\r\n   1047     ngrp = ranks.count(-1)\r\n   1048     ntot = ranks.count()\r\n-> 1049     H = 12./(ntot*(ntot+1)) * (sumrk**2/ngrp).sum() - 3*(ntot+1)\r\n   1050     # Tie correction\r\n   1051     ties = count_tied_groups(ranks)\r\n\r\n/home/br/repos/numpy/build/testenv/lib/python2.7/site-packages/numpy/ma/core.pyc in __truediv__(self, other)\r\n   3983         if self._delegate_binop(other):\r\n   3984             return NotImplemented\r\n-> 3985         return true_divide(self, other)\r\n   3986 \r\n   3987     def __rtruediv__(self, other):\r\n\r\n/home/br/repos/numpy/build/testenv/lib/python2.7/site-packages/numpy/ma/core.pyc in __call__(self, a, b, *args, **kwargs)\r\n   1126         # Get the result\r\n   1127         with np.errstate(divide='ignore', invalid='ignore'):\r\n-> 1128             result = self.f(da, db, *args, **kwargs)\r\n   1129         # Get the mask as a combination of the source masks and invalid\r\n   1130         m = ~umath.isfinite(result)\r\n\r\nValueError: operands could not be broadcast together with shapes (2,) (2,5) \r\n```\r\n\r\nand bisecting indicates\r\n\r\n```\r\n$ git bisect bad\r\n36f76ea2e6e91062df12d3a46ccaed7822bc82f2 is the first bad commit\r\ncommit 36f76ea2e6e91062df12d3a46ccaed7822bc82f2\r\nAuthor: Allan Haldane <allan.haldane@gmail.com>\r\nDate:   Sun Feb 14 02:49:19 2016 -0500\r\n\r\n    ENH: add extra kwargs and update doc of many MA methods\r\n    \r\n    Updated any, all, sum, prod, cumsum, cumprod, min, max, argmin, argmax,\r\n    mean, var\r\n\r\n:040000 040000 e41cfd58bb4bbabc11471a0bd443b24be91b45d0 7f99716492632508bc4d664d424c7cf22d6cd5cb M\tnumpy\r\n```\r\n\r\nwhich is, it seems, gh-5706?"""
astropy/astropy#4472,126086564,mirca,crawfordsm,2016-01-12 03:01:21,2016-03-13 18:30:22,2016-03-13 16:56:07,closed,,v1.2.0,26,Affects-release;Enhancement;stats,https://api.github.com/repos/astropy/astropy/issues/4472,b'Circstats issue3705',"b'Hi,\r\n\r\nSome circular statistical functions were implemented, as requested in issue #3705.\r\n\r\nThese functions are primarily based on those implemented in the R package [`CircStats`](https://cran.r-project.org/web/packages/CircStats/CircStats.pdf). Therefore, in most of the tests, results are compared with those obtained through similar functions presented in the aforementioned package.\r\n\r\nAdditionally, a function to compute the Cram\xa8\xa6r-Rao Lower Bounds for the von Mises distribution (which basically gives the uncertainties on estimating the parameters) was implemented. For this function, unfortunately, I could not find examples elsewhere in order to write a reliable enough test. Nonetheless, since the math envolved is simple and it only relies on the maximum likelihood estimation (which has been tested), I think this function is good.\r\n\r\nI would be grateful for any comments.'"
numpy/numpy#593,7718582,numpy-gitbot,,2012-10-19 15:09:15,2013-07-03 18:09:20,None,open,,,4,11 - Bug;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/593,b'overflow not caught on operators with int32 array (Trac #2133)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/2133 on 2012-05-16 by trac user mwtoews, assigned to unknown._\n\nI see good overflow warnings operations on int32 scalars, but not for int32 arrays:\n\n\n    import numpy as np\n    \n    # Case 1: Good, proper data types are used to avoid overflow\n    np.array([1], dtype=np.long) + np.int32(2**31 - 1)\n    # array([2147483648], dtype=int64)\n    \n    # Case 2: Bad, overflow happens and no warning raised\n    np.array([1], dtype=np.int32) + np.int32(2**31 - 1)\n    # array([-2147483648])\n    \n    # Case 3: Similar bad, array vs. array\n    np.array([1], dtype=np.int32) + np.array([2**31 - 1], dtype=np.int32)\n    # array([-2147483648])\n    \n    # Case 4: Better, a warning is raised\n    np.int32(1) + np.int32(2**31 - 1)\n    # __main__:1: RuntimeWarning: overflow encountered in long_scalars\n    # -2147483648\n\n\n\nHere is what I can determine between operator _op_ between arrays or scalars:\n 1. array-long _op_ scalar-32 = array-long, all good\n 2. array-32 _op_ scalar-32 = array-32, no overflow warning!\n 3. array-32 _op_ array-32 = array-32, no overflow warning!\n 4. scalar-32 _op_ scalar-32 = scalar-32, helpful overflow warning\n\nI would expect an overflow runtime warning for cases 2 and 3, similar to case 4.\n\nThese results are with NumPy 1.6.1/Python 2.5.1 on Windows 32-bit, which I installed from `numpy-unoptimized-1.6.1.win32-py2.5.exe` from [http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy ~gohlke], but are reproducible on 64-bit Linux.'"
statsmodels/statsmodels#2800,131913542,thequackdaddy,thequackdaddy,2016-02-07 00:07:46,2016-03-14 17:53:00,2016-02-10 05:57:31,closed,,,18,,https://api.github.com/repos/statsmodels/statsmodels/issues/2800,b'BUG: Scipy seems to have broken things',"b""Hello, \r\n\r\nI noticed that statsmodels appears to not be building on the new version of scipy 0.17. There seems to be a change in behavior when you have `np.nan` fed into a scipy.stat.skew and scipy.stat.kurtosis\r\n\r\nThis fixes that. \r\n\r\nThere is still some kind of an MKL bug, which I have no clue how to fix. I'm happy to investigate if someone points me in the right direction. \r\n\r\nThe build is here:\r\n\r\nhttps://travis-ci.org/thequackdaddy/statsmodels/builds/107512963\r\n\r\nThanks!"""
wesleybowman/UTide#30,133418216,saulomeirelles,wesleybowman,2016-02-13 09:50:03,2016-04-17 09:20:52,2016-04-14 17:48:08,closed,,,15,,https://api.github.com/repos/wesleybowman/UTide/issues/30,b'BUG: Python stops working when importing utide on Windows platform',"b'Hi everyone,\r\n\r\nI recently updated my Utide package on Windows 7 64-bit. Now every time I import utide, Python simply stops working and the program closes down. @ocefpaf has done some tests and confirmed the bug. I am using the following versions:\r\n\r\nScipy 0.17.0\r\nPython 2.7.11\r\nUTide 0.1\r\n\r\nI ran my scripts on Linux and all went well.\r\n\r\nCheers, Saulo\r\n\r\n'"
conda-forge/staged-recipes#351,148138419,ocefpaf,ocefpaf,2016-04-13 18:07:34,2016-04-16 00:02:56,2016-04-16 00:02:56,closed,,,5,,https://api.github.com/repos/conda-forge/staged-recipes/issues/351,b'Last batch of recipes from IOOS',"b'There are a few others that are very low priority, but with these and https://github.com/conda-forge/staged-recipes/pull/280 we can safely shut the IOOS channel down.\r\n\r\nPing @rsignell-usgs.'"
statsmodels/statsmodels#2800,131913542,thequackdaddy,thequackdaddy,2016-02-07 00:07:46,2016-03-14 17:53:00,2016-02-10 05:57:31,closed,,,18,,https://api.github.com/repos/statsmodels/statsmodels/issues/2800,b'BUG: Scipy seems to have broken things',"b""Hello, \r\n\r\nI noticed that statsmodels appears to not be building on the new version of scipy 0.17. There seems to be a change in behavior when you have `np.nan` fed into a scipy.stat.skew and scipy.stat.kurtosis\r\n\r\nThis fixes that. \r\n\r\nThere is still some kind of an MKL bug, which I have no clue how to fix. I'm happy to investigate if someone points me in the right direction. \r\n\r\nThe build is here:\r\n\r\nhttps://travis-ci.org/thequackdaddy/statsmodels/builds/107512963\r\n\r\nThanks!"""
pydata/pandas#12199,130492289,ghego,jreback,2016-02-01 21:39:18,2016-02-01 23:58:45,2016-02-01 23:58:45,closed,,No action,1,Compat;Won't Fix,https://api.github.com/repos/pydata/pandas/issues/12199,b'fftpack.rfft overwrites data when input is a pandas series',"b'Reproduce:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom scipy import fftpack\r\n\r\nx = np.array([1.,2.,3.,4.,5.])\r\nxs = pd.Series(x)\r\n\r\nprint x\r\ntmp = fftpack.rfft(x)\r\nprint x\r\n\r\nprint x\r\ntmp = fftpack.rfft(xs)\r\nprint x\r\n\r\n""""""\r\noutput:\r\n[ 1.  2.  3.  4.  5.]\r\n[ 1.  2.  3.  4.  5.]\r\n[ 1.  2.  3.  4.  5.]\r\n[ 15.          -2.5          3.4409548   -2.5          0.81229924]\r\n""""""\r\n\r\n```\r\n\r\nposted also in scipy.github:\r\nhttps://github.com/scipy/scipy/issues/5790'"
numpy/numpy#6936,124785596,ev-br,charris,2016-01-04 16:04:17,2016-01-04 17:32:06,2016-01-04 17:31:30,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/6936,b'DEP: deprecate np.testing.rand',"b""This exposes stdlib's random number generator and makes life a little unpredictable for those users who think that they have reproducibility by seeding the generator from numpy.random. \r\nSee, e.g., https://github.com/scipy/scipy/pull/5661 and https://github.com/scipy/scipy/issues/5629."""
NixOS/nixpkgs#11721,122150369,FRidh,domenkozar,2015-12-14 22:51:09,2015-12-17 09:52:55,2015-12-17 09:51:58,closed,,,8,2.status: work-in-progress;6.topic: python,https://api.github.com/repos/NixOS/nixpkgs/issues/11721,b'python scipy: fix tests',"b""This should fix #11720\r\nHaven't tested yet. Will check tomorrow."""
hpcugent/easybuild-easyconfigs#2181,120203517,boegel,boegel,2015-12-03 15:54:44,2016-02-09 20:50:54,2016-02-09 20:50:54,closed,,v2.7.0,6,,https://api.github.com/repos/hpcugent/easybuild-easyconfigs/issues/2181,b'{math}[intel/2015b] scipy 0.16.1 (WIP)',b'requires #2179'
numpy/numpy#6745,119330696,WarrenWeckesser,,2015-11-29 02:01:34,2016-01-04 12:13:39,None,open,,,14,11 - Bug;component: numpy.random,https://api.github.com/repos/numpy/numpy/issues/6745,b'The arguments of randint() and random_integers() do not broadcast.',"b'(This is either a request for an enhancement, or a bug report--take your pick.)\r\n\r\nUnlike most of the other distributions implemented in the `RandomState` class, `randint` and `random_integers` do not broadcast their arguments.\r\n\r\nFor example, `hypergeometric` broadcasts:\r\n\r\n```\r\nIn [87]: rng = np.random.RandomState()\r\n\r\nIn [88]: rng.hypergeometric(10, [10, 20, 30], 10)\r\nOut[88]: array([6, 2, 1])\r\n```\r\nBut `randint` only accepts scalar integers:\r\n```\r\nIn [89]: rng.randint(10, [10, 100])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-89-b14fbd2d38cb> in <module>()\r\n----> 1 rng.randint(10, [10, 100])\r\n\r\nmtrand.pyx in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:9556)()\r\n\r\nTypeError: an integer is required\r\n```\r\n'"
numpy/numpy#2951,10347202,dmvianna,charris,2013-01-27 04:15:29,2014-08-20 13:06:11,2014-03-25 18:05:08,closed,,,4,component: numpy.core;priority: normal;Proposal,https://api.github.com/repos/numpy/numpy/issues/2951,b'numpy.int64 is not instance of int',"b""As reported in [xlwt](https://github.com/python-excel/xlwt/issues/15):\r\n\r\nHere is an examination of numpy behaviour (Python 2.7.3, numpy 1.6.2)\r\n```python\r\n>>> import numpy\r\n>>> data = [t(123456) for t in (numpy.int32, numpy.int64, numpy.float64)]\r\n>>> [type(d) for d in data]\r\n[<type 'numpy.int32'>, <type 'numpy.int64'>, <type 'numpy.float64'>]\r\n>>> data\r\n[123456, 123456, 123456.0]\r\n>>> check_types = (int, long, float)\r\n>>> for d in data:\r\n...     for c in check_types:\r\n...         print type(d), repr(c), isinstance(d, c)\r\n...\r\n<type 'numpy.int32'> <type 'int'> True\r\n<type 'numpy.int32'> <type 'long'> False\r\n<type 'numpy.int32'> <type 'float'> False\r\n<type 'numpy.int64'> <type 'int'> False\r\n<type 'numpy.int64'> <type 'long'> False\r\n<type 'numpy.int64'> <type 'float'> False\r\n<type 'numpy.float64'> <type 'int'> False\r\n<type 'numpy.float64'> <type 'long'> False\r\n<type 'numpy.float64'> <type 'float'> True\r\n>>>\r\n```\r\nLooks like numpy has done the work to make its int32 and float64 recognisable by other software but not int64."""
JuliaLang/julia#15784,146425938,maximsch2,andreasnoack,2016-04-06 20:34:53,2016-07-06 20:42:42,2016-04-09 02:29:53,closed,andreasnoack,,6,linear algebra,https://api.github.com/repos/JuliaLang/julia/issues/15784,b'LAPACK error on simple SVD',"b'```julia\r\nX = ones(Float32, (19764, 13620));\r\nsvd(X)\r\n```\r\n\r\nthrows an error:\r\n```\r\n ** On entry to SGESDDPM parameter number 12 had an illegal value\r\nERROR: ArgumentError: invalid argument #12 to LAPACK call\r\n in gesdd! at linalg/lapack.jl:1482\r\n in svdfact! at linalg/svd.jl:17\r\n in svdfact at linalg/svd.jl:23\r\n```\r\n\r\nThis sounds very close to https://github.com/scipy/scipy/issues/5401, maybe we need a similar workaround?\r\n\r\nThis is on julia 0.4.5 built from source on Ubuntu 14.04.\r\n'"
JuliaLang/julia#15796,146771312,andreasnoack,andreasnoack,2016-04-07 23:34:03,2016-07-07 02:39:22,2016-04-09 02:29:53,closed,,,3,,https://api.github.com/repos/JuliaLang/julia/issues/15796,b'Work around issue with truncated Float32 representation of lwork in sgesdd by using nextfloat.',"b""I've not added a test since it would take quite long to finish because of the size of the matrix required.\r\n\r\nSee\r\n\r\nhttp://icl.cs.utk.edu/lapack-forum/viewtopic.php?f=13&t=4587&p=11036&hilit=sgesdd#p11036\r\n\r\nand\r\n\r\nhttps://github.com/scipy/scipy/issues/5401\r\n\r\nFixes #15784"""
numpy/numpy#6467,111188681,beckermr,charris,2015-10-13 14:12:34,2015-11-27 15:39:10,2015-10-27 20:05:56,closed,,,32,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/6467,b'performance regression for record array access in numpy 1.10.1',"b'It appears that access numpy record arrays by field name is significantly slower in numpy 1.10.1. I have put below a simple example test that illustrates the issue. (I am aware that this particular example is much better accomplished by other means. The point is that array access is slow, not that this a representative problem.)\r\n\r\nThe test script is\r\n```python\r\n#!/usr/bin/env python\r\nimport os\r\nimport time\r\nimport sys\r\nimport numpy as\tnp\r\n\r\ndef test(N=100000,verbose=False):\r\n    d = np.zeros(1,dtype=[(\'col\',\'f8\')])\r\n\r\n    t0 = time.time()\r\n    for i in xrange(N):\r\n        d[\'col\'] +=\ti\r\n    t0 = time.time() - t0\r\n\r\n    if verbose:\r\n\tprint \'numpy version:\',np.version.version\r\n        print \'time: %g\' % t0\r\n\r\nif __name__ == ""__main__"":\r\n    if len(sys.argv) > 1:\r\n\tN = int(sys.argv[1])\r\n\ttest(N=N,verbose=True)\r\n    else:\r\n\ttest(verbose=True)\r\n```\r\n\r\nHere are the running times for different versions of numpy:\r\n\r\n```\r\nnumpy version: 1.9.3\r\ntime: 0.262786\r\n\r\nnumpy version: 1.10.1\r\ntime: 3.57254\r\n``` \r\n@esheldon has reproduced the relative timing differences on linux in addition to my tests which were with my mac. \r\n\r\nI profiled the code for v1.10.1 and found this\r\n```\r\n         3200006 function calls (3000006 primitive calls) in 4.521 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n   200000    1.386    0.000    1.883    0.000 _internal.py:372(_check_field_overlap)\r\n600000/400000    0.751    0.000    0.906    0.000 _internal.py:337(_get_all_field_offsets)\r\n        1    0.566    0.566    4.521    4.521 numpy_test.py:7(test)\r\n   200000    0.401    0.000    3.189    0.000 _internal.py:425(_getfield_is_safe)\r\n   200000    0.350    0.000    3.955    0.000 _internal.py:287(_index_fields)\r\n   200000    0.323    0.000    3.513    0.000 {method \'getfield\' of \'numpy.ndarray\' objects}\r\n   400000    0.279    0.000    0.279    0.000 {range}\r\n   400000    0.155    0.000    0.155    0.000 {method \'update\' of \'set\' objects}\r\n   400000    0.106    0.000    0.106    0.000 {method \'append\' of \'list\' objects}\r\n   200000    0.093    0.000    0.093    0.000 {isinstance}\r\n   200000    0.062    0.000    0.062    0.000 {method \'difference\' of \'set\' objects}\r\n   200000    0.048    0.000    0.048    0.000 {method \'extend\' of \'list\' objects}\r\n        1    0.000    0.000    0.000    0.000 {numpy.core.multiarray.zeros}\r\n        1    0.000    0.000    4.521    4.521 <string>:1(<module>)\r\n        2    0.000    0.000    0.000    0.000 {time.time}\r\n        1    0.000    0.000    0.000    0.000 {method \'disable\' of \'_lsprof.Profiler\' objects}\r\n ```\r\n\r\nIt appears that new code added at the python level for error checking is significantly degrading performance. \r\n'"
numpy/numpy#6461,111048226,rgommers,charris,2015-10-12 20:19:12,2015-10-14 22:21:16,2015-10-12 21:47:53,closed,,,5,11 - Bug;component: numpy.testing,https://api.github.com/repos/numpy/numpy/issues/6461,b'TST: fix forgotten change of not creating errors from test warnings i 1.10.x',b'See https://github.com/numpy/numpy/blob/master/doc/HOWTO_RELEASE.rst.txt#handle-test-warnings\r\n\r\nCloses https://github.com/scipy/scipy/issues/5331#issuecomment-147504218'
scikit-learn/scikit-learn#5333,109261577,giorgiop,,2015-10-01 09:27:59,2016-02-21 22:33:59,None,open,,0.17,31,Needs Review,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5333,b'[MRG] pairwise_distances outputs Nan and negative values',"b""Fixes #4475 . The problem is about pairwise distances and not T-SNE. Refer to the issue for discussion. \r\n#4495 deals with the same issue but it does not seem active any more.\r\n\r\nTests for problems related with negative values now pass. Regarding `nan` I am going to see if a fix on the scipy's side is possible. Until then, I have written wrappers of the scipy's functions. \r\n\r\nChanges\r\n- [x] added breaking tests for `TSNE` as from #4475 but covering all distances from `sklearn.metrics.pairwise.pairwise_distances`\r\n- [x] added breaking tests for the same `pairwise_distances`\r\n- [x] implemented a robust sklearn's version of `correlation`, almost the same as `cosine`\r\n- [x] wrote wrappers for scipy's 'yule', 'dice', 'sokalsneath'"""
python-pillow/Pillow#350,20277095,wiredfool,,2013-09-30 17:16:25,2015-09-24 18:30:21,None,open,,Future,7,Bug,https://api.github.com/repos/python-pillow/Pillow/issues/350,b'1 bit images to 1 bit nd arrays',"b'The conversion of 1 bit images to an ndarray is listed as broken in test_numpy.py, Image.py lines 217 and 1931. '"
jjgreen/mvpoly#5,114478214,jjgreen,jjgreen,2015-11-01 16:10:48,2015-11-01 22:34:43,2015-11-01 22:34:43,closed,,,1,,https://api.github.com/repos/jjgreen/mvpoly/issues/5,b'exact nearest neighbours',"b'The RBF epsilon value uses an approximation with some [bad edge-case behaviour](https://github.com/scipy/scipy/issues/5263), but SciPy has KD-tree code which should allow the exact value to be calculated in _O_(_n_ log _n_) time, small beer compared to the _O_(_n_^2) needed for the interpolation matrix.  Code in [this branch](https://github.com/jjgreen/mvpoly/tree/exact-nearest-neighbour)'"
,,,,,,,,,,,,,,
numpy/numpy#6395,109166740,andsor,,2015-09-30 19:56:23,2015-11-13 15:38:51,None,open,,,2,11 - Bug;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/6395,b'masked_array ** 2 operation differs from array ** 2 operation on cluster architecture',"b'Hi @juliantaylor, it\'s me again. I encounter a similar issue as before (#6251) with the following code using now Python 3.4.3 (GCC 4.7.2) with numpy 1.9.3:\r\n\r\n```python\r\nfrom __future__ import division, print_function, absolute_import\r\n\r\nimport numpy as np\r\nimport numpy.ma as ma\r\n\r\nfrom pprint import pprint\r\n\r\n\r\na = ma.array(\r\n    np.array(\r\n        [0.25375743, 0.49557181, -0.32551902, -0.37675809, -0.04705212999999997]\r\n#        [0.67696689, 0.91878127, 0.09769044, 0.04645137, 0.37615733]\r\n    ),\r\n    mask=np.array([False, False, False, False, False], dtype=np.bool)\r\n)\r\n\r\npprint(a)\r\n\r\nma_squared = a**2\r\na = np.asarray(a)\r\na_squared = a**2\r\n\r\npprint(a_squared - ma_squared)\r\npprint(a_squared[0])\r\npprint(ma_squared[0])\r\n```\r\n\r\nThe output is\r\n\r\n```\r\nmasked_array(data = [0.25375743 0.49557181 -0.32551902 -0.37675809 -0.04705212999999997],\r\n             mask = [False False False False False],\r\n       fill_value = 1e+20)\r\n\r\nmasked_array(data = [ -1.38777878e-17   0.00000000e+00  -1.38777878e-17   0.00000000e+00\r\n   0.00000000e+00],\r\n             mask = False,\r\n       fill_value = 1e+20)\r\n\r\n0.064392833280204897\r\n0.064392833280204911\r\n```\r\n\r\n\r\nnumpy.show_config():\r\n\r\n```\r\nblas_opt_info:\r\n    libraries = [\'f77blas\', \'cblas\', \'atlas\']\r\n    language = c\r\n    library_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/lib\']\r\n    include_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/include\']\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\nopenblas_info:\r\n  NOT AVAILABLE\r\nlapack_opt_info:\r\n    libraries = [\'lapack\', \'f77blas\', \'cblas\', \'atlas\']\r\n    language = f77\r\n    library_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/lib\']\r\n    include_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/include\']\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\natlas_3_10_blas_threads_info:\r\n  NOT AVAILABLE\r\nmkl_info:\r\n  NOT AVAILABLE\r\natlas_blas_threads_info:\r\n  NOT AVAILABLE\r\natlas_3_10_threads_info:\r\n  NOT AVAILABLE\r\natlas_threads_info:\r\n  NOT AVAILABLE\r\nopenblas_lapack_info:\r\n  NOT AVAILABLE\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\natlas_3_10_blas_info:\r\n  NOT AVAILABLE\r\natlas_blas_info:\r\n    libraries = [\'f77blas\', \'cblas\', \'atlas\']\r\n    language = c\r\n    library_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/lib\']\r\n    include_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/include\']\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\natlas_info:\r\n    libraries = [\'lapack\', \'f77blas\', \'cblas\', \'atlas\']\r\n    language = f77\r\n    library_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/lib\']\r\n    include_dirs = [\'/usr/nld/atlas-3.8.4-skadi-seq/include\']\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\natlas_3_10_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\n```\r\n\r\nThe numpy tests pass, but this lets one scipy test fail, scipy/scipy#5197 .\r\n\r\nDo you think it is related. This time I used the newer C compiler, though.'"
Homebrew/homebrew-python#252,109967432,jiegec,tdsmith,2015-10-06 09:19:28,2015-10-06 23:21:04,2015-10-06 23:20:39,closed,,,5,,https://api.github.com/repos/Homebrew/homebrew-python/issues/252,b'numpy 1.10.0',
numpy/numpy#6120,97340149,rgommers,rgommers,2015-07-26 18:35:59,2015-07-30 18:34:28,2015-07-30 17:01:33,closed,,,11,11 - Bug;component: numpy.lib,https://api.github.com/repos/numpy/numpy/issues/6120,b'BUG: regression in np.histogram',"b'After gh-6100 was merged, 4 tests for ``scipy.stats.histogram`` (which uses ``np.histogram`` under the hood) have started failing:\r\n\r\n    ======================================================================\r\n    FAIL: test_increased_bins (test_stats.TestHistogram)\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/home/rgommers/Code/scipy/scipy/stats/tests/test_stats.py"", line 984, in test_increased_bins\r\n        decimal=2)\r\n      File ""/home/rgommers/Code/numpy/numpy/testing/utils.py"", line 510, in assert_almost_equal\r\n        raise AssertionError(_build_err_msg())\r\n    AssertionError: \r\n    Arrays are not almost equal to 2 decimals\r\n     ACTUAL: 0\r\n     DESIRED: 10\r\n\r\n    ======================================================================\r\n    FAIL: test_reduced_bins (test_stats.TestHistogram)\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/home/rgommers/Code/scipy/scipy/stats/tests/test_stats.py"", line 949, in test_reduced_bins\r\n        decimal=2)\r\n      File ""/home/rgommers/Code/numpy/numpy/testing/utils.py"", line 510, in assert_almost_equal\r\n        raise AssertionError(_build_err_msg())\r\n    AssertionError: \r\n    Arrays are not almost equal to 2 decimals\r\n     ACTUAL: 0.07500000000000001\r\n     DESIRED: -1.8749999999999996\r\n\r\n    ======================================================================\r\n    FAIL: test_simple (test_stats.TestHistogram)\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/home/rgommers/Code/scipy/scipy/stats/tests/test_stats.py"", line 876, in test_simple\r\n        decimal=2)\r\n      File ""/home/rgommers/Code/numpy/numpy/testing/utils.py"", line 886, in assert_array_almost_equal\r\n        precision=decimal)\r\n      File ""/home/rgommers/Code/numpy/numpy/testing/utils.py"", line 708, in assert_array_compare\r\n        raise AssertionError(msg)\r\n    AssertionError: \r\n    Arrays are not almost equal to 2 decimals\r\n\r\n    (mismatch 20.0%)\r\n     x: array([ 1.,  1.,  1.,  2.,  2.,  1.,  1.,  0.,  1.,  1.])\r\n     y: array([ 1.,  1.,  1.,  2.,  1.,  2.,  1.,  0.,  1.,  1.])\r\n\r\n    ======================================================================\r\n    FAIL: test_weighting (test_stats.TestHistogram)\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/home/rgommers/Code/scipy/scipy/stats/tests/test_stats.py"", line 923, in test_weighting\r\n        decimal=2)\r\n      File ""/home/rgommers/Code/numpy/numpy/testing/utils.py"", line 886, in assert_array_almost_equal\r\n        precision=decimal)\r\n      File ""/home/rgommers/Code/numpy/numpy/testing/utils.py"", line 708, in assert_array_compare\r\n        raise AssertionError(msg)\r\n    AssertionError: \r\n    Arrays are not almost equal to 2 decimals\r\n\r\n    (mismatch 20.0%)\r\n     x: array([   4. ,    0. ,    4.5,   -0.9,    0. ,    0.3,  110.2,    0. ,\r\n              0. ,   42. ])\r\n     y: array([   4. ,    0. ,    4.5,   -0.9,    0. ,    0.3,    7. ,  103.2,\r\n              0. ,   42. ])\r\n\r\n    ----------------------------------------------------------------------\r\n\r\nAs this is a regression, it should be looked at for 1.10'"
numpy/numpy#6270,103972655,cgohlke,cgohlke,2015-08-31 02:06:14,2015-10-10 13:45:40,2015-08-31 02:38:09,closed,,,1,50 - Duplicate,https://api.github.com/repos/numpy/numpy/issues/6270,b'Scipy 0.16.0 test failure with maintenance/1.10.x',"b'On Windows using numpy+mkl, I get a scipy test failure with numpy maintenance/1.10.x because `np.histogram` returns different results for numpy 1.9.2 and 1.10.x:\r\n\r\n```\r\n======================================================================\r\nFAIL: test_weighting (test_stats.TestHistogram)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\scipy\\stats\\tests\\test_stats.py"", line 892, in test_weighting\r\n    decimal=2)\r\n  File ""X:\\Python34\\lib\\site-packages\\numpy\\testing\\utils.py"", line 886, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""X:\\Python34\\lib\\site-packages\\numpy\\testing\\utils.py"", line 708, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 40.0%)\r\n x: array([   4. ,    0. ,    4.5,   -0.9,    0. ,    0.3,  110.2,    0. ,\r\n          0. ,   42. ])\r\n y: array([   4. ,    0. ,    4.5,   -0.9,    0.3,    0. ,    7. ,  103.2,\r\n          0. ,   42. ])\r\n```\r\nStandalone code:\r\n```\r\nimport numpy as np\r\na = np.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2])\r\nweights = np.array([1.00000000e+00,  3.00000000e+00, 4.50000000e+00,\r\n                    1.00000000e-01, -1.00000000e+00, 0.00000000e+00,\r\n                    3.00000000e-01,  7.00000000e+00, 1.03200000e+02,\r\n                    2.00000000e+00,  4.00000000e+01])\r\nhist, bin_edges = np.histogram(a, bins=10, range=(0.2, 1.2), weights=weights)\r\nprint(hist)\r\n```\r\nnumpy 1.9.2:\r\n```\r\n[   4.     0.     4.5   -0.9    0.     0.3  110.2    0.     0.    42. ]\r\n```\r\nnumpy 1.10.x:\r\n```\r\n[   4.     0.     4.5   -0.9    0.3    0.     7.   103.2    0.    42. ]\r\n```'"
pypa/pip#3194,112016470,jkseppan,BrownTruck,2015-10-18 12:22:16,2016-05-26 10:53:22,2016-05-26 10:53:22,closed,,,60,bit rotted,https://api.github.com/repos/pypa/pip/issues/3194,"b'Implement a ""pip upgrade"" command'","b'Upgrade those requirements given directly but dependencies only as needed to satisfy dependency relations. Inspired by Nathaniel Smith\'s post at http://thread.gmane.org/gmane.comp.python.scientific.user/36377.\r\n\r\nSee also: #59\n\n<!-- Reviewable:start -->\n[<img src=""https://reviewable.io/review_button.png"" height=40 alt=""Review on Reviewable""/>](https://reviewable.io/reviews/pypa/pip/3194)\n<!-- Reviewable:end -->\n'"
brian-team/brian2#639,129156884,rcaze,mstimberg,2016-01-27 14:17:13,2016-01-28 16:43:22,2016-01-28 16:43:18,closed,,,8,,https://api.github.com/repos/brian-team/brian2/issues/639,b'SpatialNeuron Events',b'No sure If I had to but this is just to add events arg.'
ContinuumIO/anaconda-issues#642,129400814,mstimberg,ilanschnell,2016-01-28 09:53:50,2016-02-04 05:01:06,2016-02-04 05:01:02,closed,ilanschnell,,1,,https://api.github.com/repos/ContinuumIO/anaconda-issues/issues/642,b'No scipy 0.16.1 for win-32/win-64?',"b'We are affected by the `solve_banded` regression from scipy 0.15.1 to 0.16.0 (scipy/scipy#5127) which was fixed with scipy release 0.16.1 -- it appears that there are no Windows builds for this release, though: https://anaconda.org/anaconda/scipy/files?version=0.16.1\r\n\r\nThis is problematic for us, since we cannot simply use scipy 0.15.1 either, since this does not have any Python 3.5 builds.'"
TUW-GEO/pytesmo#58,103057438,cpaulik,cpaulik,2015-08-25 15:55:34,2015-08-26 16:51:03,2015-08-26 16:51:03,closed,,,2,,https://api.github.com/repos/TUW-GEO/pytesmo/issues/58,b'temporal matching in version 0.3.2 is painfully slow',b'I guess this is because of my usage of the cKDTree in 2D. We should find another way.'
pysal/pysal#726,122416097,ljwolf,sjsrey,2015-12-16 02:55:29,2016-03-07 19:41:26,2016-03-07 19:41:26,closed,sjsrey,Release,10,Blocking;Bug,https://api.github.com/repos/pysal/pysal/issues/726,b'Compatibility for Scipy 16.1',"b'As @sjsrey found, upgrading to scipy 16.1 in our testing suite causes quite a few errors. [The scipy changelog](https://docs.scipy.org/doc/scipy/reference/release.0.16.1.html) shows that nothing big changed. \r\n- [bugfix for ckdtree](https://github.com/scipy/scipy/issues/5077)\r\n- [ckdtree distance computation fix](https://github.com/scipy/scipy/pull/5088)\r\n\r\nNot sure what, exactly, we need to fix yet, but tests were failing all over `weights` and `esda`. \r\n\r\nMy hope is that the issue with the changes to `ckdtree` or something slightly different in `kdtree`. '"
nipy/dipy#652,78296136,argriffing,arokem,2015-05-19 23:45:15,2015-05-31 00:08:57,2015-05-31 00:08:57,closed,,,12,,https://api.github.com/repos/nipy/dipy/issues/652,b'MAINT: work around scipy bug in sph_harm',b'related bugs/issues/PRs:\r\nhttps://github.com/scipy/scipy/issues/4887\r\nhttps://github.com/scipy/scipy/issues/4895\r\nhttps://github.com/nipy/dipy/issues/611\r\nhttps://github.com/nipy/dipy/pull/612'
numpy/numpy#5895,78423011,pv,,2015-05-20 08:08:07,2015-05-20 08:17:11,None,open,,,0,,https://api.github.com/repos/numpy/numpy/issues/5895,b'float32 has priority over float64 for scalar integer ufunc inputs with other arrays',"b""Consider this:\r\n```\r\n>>> from scipy.special import sph_harm\r\n>>> sph_harm.types\r\n['llff->F', 'lldd->D', 'ffff->F', 'dddd->D']\r\n>>> sph_harm(0,[0],0,0).dtype\r\ndtype('complex64')   # <- uh-oh\r\n>>> sph_harm(0,[0],0,0.0).dtype\r\ndtype('complex128')\r\n>>> sph_harm(0,0,0,0).dtype\r\ndtype('complex128')\r\n>>> sph_harm(0,[0],[0],0).dtype\r\ndtype('complex128')\r\n```\r\nApparently, the ufunc loop selector prefers to cast integers to float32 in this case, which is somewhat surprising. This may have to do with the fact that given an integer array as one of the first two arguments, there are still two possible loop choices, and deciding which to use would need considering appropriate scalar casting rules for the last two arguments. Note that if one of the last arguments is an array instead of a scalar, the dtype decision is as expected, so maybe this is an issue with scalar special cases."""
numpy/numpy#6892,124097575,Eric89GXL,charris,2015-12-28 17:48:30,2015-12-28 20:04:42,2015-12-28 18:50:55,closed,,,6,10 - Maintenance;component: numpy.distutils,https://api.github.com/repos/numpy/numpy/issues/6892,b'FIX: Fix MKL for Linux',"b""The fix from line 172 (`return ['/O1']  # Scipy test failures with /O2`) for `IntelVisualFCompiler` were not applied to the Linux variants `IntelFCompiler` and `IntelEM64TFCompiler`, but on Linux I need `-O1` to avoid `scipy` bugs (https://github.com/scipy/scipy/issues/5621). This fixes the issue."""
numpy/numpy#5232,46818961,markwmuller,jaimefrio,2014-10-25 18:16:36,2014-10-26 13:16:03,2014-10-26 13:16:03,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/5232,b'Linalg SVD fails if not computing UV',"b'I have a matrix which raises a type error when I run linalg.svd on it and set compute_uv to false. However, the SVD works fine if I also compute U and V.\r\nI have a pickle file with the particular matrix here, and a simple script to illustrate: https://www.wuala.com/markwmuller/Documents/NumpyLinalgSVD_problem/?key=GG0oL349Ak7J\r\n\r\nI\'m using numpy version 1.8.1, on python 2.7.6.\r\n\r\nThis works: \r\n<code>u, s, v = numpy.linalg.svd(mat)</code>\r\nBut this does not: \r\n<code>s2 = numpy.linalg.svd(mat, compute_uv=False)</code>\r\n\r\nThe message reads:\r\n<code>\r\n  File ""svdProb.py"", line 9, in <module>\r\n    s2 = numpy.linalg.svd(mat, compute_uv=False)\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\linalg\\linalg.py"", line 1339, in svd\r\n\r\n    s = gufunc(a, signature=signature, extobj=extobj)\r\nTypeError: No loop matching the specified signature was found for ufunc svd_m\r\n\r\n</code>'"
cython/cython#56,1449218,markflorisson,robertwb,2011-08-20 09:53:00,2014-08-04 10:42:01,2011-08-21 04:34:05,closed,,,0,,https://api.github.com/repos/cython/cython/issues/56,b'Acquire the GIL to build the traceback in nogil functions',
numpy/numpy#5906,79209375,njsmith,,2015-05-21 23:53:23,2016-01-06 10:46:27,None,open,,,36,,https://api.github.com/repos/numpy/numpy/issues/5906,b'lapack-lite should use 64-bit integer indices',"b""Currently lapack-lite uses `int` for indices, so large arrays cause overflow and things crash (e.g. #5898). There's a separate need to handle this issue when using real BLAS/LAPACK (which may or may not handle arrays with >2**31 elements, depending on the vendor), but when falling back on our built-in code there's no reason we shouldn't get this right."""
nipy/dipy#688,97555219,hassemlal,hassemlal,2015-07-27 21:09:33,2015-10-15 18:48:57,2015-10-15 18:46:26,closed,,,17,,https://api.github.com/repos/nipy/dipy/issues/688,b'dipy.test() fails on centos 6.x / python2.6',"b'Hi,\r\nI tried running the dipy test suite, but it returns multiple failures (see below). I am running on centos 6.5 with python2.6 and virtualenv.\r\n\r\n```\r\n$ python -c ""import dipy; dipy.test()""\r\n.................Running unit tests for dipy\r\nNumPy version 1.9.2\r\nNumPy is installed in /home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy\r\nPython version 2.6.6 (r266:84292, Jan 22 2014, 09:42:36) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)]\r\nnose version 1.3.7\r\n\r\n[clip]\r\n\r\n.........SS.........\r\n======================================================================\r\nFAIL: dipy.core.tests.test_sphere.test_interp_rbf\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/core/tests/test_sphere.py"", line 374, in test_interp_rbf\r\n    nt.assert_(np.mean(np.abs(interp_data_a - expected)) < 0.1)\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy/testing/utils.py"", line 53, in assert_\r\n    raise AssertionError(smsg)  \r\nAssertionError\r\n\r\n======================================================================\r\nFAIL: dipy.reconst.tests.test_csdeconv.test_odfdeconv\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/reconst/tests/test_csdeconv.py"", line 220, in test_odfdeconv\r\n    assert_equal(len(w) > 0, False)\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy/testing/utils.py"", line 334, in assert_equal\r\n    raise AssertionError(msg)   \r\nAssertionError:\r\nItems are not equal:\r\n ACTUAL: True\r\n DESIRED: False\r\n\r\n======================================================================\r\nFAIL: dipy.segment.tests.test_metric.test_metric_cosine\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/segment/tests/test_metric.py"", line 177, in test_metric_cosine\r\n    assert_equal(distance, 0.)  \r\nAssertionError: array([[  4.74318692e-09]]) != 0.0\r\n\r\n----------------------------------------------------------------------\r\nRan 451 tests in 1043.841s\r\n\r\nFAILED (SKIP=9, failures=3)\r\n```\r\n'"
nipy/dipy#688,97555219,hassemlal,hassemlal,2015-07-27 21:09:33,2015-10-15 18:48:57,2015-10-15 18:46:26,closed,,,17,,https://api.github.com/repos/nipy/dipy/issues/688,b'dipy.test() fails on centos 6.x / python2.6',"b'Hi,\r\nI tried running the dipy test suite, but it returns multiple failures (see below). I am running on centos 6.5 with python2.6 and virtualenv.\r\n\r\n```\r\n$ python -c ""import dipy; dipy.test()""\r\n.................Running unit tests for dipy\r\nNumPy version 1.9.2\r\nNumPy is installed in /home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy\r\nPython version 2.6.6 (r266:84292, Jan 22 2014, 09:42:36) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)]\r\nnose version 1.3.7\r\n\r\n[clip]\r\n\r\n.........SS.........\r\n======================================================================\r\nFAIL: dipy.core.tests.test_sphere.test_interp_rbf\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/core/tests/test_sphere.py"", line 374, in test_interp_rbf\r\n    nt.assert_(np.mean(np.abs(interp_data_a - expected)) < 0.1)\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy/testing/utils.py"", line 53, in assert_\r\n    raise AssertionError(smsg)  \r\nAssertionError\r\n\r\n======================================================================\r\nFAIL: dipy.reconst.tests.test_csdeconv.test_odfdeconv\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/reconst/tests/test_csdeconv.py"", line 220, in test_odfdeconv\r\n    assert_equal(len(w) > 0, False)\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy/testing/utils.py"", line 334, in assert_equal\r\n    raise AssertionError(msg)   \r\nAssertionError:\r\nItems are not equal:\r\n ACTUAL: True\r\n DESIRED: False\r\n\r\n======================================================================\r\nFAIL: dipy.segment.tests.test_metric.test_metric_cosine\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/segment/tests/test_metric.py"", line 177, in test_metric_cosine\r\n    assert_equal(distance, 0.)  \r\nAssertionError: array([[  4.74318692e-09]]) != 0.0\r\n\r\n----------------------------------------------------------------------\r\nRan 451 tests in 1043.841s\r\n\r\nFAILED (SKIP=9, failures=3)\r\n```\r\n'"
numpy/numpy#5365,51678000,cournape,charris,2014-12-11 11:54:22,2015-01-19 19:52:33,2015-01-19 19:52:33,closed,,,17,,https://api.github.com/repos/numpy/numpy/issues/5365,b'BUG: tighten alignment for complex types.',"b'Complex types memory representations in C for a given `type` are guaranteed to be equivalent to `type[2]`. As such, the alignment of `complex<type>` should be the same as `type`.\r\n\r\nI tested scipy 0.14.x (at scipy/scipy@d6c32069f1379b3517eee8511e323607c246c66c, containing the f2py workaround) in the following setups:\r\n\r\n\r\n##### numpy 1.9.1, w/ this patch\r\n\r\n* one genuine error (all platforms). Unlikely to be related to numpy.\r\n\r\n```\r\n[192.168.33.13] out: ======================================================================\r\n[192.168.33.13] out: ERROR: test_antiderivative_of_constant (test_interpolate.TestPPoly)\r\n[192.168.33.13] out: ----------------------------------------------------------------------\r\n[192.168.33.13] out: Traceback (most recent call last):\r\n[192.168.33.13] out:   File ""/home/vagrant/src/master-env/lib/python2.7/site-packages/scipy/interpolate/tests/test_interpolate.py"", line 659, in test_antiderivative_of_constant\r\n[192.168.33.13] out:     assert_equal(p.antiderivative().c, PPoly([[1], [0]], [0, 1]).c)\r\n[192.168.33.13] out:   File ""/home/vagrant/src/master-env/lib/python2.7/site-packages/scipy/interpolate/interpolate.py"", line 821, in antiderivative\r\n[192.168.33.13] out:     self.x, nu)\r\n[192.168.33.13] out:   File ""_ppoly.pyx"", line 142, in scipy.interpolate._ppoly.fix_continuity (scipy/interpolate/_ppoly.c:5462)\r\n[192.168.33.13] out: ValueError: order too large\r\n```\r\n\r\n* a few minor precision issues that have been here for a while otherwise.\r\n\r\n##### numpy 1.9.1, w/o this patch\r\n\r\n* Same results as above for rh5 and osx and windows 64 bits, but a dozen new errors for windows 32 bits:\r\n\r\n```\r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 85, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([  0.+0.j,  12.+0.j,  24.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 71, in test_x_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(real_y,y)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([  0.+0.j,   7.+0.j,  14.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 78, in test_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 219, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(x[::4],y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 213, in test_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(x,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZscal)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 145, in test_simple\r\n[172.17.5.102] err:     assert_array_almost_equal(real_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 288, in test_simple\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 315, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_x,x[::4])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 297, in test_x_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_x,x[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  2.+0.j,  4.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 307, in test_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 87, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([  0.+0.j,  12.+0.j,  24.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 80, in test_y_stride\r\n[172.17.5.102] err:     assert_array_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 219, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_equal(x[::4],y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 213, in test_y_stride\r\n[172.17.5.102] err:     assert_array_equal(x,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZscal)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 146, in test_simple\r\n[172.17.5.102] err:     assert_array_equal(real_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 287, in test_simple\r\n[172.17.5.102] err:     assert_array_equal(desired_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 314, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_equal(desired_x,x[::4])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 297, in test_x_stride\r\n[172.17.5.102] err:     assert_array_equal(desired_y,y)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  2.+0.j,  4.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 305, in test_y_stride\r\n[172.17.5.102] err:     assert_array_equal(desired_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n\r\n```\r\n'"
ContinuumIO/anaconda-issues#245,55245329,xiaoweiz,xiaoweiz,2015-01-23 05:00:48,2015-02-08 11:15:24,2015-02-08 11:15:23,closed,,,1,,https://api.github.com/repos/ContinuumIO/anaconda-issues/issues/245,b'mklfft causes scipy test failures',"b'The issue was initially reported at scipy/scipy#4400 and it turned out to be a possible Anaconda issue as hinted by @asmeurer . \r\n\r\nHere\'s system setup:\r\n+ OS X Mavericks\r\n+ Anaconda 2.1.0 with MKL optimization\r\n+ Scipy 0.15.1\r\n+ Numpy 1.9.1\r\n\r\nWith `mklfft` installed\r\n```shell\r\npython -c ""from scipy import ndimage; ndimage.test(\'full\')""\r\nRunning unit tests for scipy.ndimage\r\nNumPy version 1.9.1\r\nNumPy is installed in /Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/numpy\r\nSciPy version 0.15.0\r\nSciPy is installed in /Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/scipy\r\nPython version 2.7.9 |Continuum Analytics, Inc.| (default, Dec 15 2014, 10:37:34) [GCC 4.2.1 (Apple Inc. build 5577)]\r\nnose version 1.3.4\r\n...............................................................................................................................................................................................................................................................................................FF..............................................................................................................................................................\r\n======================================================================\r\nFAIL: test_ndimage.TestNdimage.test_fourier_shift_complex01\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/scipy/ndimage/tests/test_ndimage.py"", line 1336, in test_fourier_shift_complex01\r\n    assert_array_almost_equal(a.real[1:, 1:], expected[:-1, :-1])\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 78.9247311828%)\r\n x: array([[  1.514934e-06,   1.000002e+00,   2.000001e+00,   3.000002e+00,\r\n          4.000001e+00,   5.000002e+00,   6.000002e+00,   7.000002e+00,\r\n          8.000001e+00,   9.000001e+00,   1.000000e+01,   1.100000e+01,...\r\n y: array([[   0.+0.j,    1.+0.j,    2.+0.j,    3.+0.j,    4.+0.j,    5.+0.j,\r\n           6.+0.j,    7.+0.j,    8.+0.j,    9.+0.j,   10.+0.j,   11.+0.j,\r\n          12.+0.j,   13.+0.j,   14.+0.j],...\r\n\r\n======================================================================\r\nFAIL: test_ndimage.TestNdimage.test_fourier_shift_real01\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/scipy/ndimage/tests/test_ndimage.py"", line 1322, in test_fourier_shift_real01\r\n    assert_array_almost_equal(a[1:, 1:], expected[:-1, :-1])\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array([[  46.954332, -157.977735, -339.941516, -470.421044, -530.813966,\r\n        -511.298085, -415.024098, -256.      ,  -58.002222,  149.259413,\r\n         334.880048,  470.421044,  535.875433,  520.016407,  426.071989],...\r\n y: array([[   0.,    1.,    2.,    3.,    4.,    5.,    6.,    7.,    8.,\r\n           9.,   10.,   11.,   12.,   13.,   14.],\r\n       [  16.,   17.,   18.,   19.,   20.,   21.,   22.,   23.,   24.,...\r\n\r\n----------------------------------------------------------------------\r\nRan 447 tests in 2.189s\r\n\r\nFAILED (failures=2)\r\n```\r\n\r\nAfter removing `mklfft`, the issue goes away. \r\n\r\n```shell\r\nconda remove mklfft                                                      [11:00:43]\r\nFetching package metadata: ..\r\n\r\nPackage plan for package removal in environment /Users/zhangxiaowei/anaconda:\r\n\r\nThe following packages will be REMOVED:\r\n\r\n    mklfft: 1.0-np19py27_p0\r\n\r\nProceed ([y]/n)?\r\n\r\nUnlinking packages ...\r\n[      COMPLETE      ] |############################################################| 100%\r\n```\r\n```shell\r\npython -c ""from scipy import ndimage; ndimage.test(\'full\')""              [11:01:06]\r\nRunning unit tests for scipy.ndimage\r\nNumPy version 1.9.1\r\nNumPy is installed in /Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/numpy\r\nSciPy version 0.15.1\r\nSciPy is installed in /Users/zhangxiaowei/anaconda/lib/python2.7/site-packages/scipy\r\nPython version 2.7.9 |Continuum Analytics, Inc.| (default, Dec 15 2014, 10:37:34) [GCC 4.2.1 (Apple Inc. build 5577)]\r\nnose version 1.3.4\r\n...............................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\n----------------------------------------------------------------------\r\nRan 447 tests in 2.088s\r\n\r\nOK\r\n```'"
argriffing/scipy#3,52406592,larsmans,argriffing,2014-12-18 19:33:39,2016-02-23 13:39:10,2014-12-18 19:39:25,closed,,,0,,https://api.github.com/repos/argriffing/scipy/issues/3,b'TST: spatial: regression test for gh-4290',
scikit-learn/scikit-learn#4014,52964375,argriffing,argriffing,2014-12-27 19:45:33,2014-12-28 19:23:05,2014-12-28 19:23:05,closed,,,1,,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4014,b'scipy breakage of scikit-learn hamming distance between ... things',"b'My scipy PR https://github.com/scipy/scipy/pull/4293 merged to fix problems in scikit-bio is now causing a test failure in scikit-learn.  I\'ll try to figure out the best way to fix this, probably by figuring out exactly how CLASSIFICATION_METRICS\\[\'hamming\'\\](y1_str, y2_str, pos_label=""spam"") in sklearn ends up calling `hamming(x, y)` in scipy so that I can add a scipy regression test or possibly a sklearn PR.\r\n```\r\n======================================================================\r\nERROR: Ensure that classification metrics with string labels\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  /sklearn/metrics/tests/test_common.py"", line 531, in test_invariance_string_vs_numbers_labels\r\n    measure_with_str = metric_str(y1_str, y2_str)\r\n  /sklearn/metrics/classification.py"", line 1307, in hamming_loss\r\n    return sp_hamming(y_true, y_pred)\r\n  /scipy/spatial/distance.py"", line 369, in hamming\r\n    return np.not_equal(u, v).mean()\r\nAttributeError: \'NotImplementedType\' object has no attribute \'mean\'\r\n```'"
biocore/scikit-bio#514,36814040,wasade,jairideout,2014-06-30 16:31:44,2015-02-19 17:13:55,2015-02-19 17:13:04,closed,johnchase,,9,quick fix,https://api.github.com/repos/biocore/scikit-bio/issues/514,b'BiologicalSequences.distance can only length check Hamming',"b'See [here](https://github.com/biocore/scikit-bio/blob/master/skbio/core/sequence.py#L659) with a discussion in #508. Since it is the responsibility of the distance methods to length check, one possible solution is to try/except on the call to the distance method and thrown an informative error. '"
biocore/scikit-bio#514,36814040,wasade,jairideout,2014-06-30 16:31:44,2015-02-19 17:13:55,2015-02-19 17:13:04,closed,johnchase,,9,quick fix,https://api.github.com/repos/biocore/scikit-bio/issues/514,b'BiologicalSequences.distance can only length check Hamming',"b'See [here](https://github.com/biocore/scikit-bio/blob/master/skbio/core/sequence.py#L659) with a discussion in #508. Since it is the responsibility of the distance methods to length check, one possible solution is to try/except on the call to the distance method and thrown an informative error. '"
numpy/numpy#5365,51678000,cournape,charris,2014-12-11 11:54:22,2015-01-19 19:52:33,2015-01-19 19:52:33,closed,,,17,,https://api.github.com/repos/numpy/numpy/issues/5365,b'BUG: tighten alignment for complex types.',"b'Complex types memory representations in C for a given `type` are guaranteed to be equivalent to `type[2]`. As such, the alignment of `complex<type>` should be the same as `type`.\r\n\r\nI tested scipy 0.14.x (at scipy/scipy@d6c32069f1379b3517eee8511e323607c246c66c, containing the f2py workaround) in the following setups:\r\n\r\n\r\n##### numpy 1.9.1, w/ this patch\r\n\r\n* one genuine error (all platforms). Unlikely to be related to numpy.\r\n\r\n```\r\n[192.168.33.13] out: ======================================================================\r\n[192.168.33.13] out: ERROR: test_antiderivative_of_constant (test_interpolate.TestPPoly)\r\n[192.168.33.13] out: ----------------------------------------------------------------------\r\n[192.168.33.13] out: Traceback (most recent call last):\r\n[192.168.33.13] out:   File ""/home/vagrant/src/master-env/lib/python2.7/site-packages/scipy/interpolate/tests/test_interpolate.py"", line 659, in test_antiderivative_of_constant\r\n[192.168.33.13] out:     assert_equal(p.antiderivative().c, PPoly([[1], [0]], [0, 1]).c)\r\n[192.168.33.13] out:   File ""/home/vagrant/src/master-env/lib/python2.7/site-packages/scipy/interpolate/interpolate.py"", line 821, in antiderivative\r\n[192.168.33.13] out:     self.x, nu)\r\n[192.168.33.13] out:   File ""_ppoly.pyx"", line 142, in scipy.interpolate._ppoly.fix_continuity (scipy/interpolate/_ppoly.c:5462)\r\n[192.168.33.13] out: ValueError: order too large\r\n```\r\n\r\n* a few minor precision issues that have been here for a while otherwise.\r\n\r\n##### numpy 1.9.1, w/o this patch\r\n\r\n* Same results as above for rh5 and osx and windows 64 bits, but a dozen new errors for windows 32 bits:\r\n\r\n```\r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 85, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([  0.+0.j,  12.+0.j,  24.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 71, in test_x_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(real_y,y)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([  0.+0.j,   7.+0.j,  14.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 78, in test_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 219, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(x[::4],y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 213, in test_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(x,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZscal)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 145, in test_simple\r\n[172.17.5.102] err:     assert_array_almost_equal(real_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 288, in test_simple\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 315, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_x,x[::4])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 297, in test_x_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_x,x[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  2.+0.j,  4.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\lib\\blas\\tests\\test_fblas.py"", line 307, in test_y_stride\r\n[172.17.5.102] err:     assert_array_almost_equal(desired_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n[172.17.5.102] err:     precision=decimal)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not almost equal to 6 decimals\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 87, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([  0.+0.j,  12.+0.j,  24.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZaxpy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 80, in test_y_stride\r\n[172.17.5.102] err:     assert_array_equal(real_y,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 219, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_equal(x[::4],y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZcopy)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 213, in test_y_stride\r\n[172.17.5.102] err:     assert_array_equal(x,y[::2])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZscal)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 146, in test_simple\r\n[172.17.5.102] err:     assert_array_equal(real_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  3.+0.j,  6.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_simple (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 287, in test_simple\r\n[172.17.5.102] err:     assert_array_equal(desired_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_and_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 314, in test_x_and_y_stride\r\n[172.17.5.102] err:     assert_array_equal(desired_x,x[::4])\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  4.+0.j,  8.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_x_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 297, in test_x_stride\r\n[172.17.5.102] err:     assert_array_equal(desired_y,y)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  2.+0.j,  4.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: ======================================================================\r\n[172.17.5.102] err: FAIL: test_y_stride (test_fblas.TestZswap)\r\n[172.17.5.102] err: ----------------------------------------------------------------------\r\n[172.17.5.102] err: Traceback (most recent call last):\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\scipy\\linalg\\tests\\test_fblas.py"", line 305, in test_y_stride\r\n[172.17.5.102] err:     assert_array_equal(desired_x,x)\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n[172.17.5.102] err:     verbose=verbose, header=\'Arrays are not equal\')\r\n[172.17.5.102] err:   File ""C:\\Users\\vagrant\\src\\master-env\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n[172.17.5.102] err:     raise AssertionError(msg)\r\n[172.17.5.102] err: AssertionError:\r\n[172.17.5.102] err: Arrays are not equal\r\n[172.17.5.102] err: \r\n[172.17.5.102] err: (mismatch 66.6666666667%)\r\n[172.17.5.102] err:  x: array([ 0.+0.j,  0.+0.j,  0.+0.j])\r\n[172.17.5.102] err:  y: array([ 0.+0.j,  1.+0.j,  2.+0.j])\r\n[172.17.5.102] err: \r\n\r\n```\r\n'"
numpy/numpy#5352,51197354,argriffing,,2014-12-06 21:48:11,2014-12-06 23:14:13,None,open,,,1,,https://api.github.com/repos/numpy/numpy/issues/5352,"b""ndarray astype docstring is missing 'new in version' hints""","b""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html\r\n\r\nIt would have been nice to have known that `astype` doesn't do keyword arguments in numpy 1.5.1 before writing https://github.com/scipy/scipy/pull/4238 without having to go exploring in http://docs.scipy.org/doc/numpy-1.5.x/reference/generated/numpy.ndarray.astype.html or digging through github history.\r\n\r\nAs always pull requests are welcome!"""
TUW-GEO/pytesmo#51,93830420,cpaulik,cpaulik,2015-07-08 16:14:39,2015-07-08 23:57:55,2015-07-08 23:57:55,closed,,,0,,https://api.github.com/repos/TUW-GEO/pytesmo/issues/51,b'temporal matching produces no match on boundaries',b'This is related to https://github.com/scipy/scipy/issues/4233 \r\n\r\nWe should use http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.interpolate.NearestNDInterpolator.html#scipy.interpolate.NearestNDInterpolator directly.'
numpy/numpy#2951,10347202,dmvianna,charris,2013-01-27 04:15:29,2014-08-20 13:06:11,2014-03-25 18:05:08,closed,,,4,component: numpy.core;priority: normal;Proposal,https://api.github.com/repos/numpy/numpy/issues/2951,b'numpy.int64 is not instance of int',"b""As reported in [xlwt](https://github.com/python-excel/xlwt/issues/15):\r\n\r\nHere is an examination of numpy behaviour (Python 2.7.3, numpy 1.6.2)\r\n```python\r\n>>> import numpy\r\n>>> data = [t(123456) for t in (numpy.int32, numpy.int64, numpy.float64)]\r\n>>> [type(d) for d in data]\r\n[<type 'numpy.int32'>, <type 'numpy.int64'>, <type 'numpy.float64'>]\r\n>>> data\r\n[123456, 123456, 123456.0]\r\n>>> check_types = (int, long, float)\r\n>>> for d in data:\r\n...     for c in check_types:\r\n...         print type(d), repr(c), isinstance(d, c)\r\n...\r\n<type 'numpy.int32'> <type 'int'> True\r\n<type 'numpy.int32'> <type 'long'> False\r\n<type 'numpy.int32'> <type 'float'> False\r\n<type 'numpy.int64'> <type 'int'> False\r\n<type 'numpy.int64'> <type 'long'> False\r\n<type 'numpy.int64'> <type 'float'> False\r\n<type 'numpy.float64'> <type 'int'> False\r\n<type 'numpy.float64'> <type 'long'> False\r\n<type 'numpy.float64'> <type 'float'> True\r\n>>>\r\n```\r\nLooks like numpy has done the work to make its int32 and float64 recognisable by other software but not int64."""
Homebrew/homebrew-python#110,31058873,samueljohn,,2014-04-08 10:24:20,2015-01-11 18:47:43,None,open,,,8,question,https://api.github.com/repos/Homebrew/homebrew-python/issues/110,b'scipy: building with `--cc=gcc-4.8` leads to tests error out',"b'In https://github.com/Homebrew/homebrew-python/commit/93c28cd455a8eb2f63bf376a4a78edbed3e2e349 @arjun810 authored the addition of `-D__ACCELERATE__` in order to build with `gcc` compilers.\r\n\r\nIn a later [commit](https://github.com/Homebrew/homebrew-python/commit/eb68d3210547e67ba24ca42c59d3cc731f31bcba), I unfortunately had to revert because it seem to break the tests with the current version of scipy.\r\n\r\nI was not able to work out a solution. So not sure if it is only my haswell Mac or if anyone can come up with a solution. In a recent commit bea170d941886815ccd4337056c3f815691a28ac I fixed the conditional to allow gcc compilers.\r\n\r\nAny ideas? Does it work for you? Which deps of scipy need to be recompiled also with `--cc=gcc-4.8`.\r\n\r\n'"
flame/blis#42,137484631,insertinterestingnamehere,,2016-03-01 06:27:37,2016-03-01 22:58:45,None,open,,,22,,https://api.github.com/repos/flame/blis/issues/42,b'Match default integer sizes to standard interfaces',"b""This matches the default integers to `size_t` and `ptrdiff_t` so that they match the size of a pointer even on LLP64 platforms like Windows.\r\nI also went ahead and changed the default `f77_int` definition from long to int to more closely match the Fortran and C blas interfaces. Though, to my knowledge, it's not standardized anywhere, Fortran compilers usually use 32 bit integers by default.\r\n\r\nI'm in the process of testing this myself. Putting this here for discussion for now."""
numpy/numpy#5223,46572131,sturlamolden,juliantaylor,2014-10-22 23:15:55,2014-10-26 16:18:17,2014-10-26 16:18:17,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/5223,"b""BUG: Ugly fix for Apple's cblas_sgemv segfault""","b""SGEMV in Accelerate framework will segfault on MacOS X version 10.9\r\n(aka Mavericks) if arrays are not aligned to 32 byte boundaries\r\nand the CPU supports AVX instructions. This can produce segfaults\r\nin numpy.dot if we use numpy.float32 as dtype. This patch overshadows\r\nthe symbols cblas_sgemv, sgemv_ and sgemv exported by Accelerate\r\nto produce the correct behavior. The MacOS X version and CPU specs\r\nare checked on module import. If Mavericks and AVX are detected\r\nthe call to SGEMV is emulated with a call to SGEMM if the arrays\r\nare not 32 byte aligned. If the exported symbols cannot be\r\novershadowed on module import, a fatal error is produced and the\r\nprocess aborts. All the fixes are in a self-contained C file\r\nand do not alter the _dotblas C code. The patch is not applied\r\nunless NumPy is configured to link with Apple's Accelerate\r\nframework."""
numpy/numpy#4007,22049781,piskvorky,juliantaylor,2013-11-04 13:05:02,2014-10-29 00:02:19,2014-10-26 16:20:24,closed,,,155,component: numpy.linalg,https://api.github.com/repos/numpy/numpy/issues/4007,b'numpy.dot crash with numpy.float32 input',"b""A user of gensim @fbkarsdorp reported crash (segfault) with NumPy: piskvorky/gensim#131\r\n\r\nThe crash seems to have nothing to do with gensim, so I'm transferring the issue here. It happens in `dot` of matrix*vector in single precision, on his OS X Maverick.\r\n"""
numpy/numpy#5237,46877651,sturlamolden,charris,2014-10-27 07:37:59,2015-05-03 19:29:07,2015-05-03 19:16:14,closed,,,29,10 - Maintenance;component: build;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5237,b'BUG: Workaround segfault in Apple Accelerate framework SGEMV function',"b""SGEMV in Accelerate framework will segfault on MacOS X version 10.9\r\n(aka Mavericks) if arrays are not aligned to 32 byte boundaries\r\nand the CPU supports AVX instructions. This can produce segfaults\r\nin np.dot. This patch overshadows the symbols cblas_sgemv, sgemv_ and\r\nsgemv exported by Accelerate to produce the correct behavior. The MacOS X\r\nversion and CPU specs are checked on module import. If Mavericks and\r\nAVX are detected the call to SGEMV is emulated with a call to SGEMM\r\nif the arrays are not 32 byte aligned. If the exported symbols cannot\r\nbe overshadowed on module import, a fatal error is produced and the\r\nprocess aborts. All the fixes are in a self-contained C file\r\nand do not alter the multiarray C code. The patch is not applied\r\nunless NumPy is configured to link with Apple's Accelerate\r\nframework."""
Homebrew/homebrew-python#110,31058873,samueljohn,,2014-04-08 10:24:20,2015-01-11 18:47:43,None,open,,,8,question,https://api.github.com/repos/Homebrew/homebrew-python/issues/110,b'scipy: building with `--cc=gcc-4.8` leads to tests error out',"b'In https://github.com/Homebrew/homebrew-python/commit/93c28cd455a8eb2f63bf376a4a78edbed3e2e349 @arjun810 authored the addition of `-D__ACCELERATE__` in order to build with `gcc` compilers.\r\n\r\nIn a later [commit](https://github.com/Homebrew/homebrew-python/commit/eb68d3210547e67ba24ca42c59d3cc731f31bcba), I unfortunately had to revert because it seem to break the tests with the current version of scipy.\r\n\r\nI was not able to work out a solution. So not sure if it is only my haswell Mac or if anyone can come up with a solution. In a recent commit bea170d941886815ccd4337056c3f815691a28ac I fixed the conditional to allow gcc compilers.\r\n\r\nAny ideas? Does it work for you? Which deps of scipy need to be recompiled also with `--cc=gcc-4.8`.\r\n\r\n'"
numpy/numpy#5223,46572131,sturlamolden,juliantaylor,2014-10-22 23:15:55,2014-10-26 16:18:17,2014-10-26 16:18:17,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/5223,"b""BUG: Ugly fix for Apple's cblas_sgemv segfault""","b""SGEMV in Accelerate framework will segfault on MacOS X version 10.9\r\n(aka Mavericks) if arrays are not aligned to 32 byte boundaries\r\nand the CPU supports AVX instructions. This can produce segfaults\r\nin numpy.dot if we use numpy.float32 as dtype. This patch overshadows\r\nthe symbols cblas_sgemv, sgemv_ and sgemv exported by Accelerate\r\nto produce the correct behavior. The MacOS X version and CPU specs\r\nare checked on module import. If Mavericks and AVX are detected\r\nthe call to SGEMV is emulated with a call to SGEMM if the arrays\r\nare not 32 byte aligned. If the exported symbols cannot be\r\novershadowed on module import, a fatal error is produced and the\r\nprocess aborts. All the fixes are in a self-contained C file\r\nand do not alter the _dotblas C code. The patch is not applied\r\nunless NumPy is configured to link with Apple's Accelerate\r\nframework."""
numpy/numpy#4007,22049781,piskvorky,juliantaylor,2013-11-04 13:05:02,2014-10-29 00:02:19,2014-10-26 16:20:24,closed,,,155,component: numpy.linalg,https://api.github.com/repos/numpy/numpy/issues/4007,b'numpy.dot crash with numpy.float32 input',"b""A user of gensim @fbkarsdorp reported crash (segfault) with NumPy: piskvorky/gensim#131\r\n\r\nThe crash seems to have nothing to do with gensim, so I'm transferring the issue here. It happens in `dot` of matrix*vector in single precision, on his OS X Maverick.\r\n"""
python-pillow/Pillow#990,47860210,ccazabon,wiredfool,2014-11-05 16:58:11,2014-11-19 20:30:21,2014-11-19 20:30:21,closed,,,1,,https://api.github.com/repos/python-pillow/Pillow/issues/990,"b""scipy test failure `TypeError: 'float' object is not iterable`""","b'Current git master branch with Python 2.7, on Ubuntu Trusty 14.04 64-bit, with the dependencies all installed.  Running test-installed.py results in test error that appears to be due to `size` being a scalar rather than a 2-tuple:\r\n\r\n```\r\nERROR: Test_scipy_resize.test_imresize4\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File "".../Pillow/Tests/test_scipy.py"", line 37, in test_imresize4\r\n    im4 = misc.imresize(im, 200, mode=\'F\')  # percentage\r\n  File "".../local/lib/python2.7/site-packages/scipy/misc/pilutil.py"", line 420, in imresize\r\n    imnew = im.resize(size, resample=func[interp])\r\n  File "".../local/lib/python2.7/site-packages/Pillow-2.6.0-py2.7-linux-x86_64.egg/PIL/Image.py"", line 1531, in resize\r\n    size=tuple(size)\r\nTypeError: \'float\' object is not iterable\r\n```\r\n'"
numpy/numpy#5073,42791321,argriffing,seberg,2014-09-15 16:29:47,2014-09-23 16:05:17,2014-09-15 19:27:01,closed,,,4,,https://api.github.com/repos/numpy/numpy/issues/5073,b'np.atleast_2d of sparse matrix returns a 1x1 ndarray containing the sparse matrix object',"b'I assume this falls under ""numpy doesn\'t know about your custom matrix-like objects so they are not our problem"" but I thought I\'d mention this anyway because it was obscuring a bug in another project.  Feel free to close as invalid.\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy.sparse\r\n\r\na = scipy.sparse.identity(2).tocsc()\r\nprint a\r\n\r\nb = np.atleast_2d(a)\r\nprint b\r\n```\r\n\r\n```\r\n  (0, 0)        1.0\r\n  (1, 1)        1.0\r\n[[ <2x2 sparse matrix of type \'<type \'numpy.float64\'>\'\r\n        with 2 stored elements in Compressed Sparse Column format>]]\r\n```'"
numpy/numpy#5097,43491527,argriffing,charris,2014-09-22 14:24:46,2014-09-25 23:51:09,2014-09-25 23:51:09,closed,,,4,,https://api.github.com/repos/numpy/numpy/issues/5097,b'should logaddexp really give a warning for -inf?',"b'```\r\n>>> np.logaddexp(-np.inf, -np.inf)\r\n__main__:1: RuntimeWarning: invalid value encountered in logaddexp\r\n-inf\r\n```\r\n\r\nMaybe it could be argued that it should give a warning because log(0) gives a warning, but on the other hand it could be special-cased.\r\n\r\nhttps://github.com/scipy/scipy/issues/4014'"
numpy/numpydoc#27,40661296,stringhamc,rgommers,2014-08-20 03:56:44,2014-08-20 05:51:27,2014-08-20 05:51:22,closed,,,1,invalid,https://api.github.com/repos/numpy/numpydoc/issues/27,b'Anyone know why docs.scipy.org is down?',"b'I know there is probably a better place to post this, but I keep getting gateway timeout errors when trying to go to docs.scipy.org or wiki.scipy.org. Is anyone else having the same problem?'"
numpy/numpy#4972,40459763,nuwan600,rgommers,2014-08-18 06:42:14,2014-08-18 17:43:50,2014-08-18 17:42:20,closed,,,3,50 - Duplicate,https://api.github.com/repos/numpy/numpy/issues/4972,"b'cann\'t go to  ""numpy.scipy.org"" site or can\'t view any documentation sites'","b""Can't go to or refer any documentation(docs.scipy.org/doc/) in numpy and can't view numpy.scipy.org"""
Homebrew/homebrew-python#178,48944151,stonemirror,tdsmith,2014-11-16 04:29:36,2014-11-16 05:31:08,2014-11-16 05:31:08,closed,,,3,upstream,https://api.github.com/repos/Homebrew/homebrew-python/issues/178,b'scipy 0.14.0 ufunc tests fail with numpy 1.9',"b'I\'m attempting to put together the pieces of Scientific Python as described in [this blog posting](https://joernhees.de/blog/2014/02/25/scientific-python-on-mac-os-x-10-9-with-homebrew/), and I\'m running into the same problem that\'s being described in [this posting on StackOverflow](http://stackoverflow.com/questions/26889467/mavericks-brew-install-scipy-okay-brew-test-scipy-failure/26953434#26953434).\r\n\r\nBriefly, scipy appears to build okay, but running the test suite (via ""brew test scipy"") produces a failure. I noted that `which gcc` pointed to /usr/bin/gcc, so I\'ve tried ensuring that the correct gcc is being used by creating a soft link to the brewed gcc-4.9:\r\n\r\n`sudo ln -s /usr/local/Cellar/gcc/4.9.2/bin/gcc-4.9 /usr/local/bin/gcc`\r\n\r\nas well as insuring that the correct flags were being passed by building scipy using the `--default-fortran-flags` option, but I\'m still getting the same error.'"
scikit-learn/scikit-learn#4787,82251433,bnaul,,2015-05-29 04:17:21,2016-05-18 21:22:44,None,open,,,2,Enhancement,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4787,b'Change graph_lasso to exploit block diagonal structure',"b'I took a stab at implementing the optimization described [here](http://faculty.washington.edu/dwitten/Papers/jcgs.2011.pdf): the block diagonal structure of the graphical lasso solution can be identified by thresholding the sample covariance, and the exact solution is found by solving the graphical lasso for each block separately. The authors find that there is a huge speedup when the solution is very sparse; when the solution is mostly dense, the results are basically the same (or very slightly slower due to the extra thresholding step). This modification was made in the `glasso` R package some time ago; timing results are given in the paper above, but I also ran a test of my implementation with p=1000, n=100 and a block diagonal population covariance matrix.\r\n<img src=""https://cloud.githubusercontent.com/assets/903655/7876255/b1e0e08c-057d-11e5-8d90-5b52f531e04b.png"" width=""400px"" height=""400px"">\r\n\r\nCouple of questions:\r\n1. the `glasso` R package was changed to only use this algorithm, so I followed the same convention and did not allow the user to choose whether to perform the block diagonal screening procedure. It would be very easy to add this, I\'m just not sure if there\'s a case where it would ever be desired.\r\n2. There\'s a bug in our connected_components function that was fixed a while ago in scipy (https://github.com/scipy/scipy/pull/3819). I included this in my commit, but maybe it should be a separate pull request?\r\n3. Does the overall logic make sense here? I only added a couple of comments but if it\'s not clear what\'s going on then I can try to clarify.'"
numpy/numpy#4733,34125905,argriffing,charris,2014-05-22 21:57:35,2014-08-05 20:46:08,2014-05-22 22:40:30,closed,,,0,,https://api.github.com/repos/numpy/numpy/issues/4733,b'svd bug',"b'tracking down the `matrix_rank` bug report in http://mail.scipy.org/pipermail/numpy-discussion/2014-May/070193.html I find\r\n```\r\n>>> import numpy as np\r\n>>> b = np.ones((3,4), dtype=complex)\r\n>>> np.linalg.svd(b) # seems OK\r\n>>> np.linalg.svd(b, compute_uv=False) # raises the error reported in the email\r\n```\r\n'"
statsmodels/statsmodels#1838,38204670,jbrockmendel,,2014-07-18 19:21:28,2014-08-10 09:35:33,None,open,,,7,type-bug,https://api.github.com/repos/statsmodels/statsmodels/issues/1838,b'dtype complex: statsmodels.api.Probit leads to _flapack.error in scipy.linag.decomp_svd.py',"b'Amateur user, first time submitting to an issue tracker.  Please let me know if more/different information is needed to be useful.\r\n\r\nX and Y are numpy matrices.  The columns of X are constructed so as to be uncorrelated.  The following error persists if I drop one or more columns of X.  The same error also occurs if I use statsmodels.api.Logit instead of Probit.\r\n\r\n```\r\n>>>X.shape\r\n(92007, 7)\r\n>>>Y.shape\r\n(1,92007)\r\n\r\n>>> model = statsmodels.api.Probit(Y, X)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Library/Python/2.7/site-packages/statsmodels/discrete/discrete_model.py"", line 117, in __init__\r\n    super(DiscreteModel, self).__init__(endog, exog, **kwargs)\r\n  File ""/Library/Python/2.7/site-packages/statsmodels/base/model.py"", line 137, in __init__\r\n    self.initialize()\r\n  File ""/Library/Python/2.7/site-packages/statsmodels/discrete/discrete_model.py"", line 126, in initialize\r\n    self.df_model = float(tools.rank(self.exog) - 1)  # assumes constant\r\n  File ""/Library/Python/2.7/site-packages/statsmodels/tools/tools.py"", line 381, in rank\r\n    D = svdvals(X)\r\n  File ""/Library/Python/2.7/site-packages/scipy/linalg/decomp_svd.py"", line 146, in svdvals\r\n    check_finite=check_finite)\r\n  File ""/Library/Python/2.7/site-packages/scipy/linalg/decomp_svd.py"", line 100, in svd\r\n    full_matrices=full_matrices, overwrite_a=overwrite_a)\r\n_flapack.error: (lwork>=(compute_uv?2*minmn*minmn+MAX(m,n)+2*minmn:2*minmn+MAX(m,n))) failed for 3rd keyword lwork: zgesdd:lwork=1547\r\n\r\n\r\n>>> statsmodels.__version__\r\n\'0.5.0\'\r\n>>> scipy.__version__\r\n\'0.14.0\'\r\n>>> numpy.__version__\r\n\'1.8.1\'\r\n>>> platform.platform()\r\n\'Darwin-13.3.0-x86_64-i386-64bit\'\r\n```\r\n'"
statsmodels/statsmodels#1803,37187153,charris,josef-pkt,2014-07-04 21:49:22,2014-07-18 05:36:46,2014-07-18 05:36:46,closed,,,8,,https://api.github.com/repos/statsmodels/statsmodels/issues/1803,b'Error revealed by numpy 1.9.0r1',"b'```\r\n\r\n======================================================================\r\nERROR: statsmodels.emplike.tests.test_aft.Test_AFTModel.test_beta_vect\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\tests\\test_aft.py"", line 34, in test_beta_vect\r\n    assert_almost_equal(self.res1.test_beta([3.5, -.035], [0, 1]),\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\aft_el.py"", line 481, in test_beta\r\n    llr, pval, new_weights = reg_model.el_test(b0_vals, param_nums, return_weights=True)  # Needs to be changed\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\regression\\linear_model.py"", line 1519, in el_test\r\n    stochastic_exog=stochastic_exog)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\elregress.py"", line 58, in _opt_nuis_regress\r\n    params[nuis_param_index] = nuisance_params\r\nValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (0,)\r\n\r\n======================================================================\r\nERROR: statsmodels.emplike.tests.test_origin.TestOrigin.test_ci_beta\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\tests\\test_origin.py"", line 35, in test_ci_beta\r\n    ci = self.res1.conf_int_el(1)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\originregress.py"", line 256, in conf_int_el\r\n    lowerl = optimize.brentq(f, lower_bound, self.params[param_num])\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\scipy\\optimize\\zeros.py"", line 415, in brentq\r\n    r = _zeros._brentq(f,a,b,xtol,rtol,maxiter,args,full_output,disp)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\originregress.py"", line 255, in <lambda>\r\n    stochastic_exog=stochastic_exog)[0] - r0\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\originregress.py"", line 202, in el_test\r\n    return_weights=return_weights)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\regression\\linear_model.py"", line 1519, in el_test\r\n    stochastic_exog=stochastic_exog)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\elregress.py"", line 58, in _opt_nuis_regress\r\n    params[nuis_param_index] = nuisance_params\r\nValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (0,)\r\n\r\n======================================================================\r\nERROR: statsmodels.emplike.tests.test_origin.TestOrigin.test_hypothesis_beta1\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\tests\\test_origin.py"", line 31, in test_hypothesis_beta1\r\n    assert_almost_equal(self.res1.el_test([.0034],[1])[0],\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\originregress.py"", line 202, in el_test\r\n    return_weights=return_weights)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\regression\\linear_model.py"", line 1519, in el_test\r\n    stochastic_exog=stochastic_exog)\r\n  File ""X:\\Python27-x64\\lib\\site-packages\\statsmodels\\emplike\\elregress.py"", line 58, in _opt_nuis_regress\r\n    params[nuis_param_index] = nuisance_params\r\nValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (0,)\r\n```\r\nNot sure what is going on here, whether this is a mistake on the stats-models end or a deliberate use of numpy\'s old indexing behavior.\r\n'"
erikd/libsndfile#70,37376798,mgeier,erikd,2014-07-08 14:44:54,2014-07-12 14:58:56,2014-07-11 23:43:53,closed,,,6,,https://api.github.com/repos/erikd/libsndfile/issues/70,b'Data gets lost when opening and closing certain files in SFM_RDWR mode',"b'While writing unit tests for [PySoundFile](https://github.com/bastibe/PySoundFile), I witnessed very strange behavior:\r\n\r\nAfter opening and closing (and nothing in between!) a certain WAV file in SFM_RDWR mode, the file was changed and a few samples were missing in the front.\r\n\r\nThe WAV file in question has 32-bit float format and was generated in Python using `scipy.io`.\r\n\r\nThis is the code that changes the file:\r\n\r\n```c\r\n#include <sndfile.h>\r\nint main() {\r\nSF_INFO sfinfo;\r\nSNDFILE* sf;\r\nsf = sf_open(""stereo.wav"", SFM_RDWR, &sfinfo);\r\nsf_close(sf);\r\n}\r\n```\r\n\r\nThe original WAV file has 4 frames, after running this program it has only 2 frames!\r\n\r\nThe first 3 samples (= one and a half frames) is missing, so maybe it has something to do with a wrong header size or something?\r\n\r\nThe WAV file itself and the code to generate it is available at https://gist.github.com/mgeier/7224433.\r\n\r\nThis seems to happen only with 32-bit and 64-bit float files, but I\'m not sure.\r\nReading the file in SFM_READ mode works correctly.\r\n\r\nIs the WAV file malformed?\r\nIf yes, shouldn\'t libsndfile return some kind of error?\r\nIf no, what\'s going on?'"
bastibe/PySoundFile#55,37541772,mgeier,bastibe,2014-07-10 08:32:03,2014-07-15 11:56:33,2014-07-15 11:49:42,closed,,,10,,https://api.github.com/repos/bastibe/PySoundFile/issues/55,b'Generate proper test WAV files',"b""Originally, I wanted to avoid creating the test files with the system under test, this just didn't seem right.\r\nTherefore, I created the test WAV files with `scipy.io.wavfile.write()`.\r\n\r\nRecently, I found out that the 32-bit floating point WAV file is malformed.\r\nSee https://github.com/erikd/libsndfile/issues/70 and https://github.com/scipy/scipy/issues/3778.\r\n\r\nI tried to find an alternative way to generate a floating point WAV file, but it's surprisingly hard to find something that doesn't involve libsndfile.\r\n\r\nSox produces correct files, but I didn't find a way to write specific floating-point values (like 1.0 and 0.75) to a file.\r\n\r\nIs there a better way to create our test WAV files?\r\n\r\nIf not, I guess we'll have to create them with PySoundFile.\r\n\r\nAnother possibility would be to use only PCM test files (which can be generated correctly with many tools), but then we would have to add `dtype='float16'` in many places in the test code, which I originally wanted to avoid."""
numpy/numpy#4681,32921483,seberg,charris,2014-05-06 18:51:21,2015-10-13 19:50:20,2015-10-13 19:50:20,closed,,,35,06 - Task,https://api.github.com/repos/numpy/numpy/issues/4681,b'Change RELAXED_STRIDES to not intentionally mangle them up',"b""Currently when RELAXED_STRIDES is active, new arrays which have a dimension with size 1 get a stride of `NPY_MAX_INTP` set to help debugging (since this will almost certainly trigger such bugs for any input array with such a shape, and not juts when one happens to have an input array with funny strides).\r\n\r\nIn release versions we should probably *not* do this. So for 1.10 I would suggest creating a RELAXED_STRIDES=2 setting where we actually still scramble up the stride, but as default with RELAXED_STRIDES=1 don't in the release. This requires `ctors.h` around line 3718 (`/* For testing purpose only */`) to be changed."""
mdtraj/mdtraj#821,74258465,hainm,rmcgibbo,2015-05-08 06:54:27,2015-05-08 07:59:41,2015-05-08 07:59:13,closed,,,2,,https://api.github.com/repos/mdtraj/mdtraj/issues/821,b'netcdf should be close properly',"b'Hi,\r\n\r\nI am playing with `load_netcdf` but got this kind of error\r\n\r\n```\r\nIn [54]: m_traj = md.load_netcdf (""md11.nc"", top=""myparm.prmtop"")\r\n/home/haichit/anaconda3/lib/python3.4/site-packages/scipy/io/netcdf.py:287: \r\nRuntimeWarning: Cannot close a netcdf_file opened with mmap=True, \r\nwhen netcdf_variables or arrays referring to its data still exist. \r\nAll data arrays obtained from such files refer directly to data on disk, and \r\nmust be copied before the file can be clean\r\n```\r\n\r\nthis did not happen when I call `m_traj = md.load_netcdf (""md11.nc"", top=""myparm.prmtop"")` in the first time. But calling 2nd time in ipython notebook cause this error.\r\n\r\nI got the same problem when playing with `netcdf` in `scipy`? Just want to report to you guys (this happens randomly but not very frequently).\r\n\r\nHai'"
numpy/numpy#4703,33427437,gauteh,pv,2014-05-13 18:50:10,2014-05-14 08:51:00,2014-05-14 08:51:00,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/4703,b'segfault in numpy when doing pcolormesh (through basemap / matplotlib)',"b'I am running Arch Linux on a Macbook Pro with numpy 1.8.1 and python 3.4, I am running this script: http://scipy-central.org/item/75/1/ibcao-international-bathymetric-chart-of-the-arctic-ocean-class-for-python-and-scipy which plots a large map using Basemap. I recompiled python and numpy with debugging symbols and I get the below attached backtrace in gdb.\r\n\r\nThe offending line in my script is: https://github.com/gauteh/ibcao_py/blob/master/ibcao.py#L151\r\n\r\n```\r\n#0  0x00007ffff74b57a0 in __memmove_ssse3_back () from /usr/lib/libc.so.6\r\n#1  0x00007ffff4bbdca8 in memmove (__len=4, __src=0x7fffc3bf4888, \r\n    __dest=<optimized out>) at /usr/include/bits/string3.h:57\r\n#2  _strided_to_contig_size4 (dst=<optimized out>, dst_stride=<optimized out>, \r\n    src=0x7fffc3bf4888 <error: Cannot access memory at address 0x7fffc3bf4888>, src_stride=40, N=1161, __NPY_UNUSED_TAGGEDsrc_itemsize=<optimized out>, \r\n    __NPY_UNUSED_TAGGEDdata=0x0)\r\n    at numpy/core/src/multiarray/lowlevel_strided_loops.c.src:144\r\n#3  0x00007ffff4b76219 in PyArray_CopyAsFlat (dst=0x2a30220, src=0x489, \r\n    order=-1010874232) at numpy/core/src/multiarray/ctors.c:2566\r\n#4  0x00007ffff4bf98c1 in PyArray_Flatten (a=0x154ed40, order=NPY_CORDER)\r\n    at numpy/core/src/multiarray/shape.c:1039\r\n#5  0x00007ffff4bf99c1 in PyArray_Ravel (arr=0x154ed40, order=NPY_CORDER)\r\n    at numpy/core/src/multiarray/shape.c:1011\r\n#6  0x00007ffff4bd7209 in array_ravel (self=0x154ed40, args=<optimized out>, \r\n    kwds=<optimized out>) at numpy/core/src/multiarray/methods.c:2182\r\n#7  0x00007ffff7a5e8b5 in call_function (oparg=<optimized out>, \r\n    pp_stack=0x7fffffffdd10) at Python/ceval.c:4227\r\n#8  PyEval_EvalFrameEx (f=0x1966c98, throwflag=<optimized out>)\r\n    at Python/ceval.c:2829\r\n#9  0x00007ffff7a5ff49 in PyEval_EvalCodeEx (_co=0x2a30224, globals=0x1, \r\n    locals=0x1966e10, args=0x7fffe40f3470, argcount=1161, kws=0x2a30220, \r\n    kws@entry=0x7ffff7f89060, kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, \r\n    closure=0x0) at Python/ceval.c:3578\r\n#10 0x00007ffff79d1cd6 in function_call (func=0x7fffec86e9d8, \r\n    arg=0x7fffe40f3458, kw=0x7fffe3eb1308) at Objects/funcobject.c:632\r\n#11 0x00007ffff79a9d48 in PyObject_Call (func=func@entry=0x7fffec86e9d8, \r\n    arg=arg@entry=0x7fffe40f3458, kw=kw@entry=0x7fffe3eb1308)\r\n    at Objects/abstract.c:2067\r\n#12 0x00007ffff7a583e1 in ext_do_call (nk=<optimized out>, na=1, \r\n    flags=<optimized out>, pp_stack=0x7fffffffdfc8, func=0x7fffec86e9d8)\r\n    at Python/ceval.c:4551\r\n#13 PyEval_EvalFrameEx (f=0x13bcbe8, throwflag=<optimized out>)\r\n    at Python/ceval.c:2869\r\n#14 0x00007ffff7a5ff49 in PyEval_EvalCodeEx (_co=0x2a30224, \r\n    _co@entry=0x7fffec830810, globals=0x0, locals=0x13bcd60, \r\n    args=0x7ffff7f2d5b8, argcount=1161, argcount@entry=3, kws=0x2a30220, \r\n    kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0)\r\n    at Python/ceval.c:3578\r\n#15 0x00007ffff7a5d4e5 in fast_function (nk=0, na=3, n=<optimized out>, \r\n    pp_stack=0x7fffffffe1c0, func=<optimized out>) at Python/ceval.c:4334\r\n#16 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe1c0)\r\n    at Python/ceval.c:4252\r\n#17 PyEval_EvalFrameEx (f=0x7ffff7f2d438, throwflag=<optimized out>)\r\n    at Python/ceval.c:2829\r\n#18 0x00007ffff7a5ff49 in PyEval_EvalCodeEx (_co=0x2a30224, \r\n    _co@entry=0x7ffff7ef95d0, globals=0x0, locals=0x7ffff7f2d5b0, args=0x0, \r\n    argcount=1161, argcount@entry=0, kws=0x2a30220, kws@entry=0x0, kwcount=0, \r\n    defs=0x0, defcount=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:3578\r\n#19 0x00007ffff7a5ffeb in PyEval_EvalCode (co=co@entry=0x7ffff7ef95d0, \r\n    globals=globals@entry=0x7ffff7f2a488, locals=locals@entry=0x7ffff7f2a488)\r\n    at Python/ceval.c:773\r\n#20 0x00007ffff7a7c174 in run_mod (mod=mod@entry=0x6837c0, \r\n    filename=filename@entry=0x7ffff7e1a2f0, \r\n    globals=globals@entry=0x7ffff7f2a488, locals=locals@entry=0x7ffff7f2a488, \r\n    flags=flags@entry=0x7fffffffe440, arena=arena@entry=0x631980)\r\n    at Python/pythonrun.c:2175\r\n#21 0x00007ffff7a7e3a5 in PyRun_FileExFlags (fp=0x690ea0, \r\n    filename_str=<optimized out>, start=<optimized out>, \r\n    globals=0x7ffff7f2a488, locals=0x7ffff7f2a488, closeit=1, \r\n    flags=0x7fffffffe440) at Python/pythonrun.c:2128\r\n#22 0x00007ffff7a7f363 in PyRun_SimpleFileExFlags (fp=0x2a30224, \r\n    filename=0x7ffff7f22440 ""ibcao.py"", closeit=1, flags=0x28)\r\n    at Python/pythonrun.c:1601\r\n#23 0x00007ffff7a9543c in run_file (p_cf=<optimized out>, \r\n    filename=<optimized out>, fp=<optimized out>) at Modules/main.c:319\r\n#24 Py_Main (argc=-136207696, argv=0x7ffff7f22420) at Modules/main.c:751\r\n#25 0x0000000000400af6 in main (argc=2, argv=<optimized out>)\r\n    at ./Modules/python.c:69\r\n'"
,,,,,,,,,,,,,,
Try2Code/cdo-bindings#4,45360314,martinclaus,Try2Code,2014-10-09 12:29:49,2014-12-03 09:34:25,2014-12-03 09:34:25,closed,,,5,,https://api.github.com/repos/Try2Code/cdo-bindings/issues/4,"b'returnArray cause segmentation fault when using cdfMod ""scipy""'","b'Running the test suite aborts with a segmentation fault in `test_returnArray` on line 252.\r\nAlso doing something like\r\n```\r\ntemperature = cdo.stdatm(0,options = \'-f nc\', returnCdf = True).variables[\'T\'][:]\r\nprint temperature\r\n```\r\nwill end in an segmentation fault, when temperature is accessed by print. Everything works as expected, when `cdfMod` is""netcdf4"". Maybe the netcdf4 should be used as default.\r\n```\r\n$ cdo -V\r\nClimate Data Operators version 1.6.3 (http://code.zmaw.de/projects/cdo)\r\nCompiled: by mclaus on ares.geomar.de (x86_64-unknown-linux-gnu) Feb 25 2014 10:44:28\r\nCompiler: gcc -std=gnu99 -g -O2 -pthread\r\n version: gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\r\nFeatures: PTHREADS NC4 OPeNDAP Z JASPER UDUNITS2 PROJ.4 XML2 MAGICS CURL\r\nLibraries: proj/4.8 xml2/2.7.8 curl/7.35.0(h7.32.0)\r\nFiletypes: srv ext ieg grb grb2 nc nc2 nc4 nc4c \r\n     CDI library version : 1.6.3 of Feb 25 2014 10:44:20\r\n CGRIBEX library version : 1.6.3 of Jan  8 2014 19:55:18\r\nGRIB_API library version : 1.11.0\r\n  netCDF library version : 4.3.0 of Sep 20 2013 12:21:08 $\r\n    HDF5 library version : 1.8.10\r\n SERVICE library version : 1.3.1 of Feb 25 2014 10:44:06\r\n   EXTRA library version : 1.3.1 of Feb 25 2014 10:44:01\r\n     IEG library version : 1.3.1 of Feb 25 2014 10:44:04\r\n    FILE library version : 1.8.2 of Feb 25 2014 10:44:01\r\n\r\n```\r\n```\r\npython -c ""import scipy; print scipy.__version__""\r\n0.14.0\r\n```\r\n```\r\npython -c ""import netCDF4; print netCDF4.__version__""\r\n1.0.8\r\n```'"
pydata/xarray#368,60766810,shoyer,,2015-03-12 03:44:41,2015-06-01 10:53:08,None,open,,,3,backends;design question,https://api.github.com/repos/pydata/xarray/issues/368,b'Default reading netCDF3 files with scipy.io instead of netCDF4?',"b""In my microbenchmarks, scipy.io appears to be ~3x faster than netCDF4 for reading netCDF3 files:\r\n```python\r\nds = xray.Dataset({'foo': (['x', 'y'], np.random.randn(10000, 10000).astype(np.float32))})\r\nds.to_netcdf('test.nc', engine='scipy')\r\nds_scipy = xray.open_dataset('test.nc', engine='scipy')\r\nds_nc4 = xray.open_dataset('test.nc', engine='netcdf4')\r\n\r\n%timeit ds_scipy.isel(x=slice(5000)).load_data()\r\n# 10 loops, best of 3: 123 ms per loop\r\n\r\n%timeit ds_nc4.isel(x=slice(5000)).load_data()\r\n# 1 loops, best of 3: 319 ms per loop\r\n```\r\n\r\nWe might want to switch the default engine to use scipy for reading netCDF3 files. Note that netCDF4 does seem to be a bit faster for *writing*."""
igraph/igraph#551,23819817,gaborcsardi,,2013-12-05 21:39:02,2015-03-24 14:34:12,None,open,,,6,,https://api.github.com/repos/igraph/igraph/issues/551,b'Replace ARPACK',"b'Possible solutions to look at: \r\n- arpack-ng https://github.com/pv/arpack-ng\r\n- feast http://www.ecs.umass.edu/~polizzi/feast/features.htm\r\n- eigen http://eigen.tuxfamily.org/index.php?title=Main_Page \r\n- more ??? check http://www.grycap.upv.es/slepc/documentation/reports/str6.pdf\r\n\r\nCons:\r\n- arpack-ng is the same codebase, just repackaged?\r\n- feast does not have a non-symmetric solver\r\n- eigen is C++ templates \r\n- eigen is MPL 2.0, which is somewhat problematic. (If we do not change anything in the source, then eigen files will be dual licensed, if we change something in it, the same happens, but care must be taken not to put any (L)GPL code in these files.) Actually, eigen might not even have sparse eigensolvers....\r\n'"
igraph/igraph#544,22764033,kalenedrael,,2013-11-15 22:47:03,2014-05-05 18:11:56,None,open,,,2,Confirmed,https://api.github.com/repos/igraph/igraph/issues/544,b'ARPACK returns bad eigenvalues in weighted directed eigenvector centrality',"b""When calculating eigenvector centrality for a particular directed graph with weighted edges, igraph_eigenvector_centrality() unexpectedly returns a negative eigenvalue. It appears that ARPACK is not returning the largest eigenvalue as requested - the eigenvalue returned isn't even the one with the greatest magnitude. All edge weights are positive and there are no loop edges. Unfortunately I can't share the offending graph because it contains confidential data. I haven't been able to reproduce the issue with other graphs.\r\n\r\nI did find that ARPACK returns different a different result if asked to calculate more than one. If a bipartite graph with 4 vertices is constructed with the following adjacency matrix, for instance:\r\n```\r\n0 0 0.803829 0.644412\r\n0 0 0.385865 1.075488\r\n0.380149 0.100474 0 0\r\n0.164536 0.220675 0 0\r\n```\r\nthen ARPACK correctly returns 0.787969 as the eigenvalue, but when two eigenvalues are computed (setting options->nev = 2 and initializing the result vector to 2 columns in igraph_eigenvector_centrality_directed()), it results in the two negative eigenvalues -0.258478 and -0.787969, of which the first is returned from igraph_eigenvector_centrality(). Conversely, for the original problematic graph, computing two eigenvalues causes the correct eigenvalue to be returned, but the default results in a negative eigenvalue.\r\n\r\nThe code for this example and the fuzzer used to find it are in this gist: https://gist.github.com/kalenedrael/7492873\r\nI should clarify that running the fuzzer with an unmodified igraph hasn't yielded any negative eigenvalues - only when igraph is modified to compute two eigenvalues does it return negative."""
numpy/numpy#4664,32757571,charris,charris,2014-05-03 23:43:47,2014-05-04 13:23:53,2014-05-04 12:59:53,closed,,,13,11 - Bug;component: build;priority: high,https://api.github.com/repos/numpy/numpy/issues/4664,b'Travis tests are busted.',"b'I suspect there has been a change in the configuration of some of the build machines, but maybe something else is broken. The end of the traceback is:\r\n```\r\nFile ""/tmp/pip-0jmng0-build/numpy/distutils/command/install.py"", line 33, in setuptools_run\r\n\r\nreturn old_install_mod._install.run(self)\r\n\r\nAttributeError: \'module\' object has no attribute \'_install\'\r\n```\r\nWhere `old_install_mod` is python\'s `distutils.command.install`.'"
pypa/setuptools#199,144277650,bb-migration,bb-migration,2014-05-04 03:35:15,2016-03-29 14:15:50,2016-03-29 14:15:49,closed,,,5,bug;critical,https://api.github.com/repos/pypa/setuptools/issues/199,b'setuptools 3.5 breaks numpy/scipy install with Python 2.7 on Mac OS X',"b'Originally reported by: **Anonymous**\n\n----------------------------------------\n\nAfter update to setup tools 3.5 numpy and scipy fail to install on Mac OS X 10.9.2 with system Python 2.7. This is a regression from 3.4.4. (Rolling back to this version successfully installs numpy and scipy). \n\nHere is the output with setuptools 3.5 when installing scipy:\n\n```\n Running from scipy source directory.\n    Splitting linalg.interpolative Fortran source files\n    non-existing path in \'scipy/sparse\': \'sparsetools/sparsetools_impl.h\'\n    non-existing path in \'scipy/sparse\': \'sparsetools/bsr_impl.h\'\n    non-existing path in \'scipy/sparse\': \'sparsetools/csc_impl.h\'\n    non-existing path in \'scipy/sparse\': \'sparsetools/csr_impl.h\'\n    non-existing path in \'scipy/sparse\': \'sparsetools/other_impl.h\'\n    Traceback (most recent call last):\n      File ""<string>"", line 1, in <module>\n      File ""/private/tmp/pip_build_root/scipy/setup.py"", line 237, in <module>\n        setup_package()\n      File ""/private/tmp/pip_build_root/scipy/setup.py"", line 234, in setup_package\n        setup(**metadata)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/core.py"", line 135, in setup\n        config = configuration()\n      File ""/private/tmp/pip_build_root/scipy/setup.py"", line 173, in configuration\n        config.add_subpackage(\'scipy\')\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 966, in add_subpackage\n        caller_level = 2)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 935, in get_subpackage\n        caller_level = caller_level + 1)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 872, in _get_configuration_from_setup_py\n        config = setup_module.configuration(*args)\n      File ""scipy/setup.py"", line 23, in configuration\n        config.add_subpackage(\'special\')\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 966, in add_subpackage\n        caller_level = 2)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 935, in get_subpackage\n        caller_level = caller_level + 1)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 872, in _get_configuration_from_setup_py\n        config = setup_module.configuration(*args)\n      File ""scipy/special/setup.py"", line 83, in configuration\n        extra_info=get_info(""npymath""))\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 2138, in get_info\n        pkg_info = get_pkg_info(pkgname, dirs)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/misc_util.py"", line 2090, in get_pkg_info\n        return read_config(pkgname, dirs)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/npy_pkg_config.py"", line 393, in read_config\n        v = _read_config_imp(pkg_to_filename(pkgname), dirs)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/npy_pkg_config.py"", line 329, in _read_config_imp\n        meta, vars, sections, reqs = _read_config(filenames)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/npy_pkg_config.py"", line 312, in _read_config\n        meta, vars, sections, reqs = parse_config(f, dirs)\n      File ""/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/distutils/npy_pkg_config.py"", line 284, in parse_config\n        raise PkgNotFound(""Could not find file(s) %s"" % str(filenames))\n    numpy.distutils.npy_pkg_config.PkgNotFound: Could not find file(s) [\'/Library/Python/2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-intel.egg/numpy/core/lib/npy-pkg-config/npymath.ini\']\n    Complete output from command /usr/bin/python -c ""import setuptools, tokenize;__file__=\'/private/tmp/pip_build_root/scipy/setup.py\';exec(compile(getattr(tokenize, \'open\', open)(__file__).read().replace(\'\\r\\n\', \'\\n\'), __file__, \'exec\'))"" install --record /tmp/pip-TyVdHj-record/install-record.txt --single-version-externally-managed --compile:\n    blas_opt_info:\n```\n\n----------------------------------------\n- Bitbucket: https://bitbucket.org/pypa/setuptools/issue/199\n'"
numpy/numpy#4656,32708975,foogod,charris,2014-05-02 17:19:13,2014-06-23 22:15:11,2014-05-06 23:13:34,closed,,,22,11 - Bug;component: numpy.fft,https://api.github.com/repos/numpy/numpy/issues/4656,b'Fix for #4655: Make fftpack._raw_fft threadsafe',b'This patch replaces the naive dictionary-cache implementation in _raw_fft with an equivalent cache that will perform correctly in a multithreaded context.'
numpy/numpy#4655,32707928,foogod,juliantaylor,2014-05-02 17:03:54,2014-05-09 10:40:05,2014-05-09 10:40:05,closed,,,1,11 - Bug;component: numpy.fft,https://api.github.com/repos/numpy/numpy/issues/4655,b'numpy.fft.fft / rfft / etc routines are not threadsafe',"b'It appears that numpy.fft.fftpack._raw_fft is doing internal caching in a non-threadsafe manner, causing data corruption if FFTs are done on similarly-sized arrays from multiple threads at the same time.\r\n\r\nThe following sample code can be used to reproduce the issue:\r\n\r\n```\r\nimport numpy\r\nimport threading\r\n\r\nmat = numpy.random.random((1000,1000))\r\nresult = numpy.fft.fft(mat)\r\n\r\ndef do_test():\r\n    global mat, result\r\n    r = numpy.fft.fft(mat)\r\n    if not (r == result).all():\r\n        print ""FAILED: %s produced wrong result."" % (threading.current_thread().name,)\r\n\r\nfor i in xrange(16):\r\n    threading.Thread(target=do_test).start()\r\n```\r\n'"
cvxgrp/cvxpy#136,42790332,argriffing,SteveDiamond,2014-09-15 16:20:53,2014-09-22 21:49:28,2014-09-22 21:41:10,closed,,,3,,https://api.github.com/repos/cvxgrp/cvxpy/issues/136,b'BUG: conversion from scipy sparse matrix to numpy ndarray',"b'Although cvxpy has a conversion interface from scipy.sparse to np.matrix, it did not have a conversion from scipy.sparse to np.ndarray.\r\n\r\nI think something is also funny in the interface tests.  Maybe some of the ndarray interface implementation test code actually tests the np.matrix interface instead.  This PR does not address those possible problems in the test code.'"
ContinuumIO/anaconda-issues#425,103777954,bryanwweber,,2015-08-28 19:08:27,2016-07-03 23:49:06,None,open,ilanschnell,,13,,https://api.github.com/repos/ContinuumIO/anaconda-issues/issues/425,b'Lambert W function in Scipy 0.16.0 on Linux returns NaN',"b""The `lambertw` function in Scipy 0.16.0 for Python 3.4.3 returns NaN, no matter the input value. This only happens for Linux (I tested on Ubuntu 14.04), no problems on Windows. Furthermore, building `numpy` and `scipy` myself resolves the issue. The `lambertw` tests in scipy also fail. See also: http://stackoverflow.com/questions/30792319/anaconda3-scipy-special-lambertw-function-return-nan\r\n\r\n    $ conda info\r\n    Current conda install:\r\n    \r\n                 platform : linux-64\r\n            conda version : 3.16.0\r\n      conda-build version : 1.17.0\r\n           python version : 3.4.3.final.0\r\n    \r\n    $ ipython\r\n    \r\n    In [1]: import scipy\r\n    \r\n    In [2]: scipy.__version__\r\n    Out[2]: '0.16.0'\r\n    \r\n    In [3]: import numpy\r\n    \r\n    In [4]: numpy.__version__\r\n    Out[4]: '1.9.2'\r\n    \r\n    In [5]: from scipy.special import lambertw\r\n    \r\n    In [6]: lambertw(1.0)\r\n    Out[6]: (nan+0j)\r\n    """
numpy/numpy#4627,31852319,rgommers,charris,2014-04-19 18:58:05,2014-06-25 00:14:51,2014-06-25 00:14:51,closed,,,2,11 - Bug;12 - Regression;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/4627,b'``ma.apply_along_axis`` regression',"b'After merging gh-4463, a ``scipy.stats.mstats.trim`` is failing: https://github.com/scipy/scipy/issues/3554\r\n\r\nIt looks to me like the expected result for this test is correct, but the mask has one row too many of the mask set to true:\r\n\r\n    import numpy as np\r\n    from scipy import stats\r\n\r\n    x = np.ma.arange(110).reshape(11, 10)\r\n    x[1] = np.ma.masked\r\n\r\n    trimx = stats.mstats.trim(x, (0.1,0.2), relative=True, axis=0)\r\n    np.testing.assert_equal(trimx._mask.ravel(), [1]*20 + [0]*70 + [1]*20)\r\n\r\n    trimx = stats.mstats.trim(x.T, (0.1,0.2), relative=True, axis=-1)\r\n    np.testing.assert_equal(trimx.T._mask.ravel(), [1]*20 + [0]*70 + [1]*20)'"
numpy/numpy#4463,29034260,abalkin,charris,2014-03-09 01:06:14,2014-06-23 20:27:57,2014-04-03 22:24:58,closed,,,12,,https://api.github.com/repos/numpy/numpy/issues/4463,b'BUG: Masked arrays and apply_over_axes',b'Masked arrays version of apply_over_axes did not apply\r\nfunction correctly to arrays with non-trivial masks.\r\n\r\nFixes #4461.'
,,,,,,,,,,,,,,
statsmodels/statsmodels#1548,30819862,jseabold,josef-pkt,2014-04-03 22:31:34,2014-04-05 09:54:02,2014-04-05 09:54:02,closed,,,2,,https://api.github.com/repos/statsmodels/statsmodels/issues/1548,b'testtransf error',"b'    [2]: scipy.__version__\r\n    [2]: \'0.15.0.dev-54796c7\'\r\n\r\nReport\r\n\r\n    ======================================================================\r\n    ERROR: Failure: AttributeError (\'module\' object has no attribute \'rv_frozen\')\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n    File ""/home/skipper/.local/lib/python2.7/site-packages/nose/loader.py"", line 413, in loadTestsFromName\r\n        addr.filename, addr.module)\r\n    File ""/home/skipper/.local/lib/python2.7/site-packages/nose/importer.py"", line 47, in importFromPath\r\n        return self.importFromDir(dir_path, fqname)\r\n    File ""/home/skipper/.local/lib/python2.7/site-packages/nose/importer.py"", line 94, in importFromDir\r\n        mod = load_module(part_fqname, fh, filename, desc)\r\n    File ""/home/skipper/statsmodels/statsmodels-cython/statsmodels/sandbox/distributions/tests/testtransf.py"", line 30, in <module>\r\n        stats.distributions.rv_frozen.name = property(lambda self: self.dist.name)\r\n    AttributeError: \'module\' object has no attribute \'rv_frozen\''"
statsmodels/statsmodels#1698,34390154,ChadFulton,josef-pkt,2014-05-27 16:40:33,2016-01-16 08:47:34,2015-02-12 18:55:16,closed,,0.7,213,comp-tsa;type-enh,https://api.github.com/repos/statsmodels/statsmodels/issues/1698,b'Multivariate Kalman Filter',"b'Here\'s a simple branch with the code added into statsmodels.tsa.statespace. A couple of thoughts\r\n\r\n- At least in the dev process, I thought it might be nicer to keep it in its own module, rather than putting it with the kalmanf. I don\'t know what makes the most sense in the long run.\r\n- I have unit tests that rely on the statespace model, but I\'m rewriting them so the KF pull request can be done on its own without other dependencies, especially since the statespace model is likely to change.\r\n\r\n**Edit**: Original line comments.\r\n\r\n**L72**\r\n> Question: do we want to keep the single-precision version (and the complex single precision, below)? I don\'t really see a use case, and it appears from preliminary tests that the results tend to overflow. Maybe I\'ll post a unit test to demonstrate and we can go from there.\r\n\r\n**L332**\r\n> Question: there are a bunch of ways to initialize the KF, depending on the theory underlying the data. This one is only valid for stationary processes. Probably best to move out to the Python wrapper?\r\n\r\n**L444**\r\nQuestion: I think we\'ll want to add the ability to specify whether or not to check for convergence and alter the tolerance.\r\n\r\n**L414**\r\n> This inversion is using an LU decomposition, but I think in this case I can rely on f to be positive definite since it\'s the covariance matrix of the forecast error, in which case I could use the much faster Cholesky decomposition approach. This is something I\'m looking into, but if you happen to know one way or the other, that would be great too.\r\n> \r\n> Related to this is the idea that you should ""never take inverses"", and I guess I need to look into replacing this with a linear solver routine if possible, in the updating stage below.'"
numpy/numpy#4515,29741098,ogrisel,juliantaylor,2014-03-19 15:18:17,2014-08-08 07:21:39,2014-03-19 17:57:17,closed,,,8,,https://api.github.com/repos/numpy/numpy/issues/4515,b'FIX: missing asarray import in numpy.libs.utils',"b""There was a missing import for the `asarray` function. The doctests should have caught this but:\r\n\r\n- they do not seem to be run by travis\r\n- there were missing imports for the np symbols in the doctests (fixed)\r\n- there are blanklines / ellipsis issues to have them pass. I don't know what is the preferred way to address this."""
scikit-learn/scikit-learn#2969,29433687,ogrisel,,2014-03-14 13:37:52,2015-01-23 03:43:00,None,open,,,3,Enhancement,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2969,b'Update cython code to support 64 bit indexed sparse inputs',"b'In scipy master (to be released as 0.14), scipy [sparse matrices can now be indexed with 64 bit integers](https://github.com/scipy/scipy/blob/master/doc/release/0.14.0-notes.rst#scipysparse-improvements).\r\n\r\nThis means that we will probably need to use fused types for `indptr` and `indices` arrays whenever we deal with CSC or CSR datastructures in our Cython code base.'"
scikit-learn/scikit-learn#3235,34799605,jseabold,larsmans,2014-06-02 18:42:13,2014-06-03 10:51:01,2014-06-03 10:51:01,closed,,,3,,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3235,b'Buffer dtype mismatch in sparsefuncs_fast with recent scipy ',"b""I'm not sure yet whether this is a local problem. On\r\n\r\n    |4 $ git log -n 1\r\n    commit 083b5f569d54753e943de9c7e1207e24db238ec2\r\n\r\nCode to reproduce\r\n\r\n    [~/]\r\n    [1]: paste\r\n    >>> corpus = [\r\n    ...     'This is the first document.',\r\n    ...     'This is the second second document.',\r\n    ...     'And the third one.',\r\n    ...     'Is this the first document?',\r\n    ... ]\r\n\r\n    ## -- End pasted text --\r\n\r\n    [~/]\r\n    [2]: paste\r\n    >>> from sklearn.feature_extraction.text import TfidfVectorizer\r\n    >>> vectorizer = TfidfVectorizer(min_df=1)\r\n    >>> vectorizer.fit_transform(corpus).todense()\r\n\r\n    ## -- End pasted text --\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-2-0d928a0a70df> in <module>()\r\n        1 from sklearn.feature_extraction.text import TfidfVectorizer\r\n        2 vectorizer = TfidfVectorizer(min_df=1)\r\n    ----> 3 vectorizer.fit_transform(corpus)\r\n\r\n    /home/skipper/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc in fit_transform(self, raw_documents, y)\r\n    1239         # X is already a transformed view of raw_documents so\r\n    1240         # we set copy to False\r\n    -> 1241         return self._tfidf.transform(X, copy=False)\r\n    1242 \r\n    1243     def transform(self, raw_documents, copy=True):\r\n\r\n    /home/skipper/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc in transform(self, X, copy)\r\n    1011 \r\n    1012         if self.norm:\r\n    -> 1013             X = normalize(X, norm=self.norm, copy=False)\r\n    1014 \r\n    1015         return X\r\n\r\n    /home/skipper/.local/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc in normalize(X, norm, axis, copy)\r\n        532             inplace_csr_row_normalize_l1(X)\r\n        533         elif norm == 'l2':\r\n    --> 534             inplace_csr_row_normalize_l2(X)\r\n        535     else:\r\n        536         if norm == 'l1':\r\n\r\n    /home/skipper/.local/lib/python2.7/site-packages/sklearn/utils/sparsefuncs_fast.so in sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2 (sklearn/utils/sparsefuncs_fast.c:2768)()\r\n\r\n    ValueError: Buffer dtype mismatch, expected 'int' but got 'long'\r\n\r\n    [~/]\r\n    [3]: from scipy import version\r\n\r\n    [~/]\r\n    [4]: version.version\r\n    [4]: '0.15.0.dev-54796c7'\r\n"""
numpy/numpy#4145,24795774,WarrenWeckesser,juliantaylor,2013-12-26 18:22:32,2014-03-02 12:45:36,2014-03-01 13:39:19,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/4145,b'BUG: Incorrect shape of broadcast result with the exponentiation operator `**`.',"b""I haven't explored the full range of inputs that produce the error, but when `x` has shape `(n,)` and `y` has shape `(1,1)`, the result of `x ** y` does not have the correct (broadcast) shape.  It should be `(1, n)`, but in all the numpy versions I've tried (1.6.1, 1.7.1, 1.8.0, and 1.9.0.dev-e7fe68a), the result has shape `(n,)`.\r\n\r\nAlso, in numpy 1.8.0 and 1.9.0.dev-e7fe68a, the operation generates an unexpected `DeprecationWarning`.\r\n\r\nThe following shows an example.  Python was run with the command `python -W always`.\r\n```\r\n>>> np.__version__\r\n'1.8.0'\r\n\r\n>>> x\r\narray([1, 2])\r\n\r\n>>> y = np.array([[2]])\r\n\r\n>>> x ** y     # This should not generate a warning.  Result should be `array([[1, 4]])`\r\n__main__:1: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\r\narray([1, 4])\r\n\r\n>>> x + y    # No warning, correct shape.\r\narray([[3, 4]])\r\n\r\n>>> y = np.array([[2], [3]])\r\n\r\n>>> x ** y    # No warning, correct shape.\r\narray([[1, 4],\r\n       [1, 8]])\r\n```\r\n"""
scikit-learn/scikit-learn#2882,28096010,larsmans,larsmans,2014-02-22 13:37:19,2014-06-13 12:05:54,2014-02-23 21:41:17,closed,,,24,,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2882,b'[MRG] speed up RBM training with scipy.special.expit',"b""SciPy 0.10 already has an implementation of the logistic function called `scipy.special.expit`. Moved our `logistic_sigmoid` to `fixes.expit`, keeping its log-of-logistic functionality as `log_logistic` (should that be `log_expit`? I don't like the name `expit` at all.)\r\n\r\nUsing this makes RBM training 12% faster, as measured by observing the time per iteration as reported by the `plot_rbm_logistic_classification.py` example, disregarding the first iteration as an outlier (it's consistently faster than the rest, not sure why). The speedup is in the `expit` function, not the inplace operations; those are there to make benchmarking easier, but they certainly won't hurt."""
numpy/numpy#3283,13727683,0todd0000,charris,2013-04-28 00:01:04,2014-02-23 00:19:10,2014-02-22 23:31:40,closed,,,4,,https://api.github.com/repos/numpy/numpy/issues/3283,b'stats.f_oneway needs floats',"b'Tested on:\r\nMac OS X 10.8.3\r\nscipy 0.11.0\r\n\r\n**stats.f_oneway** does not handle integer data properly:\r\n\r\n    >>> import numpy as np\r\n    >>> from scipy import stats\r\n    >>> y0 = np.array([655, 788, 734, 721, 679, 699], dtype=np.uint16)\r\n    >>>  y1 = np.array([789, 772, 786, 686, 732, 774], dtype=np.uint16)\r\n    >>> print stats.f_oneway(y0, y1)\r\n\r\nyields:    f = -0.009,  p = nan\r\n\r\nbut repeating with floats:\r\n    \r\n    >>> y0 = np.array([655, 788, 734, 721, 679, 699], dtype=float)\r\n    >>> y1 = np.array([789, 772, 786, 686, 732, 774], dtype=float)\r\n    >>> print stats.f_oneway(y0, y1)\r\n\r\nyields the correct results:    f = 3.052, p = 0.111\r\n\r\nSuggested fix:  change the first line of f_oneway from:\r\n\r\n    >>> args = map(np.asarray, args)\r\n\r\nto:\r\n\r\n    >>> args = [np.asarray(arg, dtype=float)  for arg in args]\r\n\r\n\r\n'"
numpy/numpy#5858,74579333,jaimefrio,charris,2015-05-09 04:13:08,2015-05-10 16:13:46,2015-05-10 14:47:07,closed,,,3,11 - Bug;component: numpy.random,https://api.github.com/repos/numpy/numpy/issues/5858,b'BUG: random.beta with small parameters',
numpy/numpy#2056,7729354,thouis,jaimefrio,2012-10-19 21:03:21,2015-05-12 10:53:09,2015-05-12 10:53:09,closed,,,5,11 - Bug;component: numpy.random;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2056,b'mtrand.beta does not handle small parameters well (Trac #1459)',"b""_Original ticket http://projects.scipy.org/numpy/ticket/1459 on 2010-04-20 by trac user JohnReid, assigned to unknown._\n\nI've been using scipy.stats.beta and I've noticed it returns NaN when the parameters are small. I've tried using the same parameters in R code and it handles them just fine. R also handles parameters up to 17 orders of magnitude smaller. Is there any documentation on which parameter ranges are acceptable? Can I expect similar results with other distributions?\n\nPlease see my recent post on scipy-user@scipy.org with Josef's follow-up.\n\nHere's the code I used:\n\n\n\n    import scipy.stats as S, numpy as N\n    from rpy2.robjects import r\n    \n    alpha, beta = 0.0710, 0.4222\n    for i in xrange(20):\n         x_from_scipy = S.beta.rvs(alpha, beta)\n         x_from_R = r.rbeta(1, alpha, beta)\n         print 'Alpha=%.2e; Beta=%.2e; scipy.stats.beta.rvs=%.2e; \n    R.rbeta=%.2e' % (alpha, beta, x_from_scipy, x_from_R[0])\n         alpha /= 10.\n         beta /= 10.\n    \n\n\n\nand the output from it:\n\n\n    Alpha=7.10e-02; Beta=4.22e-01; scipy.stats.beta.rvs=2.75e-11; \n    R.rbeta=6.60e-02\n    Alpha=7.10e-03; Beta=4.22e-02; scipy.stats.beta.rvs=3.73e-84; \n    R.rbeta=4.50e-124\n    Alpha=7.10e-04; Beta=4.22e-03; scipy.stats.beta.rvs=1.00e+00; \n    R.rbeta=1.00e+00\n    Alpha=7.10e-05; Beta=4.22e-04; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-313\n    Alpha=7.10e-06; Beta=4.22e-05; scipy.stats.beta.rvs=nan; R.rbeta=1.00e+00\n    Alpha=7.10e-07; Beta=4.22e-06; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-315\n    Alpha=7.10e-08; Beta=4.22e-07; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-316\n    Alpha=7.10e-09; Beta=4.22e-08; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-317\n    Alpha=7.10e-10; Beta=4.22e-09; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-318\n    Alpha=7.10e-11; Beta=4.22e-10; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-319\n    Alpha=7.10e-12; Beta=4.22e-11; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-320\n    Alpha=7.10e-13; Beta=4.22e-12; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-321\n    Alpha=7.10e-14; Beta=4.22e-13; scipy.stats.beta.rvs=nan; R.rbeta=3.95e-322\n    Alpha=7.10e-15; Beta=4.22e-14; scipy.stats.beta.rvs=nan; R.rbeta=1.00e+00\n    Alpha=7.10e-16; Beta=4.22e-15; scipy.stats.beta.rvs=nan; R.rbeta=1.00e+00\n    Alpha=7.10e-17; Beta=4.22e-16; scipy.stats.beta.rvs=nan; R.rbeta=0.00e+00\n    Alpha=7.10e-18; Beta=4.22e-17; scipy.stats.beta.rvs=nan; R.rbeta=1.00e+00\n    Alpha=7.10e-19; Beta=4.22e-18; scipy.stats.beta.rvs=nan; R.rbeta=0.00e+00\n    Alpha=7.10e-20; Beta=4.22e-19; scipy.stats.beta.rvs=nan; R.rbeta=0.00e+00\n    Alpha=7.10e-21; Beta=4.22e-20; scipy.stats.beta.rvs=nan; R.rbeta=0.00e+00\n\n"""
numpy/numpy#2655,7731807,numpy-gitbot,numpy-gitbot,2012-10-19 22:34:42,2014-02-25 23:12:58,2012-10-19 22:34:45,closed,,NumPy 1.7,11,11 - Bug;component: numpy.core;priority: highest,https://api.github.com/repos/numpy/numpy/issues/2655,b'unique() does not seem to return correct index if array larger than 16 (Trac #2063)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/2063 on 2012-02-23 by trac user lolowizard, assigned to unknown._\n\nUnique seem to return the index of the last occurence rather than first occurence if len(array)>16\n\nTo reproduce:\n\n\n    v=[0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2]\n    w=[0,0,0,0,0,1,1,1,1,1,1,2,2,2,2]\n\n\nOnly difference between v and w is the two extra values at the end.\n \nThen:\n\n\n    unique(v,return_index=True)\n    unique(w,return_index=True)\n\n\nLook at the results for both commands:\n\n\n    (array([0, 1, 2]), array([ 0, 10, 15]))\n     \n    (array([0, 1, 2]), array([0, 5, 9]))\n\n\nThe two commands should return the same index!'"
numpy/numpy#4602,31126988,cgohlke,charris,2014-04-09 03:22:56,2014-06-23 21:08:01,2014-04-09 14:12:53,closed,,,1,,https://api.github.com/repos/numpy/numpy/issues/4602,b'BUG: ifort has issues with optimization flag /O2',"b'Fixes several hard to track scipy test failures. \r\nSee scipy/scipy#3306, scipy/scipy#3340, scipy/scipy#1205, and discussion at http://software.intel.com/en-us/articles/numpyscipy-with-intel-mkl.\r\nNeeds backporting to numpy 1.8.x.'"
rgommers/scipy#8,27199271,juliantaylor,rgommers,2014-02-08 12:08:10,2015-08-30 11:17:37,2014-02-16 17:15:47,closed,,,1,,https://api.github.com/repos/rgommers/scipy/issues/8,b'TST: avoid use of unsafe mktemp and cleanup test_dir in test_c_spec',
Alexpux/MINGW-packages#211,43039866,tenko,Alexpux,2014-09-17 17:24:47,2014-09-19 17:14:21,2014-09-17 19:24:50,closed,,,12,,https://api.github.com/repos/Alexpux/MINGW-packages/issues/211,b'Pyscipkg',"b'Added openblas package built from a regular release and updated numpy to use this package.\r\nThe openblas backage was built with the optimized lapack included so therfore the lapack\r\npackage is not needed by numpy.\r\n\r\nI addition the following packages where added with dependency:\r\n * scipy\r\n * matplotlib\r\n * cython\r\n\r\nNote:\r\n- The test suite for numpy and scipy does only partly run. Therefore the automatic checks in\r\n   scipy is removed.\r\n- matplotlib is dependent on python-cairo. For python3 this feature is removed due to python-cairo is broken.\r\n  Saving of plots to images, pdf etc is therfore not available in python3.\r\n  As an alternative the cairocffi can be used as a drop-in replacement for pycairo. However I was not\r\n  able to build cairocffi yet.'"
astropy/astropy#2816,39406320,mcara,mdboom,2014-08-04 10:00:05,2014-08-08 21:53:13,2014-08-08 14:01:22,closed,,,8,wcs,https://api.github.com/repos/astropy/astropy/issues/2816,b'Performance and stability improvements to wcs.all_world2pix - 2nd iterat...',"b'This PR is a second iteration of PR #2373 with the main difference that now the changes are on a branch (instead of master), hopefully does not have (yet) merge conflicts, and addresses the issue of failing doctests from PR #2373 by using data file name relative to the package as done in PR #2807 and with updated output of the examples.\r\n\r\nThis PR (as did #2373) provides significant improvements to the performance (typically 1000-2000 times faster) and stability of `wcs.all_world2pix` function. The used algorithm is a completely vectorized implementation of the method of consecutive approximations (fixed point iterations). While this method is a low-order method, it is more than adequate for geometric distortions that are typical in astrophysical images (smooth, slowly varying polynomials).\r\n\r\nCompared to the original implementation of `all_world2pix`, this implementation does not require users to have SciPy installed in order to run.\r\n\r\nWhile there is a check to see if WCS contains higher-order distortions (and return the exact solution if it does not), the method itself does not *require* this check and will converge to the exact solution after one iteration if WCS does not contain higher order distortions. This check is mostly for performance purposes to avoid unnecessary iterative solution for images that do not contain distortions (and pix->sky requires only WCS-based, i.e., CD-only transformations).\r\n\r\nThe proposed replacement for the current implementation of `all_world2pix` avoids unnecessary use of `wcs_world2pix` and `all_pix2world` such as `wcs_world2pix(all_pix2world(pix))` by replacing this with `pix2foc`. This avoids unnecessary CD-matrix based transformation CD(CD^{-1})=I which in itself results in about 50% performance improvement over `wcs_world2pix(all_pix2world(pix))` and likely 25% over `all_pix2world` alone.\r\n\r\nThe function has additional code to check for *divergence* condition (as opposite to *slow convergence*) when consecutive approximations *diverge*. This can happen when the pixel coordinates are *far* away from the reference pixel (distortions are ""severely"" extrapolated), or for poorly-defined geometric distortions (at least for this numerical method).\r\n\r\nA new and improved `NoConvergence` exception has attributes that allows users to retrieve best solution reached by the function, and indices of *slowly converging* solutions (points) that *did not converge* in the specified maximum number of iterations as well as indices of *diverging* solutions (likely an indication of poorly described distortions or very large distortions).\r\n\r\nFinally, the new test function `test_all_world2pix_direct` (proposed as a replacement to the old test) provides a much more improved (versatile) and *thorough* test of the `all_world2pix` function.\r\n\r\nThese improvements address the issues raised in #1977, https://github.com/scipy/scipy/issues/3227 that plagued `all_world2pix`, and #2294 (in addition to performance improvements).\r\n\r\nTo keep things together, in the next comment I will outline the proposed method and code. For a mode detailed discussion and comments - see #2373.'"
astropy/astropy#2373,32299512,mcara,mcara,2014-04-26 22:09:40,2014-08-04 07:16:29,2014-08-04 07:16:29,closed,,v1.0.0,77,wcs,https://api.github.com/repos/astropy/astropy/issues/2373,b'Performance and stability improvements to wcs.all_world2pix',"b'Significant improvements to the performance (typically 1000-2000 times faster) and stability of `wcs.all_world2pix` function. The used algorithm is a completely vectorized implementation of the method of consecutive approximations (fixed point iterations). While this method is a low-order method, it is more than adequate for geometric distortions that are typical in astrophysical images (smooth, slowly varying polynomials). ~~The method is somewhat similar to the existing `HSTWCS.all_sky2pix`, though~~ The\r\nchoice of the function, initial approximation, and implementation, makes the method proposed here to converge in half of the iterations of `HSTWCS.all_sky2pix`.\r\n\r\nCompared to the original implementation of `all_world2pix`, this implementation does not require users to have SciPy installed in order to run.\r\n\r\nWhile there is a check to see if WCS contains higher-order distortions (and return the exact solution if it does not), the method itself does not *require* this check and will converge to the exact solution after one iteration if WCS does not contain higher order distortions. This check is mostly for performance purposes to avoid unnecessary iterative solution for images that do not contain distortions (and pix->sky requires only WCS-based, i.e., CD-only transformations).\r\n\r\nThe proposed replacement for the current implementation of `all_world2pix` avoids unnecessary use of `wcs_world2pix` and `all_pix2world` such as `wcs_world2pix(all_pix2world(pix))` by replacing this with `pix2foc`. This avoids unnecessary CD-matrix based transformation CD(CD^{-1})=I which in itself results in about 50% performance improvement over `wcs_world2pix(all_pix2world(pix))` and likely 25% over `all_pix2world` alone.\r\n\r\nThe function has additional code to check for *divergence* condition (as opposite to *slow convergence*) when consecutive approximations *diverge*. This can happen when the pixel coordinates are *far* away from the reference pixel (distortions are ""severely"" extrapolated), or for poorly-defined geometric distortions (at least for this numerical method).\r\n\r\nA ""new and improved"" :smiley: `NoConvergence` exception has attributes that allows users to retrieve best solution reached by the function, and indices of *slowly converging* solutions (points) that *did not converge* in the specified maximum number of iterations as well as indices of *diverging* solutions (likely an indication of poorly described distortions or very large distortions).\r\n\r\nFinally, the new test function `test_all_world2pix_direct` (proposed as a replacement to the old test) provides a much more improved (versatile) and *thorough* test of the `all_world2pix` function.\r\n\r\nThese improvements address the issues raised in #1977, https://github.com/scipy/scipy/issues/3227 that plagued `all_world2pix`, and #2294 (in addition to performance improvements).'"
astropy/astropy#1977,25891750,rmjarvis,astrofrog,2014-01-19 23:29:53,2014-10-30 13:48:40,2014-10-30 13:48:40,closed,,v1.0.0,27,Close?;wcs,https://api.github.com/repos/astropy/astropy/issues/1977,b'Problems with wcs.all_world2pix function.',"b""I've been working to incorporate the astropy.wcs code as a backend (one of several options) to the WCS functionality in [GalSim](https://github.com/GalSim-developers/GalSim), and I ran into a few problems getting it to work correctly.\r\n\r\nThe first is that the `ra_dec_order=True` option seems to be broken.  Both for the `pix2world` direction and the `world2pix` direction.  However, I believe this may have already been fixed in PR #1463.  So I won't bother about that one further, except to give a +1 to getting that merged and released.\r\n\r\nThe next problem is that the scipy `broyden1` function cannot handle being given the exactly correct answer as an input.  Since this is the case in `_all_world2pix` when using a WCS type like TAN that doesn't have any non-linear component, it fails.  This is fixed by giving it a good estimate of `alpha` on input, which you can do easily in this case.  it is just the `cdelt` value.  And even for things like SIP that do need the non-linear part, it is still hugely important to give a good value of `alpha`, since it is extremely slow without it as it thrashes around for a while with the wrong step size.\r\n\r\nThird, if the input `ra` does not have the same phase as CRVAL, then the current code simply does not work.  e.g. if CRVAL has ra = 290, and the input has ra = -70, it won't converge.  This is fixed by using fmod in the function you pass to `broyden1`.\r\n\r\nI have code that shows the failures I'm talking about and includes a fix.  I'll put it in the next comment to keep this post from getting too long.  You can incorporate it into your code base as you see fit (including perhaps taking some of the main function for your unit tests...)."""
astropy/pyregion#35,37227657,mcara,,2014-07-06 20:38:38,2014-07-06 20:39:03,None,open,,,0,,https://api.github.com/repos/astropy/pyregion/issues/35,b'Pyregion should use distortion-aware WCS functions for coordinate transformations',"b'Currently, `pyregion` uses WCS functions that do not take into account distortion corrections: `wcs.wcs_world2pix()` and `wcs.wcs_pix2world()`. However, distortions for most HST instruments are significant and should be taken into account. This could be done  by switching to  `wcs.all_world2pix()` and `wcs.all_pix2world()` - really a very simple change to the code.\r\n\r\nWhile at this moment I cannot recommend this change until issues in https://github.com/astropy/astropy/issues/1977, https://github.com/scipy/scipy/issues/3227, https://github.com/astropy/astropy/pull/2294 are solved. I have submitted a PR that fixes these issues in `all_world2pix()` and provides performance improvements https://github.com/astropy/astropy/pull/2373 but it is not yet merged.\r\n\r\nI will post here when it is safe to do this transition.\r\n\r\nNOTE: for images that do not have distortions  `wcs.all_world2pix()` and `wcs.all_pix2world()` are similar to their -non-distortion-aware analogs `wcs.wcs_world2pix()` and `wcs.wcs_pix2world()`.'"
,,,,,,,,,,,,,,
Homebrew/homebrew-python#139,36730933,tdsmith,tdsmith,2014-06-28 18:51:36,2015-03-20 16:19:19,2015-03-20 16:19:19,closed,,,4,,https://api.github.com/repos/Homebrew/homebrew-python/issues/139,b'scipy build fails with --cc=gcc-4.2 on 10.9',"b'Is this an important configuration? Error is:\r\n\r\n    compile options: \'-Iscipy/sparse/linalg/eigen/arpack/ARPACK/SRC -I/usr/local/lib/python2.7/site-packages/numpy/core/include -c\'\r\n    gcc-4.2: /private/tmp/scipy-pX9L/scipy-0.14.0/scipy/_build_utils/src/wrap_g77_abi_c.c\r\n    In file included from     /System/Library/Frameworks/vecLib.framework/Headers/vecLib.h:31,\r\n                     from     /System/Library/Frameworks/Accelerate.framework/Headers/Accelerate.h:20,\r\n                     from /private/tmp/scipy-pX9L/scipy-0.14.0/scipy/_build_utils/src/wrap_g77_abi_c.c:1:\r\n    /System/Library/Frameworks/vecLib.framework/Headers/vBasicOps.h:153:23: error: immintrin.h: No such file or directory\r\n    In file included from     /System/Library/Frameworks/vecLib.framework/Headers/vecLib.h:31,\r\n                     from     /System/Library/Frameworks/Accelerate.framework/Headers/Accelerate.h:20,\r\n                     from /private/tmp/scipy-pX9L/scipy-0.14.0/scipy/_build_utils/src/wrap_g77_abi_c.c:1:\r\n    /System/Library/Frameworks/vecLib.framework/Headers/vBasicOps.h:153:23: error: immintrin.h: No such file or directory\r\n    error: Command ""gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -I/usr/local/include -I/usr/local/opt/sqlite/include -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -Iscipy/sparse/linalg/eigen/arpack/ARPACK/SRC -I/usr/local/lib/python2.7/site-packages/numpy/core/include -c /private/tmp/scipy-pX9L/scipy-0.14.0/scipy/_build_utils/src/wrap_g77_abi_c.c -o build/temp.macosx-10.9-x86_64-2.7/private/tmp/scipy-pX9L/scipy-0.14.0/scipy/_build_utils/src/wrap_g77_abi_c.o"" failed with exit status 1\r\n\r\nBuild log: https://gist.github.com/995b588397713b861603'"
darbula/pymote#15,88157370,sglumac,darbula,2015-06-14 09:47:54,2015-06-14 09:57:01,2015-06-14 09:57:01,closed,,,0,,https://api.github.com/repos/darbula/pymote/issues/15,b'Update setup.py (scipy version)',"b""Newer version of scipy don't support pickling stats probability distributions (https://github.com/scipy/scipy/issues/3125). This disables pickling of pymote networks. Until scipy addresses this issue, one possible solution would be to fix scipy version to 0.12."""
QInfer/python-qinfer#24,49151615,cgranade,cgranade,2014-11-17 22:04:35,2014-11-17 23:49:29,2014-11-17 23:49:28,closed,,,2,,https://api.github.com/repos/QInfer/python-qinfer/issues/24,"b'Parallel performance tracking, online monitoring, rich metadata'","b""Using @ysanders's improvements in #23, this PR also adds in parallelization using IPython to perf_testing, and can monitor tasks remotely using\r\n``tskmon`` (that server is not yet production-ready, but since this is an optional feature of ``perf_testing``, I think that should work fine)."""
scikit-learn/scikit-learn#4102,54386881,vineetp13,amueller,2015-01-14 22:34:17,2015-02-10 16:19:21,2015-02-10 16:19:21,closed,,0.16,27,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4102,b'nosetest failure:assertionerror',"b'```\r\n======================================================================\r\nFAIL: sklearn.feature_extraction.tests.test_image.test_connect_regions\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Library/Python/2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/tests/test_image.py"", line 63, in test_connect_regions\r\n    assert_equal(ndimage.label(mask)[1], connected_components(graph)[0])\r\nAssertionError: 777 != 767\r\n\r\n======================================================================\r\nFAIL: sklearn.feature_extraction.tests.test_image.test_connect_regions_with_grid\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Library/Python/2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/tests/test_image.py"", line 70, in test_connect_regions_with_grid\r\n    assert_equal(ndimage.label(mask)[1], connected_components(graph)[0])\r\nAssertionError: 777 != 767\r\n\r\n----------------------------------------------------------------------\r\nRan 3342 tests in 148.698s\r\n\r\nFAILED (SKIP=20, failures=2)\r\n```'"
scikit-learn/scikit-learn#1968,14368763,haroldn,arjoly,2013-05-15 17:06:27,2014-01-29 18:15:00,2014-01-29 18:15:00,closed,,,2,,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1968,b'failing nose tests related to images',"b'I appreciate any aid in this issue, and apologize I\'ve missed finding it in a previous issue. I\'ve been getting the following message (I\'m running on Mac OS X 10.7):\r\n\r\nHere is the full traceback:\r\n\r\n```\r\n$ nosetests sklearn -exe\r\n....................S........................................../usr/local/lib/python2.7/site-packages/sklearn/manifold/spectral_embedding.py:225: UserWarning: Graph is not fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n...........SS.........F.....SE................................................S.........................................................S.........................................SSS....................../usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x10a829788>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................FF......................................................................................S......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................SSS....S....S...................................................................................................................................\r\n======================================================================\r\nERROR: test suite for <module \'sklearn.datasets.tests.test_lfw\' from \'/usr/local/lib/python2.7/site-packages/sklearn/datasets/tests/test_lfw.pyc\'>\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/suite.py"", line 208, in run\r\n    self.setUp()\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/suite.py"", line 291, in setUp\r\n    self.setupContext(ancestor)\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/suite.py"", line 314, in setupContext\r\n    try_run(context, names)\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/util.py"", line 469, in try_run\r\n    return func()\r\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/datasets/tests/test_lfw.py"", line 72, in setup_module\r\n    imsave(file_path, uniface)\r\n  File ""/Users/admin/src/scipy/scipy/misc/pilutil.py"", line 160, in imsave\r\n    im.save(name)\r\n  File ""/usr/local/lib/python2.7/site-packages/PIL/Image.py"", line 1439, in save\r\n    save_handler(self, fp, filename)\r\n  File ""/usr/local/lib/python2.7/site-packages/PIL/JpegImagePlugin.py"", line 471, in _save\r\n    ImageFile._save(im, fp, [(""jpeg"", (0,0)+im.size, 0, rawmode)])\r\n  File ""/usr/local/lib/python2.7/site-packages/PIL/ImageFile.py"", line 495, in _save\r\n    e = Image._getencoder(im.mode, e, a, im.encoderconfig)\r\n  File ""/usr/local/lib/python2.7/site-packages/PIL/Image.py"", line 401, in _getencoder\r\n    raise IOError(""encoder %s not available"" % encoder_name)\r\nIOError: encoder jpeg not available\r\n\r\n======================================================================\r\nFAIL: sklearn.datasets.tests.test_base.test_load_sample_image\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/datasets/tests/test_base.py"", line 142, in test_load_sample_image\r\n    assert_equal(china.dtype, \'uint8\')\r\nAssertionError: dtype(\'O\') != \'uint8\'\r\n\r\n======================================================================\r\nFAIL: sklearn.feature_extraction.tests.test_image.test_connect_regions\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/tests/test_image.py"", line 63, in test_connect_regions\r\n    assert_equal(ndimage.label(mask)[1], cs_graph_components(graph)[0])\r\nAssertionError: 777 != 767\r\n\r\n======================================================================\r\nFAIL: sklearn.feature_extraction.tests.test_image.test_connect_regions_with_grid\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/tests/test_image.py"", line 70, in test_connect_regions_with_grid\r\n    assert_equal(ndimage.label(mask)[1], cs_graph_components(graph)[0])\r\nAssertionError: 777 != 767\r\n\r\n----------------------------------------------------------------------\r\nRan 1595 tests in 81.893s\r\n\r\nFAILED (SKIP=15, errors=1, failures=3)\r\n```'"
mne-tools/mne-python#912,23062687,dengemann,Eric89GXL,2013-11-21 12:17:03,2015-03-10 01:51:36,2015-03-10 01:51:36,closed,,0.9,16,,https://api.github.com/repos/mne-tools/mne-python/issues/912,b'BUG/API with filtering',"b""@jdammers just demonstrated to me how to silently produce nans with our band pass filter.\r\n\r\nhttps://gist.github.com/dengemann/7580547\r\n\r\nOr old non-Python routines default to order 16 butterworth band pass filters \r\n(FFT implementation) ...\r\nIf you go for lower orders, in certain application evil filter artifacts would occur.\r\n\r\nI think we should \r\n\r\n1) add a clear RuntimeError if NaNs are returned by scipy.signal.filtfilt\r\n\r\n2) @jdammers suggested to port his IDL fft code, currently we don't support order type construction for fft filters.\r\n\r\nWDYT?"""
swig/swig#92,20236288,cdeil,wsfulton,2013-09-29 21:22:50,2014-02-16 22:47:23,2014-02-16 22:47:23,closed,,,1,,https://api.github.com/repos/swig/swig/issues/92,b'Python warnings from swig-generated code with latest clang on Mac',"b""With `swig @2.0.10 (devel)` and the latest clang from XCode 5 on Mac the swig-generated code in `scipy` generates tons of warnings:\r\nhttps://github.com/scipy/scipy/issues/2944\r\n\r\nCan you have a look if it's possible to adapt swig to generate code that results in fewer warnings?"""
ContinuumIO/anaconda-issues#658,132874002,marscher,ilanschnell,2016-02-11 02:35:30,2016-03-02 19:28:10,2016-02-11 04:38:34,closed,,,11,,https://api.github.com/repos/ContinuumIO/anaconda-issues/issues/658,b'NumPy 1.10.4 with MKL brakes eigensolver',"b'To reproduce:\r\n```\r\nconda create -n mkl_bug -c omnia msmtools\r\nnosetests msmtools.analysis.decomposition_test\r\n```\r\noutput:\r\n```\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    python-3.5.1               |                0        16.7 MB\r\n    numpy-1.10.4               |           py35_0         5.9 MB\r\n    setuptools-19.6.2          |           py35_0         368 KB\r\n    wheel-0.29.0               |           py35_0          82 KB\r\n    pip-8.0.2                  |           py35_0         1.5 MB\r\n    scipy-0.17.0               |      np110py35_1        29.1 MB\r\n    msmtools-1.1.1             |      np110py35_0         1.1 MB\r\n    ------------------------------------------------------------\r\n                                           Total:        54.8 MB\r\n```\r\nThe test program uses ""scipy.sparse.linalg.eig(s,sh)"" routines. The resulting values are far off the former reference values:\r\n```\r\n======================================================================\r\nFAIL: test_eigenvectors (msmtools.analysis.sparse.decomposition_test.TestDecomposition)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/marscher/miniconda/envs/foo/lib/python3.5/site-packages/msmtools/analysis/sparse/decomposition_test.py"", line 129, in test_eigenvectors\r\n    assert_allclose(vals[np.newaxis, :] * Rn, P.dot(Rn))\r\n  File ""/home/marscher/miniconda/envs/foo/lib/python3.5/site-packages/msmtools/util/numeric.py"", line 38, in assert_allclose\r\n    err_msg=err_msg, verbose=True)\r\n  File ""/home/marscher/miniconda/envs/foo/lib/python3.5/site-packages/numpy/testing/utils.py"", line 1359, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File ""/home/marscher/miniconda/envs/foo/lib/python3.5/site-packages/numpy/testing/utils.py"", line 713, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-05, atol=1e-08\r\n\r\n(mismatch 84.9%)\r\n x: array([[ -1.000000e-01+0.j,   6.281856e-03+0.j,  -1.625476e-06+0.j,\r\n          1.253260e-02+0.j,   3.279252e-06-0.j,  -1.878322e-02+0.j,\r\n          1.661728e-05-0.j,  -2.484681e-02+0.j,   3.001051e-02-0.j,...\r\n y: array([[ -1.000000e-01+0.j,  -6.269583e-03+0.j,  -1.625344e-06+0.j,\r\n         -1.245855e-02+0.j,   3.259026e-06+0.j,   1.848905e-02+0.j,\r\n          1.735485e-05+0.j,   2.253176e-02+0.j,   2.299964e-02+0.j,...\r\n```\r\n\r\nThis has been introduced with the recent (1-2 days?) MKL dependency of Scipy/NumPy. Since this seriously breaks our program, I would be very glad, if you could investigate this soon. Thank you very much!'"
ContinuumIO/anaconda-issues#59,25609767,teoliphant,ilanschnell,2014-01-14 22:08:21,2016-03-05 20:32:27,2016-03-03 22:57:30,closed,ilanschnell,,6,,https://api.github.com/repos/ContinuumIO/anaconda-issues/issues/59,b'Compiling with older versions of gfortran causes segfault in interpolative in SciPy 0.13',b'\r\nscipy.test() in Anaconda 1.9 causes a segfault \r\n\r\ndue to the use of a compiler that appears not to handle the entry() construct of fortran correctly..  \r\n\r\nHere is the scipy issue where it is noted that updating to gfortran 4.4 fixes the problem.\r\n\r\nhttps://github.com/scipy/scipy/issues/2939'
numpy/numpy#3793,20066307,juliantaylor,charris,2013-09-25 20:03:51,2013-10-23 12:40:23,2013-09-26 22:52:23,closed,,1.8 blockers,0,,https://api.github.com/repos/numpy/numpy/issues/3793,b'scalar int hashing broken on 64 bit python3',"b'```\r\nIn [1]: import numpy as np;\r\nIn [2]: i=2**31;\r\nIn [3]: print(np.int64(i) in {i:111})\r\nFalse\r\nIn [6]: print(hash(np.int64(i)), hash(i))\r\n(-2147483648, 2147483648)\r\n```\r\n\r\non python2 it returns True,\r\ndiscovered on s390x where hash(np.int64(4) returns zero because int_arrtype_hash casts the object to PyIntScalarObject which cuts of four bytes (as its really PyLongScalarObject) which are zero on big endian:\r\nhttp://bugs.debian.org/cgi-bin/bugreport.cgi?bug=724047\r\n\r\nThis is probably also the cause of the difference on amd64.\r\n\r\n\r\nmost likely also the cause scipy/scipy#2930\r\n'"
,,,,,,,,,,,,,,
cureos/jcobyla#1,54356207,stkiese,anders9ustafsson,2015-01-14 18:14:45,2015-09-14 07:31:33,2015-09-14 07:31:33,closed,,,12,,https://api.github.com/repos/cureos/jcobyla/issues/1,b'Successful termination when constraints violated',"b""I have a similar issue as described in https://github.com/scipy/scipy/issues/2891 for the python port of cobyla2: the minimization can terminate without actually satisfying the constraints.\r\n\r\nI've uploaded an example to pastebin: http://pastebin.com/B7Z0Y95h\r\n\r\nIn my case, reducing rhobeg (solution in https://github.com/scipy/scipy/issues/2891 ) won't help. With rhobeg=1 the constraints are violated by 0.091. With rhobeg=0.001 the constraints are violated by 0.254. With rhobeg=0.00001 the constraints are violated by 0.267 ... (with simultaneous increase from 710 to 27153 iterations).\r\n\r\nIn my opinion this is a bug.\r\n\r\nThanks\r\nStefan"""
xypron/jcobyla#1,54544576,stkiese,,2015-01-16 06:35:41,2015-01-17 11:25:47,None,open,xypron,,3,question,https://api.github.com/repos/xypron/jcobyla/issues/1,b'Successful termination when constraints violated',"b'I have a similar issue as described in https://github.com/scipy/scipy/issues/2891 for the python port of cobyla2: the minimization can terminate without actually satisfying the constraints.\r\n\r\nI\'ve uploaded an example to pastebin: http://pastebin.com/p61JmLsb\r\n\r\nIn my case, reducing rhobeg (solution in https://github.com/scipy/scipy/issues/2891 ) won\'t help. I\'ve tried different values for rhobeg:\r\n\r\nrhobeg=1 => MAXCV=0.091\r\nrhobeg=0.75 => MAXCV=0.0543\r\nrhobeg=0.5 => MAXCV=0.0287\r\nrhobeg=0.25 => MAXCV=0.0686\r\nrhobeg=0.1 => MAXCV=0.0925\r\nrhobeg=0.05 => MAXCV=0.0735\r\nrhobeg=0.001 => MAXCV=0.254\r\n\r\nIt seems that the\r\n\r\n6th (the 6th input var has to be greater or equal to zero), \r\n(9th or 10th) (the sum of all input vars has to be approximately 1)\r\nand 11st (to complex to explain in one sentence ;-) )\r\n\r\nconstraints are violated. But there is a solution that satisfies all constraints:\r\n\r\n{0.705, 0.0075, 0.075, 0, 0.0375, 0, 0.05, 0.125}\r\n\r\nBut even if I start with this vector, I have to decrease rhobeg to 0.0005 to get a solution with a violation level lower than 0.001.\r\n\r\nIs there a bug in my code, in jcobyla, a bug in the java port of cobyla or just ""normal behavior""? \r\n\r\nFor the sake of completeness: I\'ve also tried another java cobyla implementation, https://github.com/cureos/jcobyla . The cureos implentation exhibits the same behavior. The author mentioned that it could be that cobyla has problems with equality constraints in general: https://github.com/cureos/jcobyla/issues/1\r\n\r\nThanks!\r\nStefan'"
numpy/numpy#3768,19824669,charris,charris,2013-09-20 15:42:29,2014-07-03 22:56:13,2014-07-03 22:56:13,closed,,,18,06 - Task;priority: normal,https://api.github.com/repos/numpy/numpy/issues/3768,b'Complex128 alignment leads to scipy test failures.',b'The tests that fail check inplace fft  and fail due to copies being made on win 32 when allocated data memory is not 16 byte aligned. The change is numpy/numpy@c9bf9b0. Discussion at https://github.com/scipy/scipy/issues/2890#issuecomment-24764007.'
numpy/numpy#3796,20087482,charris,charris,2013-09-26 02:45:11,2013-09-29 00:36:23,2013-09-29 00:36:23,closed,,1.8 blockers,16,,https://api.github.com/repos/numpy/numpy/issues/3796,b'Python 3 array from buffer of unicode failure',"b""\r\nReported by @cgohlke here: https://github.com/scipy/scipy/issues/2890#issuecomment-25139024\r\nHere's standalone code for the Python 3 specific ValueError: ndarray is not C-contiguous:\r\n```\r\nimport numpy as np\r\na = np.array([['Hello', 'Foob']], dtype='<U5', order='F')\r\nnp.ndarray(shape=[1, 2, 5], dtype='<U1', buffer=a)\r\n```\r\nIt passes with numpy 1.7.1 but fails with 1.8.0.dev-5a0d09c. The expected result is\r\n```\r\n[[['H' 'e' 'l' 'l' 'o']\r\n  ['F' 'o' 'o' 'b' '']]]\r\n```\r\n"""
numpy/numpy#3491,16241851,jayvius,charris,2013-07-01 23:55:02,2014-06-12 14:47:16,2013-07-08 22:30:20,closed,,,4,,https://api.github.com/repos/numpy/numpy/issues/3491,b'Fix creation of string arrays from object types',b'Fixes issue #3398'
numpy/numpy#3761,19686534,jayvius,charris,2013-09-18 15:10:32,2014-08-09 14:18:28,2013-09-19 16:54:37,closed,,1.8 blockers,8,,https://api.github.com/repos/numpy/numpy/issues/3761,b'Fix for PR #3491',"b""Fix for PR #3491, reported on https://github.com/scipy/scipy/issues/2890\r\n\r\nDon't convert unicode string to ascii string before getting byte length. Also, get length of non string type as ascii or unicode depending on output string type."""
numpy/numpy#3658,18708457,jjhelmus,juliantaylor,2013-08-29 02:29:32,2014-06-13 11:10:45,2013-09-19 20:39:45,closed,,,9,,https://api.github.com/repos/numpy/numpy/issues/3658,b'ENH: percentile function with additional parameters and vecorization',"b""The percentile function was enhanced by adding limit and interpolation\r\nparameters to give it similar functionality to SciPy's stats.scoreatpercentile\r\nfunction.  In addition the function was vecorized along q and rewritten to\r\nuse the partition method for better performance.\r\n\r\nThis pull request is in replacement of PR #2970"""
numpy/numpy#3733,19461845,pkgw,juliantaylor,2013-09-13 16:14:00,2014-02-03 19:59:34,2014-02-03 19:59:34,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/3733,b'np.percentile broken for vector `q` argument',"b'np.percentile claims to function with the `q` argument (the percentiles to calculate) being either a scalar or an array_like. However, the initial lines of code do:\r\n\r\n```\r\nif q == 0:\r\n    return a.min(axis=axis, out=out)\r\nelif q == 100:\r\n    return a.max(axis=axis, out=out)\r\n````\r\n\r\nwhich of course breaks with a ""truth value of array undefined"" exception if `q` is in fact an ndarray.\r\n\r\n`q` isn\'t converted to an array before usage, so if it\'s passed as a list value, these checks don\'t cause an exception, but only because of the sketchiness of Python\'s built-in equality testing. I\'ve gotten a report of a crash in the `_compute_qth_percentile` function that seems to be due to the lack of coercion.'"
ipython/ipython#4784,25447586,twiecki,twiecki,2014-01-11 17:50:52,2014-02-27 23:43:49,2014-01-11 18:11:28,closed,,no action,14,,https://api.github.com/repos/ipython/ipython/issues/4784,b'Six import problem only in IPython NB',"b""IPython 1.1.0 and six 1.5.2.\r\n\r\nI can import six.moves.range just fine from regular IPython:\r\n```python\r\nIn [3]: from six.moves import range\r\n\r\nIn [4]: print range\r\n<type 'xrange'>\r\n```\r\n\r\nHowever, from inside IPython NB I can not do this style import even though six.moves.range exists:\r\n```python\r\nimport six.moves\r\n\r\nprint six.moves.range\r\nfrom six.moves import range\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-9-525d29953730> in <module>()\r\n      2 print six.moves.range\r\n      3 \r\n----> 4 from six.moves import range\r\n\r\nImportError: cannot import name range\r\n\r\n<type 'xrange'>\r\n```\r\n\r\nQuite bizarre. Is there any import-hook-trickery that might be causing this? Can anyone reproduce this?\r\n\r\nI made sure they both import the same file."""
numpy/numpy#3399,15131765,WarrenWeckesser,charris,2013-06-04 17:50:16,2014-02-15 16:25:36,2014-02-15 16:25:36,closed,,,11,,https://api.github.com/repos/numpy/numpy/issues/3399,b'Test failure and two errors',"b'This is on Ubuntu 12.04.\r\n\r\nPython 3.3.1 was built from source.  Numpy is from the master branch, commit e9e490a54.\r\n\r\nThe failure was discussed on the mailing list on June 1, and can be fixed by using appropriate precision specifications in some of the array comparison assertions in the function `test_dot_array_order()` in the file `numpy/core/tests/test_blasdot.py`.\r\n\r\n```\r\nwarren@solenoid2:~$ python3 -c ""import numpy as np; np.show_config(); np.test(\'full\')""\r\n\r\natlas_threads_info:\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\n    libraries = [\'lapack\', \'ptf77blas\', \'ptcblas\', \'atlas\']\r\n    library_dirs = [\'/usr/lib/atlas-base/atlas\', \'/usr/lib/atlas-base\']\r\n    language = f77\r\n    include_dirs = [\'/usr/include/atlas\']\r\nmkl_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblas_opt_info:\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\n    libraries = [\'ptf77blas\', \'ptcblas\', \'atlas\']\r\n    library_dirs = [\'/usr/lib/atlas-base\']\r\n    language = c\r\n    include_dirs = [\'/usr/include/atlas\']\r\natlas_blas_threads_info:\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\n    libraries = [\'ptf77blas\', \'ptcblas\', \'atlas\']\r\n    library_dirs = [\'/usr/lib/atlas-base\']\r\n    language = c\r\n    include_dirs = [\'/usr/include/atlas\']\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nlapack_opt_info:\r\n    define_macros = [(\'ATLAS_INFO\', \'""\\\\""3.8.4\\\\""""\')]\r\n    libraries = [\'lapack\', \'ptf77blas\', \'ptcblas\', \'atlas\']\r\n    library_dirs = [\'/usr/lib/atlas-base/atlas\', \'/usr/lib/atlas-base\']\r\n    language = f77\r\n    include_dirs = [\'/usr/include/atlas\']\r\n\r\nRunning unit tests for numpy\r\nNumPy version 1.8.0.dev-e9e490a\r\nNumPy is installed in /home/warren/local_numpy/lib/python3.3/site-packages/numpy\r\nPython version 3.3.1 (default, Apr 13 2013, 13:42:07) [GCC 4.6.3]\r\nnose version 1.2.1\r\n/home/warren/local_py331/lib/python3.3/site-packages/nose-1.2.1-py3.3.egg/nose/core.py:247: ResourceWarning: unclosed file <_io.TextIOWrapper name=\'/home/warren/local_py331/lib/python3.3/site-packages/nose-1.2.1-py3.3.egg/nose/usage.txt\' mode=\'r\' encoding=\'UTF-8\'>\r\n  os.path.dirname(__file__), \'usage.txt\'), \'r\').read()\r\n.....................F......S......................................................................................................................................................S.....................................................................................................................................................S.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................K........................................E.E.................................................................................................................................................SSS................................................................................................K..............................................................................................................................................................................................................................................K........................................................................................................K......................K...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................SS..SS.....................................................................................................SSSSSSSS................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................K....................................................\r\n======================================================================\r\nERROR: test_numeric.TestIsclose.test_ip_isclose_allclose([1e-08, 1, 1000020.0000000099], [0, nan, 1000000.0])\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/warren/local_py331/lib/python3.3/site-packages/nose-1.2.1-py3.3.egg/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/core/tests/test_numeric.py"", line 1253, in tst_isclose_allclose\r\n    assert_array_equal(isclose(x, y).all(), allclose(x, y), msg % (x, y))\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/core/numeric.py"", line 2008, in allclose\r\n    return all(less_equal(abs(x-y), atol + rtol * abs(y)))\r\nRuntimeWarning: invalid value encountered in absolute\r\n\r\n======================================================================\r\nERROR: test_numeric.TestIsclose.test_ip_isclose_allclose(nan, [nan, nan, nan])\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/warren/local_py331/lib/python3.3/site-packages/nose-1.2.1-py3.3.egg/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/core/tests/test_numeric.py"", line 1253, in tst_isclose_allclose\r\n    assert_array_equal(isclose(x, y).all(), allclose(x, y), msg % (x, y))\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/core/numeric.py"", line 2008, in allclose\r\n    return all(less_equal(abs(x-y), atol + rtol * abs(y)))\r\nRuntimeWarning: invalid value encountered in absolute\r\n\r\n======================================================================\r\nFAIL: Test numpy dot with different order C, F\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/warren/local_py331/lib/python3.3/site-packages/nose-1.2.1-py3.3.egg/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/core/tests/test_blasdot.py"", line 114, in test_dot_array_order\r\n    assert_almost_equal(a.dot(a), a.T.dot(a.T).T, decimal=30)\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/testing/utils.py"", line 458, in assert_almost_equal\r\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/testing/utils.py"", line 819, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/home/warren/local_numpy/lib/python3.3/site-packages/numpy/testing/utils.py"", line 652, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 30 decimals\r\n\r\n(mismatch 10.0%)\r\n x: array([[ 0.60970883,  1.6909554 , -1.0885194 , -1.82058004, -3.95746616,\r\n         1.52435604, -0.59853062, -3.72278631,  3.82375941,  5.51367039],\r\n       [-3.58154905, -2.0623123 , -0.06567267,  1.47373436,  2.60687462,...\r\n y: array([[ 0.60970883,  1.6909554 , -1.0885194 , -1.82058004, -3.95746616,\r\n         1.52435604, -0.59853062, -3.72278631,  3.82375941,  5.51367039],\r\n       [-3.58154905, -2.0623123 , -0.06567267,  1.47373436,  2.60687462,...\r\n\r\n----------------------------------------------------------------------\r\nRan 5150 tests in 41.741s\r\n\r\nFAILED (KNOWNFAIL=6, SKIP=18, errors=2, failures=1)\r\n```'"
statsmodels/statsmodels#1011,17467375,josef-pkt,josef-pkt,2013-07-31 18:15:06,2013-07-31 21:37:06,2013-07-31 21:37:06,closed,,,0,comp-stats;type-bug,https://api.github.com/repos/statsmodels/statsmodels/issues/1011,b'power ttest endless loop possible',"b'running ""statsmodels\\statsmodels\\examples\\try_power2.py""\r\nI got an endless loop for solving for alpha, when initially evaluating ttest_power with alpha=-1 (which is not valid and should return nan)\r\n\r\nsee https://github.com/scipy/scipy/issues/2667'"
,,,,,,,,,,,,,,
scikit-image/scikit-image#1792,119646819,stsievert,jni,2015-12-01 05:47:39,2015-12-14 05:37:18,2015-12-14 05:37:18,closed,,,30,enhancement,https://api.github.com/repos/scikit-image/scikit-image/issues/1792,b'Uses fftconvolve instead of convolve2d for speedups',"b'This pull request uses [`scipy.signal.fftconvolve`] instead of [`scipy.signal.convolve2d`] in skimage/restoration/deconvolution.py. These are equivalent functions, as noted in [this StackOverflow answer] but `fftconvolve` is much faster. Quoting the documentation page for fftconvolve,\r\n\r\n> This is generally much faster than convolve for large arrays (n > ~500)\r\n\r\nThis function change gives equivalent results (up a to a machine epsilon) as indicated by the test below:\r\n\r\n[this StackOverflow answer]:http://stackoverflow.com/a/1768140/1141256\r\n\r\n```python\r\nfrom scipy.signal import convolve2d, fftconvolve\r\nimport numpy as np\r\n\r\nN, m = 1e3, 10\r\nnp.random.seed(42)\r\nx = np.random.randn(int(N), int(N))\r\nh = np.ones((m, m)) / m**2\r\n\r\ny1 = convolve2d(x, h, mode=\'same\')\r\ny2 = fftconvolve(x, h, mode=\'same\')\r\n\r\nassert np.allclose(y1, y2)\r\n\r\nprint(""{:0.4e}"".format(np.max(np.abs(y1 - y2)))) # prints 1.7876e-15\r\n```\r\n\r\nWhen I use `N = 1e3`, I see speedups of roughly 6x.\r\n\r\n[`scipy.signal.fftconvolve`]:http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.fftconvolve.html#scipy.signal.fftconvolve\r\n[`scipy.signal.convolve2d`]:http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html#scipy.signal.convolve2d'"
numpy/numpy#1858,7728914,thouis,,2012-10-19 20:54:53,2013-07-24 01:39:47,None,open,,,11,09 - Enhancement;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/1858,b'Use FFT in np.correlate/convolve? (Trac #1260)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/1260 on 2009-10-12 by trac user roger, assigned to unknown._\n\nThe `convolve` and `correlate` functions appear to be much slower than their MATLAB 2009a equivalents.\n\nThe MATLAB command `xcorr(randn(1e6,1))` takes about 0.35s to execute, while the Python equivalent `x = randn(1e6);correlate(x, x)` takes more than a minute (then killed). MATLAB is also much faster for arrays of 1e5 elements.\n\n`fftconvolve` in `scipy.signal` is even slower.\n\nTested with Python 2.6.3 and numpy 1.30 (x86) under x64 Windows 7 and x64 Ubuntu on an i7.\n'"
numpy/numpy#5954,86406841,bringingheavendown,rgommers,2015-06-09 01:57:43,2016-05-28 12:51:02,2016-05-28 12:51:02,closed,,,15,,https://api.github.com/repos/numpy/numpy/issues/5954,b'How to limit cross correlation window width in Numpy?',"b'I am learning numpy/scipy, coming from a MATLAB background. The xcorr function in Matlab has an optional argument ""maxlag"" that limits the lag range from \xa8Cmaxlag to maxlag. This is very useful if you are looking at the cross-correlation between two very long time series but are only interested in the correlation within a certain time range. The performance increases are enormous considering that cross-correlation is incredibly expensive to compute.\r\n\r\nWhat is troubling me is that numpy.correlate does not have a maxlag feature. This means that even if I only want to see correlations between two time series with lags between -100 and +100 ms, for example, it will still calculate the correlation for every lag between -20000 and +20000 ms (which is the length of the time series). This (theoretically) gives a 200x performance hit! Is it possible that I could contribute this feature?\r\n\r\nI have introduced this question as a [scipy issue](https://github.com/scipy/scipy/issues/4940) and on the [scipy-dev list](http://mail.scipy.org/pipermail/scipy-dev/2015-June/020757.html).  It seems the best place to start is with numpy.correlate, so that is what I am requesting.  I have done a [simple implementation](https://gist.github.com/bringingheavendown/b4ce18aa007118e4e084) which gives 50x speedup under [my conditions](https://github.com/scipy/scipy/issues/4940#issuecomment-110187847). \r\n\r\nThis is my first experience with contributing to open-source software, so any pointers are appreciated. \r\n\r\nOther issues related to correlate functions include [ENH: Fold fftconvolve into convolve/correlate functions as a parameter #2651](https://github.com/scipy/scipy/issues/2651), [Use FFT in np.correlate/convolve? (Trac #1260) #1858](https://github.com/numpy/numpy/issues/1858), and [normalized cross-correlation (Trac #1714) #2310](https://github.com/numpy/numpy/issues/2310).'"
numpy/numpy#5978,89121570,bringingheavendown,,2015-06-17 20:44:12,2016-06-21 15:28:16,None,open,,,93,09 - Enhancement;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5978,b'ENH: adding maxlag mode to convolve and correlate',"b'What was troubling me is that numpy.correlate does not have a maxlag feature. This means that even if I only want to see correlations between two time series with lags between -100 and +100 ms, for example, it will still calculate the correlation for every lag between -20000 and +20000 ms (which is the length of the time series). This (theoretically) gives a 200x performance hit! Is it possible that I could contribute this feature?\r\n\r\nI have introduced this question as a [numpy issue](https://github.com/numpy/numpy/issues/5954), a [scipy issue](https://github.com/scipy/scipy/issues/4940) and on the [scipy-dev list](http://mail.scipy.org/pipermail/scipy-dev/2015-June/020757.html).  It seems the best place to start is with numpy.correlate, so that is what I am requesting.  \r\n\r\nThis is my first experience with contributing to open-source software, so any pointers are appreciated. \r\n\r\nPrevious discussion of this functionality can be found at [another discussion on numpy correlate (and convolution)](http://numpy-discussion.10968.n7.nabble.com/another-discussion-on-numpy-correlate-and-convolution-td32925.html).  Other issues related to correlate functions include [ENH: Fold fftconvolve into convolve/correlate functions as a parameter #2651](https://github.com/scipy/scipy/issues/2651), [Use FFT in np.correlate/convolve? (Trac #1260) #1858](https://github.com/numpy/numpy/issues/1858), and [normalized cross-correlation (Trac #1714) #2310](https://github.com/numpy/numpy/issues/2310).\r\n\r\n\r\nI have added extra options for the ""mode"" parameter, namely an int or an int tuple that determine for which lags the correlation/convolution should be calculated.  These correspond to the values in the vector that the same tuple would generate if used as an argument to the numpy.arange() function.  \r\n\r\nHere are the specific issues that could use attention in this implementation:\r\n\r\nIs it ok that this implementation breaks the previous undocumented behavior of mode=0, 1, or 2 returning the correlation for mode=\'valid\', \'same\', or \'full\' respectively? If not, how should maxlag input be passed?\r\nShould the checks that are included in convolve() be added to correlate()?\r\nIs it ok that I used NPY_MAX_INT to represent requests for a default argument in the C code?\r\nDo I need to check for ""default arguments"" at all, given that the calling functions should now always call _pyarray_correlate with the appropriate number of arguments? I kind of feel there is some legacy behavior I should be maintaining, but I\'m not 100% sure.\r\nIs the error message in _pyarray_correlate ""PyErr_SetString(PyExc_ValueError, ""mode must be 0, 1, 2, or 3"")"" still valuable? What should it be replaced with?\r\nAre there other error checks I should add to the code at this or other points?\r\nCould you all clean up the unit tests/add some new ones to be absolutely sure that it deals properly with weird combinations of input array sizes and lag inputs? I couldn\'t figure out how to run all the same unit tests with the order of input arrays switched.'"
scikit-learn/scikit-learn#2148,16587176,NicolasTr,amueller,2013-07-10 16:33:32,2014-06-13 11:04:36,2013-07-25 13:58:50,closed,,0.14-rc,71,,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2148,b'[MRG] Missing values imputation',"b""Here's some code for the data imputation. \r\n\r\nIt's still a work in progress so *Travis* will probably be mad at me.\r\n\r\nI'll work on the documentation tomorrow. Meanwhile, you can look at the tests if you want to see how it works.\r\n\r\n***What's new***\r\nI added four classes:\r\n* MeanImputer: replace the missing values with the mean of the row/column\r\n* MedianImputer: replace the missing values with the median of the row/column\r\n* MostFrequentImputer: replace the missing values with the most frequent value in the row/column\r\n* RandomImputer: replace the missing values with one of the values of the row/column\r\n\r\n***Behaviour***\r\n* Any value can represent a missing value (not only np.nan for dense matrices and 0 for sparse matrices, *any* value)\r\n* Different missing values can be imputed separately in a pipeline\r\n* Support dense and sparse matrices\r\n* **To determine:** What should the imputer do if he has nothing to work with? For example: What should be done if there's no median value because the whole row/column is missing?\r\n   * 0 for the mean and np.nan for the median/most frequent/random? \r\n   * always np.nan?\r\n   * Let the user choose with a parameter?\r\n\r\n***What's missing***\r\n- [x] **Documentation**\r\n- [x] Delete the rows/columns where `np.isnan(imputation_data_)`\r\n- [x] Add tests with copy\r\n- [x] Add tests with grid search\r\n- [x] Test pickling\r\n\r\n"""
CellProfiler/CellProfiler#662,15356085,braymp,LeeKamentsky,2013-06-10 16:21:41,2013-06-17 16:55:50,2013-06-17 14:47:33,closed,,,3,Bug,https://api.github.com/repos/CellProfiler/CellProfiler/issues/662,b'Upgrade_settings error on ridiculously old pipelines',"b'(Via the forum)\n\nThe attached pipelines are pretty old (CP 1.0, but pre-9717) and fail upon loading with the following msg:\n<pre>\nTraceback (most recent call last):\n  File ""cellprofiler\\pipeline.pyc"", line 405, in create_from_handles\n  File ""cellprofiler\\pipeline.pyc"", line 435, in instantiate_module\n  File ""cellprofiler\\modules\\__init__.pyc"", line 387, in instantiate_module\nValueError: Could not find the \\ud7c0\\udc00\\u97c0\\udc00\\u17c0\\udc00\\ud7c0\\udc00\\\nu17c0\\udc00\\u17c0\\udc00\\u17c0\\udc00\\u97c0\\udc00\\u17c0\\udc00\\u97c0\\udc00 module\n  File ""cellprofiler\\gui\\pipelinecontroller.pyc"", line 226, in do_load_pipeline\n  File ""cellprofiler\\pipeline.pyc"", line 548, in load\n  File ""cellprofiler\\pipeline.pyc"", line 416, in create_from_handles\n</pre>\nI assume it may not be worth going back this far, but it does seem that there are super-old pipelines still floating out there. Perhaps they could just be pointed to the latest MATLAB version of CP1 to upgrade the pipelines, then over to CP2 to get up to date (which is what I did to get around this).\r\n\r\nAttachments:\r\nhttp://cellprofiler.org/issues/uploaded/phi_n7/53BP1_RedPIPE.mat\r\nhttp://cellprofiler.org/issues/uploaded/V192WB/ColocalizationPIPE.mat\r\nhttp://cellprofiler.org/issues/uploaded/iEdoID/Cutoff_53BP1_RedPIPE.mat\r\nhttp://cellprofiler.org/issues/uploaded/mGJPr8/Cutoff_RAD51_GreenPIPE.mat\r\nhttp://cellprofiler.org/issues/uploaded/zGeoPq/Cutoff2_53BP1_RedPIPE.mat\r\nhttp://cellprofiler.org/issues/uploaded/D4scYK/Cutoff2_RAD51_GreenPIPE.mat\r\nhttp://cellprofiler.org/issues/uploaded/EkWXtt/RAD51_GreenPIPE.mat'"
scikit-learn/scikit-learn#2431,19208185,joernhees,ogrisel,2013-09-09 16:47:28,2014-05-21 16:04:41,2014-05-13 07:18:00,closed,ogrisel,0.15,5,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2431,b'test never finishes: sklearn.decomposition.tests.test_sparse_pca.test_fit_transform',b'I recently installed scikit-learn with ```pip install scikit-learn``` (version 0.14.1).\r\nTrying to run the tests with ```nosetests --exe sklearn -v``` the above mentioned test never seems to finish. Excluding the test with ```-e test_fit_transform``` all other tests seem to pass.\r\nEnvironment is set-up according to http://joernhees.de/blog/2013/06/08/mac-os-x-10-8-scientific-python-with-homebrew/'
scikit-learn/scikit-learn#2472,19872538,chyikwei,ogrisel,2013-09-22 13:07:26,2014-07-29 09:36:16,2014-07-29 09:36:16,closed,,0.15.1,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2472,b'test_spectral_clustering_sparse fail on OS X 10.8.2',"b'I got this error on sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse\r\nAny one knows what might be wrong?\r\n\r\nOS: Max OS X 10.8.2 \r\nnumpy: 1.7.1\r\nscipy: 0.11.0\r\n\r\n<pre><code>\r\n======================================================================\r\nFAIL: sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/chyikwei/getglue-python-webservice/virtual/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/chyikwei/scikit-learn/sklearn/cluster/tests/test_spectral.py"", line 155, in test_spectral_clustering_sparse\r\n    assert_greater(np.mean(labels == [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]), .89)\r\nAssertionError: 0.59999999999999998 not greater than 0.89\r\n    """"""Fail immediately, with the given message.""""""\r\n>>  raise self.failureException(\'0.59999999999999998 not greater than 0.89\')\r\n</code></pre>'"
numpy/numpy#3340,14494657,rgommers,charris,2013-05-19 10:18:17,2014-08-09 14:14:19,2013-05-19 15:12:16,closed,,,1,,https://api.github.com/repos/numpy/numpy/issues/3340,"b'DOC: update numpy.distutils documentation, remove info.py details.'",b'Addresses https://github.com/scipy/scipy/issues/2492'
scikit-learn/scikit-learn#3602,41458885,gatapia,amueller,2014-08-29 00:01:06,2015-01-23 21:35:27,2015-01-23 21:35:27,closed,,0.16,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3602,"b""OneHotEncoder dtype being ignored when n_values='auto'""","b'Easiest to explain in code:\r\n\r\n    na = preprocessing.OneHotEncoder(dtype=np.float16).\\\r\n      fit_transform(np.array([[1, 2, 3]]))\r\n    self.assertEqual(np.float16, na.astype(np.float16).dtype)\r\n    self.assertEqual(np.float16, na.dtype)\r\n    # Fails - na.dtype is float32\r\n'"
,,,,,,,,,,,,,,
numpy/numpy#2995,11079468,asaluja,pv,2013-02-16 21:58:24,2013-04-25 17:54:23,2013-02-18 17:54:29,closed,,,12,,https://api.github.com/repos/numpy/numpy/issues/2995,b'Multiarray Double Free or Unmap Pointer error for huge datasets',"b'Hello NumPy team,\r\n\r\nI seem to have encountered some unusual behavior when running NumPy/SciPy on massive datasets.  Please see below for the error.  Also, this ticket/bug report has been cross-posted on stackoverflow.com (http://stackoverflow.com/questions/14906962/python-double-free-error-for-huge-datasets) and the SciPy developers list (http://projects.scipy.org/scipy/ticket/1846).  It was upon the suggestion of an SOer that I decided to post here and on the SciPy dev list.  \r\n\r\nI am running Linux x86-64-bit OpenSuSE 11.4, NumPy version 1.5.1, SciPy version 0.9.0, Python 2.7. \r\n\r\nI have a very simple script in Python, but for some reason I get the following error when running a large amount of data:\r\n\r\n```\r\n*** glibc detected *** python: double free or corruption (out): 0x00002af5a00cc010 ***\r\n```\r\n\r\nI am used to these errors coming up in C or C++, when one tries to free memory that has already been freed. However, by my understanding of Python (and especially the way I\'ve written the code), I really don\'t understand why this should happen.\r\n\r\nHere is the code:\r\n\r\n```\r\n#!/usr/bin/python -tt                                                                                                                                                                                                                         \r\n\r\nimport sys, commands, string\r\nimport numpy as np\r\nimport scipy.io as io\r\nfrom time import clock\r\n\r\nW = io.loadmat(sys.argv[1])[\'W\']\r\nsize = W.shape[0]\r\nnumlabels = int(sys.argv[2])\r\nQ = np.zeros((size, numlabels), dtype=np.double)\r\nP = np.zeros((size, numlabels), dtype=np.double)\r\nQ += 1.0 / Q.shape[1]\r\nnu = 0.001\r\nmu = 0.01\r\nstart = clock()\r\nmat = -nu + mu*(W*(np.log(Q)-1))\r\nend = clock()\r\nprint >> sys.stderr, ""Time taken to compute matrix: %.2f seconds""%(end-start)\r\n```\r\n\r\nOne may ask, why declare a P and a Q numpy array? I simply do that to reflect the actual conditions (as this code is simply a segment of what I actually do, where I need a P matrix and declare it beforehand).\r\n\r\nI have access to a 192GB machine, and so I tested this out on a very large SciPy sparse matrix (2.2 million by 2.2 million, but very sparse, that\'s not the issue). The main memory is taken up by the Q, P, and mat matrices, as they are all 2.2 million by 2000 dense matrices (size = 2.2 million, numlabels = 2000). The peak memory goes up to 131GB, which comfortably fits in memory. While the mat matrix is being computed, I get the glibc error, and my process automatically goes into the sleep (S) state, without deallocating the 131GB it has taken up.\r\n\r\nGiven the bizarre (for Python) error (I am not explicitly deallocating anything), and the fact that this works nicely for smaller matrix sizes (around 1.5 million by 2000), I am really not sure where to start to debug this.\r\n\r\nAs a starting point, I have set ""ulimit -s unlimited"" before running, but to no avail.\r\n\r\nAny help or insight into numpy\'s behavior with really large amounts of data would be welcome.\r\n\r\nNote that this is NOT an out of memory error - I have 196GB, and my process reaches around 131GB and stays there for some time before giving the error above. \r\n\r\nAs per suggestions, I ran Python with GDB. Interestingly, on one GDB run I forgot to set the stack size limit to ""unlimited"", and got the following output:\r\n\r\n```\r\n*** glibc detected *** /usr/bin/python: munmap_chunk(): invalid pointer: 0x00007fe7508a9010 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x733b6)[0x7ffff6ec23b6]\r\n/usr/lib64/python2.7/site-packages/numpy/core/multiarray.so(+0x4a496)[0x7ffff69fc496]\r\n/usr/lib64/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x4e67)[0x7ffff7af48c7]\r\n/usr/lib64/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x309)[0x7ffff7af6c49]\r\n/usr/lib64/libpython2.7.so.1.0(PyEval_EvalCode+0x32)[0x7ffff7b25592]\r\n/usr/lib64/libpython2.7.so.1.0(+0xfcc61)[0x7ffff7b33c61]\r\n/usr/lib64/libpython2.7.so.1.0(PyRun_FileExFlags+0x84)[0x7ffff7b34074]\r\n/usr/lib64/libpython2.7.so.1.0(PyRun_SimpleFileExFlags+0x189)[0x7ffff7b347c9]\r\n/usr/lib64/libpython2.7.so.1.0(Py_Main+0x36c)[0x7ffff7b3e1bc]\r\n/lib64/libc.so.6(__libc_start_main+0xfd)[0x7ffff6e6dbfd]\r\n/usr/bin/python[0x4006e9]\r\n======= Memory map: ========\r\n00400000-00401000 r-xp 00000000 09:01 50336181                           /usr/bin/python2.7\r\n00600000-00601000 r--p 00000000 09:01 50336181                           /usr/bin/python2.7\r\n00601000-00602000 rw-p 00001000 09:01 50336181                           /usr/bin/python2.7\r\n00602000-00e5f000 rw-p 00000000 00:00 0                                  [heap]\r\n7fdf2584c000-7ffff0a66000 rw-p 00000000 00:00 0 \r\n7ffff0a66000-7ffff0a6b000 r-xp 00000000 09:01 50333916                   /usr/lib64/python2.7/lib-dynload/mmap.so\r\n7ffff0a6b000-7ffff0c6a000 ---p 00005000 09:01 50333916                   /usr/lib64/python2.7/lib-dynload/mmap.so\r\n7ffff0c6a000-7ffff0c6b000 r--p 00004000 09:01 50333916                   /usr/lib64/python2.7/lib-dynload/mmap.so\r\n7ffff0c6b000-7ffff0c6c000 rw-p 00005000 09:01 50333916                   /usr/lib64/python2.7/lib-dynload/mmap.so\r\n7ffff0c6c000-7ffff0c77000 r-xp 00000000 00:12 54138483                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/streams.so\r\n7ffff0c77000-7ffff0e76000 ---p 0000b000 00:12 54138483                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/streams.so\r\n7ffff0e76000-7ffff0e77000 r--p 0000a000 00:12 54138483                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/streams.so\r\n7ffff0e77000-7ffff0e78000 rw-p 0000b000 00:12 54138483                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/streams.so\r\n7ffff0e78000-7ffff0e79000 rw-p 00000000 00:00 0 \r\n7ffff0e79000-7ffff0e9b000 r-xp 00000000 00:12 54138481                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\r\n7ffff0e9b000-7ffff109a000 ---p 00022000 00:12 54138481                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\r\n7ffff109a000-7ffff109b000 r--p 00021000 00:12 54138481                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\r\n7ffff109b000-7ffff109f000 rw-p 00022000 00:12 54138481                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\r\n7ffff109f000-7ffff10a0000 rw-p 00000000 00:00 0 \r\n7ffff10a0000-7ffff10a5000 r-xp 00000000 09:01 50333895                   /usr/lib64/python2.7/lib-dynload/zlib.so\r\n7ffff10a5000-7ffff12a4000 ---p 00005000 09:01 50333895                   /usr/lib64/python2.7/lib-dynload/zlib.so\r\n7ffff12a4000-7ffff12a5000 r--p 00004000 09:01 50333895                   /usr/lib64/python2.7/lib-dynload/zlib.so\r\n7ffff12a5000-7ffff12a7000 rw-p 00005000 09:01 50333895                   /usr/lib64/python2.7/lib-dynload/zlib.so\r\n7ffff12a7000-7ffff12ad000 r-xp 00000000 00:12 54138491                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio_utils.so\r\n7ffff12ad000-7ffff14ac000 ---p 00006000 00:12 54138491                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio_utils.so\r\n7ffff14ac000-7ffff14ad000 r--p 00005000 00:12 54138491                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio_utils.so\r\n7ffff14ad000-7ffff14ae000 rw-p 00006000 00:12 54138491                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/io/matlab/mio_utils.so\r\n7ffff14ae000-7ffff14b5000 r-xp 00000000 00:12 54138562                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_csgraph.so\r\n7ffff14b5000-7ffff16b4000 ---p 00007000 00:12 54138562                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_csgraph.so\r\n7ffff16b4000-7ffff16b5000 r--p 00006000 00:12 54138562                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_csgraph.so\r\n7ffff16b5000-7ffff16b6000 rw-p 00007000 00:12 54138562                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_csgraph.so\r\n7ffff16b6000-7ffff17c2000 r-xp 00000000 00:12 54138558                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_bsr.so\r\n7ffff17c2000-7ffff19c2000 ---p 0010c000 00:12 54138558                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_bsr.so\r\n7ffff19c2000-7ffff19c3000 r--p 0010c000 00:12 54138558                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_bsr.so\r\n7ffff19c3000-7ffff19c6000 rw-p 0010d000 00:12 54138558                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_bsr.so\r\n7ffff19c6000-7ffff19d5000 r-xp 00000000 00:12 54138561                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_dia.so\r\n7ffff19d5000-7ffff1bd4000 ---p 0000f000 00:12 54138561                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_dia.so\r\n7ffff1bd4000-7ffff1bd5000 r--p 0000e000 00:12 54138561                   /home/avneesh/.local/lib/python2.7/site-packages/scipy/sparse/sparsetools/_dia.so\r\nProgram received signal SIGABRT, Aborted.\r\n0x00007ffff6e81ab5 in raise () from /lib64/libc.so.6\r\n(gdb) bt\r\n#0  0x00007ffff6e81ab5 in raise () from /lib64/libc.so.6\r\n#1  0x00007ffff6e82fb6 in abort () from /lib64/libc.so.6\r\n#2  0x00007ffff6ebcdd3 in __libc_message () from /lib64/libc.so.6\r\n#3  0x00007ffff6ec23b6 in malloc_printerr () from /lib64/libc.so.6\r\n#4  0x00007ffff69fc496 in ?? () from /usr/lib64/python2.7/site-packages/numpy/core/multiarray.so\r\n#5  0x00007ffff7af48c7 in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\r\n#6  0x00007ffff7af6c49 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\r\n#7  0x00007ffff7b25592 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\r\n#8  0x00007ffff7b33c61 in ?? () from /usr/lib64/libpython2.7.so.1.0\r\n#9  0x00007ffff7b34074 in PyRun_FileExFlags () from /usr/lib64/libpython2.7.so.1.0\r\n#10 0x00007ffff7b347c9 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython2.7.so.1.0\r\n#11 0x00007ffff7b3e1bc in Py_Main () from /usr/lib64/libpython2.7.so.1.0\r\n#12 0x00007ffff6e6dbfd in __libc_start_main () from /lib64/libc.so.6\r\n#13 0x00000000004006e9 in _start ()\r\n```\r\n\r\nWhen I set the stack size limit to unlimited, I get the following:\r\n\r\n```\r\n*** glibc detected *** /usr/bin/python: double free or corruption (out): 0x00002abb2732c010 ***\r\n^X^C\r\nProgram received signal SIGINT, Interrupt.\r\n0x00002aaaab9d08fe in __lll_lock_wait_private () from /lib64/libc.so.6\r\n(gdb) bt\r\n#0  0x00002aaaab9d08fe in __lll_lock_wait_private () from /lib64/libc.so.6\r\n#1  0x00002aaaab969f2e in _L_lock_9927 () from /lib64/libc.so.6\r\n#2  0x00002aaaab9682d1 in free () from /lib64/libc.so.6\r\n#3  0x00002aaaaaabbfe2 in _dl_scope_free () from /lib64/ld-linux-x86-64.so.2\r\n#4  0x00002aaaaaab70a4 in _dl_map_object_deps () from /lib64/ld-linux-x86-64.so.2\r\n#5  0x00002aaaaaabcaa0 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2\r\n#6  0x00002aaaaaab85f6 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#7  0x00002aaaaaabc5da in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\n#8  0x00002aaaab9fb530 in do_dlopen () from /lib64/libc.so.6\r\n#9  0x00002aaaaaab85f6 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#10 0x00002aaaab9fb5cf in dlerror_run () from /lib64/libc.so.6\r\n#11 0x00002aaaab9fb637 in __libc_dlopen_mode () from /lib64/libc.so.6\r\n#12 0x00002aaaab9d60c5 in init () from /lib64/libc.so.6\r\n#13 0x00002aaaab080933 in pthread_once () from /lib64/libpthread.so.0\r\n#14 0x00002aaaab9d61bc in backtrace () from /lib64/libc.so.6\r\n#15 0x00002aaaab95dde7 in __libc_message () from /lib64/libc.so.6\r\n#16 0x00002aaaab9633b6 in malloc_printerr () from /lib64/libc.so.6\r\n#17 0x00002aaaab9682dc in free () from /lib64/libc.so.6\r\n#18 0x00002aaaabef1496 in ?? () from /usr/lib64/python2.7/site-packages/numpy/core/multiarray.so\r\n#19 0x00002aaaaad888c7 in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\r\n#20 0x00002aaaaad8ac49 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\r\n#21 0x00002aaaaadb9592 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\r\n#22 0x00002aaaaadc7c61 in ?? () from /usr/lib64/libpython2.7.so.1.0\r\n#23 0x00002aaaaadc8074 in PyRun_FileExFlags () from /usr/lib64/libpython2.7.so.1.0\r\n#24 0x00002aaaaadc87c9 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython2.7.so.1.0\r\n#25 0x00002aaaaadd21bc in Py_Main () from /usr/lib64/libpython2.7.so.1.0\r\n#26 0x00002aaaab90ebfd in __libc_start_main () from /lib64/libc.so.6\r\n#27 0x00000000004006e9 in _start ()\r\n```\r\n\r\nThis makes me believe the basic issue is with the numpy multiarray core module (line #4 in the first output and line #18 in the second). I will bring it up as a bug report in both numpy and scipy just in case.\r\n\r\nHas anyone seen this before?\r\n'"
numpy/numpy#2956,10534888,pv,seberg,2013-02-01 10:17:20,2013-04-25 17:53:53,2013-04-03 21:41:45,closed,,,8,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/2956,b'Spurious C contiguity issues on git master',"b""This Cython code fails on Numpy git master (4600b2fe1d):\r\n```\r\nimport numpy as np\r\ndef foo(y):\r\n    cdef double[:,::1] data\r\n    y = y.reshape(y.shape[0], -1).T\r\n    y = np.ascontiguousarray(y, dtype=np.double)\r\n    data = y   # <- raises ValueError: Buffer not C contiguous.\r\n```\r\ncalled with\r\n```\r\nimport numpy as np\r\nimport asd\r\nasd.foo(np.zeros([90])[:,None])\r\n```\r\nascontiguousarray should always return C contiguous arrays with correct flags, but that doesn't seem to be the case currently. Doesn't fail on 1.7.0rc1 and 1.7.x branch.\r\n\r\nObserved in Scipy tests with Numpy master:\r\n\r\nhttp://projects.scipy.org/scipy/ticket/1832\r\nhttp://projects.scipy.org/scipy/ticket/1830"""
numpy/numpy#2956,10534888,pv,seberg,2013-02-01 10:17:20,2013-04-25 17:53:53,2013-04-03 21:41:45,closed,,,8,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/2956,b'Spurious C contiguity issues on git master',"b""This Cython code fails on Numpy git master (4600b2fe1d):\r\n```\r\nimport numpy as np\r\ndef foo(y):\r\n    cdef double[:,::1] data\r\n    y = y.reshape(y.shape[0], -1).T\r\n    y = np.ascontiguousarray(y, dtype=np.double)\r\n    data = y   # <- raises ValueError: Buffer not C contiguous.\r\n```\r\ncalled with\r\n```\r\nimport numpy as np\r\nimport asd\r\nasd.foo(np.zeros([90])[:,None])\r\n```\r\nascontiguousarray should always return C contiguous arrays with correct flags, but that doesn't seem to be the case currently. Doesn't fail on 1.7.0rc1 and 1.7.x branch.\r\n\r\nObserved in Scipy tests with Numpy master:\r\n\r\nhttp://projects.scipy.org/scipy/ticket/1832\r\nhttp://projects.scipy.org/scipy/ticket/1830"""
ksahlin/BESST#22,111349080,Tetrajf,ksahlin,2015-10-14 08:33:07,2015-10-28 22:18:18,2015-10-28 22:18:18,closed,,,2,,https://api.github.com/repos/ksahlin/BESST/issues/22,b'Install error',"b'Hi There\r\n\r\nI\'ve been struggling for a while with install errors. It seems runBESST is installed. I previously got it to work on my mac laptop but I needed more memory so I\'ve tried to install it on a cluster for which I don\'t have root access. I\'ve have a virtualenv for python 2.7 and have installed LAPACK, BLAS and ATLAS separately (added the path to my bash profile). I installed BESST using pip. There were many warnings during install but no errors and I can call the command.  \r\n\r\nI get the following error upon running BESST:\r\n\r\nTraceback (most recent call last):\r\n  File ""/scratch/sysusers/JF/virtualpython27/py2.7/bin/runBESST"", line 395, in <module>\r\n    main(args)\r\n  File ""/scratch/sysusers/JF/virtualpython27/py2.7/bin/runBESST"", line 73, in main\r\n    from BESST import CreateGraph as CG\r\n  File ""/scratch/sysusers/JF/virtualpython27/py2.7/lib/python2.7/site-packages/BESST/CreateGraph.py"", line 28, in <module>\r\n    from scipy.stats import ks_2samp\r\n  File ""/scratch/sysusers/JF/virtualpython27/py2.7/lib/python2.7/site-packages/scipy/stats/__init__.py"", line 321, in <module>\r\n    from .stats import *\r\n  File ""/scratch/sysusers/JF/virtualpython27/py2.7/lib/python2.7/site-packages/scipy/stats/stats.py"", line 180, in <module>\r\n    import scipy.special as special\r\n  File ""/scratch/sysusers/JF/virtualpython27/py2.7/lib/python2.7/site-packages/scipy/special/__init__.py"", line 601, in <module>\r\n    from ._ufuncs import *\r\nImportError: /scratch/sysusers/JF/virtualpython27/py2.7/lib/python2.7/site-packages/scipy/special/_ufuncs.so: undefined symbol: dstevr_\r\n\r\nI\'d appreciate any help you can provide. It seems from the net that this issue has got to do with my scipy compilation. I\'m not sure if the order of which I installed LAPACK, ATLAS and BLAS mattered?\r\n\r\nRegards\r\nJ\r\n\r\n'"
numpy/numpy#2939,10176575,juliantaylor,pv,2013-01-22 00:08:04,2013-04-25 17:53:28,2013-01-22 11:50:11,closed,,,9,,https://api.github.com/repos/numpy/numpy/issues/2939,b'2738 fix breaks scipy test_decomp.py in 1.7.0rc1',"b'edit: the issue is actually hash randomization and a bug in scipy, see below\r\n\r\nthe fix for gh-2738 (commit e208de6adfdd38aabe0b5a87ba22f80e04a014ad) in numpy 1.7.0rc1 breaks a scipy 0.11 and current git head test with python3.3.\r\nThis was previously reported to scipy in http://mail.scipy.org/pipermail/scipy-dev/2012-September/017995.html but I did not find a reaction.\r\nBisecting numpy lead to above commit.\r\n\r\nThe crash can be reproduced on ubuntu 13.04 with python3.3.\r\nIt occurs with the debug and regular python3.3 variant.\r\nIt manifests itself with random segfaults, glibc memory corruption aborts and hangs when you execute scipy/linalg/tests/test_decomp.py\r\nreverting the commit from the rc1 tag fixes the crash.\r\nnumpy git head does not seem affected.'"
Theano/Theano#1150,9723651,delallea,nouiz,2013-01-07 02:36:30,2014-06-13 17:20:49,2013-01-08 19:41:27,closed,,,2,,https://api.github.com/repos/Theano/Theano/issues/1150,b'Fix for new name of scipy.linalg.blas.fblas',b'Also cleaned up duplicated / useless code at the same time.\r\nThis fixes gh-1144.'
numpy/numpy#3431,15483946,WarrenWeckesser,seberg,2013-06-13 01:34:32,2014-06-15 01:18:32,2013-06-13 11:38:26,closed,,,6,,https://api.github.com/repos/numpy/numpy/issues/3431,b'ENH: random: Allow ngood=0 or nbad=0 in mtrand.hypergeometric.',"b'The hypergeometric method of mtrand can handle the edge cases ngood=0 or nbad=0, so might as well allow it.\r\n'"
numpy/numpy#2750,8444293,cgohlke,certik,2012-11-17 20:12:00,2013-04-25 17:51:31,2012-12-21 01:03:00,closed,,NumPy 1.7,6,11 - Bug;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2750,b'test_unicode fails on Python 3.3',"b'Test_unicode fails with numpy-1.7.0rc1.dev-win*-py3.3 (3a52aa06c5), built with Visual Studio 2010 and Intel MKL 11. All other tests pass.\n\n```\n======================================================================\nFAIL: Check assignment of 0-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 209, in test_values0D\n    self.content_check(ua, ua[()], 4*self.ulen)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of multi-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 223, in test_valuesMD\n    self.content_check(ua, ua[0,0,0], 4*self.ulen*2*3*4)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of single-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 215, in test_valuesSD\n    self.content_check(ua, ua[0], 4*self.ulen*2)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of 0-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 209, in test_values0D\n    self.content_check(ua, ua[()], 4*self.ulen)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of multi-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 223, in test_valuesMD\n    self.content_check(ua, ua[0,0,0], 4*self.ulen*2*3*4)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of single-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 215, in test_valuesSD\n    self.content_check(ua, ua[0], 4*self.ulen*2)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of 0-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 209, in test_values0D\n    self.content_check(ua, ua[()], 4*self.ulen)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of multi-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 223, in test_valuesMD\n    self.content_check(ua, ua[0,0,0], 4*self.ulen*2*3*4)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check assignment of single-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 215, in test_valuesSD\n    self.content_check(ua, ua[0], 4*self.ulen*2)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 199, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of 0-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 123, in test_values0D\n    self.content_check(ua, ua[()], 4*self.ulen)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of multi-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 134, in test_valuesMD\n    self.content_check(ua, ua[0,0,0], 4*self.ulen*2*3*4)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of single-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 128, in test_valuesSD\n    self.content_check(ua, ua[0], 4*self.ulen*2)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of 0-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 123, in test_values0D\n    self.content_check(ua, ua[()], 4*self.ulen)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of multi-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 134, in test_valuesMD\n    self.content_check(ua, ua[0,0,0], 4*self.ulen*2*3*4)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of single-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 128, in test_valuesSD\n    self.content_check(ua, ua[0], 4*self.ulen*2)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of 0-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 123, in test_values0D\n    self.content_check(ua, ua[()], 4*self.ulen)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of multi-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 134, in test_valuesMD\n    self.content_check(ua, ua[0,0,0], 4*self.ulen*2*3*4)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n\n======================================================================\nFAIL: Check creation of single-dimensional objects with values\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 128, in test_valuesSD\n    self.content_check(ua, ua[0], 4*self.ulen*2)\n  File ""X:\\Python33\\lib\\site-packages\\numpy\\core\\tests\\test_unicode.py"", line 114, in content_check\n    self.assertTrue(buffer_length(ua_scalar) == 2*2*self.ulen)\nAssertionError: False is not true\n```'"
wslihgt/separateLeadStereo#1,6141134,devops85,wslihgt,2012-08-10 00:35:01,2013-04-25 17:47:53,2013-01-17 10:42:05,closed,wslihgt,,7,,https://api.github.com/repos/wslihgt/separateLeadStereo/issues/1,b'Problem with separateLeadStereoParam.py',"b'Hi Team,\nI am using python 2.7 and I can not execute your script. When I put:\npython separateLeadStereoParam.py file.wav\n\nThe following message appears:\nTypeError: main() takes exactly 2 arguments (0 given)\n\ncould you please help me?\n\nThanks in advance'"
numpy/numpy#307,5012176,pv,charris,2012-06-11 21:06:02,2014-06-13 23:34:24,2012-07-14 15:53:22,closed,,,11,,https://api.github.com/repos/numpy/numpy/issues/307,"b""BUG: don't pollute public namespace with customized PyIndex_Check on Py2.4""","b'Move compatibility #define for PyIndex_Check on Python 2.4 to a private compat header\n\nThe problem is that including the public Numpy header ""arrayobject.h"" defines a compatibility wrapper for ""PyIndex_Check"" on Python 2.4. Moreover, this compatibility wrapper functions incorrectly... This #define probably should not be in the public namespace, as it already causes problems with Cython on Py2.4 (which we hit into in Scipy release):\nhttp://permalink.gmane.org/gmane.comp.python.cython.devel/13983\n\nThe second commit moves all non Py3-stuff out of npy_3compat.h to another, more private compat header, for stylistic reasons (mainly because #defining `PyIndex_Check` to `0` rather than something like `PyNumber_Check` is possibly not the correct solution, but I didn\'t want to go changing it inside Numpy for this).\n\nA question with this PR is --- is there 3rd party software out there that relies on Numpy headers defining the otherwise missing PyIndex_Check being defined, and do we care. I\'d say we don\'t care, this seems like a corner case, and the current behavior in Numpy is wrong.'"
matplotlib/matplotlib#385,1140717,pv,pelson,2011-06-29 21:24:32,2014-07-06 00:33:16,2012-09-04 21:52:30,closed,,,8,,https://api.github.com/repos/matplotlib/matplotlib/issues/385,b'BUG: plot_directive: look for plot script files relative to the .rst file',"b'BUG: plot_directive: look for plot script files relative to the .rst file directory (rather than doc source root), when plot_basedir is not given\r\n\r\nA tiny fix to the plot directive -- it should look for the plot scripts relative to the current .rst file and not the source root specified on the command line.'"
pymc-devs/pymc3#334,19167769,kforeman,jsalvatier,2013-09-08 21:44:09,2013-12-23 19:25:42,2013-12-23 19:25:42,closed,jsalvatier,Version 3.0.beta,18,defects,https://api.github.com/repos/pymc-devs/pymc3/issues/334,"b'[pymc3] Gamma(1,1) MAP issue'","b""I've found that when I run find_MAP() on a Gamma(1,1) distribution it ends up with negative value (and thus -Inf logp).\r\n\r\nHere's an illustration of the issue: http://nbviewer.ipython.org/6488376\r\n\r\nIf I change it to Gamma(10,1) it works fine..."""
pydata/pandas#972,3826735,isofer,wesm,2012-03-27 13:02:19,2013-04-25 17:45:41,2012-03-27 13:28:07,closed,,,5,,https://api.github.com/repos/pydata/pandas/issues/972,b'scoreateprecentile return wrong value',"b""scoreatprecentile of a series returns the wrong value\r\n\r\n```\r\nIn [1341]: a = np.random.rand(100)\r\n\r\nIn [1342]: b = pd.Series(a)\r\n\r\nIn [1343]: a[:10]\r\n\r\nOut[1343]: \r\narray([ 0.6131142 ,  0.65266141,  0.24583156,  0.70179786,  0.33361506,\r\n        0.65042728,  0.70192276,  0.02727854,  0.65948894,  0.44326182])\r\n\r\nIn [1348]: scoreatpercentile(a,1) \r\nOut[1348]: 0.010388922650144839 #correct value\r\n\r\nIn [1349]: scoreatpercentile(b,1) \r\nOut[1349]: 0.65226593993834392 #incorrect value\r\n\r\nIn [1350]: scoreatpercentile(a,2)\r\nOut[1350]: 0.011971896338709577 #correct value\r\n\r\nIn [1351]: scoreatpercentile(b,2)\r\nOut[1351]: 0.25396815348880808 #incorrect value\r\n\r\n\r\n\r\n```\r\nI'm not sure if this is a pandas issue or scipy issue, and I am aware of the quantile method, but I still wonder if it is possible to fix that."""
PyMVPA/PyMVPA#157,24850258,yarikoptic,hanke,2013-12-28 19:29:25,2014-02-20 09:08:12,2014-02-20 09:08:12,closed,hanke,Release 2.3,3,Test failure,https://api.github.com/repos/PyMVPA/PyMVPA/issues/157,"b'iirfilter fails on Debian wheezy due to broken filtfilt() in scipy (fixed upstream, but not present in Debian stable)'","b'http://nipy.bic.berkeley.edu/builders/pymvpa-py2.x-wheezy-sparc/builds/46/steps/shell_3/logs/stdio\r\n\r\n```\r\n\r\n======================================================================\r\nERROR: mvpa2.tests.test_filters.test_iirfilter\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pymvpa-py2_x-wheezy-sparc/build/venv/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pymvpa-py2_x-wheezy-sparc/build/venv/local/lib/python2.7/site-packages/mvpa2/tests/test_filters.py"", line 72, in test_iirfilter\r\n    mds = iir_filter(ds, b, a, padlen=150)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pymvpa-py2_x-wheezy-sparc/build/venv/local/lib/python2.7/site-packages/mvpa2/mappers/filters.py"", line 274, in iir_filter\r\n    return dm.forward(ds)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pymvpa-py2_x-wheezy-sparc/build/venv/local/lib/python2.7/site-packages/mvpa2/mappers/base.py"", line 187, in forward\r\n    return self._forward_dataset(data)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pymvpa-py2_x-wheezy-sparc/build/venv/local/lib/python2.7/site-packages/mvpa2/mappers/base.py"", line 115, in _forward_dataset\r\n    msamples = self._forward_data(dataset.samples)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pymvpa-py2_x-wheezy-sparc/build/venv/local/lib/python2.7/site-packages/mvpa2/mappers/filters.py"", line 239, in _forward_data\r\n    padlen=params.padlen)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/signal/signaltools.py"", line 1533, in filtfilt\r\n    (y, zf) = lfilter(b, a, ext, zi=zi * x0)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/signal/signaltools.py"", line 581, in lfilter\r\n    return sigtools._linear_filter(b, a, x, axis, zi)\r\nValueError: The number of initial conditions must be max([len(a),len(b)]) - 1\r\n```\r\n'"
pyhrf/pyhrf#146,129808805,thperret,thperret,2016-01-29 16:00:15,2016-03-02 16:55:43,2016-03-02 16:55:43,closed,thperret,0.4.2,1,bug;investigate;packaging,https://api.github.com/repos/pyhrf/pyhrf/issues/146,b'[deps] unittests errors with PIL (Pillow)',"b'At first look, it\'s seems to comes from Pillow dependency but there\'s investigations to do.\r\n\r\n```\r\n======================================================================\r\n\r\nERROR: test_simulate_asl_full_physio_outputs (pyhrf.test.test_sandbox_physio.SimulationTest)\r\n\r\n----------------------------------------------------------------------\r\n\r\nTraceback (most recent call last):\r\n\r\n  File ""/home/travis/build/thperret/pyhrf/python/pyhrf/test/test_sandbox_physio.py"", line 54, in test_simulate_asl_full_physio_outputs\r\n\r\n    phy.simulate_asl_full_physio(self.tmp_path)\r\n\r\n  File ""/home/travis/build/thperret/pyhrf/python/pyhrf/sandbox/physio.py"", line 415, in simulate_asl_full_physio\r\n\r\n    simbase.simulation_save_vol_outputs(simulation, output_dir)\r\n\r\n  File ""/home/travis/build/thperret/pyhrf/python/pyhrf/boldsynth/scenarios.py"", line 857, in simulation_save_vol_outputs\r\n\r\n    axes_domains={\'time\': np.arange(brfs_vol.shape[0]) * dt})\r\n\r\n  File ""/home/travis/build/thperret/pyhrf/python/pyhrf/ndarray.py"", line 90, in __init__\r\n\r\n    % (len(axes_names), nbDims, str(axes_names)))\r\n\r\nException: length of axes_names (4) is different from nb of dimensions (5).\r\n\r\nGot axes names: [\'time\', \'sagittal\', \'coronal\', \'axial\']\r\n\r\n-------------------- >> begin captured logging << --------------------\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\n\r\nFAIL: test_simulate_asl_full_physio (pyhrf.test.test_sandbox_physio.SimulationTest)\r\n\r\n----------------------------------------------------------------------\r\n\r\nTraceback (most recent call last):\r\n\r\n  File ""/home/travis/build/thperret/pyhrf/python/pyhrf/test/test_sandbox_physio.py"", line 47, in test_simulate_asl_full_physio\r\n\r\n    self.assertEqual(r[\'labels_vol\'].shape, (3, 1, 2, 2))\r\n\r\nAssertionError: Tuples differ: (3, 1, 2, 2, 3) != (3, 1, 2, 2)\r\n\r\nFirst tuple contains 1 additional elements.\r\n\r\nFirst extra element 4:\r\n\r\n3\r\n\r\n- (3, 1, 2, 2, 3)\r\n\r\n?            ---\r\n\r\n+ (3, 1, 2, 2)\r\n\r\n-------------------- >> begin captured logging << --------------------\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\n\r\nFAIL: test_simulate_asl_physio_rfs (pyhrf.test.test_sandbox_physio.SimulationTest)\r\n\r\n----------------------------------------------------------------------\r\n\r\nTraceback (most recent call last):\r\n\r\n  File ""/home/travis/build/thperret/pyhrf/python/pyhrf/test/test_sandbox_physio.py"", line 73, in test_simulate_asl_physio_rfs\r\n\r\n    self.assertEqual(r[\'labels_vol\'].shape, (3, 1, 2, 2))\r\n\r\nAssertionError: Tuples differ: (3, 1, 2, 2, 3) != (3, 1, 2, 2)\r\n\r\nFirst tuple contains 1 additional elements.\r\n\r\nFirst extra element 4:\r\n\r\n3\r\n\r\n- (3, 1, 2, 2, 3)\r\n\r\n?            ---\r\n\r\n+ (3, 1, 2, 2)\r\n\r\n-------------------- >> begin captured logging << --------------------\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IHDR 16 13\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM sRGB 41 1\r\n\r\nPIL.PngImagePlugin: DEBUG: sRGB 41 1 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM PLTE 54 96\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM pHYs 162 9\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tIME 183 7\r\n\r\nPIL.PngImagePlugin: DEBUG: tIME 183 7 (unknown)\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM tEXt 202 25\r\n\r\nPIL.PngImagePlugin: DEBUG: STREAM IDAT 239 17\r\n\r\n--------------------- >> end captured logging << ---------------------\r\n```'"
numpy/numpy#5861,74946730,LimEJET,LimEJET,2015-05-10 16:02:19,2015-05-12 00:08:20,2015-05-11 22:34:58,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/5861,b'creating an array from a Pillow 1-bit image object results in junk data.',"b'Python 3.4.2 Windows 64 bit\r\nNumPy 1.9.2\r\npillow  2.8.1\r\n\r\n\r\nWhen `numpy.array` is called on an 1-bit Pillow image, the resulting array is filled with junk data and does not correctly represent the image object it should contain.\r\n\r\nI\'ve tested this with a small image and a large image.\r\n\r\nSmall image (3x3):\r\n![test](https://cloud.githubusercontent.com/assets/4407285/7554999/853ef4c2-f73d-11e4-9747-2e9c79190344.png)\r\n\r\nPillow represents it like this:\r\n```python\r\n>>> i = Image.open(""test.png"")\r\n>>> for k in i.convert(""1"").getdata(): print(k)\r\n0\r\n255\r\n0\r\n255\r\n0\r\n255\r\n0\r\n255\r\n0\r\n```\r\nwhile the resulting numpy array varies in how it looks:\r\n```python\r\n>>> p = np.array(i.convert(""1""))\r\n>>> p\r\narray([[ True,  True,  True],\r\n       [False,  True,  True],\r\n       [ True,  True,  True]], dtype=bool)\r\n>>> i = Image.open(""test.png"")\r\n>>> p = np.array(i.convert(""1""))\r\n>>> p\r\narray([[ True,  True,  True],\r\n       [False, False, False],\r\n       [False, False,  True]], dtype=bool)\r\n```\r\n\r\nI tried this out on a bigger image (represented below), and I saw some interesting patterns in the output.\r\n\r\nThe code I tested with:\r\n```python\r\ni = Image.open(""a.png"")\r\ni.show()\r\np = numpy.array(i.convert(""1"")) # gives boolean array\r\nj = Image.fromarray(numpy.uint8(p)*255)\r\nj.show()\r\n```\r\nInput image:\r\n![2015-05-10_17-50-18](https://cloud.githubusercontent.com/assets/4407285/7554994/1031077e-f73d-11e4-9923-f07bf9117c4e.png)\r\n\r\nOutput image:\r\n![2015-05-10_17-48-37](https://cloud.githubusercontent.com/assets/4407285/7554995/121907da-f73d-11e4-8e1f-f574fba57567.png)\r\n\r\nNotice the row of miniaturized versions of the input along the top.'"
numpy/numpy#2561,7731651,numpy-gitbot,charris,2012-10-19 22:31:44,2013-10-12 19:55:05,2013-10-10 21:14:55,closed,,,1,11 - Bug;component: numpy.lib;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2561,b'PY3K bug in numpy.info (numpy/lib/utils.py) (Trac #1968)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/1968 on 2011-10-24 by @WarrenWeckesser, assigned to unknown._\n\nThe problem was reported in a scipy ticket:\n\n    http://projects.scipy.org/scipy/ticket/1541\n\nbut it is a Py3K issue in numpy.info.  Specifically, the code in numpy/lib/utils.py uses types.InstanceType, but that name no longer exists in Python 3.x.\n'"
numpy/numpy#151,1546406,chrisjordansquire,charris,2011-09-01 21:52:15,2014-06-13 00:18:50,2011-12-17 16:21:01,closed,,,17,,https://api.github.com/repos/numpy/numpy/issues/151,b'New R-like sample function',"b""This is a convenience function that wraps together several different functionalities. It allows the user to pass in a 1-D array-like and generate a possibly weighted, possibly without replacement random sample from that array. \r\n\r\nIt is essentially an implementation of the sample function in R's base package, which is a basic function for quickly generating random samples from a given vector.\r\n\r\nWith the exception of the weighted sample without replacement, all the functionality of sample can be obtained using various functions and tricks, but it would be nice to have them encapsulated in one function. It also eases the transitions for current and former R/S/S-plus users.  \r\n\r\nIt's biggest use case is for quickly and easily generating random samples from a vector of strings or non-contiguous, irregularly spaced integers (as occurs with codes in survey data). This is very useful for generating simulated categorical data, such as\r\n\r\nnumpy.random.sample( ['male', 'female'], 20, p=[0.55, 0.45])\r\n\r\nSuch a sampling can currently be accomplished by creating an rv_discrete instance in scipy.stats with P(X=0)=0.55 and P(X=1)=0.45, getting the appropriate random sample, and then using the output as indices in an array ['male', 'female']. In addition to the number of commands being somewhat cumbersome for quickly generating data, one can't use the existing stats.rv_discrete function to directly create a distribution on 'male' and 'female'. scipy.stats random variables assume the sample space is the real line. \r\n\r\nI am open to other names for this function, as long as they are short. While creating this function I realized that random.random_sample was given a number of different aliases in the random.\\__init\\__.py file which weren't in the reference docs. I removed those aliases, as well as the limited number of times they were used in the numpy code base. (All in test files.) This included random.sample as an alias for random.random_sample. While this new sample function changes the previous random.sample function, it does not break the API as the previous function was not in the docs. It will also fail on all calls to previous function as the function signature is different. """
nipy/dipy#688,97555219,hassemlal,hassemlal,2015-07-27 21:09:33,2015-10-15 18:48:57,2015-10-15 18:46:26,closed,,,17,,https://api.github.com/repos/nipy/dipy/issues/688,b'dipy.test() fails on centos 6.x / python2.6',"b'Hi,\r\nI tried running the dipy test suite, but it returns multiple failures (see below). I am running on centos 6.5 with python2.6 and virtualenv.\r\n\r\n```\r\n$ python -c ""import dipy; dipy.test()""\r\n.................Running unit tests for dipy\r\nNumPy version 1.9.2\r\nNumPy is installed in /home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy\r\nPython version 2.6.6 (r266:84292, Jan 22 2014, 09:42:36) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)]\r\nnose version 1.3.7\r\n\r\n[clip]\r\n\r\n.........SS.........\r\n======================================================================\r\nFAIL: dipy.core.tests.test_sphere.test_interp_rbf\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/core/tests/test_sphere.py"", line 374, in test_interp_rbf\r\n    nt.assert_(np.mean(np.abs(interp_data_a - expected)) < 0.1)\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy/testing/utils.py"", line 53, in assert_\r\n    raise AssertionError(smsg)  \r\nAssertionError\r\n\r\n======================================================================\r\nFAIL: dipy.reconst.tests.test_csdeconv.test_odfdeconv\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/reconst/tests/test_csdeconv.py"", line 220, in test_odfdeconv\r\n    assert_equal(len(w) > 0, False)\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/numpy/testing/utils.py"", line 334, in assert_equal\r\n    raise AssertionError(msg)   \r\nAssertionError:\r\nItems are not equal:\r\n ACTUAL: True\r\n DESIRED: False\r\n\r\n======================================================================\r\nFAIL: dipy.segment.tests.test_metric.test_metric_cosine\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/hassemlal/dipy/venv/lib/python2.6/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/hassemlal/dipy/dipy/segment/tests/test_metric.py"", line 177, in test_metric_cosine\r\n    assert_equal(distance, 0.)  \r\nAssertionError: array([[  4.74318692e-09]]) != 0.0\r\n\r\n----------------------------------------------------------------------\r\nRan 451 tests in 1043.841s\r\n\r\nFAILED (SKIP=9, failures=3)\r\n```\r\n'"
,,,,,,,,,,,,,,
SGWissInfo/pyphant1#55,22729481,aheld84,aheld84,2013-11-15 12:27:42,2013-11-15 14:53:29,2013-11-15 14:53:29,closed,aheld84,pyphant1.0b3,1,bug,https://api.github.com/repos/SGWissInfo/pyphant1/issues/55,b'ImageProcessing toolbox incompatible with scipy 0.13.0',"b'4 tests fail on mac os x with scipy 0.13.0. All seem to be related to `scipy.ndimage.find_objects`\r\n\r\n```\r\nImageProcessing$ python setup.py test\r\n\r\n...\r\n\r\n======================================================================\r\nERROR: testSingle (ImageProcessing.tests.TestAutoFocus.ZStackTestCase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/tests/TestAutoFocus.py"", line 104, in testSingle\r\n    statistics = afw.getStatistics(zstack)\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/AutoFocus.py"", line 263, in getStatistics\r\n    inclusionDict, labelData)\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/AutoFocus.py"", line 222, in autofocus\r\n    slicess = ndimage.find_objects(labelData)\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/measurements.py"", line 269, in find_objects\r\n    return _nd_image.find_objects(input, max_label)\r\nRuntimeError: data type not supported\r\n\r\n======================================================================\r\nERROR: testZStack (ImageProcessing.tests.TestAutoFocus.ZStackTestCase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/tests/TestAutoFocus.py"", line 74, in testZStack\r\n    statistics = afw.getStatistics(zstack)\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/AutoFocus.py"", line 263, in getStatistics\r\n    inclusionDict, labelData)\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/AutoFocus.py"", line 222, in autofocus\r\n    slicess = ndimage.find_objects(labelData)\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/measurements.py"", line 269, in find_objects\r\n    return _nd_image.find_objects(input, max_label)\r\nRuntimeError: data type not supported\r\n\r\n======================================================================\r\nERROR: testFindExtrPoint (ImageProcessing.tests.TestLocalExtrema.FLETestCase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/tests/TestLocalExtrema.py"", line 53, in testFindExtrPoint\r\n    result = fle.find(image)\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/FindLocalExtrema.py"", line 104, in find\r\n    newdata = self.findExtrema(image.data)\r\n  File ""/Users/aheld/src/fmf/pyphant1-aheld84/src/workers/ImageProcessing/ImageProcessing/FindLocalExtrema.py"", line 94, in findExtrema\r\n    slices = ndimage.find_objects(labeled)\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/measurements.py"", line 269, in find_objects\r\n    return _nd_image.find_objects(input, max_label)\r\nRuntimeError: data type not supported\r\n\r\n----------------------------------------------------------------------\r\nRan 91 tests in 15.035s\r\n\r\nFAILED (errors=4)\r\n```'"
paulgb/sklearn-pandas#48,123110128,dukebody,dukebody,2015-12-19 20:53:47,2016-01-16 10:39:25,2016-01-16 10:39:25,closed,,,2,enhancement,https://api.github.com/repos/paulgb/sklearn-pandas/issues/48,b'Deprecate custom CV shims in documentation and code. Refs #11.',
pypa/pip#25,674445,vbabiy,carljm,2011-03-15 06:01:32,2016-03-04 09:49:02,2011-04-29 13:55:00,closed,,,22,bug,https://api.github.com/repos/pypa/pip/issues/25,b'Do not run `egg_info` to each package in requirements list before installing the previous packages',"b""There are tons of packages using setup.py wrongly, and if we have a requirements file like:\n\n    \n    # requirements.txt\n    \n    numpy\n    scipy\n    \n\nWhat pip does is to download both, run `python setup.py egg_info` to numpy, and then `python setup.py egg_info` to scipy.\n\nThe problem is that scipy's setup.py tries to import numpy, which breaks the installation, because there is no numpy installed yet.\n\nI would suggest to download each package and then instead of running `egg_info`, running `install`, but I am not seeing the drawbacks now.\n\nCould you please give suggestions here?\n\nPS.: This issue is a split of <<issue 178>> - Non-alphabetical installation of requirements.\n\n---------------------------------------\n- Bitbucket: https://bitbucket.org/ianb/pip/issue/211\n- Originally Reported By: Hugo Lopes Tavares\n- Originally Created At: 2011-02-15 03:22:53\n"""
numpy/numpy#3368,14765979,WarrenWeckesser,seberg,2013-05-26 03:42:45,2014-01-27 17:34:16,2014-01-27 17:34:16,closed,,,1,11 - Bug;component: numpy.ma;Easy Fix;priority: normal,https://api.github.com/repos/numpy/numpy/issues/3368,"b""BUG: ma: Inconsistent return type from the 'count' method.""","b""Example:\r\n\r\n    In [6]: a = ma.masked_array([1, 2, 3])\r\n\r\n    In [7]: b = ma.masked_array([1, 2, 3], mask=False)\r\n\r\n    In [8]: a\r\n    Out[8]: \r\n    masked_array(data = [1 2 3],\r\n                 mask = False,\r\n           fill_value = 999999)\r\n\r\n    In [9]: b\r\n    Out[9]: \r\n    masked_array(data = [1 2 3],\r\n                 mask = [False False False],\r\n           fill_value = 999999)\r\n\r\nNotice the difference: `a.mask` is the scalar value `False`, while `b.mask` is an array:\r\n\r\n    In [10]: a.mask\r\n    Out[10]: False\r\n\r\n    In [11]: b.mask\r\n    Out[11]: array([False, False, False], dtype=bool)\r\n\r\nThat's not the problem.  They both represent the same data: a 1-D array with the values [1, 2, 3], with none of the values masked.\r\n\r\nBecause these represent the same data, I would expect the `count` method for these two arrays to return *exactly* the same thing. However,\r\n\r\n    In [12]: a.count(axis=0)  # Returns an integer\r\n    Out[12]: 3\r\n\r\n    In [13]: b.count(axis=0)  # Returns a scalar array\r\n    Out[13]: array(3)\r\n\r\nWhile the different return values act similarly in many cases, it is a nuisance (and a bug, in my opinion) that the return types are not the same.\r\n"""
numpy/numpy#67,725709,rkern,charris,2011-04-01 22:49:21,2014-07-21 09:50:17,2011-04-02 02:33:34,closed,,,2,,https://api.github.com/repos/numpy/numpy/issues/67,"b""Remove the advice to 'import scipy as sp' from the documentation""","b""DOC: Remove the advice to 'import scipy as sp' from the documentation. Correct a statement about how doctest namespaces are initialized in scipy."""
silx-kit/pyFAI#180,60204741,kif,kif,2015-03-07 12:39:23,2015-03-07 14:12:49,2015-03-07 14:12:49,closed,,,0,,https://api.github.com/repos/silx-kit/pyFAI/issues/180,b'Test segfault on debian7 i386',"b'The test_geometry_refinement segfault on the second part of the test:\r\nTestGeometryRefinement.test_Spline\r\nwhen on i386 computer. scipy 0.10.\r\nCould be related to https://github.com/scipy/scipy/issues/1859\r\n\r\nThis is apparently related to slsqp:\r\n#0  0xf7fdf425 in __kernel_vsyscall ()\r\n#1  0xf7e2e661 in raise () from /lib/i386-linux-gnu/i686/cmov/libc.so.6\r\n#2  0xf7e31a92 in abort () from /lib/i386-linux-gnu/i686/cmov/libc.so.6\r\n#3  0xf7e69dc5 in ?? () from /lib/i386-linux-gnu/i686/cmov/libc.so.6\r\n#4  0xf7e73eb1 in ?? () from /lib/i386-linux-gnu/i686/cmov/libc.so.6\r\n#5  0xf7e75718 in ?? () from /lib/i386-linux-gnu/i686/cmov/libc.so.6\r\n#6  0xf7e7885d in free () from /lib/i386-linux-gnu/i686/cmov/libc.so.6\r\n#7  0xf7a73acb in ?? () from /usr/lib/pymodules/python2.7/numpy/core/multiarray.so\r\n#8  0x080a6af7 in frame_dealloc.14828 (f=\r\n    Frame 0x8d5f374, for file /usr/lib/python2.7/dist-packages/scipy/optimize/slsqp.py, line 375, in fmin_slsqp (jw=<numpy.ndarray at remote 0x83da908>, b=(<float at remote 0x830f1bc>, <float at remote 0x830f1dc>), xl=<numpy.ndarray at remote 0x8d04ac0>, xu=<numpy.ndarray at remote 0x8d05448>, mode=<numpy.ndarray at remote 0x8d054b0>, majiter=<numpy.ndarray at remote 0x8cf9448>, majiter_prev=0, fx=<numpy.float64 at remote 0x8d623f0>, c_eq=<numpy.ndarray at remote 0x8d62708>, c_ieq=<numpy.ndarray at remote 0x8cf9478>, c=<numpy.ndarray at remote 0x8d62678>, g=<numpy.ndarray at remote 0x8d62d50>, a_eq=<numpy.ndarray at remote 0x8d63fe0>, a_ieq=<numpy.ndarray at remote 0x8d626a8>, a=<numpy.ndarray at remote 0x8d621f0>)) at ../Objects/frameobject.c:460\r\n'"
,,,,,,,,,,,,,,
numpy/numpy#3534,16890901,charris,charris,2013-07-17 21:26:11,2014-06-13 03:09:04,2013-08-15 16:58:59,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/3534,"b'Add nanmean, nanvar, and nanstd functions.'","b'The old nan functions and their tests are consolidated with the new functions\r\nand tests in numpy/lib/nanfunctions.py and numpy/lib/tests/test_nanfunctions.py\r\n\r\nSome refactoring is also done for the mean, var and std methods in\r\nnumpy/core/_methods.py and more extensive tests are added. In particular,\r\nthe type of scalar returns is now the same as for vector returns and the number of\r\ndegrees of freedom are checked so that negative variances are not returned. Warnings\r\nare raised in the non-positive degrees of freedom case or when taking the mean of empty\r\nslices.\r\n\r\nSome refactoring of the old nan functions has also been done. The following summarizes\r\nthe state of the nan functions after this PR.\r\n\r\nnanmax, nanmin\r\n--------------\r\nAdd out and keepdims keywords.\r\n\r\nnanargmin, nanargmax\r\n--------------------\r\nA NanWarning is raised if an all NaN slice detected. For all such\r\nslices np.iingo(np.intp).min is returned as the index value.\r\n\r\nnansum\r\n------\r\nThe keywords dtype, out, and keepdims are added.\r\n\r\nA FutureWarning is raised, as in the future the mean of an empty\r\nslice after NaN replacement will be 0 instead of the current NaN.\r\n\r\nnanmean, nanvar, nanstd\r\n-----------------------\r\nFor all, if the input array is of inexact type then the dtype and out\r\nparameters must be of inexact type if specified.  That insures that\r\nNaNs can be returned when appropriate.\r\n\r\nThe nanmean function detects empty slices after NaN replacement\r\nand raises a NanWarning. NaN is returned as the value for all such\r\nslices.\r\n\r\nThe nanmean and nanstd functions detect degrees of freedom <= 0 after\r\nNaN replacement and raise a NanWarning. NaN is returned as the value\r\nfor all such slices.\r\n\r\n'"
numpy/numpy#2981,10926853,rgommers,,2013-02-12 21:21:22,2013-04-25 17:25:41,None,open,,,0,11 - Bug;component: numpy.f2py;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2981,b'f2py issue when returning a single tuple',"b'\r\nFrom http://projects.scipy.org/scipy/ticket/1187, comment of Pearu:\r\n\r\nf2py supports user-defined functions that have multiple return values. These return values are expected to be returned as a single tuple object. So, the issue here is not a result of f2py bug but due to this f2py feature that is unintentionally triggered by the given example.\r\n\r\nAs a possible fix, we could have the f2py code to treat the returned tuples as ordinary sequences when the expected number of returned values is exactly 1. On the other hand, in the case of f2py misuse (returning 2 values when 1 is expected, for instance) the values will be silently converted to an array and may result even a more complicated issues to be analyzed.\r\n\r\nI think the best fix is to document this f2py feature in the corresponding codes, that is, never return tuple as a single return value, and perhaps improve the error message giving a hint why the crash might have been occurred.\r\n'"
scikit-image/scikit-image#1873,126292947,matthew-brett,,2016-01-12 22:31:42,2016-01-13 06:37:18,None,open,,,1,,https://api.github.com/repos/scikit-image/scikit-image/issues/1873,b'Bug in scipy.ndimage.affine_transform - what should we do?',"b""There's a discussion going on over here : https://github.com/scipy/scipy/issues/1547 - about a nasty bug in `scipy.ndimage.affine_transform`, where the result of `affine_transform` depends on how you specified the transformation matrix (as vector to be interpreted as diagonal, or full matrix).\r\n\r\nWe now have to decide whether to just fix this, or wait and warn, or not fix - see : https://github.com/scipy/scipy/issues/1547#issuecomment-171029805\r\n\r\nDo y'all have any views?\r\n"""
statsmodels/statsmodels#1196,23002995,kain88-de,jseabold,2013-11-20 16:32:48,2014-06-20 08:19:53,2014-04-03 19:39:10,closed,,,17,,https://api.github.com/repos/statsmodels/statsmodels/issues/1196,b'REF: ensure O(N log N) when using fft for acf',"b""The numpy implementation will fall back to a O(N**2) version of the fft\r\nin cases where it cannot factorize the array length, which can be a\r\nincrease in runtime of several order of magnitude. If we always ensure\r\nto fill up the array with zeros that it's total length is a power of 2\r\nthe runtime will be guaranteed to be O(N log N)."""
numpy/numpy#4602,31126988,cgohlke,charris,2014-04-09 03:22:56,2014-06-23 21:08:01,2014-04-09 14:12:53,closed,,,1,,https://api.github.com/repos/numpy/numpy/issues/4602,b'BUG: ifort has issues with optimization flag /O2',"b'Fixes several hard to track scipy test failures. \r\nSee scipy/scipy#3306, scipy/scipy#3340, scipy/scipy#1205, and discussion at http://software.intel.com/en-us/articles/numpyscipy-with-intel-mkl.\r\nNeeds backporting to numpy 1.8.x.'"
mokus0/random-fu#35,156132202,rpglover64,idontgetoutmuch,2016-05-22 03:04:17,2016-06-05 07:52:25,2016-06-05 07:52:25,closed,,,2,,https://api.github.com/repos/mokus0/random-fu/issues/35,b'Binomial CDF not numericaly stable',"b'`integralBinomialCDF 5000 (1/52) 170` computes to `Infinity`, `integralBinomialCDF 5000 (1/52) 200` computes to `NaN`.'"
