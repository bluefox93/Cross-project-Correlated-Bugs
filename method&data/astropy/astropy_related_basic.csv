issue,id,reporter,closed_by,created_at,updated_at,closed_at,state,assignee,milestone,comments,label_name,url,title,body
spacetelescope/synphot_refactor#92,165653175,pllim,,2016-07-14 20:38:24,2016-07-20 17:01:58,None,open,pllim,,0,models;Refactoring;Upstream Fix Required,https://api.github.com/repos/spacetelescope/synphot_refactor/issues/92,b'Use new Tabular model in Astropy',"b""**DO NOT MERGE**\r\n\r\nUse new `TabularModel` in Astropy, as implemented in astropy/astropy#5105. Also see spacetelescope/gwcs#28.\r\n\r\nTODO:\r\n\r\n- [ ] Run tests after astropy/astropy#5105 and astropy/astropy#5183 are merged. They already passed on local machine.\r\n- [x] Remove CI YAML hacks for both Travis and Appveyor to use Nadia's branch.\r\n- [x] Also file equivalent PR for `stsynphot` (see spacetelescope/stsynphot_refactor#22)."""
spacetelescope/synphot_refactor#94,165955672,pllim,pllim,2016-07-17 01:36:49,2016-07-19 20:59:58,2016-07-19 20:59:58,closed,,,0,Effort-low;Package-novice;testing;Upstream Fix Required,https://api.github.com/repos/spacetelescope/synphot_refactor/issues/94,b'Remove hacky AssertionError workaround in tests',b'Remove `except` block from the following from tests (added in #93) when astropy/astropy#5183 is merged:\r\n\r\n```python\r\ntry:\r\n    assert result.unit == ans.unit\r\nexcept AssertionError:  # For STMAG and ABMAG (astropy/astropy#5178)\r\n    assert result.unit.to_string() == ans.unit.to_string()\r\n```'
spacetelescope/stsynphot_refactor#20,165644529,pllim,pllim,2016-07-14 19:53:49,2016-07-14 20:24:11,2016-07-14 20:24:11,closed,pllim,,0,astropy-helpers;bug;Python3,https://api.github.com/repos/spacetelescope/stsynphot_refactor/issues/20,b'Some updates',b'Updated `astropy_helpers` to 1.2. Fixed `ConfigParser` compat with Python 3.\r\n\r\nNOTE: Tests failed because of astropy/astropy#5178'
spacetelescope/stsynphot_refactor#21,165956708,pllim,pllim,2016-07-17 02:19:38,2016-07-17 02:57:06,2016-07-17 02:57:06,closed,,,1,,https://api.github.com/repos/spacetelescope/stsynphot_refactor/issues/21,b'Change how Quantity is defined to get around mag bug',b'Similar to spacetelescope/synphot_refactor#93. Also see astropy/astropy#5178.'
spacetelescope/synphot_refactor#91,165644155,pllim,pllim,2016-07-14 19:52:08,2016-07-14 20:43:22,2016-07-14 20:43:22,closed,pllim,,0,astropy-helpers;bug;Python3;units,https://api.github.com/repos/spacetelescope/synphot_refactor/issues/91,b'Some updates',b'Updated `astropy_helpers` to 1.2. Fixed `ConfigParser` compat with Python 3. Fixes #85.\r\n\r\nNOTE: Tests failed because of astropy/astropy#5178'
spacetelescope/synphot_refactor#93,165882277,pllim,pllim,2016-07-15 21:35:29,2016-07-15 21:43:26,2016-07-15 21:43:26,closed,pllim,,1,,https://api.github.com/repos/spacetelescope/synphot_refactor/issues/93,b'Refactored how Quantity is initialized to fix test failure',b'Refactored how Quantity is initialized to fix test failure. See astropy/astropy#5178.'
desihub/desispec#212,159092961,sbailey,sbailey,2016-06-08 07:00:48,2016-06-28 22:09:11,2016-06-08 21:21:12,closed,tskisner,Pipeline 2016b,6,enhancement,https://api.github.com/repos/desihub/desispec/issues/212,b'I/O cleanup',"b'This PR addresses a number of I/O cleanup issues:\r\n  * fixes #85 by storing data as 32-bit float on disk while casting back to 64-bit when reading in.  This is essentially a factor-of-2 lossy compression technique.\r\n  * stores masks as compressed data HDUs, which oddly requires the masks to be 32 bit not 64 bit since the compressed integer fits convention doesn\'t support 64-bit integers.  If we really need 64 bit masks we can undo this on a case by case basis; in the meantime that HDU will be much much smaller.\r\n  * uses `memmap=False` for most fits I/O, since otherwise `astropy.io.fits` leaves the file open even after you call `close()` (grr)\r\n  * Optimizes the fits I/O by doing more ""open the file, read everything you need, close the file"" instead of a bunch of calls to `fits.getdata()` and `fits.getheader()`, each of which re-opens the file and parses it anew.\r\n  * Initially writes output files to {filename.fits}.tmp.  After succeeding in writing the entire file it moves it to the final {filename.fits} name.  This avoids corrupted files with the ""right"" name if the processes is killed in the middle of I/O.  The pipeline was already doing something like this (which I haven\'t undone yet, maybe that should be added here), but it seems better to directly do it as part of the desispec.io module by default.\r\n  * adds header and data checksums to fits file output to address half of issue #20 .'"
astropy/astroplan#35,96404468,bmorris3,,2015-07-21 20:01:04,2016-05-15 18:24:03,None,open,,wishlist,4,enhancement,https://api.github.com/repos/astropy/astroplan/issues/35,b'Lunar calculations not yet supported',"b'We need an [astropy issue to be addressed](https://github.com/astropy/astropy/issues/3920), which is currently being addressed by @eteq, before we can move forward on the moon phase, illumination, position calculations. \r\n\r\nOnce this issue has been addressed, we can bring in this jplephem implementation of `get_moon` from [this toy notebook](https://github.com/bmorris3/gsoc2015/blob/master/jplephem.ipynb).'"
eteq/astropy#40,158923137,mhvk,eteq,2016-06-07 13:29:30,2016-06-07 14:38:19,2016-06-07 14:38:19,closed,,,1,,https://api.github.com/repos/eteq/astropy/issues/40,b'Ensure scalar case of get_sun still works.',"b'@eteq - having led you up a blind alley, I thought the least I could do was to fix the remaining travis errors: turns out that with your change, there was no need to keep track of whether time was a scalar: the broadcasting now already ensures that in that case the output is scalar as well!'"
spacetelescope/synphot_refactor#85,158445132,pllim,pllim,2016-06-03 20:09:45,2016-07-14 20:43:22,2016-07-14 20:43:22,closed,,,0,Effort-low;Package-novice;Refactoring;units;Upstream Fix Required,https://api.github.com/repos/spacetelescope/synphot_refactor/issues/85,b'Remove spectral_density_mag equivalency when time is right',"b'When astropy/astropy#5017 is merged, `spectral_density_mag` can be removed.'"
spacetelescope/synphot_refactor#84,158241300,pllim,pllim,2016-06-02 21:14:06,2016-06-22 16:07:36,2016-06-03 20:08:12,closed,pllim,,3,Refactoring;units,https://api.github.com/repos/spacetelescope/synphot_refactor/issues/84,b'Use Astropy mag units',"b'- [x] Replaced STMAG and ABMAG with official units from Astropy.\r\n\r\nThis fixes #32. It is unlikely that VEGAMAG and OBMAG will ever be in Astropy, as they depend on standard star/telescope area, so just leave those as-is.\r\n\r\nNote: When astropy/astropy#5017 is merged, `spectral_density_mag` can be removed.'"
spacetelescope/stsynphot_refactor#16,156334142,pllim,pllim,2016-05-23 18:21:08,2016-05-25 20:37:47,2016-05-25 20:37:47,closed,pllim,,0,,https://api.github.com/repos/spacetelescope/stsynphot_refactor/issues/16,b'Use new models in synphot',b'Changes for spacetelescope/synphot_refactor#81'
radio-astro-tools/spectral-cube#301,156755557,keflavich,keflavich,2016-05-25 13:49:03,2016-06-02 07:28:57,2016-06-02 07:28:57,closed,,,4,,https://api.github.com/repos/radio-astro-tools/spectral-cube/issues/301,b'Handle WCS reversal & fix 1D slice headers',"b'WCS reversal, e.g. cube[::-1,:,:], is allowed now but does *not* work because\nof https://github.com/astropy/astropy/pull/4962.  This PR partially implements\na reverse indexing.'"
numpy/numpy#7651,155818537,ewmoore,charris,2016-05-19 19:42:33,2016-05-19 21:44:43,2016-05-19 20:24:57,closed,,,6,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/7651,b'BUG: one to any power is still 1.  Broken edgecase for int arrays',b'Fixes gh-7648.'
numpy/numpy#7649,155778729,mhvk,mhvk,2016-05-19 16:22:51,2016-05-19 16:36:42,2016-05-19 16:30:02,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/7649,b'BUG: Integers raised to negative powers gives wrong results',"b'In current master:\r\n```\r\nnp.array(1, dtype=int) ** -2\r\nOut[5]: 0\r\n```\r\nIn 1.11 and earlier:\r\n```\r\nIn [3]: np.array(1, dtype=int) ** -2\r\nOut[3]: 1\r\n```\r\n(Found in astropy testing; https://github.com/astropy/astropy/pull/4950)'"
numpy/numpy#7648,155643621,jakevdp,charris,2016-05-19 02:48:18,2016-05-19 20:24:57,2016-05-19 20:24:57,closed,,,5,11 - Bug;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/7648,b'Behavior of negative exponentiation inconsistent between integer types',b'```python\r\n>>> np.int64(2) ** -2\r\n0.25\r\n>>> np.int32(2) ** -2\r\n0\r\n```\r\n\r\nTested on OSX 10.8.5 w/ numpy 1.10 on both Python 2.7 and 3.5.'
numpy/numpy#7459,143252851,ewmoore,,2016-03-24 13:56:28,2016-06-06 22:22:01,None,open,,1.12.0 release,11,10 - Maintenance;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/7459,b'WIP: implement __rop__ logic for scalar operators',"b""Re. #7449.  \r\n\r\nThis is a work in progress, both because I'd like to get some feedback on the approach and there are currently two test failures that I don't yet understand. \r\n"""
astropy/astroquery#689,155302202,keflavich,bsipocz,2016-05-17 16:21:24,2016-05-23 15:14:48,2016-05-23 15:14:47,closed,,,4,,https://api.github.com/repos/astropy/astroquery/issues/689,"b""python2: astropy tables with unicode don't print""","b'Example:\r\n\r\n```\r\nfrom astroquery.alma import Alma\r\nfrom astropy import units as u\r\nAlma.query_region(coordinate=SkyCoord(\'04:54:08.85 -03:00:28.8\', unit=(\'hour\',\'deg\'), frame=\'icrs\'), radius=1*u.arcmin)\r\n```\r\n\r\nresults in:\r\n\r\n```\r\nOut[14]: Traceback (most recent call last):\r\n  File ""/Users/adam/anaconda/envs/astropy27/lib/python2.7/site-packages/IPython/core/formatters.py"", line 697, in __call__\r\n    printer.pretty(obj)\r\n  File ""/Users/adam/anaconda/envs/astropy27/lib/python2.7/site-packages/IPython/lib/pretty.py"", line 383, in pretty\r\n    return _default_pprint(obj, self, cycle)\r\n  File ""/Users/adam/anaconda/envs/astropy27/lib/python2.7/site-packages/IPython/lib/pretty.py"", line 503, in _default_pprint\r\n    _repr_pprint(obj, p, cycle)\r\n  File ""/Users/adam/anaconda/envs/astropy27/lib/python2.7/site-packages/IPython/lib/pretty.py"", line 685, in _repr_pprint\r\n    output = repr(obj)\r\n  File ""/Users/adam/repos/astropy/astropy/table/table.py"", line 812, in __repr__\r\n    return self._base_repr_(html=False, max_width=None)\r\n  File ""/Users/adam/repos/astropy/astropy/table/table.py"", line 799, in _base_repr_\r\n    max_lines=max_lines, tableclass=tableclass)\r\n  File ""/Users/adam/repos/astropy/astropy/table/pprint.py"", line 477, in _pformat_table\r\n    align=align_)\r\n  File ""/Users/adam/repos/astropy/astropy/table/pprint.py"", line 223, in _pformat_col\r\n    col_strs = list(col_strs_iter)\r\n  File ""/Users/adam/repos/astropy/astropy/table/pprint.py"", line 391, in _pformat_col_iter\r\n    col_str = format_func(col_format, col[i])\r\n  File ""/Users/adam/repos/astropy/astropy/table/pprint.py"", line 28, in <lambda>\r\n    _format_funcs = {None: lambda format_, val: text_type(val)}\r\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 52: ordinal not in range(128)\r\n```\r\n\r\n\r\nSpecific problem:\r\n```\r\nIn [18]: query[\'Project abstract\'].data\r\nOut[18]:\r\nmasked_BaseColumn(data = [ \'We propose to conduct deep ALMA observation of two z\\xe2\\x88\\xbc6 sub-L\\xe2\\x88\\x97 star-forming galaxies to detect their [CII] emission line and derive from the line flux their star formation activity. These two Lyman-break galaxies are unique multiple image systems found in the massive cluster lenses Abell 383 and MS0451-03 at redshift 6.027 and 6.703 respectively. Their lensing magnification is ~11 and 100 respectively thus allowing to probe the properties of &#34;normal&#34; low luminosity galaxies at early times in the Universe. The proposed observation will lead to a secure [CII] detection. However, would we not detect [CII] the upper limit we can derived will have strong consequences on our understanding of star formation in the early Universe advocating for unexpected strong evolution in the galaxy formation models.\'\r\n \'We propose to conduct deep ALMA observation of two z\\xe2\\x88\\xbc6 sub-L\\xe2\\x88\\x97 star-forming galaxies to detect their [CII] emission line and derive from the line flux their star formation activity. These two Lyman-break galaxies are unique multiple image systems found in the massive cluster lenses Abell 383 and MS0451-03 at redshift 6.027 and 6.703 respectively. Their lensing magnification is ~11 and 100 respectively thus allowing to probe the properties of &#34;normal&#34; low luminosity galaxies at early times in the Universe. The proposed observation will lead to a secure [CII] detection. However, would we not detect [CII] the upper limit we can derived will have strong consequences on our understanding of star formation in the early Universe advocating for unexpected strong evolution in the galaxy formation models.\'\r\n \'We propose to conduct deep ALMA observation of two z\\xe2\\x88\\xbc6 sub-L\\xe2\\x88\\x97 star-forming galaxies to detect their [CII] emission line and derive from the line flux their star formation activity. These two Lyman-break galaxies are unique multiple image systems found in the massive cluster lenses Abell 383 and MS0451-03 at redshift 6.027 and 6.703 respectively. Their lensing magnification is ~11 and 100 respectively thus allowing to probe the properties of &#34;normal&#34; low luminosity galaxies at early times in the Universe. The proposed observation will lead to a secure [CII] detection. However, would we not detect [CII] the upper limit we can derived will have strong consequences on our understanding of star formation in the early Universe advocating for unexpected strong evolution in the galaxy formation models.\'],\r\n                  mask = [False False False],\r\n            fill_value = N/A)\r\n\r\n```\r\n\r\nEDIT: note the dtype of this column is `(\'Project abstract\', \'S4000\')`\r\n\r\nThis is therefore a problem with astropy.table\'s pprint function'"
gammapy/gammapy#507,147419231,joleroi,joleroi,2016-04-11 12:58:55,2016-04-20 12:20:37,2016-04-11 22:48:04,closed,joleroi,0.4,4,feature,https://api.github.com/repos/gammapy/gammapy/issues/507,b'Add Fermi catalog spectrum evaluation and plotting',b'This PR introduces\r\n\r\n* [x] Integral and differential flux points Table subclasses\r\n* [x] Conversion method from int to diff (wrapper around existing functionality)\r\n* [x] read_3FGL methods for both flux points classes as well as SpectrumFitResult\r\n* [x] evaluate and evaluate_butterfly for SpectrumFitResult\r\n\r\nI replaces https://github.com/gammapy/gammapy/pull/504\r\n\r\nUp next\r\n* The same for 2FHL (this is  a little bit more involved)\r\n'
GalSim-developers/GalSim#690,116097672,rmjarvis,rmjarvis,2015-11-10 12:29:43,2016-06-06 20:09:54,2016-06-05 03:17:52,closed,rmjarvis,v1.4,65,,https://api.github.com/repos/GalSim-developers/GalSim/issues/690,b'#556 New LSST module with WCS using LSST stack for implementation',"b'(Text originally by @danielsf)\r\n\r\nI think this implements all of the functionality you need to create a WCS for an LSST detector, provided you already have the LSST stack installed.\r\n\r\nThere are two classes defined:\r\n\r\nLsstCamera wraps the whole afwCameraGeom model for the LSST camera. This class performs the exact (according to the LSST stack) transformations between RA, Dec and pixel coordinates.\r\n\r\nLsstWCS uses LsstCamera to create a WCS for a single detector on the LSST Camera.\r\n\r\nI have added a test_lsst.py script to the tests/ directory to test all of the relevant functionality.\r\n\r\nLet me know what needs fixing.'"
sunpy/sunpy#1759,152919349,wafels,Cadair,2016-05-04 02:50:47,2016-05-24 18:05:35,2016-05-24 18:05:35,closed,Cadair,0.7,11,0.6.x;0.7.x;affects-released;Bug?;effort-medium;in progress;package-intermediate;priority-high,https://api.github.com/repos/sunpy/sunpy/issues/1759,b'A problem with draw_grid=True and WCSAxes',"b'Before installing WCSAxes, the code below produced a map using draw_grid=True with no issues.  After installing WCSAxes, Map gives the following error when using the draw_grid option.  Can anyone else reproduce this?\r\n\r\n```python\r\nIn [1]: from sunpy.map import Map\r\n\r\nIn [2]: b = Map(\'/Users/ireland/sunpy/data/sample_data/aia.lev1.193A_2013-09-21T16_00_06.84Z.image_lev1.fits\')\r\nException AttributeError: AttributeError(\'_coldefs\',) in <bound method FITS_rec.__del__ of FITS_rec([(array([2150,    0], dtype=int32),),\r\n       (array([2114, 2150], dtype=int32),),\r\n       (array([1994, 4264], dtype=int32),), ...,\r\n       (array([    2067, 12414925], dtype=int32),),\r\n       (array([    2101, 12416992], dtype=int32),),\r\n       (array([    2109, 12419093], dtype=int32),)], \r\n      dtype=(numpy.record, [(\'COMPRESSED_DATA\', \'>i4\', (2,))]))> ignored\r\n\r\nIn [3]: b.peek()\r\n\r\nIn [4]: b.peek(draw_grid=True)\r\n---------------------------------------------------------------------------\r\nInconsistentAxisTypesError                Traceback (most recent call last)\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/artist.pyc in draw_wrapper(artist, renderer, *args, **kwargs)\r\n     59     def draw_wrapper(artist, renderer, *args, **kwargs):\r\n     60         before(artist, renderer)\r\n---> 61         draw(artist, renderer, *args, **kwargs)\r\n     62         after(artist, renderer)\r\n     63 \r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/figure.pyc in draw(self, renderer)\r\n   1157         dsu.sort(key=itemgetter(0))\r\n   1158         for zorder, a, func, args in dsu:\r\n-> 1159             func(*args)\r\n   1160 \r\n   1161         renderer.close_group(\'figure\')\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/wcsaxes/core.pyc in draw(self, renderer, inframe)\r\n    192         self.coords.frame._update_patch_path()\r\n    193 \r\n--> 194         super(WCSAxes, self).draw(renderer, inframe)\r\n    195 \r\n    196         # Here need to find out range of all coordinates, and update range for\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/artist.pyc in draw_wrapper(artist, renderer, *args, **kwargs)\r\n     59     def draw_wrapper(artist, renderer, *args, **kwargs):\r\n     60         before(artist, renderer)\r\n---> 61         draw(artist, renderer, *args, **kwargs)\r\n     62         after(artist, renderer)\r\n     63 \r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/axes/_base.pyc in draw(self, renderer, inframe)\r\n   2322 \r\n   2323         for zorder, a in dsu:\r\n-> 2324             a.draw(renderer)\r\n   2325 \r\n   2326         renderer.close_group(\'axes\')\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/artist.pyc in draw_wrapper(artist, renderer, *args, **kwargs)\r\n     59     def draw_wrapper(artist, renderer, *args, **kwargs):\r\n     60         before(artist, renderer)\r\n---> 61         draw(artist, renderer, *args, **kwargs)\r\n     62         after(artist, renderer)\r\n     63 \r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/lines.pyc in draw(self, renderer)\r\n    729         funcname = self._lineStyles.get(self._linestyle, \'_draw_nothing\')\r\n    730         if funcname != \'_draw_nothing\':\r\n--> 731             tpath, affine = transf_path.get_transformed_path_and_affine()\r\n    732             if len(tpath.vertices):\r\n    733                 self._lineFunc = getattr(self, funcname)\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/transforms.pyc in get_transformed_path_and_affine(self)\r\n   2709         the path necessary to complete the transformation.\r\n   2710         """"""\r\n-> 2711         self._revalidate()\r\n   2712         return self._transformed_path, self.get_affine()\r\n   2713 \r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/transforms.pyc in _revalidate(self)\r\n   2683             or self._transformed_path is None):\r\n   2684             self._transformed_path = \\\r\n-> 2685                 self._transform.transform_path_non_affine(self._path)\r\n   2686             self._transformed_points = \\\r\n   2687                 Path._fast_from_codes_and_verts(\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/matplotlib/transforms.pyc in transform_path_non_affine(self, path)\r\n   2362             return path\r\n   2363         elif not self._a.is_affine and self._b.is_affine:\r\n-> 2364             return self._a.transform_path_non_affine(path)\r\n   2365         else:\r\n   2366             return self._b.transform_path_non_affine(\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/wcsaxes/transforms.pyc in transform_path(self, path)\r\n     48             The resulting path\r\n     49         """"""\r\n---> 50         return Path(self.transform(path.vertices), path.codes)\r\n     51 \r\n     52     transform_path_non_affine = transform_path\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/wcsaxes/transforms.pyc in transform(self, world)\r\n     97             raise ValueError(""Second dimension of input values should match number of WCS coordinates"")\r\n     98 \r\n---> 99         pixel = self.wcs.wcs_world2pix(world, 1) - 1\r\n    100 \r\n    101         if self.slice is None:\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/astropy/wcs/wcs.pyc in wcs_world2pix(self, *args, **kwargs)\r\n   2141         return self._array_converter(\r\n   2142             lambda xy, o: self.wcs.s2p(xy, o)[\'pixcrd\'],\r\n-> 2143             \'input\', *args, **kwargs)\r\n   2144     wcs_world2pix.__doc__ = """"""\r\n   2145         Transforms world coordinates to pixel coordinates, using only\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/astropy/wcs/wcs.pyc in _array_converter(self, func, sky, *args, **kwargs)\r\n   1229             if self.naxis == 1 and len(xy.shape) == 1:\r\n   1230                 return _return_list_of_arrays([xy], origin)\r\n-> 1231             return _return_single_array(xy, origin)\r\n   1232 \r\n   1233         elif len(args) == self.naxis + 1:\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/astropy/wcs/wcs.pyc in _return_single_array(xy, origin)\r\n   1213             if ra_dec_order and sky == \'input\':\r\n   1214                 xy = self._denormalize_sky(xy)\r\n-> 1215             result = func(xy, origin)\r\n   1216             if ra_dec_order and sky == \'output\':\r\n   1217                 result = self._normalize_sky(result)\r\n\r\n/Users/ireland/anaconda/lib/python2.7/site-packages/astropy/wcs/wcs.pyc in <lambda>(xy, o)\r\n   2140             raise ValueError(""No basic WCS settings were created."")\r\n   2141         return self._array_converter(\r\n-> 2142             lambda xy, o: self.wcs.s2p(xy, o)[\'pixcrd\'],\r\n   2143             \'input\', *args, **kwargs)\r\n   2144     wcs_world2pix.__doc__ = """"""\r\n\r\nInconsistentAxisTypesError: ERROR 4 in wcss2p() at line 2876 of file cextern/wcslib/C/wcs.c:\r\nncoord and/or nelem inconsistent with the wcsprm.\r\n\r\n\r\nIn [5]: \r\n```'"
numpy/numpy#7393,139411294,mhvk,charris,2016-03-08 22:21:16,2016-03-10 01:58:05,2016-03-10 01:58:05,closed,,,11,,https://api.github.com/repos/numpy/numpy/issues/7393,b'BUG: np.sum / np.add.reduce / np.array change in behaviour for indexable objects',"b""`astropy` is failing with 1.11 because of a bug that seems to boil down to a change in behaviour of `np.sum` (or specifically `np.add.reduce`), which I think in turn is due to how `np.array` treats things that end up being object arrays (have not investigated in detail), in that before, if something could be indexed, it would be split into individual pieces, while now it becomes a single object array. If split into pieces, it seems `np.add.reduce` attempts to add the pieces together (which fails for our class), while if a single piece, it just returns the single object.\r\n\r\nWhile this is a regression of sorts, in a way the new behaviour is a bit more sane, and we can work around it, so I'm raising this mostly to ensure this new behaviour is in fact desired.\r\n```\r\n# in numpy 1.10.4\r\nnp.__version__\r\n# '1.10.4'\r\nfrom astropy.time import Time\r\nd = Time([50001, 50002], format='mjd')           \r\nnp.array(d)\r\n# array([<Time object: scale='utc' format='mjd' value=50001.0>,\r\n#        <Time object: scale='utc' format='mjd' value=50002.0>], dtype=object)\r\nnp.add.reduce(d)\r\nOperandTypeError                          Traceback (most recent call last)\r\n.\r\n.\r\n.\r\n   1322         if not isinstance(other, TimeDelta):\r\n-> 1323             raise OperandTypeError(self, other, '+')\r\n.\r\n.\r\n.\r\nOperandTypeError: Unsupported operand type(s) for +: 'Time' and 'Time'\r\n```\r\n\r\nWhile in more recent versions:\r\n```\r\nnp.__version__\r\n# '1.11.0.dev0+6e067b3'\r\nd = Time([50001, 50002], format='mjd')\r\nnp.array(d)\r\n# array(<Time object: scale='utc' format='mjd' value=[ 50001.  50002.]>, dtype=object)\r\nnp.add.reduce(d)\r\n# <Time object: scale='utc' format='mjd' value=[ 50001.  50002.]>\r\n```"""
gammapy/gammapy#482,138542995,joleroi,joleroi,2016-03-04 18:05:14,2016-04-20 12:04:54,2016-03-07 13:45:16,closed,joleroi,0.4,5,cleanup;infrastructure,https://api.github.com/repos/gammapy/gammapy/issues/482,b'Improve gammapy.spectrum',b'* [x] Integrate grouping code\r\n* [x] Add functionality to write configfiles (usefull for debugging e.g. pipeline analysis)\r\n* [x] Use Obstable consistently to store meta info'
astropy/ccdproc#311,134596267,MSeifert04,crawfordsm,2016-02-18 14:34:30,2016-03-01 22:32:34,2016-03-01 22:27:45,closed,,,9,combiner,https://api.github.com/repos/astropy/ccdproc/issues/311,b'Scale uncertainty with the number of combined pixel during combination.',"b""Fixes #309 \r\n\r\nPlease review this carefully.\r\n\r\nSome (remaining) open issues:\r\n- Since you did not pass ``ddof`` to ``np.ma.std`` I didn't correct for finite populations either and just divided by ``np.sqrt(valid_pixel)``. This might not be exactly correct but I'm not too sure about statistics. The downside with such an finite population correction is also that the number of valid pixel could be 0 and then subtracting 1 and taking the square root ... not good.\r\n- both combiners ignore scaling and weighting for the uncertainty computation.\r\n- median combine ignores the masked values (probably because ``median_absolute_deviation`` cannot handle these. Therefore I corrected just with the whole number of pixel because these are used for uncertainty computation. It's inconsistent but I don't have time right now to validate what needs to be done to make that take masked arrays.\r\n\r\nIf you have any suggestions regarding these issues I would be very happy to include these and if not I can raise another issue for these."""
numpy/numpy#7330,136107292,MSeifert04,,2016-02-24 16:14:34,2016-03-02 14:48:04,None,open,,,5,,https://api.github.com/repos/numpy/numpy/issues/7330,b'np.median/np.average on masked-arrays',"b'I thought most/all of the numpy ufuncs would consider if masked-arrays are used but for the ``np.median`` (and ``np.average``) this is not the case:\r\n\r\n```\r\nimport numpy as np\r\nnormal_array = np.arange(10)\r\nmasked_array = np.ma.array(normal_array, mask=normal_array>7)\r\nnp.median(normal_array) # Returns 4.5\r\nnp.median(masked_array) # Returns 4.5\r\nnp.ma.median(masked_array)[0] # Returns 3.5 (it returns a masked array instead of a scalar btw)\r\n```\r\n\r\nwhereas for sums it works as one would expect:\r\n```\r\nnp.sum(normal_array) # Returns 45\r\nnp.sum(masked_array) # Returns 28\r\n```\r\nmean/std/var all work like sum, why the exception for median/average?'"
zestsoftware/zest.releaser#172,137532367,embray,,2016-03-01 10:23:47,2016-03-01 12:53:06,None,open,,,1,,https://api.github.com/repos/zestsoftware/zest.releaser/issues/172,b'Add documentation for adding hooks via setup.cfg',"b""As brought up in astropy/astropy#4650, some years ago I added an ability to specify releaser hooks in the `[zest.releaser]` section of `setup.cfg`:  zestsoftware/zest.releaser@6235dad\r\n\r\nBut this feature never made it into the documentation.  IMO this is generally a better way to specify project-specific hooks since it doesn't pollute a global namespace (entry-points), so long as the given functions can be imported at release-time.  """
zestsoftware/zest.releaser#168,135754537,hgrecco,mauritsvanrees,2016-02-23 14:40:49,2016-05-10 09:58:00,None,open,,,17,,https://api.github.com/repos/zestsoftware/zest.releaser/issues/168,b'Other project zest.releaser hooks pollute my own release workflow',"b'Something very weird happened when I was releasing my own project (not related with astropy):\r\n```\r\n  File ""/Users/username/anaconda/bin/fullrelease"", line 11, in <module>\r\n    sys.exit(main())\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/fullrelease.py"", line 32, in main\r\n    postreleaser.run()\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/baserelease.py"", line 307, in run\r\n    self._run_hooks(\'middle\')\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/baserelease.py"", line 302, in _run_hooks\r\n    utils.run_hooks(self.setup_cfg, which_releaser, when, self.data)\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/utils.py"", line 579, in run_hooks\r\n    run_entry_points(which_releaser, when, data)\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/utils.py"", line 595, in run_entry_points\r\n    plugin(data)\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/astropy/utils/release.py"", line 260, in postreleaser_middle\r\n    _update_setup_py_version(data[\'dev_version\'])\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/astropy/utils/release.py"", line 269, in _update_setup_py_version\r\n    output.write(line.decode(\'utf-8\'))\r\nAttributeError: \'str\' object has no attribute \'decode\'\r\n```\r\nIt seems that astropy\'s zest releaser hook is polluting my own release workflow. There is something very wrong about this'"
zestsoftware/zest.releaser#168,135754537,hgrecco,mauritsvanrees,2016-02-23 14:40:49,2016-05-10 09:58:00,None,open,,,17,,https://api.github.com/repos/zestsoftware/zest.releaser/issues/168,b'Other project zest.releaser hooks pollute my own release workflow',"b'Something very weird happened when I was releasing my own project (not related with astropy):\r\n```\r\n  File ""/Users/username/anaconda/bin/fullrelease"", line 11, in <module>\r\n    sys.exit(main())\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/fullrelease.py"", line 32, in main\r\n    postreleaser.run()\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/baserelease.py"", line 307, in run\r\n    self._run_hooks(\'middle\')\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/baserelease.py"", line 302, in _run_hooks\r\n    utils.run_hooks(self.setup_cfg, which_releaser, when, self.data)\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/utils.py"", line 579, in run_hooks\r\n    run_entry_points(which_releaser, when, data)\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/zest/releaser/utils.py"", line 595, in run_entry_points\r\n    plugin(data)\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/astropy/utils/release.py"", line 260, in postreleaser_middle\r\n    _update_setup_py_version(data[\'dev_version\'])\r\n  File ""/Users/username/anaconda/lib/python3.5/site-packages/astropy/utils/release.py"", line 269, in _update_setup_py_version\r\n    output.write(line.decode(\'utf-8\'))\r\nAttributeError: \'str\' object has no attribute \'decode\'\r\n```\r\nIt seems that astropy\'s zest releaser hook is polluting my own release workflow. There is something very wrong about this'"
numpy/numpy#7156,130187988,mhvk,charris,2016-01-31 21:28:07,2016-02-01 14:30:25,2016-02-01 00:38:11,closed,,,1,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/7156,"b'Reascertain that linspace respects ndarray subclasses in start, stop.'","b'This is an alternative to #7142, in which we revert to not using in-place multiplication.'"
,,,,,,,,,,,,,,
SWAT-Sheffield/pysac#10,132481829,fredgent,fredgent,2016-02-09 17:31:50,2016-02-19 12:28:57,2016-02-19 12:28:57,closed,,,12,,https://api.github.com/repos/SWAT-Sheffield/pysac/issues/10,b'numpy linspace not working on u.Quanttity on iceberg',"b'When calling Z = np.linspace(xyz[4],xyz[5],Nxyz[2]), where xyz is a 1D array in astropy u.Mm I get \r\n```python\r\nTraceback (most recent call last):\r\n  File ""spruit_atmosphere.py"", line 100, in <module>\r\n    coords = atm.model_pars.get_coords(model_pars[\'Nxyz\'], u.Quantity(model_pars[\'xyz\']))\r\n  File ""/home/sm1fg/pysac/pysac/mhs_atmosphere/parameters/model_pars.py"", line 150, in get_coords\r\n    xyz[5],Nxyz[2])\r\n  File ""/home/sm1fg/anaconda2/lib/python2.7/site-packages/numpy/core/function_base.py"", line 115, in linspace\r\n    y[-1] = stop\r\nValueError: setting an array element with a sequence.\r\n./src/common/module.cpp(194): assert ""module->m_state == State_Initialized"" failed in DoCleanUpModules(): not initialized module being cleaned up\r\n```\r\nOn iceberg numpy version is 1.10.4 whereas on my workstation it is 1.8.2\r\n                   astropy                1.1.1                                                     1.0.2\r\nIt works fine on my workstation and when I last tried it on my laptop and the Finnish cluster.\r\nAny ideas?'"
spacetelescope/PyFITS#74,38127791,embray,,2014-07-17 21:26:11,2016-01-28 22:29:56,None,open,,,0,effort-low;package-novice;port-3.1.x;port-3.2.x,https://api.github.com/repos/spacetelescope/PyFITS/issues/74,b' Prevent del on header keywords during iteration ',"b' A common newbie mistake when dealing with Python dicts is to delete from the dictionary while iterating over it. For example:\r\n\r\n```\r\n>>> d = {\'a\': \'apple\', \'b\': \'blueberry\', \'c\': \'cinnamon\'}\r\n>>> for k in d:\r\n...     del d[k]\r\n...     print d\r\n... \r\n{\'c\': \'cinnamon\', \'b\': \'blueberry\'}\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: dictionary changed size during iteration\r\n```\r\n\r\nPython (at least in newer versions) is smart enough to prevent people from doing this. PyFITS Headers should do the same thing.\r\n\r\nOriginal ticket: https://aeon.stsci.edu/ssb/trac/pyfits/ticket/210'"
radio-astro-tools/spectral-cube#258,125001949,astrofrog,astrofrog,2016-01-05 16:36:06,2016-01-07 11:14:47,2016-01-07 11:14:47,closed,,,5,Ready for final review,https://api.github.com/repos/radio-astro-tools/spectral-cube/issues/258,b'Updated .travis.yml to use ci-helpers',
casacore/casacore#384,145137586,ygrange,juliantaylor,2016-04-01 09:50:40,2016-04-06 14:32:23,2016-04-06 14:32:23,closed,,,6,,https://api.github.com/repos/casacore/casacore/issues/384,"b""WCSlib 5.14 doesn't work with casacore""","b'Hi,\r\n\r\nIf I build casacore against wcslib 5.13, all tests succeeed. If I build it against 5.14 about 40 fail. According to the release notes 5.14 fixes a memory leak (not sure if casacore relies on this leak).\r\n\r\nHere is a list of tests that fail:\r\n\t438 - dCoordinates (Failed)\r\n\t439 - dRemoveAxes (Failed)\r\n\t440 - dWorldMap (Failed)\r\n\t442 - tCoordinateSystem (Failed)\r\n\t443 - tCoordinateUtil (Failed)\r\n\t444 - tDirectionCoordinate (Failed)\r\n\t445 - tFrequencyAligner (Failed)\r\n\t446 - tGaussianConvert (Failed)\r\n\t451 - tSpectralCoordinate (Failed)\r\n\t455 - dImageInterface (Failed)\r\n\t456 - dImageStatistics (Failed)\r\n\t457 - dImageSummary (Failed)\r\n\t459 - tExtendImage (Failed)\r\n\t460 - tFITSErrorImage (Failed)\r\n\t461 - tFITSExtImage (Failed)\r\n\t462 - tFITSExtImageII (Failed)\r\n\t463 - tFITSImage (Failed)\r\n\t465 - tFITSQualityImage (Failed)\r\n\t467 - tImageAttrHandler (Failed)\r\n\t469 - tImageConcat (Failed)\r\n\t470 - tImageEmpty (Failed)\r\n\t471 - tImageExpr (Failed)\r\n\t472 - tImageExpr2Gram (Failed)\r\n\t473 - tImageExpr3Gram (Failed)\r\n\t476 - tImageInfo (Failed)\r\n\t477 - tImageRegrid (Failed)\r\n\t478 - tImageStatistics (Failed)\r\n\t480 - tImageUtilities (Failed)\r\n\t481 - tLELSpectralIndex (Failed)\r\n\t483 - tPagedImage (Failed)\r\n\t484 - tPagedImage2 (Failed)\r\n\t485 - tRebinImage (Failed)\r\n\t486 - tSubImage (Failed)\r\n\t487 - tTempImage (Failed)\r\n\t488 - tImageRegion (Failed)\r\n\t490 - tWCBox (Failed)\r\n\t491 - tWCEllipsoid (Failed)\r\n\t492 - tWCExtension (Failed)\r\n\t493 - tWCLELMask (Failed)\r\n\t494 - tWCUnion (Failed)\r\n\r\nThis all for release version 2.0.3 but we have seen the same effect in 2.1.0 yesterday.'"
rootpy/rootpy#680,131062069,tunnell,ndawe,2016-02-03 15:41:06,2016-03-23 00:00:06,2016-03-23 00:00:06,closed,,,3,,https://api.github.com/repos/rootpy/rootpy/issues/680,b'Warning if import from Jupyter notebook',"b'I get the following warning if I import `rootpy` from an IPython notebook:\r\n\r\n```/Users/tunnell/anaconda3/envs/pax/lib/python3.4/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.\r\n  ""You should import from ipykernel or jupyter_client instead."", ShimWarning)```\r\n\r\nHere is the information on my setup:\r\n\r\n```\r\nThe version of the notebook server is 4.0.6 and is running on:\r\nPython 3.4.3 |Anaconda 2.4.0 (x86_64)| (default, Oct 20 2015, 14:27:51) \r\n[GCC 4.2.1 (Apple Inc. build 5577)]\r\n\r\nCurrent Kernel Information:\r\n\r\nPython 3.4.4 |Anaconda 2.4.0 (x86_64)| (default, Jan  9 2016, 17:30:09) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 4.0.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n%guiref   -> A brief reference about the graphical user interface.\r\n```'"
astropy/specutils#123,116009102,nhmc,,2015-11-10 02:00:27,2015-11-25 10:11:21,None,open,,,9,,https://api.github.com/repos/astropy/specutils/issues/123,b'Spectrum1DLookupWCS incorrectly returns NaN for the first dispersion pixel',"b'See example below. It must be related to how the lower limit is applied in np.interp.\r\n\r\n```python\r\n>>> import astropy.units as u\r\n>>> import numpy as np\r\n>>> from specutils import Spectrum1D\r\n\r\n>>> wave = np.arange(10.) * u.AA\r\n>>> flux = np.ones_like(wa)\r\n>>> spec = Spectrum1D.from_array(wave, flux)\r\n>>> spec.dispersion\r\n<Quantity [ nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.] Angstrom>\r\n```'"
numpy/numpy#6562,113284944,ahaldane,charris,2015-10-26 04:48:16,2015-11-17 22:20:49,2015-10-27 03:26:57,closed,,,6,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/6562,b'Disable view safety checks',"b'This PR disables the view safety checks that were causing problems in #6467, undoing some of the work from #5548. It goes back to the old behavior of disallowing any views/setfield involving object arrays.\r\n\r\nThis is better than reverting #5548, because it turns out there were other fixes in #5548 and subsequent PRs (eg #6208), plus in this PR, which fix most of the issues involving view safety without needing the safety checks. In fact, of the 5 issues fixed in #5548, only one needs to be re-opened due to this PR (#2599). (Edit: Actually #2346 needs to be re-opened too. Half of the examples there work now, but some are broken.)\r\n\r\nI also request a day or two to sit on this before merging, please! (Ping me if I am delaying the release)'"
numpy/numpy#6208,101069501,ahaldane,charris,2015-08-14 18:11:29,2015-10-27 22:13:21,2015-10-18 20:14:54,closed,,,15,10 - Maintenance;12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/6208,b'MAINT: Speedup field access by removing unneeded safety checks',"b""#5548 causes a big slowdown in the specific case of accessing/assigning fields of structured arrays with large (eg 1kb) dtypes. Oops! I noticed this while investigating #1984. The problem is #5548 implemented an algorithm which checks view safety byte-by-byte (slow) under the assumption that most dtypes are only a small number of bytes. That's bad if the dtype is very large, as in the test script of #1984.\r\n\r\nOne fix is to speed the safety-check algorithm up, but it also turns out that the checks aren't needed in many cases. For example, during field indexing they aren't needed because we're merely viewing fields that already exist, which is therefore safe. So I've bypassed the safety checks by rewriting that indexing code (in C) so it avoids using `PyArray_View`. \r\n\r\nI also made corresponding voidtype methods call the ndarray methods to get the same benefits. Incidentally this also enables some functionality that was missing from voidtype relative to ndarray. Eg, you can now do `arr[0][['a', 'b']]` and it will give you a more correct error message. \r\n\r\nSafety checks are still important for get/setfield and for views. But even there we can skip the checks early if we know that the datatypes aren't structured or aren't of object type. \r\n\r\nTiming info for the test script in #1984, run as `./test.py 10000`\r\n\r\nNumpy 1.9.2:\r\n\r\n    array,  B['a'][:] = A['a']      0.00852 seconds\r\n    array,  B['a']    = A['a']      0.00648 seconds\r\n    scalar, B['a'][:] = A['a']      0.016 seconds\r\n    scalar, B['a']    = A['a']      0.132 seconds\r\n\r\nMaster:\r\n\r\n    array,  B['a'][:] = A['a']      16.4 seconds\r\n    array,  B['a']    = A['a']      16.3 seconds\r\n    scalar, B['a'][:] = A['a']      16.1 seconds\r\n    scalar, B['a']    = A['a']      16.6 seconds\r\n\r\nThis PR:\r\n\r\n    array,  B['a'][:] = A['a']      0.00733 seconds\r\n    array,  B['a']    = A['a']      0.00673 seconds\r\n    scalar, B['a'][:] = A['a']      0.0171 seconds\r\n    scalar, B['a']    = A['a']      0.0151 seconds\r\n\r\n\r\nLater I'll look into a faster algorithm for the structured safety checks.\r\n\r\nI think this PR also fixes #1984, if 0.0151s is fast enough! \r\n\r\nBy the way, I think get/setfield are no longer used anywhere internally in numpy."""
numpy/numpy#6467,111188681,beckermr,charris,2015-10-13 14:12:34,2015-11-27 15:39:10,2015-10-27 20:05:56,closed,,,32,12 - Regression;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/6467,b'performance regression for record array access in numpy 1.10.1',"b'It appears that access numpy record arrays by field name is significantly slower in numpy 1.10.1. I have put below a simple example test that illustrates the issue. (I am aware that this particular example is much better accomplished by other means. The point is that array access is slow, not that this a representative problem.)\r\n\r\nThe test script is\r\n```python\r\n#!/usr/bin/env python\r\nimport os\r\nimport time\r\nimport sys\r\nimport numpy as\tnp\r\n\r\ndef test(N=100000,verbose=False):\r\n    d = np.zeros(1,dtype=[(\'col\',\'f8\')])\r\n\r\n    t0 = time.time()\r\n    for i in xrange(N):\r\n        d[\'col\'] +=\ti\r\n    t0 = time.time() - t0\r\n\r\n    if verbose:\r\n\tprint \'numpy version:\',np.version.version\r\n        print \'time: %g\' % t0\r\n\r\nif __name__ == ""__main__"":\r\n    if len(sys.argv) > 1:\r\n\tN = int(sys.argv[1])\r\n\ttest(N=N,verbose=True)\r\n    else:\r\n\ttest(verbose=True)\r\n```\r\n\r\nHere are the running times for different versions of numpy:\r\n\r\n```\r\nnumpy version: 1.9.3\r\ntime: 0.262786\r\n\r\nnumpy version: 1.10.1\r\ntime: 3.57254\r\n``` \r\n@esheldon has reproduced the relative timing differences on linux in addition to my tests which were with my mac. \r\n\r\nI profiled the code for v1.10.1 and found this\r\n```\r\n         3200006 function calls (3000006 primitive calls) in 4.521 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n   200000    1.386    0.000    1.883    0.000 _internal.py:372(_check_field_overlap)\r\n600000/400000    0.751    0.000    0.906    0.000 _internal.py:337(_get_all_field_offsets)\r\n        1    0.566    0.566    4.521    4.521 numpy_test.py:7(test)\r\n   200000    0.401    0.000    3.189    0.000 _internal.py:425(_getfield_is_safe)\r\n   200000    0.350    0.000    3.955    0.000 _internal.py:287(_index_fields)\r\n   200000    0.323    0.000    3.513    0.000 {method \'getfield\' of \'numpy.ndarray\' objects}\r\n   400000    0.279    0.000    0.279    0.000 {range}\r\n   400000    0.155    0.000    0.155    0.000 {method \'update\' of \'set\' objects}\r\n   400000    0.106    0.000    0.106    0.000 {method \'append\' of \'list\' objects}\r\n   200000    0.093    0.000    0.093    0.000 {isinstance}\r\n   200000    0.062    0.000    0.062    0.000 {method \'difference\' of \'set\' objects}\r\n   200000    0.048    0.000    0.048    0.000 {method \'extend\' of \'list\' objects}\r\n        1    0.000    0.000    0.000    0.000 {numpy.core.multiarray.zeros}\r\n        1    0.000    0.000    4.521    4.521 <string>:1(<module>)\r\n        2    0.000    0.000    0.000    0.000 {time.time}\r\n        1    0.000    0.000    0.000    0.000 {method \'disable\' of \'_lsprof.Profiler\' objects}\r\n ```\r\n\r\nIt appears that new code added at the python level for error checking is significantly degrading performance. \r\n'"
MSeifert04/astropy#1,107613853,MSeifert04,MSeifert04,2015-09-21 22:50:48,2015-09-27 21:19:28,2015-09-27 21:19:28,closed,,,0,,https://api.github.com/repos/MSeifert04/astropy/issues/1,b'NDArithmetic: Uncertainty propagation works even if one has no uncertainty',b'Test'
sunpy/sunpy#1532,101286698,wafels,ayshih,2015-08-16 18:19:14,2015-09-04 15:04:31,2015-09-02 16:15:56,closed,ayshih,,7,0.6.x;affects-released;Bug?;effort-medium;priority-high,https://api.github.com/repos/sunpy/sunpy/issues/1532,b'MapCube animation broken',"b""Keeping the astropy normalizer (the sunpy default for the map type below) leads to a TypeError when playing a mapcube peek animation.  Also, the map is not shown. If you change the plot settings to use a native matplotlib normalizer the peek animation does not error.  The code below illustrates the issue (using sunpy/master).\r\n```python\r\nimport sunpy.map\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.colors import PowerNorm\r\nm = sunpy.map.Map('/Users/ireland/sunpy/data/sample_data/ssw_cutout_20121030_153001_AIA_94_.fts')\r\n```\r\n\r\nThe following line works.  An image is shown.\r\n```python\r\nm.peek()\r\n````\r\n\r\nThe code errors but keeps playing when the play button is pressed.\r\n```python\r\nmc = sunpy.map.Map([m,m,m], cube=True)\r\nmc.peek()\r\nplt.show()\r\n```\r\n\r\nThis runs when the play button is pressed.\r\n```python\r\nm2 = sunpy.map.Map('/Users/ireland/sunpy/data/sample_data/ssw_cutout_20121030_153001_AIA_94_.fts')\r\nm2.plot_settings['norm'] = PowerNorm(0.5)\r\nmc2 = sunpy.map.Map([m2,m2,m2], cube=True)\r\nmc2.peek()\r\nplt.show()\r\n```\r\n\r\nThe third map generates the same error as mc.\r\n```python\r\nmc3 = sunpy.map.Map([m2,m2,m], cube=True) \r\nmc3.peek()\r\nplt.show()\r\n```"""
nanograv/PINT#29,62211243,paulray,paulray,2015-03-16 21:12:02,2015-12-04 14:30:45,2015-12-04 14:30:45,closed,,,6,,https://api.github.com/repos/nanograv/PINT/issues/29,b'Reading TOAs from a pickle file fails on Macs',"b'For some reason, the pickle files don\'t read on Macs.  The error looks like this:\r\n```\r\nINFO: Reading toas from \'examples/NGC6440E.tim.pickle.gz\'...\r\n [pint.toa]\r\nTraceback (most recent call last):\r\n  File ""examples/fit_NGC6440E.py"", line 18, in <module>\r\n    t = pint.toa.get_TOAs(timfile)\r\n  File ""/Users/paulr/src/PINT/pint/toa.py"", line 33, in get_TOAs\r\n    t = TOAs(timfile,usepickle=usepickle)\r\n  File ""/Users/paulr/src/PINT/pint/toa.py"", line 223, in __init__\r\n    self.read_toa_file(toafile, usepickle=usepickle)\r\n  File ""/Users/paulr/src/PINT/pint/toa.py"", line 518, in read_toa_file\r\n    tmp = cPickle.load(gzip.open(filename+ext, \'rb\'))\r\nValueError: non-string names in Numpy dtype unpickling\r\n```\r\n\r\nThis should be fixed, and a unit test designed specifically to test the TOA pickle/unpickle process should be added.  All other unit tests should _not_ use pickling.\r\n'"
,,,,,,,,,,,,,,
matplotlib/matplotlib#4855,98753395,mdboom,mdboom,2015-08-03 14:11:19,2015-12-31 19:02:19,2015-12-31 19:02:15,closed,mdboom,2.0 (style change major release),4,default changes,https://api.github.com/repos/matplotlib/matplotlib/issues/4855,b'Limit what `style.use` can affect?',"b'This might be a controversial proposal, so I thought I\'d get some discussion first.\r\n\r\nIn `astropy`, there is a ""house style"", which, surprising to me, included `interactive: True`.  I wouldn\'t have expected a style to affect the interactive setting (which I see more as a user preference than affecting the style of the output).  I\'m going to advocate that `astropy` removes that but relatedly...\r\n\r\nWould it make sense to filter out rcParams that don\'t really relate to style in `style.use`?  I realize this somewhat breaks DWIM, but it does prevent a certain kind of shooting oneself in the foot...'"
spacetelescope/asdf#166,98248856,mdboom,mdboom,2015-07-30 19:52:45,2015-07-30 20:49:21,2015-07-30 20:49:21,closed,,,0,,https://api.github.com/repos/spacetelescope/asdf/issues/166,b'Support all of the FITS WCS projections',b'Requires astropy/astropy#4021 in order to pass.'
spacetelescope/asdf-standard#107,98248198,mdboom,mdboom,2015-07-30 19:50:28,2015-07-30 20:05:45,2015-07-30 20:05:45,closed,,1.0,1,,https://api.github.com/repos/spacetelescope/asdf-standard/issues/107,b'Fix default values in projections',
spacetelescope/gwcs#14,96145626,nden,nden,2015-07-20 19:56:18,2015-07-23 18:35:21,2015-07-23 18:35:21,closed,,,6,,https://api.github.com/repos/spacetelescope/gwcs/issues/14,b'Refactor coordinate frames and wcs',"b""A lot in this PR is removing code. Based on feedback in the PIA workshop, it didn't seem like many were interested in spectral coordinate frames (similar to astropy.coordinates) or spectral coordinate objects (similar to SkyCoord objects). This PR removes spectral_builtin_frames. In addition representation.py was removed (it extended astropy.coordinates.representation.py to 2D frames) for the above reasons and because it is not strictly necessary. \r\nIn this PR spectral coordinates are simply quantities and astropy.coordinates classes are used only as reference frames in `CelestialFrame` .\r\n\r\nOther changes:\r\n\r\nCoordinate frames are now accessible as properties of the WCS object.\r\nTwo methods were added/modified to `CoordinateFrame` - `coordinates` and `transform_to`.\r\nFor example\r\n\r\n`w.frame.coordinates(x, y)` is equivalent to `w(x, y)` but returns a `SkyCoordObject` or a quantity with spectral units, depending on the frame.\r\nSimilarly `w.frame.transform_to(other, x, y)` performs the WCS transformation from `input_frame` to `other`.\r\n\r\nA WCS can be initialized now by passing a pipeline, i.e. a list of (frame, transform) tuples.\r\n\r\nFinally the separability of axes discussed in #13 , although not finalized, is also included here.\r\n\r\nThe `Getting Started` page was updated, I'm working on the rest of the docs. \r\nA question - should `CoordinateFrame` be renamed to `CoordinateAxes` or just `Axes`? And likewise for the rest of the frames (`CelestialFrame` --> `CelestialAxes`)? Any preferences?\r\n\r\n@mdboom @perrygreenfield @embray """
,,,,,,,,,,,,,,
astrofrog/wcsaxes#168,91083238,astrofrog,astrofrog,2015-06-25 21:40:45,2015-07-01 11:22:03,2015-07-01 11:22:02,closed,,,8,upstream-fix-required,https://api.github.com/repos/astrofrog/wcsaxes/issues/168,b'Use pytest-mpl',"b'This will need the 0.2 release of pytest-mpl to work. Will update .travis.yml once 0.2 is available.\r\n\r\n@cdeil, just pinging you so you can see it in use in a real package.'"
gammapy/gammapy#286,88012690,JonathanDHarris,cdeil,2015-06-13 14:33:10,2015-06-29 11:11:56,2015-06-29 08:46:21,closed,cdeil,0.3,23,feature,https://api.github.com/repos/gammapy/gammapy/issues/286,b'Add function to plot Fermi catalog light curves',"b""@cdeil \r\nHere's a first version for review.  I'm not that familiar with Python, so don't be afraid to be pedantic about:\r\n-naming conventions,\r\n-how / where I import things\r\n-comment styles\r\n-etc\r\n\r\nAt the moment the tool plots Fermi MET as the x-axis.  I don't know if there is a package default to use MJD or something else, in which case I can change it.  I'm also just showing the plot when you call the function, if you want an object returned then let me know, I might need an example of the format.\r\n\r\n"""
astrofrog/wcsaxes#168,91083238,astrofrog,astrofrog,2015-06-25 21:40:45,2015-07-01 11:22:03,2015-07-01 11:22:02,closed,,,8,upstream-fix-required,https://api.github.com/repos/astrofrog/wcsaxes/issues/168,b'Use pytest-mpl',"b'This will need the 0.2 release of pytest-mpl to work. Will update .travis.yml once 0.2 is available.\r\n\r\n@cdeil, just pinging you so you can see it in use in a real package.'"
numpy/numpy#4576,30635822,mhvk,charris,2014-04-01 20:41:01,2014-07-16 18:46:06,2014-04-04 15:45:10,closed,,,6,,https://api.github.com/repos/numpy/numpy/issues/4576,b'ENH: Ensure that repr and str work for MaskedArray non-ndarray bases',"b'(@charris: this is one of the corrections I mentioned in #3907 that we need to make to MaskedArray to get astropy\'s more complicated ndarray subclasses to be used as base classes.)\r\n\r\nCurrently, `repr(ma)` and `str(ma)` do not deal well with base classes that redefine `__repr__` to use a different way to print the class name, or that override `__setitem__` to do checks on values, respectively. This PR corrects this, by simply letting `__repr__` use `__class__.__name__` instead of trying to be smart in how to get it, and by ensuring that `__str__` inserts `masked_print_option` in an `ndarray` view of the object array that is created for string output (the current way `__str__` is handled is perhaps too clever a trick, but I\'m not quite sure yet how to make it safer; my PR should at least help a little.). \r\n\r\nSpecifically, currently, after defining:\r\n```\r\nimport numpy as np\r\n\r\nclass ComplicatedSubArray(np.ndarray):\r\n    def __new__(cls, arr):\r\n        return np.asanyarray(arr).view(cls)\r\n\r\n    def __str__(self):\r\n        return \'myprefix {0} mypostfix\'.format(\r\n            super(ComplicatedSubArray, self).__str__())\r\n\r\n    def __repr__(self):\r\n        # Return a repr that does not start with \'name(\'\r\n        return \'<{0} {1}>\'.format(self.__class__.__name__, self)\r\n\r\n    def __setitem__(self, item, value):\r\n        # this ensures direct assignment to masked_print_option will fail\r\n        if not isinstance(value, ComplicatedSubArray):\r\n            raise ValueError(""Can only set to MySubArray values"")\r\n        super(ComplicatedSubArray, self).__setitem__(item, value)\r\n\r\nxcsub = ComplicatedSubArray(np.arange(5))\r\nmxcsub1 = np.ma.masked_array(xcsub)\r\nmxcsub2 = np.ma.masked_array(xcsub, mask=[True, False, True, False, False])\r\n```\r\none gets\r\n```\r\nIn [15]: mxcsub1\r\nOut[15]: \r\nmasked_<ComplicatedSubArray myprefix [0 1 2 3 4] mypostfix>(data = myprefix [0 1 2 3 4] mypostfix,\r\n                                                            mask = False,\r\n                                                      fill_value = 999999)\r\nIn [16]: mxcsub2\r\nOut[16]: \r\n.\r\n.\r\n.\r\nValueError: Can only set to MySubArray values\r\n```\r\n\r\nWith the PR,\r\n```\r\nIn [2]: mxcsub1\r\nOut[2]: \r\nmasked_ComplicatedSubArray(data = myprefix [0 1 2 3 4] mypostfix,\r\n                           mask = False,\r\n                     fill_value = 999999)\r\nIn [3]: mxcsub2\r\nOut[3]: \r\nmasked_ComplicatedSubArray(data = myprefix [-- 1 -- 3 4] mypostfix,\r\n                           mask = [ True False  True False False],\r\n                     fill_value = 999999)\r\n```\r\n'"
numpy/numpy#6019,90928354,taldcroft,charris,2015-06-25 10:53:12,2015-10-05 16:09:11,2015-10-05 16:08:50,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/6019,b'Masked array repr fails for structured array with multi-dimensional column',"b'As first noted in https://github.com/astropy/astropy/issues/3877, there is a problem printing out a masked structured array with a multi-dimensional column where at least one of the mask values is True:\r\n```\r\nIn [33]: t_ma = np.ma.masked_array(data = [([1, 2, 3],)],\r\n             mask = [([False, True, False],)],\r\n       fill_value = ([999999, 999999, 999999],),\r\n            dtype = [(\'a\', \'<i8\', (3,))])\r\n\r\nIn [34]: print(t_ma[0])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-34-454d85211b19> in <module>()\r\n----> 1 print(t_ma[0])\r\n\r\n/Users/aldcroft/anaconda/envs/astropy/lib/python2.7/site-packages/numpy/ma/core.pyc in __str__(self)\r\n   5691         else:\r\n   5692             p = str(p)\r\n-> 5693         r = [(str(_), p)[int(_m)] for (_, _m) in zip(r, m)]\r\n   5694         return ""(%s)"" % "", "".join(r)\r\n   5695 \r\n\r\nTypeError: only length-1 arrays can be converted to Python scalars\r\n\r\nIn [35]: t_ma = np.ma.masked_array(data = [([1, 2, 3],)],\r\n             mask = [([False, False, False],)],\r\n       fill_value = ([999999, 999999, 999999],),\r\n            dtype = [(\'a\', \'<i8\', (3,))])\r\n\r\nIn [36]: print(t_ma[0])\r\n([1, 2, 3],)\r\n```\r\n'"
numpy/numpy#6094,95924438,astrofrog,ahaldane,2015-07-19 15:28:40,2015-11-26 17:27:24,2015-10-05 02:04:16,closed,,,13,11 - Bug;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/6094,b'BUG: Fixed a bug with string representation of masked structured arrays',"b""This is a fix for the issue reported in https://github.com/numpy/numpy/issues/6019 and includes a regression test.\r\n\r\nOne thing I am not sure about is what the string representation should look like if the column is e.g. 2-dimensional - at the moment it will include a line return, but I'm not sure if that's desirable? I could strip out the line returns in that case?"""
uqfoundation/dill#170,159137851,Lyalpha,Lyalpha,2016-06-08 11:08:44,2016-07-20 11:23:56,2016-06-20 17:25:30,closed,,,6,wontfix,https://api.github.com/repos/uqfoundation/dill/issues/170,b'AssertionError with trying to use dill on astropy compound model',"b'I am using [`astropy`\'s compound models](http://docs.astropy.org/en/stable/modeling/compound-models.html) (i.e. `g2` below) and attempting to pickle them with `dill`.\r\n\r\nThe python `pickle` module appears to pickle them successfully however `dill` gives an `AssertionError`:\r\n\r\n    from cStringIO import StringIO\r\n    #import cPickle as pickle     -> works with this line instead of dill\r\n    import dill as pickle\r\n    from astropy.modeling import models\r\n\r\n    s = StringIO()\r\n    g1 =  models.Gaussian1D()\r\n    pickle.dump(g1,s)\r\n    g2 =  models.Gaussian1D() + models.Gaussian1D()\r\n    pickle.dump(g2, s)\r\n\r\n```\r\nTraceback (most recent call last):\r\n    pickle.dump(g2, s)\r\n  File ""/usr/lib/python2.7/site-packages/dill/dill.py"", line 236, in dump\r\n    pik.dump(obj)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 224, in dump\r\n    self.save(obj)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 331, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 396, in save_reduce\r\n    save(cls)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/usr/lib/python2.7/site-packages/dill/dill.py"", line 1216, in save_type\r\n    obj.__bases__, _dict), obj=obj)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 401, in save_reduce\r\n    save(args)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 562, in save_tuple\r\n    save(element)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/usr/lib/python2.7/site-packages/dill/dill.py"", line 835, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 649, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 681, in _batch_setitems\r\n    save(v)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/usr/lib/python2.7/site-packages/dill/dill.py"", line 798, in save_function\r\n    obj.__dict__), obj=obj)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 405, in save_reduce\r\n    self.memoize(obj)\r\n  File ""/usr/lib64/python2.7/pickle.py"", line 244, in memoize\r\n    assert id(obj) not in self.memo\r\nAssertionError\r\n```\r\n\r\nI guess this may need knowledge from the `astropy` side, but since it worked with vanilla `pickle` I thought I\'d try here. The end game is using `lambda` functions within these models and so I need to use `dill` to serialize them.\r\n\r\n`python 2.7.5`\r\n`dill 0.2.5`\r\n`astropy 1.1.2`'"
numpy/numpy#5962,87791372,mhvk,charris,2015-06-12 17:10:29,2015-06-18 13:07:03,2015-06-18 03:16:51,closed,,,17,11 - Bug;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/5962,b'BUG Ensure masked object arrays can always return single items.',"b'For a masked array holding objects that themselves are arrays, when selecting a single item, it is treated as if it were a slice of the array and an attempt is made to set its mask.  This was always a bug, but it become visible with a recent change to `MaskedArray.__getitem__` (#4586) where it is attempted to change the shape of the mask (to help matrices, sigh, was not even part of PR initially!).  With this PR, the treatment is the same as for other single items: one either gets the array stored in the object, or masked.\r\n\r\nTest case that now works but used to fail:\r\n```\r\nmx1 = MaskedArray([1.], mask=[True])\r\nmx2 = MaskedArray_array([1., 2.])\r\nmx = MaskedArray([mx1, mx2], mask=[False, True])\r\nmx[0]\r\n```'"
numpy/numpy#5964,88058012,charris,charris,2015-06-13 20:15:34,2015-06-15 09:28:03,2015-06-14 15:54:43,closed,,,5,09 - Enhancement;component: numpy.core;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/5964,b'Fixup gh 5864',"b'Fix failing test in #5864 and make some style cleanups. Original message from Nathaniel follows.\r\n\r\nWe\'ve long had consensus that returning NotImplemented from ufuncs was a bad and ugly hack, since it violates layering -- NotImplemented is the responsibility of __op__ methods, not ufuncs. And one of the things that came out of the discussion in gh-5844 is that the NotImplemented handling inside the ufunc machinery was actually mostly useless and broken (see the first post on that report for analysis).\r\n\r\nOf course, it turns out it\'s not quite that simple -- there were some places, esp. in the trainwreck that is array_richcompare, which were depending on the current behavior.\r\n\r\nThis patch series fixes the main pieces of code that used to rely on this behavior, and then removes as much of the NotImplemented handling as it can. And for what isn\'t removed, it adds deprecation or future warnings.\r\n\r\nIn particular, there are a bunch of cases where array_richcompare returns bad and wrong results, like where you can compare two arrays (which should produce a boolean array), some failure occurs, and then the error gets thrown away and we end up returning a boolean scalar. Some of these were deprecated in 9b8f6c7, but this deprecates the rest. Probably the most controversial part of this is that this patch deprecates scalar ordering comparisons between unlike types, like np.float64(1) < ""foo"", which are already an error on py3 but have traditionally returned some semi-arbitrary value on py2. To convince you that this is broken and we should stop supporting it, check this out:\r\n\r\n# Note: the name of this class affects the results below\r\nIn [1]: class quux(object):\r\n   ...:     pass\r\n   ...: \r\n\r\nIn [2]: q = quux()\r\n\r\nIn [3]: np.string_(""foo"") < q\r\nOut[3]: True\r\n\r\nIn [4]: np.asarray(np.string_(""foo"")) < q\r\nOut[4]: False\r\n\r\nFor a full analysis of how we handle all the legacy comparison cases, see the long comment that starts here: https://github.com/numpy/numpy/blob/19b829a660a6adc8118dc427e1a7e4ba6946ccf7/numpy/core/src/umath/ufunc_object.c#L867\r\n\r\nNB this PR is probably easier to review if you read the commits one at a time in order, instead of jumping straight to the big diff.\r\n'"
numpy/numpy#5864,74993189,njsmith,charris,2015-05-10 20:16:17,2015-06-13 20:16:04,2015-06-13 20:16:04,closed,,,64,09 - Enhancement;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5864,b'Remove NotImplemented handling from the ufunc machinery (almost)',"b'We\'ve long had consensus that returning `NotImplemented` from ufuncs was a bad and ugly hack, since it violates layering -- `NotImplemented` is the responsibility of `__op__` methods, not ufuncs. And one of the things that came out of the discussion in gh-5844 is that the `NotImplemented` handling inside the ufunc machinery was actually mostly useless and broken (see the first post on that report for analysis).\r\n\r\nOf course, it turns out it\'s not *quite* that simple -- there were some places, esp. in the trainwreck that is `array_richcompare`, which were depending on the current behavior.\r\n\r\nThis patch series fixes the main pieces of code that used to rely on this behavior, and then removes as much of the `NotImplemented` handling as it can. And for what isn\'t removed, it adds deprecation or future warnings.\r\n\r\nIn particular, there are a bunch of cases where `array_richcompare` returns bad and wrong results, like where you can compare two arrays (which should produce a boolean array), some failure occurs, and then the error gets thrown away and we end up returning a boolean scalar. Some of these were deprecated in 9b8f6c72, but this deprecates the rest. Probably the most controversial part of this is that this patch deprecates scalar ordering comparisons between unlike types, like `np.float64(1) < ""foo""`, which are already an error on py3 but have traditionally returned some semi-arbitrary value on py2. To convince you that this is broken and we should stop supporting it, check this out:\r\n```\r\n# Note: the name of this class affects the results below\r\nIn [1]: class quux(object):\r\n   ...:     pass\r\n   ...: \r\n\r\nIn [2]: q = quux()\r\n\r\nIn [3]: np.string_(""foo"") < q\r\nOut[3]: True\r\n\r\nIn [4]: np.asarray(np.string_(""foo"")) < q\r\nOut[4]: False\r\n```\r\nFor a full analysis of how we handle all the legacy comparison cases, see the long comment that starts here: https://github.com/numpy/numpy/blob/19b829a660a6adc8118dc427e1a7e4ba6946ccf7/numpy/core/src/umath/ufunc_object.c#L867\r\n\r\nNB this PR is probably easier to review if you read the commits one at a time in order, instead of jumping straight to the big diff.'"
numpy/numpy#5947,85778119,ahaldane,charris,2015-06-06 16:31:34,2015-06-11 22:09:46,2015-06-11 22:09:46,closed,,,3,11 - Bug;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5947,b'BUG: make void-scalar getfield/setfield use ndarray methods',"b""This is a continuation of #5642.\r\n\r\nThis PR removes most of the logic in `voidtype_getfield` and `voidtype_setfield`. Instead they simply call the ndarray `getfield` and `setfield`. This solves bugs related to void-scalar assignment, #3126 and #3561. It also makes these functions safer since ndarray's get/setfield does safety checks to avoid segfaults involving object arrays.\r\n\r\nAdditional minor notes:\r\n   \r\nI changed the calling convention of `voidtype_getfield`. Previously it accepted `(dtype, offset, title)` and dropped the (optional) title. Now it expects only `(dtype, offset)`, just like ndarray getfield. This simplifies the `voidtype_getfield` code.\r\n\r\nI also removed code that does byteswapping in both getfield and setfield. After looking at it for a while, I am pretty convinced this code is unnecessary since in both cases we use` gentype_generic_method` to convert the void scalar to a 0-d ndarray, do the get/setfield, and then convert the returned value to a scalar using `PyArray_ToScalar` (in `PyArray_Return`). `PyArray_ToScalar` already does the byteswap for us. Therefore, any scalars that reach `voidtype_getfield` are already in NBO and there is no need to swap again.\r\n\r\nAlso, at the end of `voidtype_setfield` I effectively do `arr[()] = val`, and actually create an empty tuple object. Is that really the best way to use setfield, given that arr is possible 0-d?\r\n"""
numpy/numpy#5962,87791372,mhvk,charris,2015-06-12 17:10:29,2015-06-18 13:07:03,2015-06-18 03:16:51,closed,,,17,11 - Bug;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/5962,b'BUG Ensure masked object arrays can always return single items.',"b'For a masked array holding objects that themselves are arrays, when selecting a single item, it is treated as if it were a slice of the array and an attempt is made to set its mask.  This was always a bug, but it become visible with a recent change to `MaskedArray.__getitem__` (#4586) where it is attempted to change the shape of the mask (to help matrices, sigh, was not even part of PR initially!).  With this PR, the treatment is the same as for other single items: one either gets the array stored in the object, or masked.\r\n\r\nTest case that now works but used to fail:\r\n```\r\nmx1 = MaskedArray([1.], mask=[True])\r\nmx2 = MaskedArray_array([1., 2.])\r\nmx = MaskedArray([mx1, mx2], mask=[False, True])\r\nmx[0]\r\n```'"
numpy/numpy#5982,89417399,embray,charris,2015-06-18 22:57:01,2015-06-23 13:18:55,2015-06-22 23:44:55,closed,,,11,11 - Bug;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5982,b'BUG: Fixed slicing of chararrays on Python 3.',"b""When taking a slice of a chararray it was calling the rstrip() method\r\non the resulting slice, resulting in a new array rather than a view\r\nof the original.  This was an unintended consequence of the `sq_slice`\r\nmember of the `tp_as_sequence` mapping being ignored in Python 3, so\r\nthat slice lookups go directly through `__getitem__`.\r\n\r\nI realize the chararray type is deprecated now which is fine, but this bug\r\nintroduced a regression for the `astropy.io.fits` module on Python 3 (I'm not\r\nsure why this only seems to have broken recently, but it did).  So it can't hurt\r\nto fix this, and in the meantime I'm going to work on eliminating the dependency\r\non chararray in Astropy."""
numpy/numpy#5505,55436504,ahaldane,charris,2015-01-26 00:11:14,2015-01-27 01:52:24,2015-01-27 01:51:58,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/5505,b'BUG: Fix recarray getattr and getindex return types',"b'\r\nThis pull request was originally requested as https://github.com/numpy/numpy/pull/5454, but I moved it here because it was on the wrong branch, plus other reasons.\r\n\r\nThis pull request makes changes to `__getitem__` and `__getattr__` of recarrays. It makes three notable changes:\r\n\r\nA. recarrays no longer convert string ndarrays to chararrays, and instead simply return ndarrays of string type.\r\n\r\nThis was confusing, and led to bugs for anyone unaware of this special conversion (ie, me) since chararrays trim trailing whitespace but ndarrays fo string type do not, and because it only occured when the field was accessed by attribute (not index).\r\n\r\nOld behavior:\r\n\r\n    >>> rec = np.rec.array([(\'abc \', (1,1), 1), (\'abc\', (2,3), 1)],\r\n    ...       dtype=[(\'foo\', \'S4\'), (\'bar\', [(\'A\', int), (\'B\', int)]), (\'baz\', int)])\r\n    >>> rec.foo[0] == rec.foo[1]\r\n    True\r\n    >>> rec[\'foo\'][0] == rec[\'foo\'][1]\r\n    False\r\n\r\nNew behavior: Both lines return False.\r\n\r\nI think this is the main compatability risk in this pull request, as some people might have *expected* the whitespace removal. But I didn\'t see anything in a quick github search.\r\n\r\nB: the return type of fields accessed by index and by attribute was inconsistent for flexible types.\r\n\r\nPrevious behavior:\r\n\r\n    >>> type(rec.foo), type(rec[\'foo\'])\r\n    (numpy.core.defchararray.chararray, numpy.recarray)\r\n    >>> type(rec.bar), type(rec[\'bar\'])\r\n    (numpy.recarray, numpy.recarray)\r\n    >>> type(rec.baz), type(rec[\'baz\'])\r\n    (numpy.ndarray, numpy.ndarray)\r\n\r\nNew behavior:\r\n\r\n    >>> type(rec.foo), type(rec[\'foo\'])\r\n    (numpy.ndarray, numpy.ndarray)\r\n    >>> type(rec.bar), type(rec[\'bar\'])\r\n    (numpy.recarray, numpy.recarray)\r\n    >>> type(rec.baz), type(rec[\'baz\'])\r\n    (numpy.ndarray, numpy.ndarray)\r\n\r\nC. dtype.type is now inherited when fields of structured type are accessed\r\n\r\nOld Behavior:\r\n\r\n    >>> rec.dtype.type, rec.bar.dtype.type\r\n    (numpy.record, numpy.void)\r\n\r\nNew behavior:\r\n\r\n    >>> rec.dtype.type, rec.bar.dtype.type\r\n    (numpy.record, numpy.record)\r\n\r\nThis guarantees that if an array is a ""record"" array, structured fields will also be returned as ""record arrays"" (rather than merely views of np.recarray).\r\n\r\nI am planning two more recarray-related pull requests: One to fix `recarray.__repr__`, and another to reduce dependence on `numpy.rec.format_parser`, which duplicates logic in the descriptor.c parser.'"
numpy/numpy#5943,85577045,ahaldane,charris,2015-06-05 16:39:24,2015-06-19 21:55:20,2015-06-19 21:38:50,closed,,,16,11 - Bug;component: numpy.core;component: numpy.lib,https://api.github.com/repos/numpy/numpy/issues/5943,b'BUG: automatically convert recarray dtype to np.record',"b""As discussed in #3581, this PR automatically converts the dtype of `np.recarray`s to `np.record`. \r\n\r\nTo get unit tests to pass I also had to add a setattr to the `MaskedArray` class: MaskedArrays did not support assignment to dtype attribute, as demonstrated in the following examples:\r\n\r\n    >>> a = np.zeros(4, dtype='f4,i4')\r\n    >>> m = np.ma.array(a)\r\n    >>> m.dtype = np.dtype('f8')\r\n    >>> m   # Exception\r\n    >>> a.view(dtype='f8', type=np.ma.MaskedArray) #Exception\r\n\r\nMaskedArray.setattr now catches assignments to dtype and updates the mask accordingly.\r\n\r\n\r\nPossible issues with this PR, if anyone has any ideas: \r\n\r\n1) Viewing as `a.view(np.recarray)` now changes the dtype. It's not totally clear to me this won't break anything. (Example: It broke maskedarrays above, but due to a maskedaray bug).\r\n\r\n2) Another example of the last point: Views are not reversible.\r\n\r\n    >>> a = np.zeros(4, 'f4,i4')\r\n    >>> b = a.view(np.recarray).view(np.ndarray)\r\n    >>> b.dtype\r\n    dtype((numpy.record, [('f0', '<f4'), ('f1', '<i4')]))\r\n\r\n3) Note (not really a problem): Attempt to create record arrays of non-structured type does not set the dtype to `np.record` (since it is not possible to do so). This is not new to this PR.\r\n\r\n    >>> a = np.zeros(5, 'f8')\r\n    >>> np.rec.array(a)\r\n    array([ 0.,  0.,  0.,  0.], \r\n          dtype=float64).view(numpy.recarray)\r\n    \r\nBenefits:\r\n\r\n * Goal is to improve results in #3581\r\n\r\nI still need to finish writing unit tests"""
numpy/numpy#5962,87791372,mhvk,charris,2015-06-12 17:10:29,2015-06-18 13:07:03,2015-06-18 03:16:51,closed,,,17,11 - Bug;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/5962,b'BUG Ensure masked object arrays can always return single items.',"b'For a masked array holding objects that themselves are arrays, when selecting a single item, it is treated as if it were a slice of the array and an attempt is made to set its mask.  This was always a bug, but it become visible with a recent change to `MaskedArray.__getitem__` (#4586) where it is attempted to change the shape of the mask (to help matrices, sigh, was not even part of PR initially!).  With this PR, the treatment is the same as for other single items: one either gets the array stored in the object, or masked.\r\n\r\nTest case that now works but used to fail:\r\n```\r\nmx1 = MaskedArray([1.], mask=[True])\r\nmx2 = MaskedArray_array([1., 2.])\r\nmx = MaskedArray([mx1, mx2], mask=[False, True])\r\nmx[0]\r\n```'"
numpy/numpy#4586,30897875,mhvk,charris,2014-04-04 21:52:11,2015-06-04 00:54:36,2015-06-04 00:48:38,closed,charris,,20,10 - Maintenance;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/4586,"b'ENH: Let MaskedArray getter, setter respect baseclass overrides'","b""As is, `MaskedArray` uses explicity calls to `ndarray.__setitem__(self.data, indx, value)` and `ndarray.__getitem__(self.data, indx)` to access elements of its baseclass. This PR uses `self.data[indx]` instead, so that possible overrides of `__getitem__` and `__setitem__` are respected.\r\n\r\nThis is particularly useful for astropy's `Quantity` ndarray subclass, since it should check that anything written to it has the correct unit, and associate units for anything gotten from it.\r\n\r\n(Note: this includes #4585, as otherwise some of the tests would fail; I'll rebase once that is in.)"""
numpy/numpy#5921,81491583,embray,charris,2015-05-27 15:20:02,2015-06-02 21:45:10,2015-06-02 21:04:23,closed,,,7,11 - Bug;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5921,b'BUG: Further fixes to record and recarray getitem/getattr',"b""This is a followup to PR #5505, which didn't go quite far\r\nenough.  This fixes two issues in particular:\r\n\r\n1) The record class also needs an updated `__getitem__` that works analogously to its `__getattribute__` so that a nested record is returned as a `record` object and not a plain `np.void`.  In other words the old behavior is:\r\n\r\n```python\r\n>>> rec = np.rec.array([('abc ', (1,1), 1), ('abc', (2,3), 1)],\r\n...       dtype=[('foo', 'S4'), ('bar', [('A', int), ('B', int)]),\r\n('baz', int)])\r\n>>> rec[0].bar\r\n(1, 1)\r\n>>> type(rec[0].bar)\r\n<class 'numpy.record'>\r\n>>> type(rec[0]['bar'])\r\n<type 'numpy.void'>\r\n```\r\n\r\ndemonstrated inconsistency between `.bar` and `['bar']` on the record object.  The new behavior is:\r\n\r\n```python\r\n>>> type(rec[0]['bar'])\r\n<class 'numpy.record'>\r\n```\r\n\r\n2) The second issue is more subtle.  The fix to #5505 used the `obj.dtype.descr` attribute to create a new dtype of type `record`.  However, this does not recreate the correct type if the fields are not aligned.  To demonstrate:\r\n\r\n```python\r\n>>> dt = np.dtype({'C': ('S5', 0), 'D': ('S5', 6)})\r\n>>> dt.fields\r\ndict_proxy({'C': (dtype('S5'), 0), 'D': (dtype('S5'), 6)})\r\n>>> dt.descr\r\n[('C', '|S5'), ('', '|V1'), ('D', '|S5')]\r\n>>> new_dt = np.dtype((np.record, dt.descr))\r\n>>> new_dt\r\ndtype((numpy.record, [('C', 'S5'), ('f1', 'V1'), ('D', 'S5')]))\r\n>>> new_dt.fields\r\ndict_proxy({'f1': (dtype('V1'), 5), 'C': (dtype('S5'), 0), 'D':\r\n(dtype('S5'), 6)})\r\n```\r\n\r\nUsing the `fields` dict to construct the new type reconstructs the correct type with the correct offsets:\r\n\r\n```python\r\n>>> new_dt2 = np.dtype((np.record, dt.fields))\r\n>>> new_dt2.fields\r\ndict_proxy({'C': (dtype('S5'), 0), 'D': (dtype('S5'), 6)})\r\n```\r\n\r\n(Note: This is based on #5920 for convenience, but I could decouple the changes if that's preferable.)"""
sherpa/sherpa#27,81443894,DougBurke,olaurino,2015-05-27 13:23:09,2016-01-14 15:41:31,2016-01-08 19:31:14,closed,DougBurke,4.8.1,6,area:code;priority:medium;type:enhancement,https://api.github.com/repos/sherpa/sherpa/issues/27,b'pyfits/astropy backend is using deprecated functionality',"b'# Summary\r\n\r\nThe pyfits back end uses deprecated functions and classes - it includes `new_table`, `CardList`, and `append` but I have not done an exhaustive check. Once #6 is resolved we should address these issues.\r\n\r\nThe error, whereby the Chandra PHA file can not be written out is discussed in #46.\r\n\r\n# Details\r\n\r\nThe following test script:\r\n\r\n```\r\n% cat test_fits.py\r\n# does writing out a FITS file cause warnings?\r\nimport sherpa\r\nfrom sherpa.astro import ui\r\nprint(""*** Sherpa version: {}"".format(sherpa.__version__))\r\nui.load_arrays(1, [1,2,3], [4,5,6])\r\nui.save_data(1, \'test_fits.dat\', ascii=True, clobber=True)\r\nprint(""*** created test_fits.dat ***"")\r\nui.save_data(1, \'test_fits.fits\', ascii=False, clobber=True)\r\nprint(""*** created test_fits.fits ***"")\r\n```\r\n\r\ncauses the following messages when writing the FITS version of the data. \r\n\r\n## When using pyfits\r\n\r\nI meant to use the master branch, but used PR #26 by accident. As the code changes there are not functional, it is not the cause of the error (although the line numbering may be out slightly).\r\n\r\n```\r\n% conda list pyfits\r\n# packages in environment at /home/naridge/local/anaconda/envs/sherpa-pep8:\r\n#\r\nYou are using pip version 6.1.1, however version 7.0.1 is available.\r\nYou should consider upgrading via the \'pip install --upgrade pip\' command.\r\npyfits                    3.3.0                np18py27_1  \r\n\r\n% python test_fits.py \r\nWARNING: imaging routines will not be available, \r\nfailed to import sherpa.image.ds9_backend due to \r\n\'RuntimeErr: DS9Win unusable: Could not find ds9 on your PATH\'\r\nWARNING: failed to import sherpa.astro.xspec; XSPEC models will not be available\r\n*** Sherpa version: 4.7+375.g1693f34\r\n*** created test_fits.dat ***\r\n/home/naridge/local/anaconda/envs/sherpa-pep8/lib/python2.7/site-packages/sherpa/astro/io/pyfits_backend.py:877: PyfitsDeprecationWarning: The new_table function is deprecated as of version 3.3 and may be removed in a future version.\r\n\r\n        Use :meth:`BinTableHDU.from_columns` for new BINARY tables or :meth:`TableHDU.from_columns` for new ASCII tables instead.\r\n  tbl = pyfits.new_table(pyfits.ColDefs(collist))\r\n*** created test_fits.fits ***\r\n```\r\n\r\n## When using astropy\r\n\r\nThis is using PR #6 \r\n\r\n```\r\n% conda list astropy\r\n# packages in environment at /home/naridge/local/anaconda/envs/sherpa-astropy:\r\n#\r\nYou are using pip version 6.1.1, however version 7.0.1 is available.\r\nYou should consider upgrading via the \'pip install --upgrade pip\' command.\r\nastropy                   1.0.2                np19py27_0  \r\n\r\n% python test_fits.py \r\nWARNING: imaging routines will not be available, \r\nfailed to import sherpa.image.ds9_backend due to \r\n\'RuntimeErr: DS9Win unusable: Could not find ds9 on your PATH\'\r\nWARNING: failed to import sherpa.astro.xspec; XSPEC models will not be available\r\n*** Sherpa version: 4.7+2.gffe559b\r\n*** created test_fits.dat ***\r\nWARNING: AstropyDeprecationWarning: The new_table function is deprecated and may be removed in a future version.\r\n        Use :meth:`BinTableHDU.from_columns` for new BINARY tables or :meth:`TableHDU.from_columns` for new ASCII tables instead. [astropy.utils.decorators]\r\n*** created test_fits.fits ***\r\n```\r\n\r\n## Other deprecation warnings\r\n\r\nNote that I\'ve seen other warnings from the pyfits back-end. For instance, if I load in a CIAO PHA file, then I get - this is from the `astropy` branch, but you see essentially the same from pyfits:\r\n\r\n - deprecation warning from the CardList class\r\n\r\n - deprecation warning from the append function\r\n\r\n - the error message is being tracked in a separate bug, #46\r\n\r\n```\r\nIn [1]: from sherpa.astro import ui\r\nWARNING: imaging routines will not be available, \r\nfailed to import sherpa.image.ds9_backend due to \r\n\'RuntimeErr: DS9Win unusable: Could not find ds9 on your PATH\'\r\nWARNING: failed to import sherpa.astro.xspec; XSPEC models will not be available\r\n\r\nIn [2]: ui.load_pha(1, \'grp.pi\')\r\nWARNING: [Errno 2] No such file or directory: \'srcs_src1.corr.arf\'\r\nWARNING: [Errno 2] No such file or directory: \'srcs_src1.rmf\'\r\n\r\nIn [3]: ui.save_data(\'test.fits\', ascii=False, clobber=True)\r\nWARNING: AstropyDeprecationWarning: The CardList class has been deprecated; all its former functionality has been subsumed by the Header class, so CardList objects should not be directly created.  See the PyFITS 3.1.0 CHANGELOG for more details. [astropy.io.fits.card]\r\nWARNING: AstropyDeprecationWarning: The append function is deprecated and may be removed in a future version.\r\n        Use :meth:`Header.append` instead. [astropy.utils.decorators]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-0a6e7bbb21bb> in <module>()\r\n----> 1 ui.save_data(\'test.fits\', ascii=False, clobber=True)\r\n\r\n<string> in save_data(id, filename, bkg_id, ascii, clobber)\r\n\r\n/home/naridge/local/anaconda/envs/sherpa-astropy/lib/python2.7/site-packages/sherpa/astro/ui/utils.pyc in save_data(self, id, filename, bkg_id, ascii, clobber)\r\n   3215 \r\n   3216         try:\r\n-> 3217             sherpa.astro.io.write_pha(filename, d, ascii, clobber)\r\n   3218         except IOErr:\r\n   3219             try:\r\n\r\n/home/naridge/local/anaconda/envs/sherpa-astropy/lib/python2.7/site-packages/sherpa/astro/io/__init__.pyc in write_pha(filename, dataset, ascii, clobber)\r\n    398     data, col_names, hdr = _pack_pha( dataset )\r\n    399     backend.set_pha_data(filename, data, col_names, hdr, ascii=ascii,\r\n--> 400                          clobber=clobber)\r\n    401 \r\n    402 def pack_table(dataset):\r\n\r\n/home/naridge/local/anaconda/envs/sherpa-astropy/lib/python2.7/site-packages/sherpa/astro/io/pyfits_backend.pyc in set_pha_data(filename, data, col_names, header, ascii, clobber, packup)\r\n    898         if header[key] is None:\r\n    899             continue\r\n--> 900         hdrlist.append(pyfits.Card( str(key.upper()), header[key] ))\r\n    901 \r\n    902     collist = []\r\n\r\n/home/naridge/local/anaconda/envs/sherpa-astropy/lib/python2.7/site-packages/astropy/io/fits/card.pyc in __init__(self, keyword, value, comment, **kwargs)\r\n    446                 self.keyword = keyword\r\n    447             if value is not None:\r\n--> 448                 self.value = value\r\n    449 \r\n    450         if comment is not None:\r\n\r\n/home/naridge/local/anaconda/envs/sherpa-astropy/lib/python2.7/site-packages/astropy/io/fits/card.pyc in value(self, value)\r\n    568                                  (float, complex, bool, Undefined, np.floating,\r\n    569                                   np.integer, np.complexfloating, np.bool_)):\r\n--> 570             raise ValueError(\'Illegal value: %r.\' % value)\r\n    571 \r\n    572         if isinstance(value, float) and (np.isnan(value) or np.isinf(value)):\r\n\r\nValueError: Illegal value:   This FITS file may contain long string keyword values that are\r\n  continued over multiple keywords.  The HEASARC convention uses the &\r\n  character at the end of each substring which is then continued\r\n  on the next keyword which has the name CONTINUE.\r\nspecextract version 18 September 2014.\r\n\r\n```'"
liberfa/erfa#28,81840156,sergiopasra,eteq,2015-05-28 09:11:01,2015-05-29 11:09:05,2015-05-28 21:28:27,closed,,,7,,https://api.github.com/repos/liberfa/erfa/issues/28,b'Please release erfa-1.2',"b'Given that astropy is going to update its copy of erfa to include 2015-Jun-30 leap second (astropy/astropy/pull/3794), we need a release of erfa with (at least) the same change. Package managers that use system erfa will need it.\r\n\r\nIt seems that the after the last update of the library (#26 ) it was never released'"
liberfa/erfa#26,58944975,timj,eteq,2015-02-25 18:10:24,2015-03-02 23:40:20,2015-02-27 22:10:09,closed,,,23,,https://api.github.com/repos/liberfa/erfa/issues/26,b'Update to SOFA 2015-02-09',"b'Syncs up ERFA with release 2015-02-09 of SOFA.\r\n\r\nI have attempted to retain the local patch to `dat.c`.\r\n\r\nThe change looks larger than it really is because of the change in copyright date.\r\n\r\nSOFA release notes:\r\n\r\n- [New Material] 2 routines, iauG2icrs and iauIcrs2g, have been released for the new Galactic Coordinates section. They allow transformations to be made between the 1958 IAU galactic coordinate system and the ICRS.\r\n- [Update/Change] Updated iauDat to account for the leap second at the end of June 2015.\r\n- [Update/Change] Changes to iauC2ixys, iauGc2gde, iauRm2v and iauRv2m. There was a test for zero that could potentially cause compiler warnings. The test has been changed.\r\n- [Update/Change] Changes to sofa.h. Addition of the new functions, removal of duplicate functions. All functions are listed in the categories that are given on the website and in the manual.\r\n- [Update/Change] Update to t_sofa_c.c to accommodate the new routines.'"
radio-astro-tools/radio_beam#16,100856904,e-koch,keflavich,2015-08-13 20:05:29,2015-08-13 20:23:53,2015-08-13 20:23:18,closed,,,1,,https://api.github.com/repos/radio-astro-tools/radio_beam/issues/16,b'Update kernel with astropy 1.0.3 changes',b'[Kernel normalization](https://github.com/astropy/astropy/pull/3747) was changed in astropy 1.0.3. ```EllipticalGaussian2DKernel``` needed to be updated accordingly.'
spacetelescope/PyFITS#102,86275455,bgarwood,,2015-06-08 18:04:19,2015-08-11 22:12:36,None,open,,,17,Bug,https://api.github.com/repos/spacetelescope/PyFITS/issues/102,b'problem changing random group parameter values',"b'I have a file with a random groups PHDU.  I\'d like to change some of the values in that file and have those value stay changed in the file on disk.  But when I attempt it via several different ways, the parameter files remain unchanged on disk.  I can change the data array values (in one group, multiple groups, one or more elements in the array in one group, etc) without any problem.\r\n\r\nIs this a bug, feature, or am I just not doing it right.\r\n\r\ne.g. one parameter is ""RA""\r\n\r\n```\r\nf = pyfits.open(\'myfile.fits\',mode=\'update\')\r\n# this gets the entire parameter vector just fine\r\nra = f[0].data.field(\'RA\') \r\nra[0] = 0.5\r\n# This doesn\'t work\r\nf[0].data.field[\'RA\'] = ra\r\n```\r\n\r\nOther things that also don\'t work:\r\n```\r\nf[0].data[0][\'RA\'] = 0.5\r\nf[0].data.field(\'RA\')[0] = 0.5\r\nf[0].data.field(\'RA\')[:] = ra\r\n```\r\n\r\nAll of the above change the value in memory so that f[0].data[0] shows a value of 0.5 after each of those attempts.  But it\'s never written back to disk.  Explicitly doing f.flush() doesn\'t help.  Using f.writeto(\'newfile.fits\') also doesn\'t write out those values to the new file - the values found in the original myfile.fits are just copied over.  Equivalent operations on the data array associated with group 0 all work and change the value on disk without any problem.\r\n\r\nThanks.'"
mperrin/poppy#130,117699866,mperrin,mperrin,2015-11-18 22:46:28,2015-11-20 17:36:35,2015-11-19 20:34:08,closed,,0.4,6,,https://api.github.com/repos/mperrin/poppy/issues/130,b'figure out why test_multiprocessing is being skipped unnecessarily',"b'Moving this from email to a github issue: \r\n\r\n@mperrin wrote:\r\nSo I am double checking through the test suite, and I\xa1\xafve noticed that the test_multiprocessing function is getting skipped even when we are using Python 3.4, both on Travis and on my local machine.  See output below. \r\n\r\nThe code to check version and skip looks fine: \r\n```\r\n    @pytest.mark.skipif( (sys.version_info < (3,4,0) ),\r\n            reason=""Python 3.4 required for reliable forkserver start method"")\r\n```\r\n\r\nI\xa1\xafm testing on python 3.4.3 so I don\xa1\xaft understand why this is getting skipped. I verified that sys.version_info < (3,4,0)  is False, so I\xa1\xafm stumped. Any ideas? \r\n\r\n@josePhoenix replied:\r\nI think it\'s being skipped because you don\'t have the pytest package installed in your 3.4 environment. Probably the same thing on Travis. I\'m surprised it shows up as skipped, when if it were executing as a ""real"" script it would never enter the ""if _HAVE_PYTEST:"" block in test_multiprocessing.py.\r\n\r\nWait, no, it\'s the \'remote_data\' helper\xa1\xad I think\xa1\xad'"
mperrin/poppy#23,51823236,josePhoenix,mperrin,2014-12-12 16:17:54,2015-04-22 00:35:51,2015-04-22 00:35:51,closed,,0.3.5,12,bug,https://api.github.com/repos/mperrin/poppy/issues/23,"b'Under certain (unclear) conditions, use_multiprocessing causes calculation to hang'","b'For some reason, this sample code (using WebbPSF) causes a hang on my Mac Pro but not the science5 server or my personal machine (MacBook Air). Needs a better test case that uses only POPPY.\r\n\r\n(To avoid the sometimes unusual behavior of multiprocessing + the interactive interpreter, I saved this as a separate script and ran it from the command line.)\r\n\r\n```python\r\nimport webbpsf\r\nimport poppy\r\nnc = webbpsf.NIRCam()\r\nnc.filter = \'F150W2\'\r\npoppy.conf.use_fftw = False\r\npoppy.conf.use_multiprocessing = True\r\npoppy.conf.n_processes = 8\r\nnc.calcPSF()\r\n```\r\n\r\nRunning this prints out logging output through ""Propagating wavelength..."" lines for the first 20 of 40 wavelengths, and then nothing happens.\r\n\r\nIf I interrupt the script with Ctrl-C, I get tracebacks from all the running workers interleaved (to be expected, but hard to interpret) and it looks like they\'re all waiting on ""task = get()"" which in turn is waiting\r\non ""racquire()"" which seems to acquire a lock on a pipe.'"
mperrin/poppy#170,159676140,mperrin,,2016-06-10 16:45:41,2016-07-12 21:00:31,None,open,,,5,,https://api.github.com/repos/mperrin/poppy/issues/170,b'issues with astropy-helpers',"b'splitting off this topic from https://github.com/mperrin/poppy/issues/160#issuecomment-225229716\r\n\r\nHi @eteq, @josePhoenix can give more of the details, but I think it\'s a bit of both. \r\n\r\nFor literally years, every so often I\'ve encountered some random and hard-to-debug failures which eventually traced back to ""the astropy helpers are doing something unexpected behind the scenes"":\r\n\r\n* https://github.com/astropy/package-template/issues/106\r\n* https://github.com/astropy/astropy/issues/2502\r\n* https://github.com/astropy/astropy/pull/3713\r\n* and now here this new ""the astropy helpers are using version 2.7 of py.test even though I\'ve installed 2.8""\r\n* which is possibly related to why we\'re getting consistent failures on Travis on python 3.5, even though all tests pass on 3.5 locally. https://github.com/mperrin/poppy/issues/159\r\n\r\nEach of the above has led to time-consuming debugging to track down what\'s going on, which has been a time sink and distraction from the main tasks we\'re actually trying to get done. I readily admit that software is hard, and we can\'t expect a bug-free existence or anything close to it. But overall given the limited functionality we\'re actually gaining from the astropy helpers (mostly some setup.py tweaks and the sphinx extensions), it seems like the overall return on time invested is negative.  '"
numpy/numpy#5752,66695318,josePhoenix,charris,2015-04-06 20:15:42,2016-04-27 04:24:02,2015-04-22 16:29:54,closed,,,20,,https://api.github.com/repos/numpy/numpy/issues/5752,"b'NumPy ""dot"" hangs when used with multiprocessing (potentially Apple Accelerate related?)'","b""I'm having a devil of a time making a minimal test case, but this seems to be my issue: http://stackoverflow.com/questions/23963997/python-child-process-crashes-on-numpy-dot-if-pyside-is-imported\r\n\r\nIn my case, I have code that farms out a bunch of calculations, including matrix products to a `multiprocessing.Pool` with `pool.map`. The computation hangs partway through, and some hacky `print`-based debugging shows it hanging on a call to `np.dot` down in the guts of the program. \r\n\r\nReplacing `pool.map` with the built-in (serial) `map` makes everything work.\r\n\r\nI used not to have this issue, then something changed (lunar eclipse?) and now my computation hangs consistently whenever multiprocessing is used. A minimal test case continues to elude me. (It's not enough to simply generate 10 random NxN arrays and dot them in a `multiprocessing`-based way.)\r\n\r\n```\r\n>>> numpy.show_config()\r\nlapack_opt_info:\r\n    extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\r\n    extra_compile_args = ['-msse3', '-DAPPLE_ACCELERATE_SGEMV_PATCH']\r\n    define_macros = [('NO_ATLAS_INFO', 3)]\r\nopenblas_lapack_info:\r\n  NOT AVAILABLE\r\natlas_3_10_blas_threads_info:\r\n  NOT AVAILABLE\r\natlas_threads_info:\r\n  NOT AVAILABLE\r\natlas_3_10_threads_info:\r\n  NOT AVAILABLE\r\natlas_blas_info:\r\n  NOT AVAILABLE\r\natlas_3_10_blas_info:\r\n  NOT AVAILABLE\r\natlas_blas_threads_info:\r\n  NOT AVAILABLE\r\nopenblas_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblas_opt_info:\r\n    extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\r\n    extra_compile_args = ['-msse3', '-DAPPLE_ACCELERATE_SGEMV_PATCH', '-I/System/Library/Frameworks/vecLib.framework/Headers']\r\n    define_macros = [('NO_ATLAS_INFO', 3)]\r\natlas_info:\r\n  NOT AVAILABLE\r\natlas_3_10_info:\r\n  NOT AVAILABLE\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nmkl_info:\r\n  NOT AVAILABLE\r\n```"""
spacetelescope/PyFITS#99,64631480,mauriceleutenegger,,2015-03-26 20:57:53,2015-03-27 22:41:06,None,open,,,6,Bug;effort-medium;package-expert,https://api.github.com/repos/spacetelescope/PyFITS/issues/99,b'boolean mask issue with FITS_rec created using from_columns ()',"b'Suppose I want to merge two FITS_rec that have identical dtype, and then do a normal boolean mask operation on that :\r\n\r\n```\r\nimport pyfits as pf\r\ndata1 = pf.getdata (myfile, 1)\r\ndata2 = pf.getdata (myfile, 2)\r\nnrows = data1.size + data2.size\r\nmergeddata = pf.FITS_rec.from_columns (data1, nrows=nrows)\r\nmergeddata[data1.size:] = data2\r\nmymask = (mergeddata.field (\'myfield\') > somecriterion)\r\nmaskeddata = mergeddata [mymask]\r\n```\r\n\r\nThis gives an error message:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/mleutene/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pyfits/fitsrec.py"", line 496, in __getitem__\r\n    arrays.append(self._coldefs._arrays[idx][key])\r\nValueError: too many boolean indices\r\n```\r\n\r\nIf I limit the number of values in the mask to the number of rows in the first of the two merging tables, the error message goes away, even though the size of the merged FITS_rec is the same as the size of the mask:\r\n\r\n```\r\nmaskeddata = mergeddata [mymask[:data1.size]]\r\n```\r\n\r\nThis implies that somehow the new FITS_rec created using from_columns () remembers the size of the parent FITS_rec, even though its size is the expected sum of the sizes of the two FITS_recs that were merged.'"
astropy/astroquery#503,64561315,mdboom,keflavich,2015-03-26 15:43:02,2015-03-27 20:36:49,2015-03-26 20:17:45,closed,,,4,,https://api.github.com/repos/astropy/astroquery/issues/503,b'Monkey patch astropy so that configuration system works',b'Ugh.  Embarrassing.\r\n\r\n@embray: I guess I should put the equivalent in the package template?'
matplotlib/matplotlib#4274,64261825,mdboom,tacaswell,2015-03-25 12:53:42,2015-11-09 02:35:36,2015-04-12 00:00:17,closed,,v1.5.0,1,text/unicode,https://api.github.com/repos/matplotlib/matplotlib/issues/4274,b'Fix Angstrom issues',"b'This is a fix for an issue that started in astropy over here: https://github.com/astropy/astropy/issues/3617\r\n\r\nThe gist of it is that there is no good way to represent an Angstrom symbol () in ""pure"" LaTeX (meaning no extra packages), mathjax and matplotlib.\r\n\r\nThe example string `$\\mathring{A}  \\stackrel{\\circ}{A}  \\AA$` renders like:\r\n\r\nLaTeX:\r\n\r\n![latex](https://cloud.githubusercontent.com/assets/38294/6824835/f9f54816-d2cb-11e4-8d12-0d4c51b21b0b.png)\r\n\r\nMathJax:\r\n\r\n![mathjax](https://cloud.githubusercontent.com/assets/38294/6824838/feb3af6e-d2cb-11e4-8776-52871ca91108.png)\r\n\r\nmatplotlib (prior to this PR):\r\n\r\n![matplotlib](https://cloud.githubusercontent.com/assets/38294/6824840/059a20ba-d2cc-11e4-811e-2031369ec260.png)\r\n\r\nWith this PR, we add support for the `\\mathring` command, and also fix `\\AA` so it is italic in order to match LaTeX.\r\n\r\nmatplotlib, this PR, computer modern:\r\n\r\n![mathtext_cm_68](https://cloud.githubusercontent.com/assets/38294/6824862/3790de6a-d2cc-11e4-9bf1-f9a0f04a6cb2.png)\r\n\r\nmatplotlib, this PR, STIX:\r\n\r\n![mathtext_stix_68](https://cloud.githubusercontent.com/assets/38294/6824867/43897880-d2cc-11e4-80d0-896a03c579c2.png)\r\n\r\nmatplotlib, this PR, STIX Sans (note that the STIX fonts don\'t have a sans-serif version of the Angstrom symbol.  This isn\'t really a limitation we can address on the matplotlib side other than by some sort of special casing -- and there\'s a lot of symbols like that to special case):\r\n\r\n![mathtext_stixsans_68](https://cloud.githubusercontent.com/assets/38294/6824878/5cab6076-d2cc-11e4-86b0-11bf46570d60.png)\r\n'"
zblz/naima#73,60859114,cdeil,zblz,2015-03-12 17:27:25,2015-08-14 18:19:17,2015-08-03 15:32:09,closed,,,10,,https://api.github.com/repos/zblz/naima/issues/73,"b""Don't use pickle""","b""I think `naima` shouldn't use `pickle` ... instead of giving reasons I'll just point to this talk:\r\nhttps://www.youtube.com/watch?v=7KnfGDajDQw\r\n\r\nCVS, FITS, JSON and YAML, even XML or HDF5 are all better options any time you store and possibly exchange data or results."""
numpy/numpy#5590,58440260,chebee7i,charris,2015-02-21 03:50:34,2015-03-31 13:41:37,2015-02-23 00:55:41,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/5590,b'Make allclose use isclose.',"b""Presently, `allclose` does not use `isclose`, preferring instead to use to `_allclose_points`, which barely differs from `isclose`. One of the subtle differences is that `isclose` has a bug that was previously fixed in #2280 for `allclose`. Similarly, `allclose` doesn't work nicely with `MaskedArray`, while `isclose` does. This PR fixes both issues. Finally, `allclose` doesn't have the option to compare NaNs as equal. This is now possible."""
spacetelescope/asdf#87,66927444,nden,mdboom,2015-04-07 16:17:37,2015-04-07 18:44:23,2015-04-07 18:44:23,closed,mdboom,,4,,https://api.github.com/repos/spacetelescope/asdf/issues/87,b'compound transforms loose attributes',"b""This only happens with compound transforms. Attributes like `name` and `inverse` are lost.\r\n```\r\noffx = models.Shift(1)\r\nscl = models.Scale(2)\r\nmodel = (offx | scl).rename('compound_model')\r\nf = AsdfFile()\r\nf.tree['model'] = model\r\nf.write_to('test.asdf')\r\nf1 = AsdfFile.read('test.asdf')\r\nf1.tree['model'].name\r\nmodel.name\r\nOut[97]: 'compound_model'\r\n```"""
mperrin/poppy#78,60234835,josePhoenix,josePhoenix,2015-03-08 01:08:27,2015-11-02 19:25:21,2015-03-08 02:52:56,closed,,,6,,https://api.github.com/repos/mperrin/poppy/issues/78,b'Add minimum versions (targeting Spring 2015 SSBrel) for setup',"b'A bug in the Astropy helpers prevented automatic dependency resolution for `pip install poppy`. This was fixed in a patch release to Astropy, so we can avoid it by requiring `astropy>=1.0.1`. \r\n\r\nWe should also update our copies of the various astropy package template and helper files.\r\n\r\nSee astropy/astropy#3541 for additional details.'"
numpy/numpy#2434,7731448,numpy-gitbot,,2012-10-19 22:26:52,2016-07-12 11:40:55,None,open,,,24,11 - Bug;component: Other;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2434,b'numpy installation thru install_requires directive issue (Trac #1841)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/1841 on 2011-05-25 by trac user ohe, assigned to unknown._\n\nHello there,\n\nIn a virtualenv, with python v2.7.1, i can\'t install numpy thru the install_requires directive of setuptools/distribute.\n\n\n\n    from setuptools import setup\n    setup(...\n          install_requires=[\'numpy\'],\n          ...)\n\n\n\nA `python setup.py install` gives me the following traceback :\n\n\n\n    running install\n    running bdist_egg\n    running egg_info\n    writing requirements to UNKNOWN.egg-info/requires.txt\n    writing UNKNOWN.egg-info/PKG-INFO\n    writing top-level names to UNKNOWN.egg-info/top_level.txt\n    writing dependency_links to UNKNOWN.egg-info/dependency_links.txt\n    reading manifest file \'UNKNOWN.egg-info/SOURCES.txt\'\n    writing manifest file \'UNKNOWN.egg-info/SOURCES.txt\'\n    installing library code to build/bdist.macosx-10.6-intel/egg\n    running install_lib\n    warning: install_lib: \'build/lib\' does not exist -- no Python modules to install\n    \n    creating build/bdist.macosx-10.6-intel\n    creating build/bdist.macosx-10.6-intel/egg\n    creating build/bdist.macosx-10.6-intel/egg/EGG-INFO\n    copying UNKNOWN.egg-info/PKG-INFO -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\n    copying UNKNOWN.egg-info/SOURCES.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\n    copying UNKNOWN.egg-info/dependency_links.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\n    copying UNKNOWN.egg-info/requires.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\n    copying UNKNOWN.egg-info/top_level.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\n    zip_safe flag not set; analyzing archive contents...\n    creating \'dist/UNKNOWN-0.0.0-py2.7.egg\' and adding \'build/bdist.macosx-10.6-intel/egg\' to it\n    removing \'build/bdist.macosx-10.6-intel/egg\' (and everything under it)\n    Processing UNKNOWN-0.0.0-py2.7.egg\n    Copying UNKNOWN-0.0.0-py2.7.egg to /Users/olivier/tc/karmatest/lib/python2.7/site-packages\n    Adding UNKNOWN 0.0.0 to easy-install.pth file\n    \n    Installed /Users/olivier/tc/karmatest/lib/python2.7/site-packages/UNKNOWN-0.0.0-py2.7.egg\n    Processing dependencies for UNKNOWN==0.0.0\n    Searching for numpy\n    Reading http://pypi.python.org/simple/numpy/\n    Reading http://numpy.scipy.org\n    Reading http://sourceforge.net/project/showfiles.php?group_id=1369&package_id=175103\n    Reading http://numeric.scipy.org\n    Best match: numpy 1.6.0\n    Downloading http://sourceforge.net/projects/numpy/files/NumPy/1.6.0/numpy-1.6.0.tar.gz/download\n    Processing download\n    Running numpy-1.6.0/setup.py -q bdist_egg --dist-dir /var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/egg-dist-tmp-Zr2lMr\n    Running from numpy source directory.Warning: distutils distribution has been initialized, it may be too late to add a subpackage commandWarning: distutils distribution has been initialized, it may be too late to add a subpackage fcompilernon-existing path in \'/private/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils\': \'site.cfg\'\n    Warning: distutils distribution has been initialized, it may be too late to add a subpackage distutilsWarning: distutils distribution has been initialized, it may be too late to add a subpackage testingWarning: distutils distribution has been initialized, it may be too late to add a subpackage f2pyWarning: distutils distribution has been initialized, it may be too late to add an extension _sortWarning: distutils distribution has been initialized, it may be too late to add an extension multiarrayWarning: distutils distribution has been initialized, it may be too late to add an extension umathWarning: distutils distribution has been initialized, it may be too late to add an extension scalarmathWarning: distutils distribution has been initialized, it may be too late to add an extension _dotblasWarning: distutils distribution has been initialized, it may be too late to add an extension umath_testsWarning: distutils distribution has been initialized, it may be too late to add an extension multiarray_testsWarning: distutils distribution has been initialized, it may be too late to add a subpackage coreWarning: distutils distribution has been initialized, it may be too late to add an extension _compiled_baseWarning: distutils distribution has been initialized, it may be too late to add a subpackage libWarning: distutils distribution has been initialized, it may be too late to add a subpackage oldnumericWarning: distutils distribution has been initialized, it may be too late to add an extension _capiWarning: distutils distribution has been initialized, it may be too late to add a subpackage numarrayWarning: distutils distribution has been initialized, it may be too late to add an extension fftpack_liteWarning: distutils distribution has been initialized, it may be too late to add a subpackage fftWarning: distutils distribution has been initialized, it may be too late to add an extension lapack_liteWarning: distutils distribution has been initialized, it may be too late to add a subpackage linalgWarning: distutils distribution has been initialized, it may be too late to add an extension mtrandWarning: distutils distribution has been initialized, it may be too late to add a subpackage randomWarning: distutils distribution has been initialized, it may be too late to add a subpackage maWarning: distutils distribution has been initialized, it may be too late to add a subpackage matrixlibWarning: distutils distribution has been initialized, it may be too late to add a subpackage compatWarning: distutils distribution has been initialized, it may be too late to add a subpackage polynomialWarning: distutils distribution has been initialized, it may be too late to add a subpackage docWarning: distutils distribution has been initialized, it may be too late to add a subpackage numpyCould not locate executable f95\n    Could not locate executable f90\n    Could not locate executable f77\n    Could not locate executable xlf90\n    Could not locate executable xlf\n    Could not locate executable ifort\n    Could not locate executable ifc\n    Could not locate executable g77\n    Found executable /usr/local/bin/gfortran\n    _configtest.c:1: warning: conflicting types for built-in function \xa1\xaeexp\xa1\xaf\n    _configtest.c:1: warning: conflicting types for built-in function \xa1\xaeexp\xa1\xaf\n    _configtest.c:1:20: error: Python.h: No such file or directory\n    _configtest.c:1:20: error: Python.h: No such file or directory\n    lipo: can\'t figure out the architecture type of: /var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-//ccLgEqLk.out\n    _configtest.c:1:20: error: Python.h: No such file or directory\n    _configtest.c:1:20: error: Python.h: No such file or directory\n    lipo: can\'t figure out the architecture type of: /var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-//ccLgEqLk.out\n    Traceback (most recent call last):\n      File ""setup.py"", line 4, in <module>\n        install_requires=[\'numpy\']\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py"", line 152, in setup\n        dist.run_commands()\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py"", line 953, in run_commands\n        self.run_command(cmd)\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py"", line 972, in run_command\n        cmd_obj.run()\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/install.py"", line 76, in run\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/install.py"", line 104, in do_egg_install\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 211, in run\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 427, in easy_install\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 478, in install_item\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 519, in process_distribution\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py"", line 563, in resolve\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py"", line 799, in best_match\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py"", line 811, in obtain\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 446, in easy_install\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 476, in install_item\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 655, in install_eggs\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 930, in build_and_install\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/easy_install.py"", line 919, in run_setup\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/sandbox.py"", line 62, in run_setup\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/sandbox.py"", line 105, in run\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/sandbox.py"", line 64, in <lambda>\n      File ""setup.py"", line 196, in <module>\n        \n      File ""setup.py"", line 189, in setup_package\n        \n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/core.py"", line 186, in setup\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py"", line 152, in setup\n        dist.run_commands()\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py"", line 953, in run_commands\n        self.run_command(cmd)\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py"", line 972, in run_command\n        cmd_obj.run()\n      File ""/Users/olivier/tc/karmatest/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/setuptools/command/bdist_egg.py"", line 167, in run\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/cmd.py"", line 326, in run_command\n        self.distribution.run_command(command)\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py"", line 972, in run_command\n        cmd_obj.run()\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/command/egg_info.py"", line 8, in run\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/cmd.py"", line 326, in run_command\n        self.distribution.run_command(command)\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py"", line 972, in run_command\n        cmd_obj.run()\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/command/build_src.py"", line 152, in run\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/command/build_src.py"", line 169, in build_sources\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/command/build_src.py"", line 328, in build_extension_sources\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/command/build_src.py"", line 385, in generate_sources\n      File ""/private/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/core/setup.py"", line 410, in generate_config_h\n        \n      File ""/private/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/core/setup.py"", line 41, in check_types\n        \n      File ""/private/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/core/setup.py"", line 271, in check_types\n        \n    SystemError: Cannot compile \'Python.h\'. Perhaps you need to install python-dev|python-devel.\n    /var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/misc_util.py:251: RuntimeWarning: Parent module \'numpy.distutils\' not found while handling absolute import\n    Error in atexit._run_exitfuncs:\n    Traceback (most recent call last):\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs\n        func(*targs, **kargs)\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/misc_util.py"", line 251, in clean_up_temporary_directory\n    ImportError: No module named numpy.distutils\n    Error in sys.exitfunc:\n    Traceback (most recent call last):\n      File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs\n        func(*targs, **kargs)\n      File ""/var/folders/Iy/Iyse1OVUE38-IcPmyTb65E+++TI/-Tmp-/easy_install-W5KwbN/numpy-1.6.0/numpy/distutils/misc_util.py"", line 251, in clean_up_temporary_directory\n    ImportError: No module named numpy.distutils\n\n\n\n\nI have no probleme to install numpy with ` pip install numpy ` or ` easy_install numpy ` inside or outside any virtualenv.\n\nOS: Mac OS 10.6.7\n'"
aplpy/aplpy#239,57727044,astrofrog,,2015-02-15 11:21:21,2015-03-07 14:23:30,None,open,,,7,,https://api.github.com/repos/aplpy/aplpy/issues/239,b'WIP: refactor APLpy to use WCSAxes',"b'This is work by @anizami that was in the ``wcsaxes-integration`` branch which I have now rebased.\r\n\r\nMy plan in the long term is to:\r\n\r\n- release APLpy 1.0 from the current master branch (will happen in next few days). I will then create the v1.x branch and master will then become 2.0.dev (see next item).\r\n\r\n- release APLpy 2.0 with the changes in this PR, but I will make it a hidden release on PyPI which requires using ``pip install aplpy --pre`` so that it can get some testing for a while. I also plan to add ``aplpy.legacy`` to this branch which will be a frozen version of APLpy 1.0. Then people who really need to keep the old behavior can do ``from aplpy.legacy import FITSFigure``.\r\n\r\nMain to-dos for this PR:\r\n\r\n* [ ] Documentation that explains the changes\r\n* [ ] Make sure all the pixel scale stuff is right (use functions from Astropy 1.0)\r\n* [ ] Update reference images\r\n* [ ] Check test coverage\r\n* [ ] Address any remaining ``#TODO``s in code\r\n* [ ] Include any other changes that can require astropy 1.0 (for example #231)\r\n'"
spacetelescope/PyFITS#100,69995248,mauriceleutenegger,,2015-04-22 01:31:23,2015-05-04 22:28:38,None,open,,,5,Bug;Fixed-in-Astropy,https://api.github.com/repos/spacetelescope/PyFITS/issues/100,"b""Cannot create Column with 'F' format""","b'I would like to create an ASCII FITS table containing a column specifically with an \'F\' format (for example, I would like a human readable wavelength column that says 18.9689 instead of 1.89689E+1).\r\n\r\nOn this page the documentation for PyFITS 3.3 claims that I should be able to do this in the same way as creating a binary table with a column of \'E\' format, as long as I use TableHDU.from_columns() instead of BinTableHDU.from_columns(), and also set ascii=True for all of my Columns.\r\n\r\nHowever, if I try to create an appropriate Column:\r\n\r\n```\r\n>>> import numpy as np\r\n>>> import pyfits as pf\r\n>>> print pf.__version__\r\n3.3\r\n>>> a = np.array([1.1,2.2,3.3])\r\n>>> c = pf.Column (array=a,format=\'F\', ascii=True)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/mleutene/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pyfits/column.py"", line 482, in __init__\r\n    array = self._convert_to_valid_data_type(array)\r\n  File ""/Users/mleutene/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pyfits/column.py"", line 845, in _convert_to_valid_data_type\r\n    new_format = _convert_format(format.format)\r\n  File ""/Users/mleutene/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pyfits/column.py"", line 1937, in _convert_format\r\n    return _convert_fits2record(format)\r\n  File ""/Users/mleutene/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pyfits/column.py"", line 1834, in _convert_fits2record\r\n    repeat, dtype, option = _parse_tformat(format)\r\n  File ""/Users/mleutene/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pyfits/column.py"", line 1689, in _parse_tformat\r\n    raise VerifyError(\'Format %r is not recognized.\' % tform)\r\npyfits.verify.VerifyError: Format \'F\' is not recognized.\r\n```\r\n\r\nThanks,\r\n\r\nMaurice'"
numpy/numpy#5921,81491583,embray,charris,2015-05-27 15:20:02,2015-06-02 21:45:10,2015-06-02 21:04:23,closed,,,7,11 - Bug;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5921,b'BUG: Further fixes to record and recarray getitem/getattr',"b""This is a followup to PR #5505, which didn't go quite far\r\nenough.  This fixes two issues in particular:\r\n\r\n1) The record class also needs an updated `__getitem__` that works analogously to its `__getattribute__` so that a nested record is returned as a `record` object and not a plain `np.void`.  In other words the old behavior is:\r\n\r\n```python\r\n>>> rec = np.rec.array([('abc ', (1,1), 1), ('abc', (2,3), 1)],\r\n...       dtype=[('foo', 'S4'), ('bar', [('A', int), ('B', int)]),\r\n('baz', int)])\r\n>>> rec[0].bar\r\n(1, 1)\r\n>>> type(rec[0].bar)\r\n<class 'numpy.record'>\r\n>>> type(rec[0]['bar'])\r\n<type 'numpy.void'>\r\n```\r\n\r\ndemonstrated inconsistency between `.bar` and `['bar']` on the record object.  The new behavior is:\r\n\r\n```python\r\n>>> type(rec[0]['bar'])\r\n<class 'numpy.record'>\r\n```\r\n\r\n2) The second issue is more subtle.  The fix to #5505 used the `obj.dtype.descr` attribute to create a new dtype of type `record`.  However, this does not recreate the correct type if the fields are not aligned.  To demonstrate:\r\n\r\n```python\r\n>>> dt = np.dtype({'C': ('S5', 0), 'D': ('S5', 6)})\r\n>>> dt.fields\r\ndict_proxy({'C': (dtype('S5'), 0), 'D': (dtype('S5'), 6)})\r\n>>> dt.descr\r\n[('C', '|S5'), ('', '|V1'), ('D', '|S5')]\r\n>>> new_dt = np.dtype((np.record, dt.descr))\r\n>>> new_dt\r\ndtype((numpy.record, [('C', 'S5'), ('f1', 'V1'), ('D', 'S5')]))\r\n>>> new_dt.fields\r\ndict_proxy({'f1': (dtype('V1'), 5), 'C': (dtype('S5'), 0), 'D':\r\n(dtype('S5'), 6)})\r\n```\r\n\r\nUsing the `fields` dict to construct the new type reconstructs the correct type with the correct offsets:\r\n\r\n```python\r\n>>> new_dt2 = np.dtype((np.record, dt.fields))\r\n>>> new_dt2.fields\r\ndict_proxy({'C': (dtype('S5'), 0), 'D': (dtype('S5'), 6)})\r\n```\r\n\r\n(Note: This is based on #5920 for convenience, but I could decouple the changes if that's preferable.)"""
spacetelescope/PyFITS#3,12871547,ajmendez,embray,2013-04-06 03:23:23,2013-04-10 17:52:34,2013-04-10 17:52:34,closed,,v3.1.2,9,,https://api.github.com/repos/spacetelescope/PyFITS/issues/3,b'TDIMn',"b'I am running into an issue with creating a multidimension string array.    Originally I discovered this issue when I loaded a fits file, but the below code should recreate the issue.  I think this is the same issue as http://trac.assembla.com/pyfits/ticket/201 , but I cannot seem to login there.\r\n\r\n\r\n    import numpy as np\r\n    import pyfits\r\n    output = [pyfits.Column(name=\'number\',format=\'1A\',array=np.array([\'a\',\'b\'])),\r\n          pyfits.Column(name=\'array\',format=\'4A\',dim=\'(2,2)\',\r\n                        array=np.array([[\'a\',\'bc\'],\r\n                                        [\'cd\',\'e\']])),\r\n          ]\r\n\r\n    hdu = pyfits.new_table(pyfits.ColDefs(output))\r\n    hdu.writeto(\'test.fits\', clobber=True)\r\n\r\n\r\nThe error that python 2.7 running \'3.2.dev\' installed from the github repo gives:\r\n\r\n    ERROR: ValueError: could not broadcast input array from shape (2,2) into shape (2)     [pyfits.hdu.table]\r\n        Traceback (most recent call last):\r\n      File ""run.py"", line 145, in <module>\r\n        hdu = pyfits.new_table(pyfits.ColDefs(output))\r\n      File ""/Users/ajmendez/usr/local/lib/python2.7/site-packages/pyfits-3.2.dev2086-py2.7-    macosx-10.5-i386.egg/pyfits/hdu/table.py"", line 1229, in new_table\r\n        field[:n] = arr[:n]\r\n        ValueError: could not broadcast input array from shape (2,2) into shape (2)\r\n\r\n'"
astropy/photutils#247,55977632,kpardeshi,astrofrog,2015-01-30 00:18:06,2015-02-03 20:47:01,2015-02-03 20:47:01,closed,,,10,,https://api.github.com/repos/astropy/photutils/issues/247,"b'ImageNormalize returns  TypeError: must be type, not classobj'",
astropy/astroquery#481,56105548,EricDepagne,keflavich,2015-01-31 01:10:55,2015-03-02 15:04:46,2015-03-02 15:04:44,closed,,,2,,https://api.github.com/repos/astropy/astroquery/issues/481,b'Simbad.simbad returns bytes objects.',"b'Using astroquery.Simbad.simbad in conjunction with astropy.SkyCoords produces strange results.\r\n\r\nhttps://github.com/astropy/astropy/issues/3399\r\n\r\nIt was suggested to report this here rather than to astropy, so here it is!'"
liberfa/erfa#24,39037753,mdboom,,2014-07-29 21:30:01,2014-07-30 06:31:45,None,open,,,2,,https://api.github.com/repos/liberfa/erfa/issues/24,"b'Documentation is in implementation, not header files'","b'SOFA is very unusual for a C project in that it includes its documentation in the `.c` implementation files, not in the `.h` header files.  This is the opposite of standard procedure in C, where generally a system library may have the header files (which also serve as usage documentation) and compiled binaries, but the original `.c` files are not available.\r\n\r\n- IDEs/editors that provide pop-up help about functions are unable to do so.\r\n\r\n- Tools that want to generate wrappers and include the documentation, or use the extra information about in/out variables contained therein, can not do so without the full source.\r\n\r\nAt the end of the day, this is probably a minor point relative to the effort it would take to fix it, but its rather irksome.'"
gammapy/gammapy#235,55739610,cdeil,cdeil,2015-01-28 10:38:27,2015-04-08 08:19:37,2015-02-10 14:36:09,closed,cdeil,0.2,0,feature,https://api.github.com/repos/gammapy/gammapy/issues/235,b'Add some catalog utilities',"b'This PR adds:\r\n\r\n- [x] Add dataset load functions for the ATNF and Green catalog, and move the TeVCat file to gammapy-extra.\r\n- [x] Some functions to cross-match source catalogs (a.k.a. find associations)\r\n- [x] Utility function to generate spherical test catalogs\r\n- [x] Misc changes and fixes'"
sunpy/sunpy#1255,54001290,wafels,ayshih,2015-01-11 19:13:13,2015-01-23 15:59:47,2015-01-23 15:59:47,closed,Cadair,,5,Bug?,https://api.github.com/repos/sunpy/sunpy/issues/1255,b'Possible issue with checking of Quantity input',"b""In my branch ji_diff_rot3 I've started using the quantity_input decorator.  It now decorates rot_hpc, a differential rotation calculator.  The decorator crashes when applied to this function\r\n```python\r\nIn [1]: from sunpy.physics.transforms.differential_rotation import rot_hpc\r\nIn [2]: import astropy.units as u\r\nIn [3]: rot_hpc( -570 * u.arcsec, 120 * u.arcsec, '2010-09-10 12:34:56', '2010-09-10 13:34:56')\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-3-0803dabeac04> in <module>()\r\n----> 1 rot_hpc( -570 * u.arcsec, 120 * u.arcsec, '2010-09-10 12:34:56', '2010-09-10 13:34:56')\r\n\r\n/Users/ireland/sunpy/sunpy/util/unit_decorators.pyc in wrapper(*func_args, **func_kwargs)\r\n     81 \r\n     82                 # Get the value of this parameter (argument to new function)\r\n---> 83                 arg = bound_args.arguments[param.name]\r\n     84 \r\n     85                 # Get target unit, either from decotrator kwargs or annotations\r\n\r\nKeyError: 'kwargs'\r\n```\r\nThe error is reproducible with much simpler functions:\r\n```python\r\nIn [2]: import astropy.units as u\r\nIn [3]: from sunpy.util import quantity_input\r\n\r\nIn [4]: @quantity_input(a=u.deg)\r\n   ...: def funct(a, **kwargs):\r\n   ...:     pass\r\n   ...: \r\n\r\nIn [5]: funct(1 * u.deg)\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-5fb4e89adcc1> in <module>()\r\n----> 1 funct(1 * u.deg)\r\n\r\n/Users/ireland/sunpy/sunpy/util/unit_decorators.pyc in wrapper(*func_args, **func_kwargs)\r\n     81 \r\n     82                 # Get the value of this parameter (argument to new function)\r\n---> 83                 arg = bound_args.arguments[param.name]\r\n     84 \r\n     85                 # Get target unit, either from decotrator kwargs or annotations\r\n\r\nKeyError: 'kwargs'\r\n```"""
sunpy/sunpy#1255,54001290,wafels,ayshih,2015-01-11 19:13:13,2015-01-23 15:59:47,2015-01-23 15:59:47,closed,Cadair,,5,Bug?,https://api.github.com/repos/sunpy/sunpy/issues/1255,b'Possible issue with checking of Quantity input',"b""In my branch ji_diff_rot3 I've started using the quantity_input decorator.  It now decorates rot_hpc, a differential rotation calculator.  The decorator crashes when applied to this function\r\n```python\r\nIn [1]: from sunpy.physics.transforms.differential_rotation import rot_hpc\r\nIn [2]: import astropy.units as u\r\nIn [3]: rot_hpc( -570 * u.arcsec, 120 * u.arcsec, '2010-09-10 12:34:56', '2010-09-10 13:34:56')\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-3-0803dabeac04> in <module>()\r\n----> 1 rot_hpc( -570 * u.arcsec, 120 * u.arcsec, '2010-09-10 12:34:56', '2010-09-10 13:34:56')\r\n\r\n/Users/ireland/sunpy/sunpy/util/unit_decorators.pyc in wrapper(*func_args, **func_kwargs)\r\n     81 \r\n     82                 # Get the value of this parameter (argument to new function)\r\n---> 83                 arg = bound_args.arguments[param.name]\r\n     84 \r\n     85                 # Get target unit, either from decotrator kwargs or annotations\r\n\r\nKeyError: 'kwargs'\r\n```\r\nThe error is reproducible with much simpler functions:\r\n```python\r\nIn [2]: import astropy.units as u\r\nIn [3]: from sunpy.util import quantity_input\r\n\r\nIn [4]: @quantity_input(a=u.deg)\r\n   ...: def funct(a, **kwargs):\r\n   ...:     pass\r\n   ...: \r\n\r\nIn [5]: funct(1 * u.deg)\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-5fb4e89adcc1> in <module>()\r\n----> 1 funct(1 * u.deg)\r\n\r\n/Users/ireland/sunpy/sunpy/util/unit_decorators.pyc in wrapper(*func_args, **func_kwargs)\r\n     81 \r\n     82                 # Get the value of this parameter (argument to new function)\r\n---> 83                 arg = bound_args.arguments[param.name]\r\n     84 \r\n     85                 # Get target unit, either from decotrator kwargs or annotations\r\n\r\nKeyError: 'kwargs'\r\n```"""
embray/astropy#12,52669338,nden,embray,2014-12-22 17:17:42,2015-03-12 20:44:42,2015-03-12 20:44:42,closed,,,3,,https://api.github.com/repos/embray/astropy/issues/12,b'Handle case of n_outputs > n_inputs',"b""@embray I feel the problem with `n_outputs > n_inputs` is relevant to this PR. Consider the following failing example:\r\n\r\n```\r\ng1 = models.Gaussian1D(9.4, 3, 1.2)\r\ng2 = models.Gaussian1D(4.4, 2, 1.2)\r\ng3 = models.Gaussian1D(9, 3.3, 1)\r\nm = Mapping((0, 1, 0))\r\nm1 = m | (g1 & g2 & g3)\r\n```\r\n(It may also be relevant to single models but I don't have an example of such a model now.)\r\nThis PR suggests a way of fixing the problem, making the assumption that all outputs from a model have the same shape. I think this is a reasonable assumption for now but it certainly may be revisited later."""
numpy/numpy#2806,9164068,lukauskas,charris,2012-12-10 23:52:27,2014-11-06 21:31:33,2014-05-05 18:14:49,closed,,,19,,https://api.github.com/repos/numpy/numpy/issues/2806,b'save/load and tofile/fromfile fail silently for large arrays on Mac os X',"b""It seems that saving large numpy arrays to disk using numpy.save fails silently for very large matrices, example:\r\n\r\n```python\r\nIn [5]: a = np.random.randn(500000000)\r\n\r\nIn [6]: \r\n\r\nIn [6]: a\r\nOut[6]: \r\narray([-1.00736353, -2.06696394,  1.46569636, ...,  0.89738222,\r\n       -0.06982733,  0.06954417])\r\n\r\nIn [7]: np.save('a', a)\r\n\r\nIn [8]: b = np.load('a.npy')\r\n\r\nIn [9]: b\r\nOut[9]: \r\narray([-1.00736353, -2.06696394,  1.46569636, ...,  0.        ,\r\n        0.        ,  0.        ])\r\n```\r\nNote zeros in the end of the matrix that do not exist in a\r\n\r\nI'm running numpy version 1.6.2:\r\n\r\n```python\r\nIn [11]: np.version.version\r\nOut[11]: '1.6.2'\r\n````\r\n\r\non python 2.7.3 on Mac os X 10.8.2 if that helps\r\n"""
matplotlib/matplotlib#3196,37304142,astrofrog,mdboom,2014-07-07 19:54:23,2014-12-03 17:35:38,2014-12-03 16:39:34,closed,mdboom,v1.4.x,18,confirmed bug,https://api.github.com/repos/matplotlib/matplotlib/issues/3196,b'Issue with iterability of axes arguments [backport to 1.4.x]',"b""I have been trying to use the ``_as_mpl_axes`` method to create a custom projection. I am returning an Axes subclass and a dictionary of arguments to pass to the class. One of the arguments is an item of type ``astropy.wcs.WCS``. In ``figure.py``, the following code:\r\n\r\n```\r\n        def fixitems(items):\r\n            #items may have arrays and lists in them, so convert them\r\n            # to tuples for the key\r\n            ret = []\r\n            for k, v in items:\r\n                if iterable(v):\r\n                    v = tuple(v)\r\n                ret.append((k, v))\r\n            return tuple(ret)\r\n```\r\n\r\nresults in an error, specifically when doing ``tuple(v)``. The issue is that ``iterable(wcs)`` returns ``True`` but ``tuple(wcs)`` crashes. As a general issue, ``iter(v)`` working does not guarantee that ``tuple(v)`` will work. One solution is to simply do:\r\n\r\n```\r\nif iterable(v):\r\n    try:\r\n        v = tuple(v)\r\n    except:\r\n        pass\r\n```\r\n\r\nI'm not sure if there is a better way to deal with this kind of issue. Just for the record, ``tuple(wcs)`` gives:\r\n\r\n```\r\nValueError: Cannot downsample a WCS with indexing.  Use wcs.sub or wcs.dropaxis if you want to remove axes.\r\n```\r\n\r\nTo some extent, the issue here is that while ``WCS`` has a ``__getitem__`` method, it is not strictly iterable. So maybe the issue is actually that ``iterable`` should not rely just on ``iter``. Maybe the ``iterable`` function:\r\n\r\n```\r\n\r\ndef iterable(obj):\r\n    'return true if *obj* is iterable'\r\n    try:\r\n        iter(obj)\r\n    except TypeError:\r\n        return False\r\n    return True\r\n```\r\n\r\nin ``cbook.py`` should be using:\r\n\r\n```\r\nnext(iter(wcs))\r\n```\r\n\r\n? (which does raise an error in my case)\r\n\r\ncc @mdboom, since it involves the WCS class.\r\n"""
numpy/numpy#5251,47404106,embray,charris,2014-10-31 15:07:23,2014-11-04 23:30:04,2014-10-31 21:19:28,closed,,,10,,https://api.github.com/repos/numpy/numpy/issues/5251,b'Issue with fromarrays not using correct format for unicode arrays',"b'If I understand correctly the records module is all but deprecated, so this is probably a bit obscure.  But it came up in Astropy (see astropy/astropy#3052).  This ensures that `np.rec.fromarrays`, and by extension `np.rec.fromrecords` uses the correct format for unicode arrays.'"
numpy/numpy#4586,30897875,mhvk,charris,2014-04-04 21:52:11,2015-06-04 00:54:36,2015-06-04 00:48:38,closed,charris,,20,10 - Maintenance;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/4586,"b'ENH: Let MaskedArray getter, setter respect baseclass overrides'","b""As is, `MaskedArray` uses explicity calls to `ndarray.__setitem__(self.data, indx, value)` and `ndarray.__getitem__(self.data, indx)` to access elements of its baseclass. This PR uses `self.data[indx]` instead, so that possible overrides of `__getitem__` and `__setitem__` are respected.\r\n\r\nThis is particularly useful for astropy's `Quantity` ndarray subclass, since it should check that anything written to it has the correct unit, and associate units for anything gotten from it.\r\n\r\n(Note: this includes #4585, as otherwise some of the tests would fail; I'll rebase once that is in.)"""
numpy/numpy#5086,43258657,mdboom,charris,2014-09-19 16:48:19,2014-10-26 16:10:08,2014-10-26 16:09:31,closed,,,28,,https://api.github.com/repos/numpy/numpy/issues/5086,b'Fix how strides is set on Py_buffer object when the array is C-contiguous',"b""This is a possible fix for #5085.\r\n\r\nA complete solution would probably convert the strides array from something containing -1's into the equivalent without them, but I don't know all the corner cases involved with that."""
numpy/numpy#2256,7731173,numpy-gitbot,numpy-gitbot,2012-10-19 22:19:38,2012-10-19 22:19:42,2012-10-19 22:19:42,closed,,1.5.1,17,11 - Bug;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2256,b'~2**32 byte tofile()/fromfile() limit in 64-bit Windows (Trac #1660)',"b""_Original ticket http://projects.scipy.org/numpy/ticket/1660 on 2010-11-03 by trac user mspacek, assigned to @charris._\n\nI'm using Christoph Gohlke's amd64 builds (http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy) on 64-bit Windows 7, with the official Python 2.6.6 amd64 install. I can't tofile() or save() or fromfile() numpy arrays roughly > 2**32 bytes. For save() and tofile() Python hangs, using 100% CPU on one core. For fromfile(), it raises an IOError. This doesn't happen in 64-bit Linux on the same machine. load() seems to work on any size of file. Here are some examples, and some caveats:\n\n\n\n    # if .tofile() hangs, it hangs immediately:\n    np.zeros(2**32+2**16, np.int8).tofile('test.npy') # hangs, file is 2**16 bytes\n    np.zeros(2**32+2**2,  np.int8).tofile('test.npy') # hangs, file is 0 bytes\n    np.zeros(2**32+2,     np.int8).tofile('test.npy') # hangs, file is 0 bytes\n    np.zeros(2**32+1,     np.int8).tofile('test.npy') # hangs, file is 0 bytes\n    np.zeros(2**32,       np.int8).tofile('test.npy') # hangs, file is 0 bytes\n    np.zeros(2**32-1,     np.int8).tofile('test.npy') # works\n    \n    # if np.save() hangs, it takes a while for it to hang:\n    np.save('test.npy', np.zeros(2**33, np.int8)) # hangs, file is 2**32 bytes, npy header is intact\n    np.save('test.npy', np.zeros(2**33-1, np.int8)) # hangs, file is 2**32 bytes, npy header is intact\n    np.save('test.npy', np.zeros(2**32+2**31, np.int8)) # hangs, file is 2**31 bytes, npy header is intact\n    np.save('test.npy', np.zeros(2**32+2**16, np.int8)) # hangs, file is 2**16 bytes, npy header is intact\n    np.save('test.npy', np.zeros(2**32+2**12, np.int8)) # hangs, file is 2**12 bytes, npy header is intact\n    np.save('test.npy', np.zeros(2**32+2**11, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32+2**10, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32+2**8, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32+2**4, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32+2**2, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32+2, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32+1, np.int8)) # works\n    np.save('test.npy', np.zeros(2**32, np.int8)) # works\n    \n    # I also generated these 3 large files successfully in 64-bit Linux...\n    np.zeros(2**33, np.int8).tofile('test1.npy') # 8 GB\n    np.zeros(2**32, np.int8).tofile('test2.npy') # 4 GB\n    np.save('test3.npy', np.zeros(2**33, np.int8)) # 8 GB\n    \n    # ...and then tried reading them back in 64-bit Win7:\n    a = np.fromfile('test1.npy', dtype=np.int8) # IOError: could not seek in file\n    a = np.fromfile('test2.npy', dtype=np.int8) # IOError: could not seek in file\n    a = np.load('test3.npy') # strangely enough, this works fine, even though it's 8 GB!\n\n\n\nI have this problem regardless of which of Christoph's amd64 numpy builds (1.4.1, 1.5.0, 1.5.1RC1, MKL or non-MKL) I use. This happens on both my 64-bit Win7 i7 12 GB machine, and on another machine running 64-bit WinXP. Christoph has also confirmed this on Python 2.5 through 3.1 and says there is nothing special about his builds, and that this will affect all 64-bit Windows distributions of numpy (EPD, ActiveState, etc).\n"""
numpy/numpy#5085,43248924,mdboom,seberg,2014-09-19 15:12:05,2014-11-25 10:10:00,2014-11-25 10:10:00,closed,,,31,06 - Task,https://api.github.com/repos/numpy/numpy/issues/5085,b'Broken stride calculation on master',"b'I\'m running into a problem on Numpy master that I don\'t see on 1.9.0.  This is enough to trigger it\r\n\r\n```\r\nimport numpy as np\r\nimport io\r\n\r\nx = np.arange(10)\r\ny = np.array([x])\r\n\r\nprint(y.strides)\r\n\r\nfd = io.BytesIO()\r\nfd.write(y.data)\r\n```\r\n\r\nOn Python 2:\r\n\r\n```\r\n(9223372036854775807, 4)\r\n```\r\n\r\nOn Python 3:\r\n\r\n```\r\n(9223372036854775807, 4)\r\nTraceback (most recent call last):\r\n  File ""arr.py"", line 11, in <module>\r\n    fd.write(y.data)\r\nBufferError: memoryview: underlying buffer is not C-contiguous\r\n```\r\n\r\n9223372036854775807 is of course -1, so perhaps this is a deliberate marker for the outermost stride?  (Not sure).  However, Python 3 doesn\'t seem to like that in the buffer interface.\r\n\r\nSee astropy/astropy#2960'"
numpy/numpy#4681,32921483,seberg,charris,2014-05-06 18:51:21,2015-10-13 19:50:20,2015-10-13 19:50:20,closed,,,35,06 - Task,https://api.github.com/repos/numpy/numpy/issues/4681,b'Change RELAXED_STRIDES to not intentionally mangle them up',"b""Currently when RELAXED_STRIDES is active, new arrays which have a dimension with size 1 get a stride of `NPY_MAX_INTP` set to help debugging (since this will almost certainly trigger such bugs for any input array with such a shape, and not juts when one happens to have an input array with funny strides).\r\n\r\nIn release versions we should probably *not* do this. So for 1.10 I would suggest creating a RELAXED_STRIDES=2 setting where we actually still scramble up the stride, but as default with RELAXED_STRIDES=1 don't in the release. This requires `ctors.h` around line 3718 (`/* For testing purpose only */`) to be changed."""
numpy/numpy#5112,43790157,mhvk,mhvk,2014-09-24 17:08:00,2014-09-25 13:13:04,2014-09-25 13:13:04,closed,,,8,,https://api.github.com/repos/numpy/numpy/issues/5112,b'BUG subclass ignored for inplace operations',"b'Normally, `__array_prepare__` and `__array_wrap__` can be used to prepare output given the subclass. However, for inplace operations where the output is an `ndarray`, these are ignored (independent of `__array_priority__`):\r\n```\r\nimport numpy as np\r\nclass Mine(np.ndarray):\r\n    __array_priority__ = 10000\r\n    def __array_wrap__(self, obj, context=None):\r\n        raise TypeError\r\n\r\na = np.arange(3.)\r\nme = np.array([2.]).view(Mine)\r\na * me\r\n# ... TypeError\r\na *= me\r\na\r\n# array([ 0.,  2.,  4.])\r\n```\r\n(This is not an issue with `__numpy_ufunc__`)'"
radio-astro-tools/spectral-cube#139,41740591,keflavich,keflavich,2014-09-02 18:39:49,2014-09-08 08:27:02,2014-09-08 08:27:01,closed,,,1,,https://api.github.com/repos/radio-astro-tools/spectral-cube/issues/139,b'Update WCS slicing to fix bug noted in astropy #2909',"b'Astropy #2909 (https://github.com/astropy/astropy/pull/2909) noted an error in the treatment of WCS slices.  This PR pulls the astropy changes into spectral_cube.  After the 0.4.2 astropy release, we can change the version requirement of spectral_cube to astropy 0.4.2 and remove this entirely.'"
astropy/photutils#128,39446801,larrybradley,larrybradley,2014-08-04 18:24:07,2014-08-04 21:11:34,2014-08-04 21:11:31,closed,larrybradley,0.1,1,bug;docs,https://api.github.com/repos/astropy/photutils/issues/128,b'Add skimage to doctest-requires',b'This PR fixes an issue where doctests were failing if `scikit-image` was not installed.\r\n\r\nThis also uncovered a bug in `doctest-requires` (https://github.com/astropy/astropy/pull/2818) that needs to be fixed before this PR.'
spacetelescope/PyFITS#97,48103191,erykoff,,2014-11-07 16:45:12,2016-01-28 22:29:56,None,open,,,1,Bug;effort-low;package-intermediate;port-3.2.x,https://api.github.com/repos/spacetelescope/PyFITS/issues/97,b'gzip (bintable) fits writing is broken',"b""Writing .gz files leaves corrupted files that cannot be read by pyfits or gunzip.  Strangely, these files can be read by IDL mrdfits.  This bug goes back to (at least) 3.2.4, but is not present in 3.2.1 (at least).  Also present in astropy.io.fits\r\n\r\nTry:\r\n```python\r\nimport pyfits\r\ndata=pyfits.getdata('testbintable.fits',ext=1)\r\npyfits.writeto('test1.fits',data)\r\npyfits.writeto('test2.fits.gz',data)\r\ntest1=pyfits.getdata('test1.fits',ext=1)\r\ntest2=pyfits.getdata('test2.fits.gz',ext=1)\r\nIOError: Incorrect length of data produced\r\n```\r\n"""
gammapy/gammapy#147,38295409,cdeil,cdeil,2014-07-21 11:40:34,2014-08-24 14:50:11,2014-08-24 14:50:11,closed,cdeil,0.1,2,astropy;bug,https://api.github.com/repos/gammapy/gammapy/issues/147,b'Fix exposure cube test on Python 2.6',"b""I see this error from `astropy.io.fits` with Python 2.6 :\r\nhttps://gist.github.com/cdeil/0875c51b495f51066f2e\r\n\r\nThe same error can be seen on travis-ci on Linux:\r\nhttps://travis-ci.org/gammapy/gammapy/jobs/30449578#L623\r\n\r\nWe'll probably have to wait for https://github.com/astropy/astropy/issues/2774 to fix this and until the fix is in a stable version before we can merge this PR."""
GalSim-developers/GalSim#586,38012111,richardgmcmahon,richardgmcmahon,2014-07-16 18:44:49,2014-09-16 15:35:08,2014-07-17 17:43:23,closed,,,4,,https://api.github.com/repos/GalSim-developers/GalSim/issues/586,b'astropy fits.io problem with des example data',"b'Mainly for info.\r\n\r\nI have hit a header reading problem running examples/des/draw_psf.py\r\nusing astropy 0.3.2 and also 0.4rc3.dev9443\r\n \r\nastropy 0.3 reads the header \r\n\r\nI have some .fz fits files that are giving the error:\r\n\r\nKeyError: ""Keyword \'ZSIMPLE\' not found.""\r\n\r\nwhen I use astropy 0.3.2 and also 0.4rc3.dev9443 but 0.3 reads the header OK\r\n\r\nI reported the astropy error to the astropy folks and it is discussed here and is due to a duplicate ZSIMPLE keyword in the DECAM file.\r\n\r\nSee https://github.com/astropy/astropy/issues/2750 \r\n\r\nand workaround\r\n>>> hdulist = fits.open(\'des_data/DECam_00154912_01.fits.fz\')\r\n>>> del hdulist[1]._header[(\'ZSIMPLE\', 1)]  # Note the underscore in ._header\r\n>>> hdulist.info()\r\n\r\nThis I assume deletes the first ZSIMPLE keyword. \r\n\r\nbut the root problem solution is probable to fix the des data.\r\n'"
numpy/numpy#3907,20922434,mhvk,charris,2013-10-13 06:04:37,2016-02-13 00:59:07,2015-06-17 19:44:39,closed,,,32,10 - Maintenance;component: numpy.ma,https://api.github.com/repos/numpy/numpy/issues/3907,b'BUG allow subclasses in MaskedArray ufuncs -- for non-ndarray _data',"b""This is a follow-on to #3900, making two furthe changes to tryto reach the ultimate goal of `MaskedArray` behaving sensibly for subclasses. \r\n1. Ensure that for two-argument ufunc's, the data in the subclass are no longer converted to `ndarray`; rather, a possible subclass is kept. This is particularly relevant for my work on a `MaskedQuantity` class, where `_data` contains a `Quantity` which has not just a value (the ndarray) but also a unit.\r\n2. Avoid the creation of `object` arrays, so that non-array Classes with an `__array_priority__` set and with a reverse method defined properly lead to `NotImplemented` (i.e., making the behaviour match that of regular arrays).\r\n\r\nIn order for this to work, I\r\n- removed `subok=False` in the `getdata` calls in the two `BinaryOperation` classes\r\n- slightly changed the method used to keep previous values when they are masked, including allowing this to fail (e.g., when I multiply two masked quantities in meters, the result is an area, and hence it is senseless to keep the original values, and quantities raise an appropriate UnitError). (Without this, the matrix subclass tests would not pass).\r\n- slightly changed the order, with result masks only calculated if the primary calculation succeeded.\r\n- slightly changed `getdata` to avoid the creation of `object` arrays, and `getmaskedarray` to deal with the consequences.\r\n\r\nI also added a test case, which perhaps best makes clear why this is useful. \r\n\r\nCC @charris -- I consider the above a correction of a bug, but realise this is subjective; it could also be ENH, even though the API does not change. It would be wonderful if this could still make it to 1.8."""
numpy/numpy#5864,74993189,njsmith,charris,2015-05-10 20:16:17,2015-06-13 20:16:04,2015-06-13 20:16:04,closed,,,64,09 - Enhancement;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/5864,b'Remove NotImplemented handling from the ufunc machinery (almost)',"b'We\'ve long had consensus that returning `NotImplemented` from ufuncs was a bad and ugly hack, since it violates layering -- `NotImplemented` is the responsibility of `__op__` methods, not ufuncs. And one of the things that came out of the discussion in gh-5844 is that the `NotImplemented` handling inside the ufunc machinery was actually mostly useless and broken (see the first post on that report for analysis).\r\n\r\nOf course, it turns out it\'s not *quite* that simple -- there were some places, esp. in the trainwreck that is `array_richcompare`, which were depending on the current behavior.\r\n\r\nThis patch series fixes the main pieces of code that used to rely on this behavior, and then removes as much of the `NotImplemented` handling as it can. And for what isn\'t removed, it adds deprecation or future warnings.\r\n\r\nIn particular, there are a bunch of cases where `array_richcompare` returns bad and wrong results, like where you can compare two arrays (which should produce a boolean array), some failure occurs, and then the error gets thrown away and we end up returning a boolean scalar. Some of these were deprecated in 9b8f6c72, but this deprecates the rest. Probably the most controversial part of this is that this patch deprecates scalar ordering comparisons between unlike types, like `np.float64(1) < ""foo""`, which are already an error on py3 but have traditionally returned some semi-arbitrary value on py2. To convince you that this is broken and we should stop supporting it, check this out:\r\n```\r\n# Note: the name of this class affects the results below\r\nIn [1]: class quux(object):\r\n   ...:     pass\r\n   ...: \r\n\r\nIn [2]: q = quux()\r\n\r\nIn [3]: np.string_(""foo"") < q\r\nOut[3]: True\r\n\r\nIn [4]: np.asarray(np.string_(""foo"")) < q\r\nOut[4]: False\r\n```\r\nFor a full analysis of how we handle all the legacy comparison cases, see the long comment that starts here: https://github.com/numpy/numpy/blob/19b829a660a6adc8118dc427e1a7e4ba6946ccf7/numpy/core/src/umath/ufunc_object.c#L867\r\n\r\nNB this PR is probably easier to review if you read the commits one at a time in order, instead of jumping straight to the big diff.'"
numpy/numpy#4843,37219662,juliantaylor,charris,2014-07-06 13:53:29,2014-07-06 19:37:59,2014-07-06 16:43:41,closed,,,12,,https://api.github.com/repos/numpy/numpy/issues/4843,b'BUG: retain writeable flag when indexing subclasses',
numpy/numpy#4804,35534422,seberg,charris,2014-06-11 22:32:44,2014-07-06 19:06:07,2014-07-06 16:06:50,closed,,1.9 blockers,56,,https://api.github.com/repos/numpy/numpy/issues/4804,"b'WIP: Fix matplotlib, etc. errors'",b'I would like the deprecation warning to show the actual error message. It might also make sense to replace the error again with the original one if the fallback failed as well.'
PritishC/sunpy#3,37177248,Cadair,PritishC,2014-07-04 15:58:21,2014-07-05 10:28:45,2014-07-05 10:28:45,closed,,,5,,https://api.github.com/repos/PritishC/sunpy/issues/3,b'Coordinates framework',
PritishC/sunpy#4,37205427,Cadair,PritishC,2014-07-05 20:07:02,2014-07-06 09:22:14,2014-07-06 07:35:19,closed,,,2,,https://api.github.com/repos/PritishC/sunpy/issues/4,b'This makes the representation subclassing work using astropy/astropy#2691',
,,,,,,,,,,,,,,
spacetelescope/PyFITS#72,36536274,mdboom,embray,2014-06-26 00:58:33,2014-06-26 15:44:27,2014-06-26 15:44:26,closed,,v3.3.0,1,,https://api.github.com/repos/spacetelescope/PyFITS/issues/72,b'Fix memory-access-after-free bug.',"b'In the process of running valgrind over the test suite to see if it\r\nwould find anything related to the random crashes, I came across this.\r\nIn get_header_string, a header value is retrieved from the Python side,\r\nand a `char *` to the character data is obtained and returned.\r\nUnfortunately, when the Python string that owns that character data\r\nis dereferenced at the end of that function, Python is free to\r\ndeallocate or move that character data around.  When the `char *` is\r\nlater accessed by the calling function, it, in some circumstances, is\r\nno longer valid.\r\n\r\nThe change here is to allocate character buffers on the stack of the\r\ncalling function and copy the character data into that buffer\r\nimmediately, so that Python can deallocate its original character buffer\r\nwith no problems.\r\n\r\nNote: This is a backport of astropy/astropy#2663.'"
numpy/numpy#3213,12987174,hamogu,charris,2013-04-09 19:10:23,2014-03-28 23:33:13,2014-03-28 23:33:13,closed,,,4,11 - Bug;component: numpy.ma;priority: normal,https://api.github.com/repos/numpy/numpy/issues/3213,b'Assigning a fill_value in masked array fails silently for wrong type',"b""I try to assign a `fill_value` to a masked array. If it's the right type (e.g. an `int` in an array of type `int`), it works, but if I try to assign a `fill_value` of the wrong dtype (e.g. a `string` in an array of type `int`, it fails silently. I don't know if this is a bug or a feature, but I had expected a `ValueError` or something similar if the value cannot be broadcasted to the required type.\r\n\r\n```\r\nIn [1]: x = np.ma.array(np.arange(3).reshape(3), mask=[[1, 0, 0]])\r\n\r\nIn [2]: print repr(x)\r\nmasked_array(data = [-- 1 2],\r\n             mask = [ True False False],\r\n       fill_value = 999999)\r\n\r\n\r\nIn [3]: x.fill_value = 'n.a.'\r\n\r\nIn [4]: print repr(x)\r\nmasked_array(data = [-- 1 2],\r\n             mask = [ True False False],\r\n       fill_value = 999999)\r\n\r\n\r\nIn [5]: x.fill_value = -99\r\n\r\nIn [6]: print repr(x)\r\nmasked_array(data = [-- 1 2],\r\n             mask = [ True False False],\r\n       fill_value = -99)\r\n```"""
spacetelescope/PyFITS#71,36098486,embray,embray,2014-06-19 17:13:27,2014-06-19 19:08:59,2014-06-19 18:24:44,closed,,v3.3.0,0,,https://api.github.com/repos/spacetelescope/PyFITS/issues/71,b'Heap data for compressed images written in wrong order',"b""This is an issue that I *think* may have been introduced as a regression in 7215b4b0aa1ca72799f51055f62141df26eea64e, but I'm not certain--it might go further back.\r\n\r\nAt issue is that when the CFITSIO compression code writes compressed data, it outputs it to the heap one tile after another, so that the data for each tile is consecutive in the heap.\r\n\r\nHowever, in some cases the data is broken between two columns, such as when the GZIP_COMPRESSED_DATA column is used.  When *pyfits* writes VLA data to the heap it writes it one column at a time.  For example it will first write all the `COMPRESSED_DATA` column's data followed by all the `GZIP_COMPRESSED_DATA`.\r\n\r\nDepending on how the compression code in CFITSIO works the way it has it laid out may be more efficient, but probably not by much.  The nature of how the FITS format defines VLA columns really allows no guarantees as to data contiguity, though at the OS level it might improve page pre-caching and the like.  Hard to say.\r\n\r\nThe problem is pyfits is writing the heap data in its standard order, while not updating the heap *pointers*, causing the data to be unreadable.  """
spacetelescope/PyFITS#69,34955565,embray,embray,2014-06-04 12:52:26,2014-06-04 17:00:26,2014-06-04 17:00:26,closed,,v3.3.0,1,,https://api.github.com/repos/spacetelescope/PyFITS/issues/69,b'Fix Astropy #2595',b'Reported in https://github.com/astropy/astropy/issues/2595'
numpy/numpy#4709,33506082,mdboom,juliantaylor,2014-05-14 15:59:15,2014-06-19 15:54:32,None,open,,,11,,https://api.github.com/repos/numpy/numpy/issues/4709,"b'No way to mark ""NotImplemented"" for comparison operations on subclasses'","b'In astropy, we have a `Quantity` class which is a subclass of `ndarray` for physical quantities.  One of its features is that if you compare two of them and the units aren\'t equivalent, they compare to false.\r\n\r\nIn Numpy master (but not Numpy 1.8.x), as of 9b8f6c72 (@seberg), it seems there might no longer be a way to implement this, since raising an exception in `__array_prepare__` bubbles up through `ndarray.richcompare` (it currently emits a `DeprecationWarning`, but I understand will eventually pass the original exception through).  Since `__array_prepare__` is only able to return an instance `ndarray` (or subclass), there doesn\'t seem to be a way to flag to the comparison operator that what we really want is `NotImplemented`.  Overriding the `__eq__` operators, etc., doesn\'t seem to be sufficient, since it can\'t handle the case where there is a (regular) array on the left hand side.\r\n\r\nHere\'s a minimal script to reproduce the issue (the actual code is much more complex, but this boils it down to the essence of the problem):\r\n\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(""error"", "".*"", DeprecationWarning)\r\n\r\nimport numpy as np\r\n\r\n\r\nclass Quantity(np.ndarray):\r\n    def __repr__(self):\r\n        return \'<{0} {1}>\'.format(\r\n            np.ndarray.__repr__(self), self._unit)\r\n\r\n    def __new__(cls, value, unit, dtype=None):\r\n        value = np.array(value, dtype=dtype)\r\n        value = value.view(cls)\r\n        value._unit = unit\r\n        return value\r\n\r\n    def __array_prepare__(self, obj, context=None):\r\n        function = context[0]\r\n        args = context[1][:function.nin]\r\n\r\n        if ((not isinstance(args[0], Quantity) or args[0]._unit != self._unit) or\r\n            (not isinstance(args[1], Quantity) or args[1]._unit != self._unit)):\r\n            raise ValueError(""Units don\'t match"")\r\n\r\n        if not isinstance(obj, np.ndarray):\r\n            obj = np.array(obj)\r\n        view = obj.view(Quantity)\r\n        view._unit = self._unit\r\n        return view\r\n\r\n    # def __eq__(self, other):\r\n    #     try:\r\n    #         super(Quantity, self).__eq__(other)\r\n    #     except (DeprecationWarning, ValueError):\r\n    #         return NotImplemented\r\n\r\n\r\nq = Quantity([10.0, 20.0], \'m\')\r\nprint 10 == q\r\nprint np.int64(10) == q\r\n```\r\n\r\nThe output on Numpy 1.8.x is:\r\n\r\n```\r\nFalse\r\nFalse\r\n```\r\n\r\nThe output on Numpy master is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""quantity_compare.py"", line 40, in <module>\r\n    print 10 == q\r\nDeprecationWarning: elementwise comparison failed; this will raise the error in the future.\r\n```\r\n\r\nIf you uncomment the `__eq__` function above, the first test passes, but the second still fails.\r\n\r\nIs there a workaround here, or something I\'m missing?\r\n\r\n@mhvk, @astrofrog'"
astropy/ccdproc#74,33324269,mwcraig,crawfordsm,2014-05-12 16:35:02,2014-06-13 18:07:51,2014-05-27 17:38:38,closed,mwcraig,0.1,7,combiner;enhancement;question,https://api.github.com/repos/astropy/ccdproc/issues/74,b'masked median_combine is excruciatingly slow',"b'This is a known issue, though with no solution in ``numpy``: numpy/numpy#4683\r\n\r\nBy ""excruciating"" I mean masked median is roughly 1,000x slower than unmasked median.\r\n\r\nSummary of several options (the ""reference"" time is ``np.median`` with no masks):\r\n\r\nfunction | masked? | time (sec) | slowness factor\r\n-----------| ------------| ------------- | --------------------\r\n``np.median`` | no | 1.07s | 1x\r\n``np.ma.median`` | yes | 838s | 783x\r\nloop over pixels | yes | 254s | 237x\r\n``scipy.stats.nanmedian`` | yes | 232s | 216x\r\n``bottleneck.nanmedian`` | yes | 1.35s  | 1.26x\r\n\r\nnotebook with timings is at: http://nbviewer.ipython.org/gist/mwcraig/9711663484d7cc264fa7\r\n\r\nThe fastest by far is [bottleneck](https://pypi.python.org/pypi/Bottleneck) but requiring bottleneck adds a dependency. \r\n\r\nOne way around this is:\r\n```python\r\ndef median_combine(self, median_nan_func=None):\r\n""""""\r\n...\r\n           Parameters\r\n           ----------\r\n           median_nan_func : function, optional\r\n               Function that calculates median of a ``numpy.ndarray``, ignoring any\r\n               values that are ``NaN``. Default is to use ``np.ma.median`` to\r\n               calculate median.\r\n""""""\r\n```'"
spacetelescope/PyFITS#23,16136915,bmampaey,embray,2013-06-28 11:02:14,2014-04-23 21:14:02,2014-02-06 00:30:49,closed,embray,v3.2.1,2,,https://api.github.com/repos/spacetelescope/PyFITS/issues/23,b'Header modifications of compressed HDU are not writen to file after flush or close',"b'When opening a fits file in update mode, and modifying the header of a \r\nhdu.compressed.CompImageHDU, the modifications are not written down to the fits file when closing the file.\r\n\r\nModification to the _header attribute on the other hand are written down. But the documentation says : \r\n""The contents of the corresponding binary table HDU may be accessed using the hidden ._header attribute. However, all user interface with the HDU header should be accomplished through the image header (the .header attribute)""\r\n\r\nSo in my understanding, modifications to the .header attribute should be written down to file.\r\n\r\nMy version of pyfits is 3.1.2\r\n\r\nHere is an example demonstrating the odd behavior, where /tmp/test.fits contains a compressed image.\r\n```\r\nimport pyfits\r\n# Open fits file in update mode\r\nhdulist = pyfits.open(""/tmp/test.fits"", mode = \'update\')\r\n\r\n# Print the type of the second hdu\r\nprint type(hdulist[1])\r\n>>> <class \'pyfits.hdu.compressed.CompImageHDU\'>\r\n\r\n# Assign a test value to a test keyword of the .header\r\nhdulist[1].header[\'test\'] = ""test""\r\nprint hdulist[1].header[\'test\']\r\n>>> test\r\n# Assign a test value to a test keyword of the ._header\r\nhdulist[1]._header[\'test2\'] = ""test2""\r\nprint hdulist[1]._header[\'test2\']\r\n>>> test2\r\n\r\n# Close the file. I expect that modification to the header would be written to the fits file\r\nhdulist.close()\r\n\r\n# Verify if it has been really updated\r\nhdulist2 = pyfits.open(""/tmp/test.fits"")\r\nprint \'test\' in hdulist2[1].header\r\n>>> False\r\nprint \'test2\' in hdulist2[1].header\r\n>>> True\r\n\r\n```'"
spacetelescope/PyFITS#58,32100015,embray,embray,2014-04-23 21:14:02,2014-04-24 16:04:27,2014-04-24 16:03:28,closed,,v3.2.3,0,,https://api.github.com/repos/spacetelescope/PyFITS/issues/58,b'Fix astropy #2363',b'astropy/astropy#2363 looks like it was probably introduced by the fix for #23.'
mathjax/MathJax#795,31907433,eteq,,2014-04-21 16:59:05,2015-02-26 09:26:14,None,open,,A future font release,15,Feature Request,https://api.github.com/repos/mathjax/MathJax/issues/795,b'Angstrom symbol (\\AA) does not render in MathJax',"b""It appears the [Angstrom symbol](http://en.wikipedia.org/wiki/Angstrom) is not rendering in MathJax, despite being quite common in LaTeX packages, and has a unicode character (U+212B , normalized to U+00C5 ).  Can this be added?  It's used in a variety of physical sciences.\r\n\r\nSee astropy/astropy#2286 ipython/ipython#5533 for the original trigger/use-case for this."""
ipython/ipython#5533,30921694,eteq,eteq,2014-04-05 19:01:24,2014-05-15 08:53:14,None,open,,wishlist,4,,https://api.github.com/repos/ipython/ipython/issues/5533,b'Angstrom latex (or any \\r{...}) not rendering in notebook',"b'If I make the following notebook cell:\r\n```\r\n%%latex\r\n4000 \\r{A}\r\n```\r\nWhat I *expect* is to see for the second part is the ""angstrom"" symbol - an A with a ring over it.  But instead I just see ``4000 \\r{A}`` in the output.  Is there some reason \\r isn\'t supposed to work in ipython\'s latex, or is this a bug? \r\n\r\nThe actual use case here are objects that have a `_repr_latex_` that want to be able to do ``\\AA`` or ``\\r{A}``, but the above is a more stripped-down example of what is probably the same issue.'"
MathJax/MathJax#795,31907433,eteq,,2014-04-21 16:59:05,2015-02-26 09:26:14,None,open,,A future font release,15,Feature Request,https://api.github.com/repos/mathjax/MathJax/issues/795,b'Angstrom symbol (\\AA) does not render in MathJax',"b""It appears the [Angstrom symbol](http://en.wikipedia.org/wiki/Angstrom) is not rendering in MathJax, despite being quite common in LaTeX packages, and has a unicode character (U+212B , normalized to U+00C5 ).  Can this be added?  It's used in a variety of physical sciences.\r\n\r\nSee astropy/astropy#2286 ipython/ipython#5533 for the original trigger/use-case for this."""
numpy/numpy#4782,35008874,juliantaylor,charris,2014-06-04 22:03:42,2014-09-23 19:07:41,2014-06-04 22:36:18,closed,,,2,,https://api.github.com/repos/numpy/numpy/issues/4782,b'BUG: fix where not filling string types properly',b'the copyswap part of where used the input arrays descriptions to copy\r\ninto the destination so if they had a smaller size the destination was\r\nnot properly padded with zeros.\r\nCloses gh-4778'
numpy/numpy#4778,34990153,taldcroft,charris,2014-06-04 18:46:17,2014-06-04 22:36:18,2014-06-04 22:36:18,closed,,1.9 blockers,15,11 - Bug,https://api.github.com/repos/numpy/numpy/issues/4778,b'PR #4606 introduces regression into astropy table join',"b""The numpy commit e47a50efc284c677ba4d0337e11dc8514fca7e5b causes a failure of an astropy test that works fine with all previous numpy versions through 1.5.  This was determined with git bisect on numpy dev.\r\n\r\nThe failure manifests as an apparent leak of memory bytes between two tables during a table join operation.  This is demonstrated below.  Apologies that I haven't had time to make a simpler test to reproduce without using the `astropy.table.np_utils.join` function, but perhaps the symptoms may be obvious enough?  If not I can try harder to figure out where this is happening within the `join` function.\r\n\r\nThis is discussed in https://github.com/astropy/astropy/issues/2281.\r\n\r\n```\r\nIn [8]: t1\r\nOut[8]: \r\narray([('M31', '2012-01-02', 17.0, 16.0),\r\n       ('M82', '2012-10-29', 16.2, 15.2),\r\n       ('M101', '2012-10-31', 15.1, 15.5)], \r\n      dtype=[('name', 'S4'), ('obs_date', 'S10'), ('mag_b', '<f8'), ('mag_v', '<f8')])\r\n\r\nIn [9]: t2\r\nOut[9]: \r\narray([('NGC3516', '2011-11-11', 42.1), ('M31', '1999-01-05', 43.1),\r\n       ('M82', '2012-10-29', 45.0)], \r\n      dtype=[('name', 'S7'), ('obs_date', 'S10'), ('logLx', '<f8')])\r\n\r\nIn [10]: from astropy.table import np_utils\r\n\r\nIn [11]: t3 = np_utils.join(t1, t2, keys='name', join_type='left')\r\n\r\nIn [12]: t3\r\nOut[12]: \r\nmasked_array(data = [('M101516', '2012-10-31', 15.1, 15.5, --, --)\r\n ('M31', '2012-01-02', 17.0, 16.0, '1999-01-05', 43.1)\r\n ('M82', '2012-10-29', 16.2, 15.2, '2012-10-29', 45.0)],\r\n             mask = [(False, False, False, False, True, True)\r\n (False, False, False, False, False, False)\r\n (False, False, False, False, False, False)],\r\n       fill_value = ('N/A', 'N/A', 1e+20, 1e+20, 'N/A', 1e+20),\r\n            dtype = [('name', 'S7'), ('obs_date_1', 'S10'), ('mag_b', '<f8'), ('mag_v', '<f8'), ('obs_date_2', 'S10'), ('logLx', '<f8')])\r\n\r\n\r\nIn [13]: print t3\r\n[('M101516', '2012-10-31', 15.1, 15.5, --, --)\r\n ('M31', '2012-01-02', 17.0, 16.0, '1999-01-05', 43.1)\r\n ('M82', '2012-10-29', 16.2, 15.2, '2012-10-29', 45.0)]\r\n```\r\nThe expected first value of the first row is `'M101'`.  It appears that the final three characters of the first value in the `t2` table are leaking into the `t3` table (the `516` characters from `NGC3516`)."""
numpy/numpy#4564,30387799,mhvk,,2014-03-28 13:46:09,2015-06-20 19:35:56,None,open,,,3,,https://api.github.com/repos/numpy/numpy/issues/4564,b'BUG .flat on ndarray subclasses does not return subclass items',"b""In astropy, we have defined an array subclass `Quantity` that carries units. One method that does not always keep subclass information is `.flat`:\r\n```\r\nimport astropy.units as u\r\nq = u.Quantity([1., 2., 3.], 'm')\r\nq.flat[:2]\r\n# <Quantity [ 1., 2.] m>  --> slices are fine\r\nq.flat[0], q[0]\r\n# (1.0, <Quantity 1.0 m>)  --> this is not; returned a float\r\nfor _q in q.flat: print(_q)\r\n# 1.0\r\n# 2.0\r\n# 3.0  --> not good either\r\n```\r\n\r\nRight now, we are working around this by overriding `.flat` and using our own iterator, similar to what is done in `MaskedArray` with `MaskedIterator`. In principle, though, it would seem that the iterator should not just return python float, but perhaps pass through some hook that can be overridden (possibly `__array_finalize__`)."""
spacetelescope/PyFITS#23,16136915,bmampaey,embray,2013-06-28 11:02:14,2014-04-23 21:14:02,2014-02-06 00:30:49,closed,embray,v3.2.1,2,,https://api.github.com/repos/spacetelescope/PyFITS/issues/23,b'Header modifications of compressed HDU are not writen to file after flush or close',"b'When opening a fits file in update mode, and modifying the header of a \r\nhdu.compressed.CompImageHDU, the modifications are not written down to the fits file when closing the file.\r\n\r\nModification to the _header attribute on the other hand are written down. But the documentation says : \r\n""The contents of the corresponding binary table HDU may be accessed using the hidden ._header attribute. However, all user interface with the HDU header should be accomplished through the image header (the .header attribute)""\r\n\r\nSo in my understanding, modifications to the .header attribute should be written down to file.\r\n\r\nMy version of pyfits is 3.1.2\r\n\r\nHere is an example demonstrating the odd behavior, where /tmp/test.fits contains a compressed image.\r\n```\r\nimport pyfits\r\n# Open fits file in update mode\r\nhdulist = pyfits.open(""/tmp/test.fits"", mode = \'update\')\r\n\r\n# Print the type of the second hdu\r\nprint type(hdulist[1])\r\n>>> <class \'pyfits.hdu.compressed.CompImageHDU\'>\r\n\r\n# Assign a test value to a test keyword of the .header\r\nhdulist[1].header[\'test\'] = ""test""\r\nprint hdulist[1].header[\'test\']\r\n>>> test\r\n# Assign a test value to a test keyword of the ._header\r\nhdulist[1]._header[\'test2\'] = ""test2""\r\nprint hdulist[1]._header[\'test2\']\r\n>>> test2\r\n\r\n# Close the file. I expect that modification to the header would be written to the fits file\r\nhdulist.close()\r\n\r\n# Verify if it has been really updated\r\nhdulist2 = pyfits.open(""/tmp/test.fits"")\r\nprint \'test\' in hdulist2[1].header\r\n>>> False\r\nprint \'test2\' in hdulist2[1].header\r\n>>> True\r\n\r\n```'"
barentsen/astropy#1,27160221,mdboom,barentsen,2014-02-07 18:33:59,2014-06-26 20:13:37,2014-02-07 18:47:12,closed,,,1,,https://api.github.com/repos/barentsen/astropy/issues/1,"b""Display 'alt' correctly in wcslint""",
astropy/photutils#32,27760400,kbarbary,eteq,2014-02-18 00:41:34,2014-06-25 20:55:44,2014-02-27 08:03:55,closed,,,7,,https://api.github.com/repos/astropy/photutils/issues/32,b'Doc Improvements',"b'This adds a note about coordinate conventions in photutils (specifically, which axes of the data array the x and y coordinates refer to) as requested by @eteq.\r\n\r\nAlong the way, it also fixes some rst formatting (mainly single vs double backticks) and improves a section of the docs that were outdated.\r\n\r\nAlso also, fixes a bug where the `method` keyword was not being passed along by the wrapper functions (and so was being ignored).'"
numpy/numpy#4494,29448082,embray,juliantaylor,2014-03-14 16:17:02,2014-06-18 07:41:22,2014-03-17 20:50:17,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/4494,b'Adds missing Py_INCREF in array_boolean_subscript',"b'This bug was introduced, as far as I can tell, in 52b4a94.  It came up in the context of astropy/astropy#1987 where we were getting semi-irregular segfaults.  The problem is that after this line a new array is created with `PyArray_NewFromDescr` using an existing dtype object, but it does not incref that dtype object.\r\n\r\nA reliable way that I found to demonstrate this bug was something like:\r\n\r\n```\r\n>>> import numpy as np\r\n>>> c = np.rec.array([(1, 2, 3), (4, 5, 6)])\r\n>>> masked = c[np.array([True, False])]\r\n>>> base = masked.base\r\n>>> del masked, c\r\n>>> base.dtype\r\n```\r\n\r\nat this point various different things could happen, but most often it just results in a segfault.'"
numpy/numpy#4804,35534422,seberg,charris,2014-06-11 22:32:44,2014-07-06 19:06:07,2014-07-06 16:06:50,closed,,1.9 blockers,56,,https://api.github.com/repos/numpy/numpy/issues/4804,"b'WIP: Fix matplotlib, etc. errors'",b'I would like the deprecation warning to show the actual error message. It might also make sense to replace the error again with the original one if the fallback failed as well.'
numpy/numpy#4086,23539162,astrofrog,astrofrog,2013-12-01 16:45:13,2013-12-18 17:19:32,2013-12-06 22:16:51,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/4086,b'MaskedArray.__getitem__ does not call __array_finalize__',"b'``MaskedArray.__getitem__`` does not call ``__array_finalize__`` before returning the slice (unlike ``ndarray.__getitem__``). This causes issues for sub-classes of ``MaskedArray``. As a workaround, sub-classes can overload ``_update_from`` but this is a hack.\r\n\r\nThis should be reasonably straightforward to fix, so if no one else does it soon I will try and open a pull request.'"
numpy/numpy#4129,24504541,taldcroft,taldcroft,2013-12-18 17:19:32,2013-12-31 01:56:26,2013-12-31 01:56:26,closed,,,1,,https://api.github.com/repos/numpy/numpy/issues/4129,"b""Pickling MaskedArray subclass doesn't preserve extra attributes""","b'I\'m having trouble getting a MaskedArray subclass to round-trip through pickle and preserve the extra subclass attributes.  I\'m not sure if this is a bug or a lack of understanding on my part.  Following is the strawman code I\'m using, which is basically a stripped down version of `numpy/ma/tests/test_subclassing.py`.  The only change is the addition of a `dtype` keyword arg in `MSubArray.__new__` (otherwise pickling fails completely).\r\n```\r\nimport numpy as np\r\nimport cPickle as pickle\r\nfrom numpy import ma\r\n\r\n\r\nclass SubArray(np.ndarray):\r\n    """"""Defines a generic np.ndarray subclass, that stores some metadata\r\n    in the  dictionary `info`.""""""\r\n    def __new__(cls, arr, info={}):\r\n        x = np.asanyarray(arr).view(cls)\r\n        x.info = info\r\n        return x\r\n\r\n    def __array_finalize__(self, obj):\r\n        self.info = getattr(obj, \'info\', {\'ATTR\': \'MISSING\'})\r\n        return\r\n\r\n\r\nclass MSubArray(SubArray, ma.MaskedArray):\r\n    def __new__(cls, data, info={}, mask=ma.nomask, dtype=None):\r\n        subarr = SubArray(data, info)\r\n        _data = ma.MaskedArray.__new__(cls, data=subarr, mask=mask, dtype=dtype)\r\n        _data.info = subarr.info\r\n        return _data\r\n\r\n    def __array_finalize__(self, obj):\r\n        ma.MaskedArray.__array_finalize__(self, obj)\r\n        SubArray.__array_finalize__(self, obj)\r\n        return\r\n\r\nms = MSubArray([1, 2], info={\'a\': 1})\r\nprint(\'Pre-pickle:\', ms.info, ms.data.info)\r\n\r\npkl = pickle.dumps(ms)\r\nms_from_pkl = pickle.loads(pkl)\r\nprint(\'Post-pickle:\', ms_from_pkl.info, ms_from_pkl.data.info)\r\n```\r\nThis produces:\r\n```\r\nPre-pickle: {\'a\': 1} {\'a\': 1}\r\nPost-pickle: {} {}\r\n```\r\n\r\nThis is related to the suggestion of @pierregm in #4086 that we rework the way that astropy Table subclasses MaskedArray (https://github.com/astropy/astropy/pull/1872).  I\'m using numpy 1.7.1 here.\r\n'"
gammapy/gammapy#28,17650787,cdeil,cdeil,2013-08-05 18:44:30,2013-09-06 08:21:46,2013-09-06 08:21:46,closed,cdeil,0.1,1,bug,https://api.github.com/repos/gammapy/gammapy/issues/28,b'Python 3.2 tests fail on travis-ci',"b""This is just a reminder ... it's an astropy issue:\r\nhttps://github.com/astropy/astropy/issues/1256\r\n\r\nI'll disable Python 3.2 tests on travis-ci for now in gammapy until this gets fixed."""
numpy/numpy#3265,13439475,jamestwebber,charris,2013-04-20 18:50:49,2014-07-27 08:13:37,2013-04-28 00:13:00,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/3265,b'Update masked array copy to preserve array order',"b""Using 'K' to try to match array order, fixes https://github.com/numpy/numpy/issues/3156"""
STScI-Citizen-Science/MTPipeline#102,36835736,acviana,kevinfhale,2014-06-30 20:31:15,2014-07-07 18:35:09,2014-07-07 18:35:09,closed,acviana,WFPC2,16,new-feature,https://api.github.com/repos/STScI-Citizen-Science/MTPipeline/issues/102,b'Get Travis CI to Pass',"b""I'm creating a branch to get the tests organized and to get the Travis CI build to pass."""
gammapy/gammapy#22,16397316,cdeil,cdeil,2013-07-05 10:36:01,2013-08-15 15:28:00,2013-08-15 15:28:00,closed,cdeil,0.1,3,bug,https://api.github.com/repos/gammapy/gammapy/issues/22,b'cython not found by setup.py',"b""I am trying to add Cython files to this package which I forked from `astropy/package-template`, but for some reason `setup.py` doesn't find `cython`:\r\n```\r\n$ python setup.py build\r\nrunning build\r\nrunning build_py\r\nrunning build_ext\r\nerror: tevpy/example_c.c: Could not find C file tevpy/example_c.c for Cython file tevpy/example_c.pyx when building extension tevpy.example_c. Cython must be installed to build from a git checkout\r\n```\r\n\r\nTo reproduce:\r\n```\r\ngit clone https://github.com/gammapy/tevpy.git\r\ncd tevpy\r\ncurl -L https://raw.github.com/astropy/package-template/master/packagename/example_c.pyx > tevpy/example_c.pyx\r\npython setup.py build\r\n```\r\n\r\n@astrofrog @iguananaut I don't understand why this works for other affiliated packages (I tried with `package-template` and `astroquery`), but not in `tevpy`! Can one of you please have a look?\r\n"""
STScI-Citizen-Science/MTPipeline#108,37286110,kevinfhale,acviana,2014-07-07 16:33:17,2014-07-07 19:24:48,2014-07-07 18:32:32,closed,,,4,intern;Kevin,https://api.github.com/repos/STScI-Citizen-Science/MTPipeline/issues/108,b'Merging travis-fix with master',"b""As of [Build#37](https://travis-ci.org/STScI-Citizen-Science/MTPipeline/builds/29332642), run on July 7th 2014, the pipeline installs, and its tests pass, on the Travis virtual environment. Three major things have been done to make this happen:\r\n\r\n1. The `CPLUS_INCLUDE_PATH` and `LIBRARY_PATH` have been manually set in `.travis.yml`  to `/sw/include` and `/sw/lib`, respectively, where we set the `cfitsio` library to build. This is terribly kludgy, as it overwrites anything that might have already been in these paths. At the moment, they are undefined prior to our setting them. Curiously, attempting to merely append the `/sw` directories doesn't work. This is something to come back to, and a potential source of C-related errors in the future.\r\n2. In `setup.py`, we force using `Pillow` version 2.4.0. Installing the latest version appears to cause complications in the installation of `matplotlib`. In the new version of `Pillow`, `Pillow` successfully builds using multiple processes, doing some kind of multithreading. However, after `Pillow` is installed by this method, `matplotlib` tries to install in the same way, and this produces a fatal exception.\r\n3. In `setup.py`, we force using `astropy` version 0.3.2. At the moment, `setup.py` automatically grabs version 0.4rc1, which is [apparently unreleased](https://github.com/astropy/astropy/blob/master/CHANGES.rst) as per their changelog. Using this version appears to result in complications with cython, and may be related to their [Ticket#1198](https://github.com/astropy/astropy/pull/1198), which was closed last year.\r\n\r\n\r\nAdditionally, the python test files have been rearranged. Four test files are listed in this request as deletions, however all four of them are already present (in both the `master` and `travis-fix` branches) inside `tests/legacy`. The merge should remove the unnecessary duplications.\r\n\r\nIt's a little concerning how easily updates to our dependencies can introduce errors in our travis build. However, neither the `Pillow` or `astropy` issues appear impact building the pipeline on my STScI laptop, probably because it's using the older versions of those packages by default."""
atpy/atpy#44,4366068,ebressert,astrofrog,2012-05-01 12:04:07,2013-07-05 11:49:28,2013-07-05 11:49:28,closed,,,9,Feature Request,https://api.github.com/repos/atpy/atpy/issues/44,b'Integrate Pandas into ATpy',"b""Pandas has some notable features that ATpy could use:\r\n\r\n(copy-pasted from Pandas website)\r\n\r\n* Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data\r\n* Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data\r\n* Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects\r\n* Intelligent label-based slicing, fancy indexing, and subsetting of large data sets\r\n* Intuitive merging and joining data sets\r\n* Flexible reshaping and pivoting of data sets\r\n\r\nWould it be possible to integrate into ATpy? I'll start testing this to see if it's possible. If anyone else is interested, fork it :). \r\n"""
pydata/pandas#9989,70976302,astrofrog,jreback,2015-04-25 22:27:15,2015-04-25 22:40:01,2015-04-25 22:39:32,closed,,,1,Compat;Numeric,https://api.github.com/repos/pydata/pandas/issues/9989,"b'""Big-endian buffer not supported on little-endian compiler"" when accessing multiple columns'","b""If I create a dataframe with two arrays that have different endianness, it works fine if I access the whole DataFrame or individual arrays:\r\n\r\n```python\r\nIn [1]: import pandas\r\n\r\nIn [2]: d = pandas.DataFrame()\r\n\r\nIn [3]: import numpy as np\r\n\r\nIn [4]: d['a'] = np.array([1,2,3], dtype='>f4')\r\n\r\nIn [5]: d['b'] = np.array([1,2,3], dtype='<f4')\r\n\r\nIn [6]: d['a']\r\nOut[6]: \r\n0    1\r\n1    2\r\n2    3\r\nName: a, dtype: float32\r\n\r\nIn [7]: d['b']\r\nOut[7]: \r\n0    1\r\n1    2\r\n2    3\r\nName: b, dtype: float32\r\n\r\nIn [8]: d\r\nOut[8]: \r\n   a  b\r\n0  1  1\r\n1  2  2\r\n2  3  3\r\n```\r\n\r\nHowever if I try and access multiple arrays in one go I get an error:\r\n\r\n```\r\nIn [9]: d[['a','b']]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-63d4e2414242> in <module>()\r\n----> 1 d[['a','b']]\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/frame.py in __getitem__(self, key)\r\n   1772         if isinstance(key, (Series, np.ndarray, Index, list)):\r\n   1773             # either boolean or fancy integer index\r\n-> 1774             return self._getitem_array(key)\r\n   1775         elif isinstance(key, DataFrame):\r\n   1776             return self._getitem_frame(key)\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/frame.py in _getitem_array(self, key)\r\n   1817         else:\r\n   1818             indexer = self.ix._convert_to_indexer(key, axis=1)\r\n-> 1819             return self.take(indexer, axis=1, convert=True)\r\n   1820 \r\n   1821     def _getitem_multilevel(self, key):\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/generic.py in take(self, indices, axis, convert, is_copy)\r\n   1330         new_data = self._data.take(indices,\r\n   1331                                    axis=self._get_block_manager_axis(axis),\r\n-> 1332                                    convert=True, verify=True)\r\n   1333         result = self._constructor(new_data).__finalize__(self)\r\n   1334 \r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/internals.py in take(self, indexer, axis, verify, convert)\r\n   3273         new_labels = self.axes[axis].take(indexer)\r\n   3274         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\r\n-> 3275                                     axis=axis, allow_dups=True)\r\n   3276 \r\n   3277     def merge(self, other, lsuffix='', rsuffix=''):\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/internals.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\r\n   3155         if axis == 0:\r\n   3156             new_blocks = self._slice_take_blocks_ax0(\r\n-> 3157                 indexer, fill_tuple=(fill_value,))\r\n   3158         else:\r\n   3159             new_blocks = [blk.take_nd(indexer, axis=axis,\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/internals.py in _slice_take_blocks_ax0(self, slice_or_indexer, fill_tuple)\r\n   3236                     blocks.append(blk.take_nd(\r\n   3237                         blklocs[mgr_locs.indexer], axis=0,\r\n-> 3238                         new_mgr_locs=mgr_locs, fill_tuple=None))\r\n   3239 \r\n   3240         return blocks\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/internals.py in take_nd(self, indexer, axis, new_mgr_locs, fill_tuple)\r\n    844             fill_value = self.fill_value\r\n    845             new_values = com.take_nd(self.get_values(), indexer, axis=axis,\r\n--> 846                                      allow_fill=False)\r\n    847         else:\r\n    848             fill_value = fill_tuple[0]\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/core/common.py in take_nd(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\r\n    821 \r\n    822     indexer = _ensure_int64(indexer)\r\n--> 823     func(arr, indexer, out, fill_value)\r\n    824 \r\n    825     if flip_order:\r\n\r\n/Users/tom/miniconda3/envs/production/lib/python3.4/site-packages/pandas/algos.so in pandas.algos.take_2d_axis0_float32_float32 (pandas/algos.c:89026)()\r\n\r\nValueError: Big-endian buffer not supported on little-endian compiler\r\n```"""
glue-viz/glue-3d-selection#4,151027881,PennyQ,PennyQ,2016-04-26 02:55:40,2016-04-28 18:49:36,2016-04-28 18:49:36,closed,,,3,,https://api.github.com/repos/glue-viz/glue-3d-selection/issues/4,b'3dfloodfill with Cython',"b""There is a problem here to set input data as the FITS data with `dtype('>f4')` to Cython, the definition of Cython function is as below: \r\n\r\n`def cyfill(np.ndarray[np.float64_t, ndim=3] data, tuple start_coords, Py_ssize_t fill_value, float threshold):`\r\n\r\nbut I got an error as `ValueError: Big-endian buffer not supported on little-endian compiler`"""
sunpy/sunpy#871,28430770,DanRyanIrish,Cadair,2014-02-27 16:06:14,2014-05-05 09:34:11,2014-04-30 14:09:52,closed,Cadair,,5,0.4.x;Bug?,https://api.github.com/repos/sunpy/sunpy/issues/871,b'.peek() function for LYRALightCurve object not working',"b""Hi guys,\r\nI've started using sunpy to do some LYRA analysis and I've noticed that the .peek() function for the LYRA object doesn't appear to work.  Is this a known problem or have misunderstood something in the installation process etc.?  The error I get is\r\n```Python\r\n>>>lc = sunpy.lightcurve.LYRALightCurve.create(file)\r\n>>>lc.peek()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-96-48ba98674805> in <module>()\r\n----> 1 lc.peek()\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/sunpy/lightcurve/sources/lyra.pyc in peek(self, names, **kwargs)\r\n     57         axes = plt.gca()\r\n     58 \r\n---> 59         axes = self.data.plot(ax=axes, subplots=True, sharex=True, **kwargs)\r\n     60         #plt.legend(loc='best')\r\n     61 \r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in plot_frame(frame, x, y, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, style, title, xlim, ylim, logx, logy, xticks, yticks, kind, sort_columns, fontsize, secondary_y, **kwds)\r\n   1733                              secondary_y=secondary_y, **kwds)\r\n   1734 \r\n-> 1735     plot_obj.generate()\r\n   1736     plot_obj.draw()\r\n   1737     if subplots:\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in generate(self)\r\n    903         self._compute_plot_data()\r\n    904         self._setup_subplots()\r\n--> 905         self._make_plot()\r\n    906         self._post_plot_logic()\r\n    907         self._adorn_subplots()\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_plot(self)\r\n   1319             lines = []\r\n   1320             labels = []\r\n-> 1321             x = self._get_xticks(convert_period=True)\r\n   1322 \r\n   1323             plotf = self._get_plot_function()\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _get_xticks(self, convert_period)\r\n   1067                 x = index._mpl_repr()\r\n   1068             elif is_datetype:\r\n-> 1069                 self.data = self.data.sort_index()\r\n   1070                 x = self.data.index._mpl_repr()\r\n   1071             else:\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc in sort_index(self, axis, by, ascending, inplace, kind)\r\n   2626             self._update_inplace(new_data)\r\n   2627         else:\r\n-> 2628             return self.take(indexer, axis=axis, convert=False, is_copy=False)\r\n   2629 \r\n   2630     def sortlevel(self, level=0, axis=0, ascending=True, inplace=False):\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc in take(self, indices, axis, convert, is_copy)\r\n   1173                                                axis=baxis)\r\n   1174         else:\r\n-> 1175             new_data = self._data.take(indices, axis=baxis)\r\n   1176 \r\n   1177         result = self._constructor(new_data).__finalize__(self)\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc in take(self, indexer, new_index, axis, verify)\r\n   3421                           ref_items=new_axes[0],\r\n   3422                           new_axis=new_axes[axis],\r\n-> 3423                           axis=axis)\r\n   3424 \r\n   3425     def merge(self, other, lsuffix=None, rsuffix=None):\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc in apply(self, f, *args, **kwargs)\r\n   2373 \r\n   2374             else:\r\n-> 2375                 applied = getattr(blk, f)(*args, **kwargs)\r\n   2376 \r\n   2377             if isinstance(applied, list):\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc in take(self, indexer, ref_items, new_axis, axis)\r\n    890             raise AssertionError('axis must be at least 1, got %d' % axis)\r\n    891         new_values = com.take_nd(self.values, indexer, axis=axis,\r\n--> 892                                  allow_fill=False)\r\n    893 \r\n    894         # need to preserve the ref_locs and just shift them\r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/core/common.pyc in take_nd(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\r\n    675     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype,\r\n    676                                  axis=axis, mask_info=mask_info)\r\n--> 677     func(arr, indexer, out, fill_value)\r\n    678     return out\r\n    679 \r\n\r\n/Users/danielr/anaconda/lib/python2.7/site-packages/pandas/algos.so in pandas.algos.take_2d_axis1_float64_float64 (pandas/algos.c:80824)()\r\n\r\nValueError: Big-endian buffer not supported on little-endian compiler\r\n```\r\n\r\nMany thanks\r\nDan Ryan"""
pydata/pandas#3778,15228983,cdeil,cpcloud,2013-06-06 15:29:54,2013-06-06 19:00:03,2013-06-06 19:00:03,closed,,,25,,https://api.github.com/repos/pydata/pandas/issues/3778,b'DataFrame fails late when initialized from structured array with non-native byte order',"b""I ran into this issue trying to read in astronomy [FITS](http://fits.gsfc.nasa.gov) data into a pandas DataFrame:\r\nhttps://github.com/astropy/astropy/issues/1156\r\n\r\nNote that this doesn't fail on DataFrame construction, but on column selection (the last line):\r\n```\r\nimport pandas as pd\r\nfrom astropy.io import fits\r\ndata = fits.getdata('table.fits', 1)\r\ntable = pd.DataFrame(data)\r\ntable = table.set_index(['ROI', 'Solution'])\r\nprint(table.head())\r\n#print(table.xs('HESS_J1023m575'))\r\nprint(table[['nfev', 'statname']])\r\n```\r\n\r\nIs it possible to either make this work directly or to fail on DataFrame construction and tell the user to e.g. fix the issue by setting an option `fix_byte_order=True`?\r\n"""
liberfa/erfa#4,15567232,taldcroft,eteq,2013-06-14 16:44:04,2014-05-06 16:10:00,2014-05-06 16:10:00,closed,,v1.1.0,7,,https://api.github.com/repos/liberfa/erfa/issues/4,b'TAI => UTC => TAI does not round-trip for certain dates',"b'For dates in the range where the UTC scale was variable (around 1961? to 1972), the SOFA routines iauTaiutc and iauUtctai do not round trip.  In other words after converting from TAI => UTC => TAI, the end result is not identical to the input.  This seems like an inconsistency, but perhaps I\'m not understanding some subtlety in this situation?\r\n\r\nBelow is a demonstration of the issue.\r\n```\r\n#include <stdio.h>\r\n#include ""sofa.h""\r\n \r\nint main()\r\n{\r\n    int j, iy, im, id, ihmsf[4];\r\n    double a1, a2, u1, u2, a1_rt, a2_rt;\r\n    /* Encode TAI date and time into internal format. */\r\n    j = iauDtf2d ( ""TAI"", 1965, 1, 1, 0, 0, 33.7, &a1, &a2 );\r\n    if ( j ) return 1;\r\n\r\n    /* Transform TAI to UTC. */\r\n    j = iauTaiutc ( a1, a2, &u1, &u2 );\r\n    if ( j ) return 1;\r\n\r\n    /* Transform UTC back to TAI */\r\n    j = iauUtctai ( u1, u2, &a1_rt, &a2_rt );\r\n    if ( j ) return 1;\r\n\r\n    /* Print round trip difference */\r\n    printf(""TAI -> UTC -> TAI round trip difference: %f seconds\\n"", \r\n           ((a1_rt - a1) + (a2_rt - a2))*86400);\r\n\r\n    /* Output: TAI -> UTC -> TAI round trip difference: -0.002592 seconds */\r\n\r\n    return 0;\r\n}\r\n```'"
aplpy/aplpy#102,13095075,keflavich,astrofrog,2013-04-11 23:26:39,2014-06-24 19:42:20,2013-04-21 08:58:06,closed,,,7,,https://api.github.com/repos/aplpy/aplpy/issues/102,b'fix AVM initialization',"b'Per the PRs on pyavm, use `AVM.from_file`, `AVM.from_header`.  Ignore this PR if the pyavm issue (https://github.com/astrofrog/pyavm/issues/10) is resolved.'"
spacetelescope/PyFITS#28,17063126,embray,embray,2013-07-22 18:36:52,2013-07-26 17:16:14,2013-07-26 17:16:14,closed,,,0,port-3.0.x,https://api.github.com/repos/spacetelescope/PyFITS/issues/28,b'Backport fix for Astropy #839',b'The fix for astropy/astropy#839 should be backported to PyFITS.'
atpy/atpy#53,7199207,cdeil,astrofrog,2012-09-27 22:07:36,2013-07-05 12:43:47,2013-07-05 11:46:18,closed,,,3,,https://api.github.com/repos/atpy/atpy/issues/53,b'Table.add_column speed and reading files > 2 GB',"b""I tried reading a 3GB FITS table and got only zeros.\n\nThen I wrote this script to reproduce the bug:\nhttps://gist.github.com/3796624\n\nI now noticed that the problem is that pyfits can't read files > 2 GB, given by the 32 bit limit, even though my python is 64 bit. Using the most recent astropy.io.fits it works. So I guess this is really a pyfits bug? It should at least warn instead of returning only zeros as data?\n\nWhat I wanted to mention for atpy is that  repeated Table.add_column calls is a slow way to create a table, because each time a copy of the table data is made. Maybe you can mention this in the docs and suggest a fast way to build a table out of many numpy arrays?\n\nAlso note that adding the first column takes 3 sec, but the second column only 0.04 sec.\nEven if there is overhead setting up some Table metadata for the first column this shouldn't take 3 seconds, right?"""
numpy/numpy#2951,10347202,dmvianna,charris,2013-01-27 04:15:29,2014-08-20 13:06:11,2014-03-25 18:05:08,closed,,,4,component: numpy.core;priority: normal;Proposal,https://api.github.com/repos/numpy/numpy/issues/2951,b'numpy.int64 is not instance of int',"b""As reported in [xlwt](https://github.com/python-excel/xlwt/issues/15):\r\n\r\nHere is an examination of numpy behaviour (Python 2.7.3, numpy 1.6.2)\r\n```python\r\n>>> import numpy\r\n>>> data = [t(123456) for t in (numpy.int32, numpy.int64, numpy.float64)]\r\n>>> [type(d) for d in data]\r\n[<type 'numpy.int32'>, <type 'numpy.int64'>, <type 'numpy.float64'>]\r\n>>> data\r\n[123456, 123456, 123456.0]\r\n>>> check_types = (int, long, float)\r\n>>> for d in data:\r\n...     for c in check_types:\r\n...         print type(d), repr(c), isinstance(d, c)\r\n...\r\n<type 'numpy.int32'> <type 'int'> True\r\n<type 'numpy.int32'> <type 'long'> False\r\n<type 'numpy.int32'> <type 'float'> False\r\n<type 'numpy.int64'> <type 'int'> False\r\n<type 'numpy.int64'> <type 'long'> False\r\n<type 'numpy.int64'> <type 'float'> False\r\n<type 'numpy.float64'> <type 'int'> False\r\n<type 'numpy.float64'> <type 'long'> False\r\n<type 'numpy.float64'> <type 'float'> True\r\n>>>\r\n```\r\nLooks like numpy has done the work to make its int32 and float64 recognisable by other software but not int64."""
taldcroft/asciitable#29,6671053,hamogu,hamogu,2012-09-05 19:44:15,2015-07-31 15:03:13,2015-07-31 13:12:31,closed,,,6,,https://api.github.com/repos/taldcroft/asciitable/issues/29,b'Use DAOPHOT fixed width information',"b""Really, DAOPhot uses fixed width tables. So far, ``asciitable`` just used the normal splitter, but that sometimes fails, if values in columns are so long that there is not white space in between. This patch makes full use of the width info in the DAOPhot headers and the FixedWidthSplitter.\nPlease check with nose, though (I don't have that installed on this machine)."""
numpy/numpy#2432,7731445,numpy-gitbot,njsmith,2012-10-19 22:26:49,2012-12-08 19:57:28,2012-12-08 19:57:28,closed,,,0,11 - Bug;component: numpy.ma;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2432,b'cannot access masked array rows with np.object dtype (Trac #1839)',"b""_Original ticket http://projects.scipy.org/numpy/ticket/1839 on 2011-05-24 by trac user risa2000, assigned to @pierregm._\n\nUsing masked array with np.object type makes the rows with that type, which also contain masked values raise an exception.\n\nSlightly modified sample from documentation follows:\n\n\n    In [22]: from numpy import ma\n    \n    In [23]: y = ma.masked_array([(1,'2'), (3, '4')], mask=[(0, 0), (0, 1)], dtype=[('a', int), ('b', np.object)])\n    \n    In [24]: y[0]\n    Out[24]: (1, '2')\n    \n    In [25]: y[1]\n    ---------------------------------------------------------------------------\n    ValueError                                Traceback (most recent call last)\n    \n    f:\\Work\\python\\apps\\icao\\icao_results.py in <module>()\n    ----> 1\n          2\n          3\n          4\n          5\n    \n    C:\\Python27\\lib\\site-packages\\numpy\\ma\\core.pyc in __getitem__(self, indx)\n       2946 #                return mvoid(dout, mask=mask)\n    \n       2947                 if flatten_mask(mask).any():\n    -> 2948                     dout = mvoid(dout, mask=mask)\n       2949                 else:\n       2950                     return dout\n    \n    C:\\Python27\\lib\\site-packages\\numpy\\ma\\core.pyc in __new__(self, data, mask, dtype, fill_value)\n       5506         dtype = dtype or data.dtype\n       5507         _data = ndarray((), dtype=dtype)\n    -> 5508         _data[()] = data\n       5509         _data = _data.view(self)\n       5510         if mask is not nomask:\n    \n    ValueError: Setting void-array with object members using buffer.\n    \n    In [26]: y\n    Out[26]:\n    masked_array(data = [(1, '2') (3, --)],\n                 mask = [(False, False) (False, True)],\n           fill_value = (999999, '?'),\n                dtype = [('a', '<i4'), ('b', '|O4')])\n\n\n"""
numpy/numpy#2747,8397174,mdboom,njsmith,2012-11-15 18:50:27,2014-06-24 12:04:37,2012-12-08 19:57:26,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/2747,b'cannot access masked array rows with np.object dtype (Fixes #2432)',"b""In the astropy project we've run into bug #2432.  (See https://github.com/astropy/astropy/issues/465).  \n\nThis bug is made more prominent the recent commit 7caac2efd9b2, since `mvoid` arrays are created in all cases, not just when there are masked elements.\n\nThe fix here is just to use the Numpy array constructor instead of itemwise assignment, which does handle object fields correctly.\n\nHope this makes sense -- I can't say I'm an expert on the masked array code."""
numpy/numpy#483,7522216,astrofrog,njsmith,2012-10-11 18:35:33,2014-07-14 21:17:57,2012-10-31 18:40:36,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/483,"b'When accessing MaskedArray rows, always return an mvoid object'","b""The current behavior of MaskedArray when accessing a row object is that if all masks are ``False``, the row is an ``np.void`` object, and if any are set to ``True``, it is a ``np.ma.core.mvoid`` object. The issue with this is that users can access/modify masks for rows that already have at least one mask set to ``True``, but not for rows that have no masks set. For example:\r\n\r\n```\r\nIn [1]: a = ma.array(zip([1,2,3]), mask=[0,1,0], dtype=[('x', int)])\r\n\r\nIn [2]: a[0].mask\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-2221dd3fce8c> in <module>()\r\n----> 1 a[0].mask\r\n\r\nAttributeError: 'numpy.void' object has no attribute 'mask'\r\n\r\nIn [3]: a[1].mask\r\nOut[3]: (True,)\r\n```\r\n\r\nThere is no reason why a row should behave differently whether any values are masked or not, so this PR defaults to always returning an ``mvoid`` object, otherwise mask values cannot be set for rows that start off with no mask values set.\r\n\r\n(Of course, the present implementation in Numpy also has a performance impact - for arrays with many fields, each call to a row has to flatten the mask of the whole record just to check what type to return, which is inefficient. With this PR, row access should be faster for masked arrays. But this is secondary.)\r\n"""
