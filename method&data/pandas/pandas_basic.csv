issue,id,reporter,closed_by,created_at,updated_at,closed_at,state,assignee,milestone,comments,label_name,url,title,body
13779,167247555,tavistmorph,jreback,2016-07-24 18:47:49,2016-07-25 11:21:29,2016-07-25 11:21:29,closed,,No action,1,Bug;Difficulty Novice;Duplicate;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13779,b'pandas applymap converts zero-length dataframe to series',"b""Much simpler example\r\n```\r\nIn [1]: df = pandas.DataFrame({ 'x' : [], 'y' : [], 'z' : []})\r\n\r\nIn [2]: df\r\nOut[2]: \r\nEmpty DataFrame\r\nColumns: [x, y, z]\r\nIndex: []\r\n\r\nIn [3]: df.applymap(lambda x: x)\r\nOut[3]: \r\nx   NaN\r\ny   NaN\r\nz   NaN\r\ndtype: float64\r\n\r\nIn [4]: df = pandas.DataFrame({ 'x' : [1], 'y' : [1], 'z' : [1]})\r\n\r\nIn [5]: df.applymap(lambda x: x)\r\nOut[5]: \r\n   x  y  z\r\n0  1  1  1\r\n```\r\n\r\n\r\nThe docs for [Dataframe.applymap](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html) clearly say it will return a DataFrame. However that does not happen if you call it on a DataFrame with no rows -- it returns a series. Not only that, the series has values in it, which is not what anyone would expect from an empty DataFrame. \r\n  This is with pandas 0.18.0.\r\n\r\n```\r\n#For our use case, an 0-row dataframe is 100% legitimate. This experiment really did return 0 rows. \r\ndf = pandas.DataFrame({ 'x' : [], 'y' : [], 'z' : []})  #empty dataframe\r\nlen(df) #returns  0, just what you'd expect from an empty dataframe.\r\n\r\n#Strip off whitespace from any strings in the dataframe. Obviously not needed for this empty\r\n#dataframe, but would be needed on a dataframe that had rows. (Yes, I know there's alternative\r\n#ways to do this, I'm not looking for workarounds. I'm focused on how applymap works.)\r\ndf2 = df.applymap(lambda x : x.strip() if isinstance(x, str) and len(x) > 0 else x)\r\nlen(df2) #WHOA! This is 3, not 0. How'd that happen?\r\ntype(df2) #Oh, it returned a series instead of a dataframe. The docs do NOT say it does that. \r\n#The docs says it always returns a DataFrame, which is what most developers would expect. \r\n```\r\n\r\nAlmost anyone would expect applymap() on an empty DataFrame to return an empty DataFrame, not convert it to a Series with NaNs. We should change it to return an empty DataFrame."""
13772,167227791,sinhrks,jreback,2016-07-24 10:43:30,2016-07-24 14:19:33,2016-07-24 13:51:12,closed,,0.19.0,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13772,b'BUG: value_counts may raise OutOfBoundsDatetime',"b"" - [x] closes #13663\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nThe root cause looks normal ``Index`` raises ``OutOfBoudsDatetime``. I think it should be coerced to ``object`` dtype because user specifies no dtype.\r\n\r\n```\r\npd.Index([datetime(9999, 1, 1)])\r\n# on current master\r\n# OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 9999-01-01 00:00:00\r\n\r\n# this PR\r\nIndex([9999-01-01 00:00:00], dtype='object')\r\n```\r\n\r\n"""
13770,167221017,sinhrks,jreback,2016-07-24 07:08:39,2016-07-24 13:54:52,2016-07-24 13:54:50,closed,,0.19.0,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13770,b'BUG: datetime64[us] arrays with NaT cannot be cast to DatetimeIndex',b' - [x] closes #9114\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixed by #13692. Added tests.\r\n\r\n'
13764,167177383,gfyoung,jreback,2016-07-23 09:54:08,2016-07-24 14:01:31,2016-07-24 14:00:35,closed,,0.19.0,15,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/13764,b'BUG: Fix segfault in lib.isnullobj',"b'Weird segfault arises when you call `lib.isnullobj` with an array that uses 0-field values to mean `None`.\r\nChanged input to be a `Python` object (i.e. no typing), and the segfault went away.  Discovered when there were segfaults in printing a `DataFrame` containing such an array.\r\n\r\nCloses #13717.'"
13762,167169231,sinhrks,sinhrks,2016-07-23 05:37:22,2016-07-23 13:09:18,2016-07-23 13:09:18,closed,,0.19.0,2,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13762,b'BUG: DatetimeIndex with nanosecond frequency does not include end',b' - [x] closes #13672\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13759,167128547,sinhrks,sinhrks,2016-07-22 20:44:16,2016-07-23 13:12:45,2016-07-23 13:12:45,closed,,0.19.0,5,Bug;Categorical;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13759,"b""BUG: union_categoricals can't handle NaN""","b"" \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew not needed\r\n\r\n``union_categoricals`` doesn't handle ``NaN`` properly.\r\n\r\n**on current master:**\r\n\r\n```\r\nfrom pandas.types.concat import union_categoricals\r\nunion_categoricals([pd.Categorical([np.nan, 1]), pd.Categorical([2, np.nan])])\r\n# [1, 1, 2, 2]\r\n# Categories (2, int64): [1, 2]\r\n\r\nunion_categoricals([pd.Categorical([np.nan]), pd.Categorical([np.nan])])\r\n# IndexError: cannot do a non-empty take from an empty axes.\r\n```\r\n\r\n"""
13753,167020568,seanlaw,jreback,2016-07-22 10:53:15,2016-07-24 14:05:15,2016-07-24 14:05:15,closed,,No action,3,Bug;Duplicate;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13753,b'pd.expanding is incorrectly calculating window size when axis=1',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n\r\n    df = pd.DataFrame({'A': [0, 1, 2, np.nan, 4], \r\n                       'B': [0, 1, 2, np.nan, 4], \r\n                       'C': [0, 1, 2, np.nan, 4], \r\n                       'D': [0, 1, 2, np.nan, 4], \r\n                       'E': [0, 1, 2, np.nan, 4], \r\n                       'F': [0, 1, 2, np.nan, 4]})\r\n\r\n    print df.expanding(axis=1).sum()\r\n    \r\n#### Expected Output\r\n\r\n         A    B     C     D     E     F\r\n    0  0.0  0.0   0.0   0.0   0.0   0.0\r\n    1  1.0  2.0   3.0   4.0   5.0   5.0\r\n    2  2.0  4.0   6.0   8.0  10.0  10.0\r\n    3  NaN  NaN   NaN   NaN   NaN   NaN\r\n    4  4.0  8.0  12.0  16.0  20.0  20.0\r\n\r\nHowever, the correct result should be:\r\n\r\n         A    B     C     D     E     F\r\n    0  0.0  0.0   0.0   0.0   0.0   0.0\r\n    1  1.0  2.0   3.0   4.0   5.0   6.0\r\n    2  2.0  4.0   6.0   8.0  10.0  12.0\r\n    3  NaN  NaN   NaN   NaN   NaN   NaN\r\n    4  4.0  8.0  12.0  16.0  20.0  24.0\r\n\r\nNotice that the last column `E` is different. I've tracked this down and found that the _get_window function (for expanding) fails to return the correct number of windows when the following conditions are met:\r\n\r\n1. `axis=1` is used instead of `axis=0` (default)\r\n2. The number of rows in the dataframe is less than the number of columns\r\n\r\nThis is caused by the fact that the object is using `len(obj)` in determining the window size. Instead, it should be using `obj.shape[self.axis]` \r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.18.1+237.ge357ea1\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 20.1.1\r\nCython: 0.23.4\r\nnumpy: 1.11.1\r\nscipy: 0.17.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.7.0\r\nIPython: 4.0.3\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.4.1\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: None\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: 0.9\r\napiclient: 1.4.0\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext)\r\njinja2: 2.8\r\nboto: 2.39.0\r\npandas_datareader: 0.2.0\r\n"""
13750,166984800,sinhrks,jreback,2016-07-22 07:17:58,2016-07-24 16:12:32,2016-07-24 15:34:24,closed,,0.19.0,4,Bug;Performance;Timezones,https://api.github.com/repos/pydata/pandas/issues/13750,b'PERF/BUG: improve factorize for datetimetz',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nbecause ``factorize`` internally localize datetimetz, it raises when data contains DST boundary.\r\n\r\n```\r\ndti = pd.date_range('2016-11-06', freq='H', periods=5, tz='US/Eastern')\r\ndti.factorize()\r\n# AmbiguousTimeError: Cannot infer dst time from Timestamp('2016-11-06 01:00:00'), try using the 'ambiguous' argument\r\n```\r\n\r\nSkipped this localization to fix, also it  improves perf.\r\n\r\n```\r\ndti = pd.date_range('2011-01-01', freq='H', periods=1000000, tz='Asia/Tokyo')\r\n%timeit dti.factorize()\r\n# on current master\r\n# 1 loop, best of 3: 475 ms per loop\r\n\r\n# after this PR\r\n# 1 loop, best of 3: 262 ms per loop\r\n```\r\n\r\n**asv:**\r\n\r\n```\r\n   before     after       ratio\r\n  [bb6b5e54] [a2c3370a]\r\n-   22.46ms     9.97ms      0.44  timeseries.datetime_algorithm.time_dti_tz_factorize\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"""
13737,166770716,sinhrks,jreback,2016-07-21 09:03:42,2016-07-21 11:00:34,2016-07-21 10:54:49,closed,,0.19.0,2,Bug;Missing-data;Period,https://api.github.com/repos/pydata/pandas/issues/13737,b'TST/BUG: Added more tests for Period(NaT)',"b"" - [x] follow-up of #12759\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n- because ``Period(NaT)`` had separate impl until #12579, there were some bugs related to ``pd.isnull``. Now all the issues are fixed, thus added explicit tests and what's new. \r\n- Also updated what's new to describe ``Period(None)`` (related to #13723).\r\n\r\n**0.18.1:**\r\n\r\n```\r\ns = pd.Series([pd.Period('2011-01', freq='M'), pd.Period('NaT', freq='M')])\r\ns.isnull()\r\n# 0    False\r\n# 1    False\r\n# dtype: bool\r\n\r\ns.fillna(pd.Period('2012-01', freq='M'))\r\n# 0   2011-01\r\n# 1       NaT\r\n# dtype: object\r\n\r\ns.dropna()\r\n# 0   2011-01\r\n# 1       NaT\r\n# dtype: object\r\n```\r\n\r\n"""
13736,166769256,sinhrks,jreback,2016-07-21 08:56:13,2016-07-21 10:51:21,2016-07-21 10:51:21,closed,,0.19.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/13736,b'BUG: DatetimeIndex raises AttributeError on win',"b' - [x] 1st issue of  #13721\r\n - [x] tests added / passed\r\n  - passed appveyor (2nd job failure is caused by the same test as #13714)\r\n  - https://ci.appveyor.com/project/sinhrks/pandas-2dhxb/build/1.0.375\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] no whatsnew, as it is a regression caused by #13692.\r\n\r\n'"
13731,166708048,jcrist,jorisvandenbossche,2016-07-20 23:49:21,2016-07-23 15:38:02,2016-07-23 15:37:53,closed,,0.19.0,6,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/13731,b'Groupby getitem works with all index types',b' - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nPreviously `df.groupby(0)[df.columns]` would fail if all column names\r\nwere integers (meaning `df.columns` was an `Int64Index`). This was\r\nbecause the implementation of `__getitem__` in `SelectionMixin` was\r\nchecking for `ABCIndex` when it probably should have checked for\r\n`ABCIndexClass`.'
13717,166573426,flatberg,jreback,2016-07-20 12:50:45,2016-07-24 14:01:53,2016-07-21 12:08:26,closed,,0.19.0,18,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13717,b'Seqfault on creation of dataframe with np.empty_like',"b""\r\nThis code segfaults:\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\ndf = pd.DataFrame(np.random.randn(3, 3), index='A B C'.split())\r\npd.DataFrame(np.empty_like(df.index))\r\n```\r\n\r\n\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-91-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.1\r\nscipy: 0.17.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.7.2\r\nIPython: 5.0.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n"""
13698,166200418,jreback,jreback,2016-07-18 21:41:52,2016-07-19 01:16:41,2016-07-19 01:16:41,closed,,0.19.0,2,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13698,b'BUG: merge_asof not handling allow_exact_matches and tolerance on first entry',b'closes #13695'
13697,166192641,ekipmanager,jreback,2016-07-18 21:01:32,2016-07-18 21:46:58,2016-07-18 21:45:35,closed,,No action,4,Bug;Duplicate;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/13697,b'Accessing with .ix removes time zone info ',"b'#### Code Sample, a copy-pastable example if possible\r\nI have a dataframe with two columns `start` and `end` both have the dtype of `datetime64[ns, tzoffset(None, -21600)]` but depending on how I access it the output removes the timezone info for for example\r\n```\r\ndf.ix[0, \'end\']\r\nOut[30]: Timestamp(\'2016-07-17 06:26:00\')\r\n```\r\nbut \r\n```\r\ndf[\'end\'][0]\r\nOut[32]: Timestamp(\'2016-07-17 00:26:00-0600\', tz=\'tzoffset(None, -21600)\')\r\n```\r\nIssue rises when I am doing something like this (`length` is pandas.tslib.Timedelta):\r\n```\r\ndf.ix[0, \'end\'] = df.ix[0, \'start\'] + df.ix[0, \'length\']\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-34-f63ebfd12a2e> in <module>()\r\n----> 1 df.ix[0, \'end\'] = df.ix[0, \'start\'] + df.ix[0, \'length\']\r\n/usr/lib64/python2.7/site-packages/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n    130             key = com._apply_if_callable(key, self.obj)\r\n    131         indexer = self._get_setitem_indexer(key)\r\n--> 132         self._setitem_with_indexer(indexer, value)\r\n    133 \r\n    134     def _has_valid_type(self, k, axis):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n    534                 # scalar\r\n    535                 for item in labels:\r\n--> 536                     setter(item, value)\r\n    537 \r\n    538         else:\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/indexing.pyc in setter(item, v)\r\n    459                     s._consolidate_inplace()\r\n    460                     s = s.copy()\r\n--> 461                     s._data = s._data.setitem(indexer=pi, value=v)\r\n    462                     s._maybe_update_cacher(clear=True)\r\n    463 \r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc in setitem(self, **kwargs)\r\n   2915 \r\n   2916     def setitem(self, **kwargs):\r\n-> 2917         return self.apply(\'setitem\', **kwargs)\r\n   2918 \r\n   2919     def putmask(self, **kwargs):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc in apply(self, f, axes, filter, do_integrity_check, consolidate, raw, **kwargs)\r\n   2888 \r\n   2889             kwargs[\'mgr\'] = self\r\n-> 2890             applied = getattr(b, f)(**kwargs)\r\n   2891             result_blocks = _extend_blocks(applied, result_blocks)\r\n   2892 \r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc in setitem(self, indexer, value, mgr)\r\n    640 \r\n    641         # coerce args\r\n--> 642         values, _, value, _ = self._try_coerce_args(self.values, value)\r\n    643         arr_value = np.array(value)\r\n    644 \r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc in _try_coerce_args(self, values, other)\r\n   2270             # test we can have an equal time zone\r\n   2271             if tz is None or str(tz) != str(self.values.tz):\r\n-> 2272                 raise ValueError(""incompatible or non tz-aware value"")\r\n   2273             other_mask = isnull(other)\r\n   2274             other = other.tz_localize(None).value\r\n\r\nValueError: incompatible or non tz-aware value\r\n```\r\nHowever if I do it like this:\r\n`df.ix[0, \'end\'] = df[\'start\'][0] + df.ix[0, \'length\']`\r\nIt works fine since that type of selection does not lose the timezone info.\r\n#### output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.5.7-200.fc23.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 20.1.1\r\nCython: 0.23.4\r\nnumpy: 1.11.1\r\nscipy: 0.17.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.1.1\r\nsphinx: 1.2.3\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 0.6.0\r\ntables: None\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 1.8.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.4.4\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n'"
13695,166162646,chrisaycock,jreback,2016-07-18 18:38:35,2016-07-19 01:16:41,2016-07-19 01:16:41,closed,,0.19.0,1,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13695,b'pd.merge_asof() matches out of tolerance when allow_exact_matches=False',"b""Using these DataFrames for this example:\r\n\r\n```\r\ndf1 = pd.DataFrame({'time':pd.to_datetime(['2016-07-15 13:30:00.030']), 'username':['bob']})\r\ndf2 = pd.DataFrame({'time':pd.to_datetime(['2016-07-15 13:30:00.000', '2016-07-15 13:30:00.030']), 'version':[1, 2]})\r\n```\r\n\r\nI can perform the `pd.merge_asof()` as expected:\r\n\r\n```\r\nIn [153]: df1\r\nOut[153]:\r\n                     time username\r\n0 2016-07-15 13:30:00.030      bob\r\n\r\nIn [154]: df2\r\nOut[154]:\r\n                     time  version\r\n0 2016-07-15 13:30:00.000        1\r\n1 2016-07-15 13:30:00.030        2\r\n\r\nIn [155]: pd.merge_asof(df1, df2, on='time')\r\nOut[155]:\r\n                     time username  version\r\n0 2016-07-15 13:30:00.030      bob        2\r\n```\r\n\r\nIf I disable the exact matches, then I get the prior entry as expected:\r\n\r\n```\r\nIn [156]: pd.merge_asof(df1, df2, on='time', allow_exact_matches=False)\r\nOut[156]:\r\n                     time username  version\r\n0 2016-07-15 13:30:00.030      bob        1\r\n```\r\n\r\nBut the tolerance isn't respected when I disable the exact matches!\r\n\r\n```\r\nIn [157]: pd.merge_asof(df1, df2, on='time', allow_exact_matches=False, tolerance=pd.Timedelta('10ms'))\r\nOut[157]:\r\n                     time username  version\r\n0 2016-07-15 13:30:00.030      bob        1\r\n```\r\n\r\nI expect to get a null here.\r\n\r\nThis is pandas version `0.18.0+408.gd8f3d73`.\r\n"""
13693,166110527,bdrosen96,jreback,2016-07-18 14:43:01,2016-07-20 21:54:19,2016-07-20 21:54:09,closed,,0.19.0,11,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/13693,"b'Test case for patch, plus fix to not swallow exceptions'",b' - [x] closes #13652\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13687,165983348,yui-knk,jreback,2016-07-17 16:16:56,2016-07-19 01:16:31,2016-07-19 01:14:14,closed,,0.19.0,7,Bug;Missing-data;Timedelta,https://api.github.com/repos/pydata/pandas/issues/13687,b'BUG: Cast a key to NaT before get loc from Index',"b"" - [x] closes #13603\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\npd.NaT, None, float('nan') and np.nan are\r\nconverted NaT in TestTimedeltas.\r\nSo we should convert keys if keys are these value.\r\n\r\nFix #13603"""
13672,165917680,ChadFulton,sinhrks,2016-07-16 07:43:04,2016-07-23 13:09:18,2016-07-23 13:09:18,closed,,0.19.0,1,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13672,b'BUG: DatetimeIndex with nanosecond frequency does not include `end`',"b'I\'m not sure if this is a bug or intended behavior, but documentation says ""If periods is none, generated index will extend to first conforming time on or just past end argument"", and it appears here that in the nanosecond frequency case, the generated index only extends to just *before* the end argument.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n> start = pd.Timestamp(1)\r\n> end = pd.Timestamp(4)\r\n> pd.DatetimeIndex(start=start, end=end, freq=\'N\')\r\nDatetimeIndex([\'1970-01-01 00:00:00.000000001\',\r\n               \'1970-01-01 00:00:00.000000002\',\r\n               \'1970-01-01 00:00:00.000000003\'],\r\n              dtype=\'datetime64[ns]\', freq=\'N\')\r\n```\r\n\r\nwhereas a similar call with annual frequency gives:\r\n\r\n```python\r\n> start = pd.Timestamp(\'1971\')\r\n> end = pd.Timestamp(\'1974\')\r\n> pd.DatetimeIndex(start=start, end=end, freq=\'AS\')\r\nDatetimeIndex([\'1971-01-01\',\r\n               \'1972-01-01\',\r\n               \'1973-01-01\',\r\n               \'1974-01-01\'],\r\n              dtype=\'datetime64[ns]\', freq=\'AS-JAN\')\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\npandas: 0.18.0\r\n'"
13661,165690054,jreback,jreback,2016-07-15 00:54:01,2016-07-15 10:21:32,2016-07-15 10:21:32,closed,,0.19.0,1,Bug;Windows,https://api.github.com/repos/pydata/pandas/issues/13661,b'BUG: construction of Series with integers on windows not default to int64',b'closes #13646'
13650,165503587,hcontrast,jreback,2016-07-14 08:17:48,2016-07-14 20:12:35,2016-07-14 10:00:06,closed,,No action,4,Bug;Duplicate;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/13650,b'combine_first loses index type information with MultiIndices and different timezones',"b""See title and example below. I believe this is due to the fact that combination of indices with different timezones first converts to object dtype, then rebases all timestamps to UTC for comparison and then constructs a DatetimeIndex from that. However, this doesn't seem to be applied for the individual levels in a MultiIndex. This is on latest stable 0.18.1.\r\n\r\n```python\r\nIn [3]: %cpaste\r\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\r\n:tz1, tz2 = 'America/New_York', 'UTC'\r\n:\r\n:from1, to1 = [pd.Timestamp('20160101', tz=tz1), pd.Timestamp('20160102', tz=tz1)], [pd.Timestamp('20160102', tz=tz1), pd.Timestamp('20160103', tz=tz1)]\r\n:\r\n:from2, to2 = [pd.Timestamp('20160103', tz=tz2), pd.Timestamp('20160104', tz=tz2)], [pd.Timestamp('20160104', tz=tz2), pd.Timestamp('20160105', tz=tz2)]\r\n:\r\n:index1 = pd.MultiIndex.from_arrays([from1, to1])\r\n:df1 = pd.DataFrame([1, 2], index=index1)\r\n:\r\n:index2 = pd.MultiIndex.from_arrays([from2, to2])\r\n:df2 = pd.DataFrame([1, 2], index=index2)\r\n:\r\n:result = df1.combine_first(df2)\r\n:--\r\n\r\nIn [4]: df1.index.get_level_values(0)\r\nOut[4]: DatetimeIndex(['2016-01-01 00:00:00-05:00', '2016-01-02 00:00:00-05:00'], dtype='datetime64[ns, America/New_York]', freq=None)\r\n\r\nIn [5]: df2.index.get_level_values(0)\r\nOut[5]: DatetimeIndex(['2016-01-03', '2016-01-04'], dtype='datetime64[ns, UTC]', freq=None)\r\n\r\nIn [6]: result.index.get_level_values(0)\r\nOut[6]: \r\nIndex([2016-01-01 00:00:00-05:00, 2016-01-02 00:00:00-05:00,\r\n       2016-01-03 00:00:00+00:00, 2016-01-04 00:00:00+00:00],\r\n      dtype='object')\r\n```\r\nWorks correctly if the inputs have the same timezone\r\n```python\r\nIn [12]: %cpaste\r\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\r\n:tz1, tz2 = 'America/New_York', 'America/New_York' \r\n:\r\n:from1, to1 = [pd.Timestamp('20160101', tz=tz1), pd.Timestamp('20160102', tz=tz1)], [pd.Timestamp('20160102', tz=tz1), pd.Timestamp('20160103', tz=tz1)]\r\n:\r\n:from2, to2 = [pd.Timestamp('20160103', tz=tz2), pd.Timestamp('20160104', tz=tz2)], [pd.Timestamp('20160104', tz=tz2), pd.Timestamp('20160105', tz=tz2)]\r\n:\r\n:index1 = pd.MultiIndex.from_arrays([from1, to1])\r\n:df1 = pd.DataFrame([1, 2], index=index1)\r\n:\r\n:index2 = pd.MultiIndex.from_arrays([from2, to2])\r\n:df2 = pd.DataFrame([1, 2], index=index2)\r\n:\r\n:result = df1.combine_first(df2)\r\n:\r\n:--\r\n\r\nIn [13]: result.index.get_level_values(0)\r\nOut[13]: \r\nDatetimeIndex(['2016-01-01 00:00:00-05:00', '2016-01-02 00:00:00-05:00',\r\n               '2016-01-03 00:00:00-05:00', '2016-01-04 00:00:00-05:00'],\r\n              dtype='datetime64[ns, America/New_York]', freq=None)\r\n```\r\n\r\nBehavior is correct for single indices:\r\n\r\n```python\r\nIn [7]: %cpaste\r\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\r\n:\r\n:tz1, tz2 = 'America/New_York', 'UTC'\r\n:\r\n:index1 = [pd.Timestamp('20160101', tz=tz1), pd.Timestamp('20160102', tz=tz1)]\r\n:index2 = [pd.Timestamp('20160103', tz=tz2), pd.Timestamp('20160104', tz=tz2)]\r\n:\r\n:df1 = pd.DataFrame([1, 2], index=index1)\r\n:df2 = pd.DataFrame([1, 2], index=index2)\r\n:\r\n:result = df1.combine_first(df2)\r\n:--\r\n\r\nIn [8]: df2.index\r\nOut[8]: DatetimeIndex(['2016-01-03', '2016-01-04'], dtype='datetime64[ns, UTC]', freq=None)\r\n\r\nIn [9]: df1.index\r\nOut[9]: DatetimeIndex(['2016-01-01 00:00:00-05:00', '2016-01-02 00:00:00-05:00'], dtype='datetime64[ns, America/New_York]', freq=None)\r\n\r\nIn [10]: result.index\r\nOut[10]: \r\nDatetimeIndex(['2016-01-01 05:00:00+00:00', '2016-01-02 05:00:00+00:00',\r\n               '2016-01-03 00:00:00+00:00', '2016-01-04 00:00:00+00:00'],\r\n              dtype='datetime64[ns, UTC]', freq=None)\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-88-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.6\r\npip: 8.1.1\r\nsetuptools: 20.3\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1.post1\r\nxarray: None\r\nIPython: 3.1.0\r\nsphinx: None\r\npatsy: 0.2.1\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: None\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n```\r\n\r\n"""
13646,165423019,jreback,jreback,2016-07-13 21:13:35,2016-07-15 10:21:32,2016-07-15 10:21:32,closed,,0.19.0,0,Bug;Dtypes;Windows,https://api.github.com/repos/pydata/pandas/issues/13646,b'BUG: Series construction w/integer tuples failing on windows',"b'This just started failing on windows on master, in the last few PR\'s. Prob related to #13147.\r\n\r\ntuples are not being upcast like lists.\r\n\r\n```\r\n======================================================================\r\nFAIL: test_alignment_non_pandas (pandas.tests.frame.test_operators.TestDataFrameOperators)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\conda\\Documents\\pandas3.5\\pandas\\tests\\frame\\test_operators.py"", line 1203, in test_alignment_non_pandas\r\n    Series([1, 2, 3], index=df.index))\r\n  File ""C:\\Users\\conda\\Documents\\pandas3.5\\pandas\\util\\testing.py"", line 1157, in assert_series_equal\r\n    assert_attr_equal(\'dtype\', left, right)\r\n  File ""C:\\Users\\conda\\Documents\\pandas3.5\\pandas\\util\\testing.py"", line 882, in assert_attr_equal\r\n    left_attr, right_attr)\r\n  File ""C:\\Users\\conda\\Documents\\pandas3.5\\pandas\\util\\testing.py"", line 1021, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Attributes are different\r\n\r\nAttribute ""dtype"" are different\r\n[left]:  int32\r\n[right]: int64None\r\n```\r\n\r\n```\r\nIn [3]: import pandas as pd\r\n\r\nIn [4]: pd.__version__\r\nOut[4]: \'0.18.1+193.g20de266.dirty\'\r\n\r\nIn [5]: pd.Series((1,2,3))\r\nOut[5]:\r\n0    1\r\n1    2\r\n2    3\r\ndtype: int32\r\n\r\nIn [6]: pd.Series([1,2,3])\r\nOut[6]:\r\n0    1\r\n1    2\r\n2    3\r\ndtype: int64\r\n```'"
13624,164959416,sinhrks,jreback,2016-07-11 23:06:11,2016-07-12 10:52:19,2016-07-12 10:52:19,closed,,0.19.0,1,Bug;Error Reporting;Timedelta,https://api.github.com/repos/pydata/pandas/issues/13624,b'BUG: Invalid Timedelta op may raise ValueError',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n``Timedelta`` op may raise ``ValueError`` for invalid inputs because of internal conversion. \r\n\r\n```\r\npd.Timedelta('1 days') + 'a'\r\n# ValueError: unit abbreviation w/o a number\r\n```\r\n\r\nNow it raises ``TypeError``.\r\n\r\n\r\n"""
13603,164687563,tjader,jreback,2016-07-09 21:57:45,2016-07-19 01:14:14,2016-07-19 01:14:14,closed,,0.19.0,0,Bug;Indexing;Timedelta,https://api.github.com/repos/pydata/pandas/issues/13603,b'Checking for any NaT-like objects in a TimedeltaIndex always returns True',"b""#### Code Sample, a copy-pastable example if possible\r\n```python\r\n>>> None in pd.to_timedelta(range(5), unit='d') + pd.offsets.Hour(1)\r\nTrue\r\n```\r\nReturns `True` for any one of `[pd.NaT, None, float('nan'), np.nan]`\r\n#### Expected Output\r\n```python\r\n>>> None in pd.to_timedelta(range(5), unit='d') + pd.offsets.Hour(1)\r\nFalse\r\n```\r\n#### output of ``pd.show_versions()``\r\n```python\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 3c202b1cbcc73c4006c967c8abe1b8d9089c5be4\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.18.1+171.g3c202b1\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.10.4\r\nscipy: 0.17.1\r\nstatsmodels: None\r\nxarray: 0.7.2\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.5.2\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n```\r\n"""
13598,164671341,toobaz,jreback,2016-07-09 15:22:08,2016-07-19 01:11:27,2016-07-19 01:11:18,closed,,0.19.0,6,Bug;Difficulty Novice;Effort Low;Error Reporting;Strings,https://api.github.com/repos/pydata/pandas/issues/13598,"b""Series.str.zfill() doesn't check type""","b""#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nIn [5]: pd.Series(['a', 'b']).str.zfill('hi')\r\nOut[5]: \r\n0   NaN\r\n1   NaN\r\ndtype: float64\r\n\r\n```\r\n\r\n#### Expected Output\r\n\r\nA ``ValueError``.\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: a63bd12529ff309d957d714825b1753d0e02b7fa\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.5.0-2-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.18.1+174.ga63bd12\r\nnose: 1.3.7\r\npip: 1.5.6\r\nsetuptools: 18.4\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.16.0\r\nstatsmodels: 0.8.0.dev0+111ddc0\r\nxarray: None\r\nIPython: 5.0.0.dev\r\nsphinx: 1.3.1\r\npatsy: 0.3.0-dev\r\ndateutil: 2.2\r\npytz: 2012c\r\nblosc: None\r\nbottleneck: 1.1.0dev\r\ntables: 3.2.2\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.7.3\r\nlxml: None\r\nbs4: 4.4.0\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.1\r\napiclient: 1.5.0\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.38.0\r\npandas_datareader: 0.2.1\r\n\r\n```"""
13592,164664985,sinhrks,jreback,2016-07-09 12:26:50,2016-07-11 01:37:13,2016-07-11 01:37:09,closed,,0.19.0,5,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/13592,b'BUG: Series/Index contains NaT with object dtype comparison incorrect',"b' - [x] closes #9005 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n``Series`` and ``Index`` compares ``NaT == NaT`` as ``True`` if it has ``object`` dtype. \r\n\r\n```\r\npd.Series([pd.NaT])\r\n# 0   NaT\r\n# dtype: datetime64[ns]\r\n\r\n# OK\r\npd.Series([pd.NaT]) == pd.NaT\r\n# 0    False\r\n# dtype: bool\r\n\r\n# NG in object dtype\r\npd.Series([pd.NaT], dtype=object) == pd.NaT\r\n# 0    True\r\n# dtype: bool\r\n\r\n# OK\r\npd.Index([pd.NaT]) == pd.NaT\r\narray([False], dtype=bool)\r\n\r\n# NG in object dtype\r\npd.Index([pd.NaT], dtype=object) == pd.NaT\r\n# array([ True], dtype=bool)\r\n```\r\n\r\n'"
13587,164553546,RobertasA,jreback,2016-07-08 15:18:17,2016-07-12 10:21:21,2016-07-12 10:21:21,closed,,,1,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/13587,b'Logical comparison operators and mathematical operators are applied inconsistently for series.',"b'In series all logical operators are by position (iloc) while all mathematical operators are by location (loc).\r\nThis is probably similar to https://github.com/pydata/pandas/issues/4581.\r\n\r\ne.g.\r\n`import pandas as pd`\r\n`a = pd.Series([1, 4], index=[0, 1])`\r\n`b = pd.Series([3, 7], index=[1, 0])`\r\n\r\n`a>b`\r\n\r\nGives\r\n\r\n`0    False`\r\n`1    False`\r\n`dtype: bool`\r\n\r\nBut \r\n\r\n`a - b`\r\n\r\ngives\r\n\r\n`0   -6`\r\n`1    1`\r\n`dtype: int64`\r\n\r\n\r\n#### output of ``pd.show_versions()``\r\n`\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 1.5.6\r\nsetuptools: 3.6\r\nCython: None\r\nnumpy: 1.11.1\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n`\r\n'"
13585,164493985,jorisvandenbossche,jorisvandenbossche,2016-07-08 09:35:57,2016-07-11 15:02:25,2016-07-11 15:02:25,closed,,0.19.0,6,Bug;Groupby;Regression,https://api.github.com/repos/pydata/pandas/issues/13585,b'BUG: groupby apply on selected columns yielding scalar (GH13568)',b' - [x] closes #13568\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13583,164426159,sinhrks,jreback,2016-07-07 23:02:00,2016-07-10 22:05:12,2016-07-10 22:05:03,closed,,0.19.0,2,Bug;Dtypes;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/13583,b'BUG: Block/DTI doesnt handle tzlocal properly',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nUnable to concatenate ``Series`` with ``tzlocal``\r\n\r\n```\r\ns = pd.Series([pd.Timestamp('2011-01-01', tz=dateutil.tz.tzlocal())])\r\ns\r\n# 0   2011-01-01 00:00:00+09:00\r\n# dtype: datetime64[ns, tzlocal()]\r\n\r\npd.concat([s, s])\r\n# UnknownTimeZoneError: 'tzlocal()'\r\n```\r\n\r\nIt is caused by 2 issues:\r\n\r\n#### 1. ``tslib.maybe_get_tz`` can't handle ``tzlocal``.\r\n\r\n```\r\npd.tslib.maybe_get_tz('tzlocal()')\r\n# UnknownTimeZoneError: 'tzlocal()'\r\n```\r\n\r\n#### 2. ``DatetimeIndex.tz_localize`` may return incorrect result when ``tzlocal`` is used.\r\n\r\nit's internally used in ``conat``.\r\n\r\n```\r\npd.DatetimeIndex(['2011-01-01', '2011-01-02'], tz=dateutil.tz.tzlocal()).tz_localize(None)\r\n# DatetimeIndex(['2011-01-01', '1970-01-01'], dtype='datetime64[ns]', freq=None)\r\n```\r\n\r\n"""
13576,164244521,gte620v,jreback,2016-07-07 07:15:51,2016-07-08 09:56:54,2016-07-08 09:56:41,closed,,No action,7,Bug;Compat;Difficulty Intermediate;Duplicate;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/13576,"b'BUG: python and c engines for read_csv treat blank spaces differently, when using converters'","b""#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nIn [109]:\r\ndata = np.array([[ 'c1', 'c2'],\r\n                [ '', 0.285],\r\n                [ 10.1, 0.285]], dtype=object)\r\n\r\nIn [110]:\r\npd.DataFrame(data).to_csv('test.csv',header=False,index=False)\r\n\r\nIn [111]:\r\n!cat test.csv\r\nOut[111]:\r\nc1,c2\r\n,0.285\r\n10.1,0.285\r\n\r\nIn [113]:\r\npd.read_csv('test.csv',converters={'c1':str},engine='c').values\r\nOut[113]:\r\narray([['', 0.285],\r\n       ['10.1', 0.285]], dtype=object)\r\n\r\nIn [114]:\r\npd.read_csv('test.csv',converters={'c1':str},engine='python').values\r\nOut[114]:\r\narray([[nan, 0.285],\r\n       ['10.1', 0.285]], dtype=object)\r\n\r\n```\r\n\r\n#### Expected Output\r\n\r\nNotice that the output for the python engine and the c engine are different.  I am not sure which one is preferable/expected.\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 21.0.0\r\nCython: 0.23.4\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.9999999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: 2.39.0\r\n```\r\n\r\n### Details\r\n\r\nThe culpruit seems to be this function: https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L1299\r\n\r\nBefore that call, `results` and `values` both contain the `''` string in the python engine version of the parser.  That call changes the `''`' value in `values` to `nan` and thus changes the value of `results` as well.  Specifically, the change for the python engine happens here: https://github.com/pydata/pandas/blob/0c6226cbbc319ec22cf4c957bdcc055eaa7aea99/pandas/src/inference.pyx#L1009\r\n\r\n```\r\nif (convert_empty and val == '') or (val in na_values)\r\n```\r\n\r\nIt seems that `na_values` contains `''` by default for certain parsing calls.  This means that when `val == ''` triggers the `nan` conversion whether or not `convert_empty` is true.\r\n\r\nI haven't tracked down the change in the `c` engine.\r\n"""
13568,163873741,tpietruszka,jorisvandenbossche,2016-07-05 15:06:42,2016-07-11 15:03:02,2016-07-11 15:02:25,closed,,0.19.0,4,Bug;Groupby;Regression,https://api.github.com/repos/pydata/pandas/issues/13568,b'Apply on selected columns of a groupby object - stopped working with 0.18.1',"b""#### Code Sample\r\n```python\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> \r\n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\r\n...                            'foo', 'bar', 'foo', 'foo'],\r\n...                     'B' : ['one', 'one', 'two', 'three',\r\n...                            'two', 'two', 'one', 'three'],\r\n...                     'C' : np.random.randn(8),\r\n...                     'D' : np.random.randn(8)})\r\n>>> \r\n>>> gb = df.groupby('B')\r\n>>> \r\n>>> gb[['C', 'D']].apply(lambda group: group.C.mean() - group.D.mean())\r\n\r\n```\r\nWorks fine on 0.18.0. \r\nOn 0.18.1 throws:\r\n> TypeError: Series.name must be a hashable type\r\n\r\n#### Expected Output\r\n> B\r\n> one      0.609650\r\n> three   -1.027308\r\n> two      0.515237\r\n> dtype: float64\r\n\r\n(some other random numbers)\r\n\r\n### Comment\r\nWith 0.18.1 update, some piece of my code stopped working, in a quite surprising way. \r\n\r\nThe code is easy to fix - just by removing [['C', 'D']] and applying on the whole groupby object. \r\n\r\nNevertheless, its a regression in a point release and I think someone else may run into the same problem.\r\n\r\nI am guessing that pandas is trying to generate a name for the result series based on the column list - unfortunately I cannot investigate further right now.\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-24-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 24.0.2\r\nCython: None\r\nnumpy: 1.11.1\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n"""
13559,163644116,jorisvandenbossche,jorisvandenbossche,2016-07-04 10:04:22,2016-07-05 08:37:18,2016-07-05 08:37:18,closed,,0.19.0,1,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/13559,b'TST: confirm bug in partial string multi-index slicing is fixed (GH12685)',b'Closes #12685'
13557,163595549,gdbaldw,sinhrks,2016-07-04 03:01:38,2016-07-05 08:41:26,2016-07-05 00:09:13,closed,,No action,5,Bug;Duplicate;Multi Dimensional;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/13557,b'to_frame() broken by RangeIndex',"b'#### Code Sample, a copy-pastable example if possible\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: p = pd.Panel(np.arange(512).reshape((8,8,8)))\r\n\r\nIn [4]: p.to_frame()\r\nOut[4]: <repr(<pandas.core.frame.DataFrame at 0x7fcd99f25590>) failed: TypeError: data type ""major"" not understood>\r\n\r\nIn [5]: p.axes\r\nOut[5]: \r\n[RangeIndex(start=0, stop=8, step=1),\r\n RangeIndex(start=0, stop=8, step=1),\r\n RangeIndex(start=0, stop=8, step=1)]\r\n\r\nIssue is similar to [ Related TypeErrors in multi-indexed DataFrame #12893 ](https://github.com/pydata/pandas/issues/12893)\r\n\r\n#### Expected Output\r\n\r\nSimilar to this...\r\n\r\nIn [6]: p = pd.Panel(np.arange(8).reshape((2,2,2)))\r\n\r\nIn [7]: p.to_frame()\r\nOut[7]: \r\n```\r\n             0  1\r\nmajor minor      \r\n0     0      0  4\r\n      1      1  5\r\n1     0      2  6\r\n      1      3  7\r\n\r\n```\r\n\r\nIn [8]: p.axes\r\nOut[8]: \r\n[RangeIndex(start=0, stop=2, step=1),\r\n RangeIndex(start=0, stop=2, step=1),\r\n RangeIndex(start=0, stop=2, step=1)]\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\nIn [9]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.candidate.1\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.5.0-2-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0+git114-g6c692ae\r\nnose: 1.3.7\r\npip: None\r\nsetuptools: 20.10.1\r\nCython: None\r\nnumpy: 1.11.1rc1\r\nscipy: 0.17.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 2.4.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.2rc2\r\nopenpyxl: 2.3.0\r\nxlrd: 0.9.4\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n'"
13551,163542988,mpuels,jorisvandenbossche,2016-07-03 01:46:10,2016-07-25 15:07:12,2016-07-25 15:07:00,closed,,0.19.0,11,Bug;IO Excel;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/13551,b'BUG: Fix .to_excel() for MultiIndex containing a NaN value #13511',b' - [X] closes #13511\r\n - [X] tests added / passed\r\n - [X] passes ``git diff upstream/master | flake8 --diff``\r\n - [X] whatsnew entry\r\n\r\n'
13516,162333020,evanpw,jorisvandenbossche,2016-06-26 15:40:42,2016-07-01 14:18:43,2016-07-01 14:18:37,closed,,0.19.0,5,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/13516,"b""BUG: Can't store callables using __setitem__""",b' - [X] closes #13299\r\n - [X] tests added / passed\r\n - [X] passes ``git diff upstream/master | flake8 --diff``\r\n - [X] whatsnew entry\r\n\r\n'
13511,162273646,mpuels,jorisvandenbossche,2016-06-25 09:54:49,2016-07-25 15:07:00,2016-07-25 15:07:00,closed,,0.19.0,4,Bug;IO Excel;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/13511,b'NaN label in MultiIndex is assigned a non NaN value when writing to excel file',"b'Given a DataFrame which has a MultiIndex. When a label of the MultiIndex has the value NaN and the DataFrame is written to an excel file, the label will have a value which is not NaN in the excel file.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n    df = pd.DataFrame({\'c1\': [1,1,2,2],\r\n                       \'c2\': [None] + ""b a b"".split()})\r\n    df\r\n\r\nreturns a DataFrame where the first element of column \'c2\' is NaN:\r\n\r\n    \tc1\tc2\r\n    0\t1\t\r\n    1\t1\tb\r\n    2\t2\ta\r\n    3\t2\tb\r\n\r\nSet both columns as the index:\r\n\r\n    df_midx = df.set_index([\'c1\', \'c2\'])\r\n    df_midx\r\n\r\nreturns\r\n\r\n    c1\tc2\r\n    1\t\r\n    1\tb\r\n    2\ta\r\n    2\tb\r\n\r\nWrite DataFrame to excel file and read it back in:\r\n\r\n    df_midx.to_excel(\'df_midx.xlsx\')\r\n    df_midx_from_xlsx = pd.read_excel(\'df_midx.xlsx\')\r\n    df_midx_from_xlsx\r\n\r\nreturns\r\n\r\n    \tc1\tc2\r\n    0\t1.0\tb\r\n    1\t\tb\r\n    2\t2.0\ta\r\n    3\t\tb\r\n\r\nThe first element of column \'c2\' is now set to \'b\' instead of NaN.\r\n\r\n#### Expected Output\r\n    \tc1\tc2\r\n    0\t1.0\r\n    1\t\tb\r\n    2\t2.0\ta\r\n    3\t\tb\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: ac174349b0e1525475c2354e1c0b8ee1ed1cabad\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-88-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 1.5.4\r\nsetuptools: 2.2\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.16.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.0.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.5.2\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.3.5\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n'"
13510,162239741,jorisvandenbossche,jorisvandenbossche,2016-06-24 22:08:11,2016-06-29 12:16:23,2016-06-29 12:16:22,closed,,0.19.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13510,b'BUG: date_range closed keyword with timezone aware start/end (GH12684)',b'closes #12684\r\n\r\n\r\n'
13484,161053962,ravinimmi,jreback,2016-06-19 03:02:19,2016-06-21 12:43:08,2016-06-21 12:43:02,closed,,0.19.0,6,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/13484,b'BUG: is_normalized returned False for local tz',"b"" - [x] closes #13459 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nis_normalized returned False for normalized date_range\r\nin case of local timezone 'Asia/Kolkata'\r\nFixes: #13459"""
13481,161031300,mbrucher,jreback,2016-06-18 16:09:28,2016-06-22 10:21:40,2016-06-22 10:21:28,closed,,0.19.0,22,Bug;IO CSV;Windows,https://api.github.com/repos/pydata/pandas/issues/13481,b'BUG: windows with TemporaryFile an read_csv #13398',"b' - [ x] closes #13398 \r\n - [ x] no tests added -> could not find location for IO tests?\r\n - [ x] passes ``git diff upstream/master | flake8 --diff``\r\n\r\nChange the way of reading back to readline (consistent with the test before entering the function)\r\n\r\nOne failure on Windows 10 (Python 3.5), but expected to fail actually (should probably tag it as well?)\r\n\r\n```\r\n======================================================================\r\nFAIL: test_next (pandas.io.tests.test_common.TestMMapWrapper)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""G:\\Informatique\\pandas\\pandas\\io\\tests\\test_common.py"", line 139, in test_next   \r\n    self.assertEqual(next_line, line)\r\nAssertionError: \'a,b,c\\r\\n\' != \'a,b,c\\n\'\r\n- a,b,c\r\n?      -\r\n+ a,b,c\r\n```\r\n'"
13477,161005760,sinhrks,jorisvandenbossche,2016-06-18 04:12:59,2016-07-11 07:31:21,2016-07-11 07:31:10,closed,,0.19.0,14,Bug;Clean;Missing-data,https://api.github.com/repos/pydata/pandas/issues/13477,b'BUG: Series/Index results in datetime/timedelta incorrectly if inputs are all nan/nat like',b'- [x] closes #13467  \r\n- [x] tests added / passed\r\n- [x] passes ``git diff upstream/master | flake8 --diff``\r\n\r\n'
13464,160715382,ravinimmi,jreback,2016-06-16 17:23:06,2016-06-17 22:04:40,2016-06-17 22:04:34,closed,,0.19.0,6,Bug;Dtypes;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13464,b'BUG: fix to_datetime to handle int16 and int8',b' - [x] closes #13451 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixes #13451'
13459,160609255,ravinimmi,jreback,2016-06-16 08:49:44,2016-06-21 12:43:02,2016-06-21 12:43:02,closed,,0.19.0,7,Bug;Testing;Timezones,https://api.github.com/repos/pydata/pandas/issues/13459,b'Test case failing - test_timezones.py:TestTimeZones.test_normalize_tz',"b'#### Code Sample, a copy-pastable example if possible\r\n```\r\nnosetests pandas/tseries/tests/test_timezones.py:TestTimeZones.test_normalize_tz\r\n\r\nF\r\n======================================================================\r\nFAIL: test_normalize_tz (pandas.tseries.tests.test_timezones.TestTimeZones)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ravikumarnimmi/work/pandas/pandas/tseries/tests/test_timezones.py"", line 1398, in test_normalize_tz\r\n    self.assertTrue(result.is_normalized)\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.026s\r\n\r\nFAILED (failures=1)\r\n```\r\n#### Expected Output\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: f7528866e07048f051f46f621ca76947730c7d32\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1+129.gf752886\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.2.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n```\r\n\r\n'"
13458,160563451,parthea,jorisvandenbossche,2016-06-16 02:16:25,2016-07-07 07:26:19,2016-07-07 07:26:12,closed,,0.19.0,9,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/13458,"b'In gbq, use googleapiclient instead of apiclient'",b' - [ x ] closes #13454\r\n - [ x ] tests added / passed\r\n - [ x ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ x ] whatsnew entry\r\n\r\nAll gbq tests pass locally\r\n\r\n```\r\ntony@tonypc:~/parthea-pandas/pandas/io/tests$ nosetests test_gbq.py -v\r\ntest_read_gbq_with_corrupted_private_key_json_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_read_gbq_with_empty_private_key_file_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_read_gbq_with_empty_private_key_json_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_read_gbq_with_invalid_private_key_json_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_read_gbq_with_private_key_json_wrong_types_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.GBQUnitTests) ... ok\r\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\r\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\r\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\r\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\r\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\r\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyContentsIntegration) ... ok\r\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyContentsIntegration) ... ok\r\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyContentsIntegration) ... ok\r\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyContentsIntegration) ... ok\r\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyContentsIntegration) ... ok\r\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyPathIntegration) ... ok\r\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyPathIntegration) ... ok\r\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyPathIntegration) ... ok\r\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyPathIntegration) ... ok\r\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorServiceAccountKeyPathIntegration) ... ok\r\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_read_as_service_account_with_key_contents (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_should_read_as_service_account_with_key_path (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\r\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\r\ntest_upload_data_as_service_account_with_key_contents (pandas.io.tests.test_gbq.TestToGBQIntegrationServiceAccountKeyContents) ... ok\r\ntest_upload_data_as_service_account_with_key_path (pandas.io.tests.test_gbq.TestToGBQIntegrationServiceAccountKeyPath) ... ok\r\npandas.io.tests.test_gbq.test_requirements ... ok\r\npandas.io.tests.test_gbq.test_generate_bq_schema_deprecated ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 73 tests in 435.170s\r\n\r\nOK\r\n```'
13454,160518194,parthea,jorisvandenbossche,2016-06-15 20:38:26,2016-07-07 07:26:12,2016-07-07 07:26:12,closed,,0.19.0,1,Bug;Compat;IO Google,https://api.github.com/repos/pydata/pandas/issues/13454,b'Exception when calling read_gbq : No module named discovery',"b'I ran into the following exception when using the pandas `read_gbq()` function.\r\n\r\n```\r\n      1 import pandas as pd\r\n----> 2 pd.read_gbq(\'select * from SampleDataset.SampleTable\', project_id=\'xxxxxxx\')\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/gbq.pyc in read_gbq(query, project_id, index_col, col_order, reauth, verbose)\r\n    425         job_id = uuid.uuid4().hex\r\n    426         rows = []\r\n--> 427         remaining_rows = len(dataframe)\r\n    428 \r\n    429         if self.verbose:\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/gbq.pyc in __init__(self, project_id, reauth)\r\n    111     not match the schema of the destination\r\n    112     table in BigQuery.\r\n--> 113     """"""\r\n    114     pass\r\n    115 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/gbq.pyc in test_google_api_imports(self)\r\n    127     Raised when BigQuery reports a streaming insert error.\r\n    128     For more information see `Streaming Data Into BigQuery\r\n--> 129     <https://cloud.google.com/bigquery/streaming-data-into-bigquery>`__\r\n    130     """"""\r\n    131 \r\n\r\nImportError: Missing module required for Google BigQuery support: No module named discovery\r\n```\r\n\r\nAn exception occurs in `test_google_api_imports()` . I believe the issue is on line 49 in https://github.com/pydata/pandas/blob/master/pandas/io/gbq.py . \r\n\r\nWe use\r\n```\r\nfrom apiclient.discovery import build\r\n```\r\n\r\ninstead of\r\n```\r\nfrom googleapiclient.discovery import build\r\n```\r\n\r\n`apiclient` is an alias for `googleapiclient` according to https://github.com/google/google-api-python-client/blob/master/apiclient/__init__.py#L1\r\n\r\nA similar issue is reported here: https://github.com/google/google-api-python-client/issues/13\r\n\r\n`apiclient.__version__` fails\r\n```\r\n>> import apiclient\r\n>> apiclient.__version__\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-90-662a0f56e34b> in <module>()\r\n      1 import apiclient\r\n----> 2 apiclient.__version__\r\n\r\nAttributeError: \'module\' object has no attribute \'__version__\'\r\n```\r\n\r\n`googleapiclient.__version__` works\r\n\r\n```\r\nimport googleapiclient\r\ngoogleapiclient.__version__\r\n```\r\n\r\n\r\n```\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: None\r\nnose: 1.3.7\r\npip: 1.5.6\r\nsetuptools: 5.5.1\r\nCython: 0.24\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.3\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.2\r\napiclient: 1.5.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```'"
13451,160438398,tdhopper,jreback,2016-06-15 14:36:41,2016-06-17 22:04:34,2016-06-17 22:04:34,closed,,0.19.0,1,Bug;Difficulty Novice;Dtypes;Effort Low;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13451,"b""to_datetime can't handle int16 or int8""","b""#### Code sample \r\n\r\n```\r\ndf = pd.DataFrame({'year': [2015, 2016],\r\n                       'month': [2, 3],\r\n                       'day': [4, 5]})\r\npd.to_datetime(df.astype('int16'))\r\n```\r\n\r\n#### Expected Output\r\n\r\n```\r\n0   2015-02-04\r\n1   2016-03-05\r\ndtype: datetime64[ns]\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.3\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.5.1\r\npytz: 2016.2\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext)\r\njinja2: 2.8\r\nboto: 2.39.0\r\npandas_datareader: None```"""
13441,160219903,priyankjain,jreback,2016-06-14 16:02:36,2016-06-21 10:09:36,2016-06-21 10:09:29,closed,,0.19.0,2,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13441,b'BUG: Rolling negative window issue fix #13383',b' - [x] closes #13383 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nAdded functionality in validate function of Rolling to ensure that window size is non-negative.\r\n'
13435,160066853,cmazzullo,jreback,2016-06-13 23:28:16,2016-06-18 15:25:37,2016-06-18 15:25:16,closed,,0.19.0,4,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13435,b'BUG: df.pivot_table: margins_name ignored when aggfunc is a list',b' - [ ] closes #13354 \r\n - [ ] tests added / passed\r\n - [ ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry'
13426,159766218,sinhrks,jreback,2016-06-11 10:01:59,2016-07-05 10:42:47,2016-07-05 10:42:47,closed,sinhrks,0.19.0,5,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/13426,b'BUG: categorical unpickle to use _coerce_indexer_dtype',b' \r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nfollow up for #13080 to use `_coerce_indexer_dtype`.\r\n'
13398,159161298,mbrucher,jreback,2016-06-08 13:18:20,2016-07-02 19:31:58,2016-07-02 19:31:58,closed,,0.19.0,26,Bug;Difficulty Novice;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/13398,"b""TemporaryFile as input to read_table raises TypeError: '_TemporaryFileWrapper' object is not an iterator""","b'Although the requirement in the doc says that the input can be a file like object, it doesn\'t work with objects from tempfile. On Windows, they can\'t be reopened, so I need to pass the object itself.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nimport pandas as pd\r\nfrom tempfile import TemporaryFile\r\nnew_file = TemporaryFile(""w+"")\r\ndataframe = pd.read_table(new_file, skiprows=3, header=None, sep=r""\\s*"")\r\n```\r\n\r\n#### Expected Output\r\nNot an exception!\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\n\r\npandas: 0.18.0\r\n'"
13397,159152587,GeraintDuck,jreback,2016-06-08 12:35:01,2016-06-08 15:27:11,2016-06-08 15:27:07,closed,,0.19.0,4,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/13397,b'BUG: Fix for .extractall (single group with quantifier) #13382',"b"" - [X] closes #13382\r\n - [X] tests added / passed\r\n - [X] passes ``git diff upstream/master | flake8 --diff``\r\n - [X] whatsnew entry\r\n\r\nNote that I had to fix it based on this [thread](http://stackoverflow.com/questions/2111759/whats-the-best-practice-for-handling-single-value-tuples-in-python), rather than directly with `[x for x in ['ab']]` as this broke previous tests. """
13394,159050127,pijucha,jreback,2016-06-07 23:50:50,2016-07-03 23:28:58,2016-07-03 23:28:31,closed,,0.19.0,20,Bug;Categorical;Groupby,https://api.github.com/repos/pydata/pandas/issues/13394,"b'BUG: Fix groupby with ""as_index"" for categorical multi #13204'","b' - [x] closes #13204\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixes a bug that returns all nan\'s for groupby(as_index=False) with\r\nmultiple column groupers containing a categorical one (#13204).\r\n\r\n---\r\nAlso:\r\nfixes an internal bug in the string representation of `Grouping`.\r\n```python\r\nmi = pd.MultiIndex.from_arrays([list(""AAB""), list(""aba"")])\r\ndf = pd.DataFrame([[1,2,3]], columns=mi)\r\ngr = df.groupby(df[(\'A\', \'a\')])\r\n\r\ngr.grouper.groupings\r\n...\r\nTypeError: not all arguments converted during string formatting\r\n```'"
13383,158803947,mikegraham,jreback,2016-06-07 00:19:52,2016-06-21 10:09:29,2016-06-21 10:09:29,closed,,0.19.0,3,Bug;Difficulty Novice;Effort Low;Error Reporting;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13383,"b""Series.rolling/DataFrame.rolling don't fully check arguments, have odd behavior when used with invalid inputs""","b'```\r\nIn [171]: s = pd.Series(range(3))\r\n\r\nIn [172]: s.rolling(-1) # doesn\'t raise\r\nOut[172]: Rolling [window=-1,center=False,axis=0]\r\n\r\nIn [173]: s.rolling(-1).mean() # Odd, indirect error\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-173-cda5b8dd0812> in <module>()\r\n----> 1 s.rolling(-1).mean()\r\n\r\n/home/mike/modernpandas/local/lib/python2.7/site-packages/pandas/core/window.pyc in mean(self, **kwargs)\r\n    885     @Appender(_shared_docs[\'mean\'])\r\n    886     def mean(self, **kwargs):\r\n--> 887         return super(Rolling, self).mean(**kwargs)\r\n    888 \r\n    889     @Substitution(name=\'rolling\')\r\n\r\n/home/mike/modernpandas/local/lib/python2.7/site-packages/pandas/core/window.pyc in mean(self, **kwargs)\r\n    651 \r\n    652     def mean(self, **kwargs):\r\n--> 653         return self._apply(\'roll_mean\', \'mean\', **kwargs)\r\n    654 \r\n    655     _shared_docs[\'median\'] = dedent(""""""\r\n\r\n/home/mike/modernpandas/local/lib/python2.7/site-packages/pandas/core/window.pyc in _apply(self, func, name, window, center, check_minp, how, **kwargs)\r\n    558                 result = np.apply_along_axis(calc, self.axis, values)\r\n    559             else:\r\n--> 560                 result = calc(values)\r\n    561 \r\n    562             if center:\r\n\r\n/home/mike/modernpandas/local/lib/python2.7/site-packages/pandas/core/window.pyc in calc(x)\r\n    553 \r\n    554                 def calc(x):\r\n--> 555                     return func(x, window, min_periods=self.min_periods)\r\n    556 \r\n    557             if values.ndim > 1:\r\n\r\n/home/mike/modernpandas/local/lib/python2.7/site-packages/pandas/core/window.pyc in func(arg, window, min_periods)\r\n    540                     # GH #12373: rolling functions error on float32 data\r\n    541                     return cfunc(com._ensure_float64(arg),\r\n--> 542                                  window, minp, **kwargs)\r\n    543 \r\n    544             # calculation function\r\n\r\npandas/algos.pyx in pandas.algos.roll_mean (pandas/algos.c:28921)()\r\n\r\npandas/algos.pyx in pandas.algos._check_minp (pandas/algos.c:19103)()\r\n\r\nValueError: min_periods must be >= 0\r\n\r\nIn [174]: pd.Series([]).rolling(-1).mean() # Never raises\r\nOut[174]: Series([], dtype: float64)\r\n\r\nIn [175]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-71-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.0.2\r\nsetuptools: 20.1.1\r\nCython: 0.24\r\nnumpy: 1.11.0\r\nscipy: 0.17.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.1.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: 1.3.2\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n\r\n```'"
13382,158738996,GeraintDuck,jreback,2016-06-06 18:08:11,2016-06-08 15:27:07,2016-06-08 15:27:07,closed,,0.19.0,3,Bug;Difficulty Intermediate;Effort Low;Strings,https://api.github.com/repos/pydata/pandas/issues/13382,b'BUG: .extractall() throws AssertionError if capture group length > 1',"b'Code to replicate error:\r\n```\r\nimport pandas as pd\r\ns = pd.Series([""a13a23"", ""b13"", ""c13""], index=[""A"", ""B"", ""C""])\r\ns.str.extractall(""[ab](\\d\\d)"")\r\n```\r\n\r\nNote that the regex `[ab](\\d)` from the documentation page works, whereas `[ab](\\d\\d)` above doesn\'t. It seems that any captured group with a length of > 1 causes this error.\r\n\r\nThough playing with this a bit more, the following regex\'s all seem to work correctly without error:\r\n```\r\n([ab])(\\d\\d)\r\n()[ab](\\d+)\r\n(a13)(\\d\\d)\r\n```\r\n\r\nI\'ve reproduced the issue in both versions 0.18.0 and 0.18.1. I\'ll admit I\'ve not checked against the master branch though.\r\n\r\nNote: I posted this to the mailing list, but haven\'t had any responses - thus I assume this is a bug.\r\nI\'m unsure what the underlying cause is here (maybe it doesn\'t like the first regex character not being within a capture group?).\r\n\r\n'"
13359,158470738,chrish42,jreback,2016-06-03 23:07:02,2016-06-05 14:08:29,2016-06-05 14:08:12,closed,,0.19.0,3,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/13359,"b""Make pd.read_hdf('data.h5') work when pandas object stored contained categorical columns""",b' - [x] closes #13231 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n'
13354,158311433,ekeydar,jreback,2016-06-03 07:56:45,2016-06-18 15:25:15,2016-06-18 15:25:15,closed,,0.19.0,1,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13354,b'BUG: df.pivot_table: margins_name is ignored when there aggfunc is list',"b'#### Code Sample, a copy-pastable example if possible\r\n```\r\ncosts = pd.DataFrame({\'item\':[\'bacon\',\'cheese\',\'bacon\',\'cheese\'],\r\n                      \'cost\':[2.5,4.5,3.2,3.3],\r\n                     \'store\':[\'SuperS\',\'DollarM\',\'SuperS\',\'DollarM\'],\r\n                     \'day\':[\'M\',\'M\',\'T\',\'T\']})\r\ncosts.pivot_table(index=""item"",columns=""day"",margins=True, margins_name=""Weekly"",aggfunc=[np.mean,max])\r\n```\r\nWill ignore the margins_name\r\n\r\n#### Expected Output\r\n```\r\n      mean               max                                     \r\n       cost              cost              store                  \r\n          M     T    Weekly    M    T  Weekly        M        T      Weekly\r\nitem                                                              \r\nbacon   2.5  3.20  2.850  2.5  3.2  3.2   SuperS   SuperS   SuperS\r\ncheese  4.5  3.30  3.900  4.5  3.3  4.5  DollarM  DollarM  DollarM\r\nAll     3.5  3.25  3.375  4.5  3.3  4.5   SuperS   SuperS   SuperS\r\n```\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-43-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 1.5.6\r\nsetuptools: 20.2.2\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.17.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nhttplib2: 0.9.2\r\napiclient: 1.5.0\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n```'"
13331,157621946,chenyun90323,jreback,2016-05-31 08:58:41,2016-05-31 11:52:00,2016-05-31 11:51:01,closed,,,1,Bug;Duplicate;Missing-data,https://api.github.com/repos/pydata/pandas/issues/13331,b'pd.DataFrame.describe Invalid value encountered in percentile',"b'In RHEL 7.2, with python 2.7.5\r\n``` \r\n    a = pd.np.random.randn(10, 2)\r\n    a.itemset((3, 0), pd.np.NAN)\r\n    df = pd.DataFrame(a)\r\n    res = df.describe()\r\n```\r\n\r\n---\r\nOutput:\r\n```\r\n   /.../function_base.py:3823: RuntimeWarning: Invalid value encountered in percentile\r\n  RuntimeWarning)\r\n              0          1\r\ncount  8.000000  10.000000\r\nmean  -0.406550  -0.220345\r\nstd    1.517141   0.791003\r\nmin   -1.942997  -1.607897\r\n25%         NaN  -0.683385\r\n50%         NaN  -0.281060\r\n75%         NaN   0.303566\r\nmax    2.275532   1.227234\r\n```\r\n---\r\npd.show_versions:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-327.18.2.el7.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 0.9.8\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.17.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 1.5\r\npytz: 2012d\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: None\r\nnumexpr: 2.5.2\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n```\r\n\r\n'"
13327,157537294,randomgambit,jreback,2016-05-30 17:33:14,2016-06-18 15:41:58,2016-05-30 20:50:20,closed,,No action,9,Bug;Duplicate;Resample,https://api.github.com/repos/pydata/pandas/issues/13327,b'feature request: rolling with datetime range',"b""Hello everyone,\r\n\r\nI would like to submit one very useful addition to rolling, namely the possibility to compute any statistics over a specific **time range.**\r\n\r\nIndeed, my understanding is that `rolling(windows=5).mean()` computes, say, the mean over the last five observations. \r\n\r\nInstead, it would be very useful to specify something like `rolling(windows=5,type_windows='time_range').mean() to get the rolling mean over the last 5 days. \r\n\r\nSo if your data starts on January 1 and then the next data point is on Feb 2nd, then the rolling mean for the Feb 2nb point is NA because there was no data on Jan 29, 30, 31, Feb 1, Feb 2.\r\n\r\nI believe this would be very useful in settings where data represents trading data, so most of the time the data points are not equidistant in time. Still, you want to compute rolling metrics that are specified over the same delta.\r\n\r\nWhat do you think?\r\n\r\nThanks!\r\n"""
13326,157533751,RogerThomas,jreback,2016-05-30 16:55:24,2016-05-31 16:23:44,2016-05-31 15:43:14,closed,,0.19.0,7,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/13326,b'BUG: Fix maybe_convert_numeric for unhashable objects',b' - [x] closes #13324\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13324,157479820,RogerThomas,jreback,2016-05-30 10:54:53,2016-05-31 15:43:14,2016-05-31 15:43:14,closed,,0.19.0,1,Bug;Difficulty Novice;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/13324,"b""to_numeric(errors='coerce') broken when using unhashable objects""","b'When using pandas.to_numeric with errors=\'coerce\' and non hashable objects in the series/dataframe pandas returns the same series/dataframe as passed in, instead of coercing the non-numeric objects to nans.\r\n\r\n```python\r\npython -c ""import pandas as pd; s = pd.Series([[13], 1.0, \'apple\']); print(pd.to_numeric(s, errors=\'coerce\'))""\r\n```\r\n\r\nThe error is due to this line\r\n\r\nhttps://github.com/pydata/pandas/blob/4de83d25d751d8ca102867b2d46a5547c01d7248/pandas/src/inference.pyx#L572\r\n\r\nMore generally this will affect any call to ``maybe_convert_numeric`` that has unhashable objects in the ``values`` param.\r\n\r\nSuggested fix might be to update this check with something like\r\n\r\n```python\r\nif val.__hash__ is not None and val in na_values:\r\n    blah\r\n```\r\nAs there should never be a case where you would expect an unhashable object to be in a set of na_values\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-22-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_IE.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 19.0\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n'"
13320,157409838,gfyoung,jreback,2016-05-29 22:35:20,2016-06-01 11:14:00,2016-06-01 11:10:08,closed,,0.19.0,6,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/13320,b'BUG: Parse trailing NaN values for the Python parser',b'Fixes bug in which the Python parser failed to detect trailing `NaN` values in rows\r\n'
13316,157380290,adneu,jreback,2016-05-29 10:46:11,2016-07-06 21:48:23,2016-07-06 21:47:54,closed,,0.19.0,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/13316,b'BUG: Groupby.nth includes group key inconsistently #12839',"b' - [x] closes #12839\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nWhen the group selection changes, the cache for `_selected_obj` needs to be reset so that `nth`, `head`, and `tail` can return consistent results.'"
13313,157325137,uwedeportivo,jreback,2016-05-28 07:07:58,2016-06-02 18:01:05,2016-06-02 18:00:21,closed,,0.19.0,12,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/13313,b'Fix #13306: Hour overflow in tz-aware datetime conversions.',b' - [x] closes #13306\r\n - [x] tests passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n        Bug: tz-converting tz-aware DateTimeIndex relied on index being sorted for correct results.\r\n'
13307,157303382,gliptak,jreback,2016-05-27 22:48:21,2016-06-03 15:04:28,2016-06-03 15:04:18,closed,,0.19.0,5,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/13307,b'Fix series comparison operators when dealing with zero rank numpy arrays',b' - [x] closes #13006\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13306,157302384,dlpd,jreback,2016-05-27 22:38:17,2016-06-02 18:00:21,2016-06-02 18:00:21,closed,,0.19.0,10,Bug;Difficulty Intermediate;Effort Low;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/13306,b'Hour overflow in tz-aware datetime conversions',"b""Comparison of tz-aware timestamps fails across DST boundaries. The comment in tslib.pyx:3845\r\n```\r\n# TODO: this assumed sortedness :/\r\n```\r\nperhaps implies this is a known problem that was never resolved, so apologies if a new issue is not appropriate.\r\n\r\n#### Self contained example\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Manifesting issue:                                                                                                                                                                                                                                 \r\nts_listA = ['2008-05-12 09:50:00-04:00',\r\n            '2008-05-12 09:50:18-04:00',\r\n            '2008-05-12 09:50:33-04:00',\r\n            '2008-05-12 09:50:33-04:00',\r\n            '2008-12-12 09:50:35-05:00',\r\n            '2008-05-12 09:50:32-04:00',\r\n            '2008-05-12 09:49:15-04:00',\r\n            '2008-05-12 09:50:49-04:00',\r\n            '2008-05-12 09:50:54-04:00']\r\n\r\nts_listB = ['2008-05-12 09:50:34-04:00',\r\n            '2008-05-12 09:50:35-04:00',\r\n            '2008-05-12 09:50:36-04:00',\r\n            '2008-05-12 09:50:40-04:00',\r\n            '2008-05-12 09:50:42-04:00',\r\n            '2008-05-12 09:50:43-04:00',\r\n            '2008-05-12 09:50:55-04:00',\r\n            '2008-05-12 09:50:55-04:00',\r\n            '2008-05-12 09:50:57-04:00']\r\n\r\ndf = pd.DataFrame({'dtA' : pd.to_datetime(ts_listA).tz_localize('US/Eastern'),\r\n                   'dtB' : pd.to_datetime(ts_listB).tz_localize('US/Eastern')})\r\nprint (df.dtA - df.dtB) / pd.Timedelta('1 minutes')\r\n\r\n# Underlying tslib.tz_convert call - similar problem                                                                                                                                                                                                 \r\nasi8 = np.array([1210600200000000000,\r\n                 1210600218000000000,\r\n                 1210600233000000000,\r\n                 1210600233000000000,\r\n                 1229093435000000000,\r\n                 1210600232000000000,\r\n                 1210600155000000000,\r\n                 1210600249000000000,\r\n                 1210600254000000000])\r\n\r\ntz = pd.Timestamp('2008-05-12 12:00:00').tz_localize('US/Eastern').tz\r\nresult = pd.tslib.tz_convert(asi8, 'UTC', tz)\r\n\r\nprint pd.to_datetime(asi8)\r\nprint pd.to_datetime(result)\r\n```\r\n#### Output\r\nExpected output:\r\n\r\n```\r\n0        -0.566667\r\n1        -0.283333\r\n2        -0.050000\r\n3        -0.116667\r\n4    308219.883333\r\n5        -0.183333\r\n6        -1.666667\r\n7        -0.100000\r\n8        -0.050000\r\nName: dtA, dtype: float64\r\n```\r\n\r\nActual output of timedelta computation:\r\n```\r\n0        -0.566667\r\n1        -0.283333\r\n2        -0.050000\r\n3        -0.116667\r\n4    308219.883333\r\n5       -60.183333\r\n6       -61.666667\r\n7       -60.100000\r\n8       -60.050000\r\nName: dtA, dtype: float64\r\n```\r\nComputed timedelta for rows after the 2008-12-12 date are off by an hour.\r\n\r\nOutput of tz_convert:\r\n```\r\nDatetimeIndex(['2008-05-12 13:50:00', '2008-05-12 13:50:18',\r\n               '2008-05-12 13:50:33', '2008-05-12 13:50:33',\r\n               '2008-12-12 14:50:35', '2008-05-12 13:50:32',\r\n               '2008-05-12 13:49:15', '2008-05-12 13:50:49',\r\n               '2008-05-12 13:50:54'],\r\n              dtype='datetime64[ns]', freq=None)\r\nDatetimeIndex(['2008-05-12 09:50:00', '2008-05-12 09:50:18',\r\n               '2008-05-12 09:50:33', '2008-05-12 09:50:33',\r\n               '2008-12-12 09:50:35', '2008-05-12 08:50:32',\r\n               '2008-05-12 08:49:15', '2008-05-12 08:50:49',\r\n               '2008-05-12 08:50:54'],\r\n              dtype='datetime64[ns]', freq=None)\r\n```\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-504.1.3.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 20.3\r\nCython: None\r\nnumpy: 1.8.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext)\r\nJinja2: None\r\n```\r\n"""
13298,157074043,pijucha,jreback,2016-05-26 20:39:43,2016-05-31 15:48:24,2016-05-31 14:12:51,closed,,0.19.0,6,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/13298,"b'BUG: Fix describe(): percentiles (#13104), col index (#13288)'","b' - [x] closes #13104, #13288\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nBUG #13104:\r\n- Percentiles are now rounded to the least precision that keeps\r\nthem unique.\r\n- Supplying duplicates in percentiles will raise ValueError.\r\n\r\nBUG #13288\r\n- Fixed a column index of the output data frame.\r\nPreviously, if a data frame had a column index of object type and\r\nthe index contained numeric values, the output column index could\r\nbe corrupt. It led to ValueError if the output was displayed.'"
13288,156863011,pijucha,jreback,2016-05-25 22:17:16,2016-05-31 14:12:51,2016-05-31 14:12:51,closed,,0.19.0,7,Bug;Difficulty Novice;Effort Low,https://api.github.com/repos/pydata/pandas/issues/13288,b'BUG: DataFrame.describe() breaks with a column index of object type and numeric entries',"b'Preparing a commit for another issue in `.describe()`, I encountered this puzzling bug, surprisingly easy to trigger.\r\n\r\n#### Symptoms\r\n\r\n```python\r\ndf = pd.DataFrame({\'A\': list(""BCDE""), 0: [1,2,3,4]})\r\ndf.describe()\r\n# Long traceback listing formatting and internal functions...\r\nValueError: Buffer dtype mismatch, expected \'Python object\' but got \'long\'\r\n```\r\n\r\nHowever:\r\n\r\n```python\r\ndf.describe(include=\'all\')\r\n               0    A\r\ncount   4.000000    4\r\nunique       NaN    4\r\ntop          NaN    D\r\nfreq         NaN    1\r\nmean    2.500000  NaN\r\nstd     1.290994  NaN\r\nmin     1.000000  NaN\r\n25%     1.750000  NaN\r\n50%     2.500000  NaN\r\n75%     3.250000  NaN\r\nmax     4.000000  NaN\r\n\r\n# It\'s OK if we don\'t print on screen:\r\nx = df.describe()\r\nx.columns\r\nOut[8]: Index([0], dtype=\'int64\')\r\n\r\n# Fixing this suspicious index (int works too):\r\nx.columns = x.columns.astype(object)\r\nx\r\nOut[10]: \r\n              0\r\ncount  4.000000\r\nmean   2.500000\r\nstd    1.290994\r\nmin    1.000000\r\n25%    1.750000\r\n50%    2.500000\r\n75%    3.250000\r\nmax    4.000000\r\n```\r\n\r\nSame issue happens with a simpler data frame:\r\n```python\r\ndf0 = pd.DataFrame([1,2,3,4])\r\n# It\'s  OK now\r\ndf0.describe()\r\nOut[28]: \r\n              0\r\ncount  4.000000\r\nmean   2.500000\r\nstd    1.290994\r\nmin    1.000000\r\n25%    1.750000\r\n50%    2.500000\r\n75%    3.250000\r\nmax    4.000000\r\n\r\n# Modify column index:\r\ndf0.columns = pd.Index([0], dtype=object)\r\ndf0.describe()\r\n# ...\r\nValueError: Buffer dtype mismatch, expected \'Python object\' but got \'long\'\r\n```\r\nCurrent version (but the bug is also present in pandas release 0.18.1):\r\n```\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.1.20-1\r\nmachine: x86_64\r\nprocessor: Intel(R)_Core(TM)_i5-2520M_CPU_@_2.50GHz\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1+64.g7ed22fe.dirty\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 21.0.0\r\nCython: 0.24\r\nnumpy: 1.11.0\r\nscipy: 0.17.0.dev0+3f3c371\r\nIPython: 4.0.1\r\n...\r\n```\r\n\r\n#### Reason\r\nSome internal function gets confused by dtypes of a column index, I guess. But the faulty index is created in `.describe()`.\r\n```python\r\n# Output from %debug df.describe()\r\n# NDFrame.describe() in pandas/core/generic.py:\r\n#\r\n   4943             data = self\r\n   4944         else:\r\n   4945             data = self.select_dtypes(include=include, exclude=exclude)\r\n   4946 \r\n   4947         ldesc = [describe_1d(s, percentiles) for _, s in data.iteritems()]\r\n   4948         # set a convenient order for rows\r\n   4949         names = []\r\n   4950         ldesc_indexes = sorted([x.index for x in ldesc], key=len)\r\n   4951         for idxnames in ldesc_indexes:\r\n   4952             for name in idxnames:\r\n   4953                 if name not in names:\r\n   4954                     names.append(name)\r\n   4955 \r\n   4956         d = pd.concat(ldesc, join_axes=pd.Index([names]), axis=1)\r\n1> 4957         d.columns = self.columns._shallow_copy(values=d.columns.values)\r\n   4958         d.columns.names = data.columns.names\r\n   4959         return d\r\n```\r\n`_shallow_copy()` in the marked line changes `d.columns`:\r\n```python\r\nipdb> p d.columns\r\nInt64Index([0], dtype=\'int64\')\r\nipdb> n\r\n> /home/users/piotr/workspace/pandas-pijucha/pandas/core/generic.py(4958)describe()\r\n1  4957         d.columns = self.columns._shallow_copy(values=d.columns.values)\r\n-> 4958         d.columns.names = data.columns.names\r\n   4959         return d\r\nipdb> p d.columns\r\nIndex([0], dtype=\'int64\')\r\n```\r\n#### Possible solutions\r\n\r\nLines 4957-4958 are actually used to fix issues that `pd.concat` brings about. They try to pass the column structure from `self` to `d`.\r\nI think a simpler solution is replacing these lines with:\r\n```python\r\n d = pd.concat(ldesc, join_axes=pd.Index([names]), axis=1)\r\n d.columns = data.columns\r\n return d\r\n```\r\nor\r\n```python\r\nd = pd.DataFrame(pd.concat(ldesc, axis=1), index = pd.Index(names), columns = data.columns)\r\nreturn d\r\n```\r\n`data` is a subframe of `self` and retains the same column structure.\r\n\r\n`pd.concat` has some parameters that help pass a hierarchical index but can\'t do anything on its own with a categorical one.\r\n\r\nI\'m going to submit a pull request with this fix together with some others related with `describe()`. I hope I haven\'t overlooked anything obvious. But if so, any comments are very welcome.\r\n'"
13285,156803250,jreback,jreback,2016-05-25 17:11:32,2016-05-26 16:13:01,2016-05-26 16:13:01,closed,,0.19.0,1,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/13285,"b'COMPAT: extension dtypes (DatetimeTZ, Categorical) are now Singleton cached objects'","b""allows for proper is / == comparisons\r\nHad this odd semantic difference as these were really different objects (though they DID hash the same)\r\nThis doesn't actually affect any user code.\r\n\r\n```\r\nIn [1]: from pandas.core import common as com\r\n\r\nIn [2]: t1 = com.DatetimeTZDtype('datetime64[ns, US/Eastern]')\r\n\r\nIn [3]: t2 = com.DatetimeTZDtype('datetime64[ns, US/Eastern]')\r\n\r\nIn [4]: t1 == t2\r\nOut[4]: True\r\n\r\nIn [5]: t1 is t2\r\nOut[5]: False\r\n\r\nIn [6]: hash(t1)\r\nOut[6]: 5756291921003024619\r\n\r\nIn [7]: hash(t2)\r\nOut[7]: 5756291921003024619\r\n```"""
13282,156710959,nparley,jreback,2016-05-25 10:03:45,2016-05-25 17:35:10,2016-05-25 17:35:10,closed,,0.19.0,1,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/13282,b'pandas.show_versions causing malloc_error_break',"b'If blosc is installed show_versions() will caused python to produce a malloc_error_break (double free) error on exiting. This can be seen in the travis runs under `source activate pandas && ci/print_versions.py` section but can also be easily replicated on Linux and Mac. The fix for this is to replace imp (which has been deprecated) with importlib. I will create and link a PR.\r\n\r\n```python\r\n>>> import pandas\r\n>>> pandas.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: e0a2e3bc51f9e178f72c44e6de06700ee0bf31c6\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.18.1+66.ge0a2e3b\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.7.0\r\nCython: 0.24\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: 1.3.2\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None```\r\n\r\n>>> \r\npython(1726,0x7fff77074000) malloc: *** error for object 0x102573a00: pointer being freed was not allocated\r\n*** set a breakpoint in malloc_error_break to debug\r\nAbort trap: 6\r\n\r\n\r\n\r\n'"
13274,156631375,gfyoung,jreback,2016-05-24 23:24:22,2016-05-25 19:26:55,2016-05-25 17:32:03,closed,,0.19.0,11,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/13274,"b'BUG, ENH: Improve infinity parsing for read_csv'","b""1) Allow mixed-case infinity strings for the Python engine\r\n\r\nBug was traced back via `lib.maybe_convert_numeric` to the `floatify` function in `pandas/src/parse_helper.h`.  In addition to correcting the bug and adding tests for it, this PR also moves the `test_inf_parsing` test from `c_parser_only.py` to `common.py` in the `pandas/io/tests/parser` dir.\r\n\r\n2) Interpret `+inf` as positive infinity for both engines\r\n\r\n`float('+inf')` in Python is interpreted as positive infinity, so we should allow it too in parsing."""
13267,156500689,jreback,jreback,2016-05-24 12:58:38,2016-05-24 15:28:17,2016-05-24 15:28:17,closed,,0.19.0,0,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/13267,b'BUG: Bug in selection from a HDFStore with a fixed format and start and/or stop will now return the selected range',b'closes #8287'
13261,156375429,sinhrks,jreback,2016-05-23 21:54:00,2016-05-24 13:23:41,2016-05-24 13:23:37,closed,,0.19.0,1,Bug;Categorical;Dtypes,https://api.github.com/repos/pydata/pandas/issues/13261,b'BUG: remove_unused_categories dtype coerces to int64',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n```\r\nc = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\r\nc.codes\r\n# array([0, 1], dtype=int8)\r\n\r\n# NG, must be int8 dtype\r\nc = c.remove_unused_categories()\r\nc.codes\r\n# array([0, 1])\r\n```\r\n\r\nIt is because ``np.unique`` uses platform int for ``unique_inverse``\r\n```\r\nnp.unique(np.array([0, 3, 2, 3], dtype=np.int8), return_inverse=True)\r\n(array([0, 2, 3], dtype=int8), array([0, 2, 1, 2]))\r\n```\r\n"""
13248,156086390,sinhrks,jreback,2016-05-21 06:05:08,2016-05-21 18:46:03,2016-05-21 18:45:57,closed,,0.19.0,1,Bug;Duplicate;Numeric,https://api.github.com/repos/pydata/pandas/issues/13248,b'BUG: DataFrame and datetime scalar ops may results in incorrect dtype',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\n``Timestamp`` subtraction resuts in ``Timedelta``\r\n\r\n```\r\npd.Timestamp('2011-01-01') - pd.Timestamp('2010-01-01')\r\n# Timedelta('365 days 00:00:00')\r\n```\r\n\r\nBut it doesn't when broadcasted to ``DataFrame``. It's because ``DataFrame`` op calls\r\n``_try_corce_results`` which coerces the result to ``datetime64``.\r\n\r\n```\r\ndf = pd.DataFrame({'a': [pd.Timestamp('2011-01-01')]})\r\ndf - pd.Timestamp('2010-01-01')\r\n#            a\r\n# 0 1971-01-01\r\n```\r\n\r\n#### Expected Output\r\n\r\n```\r\n#          a\r\n# 0 365 days\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\ncurrent master\r\n"""
13241,156023039,roycoding,jreback,2016-05-20 18:25:40,2016-05-25 12:15:02,2016-05-25 12:14:52,closed,,0.19.0,3,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/13241,b'Fixes resampler for grouping kw bug #13235',b' - [x] closes #13235\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13239,155999239,zhangyingmath,jreback,2016-05-20 16:14:33,2016-05-20 16:32:58,2016-05-20 16:32:37,closed,,,1,Bug;Duplicate;Groupby;Indexing,https://api.github.com/repos/pydata/pandas/issues/13239,b'nunique Error when called by groupby from an empty dataframe',"b'Initialize a dataframe: (I am using pandas version 0.17.1)\r\n```\r\nimport pandas as pd\r\na=pd.DataFrame({\'f\':[1,2,3,4]})\r\n```\r\n\r\nFirst if I call nunique from an empty dataframe, I get what I expect:\r\n\r\n```\r\na[a[\'f\']>10][\'f\'].nunique()\r\n0\r\n```\r\n\r\nBut If I call nunique from groupby, I get an error:\r\n\r\n```\r\na[a[\'f\']>10].groupby(\'f\')[\'f\'].nunique()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/opt/ci1/plus/tc10b/sles11sp2_gcc-4.8.3_x86-64/PythonModules-2.7.8/20160310/lib/python/pandas/core/groupby.py"", line 2695, in nunique\r\n    return Series(out if ids[0] != -1 else out[1:],\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\n\r\nOn the other hand, the \'unique\' function doesn\'t have this issue:\r\n\r\n```\r\na[a[\'f\']>10][\'f\'].unique()\r\narray([], dtype=int64)\r\n\r\na[a[\'f\']>10].groupby(\'f\')[\'f\'].unique()\r\nSeries([], Name: f, dtype: float64)\r\n```\r\n\r\nI would like the \'nunique\' function to behave like \'unique\' when handling an empty dataframe + groupby. (Or please let me know if this issue is already fixed in later versions.) Thanks!'"
13235,155905085,roycoding,jreback,2016-05-20 07:41:17,2016-05-25 12:14:52,2016-05-25 12:14:52,closed,,0.19.0,2,Bug;Difficulty Novice;Effort Low;Resample,https://api.github.com/repos/pydata/pandas/issues/13235,b'label keyword argument for resample causes an error when used with a groupby object',"b'#### Code Sample, a copy-pastable example if possible\r\n```Python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nindex = pd.date_range(\'2000-01-01\', freq=\'2D\', periods=5)\r\ndf = pd.DataFrame(index=index, data={\'col1\':[0,0,1,1,2], \'col2\':[1,1,1,1,1]})\r\n\r\ndf.groupby(\'col1\').resample(\'1W\', label=\'left\').sum()\r\n```\r\n\r\n#### The dataframe\r\n```\r\n            col1  col2\r\n2000-01-01     0     1\r\n2000-01-03     0     1\r\n2000-01-05     1     1\r\n2000-01-07     1     1\r\n2000-01-09     2     1\r\n```\r\n\r\n#### Expected Output\r\n```\r\n                 col1  col2\r\ncol1\r\n0    1999-12-26     0     1\r\n     2000-01-02     0     1\r\n1    2000-01-02     2     2\r\n2    2000-01-02     2     1\r\n```\r\n\r\n#### Actual ouput\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-53-6e7ac0fde8b3> in <module>()\r\n----> 1 df.groupby(\'col1\').resample(\'1W\', label=\'left\').sum()\r\n\r\n/Users/roycoding/venv-lib-upgrade/lib/python2.7/site-packages/pandas/core/groupby.pyc in resample(self, rule, *args, **kwargs)\r\n   1080         """"""\r\n   1081         from pandas.tseries.resample import get_resampler_for_grouping\r\n-> 1082         return get_resampler_for_grouping(self, rule, *args, **kwargs)\r\n   1083\r\n   1084     @Substitution(name=\'groupby\')\r\n\r\n/Users/roycoding/venv-lib-upgrade/lib/python2.7/site-packages/pandas/tseries/resample.pyc in get_resampler_for_grouping(groupby, rule, how, fill_method, limit, kind, **kwargs)\r\n    910                                        fill_method=fill_method,\r\n    911                                        limit=limit,\r\n--> 912                                        **kwargs)\r\n    913\r\n    914\r\n\r\nTypeError: _maybe_process_deprecations() got an unexpected keyword argument \'label\'\r\n```\r\n\r\nWithout the `label` keyword, I get this (with expected right labeled dates):\r\n```\r\n                 col1  col2\r\ncol1\r\n0    2000-01-02     0     1\r\n     2000-01-09     0     1\r\n1    2000-01-09     2     2\r\n2    2000-01-09     2     1\r\n```\r\n\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 1.5.6\r\nsetuptools: 3.6\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 3.2.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.5\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: 0.9999999\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n```'"
13234,155856779,gliptak,jreback,2016-05-19 23:21:12,2016-05-23 13:05:48,2016-05-23 13:05:43,closed,,0.19.0,6,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/13234,b'Correct ValueError invalid type promotion exception',b' - [x] closes #12599\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13231,155804314,chrish42,jreback,2016-05-19 18:31:16,2016-06-05 14:08:12,2016-06-05 14:08:12,closed,,0.19.0,14,Bug;Difficulty Novice;Effort Low;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/13231,b'Automatic detection of HDF5 dataset identifier fails when data contains categoricals',"b""We use HDF5 to store our pandas dataframes on disk. We only store one dataframe per HDF5, so the feature of pandas.read_hdf() that allows omitting the key when a HDF file contains a single Pandas object is very nice for our workflow.\r\n\r\nHowever, said feature doesn't work when the dataframe saved contains one or more categorical columns:\r\n\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col1': [11, 21, 31], 'col2': ['a', 'b', 'a']})\r\n\r\n# This works fine.\r\ndf.to_hdf('no_cat.hdf5', 'data', format='table')\r\ndf2 = pd.read_hdf('no_cat.hdf5')\r\nprint((df == df2).all().all())\r\n\r\n# But this produces an exception.\r\ndf.assign(col2=pd.Categorical(df.col2)).to_hdf('cat.hdf5', 'data', format='table')\r\ndf3 = pd.read_hdf('cat.hdf5')\r\n\r\n# ValueError: key must be provided when HDF file contains multiple datasets.\r\n```\r\n\r\nIt looks like this is because pandas.read_hdf() doesn't ignore the metadata used to store the categorical codes:\r\n\r\n```\r\nprint(pd.HDFStore('cat.hdf5'))\r\n\r\n<class 'pandas.io.pytables.HDFStore'>\r\nFile path: cat.hdf5\r\n/data                                     frame_table  (typ->appendable,nrows->3,ncols->2,indexers->[index])             \r\n/data/meta/values_block_1/meta            series_table (typ->appendable,nrows->2,ncols->1,indexers->[index],dc->[values])\r\n```\r\n\r\nit'd be nice if this feature worked even when some of the columns are categoricals. It should be possible to ignore that metadata that pandas creates when looking if there is only one dataset stored, no?"""
13219,155547716,PeterKucirek,jreback,2016-05-18 16:35:22,2016-06-01 11:13:41,2016-06-01 11:13:41,closed,,0.19.0,2,Bug;Difficulty Novice;Effort Low;IO CSV;Unicode,https://api.github.com/repos/pydata/pandas/issues/13219,b'Unicode not acceptable input for `usecols` kwarg in `read_csv()`',"b""I caught this bug while updating from version 0.18.0 to 0.18.1. The kwarg `usecols` no longer accepts unicode column labels.\r\n\r\nExample below:\r\n\r\n```python\r\nfrom io import StringIO\r\nimport pandas as pd\r\n\r\ns = u'''AAA,BBB,CCC,DDD\r\n0.056674973,8,True,a\r\n2.613230982,2,False,b\r\n3.568935038,7,False,a\r\n'''\r\nbuff = StringIO(s)\r\nprint pd.read_csv(buff, usecols=[u'AAA', u'BBB'])\r\n\r\n>> ValueError: The elements of 'usecols' must either be all strings or all integers\r\n```\r\n\r\nI note that 0.18.1 introduced the requirement that `usecols` be all string or all ints. This makes sense but it looks like the implementation also throws away unicode strings.\r\n\r\n"""
13214,155493580,fmarczin,jreback,2016-05-18 12:52:14,2016-05-19 13:35:41,2016-05-19 13:14:46,closed,,0.19.0,5,Bug;IO JSON;Unicode,https://api.github.com/repos/pydata/pandas/issues/13214,b'BUG: Fix #13213 json_normalize() and non-ascii characters in keys',b' - [ ] closes #13213 \r\n - [ ] tests added / passed\r\n - [ ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n'
13213,155491787,fmarczin,jreback,2016-05-18 12:43:27,2016-05-19 13:14:51,2016-05-19 13:14:46,closed,,0.19.0,0,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/13213,"b""json_normalize() can't deal with non-ascii characters in unicode keys""","b'Example code:\r\n\r\n```python\r\nimport pandas\r\nimport json\r\n\r\ntestjson = u\'\'\'\r\n[{""nicde"":0,""sub"":{""A"":1, ""B"":2}},\r\n {""nicde"":1,""sub"":{""A"":3, ""B"":4}}]\r\n \'\'\'.encode(\'utf8\')\r\npd.io.json.json_normalize(json.loads(testjson))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""...lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2885, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File ""<ipython-input-12-f866f9c7ec7c>"", line 5, in <module>\r\n    pd.io.json.json_normalize(json.loads(testjson))\r\n  File "".../lib/python2.7/site-packages/pandas/io/json.py"", line 715, in json_normalize\r\n    data = nested_to_record(data)\r\n  File "".../lib/python2.7/site-packages/pandas/io/json.py"", line 617, in nested_to_record\r\n    newkey = str(k)\r\nUnicodeEncodeError: \'ascii\' codec can\'t encode character u\'\\xdc\' in position 0: ordinal not in range(128)\r\n\r\n```\r\n\r\nExpected output\r\n\r\n```\r\n   sub.A  sub.B  nicde\r\n0      1      2        0\r\n1      3      4        1\r\n```\r\n\r\nThe cause are probably\r\nhttps://github.com/pydata/pandas/blob/master/pandas/io/json.py#L618\r\nand https://github.com/pydata/pandas/blob/master/pandas/io/json.py#L620\r\n\r\nThose lines seemingly were introduced to deal with numeric types, but fail when `k` is a Unicode object containing non-ascii characters.\r\n\r\nIt seems to be the same bug in principle as https://github.com/pydata/pandas/issues/13101\r\n'"
13209,155367899,pijucha,jreback,2016-05-17 21:36:03,2016-05-24 00:32:35,2016-05-23 20:45:45,closed,,0.19.0,12,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/13209,"b""BUG: Fix #13149 and ENH: 'copy' param in Index.astype()""","b"" - [x] closes #13149\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n**Update**\r\n\r\n1. Float64Index.astype(int) raises ValueError if a NaN is present. \r\nPreviously, it converted NaN's to the smallest negative integer.\r\n\r\n2. TimedeltaIndex.astype(int) and DatetimeIndex.astype(int) return.\r\nInt64Index, which is consistent with behavior of other Indexes.\r\nPreviously, they returned a numpy.array of ints.\r\n\r\n3. Added:\r\n  - bool parameter 'copy' to Index.astype()\r\n  - shared doc string to .astype()\r\n  - tests on .astype() (consolidated and added new)\r\n  - bool parameter 'copy' to Categorical.astype()\r\n\r\n4. Fixed core.common.is_timedelta64_ns_dtype().\r\n\r\n5. Set a default NaT representation to a string type in a parameter of DatetimeIndex._format_native_types().\r\nPreviously, it produced a unicode u'NaT' in Python2.\r\n"""
13204,155284017,toasteez,jreback,2016-05-17 15:07:21,2016-07-03 23:28:31,2016-07-03 23:28:31,closed,,0.19.0,4,Bug;Difficulty Intermediate;Effort Medium;Groupby,https://api.github.com/repos/pydata/pandas/issues/13204,b'groupby with index = False returns NANs when column is categorical. ',"b""Please see stackoverflow for example of issue\r\n\r\nhttp://stackoverflow.com/questions/37279260/why-doesnt-pandas-allow-a-categorical-column-to-be-used-in-groupby?noredirect=1#comment62084780_37279260\r\n\r\n    >>> pd.__version__\r\n    '0.18.1'\r\n    >>> \r\n\r\n    # import the pandas module\r\n    import pandas as pd\r\n\r\n    # Create an example dataframe\r\n    raw_data = {'Date': ['2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13','2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13', '2016-05-13'],\r\n        'Portfolio': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B','B', 'B', 'B', 'C', 'C', 'C', 'C', 'C', 'C'],\r\n        'Duration': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3],\r\n        'Yield': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1],}\r\n\r\n    df = pd.DataFrame(raw_data, columns = ['Date', 'Portfolio', 'Duration', 'Yield'])\r\n\r\n    df['Portfolio'] = pd.Categorical(df['Portfolio'],['C', 'B', 'A'])\r\n    df=df.sort_values('Portfolio')\r\n\r\n    dfs = df.groupby(['Date','Portfolio'], as_index =False).sum()\r\n\r\n    print(dfs)\r\n\r\n                            Date    Portfolio   Duration   Yield\r\n    Date        Portfolio               \r\n    13/05/2016  C           NaN     NaN         NaN        NaN\r\n                B           NaN     NaN         NaN        NaN\r\n                A           NaN     NaN         NaN        NaN"""
13201,155250913,sinhrks,jreback,2016-05-17 12:48:40,2016-05-18 13:23:14,2016-05-18 13:22:36,closed,,0.19.0,2,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/13201,b'BUG: Sparse creation with object dtype may raise TypeError',b'closes #11633 \r\ncloses #11856\r\n\r\nAdded basic tests for ``groupby`` and ``pivot_table``.\r\n\r\n'
13200,155249792,sinhrks,jreback,2016-05-17 12:43:12,2016-05-17 17:21:02,2016-05-17 17:21:02,closed,,0.19.0,2,Bug;Numeric;Period,https://api.github.com/repos/pydata/pandas/issues/13200,b'BUG: Period and Series/Index comparison raises TypeError',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n``Period`` and ``Series/Index`` comparison raises ``TypeError`` if ``Period`` is on left hand side.\r\n\r\n```\r\n# OK\r\npd.Series([pd.Period('2011-01-01', freq='D')]) > pd.Period('2011-01-01', freq='D')\r\n# 0    False\r\n# dtype: bool\r\n\r\n# NG\r\npd.Period('2011-01-01', freq='D') > pd.Series([pd.Period('2011-01-01', freq='D')])\r\n# TypeError: Cannot compare type 'Period' with type 'Series'\r\n```"""
13192,154937827,chris-b1,jreback,2016-05-15 23:23:09,2016-05-18 13:19:53,2016-05-18 13:19:39,closed,,0.19.0,3,Bug;Groupby;Performance;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13192,b'PERF: DataFrame transform',"b' - [x] closes #12737, \r\ncloses #13191\r\n - [ ] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n```\r\n    before     after       ratio\r\n  [2de2884 ] [4b352d9 ]\r\n-  164.84ms     1.73ms      0.01  groupby.groupby_transform_dataframe.time_groupby_transform_dataframe\r\n     4.44ms     3.77ms      0.85  groupby.groupby_transform_series.time_groupby_transform_series\r\n     6.40ms     4.76ms      0.74  groupby.groupby_transform_series2.time_groupby_transform_series2\r\n```'"
13191,154932122,chris-b1,jreback,2016-05-15 21:43:14,2016-05-18 13:19:39,2016-05-18 13:19:39,closed,,0.19.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/13191,b'BUG: Series _transform_fast fails for datetime with null groups',"b""xref #10972 - but not the same issue.  I've got a fix for this + #12737 coming.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\nIn [20]: df = pd.DataFrame({'grouping':[np.nan,1,1,3], \r\n                            'v':[1.1, 2.1, 3.1, 4.5], \r\n                            'd':pd.date_range('2014-1-1','2014-1-4')})\r\n\r\nIn [21]: df.groupby('grouping')['d'].transform('first')\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-21-cb1ed73aabc3> in <module>()\r\n----> 1 df.groupby('grouping')['d'].transform('first')\r\n\r\nC:\\Users\\Chris\\Anaconda\\lib\\site-packages\\pandas\\core\\groupby.pyc in transform(self, func, *args, **kwargs)\r\n   2738                 # cythonized aggregation and merge\r\n   2739                 return self._transform_fast(\r\n-> 2740                     lambda: getattr(self, func)(*args, **kwargs))\r\n   2741\r\n   2742         # reg transform\r\n\r\nC:\\Users\\Chris\\Anaconda\\lib\\site-packages\\pandas\\core\\groupby.pyc in _transform_fast(self, func)\r\n   2781         out = func().values[ids]\r\n   2782         if not mask.all():\r\n-> 2783             out = np.where(mask, out, np.nan)\r\n   2784\r\n   2785         obs = np.zeros(ngroup, dtype='bool')\r\n\r\nTypeError: invalid type promotion\r\n```\r\n\r\n\r\n\r\n"""
13183,154909839,jreback,jreback,2016-05-15 13:54:32,2016-05-20 14:06:49,2016-05-20 14:06:49,closed,,0.19.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13183,"b'BUG: Bug in .to_datetime() when passing integers or floats, no unit and errors=coerce'",b'closes #13180'
13180,154884950,lvphj,jreback,2016-05-15 01:39:28,2016-05-20 14:06:49,2016-05-20 14:06:49,closed,,0.19.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13180,"b""Converting float64 values to datetime64[ns] format using pd.to_datetime results in NaT if errors='coerce'""","b""#### Code Sample, a copy-pastable example if possible\r\n\r\nA Pandas dataframe contains a date variable that is formatted as float64 as follows:\r\n```\r\ntempDF = pd.DataFrame({'group': ['a','b'],\r\n                       'date': [1.434692e+18,1.432766e+18]})\r\n\r\nprint(tempDF)\r\n```\r\nWhich appears as:\r\n```\r\n           date group\r\n0  1.434692e+18     a\r\n1  1.432766e+18     b\r\n```\r\nThe date variables can be formatted to datetime64[ns] as follows:\r\n```\r\ntempDF['date'] = pd.to_datetime(tempDF['date'],format=None)\r\nprint(tempDF)\r\n```\r\nWhich works as expected:\r\n```\r\n                 date group\r\n0 2015-06-19 05:33:20     a\r\n1 2015-05-27 22:33:20     b\r\n```\r\nHowever, if errors='coerce' is included in the do_datetime() function, the dates are returned as NaT.\r\n```\r\ntempDF['date'] = pd.to_datetime(tempDF['date'],format=None,errors='coerce')\r\nprint(tempDF)\r\n```\r\nWhich produces the following:\r\n```\r\n  date group\r\n0  NaT     a\r\n1  NaT     b\r\n```\r\nThis issue was identified as a result of a reported bug (#12821 ) where datetime64[ns] columns are returned as float64 following agg() function where all dates in a group are NaT. Converting columns back to datetime64[ns] format is necessary. However, identifying valid numeric values as errors when errors='coerce' and returning NaT values results in unnecessary data-loss.\r\n\r\n#### Expected Output\r\n```\r\n                 date group\r\n0 2015-06-19 05:33:20     a\r\n1 2015-05-27 22:33:20     b\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 1.5.6\r\nsetuptools: 20.1.1\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.3.2\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n```"""
13176,154828263,gliptak,jreback,2016-05-14 01:55:57,2016-05-26 12:44:28,2016-05-26 12:43:40,closed,,0.19.0,14,Bug;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13176,b'Correct bool to datetime conversion (behaviour based on int/float processing)',b' - [x] closes #11853\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry'
13175,154827883,jreback,jreback,2016-05-14 01:47:09,2016-05-14 12:01:13,2016-05-14 12:01:13,closed,,0.19.0,0,Bug;Groupby;Resample,https://api.github.com/repos/pydata/pandas/issues/13175,b'BUG: Bug in .groupby(..).resample(..) when the same object is called multiple times',b'closes #13174'
13174,154825956,starplanet,jreback,2016-05-14 01:07:46,2016-05-14 12:01:13,2016-05-14 12:01:13,closed,,0.19.0,1,Bug;Groupby;Resample,https://api.github.com/repos/pydata/pandas/issues/13174,b'resample strange behavior on get_item',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nimport pandas as pd\r\ndata = [{'id': 1, 'buyer': 'A'}, {'id': 2, 'buyer': 'B'}]\r\ndf = pd.DataFrame(data, index=pd.date_range('2016-01-01', periods=2))\r\nresampler = df.groupby('id').resample('1D')\r\n```\r\nIf I then run `resampler['buyer'].count()` first time, it will output normally:\r\n```\r\nid\r\n1   2016-01-01    1\r\n2   2016-01-02    1\r\nName: buyer, dtype: int64\r\n```\r\nIf I then run `resampler['buyer'].count()` again, it will report error:\r\n```\r\n/usr/local/bin/ipython:1: FutureWarning: .resample() is now a deferred operation\r\nuse .resample(...).mean() instead of .resample(...)\r\n  #!/usr/local/opt/python/bin/python2.7\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-30-5714547cf98c> in <module>()\r\n----> 1 resampler['buyer'].count()\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/tseries/resample.pyc in __getitem__(self, key)\r\n    179             # compat for deprecated\r\n    180             if isinstance(self.obj, com.ABCSeries):\r\n--> 181                 return self._deprecated()[key]\r\n    182\r\n    183             raise\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc in __getitem__(self, key)\r\n   1995             return self._getitem_multilevel(key)\r\n   1996         else:\r\n-> 1997             return self._getitem_column(key)\r\n   1998\r\n   1999     def _getitem_column(self, key):\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc in _getitem_column(self, key)\r\n   2002         # get column\r\n   2003         if self.columns.is_unique:\r\n-> 2004             return self._get_item_cache(key)\r\n   2005\r\n   2006         # duplicate columns & possible reduce dimensionality\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc in _get_item_cache(self, item)\r\n   1348         res = cache.get(item)\r\n   1349         if res is None:\r\n-> 1350             values = self._data.get(item)\r\n   1351             res = self._box_item_values(item, values)\r\n   1352             cache[item] = res\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/core/internals.pyc in get(self, item, fastpath)\r\n   3288\r\n   3289             if not isnull(item):\r\n-> 3290                 loc = self.items.get_loc(item)\r\n   3291             else:\r\n   3292                 indexer = np.arange(len(self.items))[isnull(self.items)]\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/indexes/base.pyc in get_loc(self, key, method, tolerance)\r\n   1945                 return self._engine.get_loc(key)\r\n   1946             except KeyError:\r\n-> 1947                 return self._engine.get_loc(self._maybe_cast_indexer(key))\r\n   1948\r\n   1949         indexer = self.get_indexer([key], method=method, tolerance=tolerance)\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)()\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)()\r\n\r\nKeyError: 'buyer'\r\n```\r\n\r\nIf I run `df.groupby('id').resample('1D')['buyer'].count()` instead of `resampler['buyer'].count()`, the problem will not appear.\r\n\r\nWhat's the problem?\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: zh_CN.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 19.4\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.2.0\r\nsphinx: 1.3.5\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None"""
13170,154719937,jreback,jreback,2016-05-13 14:07:22,2016-05-27 12:41:32,2016-05-27 12:41:32,closed,,0.19.0,2,Bug;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/13170,b'BUG: preserve join keys dtype',"b'- closes #8596, preserve join keys dtype\r\n- adds ``Index.where`` method for all Index types (like ``np.where/Series.where``), but preserves dtypes'"
13163,154601105,sinhrks,jreback,2016-05-12 23:08:36,2016-05-13 13:21:58,2016-05-13 13:21:47,closed,,0.19.0,2,Bug;Indexing;Output-Formatting;Sparse,https://api.github.com/repos/pydata/pandas/issues/13163,b'BUG: Misc fixes for SparseSeries indexing with MI',"b"" - [x] closes #13144\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixed following bugs. These makes ``MultiIndex`` repr work.\r\n\r\n```\r\norig = pd.Series([1, np.nan, np.nan, 3, np.nan], index=idx)\r\nsparse = orig.to_sparse()\r\nsparse['A']    \r\n# IndexError: Out of bounds access\r\n```\r\n\r\n```\r\n# the result must have MultiIndex\r\nsparse.loc['A':]\r\n# (A, 0)    1.0\r\n# (A, 1)    NaN\r\n# (B, 0)    NaN\r\n# (C, 0)    3.0\r\n# (C, 1)    NaN\r\n# dtype: float64\r\n# BlockIndex\r\n# Block locations: array([0, 3], dtype=int32)\r\n# Block lengths: array([1, 1], dtype=int32)\r\n```\r\n\r\n"""
13156,154485834,sinhrks,jreback,2016-05-12 13:44:25,2016-05-13 13:25:05,2016-05-13 13:24:58,closed,,0.19.0,2,Bug;Enhancement;Strings,https://api.github.com/repos/pydata/pandas/issues/13156,"b""ENH/BUG: str.extractall doesn't support index""","b' - [x] closes #10008\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n```\r\n# same output as pd.Series([""a1a2"", ""b1"", ""c1""]).str.extractall(""[ab](?P<digit>\\d)"")\r\nidx = pd.Index([""a1a2"", ""b1"", ""c1""])\r\nidx.str.extractall(""[ab](?P<digit>\\d)"")\r\n#         digit\r\n#   match      \r\n# 0 0         1\r\n#   1         2\r\n# 1 0         1\r\n```\r\n\r\n**NOTE** Also fixed a bug ``Series.str.extractall`` raises ``ValueError`` if its ``Index`` is ``str`` with 2 or more characters.\r\n\r\n```\r\ns = pd.Series([""a1a2"", ""b1"", ""c1""], index=pd.Index([""xx"", ""yy"", ""zz""]))\r\ns.str.extractall(""[ab](?P<digit>\\d)"")\r\n# ValueError: Length of names must match number of levels in MultiIndex.\r\n```'"
13149,154385124,ch41rmn,jreback,2016-05-12 02:42:59,2016-05-23 20:45:45,2016-05-23 20:45:45,closed,,Next Major Release,6,Bug;Difficulty Novice;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/13149,"b""NaNs in Float64Index are converted to silly integers using index.astype('int')""","b""#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\n>>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\r\n>>> df.index = [None, 1]\r\n>>> df\r\n      a  b\r\nNaN   1  3\r\n 1.0  2  4\r\n>>> df.index = df.index.astype('int')\r\n>>> df\r\n                      a  b\r\n-9223372036854775808  1  3\r\n 1                    2  4\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.1.13-100.fc21.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_AU.utf8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 20.2.2\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nxarray: 0.7.2\r\nIPython: 4.2.0\r\nsphinx: 1.3.5\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.5.2\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n```"""
13146,154344513,msure,jreback,2016-05-11 21:14:02,2016-05-11 21:37:09,2016-05-11 21:36:55,closed,,,1,Bug;Duplicate;Numeric,https://api.github.com/repos/pydata/pandas/issues/13146,b'describe() returns RuntimeWarning: Invalid value encountered in median   RuntimeWarning',"b""I just upgraded to 18.1 w/ conda. I started noticing this problem in some notebooks I created before the upgrade but recently revisited for further analysis.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\ndf = pd.DataFrame({'task_complete':['success','success','fail','fail','success','fail','success'],\r\n    'value':[np.nan,4.5,5.7,3.0,np.nan,6.7,3.78]})\r\n```\r\n\r\n`df.value.describe()` returns a RuntimeWarning from numpy, which then gives this unexpected result for the quantiles:\r\n```python\r\nIn [5]: df.value.describe()\r\n/Users/adrianpalacios/anaconda/lib/python3.4/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median\r\n  RuntimeWarning)\r\nOut[5]:\r\ncount    5.000000\r\nmean     4.736000\r\nstd      1.480703\r\nmin      3.000000\r\n25%           NaN\r\n50%           NaN\r\n75%           NaN\r\nmax      6.700000\r\nName: value, dtype: float64\r\n```\r\n\r\n#### Expected Output\r\nI got this using a different conda environment that has not been upgraded to latest pandas version:\r\n```python\r\nIn [5]: df.value.describe()\r\nOut[5]:\r\ncount    5.000000\r\nmean     4.736000\r\nstd      1.480703\r\nmin      3.000000\r\n25%      3.780000\r\n50%      4.500000\r\n75%      5.700000\r\nmax      6.700000\r\nName: value, dtype: float64\r\n```\r\n\r\nDropping the NaN's works in pandas 18.1:\r\n```python\r\nIn [9]: df.value.dropna().describe()\r\nOut[9]:\r\ncount    5.000000\r\nmean     4.736000\r\nstd      1.480703\r\nmin      3.000000\r\n25%      3.780000\r\n50%      4.500000\r\n75%      5.700000\r\nmax      6.700000\r\nName: value, dtype: float64\r\n```\r\n\r\nHowever, this work-around is not a great option when multiple columns w/ NaNs are present:\r\n```python\r\ndf2 = pd.DataFrame({'task_complete':['success','success','fail','fail','success','fail','success'],\r\n    'value':[np.nan,4.5,5.7,3.0,np.nan,6.7,3.78],\r\n    'more_values':[8.2,np.nan,np.nan,np.nan,9.4,np.nan,np.nan]\r\n})\r\n\r\nIn [17]: df2[['value','more_values']].dropna().describe()\r\nOut[17]:\r\n       value  more_values\r\ncount    0.0          0.0\r\nmean     NaN          NaN\r\nstd      NaN          NaN\r\nmin      NaN          NaN\r\n25%      NaN          NaN\r\n50%      NaN          NaN\r\n75%      NaN          NaN\r\nmax      NaN          NaN\r\n```\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.4.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.2.2\r\nCython: 0.22.1\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.0\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.3\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: 0.9.1\r\napiclient: None\r\nsqlalchemy: 1.0.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.38.0\r\npandas_datareader: None\r\n\r\n"""
13144,154328251,bryanwweber,jreback,2016-05-11 19:53:31,2016-05-13 13:21:47,2016-05-13 13:21:47,closed,,0.19.0,4,Bug;Difficulty Intermediate;Effort Low;Output-Formatting;Sparse,https://api.github.com/repos/pydata/pandas/issues/13144,b'SparseSeries throws a ValueError exception when printing large arrays',"b'#### Code Sample, a copy-pastable example if possible\r\n    import scipy.sparse as sps\r\n    import pandas as pd\r\n    A = sps.rand(350, 18)\r\n    ss = pd.SparseSeries.from_coo(A)\r\n    print(ss)\r\n\r\n    A = sps.rand(350, 17)\r\n    ss = pd.SparseSeries.from_coo(A)\r\n    print(ss)\r\n\r\n#### Expected Output\r\nThe expected output is that both cases print the SparseSeries. However, only the second case is printed properly. The first case throws the following exception:\r\n\r\n    Traceback (most recent call last):\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/series.py"", line 420, in _get_values\r\n        fastpath=True).__finalize__(self)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/series.py"", line 222, in __init__\r\n        self.index = index\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py"", line 2685, in __setattr__\r\n        return object.__setattr__(self, name, value)\r\n      File ""pandas/src/properties.pyx"", line 65, in pandas.lib.AxisProperty.__set__ (pandas/lib.c:44748)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/series.py"", line 274, in _set_axis\r\n        labels = _ensure_index(labels)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py"", line 3409, in _ensure_index\r\n        return Index(index_like)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py"", line 268, in __new__\r\n        cls._scalar_data_error(data)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py"", line 483, in _scalar_data_error\r\n        repr(data)))\r\n    TypeError: Index(...) must be called with a collection of some kind, None was passed\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/base.py"", line 46, in __str__\r\n        return self.__unicode__()\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/series.py"", line 306, in __unicode__\r\n        series_rep = Series.__unicode__(self)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/series.py"", line 984, in __unicode__\r\n        max_rows=max_rows)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/series.py"", line 1025, in to_string\r\n        dtype=dtype, name=name, max_rows=max_rows)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/series.py"", line 1052, in _get_repr\r\n        max_rows=max_rows)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/formats/format.py"", line 145, in __init__\r\n        self._chk_truncate()\r\n        File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/formats/format.py"", line 158, in _chk_truncate\r\n       series = concat((series.iloc[:row_num],\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1296, in __getitem__\r\n        return self._getitem_axis(key, axis=0)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1587, in _getitem_axis\r\n        return self._get_slice_axis(key, axis=axis)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1579, in _get_slice_axis\r\n        return self._slice(slice_obj, axis=axis, kind=\'iloc\')\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py"", line 99, in _slice\r\n        return self.obj._slice(obj, axis=axis, kind=kind)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/core/series.py"", line 578, in _slice\r\n        return self._get_values(slobj)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/series.py"", line 422, in _get_values\r\n        return self[indexer]\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/series.py"", line 396, in __getitem__\r\n        return self._get_val_at(self.index.get_loc(key))\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/series.py"", line 392, in _get_val_at\r\n        return self.block.values._get_val_at(loc)\r\n      File ""/Users/bryan/anaconda3/lib/python3.5/site-packages/pandas/sparse/array.py"", line 308, in _get_val_at\r\n        if loc < 0:\r\n    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n    >>> pd.show_versions()\r\n\r\n    INSTALLED VERSIONS\r\n    ------------------\r\n    commit: None\r\n    python: 3.5.1.final.0\r\n    python-bits: 64\r\n    OS: Darwin\r\n    OS-release: 15.4.0\r\n    machine: x86_64\r\n    processor: i386\r\n    byteorder: little\r\n    LC_ALL: None\r\n    LANG: en_US.UTF-8\r\n\r\n    pandas: 0.18.1\r\n    nose: None\r\n    pip: 8.1.1\r\n    setuptools: 20.7.0\r\n    Cython: 0.24\r\n    numpy: 1.11.0\r\n    scipy: 0.17.0\r\n    statsmodels: None\r\n    xarray: None\r\n    IPython: 4.2.0\r\n    sphinx: 1.4.1\r\n    patsy: None\r\n    dateutil: 2.5.2\r\n    pytz: 2016.3\r\n    blosc: None\r\n    bottleneck: None\r\n    tables: 3.2.2\r\n    numexpr: 2.5.2\r\n    matplotlib: 1.5.1\r\n    openpyxl: 2.3.2\r\n    xlrd: None\r\n    xlwt: None\r\n    xlsxwriter: None\r\n    lxml: None\r\n    bs4: None\r\n    html5lib: None\r\n    httplib2: 0.9.2\r\n    apiclient: None\r\n    sqlalchemy: None\r\n    pymysql: None\r\n    psycopg2: None\r\n    jinja2: 2.8\r\n    boto: None\r\n    pandas_datareader: None'"
13132,154138676,ghost,jreback,2016-05-11 00:56:27,2016-05-31 15:20:04,2016-05-31 15:20:04,closed,,,3,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/13132,b'pd.DataFrame.describe percentile string precision',b' - [ ] closes #13104 \r\n - [ ] tests added / passed\r\n - [ ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew pretty format returns based on decimalization of input and not just fixed to a single decimal precision\r\n\r\n'
13131,154131666,tacaswell,jreback,2016-05-10 23:50:50,2016-05-21 15:57:56,2016-05-20 14:34:19,closed,,0.19.0,8,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/13131,b'FIX: revert part of #11770',b'Reverts part of:\r\n\r\nhttps://github.com/pydata/pandas/pull/11770\r\n\r\nCloses:\r\n\r\nhttps://github.com/matplotlib/matplotlib/issues/6365\r\n\r\nThe mistake in #11770 was missing that pandas had a 1/us not 1/s\r\nscaled bucket.'
13124,153922335,yaduart,jreback,2016-05-10 04:37:39,2016-05-11 23:24:40,2016-05-11 22:18:27,closed,,0.19.0,10,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/13124,b'BUG: Added checks for NaN in __call__ of EngFormatter',b' - [x] closes #11981 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nCloses #11981\r\n\r\nUpdated the v0.18.2 document\r\n\r\nIn the function `__call__` of EngFormatter check if decimal input is NaN.'
13117,153670017,kawochen,jreback,2016-05-08 18:22:44,2016-05-14 12:04:34,2016-05-14 12:04:34,closed,,0.19.0,4,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/13117,b'BUG: GH12896 where extra elements are returned in MultiIndex slicing',b' - [x] closes #12896\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nIs this the right way to do this?\r\n\r\ncc @MaximilianR '
13115,153658327,brandys11,jreback,2016-05-08 14:18:28,2016-06-06 23:20:51,2016-06-06 23:20:34,closed,,0.19.0,10,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/13115,"b'[BUG] Reading multiindex, incorrectly names columns without name.'",b' - [x] closes #12453\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixed reading and writing of Multiindex. \r\nIn a situation where Multiindex looked like this:\r\n\r\n| One | Two |\r\n| ------ | ----- |\r\n| X | |\r\n\r\nit was changed to: \r\n\r\n| One | Two |\r\n| ------ | ----- |\r\n| X | X |'
13114,153628954,gliptak,jreback,2016-05-08 01:33:19,2016-05-13 13:19:49,2016-05-13 13:19:16,closed,,0.19.0,18,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/13114,b'Correct KeyError from matplotlib when processing Series yerr',b' - [x] closes #11858\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
13113,153626538,MaximilianR,jreback,2016-05-08 00:33:31,2016-05-08 00:37:47,2016-05-08 00:37:14,closed,,No action,1,Bug;Duplicate;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/13113,b'BUG: Large MultiIndex-ed series fails on slicing',"b""With a large MultiIndex-ed series, slicing doesn't fully slice on a 2nd-level.\r\n\r\nI've been struggling with this for a while, any ideas greatly appreciated...\r\n\r\n```python\r\nIn [135]: ints = (pd.np.random.rand(150000)*1e7).round().astype('int')\r\n\r\nIn [136]: index=pd.MultiIndex.from_arrays([list('a'*50000 + 'b'*50000+'c'*50000), ints])\r\n\r\nIn [137]: series=pd.Series(np.random.rand(150000), index=index).sort_index()\r\n\r\nIn [138]: series\r\nOut[138]: \r\na  66         0.763398\r\n   171        0.886328\r\n...\r\n   9999825    0.329101\r\n   9999933    0.463330\r\ndtype: float64\r\n\r\nIn [139]: series.loc[(slice(None), slice(5000000))]\r\nOut[139]: \r\na  66         0.763398\r\n   171        0.886328\r\n   186        0.315654\r\n...\r\n   9720405    0.712525\r\n   9925259    0.016771\r\n   9978827    0.555781\r\ndtype: float64\r\n```\r\n\r\n... the slice clearly has values both above and below 5,000,000. It does do _some_ slicing though, just not fully.\r\n\r\nOr am I making a rookie mistake? \r\n\r\npandas: 0.18.1\r\n"""
13104,153509175,bla1089,jreback,2016-05-06 18:19:22,2016-05-31 14:12:51,2016-05-31 14:12:51,closed,,0.19.0,36,Bug;Difficulty Novice;Effort Low;Indexing;Numeric;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/13104,b'pd.DataFrame.describe percentile string precision',"b'When using `pd.DataFrame.describe`, if your percentiles are different only at the 4th decimal place, a `ValueError` is thrown because the the percentiles that vary at the 4th decimal place become the same value.\r\n\r\n```\r\nIn [1]: s = Series(np.random.randn(10))\r\n\r\nIn [2]: s.describe()\r\nOut[2]: \r\ncount    10.000000\r\nmean      0.291571\r\nstd       1.057143\r\nmin      -1.453547\r\n25%      -0.614614\r\n50%       0.637435\r\n75%       0.968905\r\nmax       1.823964\r\ndtype: float64\r\n\r\nIn [3]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\r\nOut[3]: \r\ncount     10.000000\r\nmean       0.291571\r\nstd        1.057143\r\nmin       -1.453547\r\n0.0%      -1.453107\r\n0.1%      -1.451348\r\n0.1%      -1.449149\r\n50%        0.637435\r\n99.9%      1.817201\r\n100.0%     1.820583\r\n100.0%     1.823288\r\nmax        1.823964\r\ndtype: float64\r\n```'"
13103,153470582,erbian,jreback,2016-05-06 15:00:40,2016-05-06 15:07:33,2016-05-06 15:07:09,closed,,No action,1,Bug;Groupby;Usage Question,https://api.github.com/repos/pydata/pandas/issues/13103,b'.transform not respecting axis=1 in combine stage of group-apply-combine',"b'`.transform` does not appear to be respecting the axis argument of the groupby in the combine stage.  The code below yields:\r\n`ValueError: Length mismatch: Expected axis has 4 elements, new values have 5 elements`\r\n\r\n```\r\n#### Code Sample, a copy-pastable example if possible\r\nimport pandas as pd\r\ndf = pd.DataFrame(pd.np.arange(20).reshape(4,5))\r\ndf.groupby(lambda x: x <= 3, axis=1).transform(lambda x: x)\r\n\r\n#### Expected Output\r\n    0   1   2   3   4\r\n0   0   1   2   3   4\r\n1   5   6   7   8   9\r\n2  10  11  12  13  14\r\n3  15  16  17  18  19\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-74-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 1.5.4\r\nsetuptools: 20.1.1\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.1.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: None\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.3\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: None\r\nlxml: 3.5.0\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```\r\n'"
13098,153298994,gieseke,jreback,2016-05-05 18:56:45,2016-05-12 13:12:58,2016-05-12 13:12:58,closed,,0.19.0,2,Bug;Difficulty Intermediate;Effort Low;Numeric;Regression,https://api.github.com/repos/pydata/pandas/issues/13098,b'REGRP: Series.quantile returns NaN',"b'#### Code Sample, a copy-pastable example if possible\r\n```python\r\nimport pandas as pd\r\nimport numpy\r\ns = pd.Series([1, 2, 3, 4, numpy.nan])\r\ns.quantile(0.5)\r\nnan\r\n```\r\n\r\n#### Expected Output\r\nI would expect 2.5 as output (as with version 0.17.1).\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-85-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: de_DE.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 1.5.4\r\nsetuptools: 2.2\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n'"
13079,152919578,MaximilianR,jreback,2016-05-04 02:52:47,2016-05-21 14:53:09,2016-05-21 14:12:53,closed,,0.19.0,17,Bug;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/13079,b'BUG: Empty PeriodIndex issues',b' - [x] closes #13067 (existing PR)\r\n - [x] tests added / passed\r\n - [X] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\ncloses #13067\r\n closes #13212'
13078,152912833,sinhrks,jreback,2016-05-04 01:53:08,2016-07-10 21:02:56,2016-07-10 21:02:56,closed,,0.19.0,3,Bug;Error Reporting;Period;Timedelta,https://api.github.com/repos/pydata/pandas/issues/13078,b'BUG/ERR: DatetimeIndex - Period shows ununderstandable error',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\nSplit from #13071. ``DatetimeIndex`` - ``Period`` is not supported ATM, but errors are not understandable.\r\n\r\n```\r\npd.DatetimeIndex(['2011-01-01', '2011-02-01']) - pd.Period('2011-01-01', freq='D') \r\n# ValueError: Cannot do arithmetic with non-conforming periods\r\n\r\npd.DatetimeIndex(['2011-01-01', '2011-01-02'], freq='D') - pd.Period('2011-01-01', freq='D') \r\n# AttributeError: 'DatetimeIndex' object has no attribute 'ordinal'\r\n```\r\n\r\n#### Expected Output\r\n\r\nshow understandable error message. \r\n\r\nOr it should be supported converting ``Period`` to ``Timestamp``.\r\n\r\n```\r\npd.DatetimeIndex(['2011-01-01', '2011-02-01']) - pd.Period('2011-01-01', freq='D').to_timestamp()\r\n# TimedeltaIndex(['0 days', '31 days'], dtype='timedelta64[ns]', freq=None)\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\non current master\r\n\r\n"""
13072,152838584,sinhrks,jreback,2016-05-03 18:08:54,2016-05-11 22:51:03,2016-05-11 22:51:03,closed,,0.19.0,1,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/13072,b'BUG: Series ops with object dtype may incorrectly fail',b' - [x] closes #13043\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nThis must be after #13069.\r\n\r\n'
13071,152835127,sinhrks,jreback,2016-05-03 17:51:08,2016-05-06 11:50:06,2016-05-06 11:50:06,closed,,0.19.0,15,Bug;Numeric;Period,https://api.github.com/repos/pydata/pandas/issues/13071,b'BUG: PeriodIndex and Period subtraction error',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nSimilar to #5202. ``PeriodIndex`` subtraction raises ``AttributeError`` either side is ``Period`` (scalar).\r\n\r\n```\r\npd.PeriodIndex(['2011-01', '2011-02'], freq='M') - pd.Period('2011-01', freq='M') \r\n# AttributeError: 'PeriodIndex' object has no attribute 'ordinal'\r\n```\r\n\r\n#### Expected\r\n\r\nIf ``Period(NaT)`` is included in either side, result is ``Float64Index`` to hold ``nan``.\r\n\r\n```\r\npd.PeriodIndex(['2011-01', '2011-02'], freq='M') - pd.Period('2011-01', freq='M') \r\n# Int64Index([0, 1], dtype='int64')\r\n\r\npd.PeriodIndex(['2011-01', 'NaT'], freq='M') - pd.Period('2011-01', freq='M') \r\nFloat64Index([0.0, nan], dtype='float64')\r\n```\r\n"""
13069,152828048,sinhrks,jreback,2016-05-03 17:17:24,2016-05-07 18:54:14,2016-05-07 18:53:51,closed,,0.19.0,4,Bug;Numeric;Period,https://api.github.com/repos/pydata/pandas/issues/13069,b'BUG: Addition raises TypeError if Period is on rhs',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n``Period`` addition raises ``TypeError`` if ``Period`` is on right hand side.\r\n\r\n```\r\n1 + pd.Period('2011-01', freq='M')\r\n# TypeError: unsupported operand type(s) for +: 'int' and 'pandas._period.Period'\r\n```\r\n\r\n#### Expected\r\n\r\n```\r\n1 + pd.Period('2011-01', freq='M')\r\n# Period('2011-02', 'M')\r\n```\r\n\r\n"""
13067,152826611,sinhrks,jreback,2016-05-03 17:11:46,2016-05-05 16:14:29,2016-05-05 16:14:28,closed,,0.19.0,9,Bug;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/13067,"b""BUG: Period resample with length=0 doesn't set freq""","b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\ngiven freq is not applied if data is empty.\r\n\r\n```\r\ns = pd.Series(index=pd.PeriodIndex([], freq='D'))\r\ns.resample('M').mean()\r\n# Series([], Freq: D, dtype: float64)\r\n```\r\n\r\n#### Expected\r\n\r\n```\r\ns.resample('M').mean()\r\n# Series([], Freq: M, dtype: float64)\r\n```\r\n"""
13052,152456906,jreback,jreback,2016-05-01 21:44:32,2016-05-03 00:38:26,2016-05-01 22:59:20,closed,,0.18.1,2,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/13052,b'BUG/COMPAT: to_datetime ',"b'xref #11758, fix for bug in #13033\r\n'"
13043,152031752,sinhrks,jreback,2016-04-30 14:02:26,2016-05-11 22:51:09,2016-05-11 22:51:03,closed,,0.19.0,1,Bug;Difficulty Intermediate;Dtypes;Effort Medium;Numeric;Period;Timedelta;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/13043,b'BUG: Series with datetime-like object ops raises TypeError',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\npd.Series([pd.Timestamp('2015-01-01', tz='US/Eastern'),\r\n           pd.Timestamp('2015-01-01', tz='Asia/Tokyo')]) + pd.Timedelta('1 days')\r\n# TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'Timedelta'\r\n\r\npd.Series([pd.Period('2015-01-01', freq='D')]) + pd.Timedelta('1 days')\r\n# TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'Timedelta'\r\n```\r\n\r\n#### Expected Output\r\n\r\nops must be applied to each elem\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\ncurrent master.\r\n\r\n\r\n**NOTE:** This works:\r\n\r\n```\r\npd.Series([pd.Timestamp('2015-01-01', tz='US/Eastern'),\r\n           pd.Timestamp('2015-01-01', tz='Asia/Tokyo')]) + pd.tseries.offsets.Day()\r\n# 0    2015-01-02 00:00:00-05:00\r\n# 1    2015-01-02 00:00:00+09:00\r\n# dtype: object\r\n\r\npd.Series([pd.Period('2015-01-01', freq='D')]) + pd.tseries.offsets.Day()\r\n# 0   2015-01-02\r\n# dtype: object\r\n```\r\n\r\n"""
13039,152012350,sinhrks,jreback,2016-04-30 05:53:31,2016-04-30 14:33:46,2016-04-30 14:32:55,closed,,0.18.1,1,Bug;Missing-data;Reshaping;Testing;Timezones,https://api.github.com/repos/pydata/pandas/issues/13039,b'TST/BUG: DataFrame truncated repr with DatetimeTz and NaT column',b' - [x] closes #12211\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n\r\n'
13038,152011232,sinhrks,jreback,2016-04-30 05:27:16,2016-04-30 14:33:26,2016-04-30 14:25:43,closed,,0.18.1,4,Bug;Testing;Visualization,https://api.github.com/repos/pydata/pandas/issues/13038,b'TST/BUG: Categorical dtypes cause error when attempting stacked bar plot',b' - [x] closes #13019\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n\r\n'
13037,152009942,sinhrks,jreback,2016-04-30 04:59:59,2016-04-30 14:56:46,2016-04-30 14:56:43,closed,,0.18.1,2,Bug;Compat;Indexing,https://api.github.com/repos/pydata/pandas/issues/13037,b'BUG: subclassed .align returns normal DataFrame',"b"" - [x] closes #12983\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n\r\nAlso fixed a bug which ``Series.align(DataFrame)`` raises ``AttributeError``.\r\n```\r\npd.Series([1, 2], index=['a', 'b']).align(pd.DataFrame([1, 2], index=['a', 'c']))\r\n# AttributeError: 'Series' object has no attribute 'columns'\r\n```\r\n\r\n\r\n"""
13030,151946735,sumitbinnani,jorisvandenbossche,2016-04-29 19:06:07,2016-04-29 19:15:09,2016-04-29 19:15:00,closed,,No action,3,Bug;Duplicate;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13030,b'pd.to_datetime giving invalid output',"b""This works:\r\n```\r\n>>> pd.to_datetime(1, unit='D')\r\nTimestamp('1970-01-02 00:00:00')\r\n```\r\n\r\nHowever, the following gives incorrect output:\r\n```\r\n>>> pd.to_datetime(1, unit='D', errors='coerce')\r\nNaT\r\n```\r\n\r\n\r\n#### Details of Configuration:\r\n\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.3\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.5.1\r\npytz: 2016.2\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.39.0\r\n\r\n"""
13028,151787089,sinhrks,jreback,2016-04-29 03:35:37,2016-04-29 13:40:30,2016-04-29 13:40:30,closed,,0.18.1,0,Bug;Resample;Testing,https://api.github.com/repos/pydata/pandas/issues/13028,b'BUG/TST: TimeGrouper has erroneous groups if key length is too short',"b"" - [x] closes #8542\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - ~~whatsnew entry~~\r\n\r\nNo what's new as the fixed timing is unclear.\r\n\r\n**NOTE** Different from ``TimeGrouper`` with single key, it doesn't interpolate missing datetimes between existing keys (same as current behavior of key length >= 3).\r\n\r\n"""
13026,151783543,sinhrks,jreback,2016-04-29 02:48:16,2016-04-29 15:59:42,2016-04-29 15:59:08,closed,,0.18.1,2,Bug;Indexing;Testing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/13026,b'BUG/TST: Calling shift on a DatetimeIndex of length 0 returns an Index in\xa1\xad',"b"" - [x] closes #9903 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - ~~whatsnew entry~~\r\n\r\nNo what's new as the fixed timing is unclear."""
13025,151778642,sinhrks,jreback,2016-04-29 01:42:49,2016-04-29 16:09:20,2016-04-29 16:09:14,closed,,0.18.1,4,2/3 Compat;Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/13025,b'BUG:plot.bar misalignment when width=1',b' - [x] closes #12979\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nEnabled true division. It also prevents differences between py2 / py3.\r\n\r\n'
13014,151555281,ch41rmn,jreback,2016-04-28 05:54:12,2016-05-01 22:25:47,2016-05-01 15:29:52,closed,,0.18.1,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/13014,b'BUG: .isin(...) now coerces sets to lists',"b' - [ ] closes #12988\r\n - [ ] tests added, passed\r\n - [ ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] `comp.isin(values)` now coerces `values` from a set into a list. This was failing when `comp` is a datetime dtype and `values` is a set, where `.isin` will try to run `to_datetime(values)` on the unordered set and fail.\r\n\r\n'"
13006,151268829,rocurley,jreback,2016-04-27 00:15:47,2016-06-03 15:04:18,2016-06-03 15:04:18,closed,,0.19.0,3,Bug;Difficulty Novice;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/13006,b'BUG: Type error when comparing numpy scalar to pandas series',"b""Hi,\r\n\r\nI'm getting strange results when comparing a numpy float to a pandas series. It seems to think that the numpy float is some sort of array...\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nnp.float64(0) < pd.Series([1,2,3],dtype=np.float64)\r\n```\r\n#### Expected Output\r\n```\r\n0    True\r\n1    True\r\n2    True\r\ndtype: bool\r\n```\r\n#### Actual Output\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-0803a1472416> in <module>()\r\n      1 import numpy as np\r\n      2 import pandas as pd\r\n----> 3 np.float64(0) < pd.Series([1,2,3],dtype=np.float64)\r\n        global np.float64 = <type 'numpy.float64'>\r\n        global pd.Series = <class 'pandas.core.series.Series'>\r\n        global dtype = undefined\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/ops.pyc in wrapper(self=0    1.0\r\n1    2.0\r\n2    3.0\r\ndtype: float64, other=array(0.0), axis=None)\r\n    737             return NotImplemented\r\n    738         elif isinstance(other, (np.ndarray, pd.Index)):\r\n--> 739             if len(self) != len(other):\r\n        global len = undefined\r\n        self = 0    1.0\r\n1    2.0\r\n2    3.0\r\ndtype: float64\r\n        other = array(0.0)\r\n    740                 raise ValueError('Lengths must match to compare')\r\n    741             return self._constructor(na_op(self.values, np.asarray(other)),\r\n\r\nTypeError: len() of unsized object\r\n\r\n\r\n```\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.3.0-040300-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 20.3.1\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.8\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n```\r\n"""
12999,151188842,alzmcr,jreback,2016-04-26 17:09:57,2016-04-26 17:23:24,2016-04-26 17:23:24,closed,,No action,1,Bug;Duplicate;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12999,b'read_csv index_col ignores dtype if specified',"b'Hi, I\'m not sure if this is intended but when using the **index_col** parameter in **read_csv** it ignore the input format specified in dtype. It\'s reproducible as following using pandas 0.18.0 and numpy 1.11.0.\r\n\r\n```\r\n>>> from StringIO import StringIO\r\nimport pandas as pd\r\ndf_csv = """"""request_hour,request_date,size\r\n03,2016-04-26,2580954.0\r\n04,2016-04-26,12003662.0\r\n05,2016-04-26,13042624.0\r\n06,2016-04-26,2899309.0\r\n07,2016-04-26,-1.0""""""\r\n\r\n>>> pd.read_csv(StringIO(df_csv), dtype={\'request_hour\': \'string\'}).set_index(\'request_hour\')\r\n\r\n             request_date        size\r\nrequest_hour                         \r\n03             2016-04-26   2580954.0\r\n04             2016-04-26  12003662.0\r\n05             2016-04-26  13042624.0\r\n06             2016-04-26   2899309.0\r\n07             2016-04-26        -1.0\r\n# This is what I would expected as output\r\n\r\n>>> pd.read_csv(StringIO(df_csv), dtype={\'request_hour\': \'string\'}, index_col=0)\r\n\r\n             request_date        size\r\nrequest_hour                         \r\n3              2016-04-26   2580954.0\r\n4              2016-04-26  12003662.0\r\n5              2016-04-26  13042624.0\r\n6              2016-04-26   2899309.0\r\n7              2016-04-26        -1.0\r\n# I\'m surprise that the index has been converted to int\r\n```\r\n\r\nI couldn\'t find any specs on this anywhere, so I wonder if something with the **read_csv** or I\'m doing something wrong. \r\n\r\nThanks!'"
12998,151180455,rsdenijs,jreback,2016-04-26 16:30:27,2016-04-26 16:35:28,2016-04-26 16:34:20,closed,,No action,2,Bug;Categorical;Duplicate;Indexing,https://api.github.com/repos/pydata/pandas/issues/12998,b'Selecting a categorial column returns a DataFrame',"b""I am not 100% sure if this is a bug, but it generates very unexpected behaviour!\r\nEssentially, if columns are of type Categorical, selecting a single one using df[column_name]  a DataFrame is returned, unlike for columns of other types, for which a Series would be returned.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nprint pd.DataFrame([[1,2]], columns=['a','b'])['a'].__class__\r\nprint pd.DataFrame([[1,2]], columns=pd.Categorical(['a','b']))['a'].__class__\r\n\r\n```\r\n\r\nproduces \r\n```\r\n\r\n<class 'pandas.core.series.Series'>  <--- Ok....\r\n<class 'pandas.core.frame.DataFrame'> <----- What?!\r\n```\r\n#### Expected Output\r\n\r\n```\r\n<class 'pandas.core.series.Series'>\r\n<class 'pandas.core.series.Series'>\r\n\r\n```\r\n#### output of ``pd.show_versions()``\r\npandas: 0.18.0"""
12988,151011574,ch41rmn,jreback,2016-04-26 00:15:25,2016-05-01 15:29:52,2016-05-01 15:29:52,closed,,0.18.1,5,Bug;Difficulty Novice;Effort Low;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/12988,b'Series.isin(values) raises ValueError if values is a set',"b""When trying to use `Series.isin(values)` for Timestamps, an exception is raised if `values` is a set.\r\n```\r\nd = {'Dates':[pd.Timestamp('2013-01-02'),\r\n              pd.Timestamp('2013-01-03'),\r\n              pd.Timestamp('2013-01-04')],\r\n     'Num1':[1,2,3],\r\n     'Num2':[-1,-2,-3]}\r\n\r\ndf = pd.DataFrame(data=d)\r\n\r\n>>> df['Dates'].isin({pd.Timestamp('2013-01-04')})\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 0)\r\n\r\n>>> df['Dates'].isin([pd.Timestamp('2013-01-04')])\r\n0    False\r\n1    False\r\n2     True\r\nName: Dates, dtype: bool\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.1.13-100.fc21.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_AU.utf8\r\n\r\npandas: 0.18.0\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 20.2.2\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nxarray: 0.7.2\r\nIPython: 4.1.2\r\nsphinx: 1.3.5\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.5.2\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n```\r\n\r\n"""
12982,150912969,ajenkins-cargometrics,jreback,2016-04-25 16:11:22,2016-04-26 13:23:00,2016-04-26 13:22:55,closed,,0.18.1,3,Bug;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/12982,b'BUG: Preserve timezone in unaligned assignments',"b' - [x] closes #12981 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nA fix for #12981.  When doing a slice assign to a DataFrame where the\r\nright-hand-side was timezone-aware datetime Series which required\r\nrealignment to perform the assignment, the timezone would be stripped\r\nfrom the RHS on assignment.'"
12981,150880761,ajenkins-cargometrics,jreback,2016-04-25 14:31:11,2016-04-26 13:22:55,2016-04-26 13:22:55,closed,,0.18.1,3,Bug;Difficulty Intermediate;Effort Low;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/12981,b'Timezone lost on DataFrame assignments with realignment',"b'Starting from pandas 0.17, certain assignments to DataFrames cause offset-aware datetime columns to be converted to offset-naive columns.  Specifically, it seems that if any data realignment is required when assigning the RHS to a a slice of the DataFrame, then timezone info is lost.  Here\'s an example: \r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport pandas\r\n\r\nprint(""Pandas version:"", pandas.__version__)\r\n\r\nstart = pandas.Timestamp(\'2015-01-01\', tz=\'utc\')\r\ndf = pandas.DataFrame({\'dates\': pandas.date_range(start, periods=3)})\r\n\r\nprint(""Before assignment"")\r\nprint(df[\'dates\'])\r\n\r\n# Shuffle column and reassign, causing RHS to need to be realigned on assignment\r\ndf[\'dates\'] = df[\'dates\'][[1,0,2]]\r\n\r\nprint(""\\nAfter assignment"")\r\nprint(df[\'dates\'])\r\n```\r\n\r\nThe output I\'d expect, which is what I get from pandas 0.16.2, is:\r\n\r\n```\r\nPandas version: 0.16.2\r\nBefore assignment\r\n0    2015-01-01 00:00:00+00:00\r\n1    2015-01-02 00:00:00+00:00\r\n2    2015-01-03 00:00:00+00:00\r\nName: dates, dtype: object\r\n\r\nAfter assignment\r\n0    2015-01-01 00:00:00+00:00\r\n1    2015-01-02 00:00:00+00:00\r\n2    2015-01-03 00:00:00+00:00\r\nName: dates, dtype: object\r\n```\r\n\r\nHowever when I run this with pandas 0.18.0, after the assignment the timezone info is lost:\r\n\r\n```\r\nPandas version: 0.18.0\r\nBefore assignment\r\n0   2015-01-01 00:00:00+00:00\r\n1   2015-01-02 00:00:00+00:00\r\n2   2015-01-03 00:00:00+00:00\r\nName: dates, dtype: datetime64[ns, UTC]\r\n\r\nAfter assignment\r\n0   2015-01-01\r\n1   2015-01-02\r\n2   2015-01-03\r\nName: dates, dtype: datetime64[ns]\r\n```\r\n\r\nIt seems the custom timezone-aware dtype that pandas started using for timezone-aware time series in 0.17.x doesn\'t get correctly propagated in this operation.\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.9.0\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 3.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.5.2\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\n```\r\n'"
12979,150830862,kdebrab,jreback,2016-04-25 11:39:10,2016-04-29 16:09:14,2016-04-29 16:09:14,closed,,0.18.1,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/12979,b'plot.bar misalignment when width=1',"b""Two minor issues with the width optional argument of plot.bar() in Pandas 0.18.0:\r\n\r\n1) Using width=1 (integer) causes misalignment of the x-axis\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame(np.random.random((6, 5)) * 10, index=list('abcdef'), columns=list('ABCDE'))\r\ndf.plot.bar(stacked=True, width=1)\r\n```\r\n![figure_1](https://cloud.githubusercontent.com/assets/1150402/14782442/ade46c52-0ae9-11e6-9b90-729d413861d1.png)\r\n\r\nCompare this with the correct behaviour when defining width as a float:\r\n```python\r\ndf.plot.bar(stacked=True, width=1.)\r\n```\r\n![figure_2](https://cloud.githubusercontent.com/assets/1150402/14782452/b29a70fc-0ae9-11e6-9b72-031fccc5714f.png)\r\n\r\n2) Using width=None causes a TypeError, where I would expect the default behaviour."""
12977,150776083,adneu,jreback,2016-04-25 07:36:38,2016-05-20 14:09:46,2016-05-20 14:09:29,closed,,0.19.0,5,Bug;Groupby;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12977,b'BUG: GH12824 fixed apply() returns different result depending on whet\xa1\xad',"b' - [x] closes #12824 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nIn `_wrap_applied_output` for `NDFrameGroupBy`, if the first result of `apply` is `None`, find the first non-`None` result to determine behavior'"
12975,150696178,hnykda,jreback,2016-04-24 20:17:15,2016-04-25 21:57:21,2016-04-25 21:57:20,closed,,0.18.1,7,Bug;IO HTML,https://api.github.com/repos/pydata/pandas/issues/12975,b'fix for read_html with bs4 failing on table with header and one column',"b' - [x] closes [#9178](https://github.com/pydata/pandas/issues/9178)\r\n - [x] The test is added and passing (while failing before the fix).\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFix as had been proposed in [PR 9194](https://github.com/pydata/pandas/pull/9194#issuecomment-209849563), but this PR was closed because of tests missing. They are added now.\r\n\r\n\r\n'"
12971,150597949,sinhrks,jreback,2016-04-23 21:23:08,2016-04-25 22:00:30,2016-04-25 22:00:23,closed,,0.18.1,2,Bug;Numeric;Sparse,https://api.github.com/repos/pydata/pandas/issues/12971,b'ENH/BUG: Sparse now supports comparison op',"b' - [x] no open issue\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n## on current master\r\n\r\n```\r\npd.SparseArray([1, 2, np.nan]) > 0\r\n# [True, True, nan]\r\n# Fill: nan\r\n# IntIndex\r\n# Indices: array([0, 1], dtype=int32)\r\n\r\n# Expected\r\n# [True, True, False]\r\n```\r\n\r\n```\r\na = pd.SparseArray([1, 2, 0])\r\nb = pd.SparseArray([0, 1, np.nan])\r\na > b\r\n# ValueError: operands could not be broadcast together with shapes (3,) (2,) \r\n```\r\n\r\n'"
12966,150557120,sinhrks,jreback,2016-04-23 13:57:08,2016-04-25 13:42:24,2016-04-25 13:42:20,closed,,0.18.1,5,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/12966,b'BUG: Sparse concat may fill fill_value with NaN',"b"" - [x] closes #9765\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n This also added tests related to #12174 and fixed ``concat(axis=1)`` issue.\r\n\r\n```\r\n# on current master\r\ndense1 = pd.DataFrame({'A': [1, 2, 3, np.nan],\r\n                    'B': [0, 0, 0, 0],\r\n                    'C': [np.nan, np.nan, np.nan, np.nan],\r\n                    'D': [1, 2, 3, 4]})\r\ndense2 = pd.DataFrame({'E': [1, 2, 3, np.nan],\r\n                    'F': [0, 0, 0, 0],\r\n                    'G': [np.nan, np.nan, np.nan, np.nan],\r\n                    'H': [1, 2, 3, 4]})\r\nsparse1 = dense1.to_sparse()\r\nsparse2 = dense2.to_sparse()\r\npd.concat([sparse2, sparse1], axis=1)\r\n# AttributeError: 'int' object has no attribute 'ravel'\r\n```\r\n\r\nOne point to be discussed is the logic for return type. Currently, ``SparseDataFrame`` is returned only when all blocks are all sparse. Because ``SparseDataFrame`` can't work properly if dense block is contained.\r\n\r\nThus, dense and sparse ``concat`` with axis=0 resunts in ``SparseDataFrame``, and axis=1 results in normal ``DataFrame``.\r\n\r\n\r\n"""
12936,149885683,jreback,jreback,2016-04-20 21:04:04,2016-04-20 21:35:21,2016-04-20 21:33:57,closed,,0.18.1,1,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/12936,b'BUG: provide SparseArray creation in a platform independent manner',b'makes creation w/o specifying a dtype choose ``np.int64/float64`` regardless of the platform. This is similar to how Series works. This only is exposed on windows (as the default ndarray creation is np.int32) rather than np.int64\r\n\r\nmaybe *should* use ``pandas.core.series._sanitize_array`` (but needs some tweeks I think).\r\n\r\ncc @sinhrks '
12935,149815976,gfyoung,jreback,2016-04-20 16:14:12,2016-05-23 21:44:31,2016-05-23 21:43:04,closed,,0.19.0,40,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12935,"b'BUG, ENH: Add support for parsing duplicate columns'",b'Introduces `mappings` and `reverse_map` attributes to the parser in `pandas.io.parsers` that allow it to differentiate between duplicate columns that may be present in a file.\r\n\r\nCloses #7160.\r\nCloses #9424.'
12928,149554321,jreback,jreback,2016-04-19 19:12:54,2016-04-20 01:03:18,2016-04-20 01:03:18,closed,,0.18.1,3,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/12928,b'BUG: .asfreq on resample on PeriodIndex/TimedeltaIndex are not',b'closes #12926'
12926,149452813,jorisvandenbossche,jreback,2016-04-19 12:52:21,2016-04-20 01:03:18,2016-04-20 01:03:18,closed,,0.18.1,2,Bug;Resample;Timedelta,https://api.github.com/repos/pydata/pandas/issues/12926,b'BUG: resample().asfreq() looses end period with TimedeltaIndex',"b""```\r\nIn [16]: df=pd.DataFrame(data=[1,3], index=[dt.timedelta(), dt.timedelta(minutes=3)])\r\nIn [17]: df\r\nOut[17]:\r\n          0\r\n00:00:00  1\r\n00:03:00  3\r\n\r\nIn [18]: df.resample('1T').mean()\r\nOut[18]:\r\n            0\r\n00:00:00  1.0\r\n00:01:00  NaN\r\n00:02:00  NaN\r\n00:03:00  3.0\r\n\r\nIn [19]: df.resample('1T').asfreq()\r\nOut[19]:\r\n            0\r\n00:00:00  1.0\r\n00:01:00  NaN\r\n00:02:00  NaN\r\n```\r\n\r\nThe two examples should give the same result. \r\nTried it with a DatetimeIndex, and there I don't see this problem.\r\n\r\ncc @jreback """
12924,149449367,eddiejessup,jreback,2016-04-19 12:40:36,2016-05-19 14:53:00,2016-05-19 14:52:39,closed,,0.19.0,6,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/12924,b'BUG: Fix argument order in call to super',"b"" - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n\r\n`super` should have arguments (type, object), not (object, type).\r\n\r\nI've run `test_fast.sh` using Python 2.7 and it gives [2 failures](https://github.com/pydata/pandas/files/225866/testfail.txt) relating to language locales (I'm using en_GB). Pretty sure these are unrelated to the commit.\r\n"""
12922,149368701,batterseapower,jreback,2016-04-19 07:10:14,2016-06-16 12:32:24,2016-06-16 12:32:24,closed,,0.19.0,1,Bug;Difficulty Intermediate;Effort Medium;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/12922,b'DataFrame.to_csv(quoting=csv.QUOTE_NONNUMERIC) quotes numeric values',"b'#### Failing test\r\n\r\n```\r\ndef test_pandas():\r\n    import tempfile\r\n    import csv\r\n    import pandas as pd\r\n    import numpy as np\r\n\r\n    df = pd.DataFrame.from_dict({\'column\': [1.0, 2.0]})\r\n    assert df[\'column\'].dtype == np.dtype(\'float\')\r\n\r\n    with tempfile.TemporaryFile() as f:\r\n        df.to_csv(f, quoting=csv.QUOTE_NONNUMERIC, index=False)\r\n\r\n        f.seek(0)\r\n        lines = f.read().splitlines()\r\n        assert lines[0] == \'""column""\'\r\n        assert not lines[1].startswith(\'""\') # <--- THIS FAILS\r\n        assert [1, 2] == map(float, lines[1:])\r\n```\r\n\r\nThe issue is that the floats are being output wrapped with quotes, even though I requested QUOTE_NONNUMERIC.\r\n\r\nThe problem is that `pandas.core.internals.FloatBlock.to_native_types` (and by extension `pandas.formats.format.FloatArrayFormatter.get_result_as_array`) unconditionally formats the float array to a str array, which is then passed unchanged to the `csv` module and hence will be wrapped in quotes by that code.\r\n\r\nI\'m not 100% sure but the fix may be to have `FloatBlock.to_native_types` check if quoting is set, and if so to skip using the `FloatArrayFormatter`? I say this because `pandas.indexes.base.Index._format_native_types` already has a special case along these lines. This does seem a bit dirty though!\r\n\r\nHere is an awful monkeypatch that works around the problem:\r\n\r\n```\r\norig_to_native_types = pd.core.internals.FloatBlock.to_native_types\r\ndef to_native_types(self, *args, **kwargs):\r\n    if kwargs.get(\'quoting\'):\r\n        values = self.values\r\n        slicer = kwargs.get(\'slicer\')\r\n        if slicer is not None:\r\n            values = values[:, slicer]\r\n\r\n        return values\r\n\r\n    res = orig_to_native_types(self, *args, **kwargs)\r\n    print \'FloatBlock.to_native_types\', args, kwargs, \'=\', res\r\n    return res\r\npd.core.internals.FloatBlock.to_native_types = to_native_types\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 7.0\r\nCython: 0.20.1\r\nnumpy: 1.11.0\r\nscipy: 0.13.3\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 3.2.1\r\nsphinx: None\r\npatsy: 0.3.0\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.0.4\r\nxlrd: 0.9.2\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.3.2\r\nbs4: 4.2.0\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.7.2\r\nboto: None\r\n```'"
12921,149318462,gliptak,jreback,2016-04-19 01:17:34,2016-04-20 00:05:32,2016-04-20 00:04:18,closed,,0.18.1,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12921,b'Correct out-of-bounds error with large indeces',b' - [x] closes #12527 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n\r\n'
12917,149021110,adneu,jreback,2016-04-18 02:18:19,2016-04-18 17:10:22,2016-04-18 17:10:08,closed,,0.18.1,1,Bug;Complex;Groupby,https://api.github.com/repos/pydata/pandas/issues/12917,b'BUG: GH12902 fixed coercion of complex values to float when using gro\xa1\xad',b' - [x] closes #12902  \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry'
12916,148992598,mcwitt,jreback,2016-04-17 20:19:44,2016-04-20 01:00:36,2016-04-20 01:00:30,closed,,0.18.1,3,Bug;Compat;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/12916,b'BUG: TypeError in index coercion',b' - [x] closes #12893\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n'
12914,148965773,gfyoung,jreback,2016-04-17 16:03:27,2016-04-18 15:51:44,2016-04-18 15:51:26,closed,,0.18.1,4,Bug;IO CSV;Testing,https://api.github.com/repos/pydata/pandas/issues/12914,b'BUG: sniffing a csv raises with only a header',b'Add test to verify that #7773 is no longer an issue.  Closes #7773.\r\n'
12912,148957490,gfyoung,jreback,2016-04-17 14:35:20,2016-04-21 21:10:15,2016-04-21 21:10:15,closed,,0.18.1,13,Bug;Difficulty Intermediate;Docs;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12912,"b'DOC, BUG: Clarify and Standardize Whitespace Delimiter Behaviour with Custom Line Terminator'","b'~~~python\r\n>>> from pandas import read_csv\r\n>>> from pandas.compat import StringIO\r\n>>> data = """"""a b c~1 2 3~4 5 6~7 8 9""""""\r\n>>> df = read_csv(StringIO(data), lineterminator=\'~\', delim_whitespace=True)\r\n>>> df\r\nEmpty DataFrame\r\nColumns: [a, b, c~1, 2, 3~4, 5, 6~7, 8, 9]\r\nIndex: []\r\n~~~\r\nexpected:\r\n~~~python\r\n>>> df\r\n    a    b    c\r\n0   1    2    3\r\n1   4    5    6\r\n2   7    8    9\r\n~~~\r\n\r\nNote that this bug is only for the C engine, as the Python engine does not yet support `delim_whitespace`.'"
12910,148919538,sinhrks,jreback,2016-04-17 05:10:55,2016-04-18 17:13:21,2016-04-18 17:13:21,closed,,0.18.1,3,Bug;Numeric;Sparse,https://api.github.com/repos/pydata/pandas/issues/12910,b'BUG: SparseArray numeric ops misc fixes',"b"" - [x] no existing issue\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixed following 3 issues occurred on the current master.\r\n\r\n### 1. addition ignores rhs ``fill_value``\r\n\r\n```\r\npd.SparseArray([1., 1.]) + pd.SparseArray([1., 0.], fill_value=0.)\r\n# [2.0, nan]\r\n# Fill: nan\r\n# IntIndex\r\n# Indices: array([0], dtype=int32)\r\n\r\nExpected:\r\n# [2.0, 1.0]\r\n```\r\n\r\n### 2. mod raises ``AttributeError``\r\n\r\n```\r\npd.SparseArray([1, 1]) % pd.SparseArray([1, np.nan])\r\n# AttributeError: 'module' object has no attribute 'sparse_nanmod'\r\n```\r\n\r\n### 3. pow outputs incorrect result wiht ``1.0 ** np.nan``\r\n\r\n```\r\npd.SparseArray([1., 1.]) ** pd.SparseArray([1., np.nan])\r\n# [1.0, nan]\r\n# Fill: nan\r\n# IntIndex\r\n# Indices: array([0], dtype=int32)\r\n\r\nExpected:\r\n# [1.0, 1.0]\r\n\r\n# NumPy result\r\nnp.array([1., 1.]) ** np.array([1, np.nan])\r\n# array([ 1.,  1.])\r\n```"""
12908,148847085,sinhrks,jreback,2016-04-16 12:11:50,2016-04-20 01:07:53,2016-04-20 01:07:37,closed,,0.18.1,1,Bug;Dtypes;Indexing;Sparse,https://api.github.com/repos/pydata/pandas/issues/12908,b'BUG: SparseSeries.shift may raise NameError or TypeError',"b"" - [x] no existing issue\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixed following bugs on current master. Also, moved ``TestSparseArrayIndex`` to ``test_libsparse``\r\n\r\n#### 1. shift\r\n\r\n```\r\npd.SparseSeries([1, 2, 3], fill_value=0).shift(1)\r\n# NameError: global name 'kwds' is not defined\r\n\r\npd.SparseSeries([1, 2, 3]).shift(1)\r\n# TypeError: %d format: a number is required, not float\r\n```\r\n\r\n#### 2. dtype\r\n\r\n```\r\nfrom pandas.sparse.array import IntIndex\r\narr = pd.SparseArray([1, 2], sparse_index=IntIndex(4, [1, 2]), dtype=None)\r\narr.dtype\r\n# dtype('int64')\r\n\r\narr.values\r\n# array([-9223372036854775808,                    1,                    2,\r\n#        -9223372036854775808])\r\n\r\n# Expected outputs\r\narr.dtype\r\n# dtype('float64')\r\n\r\narr.values\r\n# array([ nan,   1.,   2.,  nan])\r\n```\r\n"""
12906,148787898,sinhrks,jreback,2016-04-15 23:00:05,2016-04-17 13:54:55,2016-04-17 13:54:53,closed,,0.18.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12906,b'BUG: Subclassed DataFrame slicing may return normal Series',b' - [x] closes #11559\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12902,148499193,elinorer,jreback,2016-04-14 22:21:21,2016-04-18 17:10:08,2016-04-18 17:10:08,closed,,0.18.1,2,Bug;Complex;Difficulty Novice;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/12902,b'BUG: Complex types are coerced to float when summing along level of MultiIndex',"b""When trying to sum complex numbers along one level of a MultiIndex, I get a `ComplexWarning` and the resulting datatypes are `float64` with imaginary components discarded.  Summing without specifying the level or by unstacking the MultiIndex works as expected.  \r\n\r\nBased on the error message, I assume that groupby is being called internally, and that the bug is actually there.  I'm using pandas version 17.1.\r\n\r\n#### Code Sample\r\n```\r\nIn [1]: df = pd.DataFrame(data=np.arange(4)*(1+2j), index=pd.MultiIndex.from_product(([0,1], [0,1]), names=['a','b']))\r\nIn [2]: print df\r\n          0\r\na b        \r\n0 0      0j\r\n  1  (1+2j)\r\n1 0  (2+4j)\r\n  1  (3+6j)\r\n\r\nIn [3]: df.sum(level='a')\r\n/Users/elinore/Library/Python/2.7/lib/python/site-packages/pandas/core/groupby.py:1629: ComplexWarning: Casting complex values to real discards the imaginary part\r\n  values = _algos.ensure_float64(values)\r\nOut[3]: \r\n   0\r\na   \r\n0  1\r\n1  5\r\n```\r\n#### Expected Output\r\n```\r\nOut[3]:\r\n       0\r\na   \r\n0  (1+2j)\r\n1  (5+10j)\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 19.2\r\nCython: 0.23.5\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nIPython: 4.1.2\r\nsphinx: 1.4\r\npatsy: None\r\ndateutil: 2.5.1\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.5.1\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n"""
12900,148437365,gfyoung,jreback,2016-04-14 17:55:52,2016-04-22 15:24:41,2016-04-22 15:20:13,closed,,0.18.1,28,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12900,b'Allow parsing in skipped row for C engine',b'Changes behaviour of C engine parser so that parsing is done on skipped rows so that they\r\nare properly skipped.\r\n\r\nCloses #10911.\r\nCloses #12775.'
12896,148228431,tntdynamight,jreback,2016-04-14 01:53:33,2016-05-14 12:04:34,2016-05-14 12:04:34,closed,,0.19.0,8,Bug;Difficulty Advanced;Effort Low;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/12896,"b'Multiindex slicing df.loc[idx[dim1,dim2,dim3],:] not working right in some cases'","b""#### Code Sample, a copy-pastable example if possible\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nnp.random.seed(0) \r\nidx = pd.IndexSlice\r\nmidx = pd.MultiIndex.from_product([['CS'], range(20), range(-1100, 6000)]) \r\ndf = pd.DataFrame(np.random.randn(7100*20, 3), columns=['dat1', 'dat2', 'dat3'], index=midx)\r\n```\r\n#### Output\r\n```\r\n> df.loc[idx['CS', :, -1000:-950], :].head()\r\n                dat1      dat2      dat3\r\nCS 0 -1000 -1.306527  1.658131 -0.118164\r\n     -999  -0.680178  0.666383 -0.460720\r\n     -998  -1.334258 -1.346718  0.693773\r\n     -997  -0.159573 -0.133702  1.077744\r\n     -996  -1.126826 -0.730678 -0.384880\r\n```\r\n```\r\n> df.loc[idx['CS', :, -1000:-50], :].head()\r\n                dat1      dat2      dat3\r\nCS 0 -1100  1.764052  0.400157  0.978738  # <<< Index Level 2 should start at -1000\r\n     -1099  2.240893  1.867558 -0.977278\r\n     -1098  0.950088 -0.151357 -0.103219\r\n     -1097  0.410599  0.144044  1.454274\r\n     -1096  0.761038  0.121675  0.443863\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-59-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.2.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.0.3\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.39.0\r\n\r\n"""
12895,148187314,OXPHOS,jreback,2016-04-13 21:25:53,2016-04-16 01:44:35,2016-04-16 01:44:27,closed,,0.18.1,5,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/12895,b'BUG: add names parameter to read_excel',"b' - [x] closes #12870, xref #11874\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'"
12893,148122496,mcwitt,jreback,2016-04-13 17:01:02,2016-04-20 01:00:30,2016-04-20 01:00:30,closed,,0.18.1,1,Bug;Compat;Difficulty Advanced;Effort Low;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/12893,b'Related TypeErrors in multi-indexed DataFrame',"b'## Code sample\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfoo = pd.DataFrame(np.arange(100).reshape((10,10)))\r\nbar = pd.DataFrame(np.arange(100).reshape((10,10)))\r\n\r\ndf = pd.concat({\r\n        \'foo\': foo.stack(),\r\n        \'bar\': bar.stack()\r\n    }, axis=1)\r\n\r\ndf.index.names = [\'fizz\',\'buzz\']\r\n```\r\n## Output\r\n```python\r\n> df\r\nTypeError: data type ""fizz"" not understood\r\n\r\n> df_ = df.set_index(\'foo\', append=True)  # shouldn\'t call __repr__\r\nTypeError: data type ""fizz"" not understood\r\n\r\n> df[:61]\r\nTypeError: data type ""fizz"" not understood\r\n\r\n> df[:60]\r\n           bar  foo\r\nfizz buzz          \r\n0    0       0    0\r\n     1       1    1\r\n     2       2    2\r\n...\r\n```\r\n## show_versions() output\r\n```\r\n> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 2ba977a917ac5c092378a2f172f59d53b14de968\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0+111.g2ba977a\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 20.6.7\r\nCython: 0.24\r\nnumpy: 1.11.0\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.9.2\r\napiclient: 1.5.0\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n```\r\n'"
12883,147874014,jreback,jreback,2016-04-12 21:12:18,2016-04-12 21:34:45,2016-04-12 21:34:44,closed,,Next Major Release,3,Bug;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12883,"b""BUG: resample on PeriodIndex with 'W' causes recursion""","b""```\r\nIn [32]: s = pd.Series(range(2), pd.period_range('1/1/1990', periods=2, freq='M'))\r\n\r\nIn [33]: s\r\nOut[33]: \r\n1990-01    0\r\n1990-02    1\r\nFreq: M, dtype: int64\r\n\r\nIn [34]: s.resample('W').asfreq()\r\n```\r\n\r\n"""
12881,147759297,jreback,jreback,2016-04-12 13:56:13,2016-04-18 17:22:25,2016-04-18 17:22:25,closed,,0.18.1,1,Bug;Difficulty Intermediate;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12881,b'BUG: .astype of Float64Index',"b""xref #12866 \r\n```\r\ndata = Index([u'0', u'1', u'2'], dtype='object')\r\ndata.astype('float64').astype('int64')\r\nTypeError: Setting <class 'pandas.indexes.numeric.Float64Index'> dtype to anything other than float64 or object is not supported\r\n```\r\nthis should work"""
12878,147723209,Komnomnomnom,jreback,2016-04-12 11:45:25,2016-04-27 14:29:01,2016-04-27 14:28:37,closed,,0.18.1,6,Bug;Complex;Enhancement;IO JSON,https://api.github.com/repos/pydata/pandas/issues/12878,b'BUG: json invoke default handler for unsupported numpy dtypes',b' - [x] closes #12554\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n - [x] valgrind clean\r\n - [ ] windows tests ok\r\n - [x] vbench ok\r\n\r\nRecommend to merge after #12802 has been accepted. (valgrind should be clean then)\r\n\r\n'
12874,147600984,MaximilianR,jreback,2016-04-12 00:57:13,2016-04-13 14:28:27,2016-04-13 01:12:57,closed,,0.18.1,9,Bug;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12874,b'PeriodIndex count & size fix',b'closes #12774 \r\ncloses #12868 \r\ncloses #12770 \r\n \r\n- [x] tests added / passed\r\n- [x] passes ``git diff upstream/master | flake8 --diff``\r\n- [x] whatsnew entry\r\n\r\nInitial attempt at fixing some of the more urgent issues'
12870,147557735,swkoh,jreback,2016-04-11 21:17:18,2016-04-18 10:26:06,2016-04-16 01:44:27,closed,,0.18.1,2,Bug;Difficulty Novice;Effort Low;IO Excel;Regression,https://api.github.com/repos/pydata/pandas/issues/12870,b'read_excel with names keyword argument not working',"b""With pandas 0.18,  pd.read_excel() with name keyworkd does not work anymore.  (pandas 0.17 works fine.) pandas 0.18 has a change to add names as a keyword argument in excel.py at line 76.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\nfilename = 'test.xlsx'\r\ncolumn_names = ['field1', 'field2']\r\ndf = pd.read_excel(filename, index_col=None,  names=column_names)\r\n\r\n#### Expected Output\r\ncolumn_names associated with keyword argument names is supposed to pass down to kwds at C:\\Python27\\Lib\\site-packages\\pandas\\io\\excel.py:177.\r\n\r\nkwds is supposed to carry the arguments from application (in this case, column_names above) but it does not get passed. \r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: None\r\npip: 8.1.1\r\nsetuptools: 18.3.2\r\nCython: 0.23.5\r\nnumpy: 1.11.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.2.2\r\nxlrd: 0.9.3\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.3\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n"""
12868,147536959,jduncavage,jreback,2016-04-11 19:56:41,2016-04-13 01:12:56,2016-04-13 01:12:56,closed,,0.18.1,5,Bug;Difficulty Intermediate;Effort Low;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12868,b'BUG: Resample on empty Period-Indexed Series returns Datetime-Indexed Series',"b""#### Code Sample, a copy-pastable example if possible\r\n`\r\nIn [7]: pd.Series([], pd.period_range(start='2000', periods=0)).resample('W').mean().index\r\nOut[7]: DatetimeIndex([], dtype='datetime64[ns]', freq='W-SUN')\r\n`\r\n\r\n#### Expected Output\r\n``\r\nPeriodIndex([], dtype='int64', freq='W-SUN')\r\n``\r\n\r\n\r\nOn pandas: 0.18.0\r\n"""
12866,147491464,stellasia,jreback,2016-04-11 17:03:37,2016-04-18 17:22:25,2016-04-18 17:22:25,closed,,0.18.1,6,Bug;Dtypes;IO JSON,https://api.github.com/repos/pydata/pandas/issues/12866,b'read_json changes dtype (int => float)',"b'Hi there!\r\n\r\nThis might be a well-known problem but could not find a track/explaination about it. When reading a json to create a pandas object (Series or DataFrame), the index dtype is changed from int to float.\r\n\r\nThe  [doc](http://pandas.pydata.org/pandas-docs/stable/io.html#data-conversion) only mentions the inverse trans-typing:\r\n\r\n> a column that was float data will be converted to integer if it can be done safely, e.g. a column of 1.\r\n\r\nIs this behaviour expected as well? Is the only solution giving the `read_json` a `dtype` argument? \r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\n    In [1]: import pandas as pd\r\n\r\n    In [2]: s = pd.Series([11, 12, 13])\r\n\r\n    In [3]: s\r\n    Out[3]: \r\n    0    11\r\n    1    12\r\n    2    13\r\n    dtype: int64\r\n\r\n    In [4]: s.to_json()\r\n    Out[4]: \'{""0"":11,""1"":12,""2"":13}\'\r\n\r\n    In [5]: pd.read_json(s.to_json(), typ=""serie"")\r\n    Out[5]:  \r\n    0.0    11\r\n    1.0    12\r\n    2.0    13\r\n    dtype: int64\r\n\r\n    In [6]: s.index.dtype  \r\n    Out[6]: dtype(\'int64\')\r\n\r\n    In [7]: pd.read_json(s.to_json(), typ=""serie"").index.dtype\r\n    Out[7]: dtype(\'float64\')\r\n``` \r\n\r\n#### Expected Output\r\n\r\nI would like to recover the integer index for the new `Series` : \r\n```\r\n    In [5]: pd.read_json(s.to_json(), typ=""serie"")\r\n    Out[5]:  \r\n    0    11\r\n    1    12\r\n    2    13\r\n    dtype: int64\r\n\r\n    In [7]: pd.read_json(s.to_json(), typ=""serie"").index.dtype\r\n    Out[7]: dtype(\'int64\')\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-44-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.4\r\npip: 1.5.6\r\nsetuptools: 12.2\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.14.1\r\nstatsmodels: 0.5.0\r\nxarray: None\r\nIPython: 4.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.2.0-b1\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: None\r\nsqlalchemy: 1.0.0\r\npymysql: None\r\npsycopg2: 2.6 (dt dec mx pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\n```\r\n\r\nThanks!'"
12846,147205382,sinhrks,jreback,2016-04-10 08:25:40,2016-04-10 14:22:50,2016-04-10 14:22:42,closed,,0.18.1,1,API Design;Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12846,b'BUG: empty Series concat has no effect',"b' - [x] closes #11082, closes #12695, closes #12696\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nAlso includes some cleanup for #10723\r\n\r\nCC: @AnkurDedania , @IamGianluca'"
12845,147191817,sinhrks,jreback,2016-04-10 04:26:03,2016-04-10 14:33:13,2016-04-10 14:32:47,closed,,0.18.1,1,Bug;Indexing;Sparse,https://api.github.com/repos/pydata/pandas/issues/12845,b'BUG: SparseSeries slicing with Ellipsis raises KeyError',b' - [x] closes #9467\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12844,147191159,sinhrks,jreback,2016-04-10 04:09:27,2016-04-18 19:55:34,2016-04-18 19:55:34,closed,,0.18.1,3,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/12844,b'BUG: SparseSeries concat results in dense',b' - [x] closes #10536\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12843,147187860,sinhrks,jreback,2016-04-10 03:18:14,2016-04-10 14:26:31,2016-04-10 14:26:28,closed,,0.18.1,1,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/12843,b'BUG: .str methods with expand=True may raise ValueError if input has name',b' - [x] closes #12617 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12840,147149836,sinhrks,jreback,2016-04-09 18:21:06,2016-04-10 14:04:45,2016-04-10 14:04:45,closed,,0.18.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/12840,b'BUG: GroupBy with TimeGrouper sorts unstably',"b' - [x] closes #7453 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nBecause it may affect to resample perf, alternative idea is to add ``TimeGropuper`` path to ``GroupBy.first`` and ``last``.'"
12839,147149218,sinhrks,jreback,2016-04-09 18:15:06,2016-07-06 21:47:54,2016-07-06 21:47:54,closed,,0.19.0,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/12839,b'GroupBy.nth includes group key inconsistently',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\n``nth`` doesn't inlcude group key as the same as ``first`` and ``last``.\r\n\r\n```\r\ndf = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [1, 2, 3, 4, 5],\r\n                   'G': [1, 1, 2, 2, 1]})\r\n\r\ng = df.groupby('G')\r\ng.nth(1)\r\n#    A  B\r\n# G      \r\n# 1  2  2\r\n# 2  4  4\r\n```\r\n\r\nHowever, calling ``head`` makes the behavior change. Looks to be caused by ``_set_selection_from_grouper`` caches its selection.\r\n\r\n```\r\ng = df.groupby('G')\r\ng.head()\r\ng.nth(1)\r\n#    A  B  G\r\n# G         \r\n# 1  2  2  1\r\n# 2  4  4  2\r\n```\r\n\r\n#### Expected Output\r\n\r\nalways as below.\r\n\r\n```\r\ng.nth(1)\r\n#    A  B\r\n# G      \r\n# 1  2  2\r\n# 2  4  4\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\ncurrent master.\r\n"""
12836,147093943,sinhrks,jreback,2016-04-09 05:36:48,2016-04-09 18:02:18,2016-04-09 18:01:56,closed,,0.18.1,3,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/12836,b'BUG: SparseSeries.to_frame results in dense',b' - [x] closes #9850\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nThis should be after #12831.\r\n\r\n'
12835,147093660,sinhrks,jreback,2016-04-09 05:33:29,2016-04-29 17:17:59,2016-04-29 17:17:59,closed,,0.18.1,4,Algos;Bug;Categorical;Sparse;Timezones,https://api.github.com/repos/pydata/pandas/issues/12835,b'BUG: SparseSeries.value_counts ignores fill_value',b' - [x] closes #6749\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nAlso fixed an issue which categorical ``value_counts`` resets name.'
12834,147089283,mrpoor,jreback,2016-04-09 04:40:27,2016-04-11 13:00:05,2016-04-11 12:59:59,closed,,0.18.1,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12834,b'BUG 12800 fixed inconsistent behavior of last_valid_index and first_valid_index',b' - [x] closes #12800\r\n - [x] tests added / passed\r\n - [ ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n\r\n'
12831,147049682,sinhrks,jreback,2016-04-08 21:57:48,2016-04-09 14:44:55,2016-04-09 14:44:21,closed,,0.18.1,1,Bug;Indexing;Missing-data;Sparse,https://api.github.com/repos/pydata/pandas/issues/12831,b'BUG: SparseSeries.reindex with fill_value',b' - [x] closes #12797\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12824,146846474,ruoyu0088,jreback,2016-04-08 07:50:09,2016-05-20 14:09:29,2016-05-20 14:09:29,closed,,0.19.0,1,Bug;Difficulty Intermediate;Effort Medium;Groupby;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12824,b'groupby().apply() returns different result depends on the first result is None or not.',"b'The apply document says that it can:\r\n\r\n> apply can act as a reducer, transformer, or filter function, depending on exactly what is passed to apply. So depending on the path taken, and exactly what you are grouping. Thus the grouped columns(s) may be included in the output as well as set the indices. \r\n\r\nSo, I think it may discard the group if the callback function returns None. Here is two exmaple that works and not works:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({""A"":np.arange(10), ""B"":[1, 1, 1, 2, 3, 3, 3, 4, 4, 4]})\r\nprint(df.groupby(""B"").apply(lambda df2:None if df2.shape[0] <= 2 else df2.iloc[[0, -1]]))\r\n\r\ndf = pd.DataFrame({""A"":np.arange(10), ""B"":[1, 2, 2, 2, 3, 3, 3, 4, 4, 4]})\r\nprint(df.groupby(""B"").apply(lambda x:None if x.shape[0] <= 2 else x.iloc[[0, -1]]))\r\n```\r\n\r\nthe output is as following, the first one returns a DataFrame, the second one returns a Series with DataFrames inside:\r\n\r\n```\r\n     A  B\r\nB        \r\n1 0  0  1\r\n  2  2  1\r\n3 4  4  3\r\n  6  6  3\r\n4 7  7  4\r\n  9  9  4\r\n\r\n\r\nB\r\n1         A    B\r\n1  NaN  NaN\r\n3  NaN  NaN\r\n2                   A  B\r\n1  1  2\r\n3  3  2\r\n3                   A  B\r\n4  4  3\r\n6  6  3\r\n4                   A  B\r\n7  7  4\r\n9  9  4\r\ndtype: object\r\n```\r\n\r\nThe problem is that `_wrap_applied_output()` in `groupby.py` checks the first element to determine the concat method:\r\n\r\n```\r\n    if isinstance(values[0], DataFrame):\r\n        return self._concat_objects(keys, values,\r\n                                    not_indexed_same=not_indexed_same)\r\n```             \r\n\r\nI want to know, does `groupby().apply()` support discarding groups?'"
12802,146054168,Komnomnomnom,jreback,2016-04-05 17:21:19,2016-04-26 18:55:09,2016-04-26 18:54:47,closed,,0.18.1,26,Bug;Categorical;Compat;Dtypes;IO JSON;Timezones,https://api.github.com/repos/pydata/pandas/issues/12802,b'BUG: fix json segfaults',"b'closes #11473\r\ncloses #10778\r\ncloses #11299\r\n\r\n - [x] tests added / passed\r\n - [x] vbench / asv ok\r\n - [x] windows tests pass\r\n - [x] valgrind clean\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nThis fixes several potential segfaults in the json code:\r\n- time objects were freed and potentially garbage collected before their data had been read #11473. Was also segfaulting if an exception was raised during conversion (e.g. when using dateutil timezones).\r\n- 0d arrays were not being handled corectly (needed `goto INVALID`) #11299\r\n- all blocks were assumed to be ndarrays #10778 \r\n\r\nFixing #10778 means non-ndarray blocks are now supported (although I think at present `category` is the only one?).\r\n\r\nNot tested on windows yet. Seeing some unrelated travis failures on my fork (msgpack), is that normal?'"
12800,145974975,phil20686,jreback,2016-04-05 12:36:26,2016-04-11 12:59:58,2016-04-11 12:59:58,closed,,0.18.1,3,Bug;Difficulty Novice;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/12800,"b'inconsistent behavior of series.last_valid_index, and df.last_valid_index when dataframe empty'","b'\r\n    idx = pd.DataFrame().last_valid_index() #throws index error\r\n    idx = pd.Series().last_valid_index() #returns None\r\n\r\nif you call last_valid_index() on df it throws an IndexError, if you call it on series it returns None. It seems like it would be very little effort to make these two cases consistent. Moreover, I believe they should be consistent with the behavior of \r\n\r\n    series.get_value(index_value)\r\n\r\nwhere index value is not in the index (i.e. a KeyError). \r\n\r\n\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 45 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: 1.3.4\r\npip: 8.1.1\r\nsetuptools: 20.2.2\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 2.3.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: 2.3.1\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.7.3\r\nboto: None\r\n'"
12797,145818772,sinhrks,jreback,2016-04-04 22:10:28,2016-04-09 14:44:21,2016-04-09 14:44:21,closed,,0.18.1,5,Bug;Missing-data;Sparse,https://api.github.com/repos/pydata/pandas/issues/12797,b'BUG: Sparse incorrectly handle fill_value',"b""Sparse looks to handle ``missing (NaN)`` and ``fill_value`` confusingly. Based on the doc, I understand ``fill_value`` is a user-specified value to be omitted in the sparse internal repr. ``fill_value`` may be different from missing (NaN).\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\n# NG, 2nd and last element must be NaN\r\npd.SparseArray([1, np.nan, 0, 3, np.nan], fill_value=0).to_dense()\r\n# array([ 1.,  0.,  0.,  3.,  0.])\r\n\r\n# NG, 2nd element must be NaN\r\norig = pd.Series([1, np.nan, 0, 3, np.nan], index=list('ABCDE'))\r\nsparse = orig.to_sparse(fill_value=0)\r\nsparse.reindex(['A', 'B', 'C'])\r\n# A    1.0\r\n# B    0.0\r\n# C    0.0\r\n# dtype: float64\r\n# BlockIndex\r\n# Block locations: array([0], dtype=int32)\r\n# Block lengths: array([1], dtype=int32)\r\n```\r\n\r\n#### Expected Output\r\n\r\n```\r\npd.SparseArray([1, np.nan, 0, 3, np.nan], fill_value=0).to_dense()\r\n# array([ 1.,  np.nan,  0.,  3.,  np.nan])\r\n\r\nsparse = orig.to_sparse(fill_value=0)\r\nsparse.reindex(['A', 'B', 'C'])\r\n# A    1.0\r\n# B    NaN\r\n# C    0.0\r\n# dtype: float64\r\n# BlockIndex\r\n# Block locations: array([0], dtype=int32)\r\n# Block lengths: array([1], dtype=int32)\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\nCurrent master.\r\n\r\nThe fix itself looks straightforward, but it breaks some tests use dubious comparison.\r\n\r\n- https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L1730\r\n\r\n"""
12791,145588931,ningchi,jreback,2016-04-04 05:28:47,2016-04-17 14:13:11,2016-04-17 14:13:08,closed,,0.18.1,6,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12791,b'Fix for #12723: Unexpected behavior with binary operators and fill\xa1\xad',b' - [ ] closes #xxxx\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\ncloses #12723 '
12787,145532605,sinhrks,jreback,2016-04-03 19:29:38,2016-04-04 17:53:42,2016-04-04 17:53:40,closed,,0.18.1,2,Bug;Indexing;Sparse,https://api.github.com/repos/pydata/pandas/issues/12787,b'BUG: SparseDataFrame indexing may return normal Series',"b' - [x] related to #4400 (not close yet)\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFound a below bug in ``SparseDataFrame`` indexing.\r\n\r\n```\r\n# NG, must be SparseSeries\r\ndf = pd.DataFrame([[1, 2], [np.nan, 4]]).to_sparse()\r\ntype(df.loc[0])  \r\n# pandas.core.series.Series\r\n```'"
12783,145509172,jonaslb,jreback,2016-04-03 15:05:02,2016-04-03 17:52:49,2016-04-03 17:52:22,closed,,0.18.1,3,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/12783,b'BUG: DataFrame.drop() does nothing for non-unique Datetime MultiIndex',"b"" - [x] closes #12701\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nCloses #12701\r\n\r\nFollows the suggested fix in the comments to the bug report.\r\nAlso added a line in whatsnew and a test.\r\n\r\nRegarding the tests it passes the one I added. But I just noticed that plenty others fail and I have yet to find out whether it's caused by this (who knows, it might)\r\nUpdate: It's the same tests failing on master branch as on this one (phew..). I guess the errors deserve bug reports but that's for another time."""
12779,145466568,sinhrks,jreback,2016-04-03 07:53:56,2016-04-03 15:35:10,2016-04-03 14:20:30,closed,,0.18.1,3,Bug;Indexing;Output-Formatting;Sparse,https://api.github.com/repos/pydata/pandas/issues/12779,b'BUG: Sparse misc fixes including __repr__',"b"" - [x] closes #10560\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixes following issues.\r\n\r\n```\r\n# NG, should be SparseArray (categorized as API change)\r\na = pd.SparseArray([1, np.nan, np.nan, 3, np.nan])\r\ntype(a.take([1, 2]))\r\n# numpy.ndarray\r\n\r\n# NG\r\ns = pd.Series([1, np.nan, np.nan, 3, np.nan]).to_sparse()\r\ns.loc[[1, 3]]\r\n# TypeError: reindex() got an unexpected keyword argument 'level'\r\n\r\n# NG\r\ns.iloc[2]\r\n# IndexError: index out of bounds\r\n\r\n# NG, must be SparseSeries (root cause of 10560)\r\ntype(s.iloc[2:])\r\n# pandas.sparse.array.SparseArray\r\n```\r\n"""
12778,145465631,sinhrks,jreback,2016-04-03 07:47:43,2016-04-03 14:24:32,2016-04-03 14:24:21,closed,,0.18.1,1,Bug;Dtypes;Sparse,https://api.github.com/repos/pydata/pandas/issues/12778,b'BUG: to_dense does not preserve dtype in SparseArray',b' - [x] closes #10648\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12776,145396110,jonaslb,jreback,2016-04-02 17:22:59,2016-04-03 14:34:00,2016-04-03 14:33:08,closed,,0.18.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/12776,b'BUG: filter (with dropna=False) when there are no groups fulfilling the condition',b' - [x] closes #12768\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nImplemented the fix for #12768 which was proposed in the bug report. '
12775,145348363,strnam,jreback,2016-04-02 07:46:34,2016-04-22 15:20:13,2016-04-22 15:20:13,closed,,0.18.1,5,Bug;Difficulty Intermediate;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12775,b'read_csv return wrong dataframe when setting skiprows. ',"b'#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> from StringIO import StringIO\r\n>>> data = """"""id,text,num_lines\r\n1,""line 11\r\nline 12"",2\r\n2,""line 21\r\nline 22"",2\r\n3,""line 31"",1""""""\r\n\r\n>>> pd.read_csv(StringIO(data))\r\nOut[2]: \r\n   id              text  num_lines\r\n0   1  \'line 11\\nline 12\'          2\r\n1   2  \'line 21\\nline 22\'          2\r\n2   3           \'line 31\'          1\r\n\r\n>>> pd.read_csv(StringIO(data), skiprows=[1])\r\nOut[3]: \r\n         id              text  num_lines\r\n0  \'line 12""\'                 2        NaN\r\n1         2  \'line 21\\nline 22\'        2.0\r\n2         3           \'line 31\'        1.0\r\n...\r\n```\r\n\r\n#### Expected Output\r\n```python\r\n>>> pd.read_csv(StringIO(data), skiprows=[1])\r\nOut[3]: \r\n   id              text  num_lines\r\n0   2  \'line 21\\nline 22\'          2\r\n1   3           \'line 31\'          1\r\n...\r\n```\r\n\r\nIt should skip \'1,""line 11\\nline 12"",2\' instead skip    \'1,""line 11\' \r\n \r\n#### output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.2.3-300.fc23.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 18.0.1\r\nCython: None\r\nnumpy: 1.11.0\r\nscipy: 0.14.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 3.2.1\r\nsphinx: 1.2.3\r\npatsy: 0.4.1\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: 0.6.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n\r\n'"
12774,145336332,MaximilianR,jreback,2016-04-02 05:03:08,2016-04-13 01:12:56,2016-04-13 01:12:56,closed,,0.18.1,2,Bug;Difficulty Intermediate;Effort Medium;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12774,b'BUG: Count on resampled PeriodIndex fails',"b'```python\r\nIn [38]: pd.Series(1, index=pd.period_range(start=\'2000\', periods=100)).resample(\'M\').count()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-38-295afa97781f> in <module>()\r\n----> 1 pd.Series(1, index=pd.period_range(start=\'2000\', periods=100)).resample(\'M\').count()\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in f(self, _method)\r\n    473 \r\n    474     def f(self, _method=method):\r\n--> 475         return self._groupby_and_aggregate(None, _method)\r\n    476     f.__doc__ = getattr(GroupBy, method).__doc__\r\n    477     setattr(Resampler, method, f)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _groupby_and_aggregate(self, grouper, how, *args, **kwargs)\r\n    353 \r\n    354         if grouper is None:\r\n--> 355             self._set_binner()\r\n    356             grouper = self.grouper\r\n    357 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _set_binner(self)\r\n    202 \r\n    203         if self.binner is None:\r\n--> 204             self.binner, self.grouper = self._get_binner()\r\n    205 \r\n    206     def _get_binner(self):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _get_binner(self)\r\n    210         """"""\r\n    211 \r\n--> 212         binner, bins, binlabels = self._get_binner_for_time()\r\n    213         bin_grouper = BinGrouper(bins, binlabels)\r\n    214         return binner, bin_grouper\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _get_binner_for_time(self)\r\n    538         if self.kind == \'period\':\r\n    539             return self.groupby._get_time_period_bins(self.ax)\r\n--> 540         return self.groupby._get_time_bins(self.ax)\r\n    541 \r\n    542     def _downsample(self, how, **kwargs):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _get_time_bins(self, ax)\r\n    905         if not isinstance(ax, DatetimeIndex):\r\n    906             raise TypeError(\'axis must be a DatetimeIndex, but got \'\r\n--> 907                             \'an instance of %r\' % type(ax).__name__)\r\n    908 \r\n    909         if len(ax) == 0:\r\n\r\nTypeError: axis must be a DatetimeIndex, but got an instance of \'PeriodIndex\'\r\n```\r\nExpected:\r\n```\r\nOut[39]: \r\n2000-01    31\r\n2000-02    29\r\n2000-03    31\r\n2000-04     9\r\nFreq: M, dtype: int64\r\n```\r\n\r\nOn pandas 0.18\r\n\r\n'"
12772,145328874,sinhrks,jreback,2016-04-02 02:56:04,2016-04-03 17:03:18,2016-04-03 17:03:18,closed,,0.18.1,0,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/12772,b'BUG: Series.quantile dtype / nan handling issue',"b""Found some issues when working on #12572. Both being fixed.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n##### 1. DataFrame may coerce to float even if interpolation\r\n\r\n```\r\n# OK, same as numpy\r\npd.Series([1, 2, 3]).quantile([0.5], interpolation='lower').dtypes\r\n# dtype('int64')\r\n\r\n# NG, it must be int\r\npd.DataFrame({'A': [1, 2, 3]}).quantile([0.5], interpolation='lower').dtypes\r\n# A    float64\r\n# dtype: object\r\n```\r\n\r\n##### 2. May return scalar even if its input is a list-like\r\n\r\n```\r\n# NG, it must be a Series\r\npd.Series([]).quantile([0.2, 0.3])\r\n# nan\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\nCurrent master.\r\n"""
12771,145314293,MaximilianR,jreback,2016-04-02 00:08:01,2016-04-03 20:37:39,2016-04-03 20:35:52,closed,,0.18.1,9,Bug;Indexing;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12771,b'Retain name in PeriodIndex resample',b' - [x] closes #12769\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12770,145306541,MaximilianR,jreback,2016-04-01 23:07:50,2016-04-13 01:12:56,2016-04-13 01:12:56,closed,,0.18.1,3,Bug;Difficulty Novice;Effort Low;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12770,b'ERR: Resample pad on existing freq causes recursion error',"b'Not a big bug, but not ideal behavior:\r\n\r\n```python\r\n\r\nIn [26]: series=pd.Series(range(10), pd.period_range(start=\'2000\', periods=10, name=\'date\', freq=\'M\'))\r\n\r\nIn [25]: series\r\nOut[25]: \r\ndate\r\n2000-01    0\r\n2000-02    1\r\n2000-03    2\r\n2000-04    3\r\n2000-05    4\r\n2000-06    5\r\n2000-07    6\r\n2000-08    7\r\n2000-09    8\r\n2000-10    9\r\nFreq: M, dtype: int64\r\n\r\n\r\nIn [30]: series.resample(\'M\').first()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-30-4637af8f5d6f> in <module>()\r\n----> 1 series.resample(\'M\').first()\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in f(self, _method)\r\n    465 \r\n    466     def f(self, _method=method):\r\n--> 467         return self._downsample(_method)\r\n    468     f.__doc__ = getattr(GroupBy, method).__doc__\r\n    469     setattr(Resampler, method, f)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _downsample(self, how, **kwargs)\r\n    709                          \'resampled to {freq}\'.format(\r\n    710                              axfreq=ax.freq,\r\n--> 711                              freq=self.freq))\r\n    712 \r\n    713     def _upsample(self, method, limit=None):\r\n\r\nValueError: Frequency <MonthEnd> cannot be resampled to <MonthEnd>\r\n\r\n# --> which is fair, even if I could imagine it returning itself\r\n\r\nIn [31]: series.resample(\'M\').pad()\r\n\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-24-9df8c7209780> in <module>()\r\n----> 1 series.resample(\'M\').pad()\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in pad(self, limit)\r\n    393         DataFrame.fillna\r\n    394         """"""\r\n--> 395         return self._upsample(\'pad\', limit=limit)\r\n    396     ffill = pad\r\n    397 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in _upsample(self, method, limit)\r\n    736 \r\n    737         if not is_superperiod(ax.freq, self.freq):\r\n--> 738             return self.asfreq()\r\n    739 \r\n    740         # Start vs. end of period\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/resample.py in asfreq(self)\r\n    435         essentially a reindex with (no filling)\r\n    436         """"""\r\n--> 437         return self._upsample(None)\r\n    438 \r\n    439     def std(self, ddof=1):\r\n\r\n# ...............\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/period.py in _generate_range(cls, start, end, periods, freq, fields)\r\n    200                 raise ValueError(\'Can either instantiate from fields \'\r\n    201                                  \'or endpoints, but not both\')\r\n--> 202             subarr, freq = _get_ordinal_range(start, end, periods, freq)\r\n    203         elif field_count > 0:\r\n    204             subarr, freq = _range_from_fields(freq=freq, **fields)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/tseries/period.py in _get_ordinal_range(start, end, periods, freq, mult)\r\n    995 \r\n    996     if start is not None:\r\n--> 997         start = Period(start, freq)\r\n    998     if end is not None:\r\n    999         end = Period(end, freq)\r\n\r\npandas/src/period.pyx in pandas._period.Period.__init__ (pandas/src/period.c:10867)()\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py in __enter__(self)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py in _get_module_lock(name)\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\n#### Expected Output\r\n\r\nRaise on the initial resample? Return itself? Deliberately raise on `pad`?\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n18.0'"
12769,145303593,MaximilianR,jreback,2016-04-01 22:49:34,2016-04-03 20:35:52,2016-04-03 20:35:52,closed,,0.18.1,1,Bug;Difficulty Novice;Effort Low;Indexing;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/12769,b'BUG: Resample loses PeriodIndex name',"b""#### Code Sample, a copy-pastable example if possible\r\n```python\r\n\r\nIn [17]: series=pd.Series(range(10), pd.period_range(start='2000', periods=10, name='date', freq='M'))\r\n\r\nIn [18]: series\r\nOut[18]: \r\ndate\r\n2000-01    0\r\n2000-02    1\r\n2000-03    2\r\n2000-04    3\r\n2000-05    4\r\n2000-06    5\r\n2000-07    6\r\n2000-08    7\r\n2000-09    8\r\n2000-10    9\r\nFreq: M, dtype: int64\r\n\r\nIn [19]: series.index\r\nOut[19]: \r\nPeriodIndex(['2000-01', '2000-02', '2000-03', '2000-04', '2000-05', '2000-06',\r\n             '2000-07', '2000-08', '2000-09', '2000-10'],\r\n            dtype='int64', name='date', freq='M')\r\n# Note name='date'\r\n\r\nIn [20]: series.resample('D').pad().index\r\nOut[20]: \r\nPeriodIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',\r\n             '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08',\r\n             '2000-01-09', '2000-01-10',\r\n             ...\r\n             '2000-10-22', '2000-10-23', '2000-10-24', '2000-10-25',\r\n             '2000-10-26', '2000-10-27', '2000-10-28', '2000-10-29',\r\n             '2000-10-30', '2000-10-31'],\r\n            dtype='int64', length=305, freq='D')\r\n\r\n# now no name\r\n```\r\n#### Expected Output\r\n\r\n```python\r\nOut[20]: \r\nPeriodIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',\r\n             '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08',\r\n             '2000-01-09', '2000-01-10',\r\n             ...\r\n             '2000-10-22', '2000-10-23', '2000-10-24', '2000-10-25',\r\n             '2000-10-26', '2000-10-27', '2000-10-28', '2000-10-29',\r\n             '2000-10-30', '2000-10-31'],\r\n            dtype='int64', length=305, freq='D', name='date')\r\n```\r\n\r\n#### output of `pd.show_versions()`\r\n\r\n`Out[21]: '0.18.0'`"""
12768,145216340,sebov,jreback,2016-04-01 15:53:52,2016-04-03 14:34:12,2016-04-03 14:33:08,closed,,0.18.1,2,Bug;Difficulty Intermediate;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/12768,b'BUG: filter (with dropna=False) when there are no groups fulfilling the condition',"b'For a DataFrame I want to preserve rows that belong to groups that fulfil specific condition and replace other rows with NaN. I have used a combination of \'groupby\' and \'filter\' (with dropna=False). In a special case when there are no groups fulfilling the condition an exception occured.\r\n\r\n```python\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-11-ffb9adbc134a> in <module>()\r\n----> 1 pd.DataFrame({\'a\': [1,1,2], \'b\':[1,2,0]}).groupby(\'a\').filter(lambda x: x[\'b\'].sum() > 5, dropna=False)\r\n\r\n....../local/lib/python2.7/site-packages/pandas/core/groupby.py in filter(self, func, dropna, *args, **kwargs)\r\n   3570                                 type(res).__name__)\r\n   3571 \r\n-> 3572         return self._apply_filter(indices, dropna)\r\n   3573 \r\n   3574 \r\n\r\n....../local/lib/python2.7/site-packages/pandas/core/groupby.py in _apply_filter(self, indices, dropna)\r\n    831             mask = np.empty(len(self._selected_obj.index), dtype=bool)\r\n    832             mask.fill(False)\r\n--> 833             mask[indices.astype(int)] = True\r\n    834             # mask fails to broadcast when passed to where; broadcast manually.\r\n    835             mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\r\n\r\nAttributeError: \'list\' object has no attribute \'astype\'\r\n```\r\n\r\nThe problem I have identified is in the _apply_filter method of _GroupBy class (core/groupby.py) -- line with ""mask[indices.astype(int)] = True"" throws because in my case indices is equal to []; shouldn\'t it be ""indices = np.array([])"" instead of ""indices = []"" in the case when len(indices) == 0\r\n\r\n```python\r\n    def _apply_filter(self, indices, dropna):\r\n        if len(indices) == 0:\r\n            indices = []\r\n        else:\r\n            indices = np.sort(np.concatenate(indices))\r\n        if dropna:\r\n            filtered = self._selected_obj.take(indices, axis=self.axis)\r\n        else:\r\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\r\n            mask.fill(False)\r\n            mask[indices.astype(int)] = True\r\n            # mask fails to broadcast when passed to where; broadcast manually.\r\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\r\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\r\n        return filtered\r\n```\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.DataFrame({\'a\': [1,1,2], \'b\': [1,2,0]}).groupby(\'a\').filter(lambda x: x[\'b\'].sum() > 5, dropna=False)\r\n```\r\n#### Expected Output\r\n```python\r\n    a   b\r\n0 NaN NaN\r\n1 NaN NaN\r\n2 NaN NaN\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n```python\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-56-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 1.5.6\r\nsetuptools: 12.2\r\nCython: 0.23.4\r\nnumpy: 1.11.0\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.0.3\r\nsphinx: None\r\npatsy: 0.4.0\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.7.6\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: 0.6.6.None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n```'"
12757,144896286,onesandzeroes,jreback,2016-03-31 13:08:17,2016-03-31 13:15:40,2016-03-31 13:15:40,closed,,0.18.1,1,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/12757,b'BUG: loffset argument not applied for resample().count() on timeseries',"b' - [x] closes #12725\r\n - [ ] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nThe code to do this already existed in the `_downsample` method, which is called when using functions like `mean`. `max`, etc., but there was nothing in the `_groupby_and_aggregate` method used for `count` and `size`. If we pull out the offset code from the `_downsample` method into a separate method, we can reuse it without duplicating it.'"
12742,144437697,sinhrks,jreback,2016-03-30 01:54:22,2016-03-30 12:40:56,2016-03-30 12:39:02,closed,,0.18.1,1,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/12742,b'BUG: SparseSeries.shape ignores fill_value',"b' - [x] closes #10452\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nBased on existing tests / text search, no other metohd depends on current ``SparseSeries.shape`` (buggy) behavior.'"
12726,144113437,TurnaevEvgeny,jreback,2016-03-28 23:50:52,2016-03-29 01:05:36,2016-03-29 01:05:36,closed,,,1,Bug;Duplicate;Indexing,https://api.github.com/repos/pydata/pandas/issues/12726,"b""UnboundLocalError: local variable 'mask' referenced before assignment""","b'#### Code Sample, a copy-pastable example if possible\r\npd.to_datetime(pd.Series([\'Jul 31, 2009\', \'2010-01-10\', None])).to_frame().replace(\'\', np.nan)\r\n\r\n```\r\nFile ""/var/www/envs/fring_internal/local/lib/python2.7/site-packages/pandas/core/generic.py"", line 3110, in replace\r\n    inplace=inplace, regex=regex)\r\n  File ""/var/www/envs/fring_internal/local/lib/python2.7/site-packages/pandas/core/internals.py"", line 2870, in replace\r\n    return self.apply(\'replace\', **kwargs)\r\n  File ""/var/www/envs/fring_internal/local/lib/python2.7/site-packages/pandas/core/internals.py"", line 2823, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""/var/www/envs/fring_internal/local/lib/python2.7/site-packages/pandas/core/internals.py"", line 607, in replace\r\n    if not mask.any():\r\nUnboundLocalError: local variable \'mask\' referenced before assignment\r\n```\r\n\r\n#### Expected Output\r\nNot sure, raise a TypeError(""datetime64[ns] is not coercible to string"") probably.\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.0.3\r\nsetuptools: 17.0\r\nCython: 0.23.4\r\nnumpy: 1.8.2\r\nscipy: 0.16.0\r\nstatsmodels: None\r\nIPython: 3.2.0\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 1.5\r\npytz: 2013.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.3.0\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.8\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\nJinja2: None\r\n'"
12723,144011686,jcrist,jreback,2016-03-28 15:58:32,2016-04-17 14:13:08,2016-04-17 14:13:08,closed,,0.18.1,3,Bug;Difficulty Intermediate;Effort Low;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12723,b'Unexpected behavior with binary operators and fill_value',"b""From https://github.com/dask/dask/issues/1063. `fill_value` doesn't seem to apply if the argument to a binary operator is a constant, but works fine for other arguments:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame(range(10), columns=['foo'])\r\n\r\nIn [3]: df2 = df.copy()\r\n\r\nIn [4]: df2.iloc[0] = None\r\n\r\nIn [5]: df2.add(2, fill_value=0)\r\nOut[5]:\r\n    foo\r\n0   NaN\r\n1   3.0\r\n2   4.0\r\n3   5.0\r\n4   6.0\r\n5   7.0\r\n6   8.0\r\n7   9.0\r\n8  10.0\r\n9  11.0\r\n\r\nIn [6]: df2.add(df, fill_value=0)\r\nOut[6]:\r\n    foo\r\n0   0.0\r\n1   2.0\r\n2   4.0\r\n3   6.0\r\n4   8.0\r\n5  10.0\r\n6  12.0\r\n7  14.0\r\n8  16.0\r\n9  18.0\r\n\r\nIn [7]: pd.__version__\r\nOut[7]: u'0.18.0rc2+2.g19e40a0'\r\n```\r\n\r\nFrom the docstring I'd expect it to be equivalent to `df2.fillna(0).add(2)`. If this is intended behavior, then the docstring should be updated to clarify this.\r\n"""
12716,143566516,jbandlow,jreback,2016-03-25 18:48:36,2016-04-06 20:40:14,2016-04-06 20:40:14,closed,,0.18.1,3,Bug;Difficulty Advanced;Effort Low;Groupby;Timezones,https://api.github.com/repos/pydata/pandas/issues/12716,b'BUG: groupby.first() corrupts timezone',"b""#### Code Sample, a copy-pastable example if possible\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame([{'ts': pd.Timestamp('2016-01-01', tz='America/Los_Angeles'), 'a': 1}])\r\ndf.groupby('a').first()\r\n```\r\nThe output is\r\n```\r\n                         ts\r\na                          \r\n1 2016-01-01 08:00:00-08:00\r\n```\r\n\r\n#### Expected Output\r\n```\r\n                         ts\r\na                          \r\n1 2016-01-01 00:00:00-08:00\r\n```\r\nNote that the issue is with `first()` and not `groupby`:\r\n```\r\nfor _, group in df.groupby('a'):\r\n    print(group.ix[0])\r\n```\r\ngives the correct output of \r\n```\r\na                             1\r\nts    2016-01-01 00:00:00-08:00\r\nName: 0, dtype: object\r\n```\r\n  \r\n#### output of ``pd.show_versions()``\r\n```\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-60-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.1\r\npip: 1.5.4\r\nsetuptools: 3.3\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: 0.13.3\r\nstatsmodels: 0.5.0\r\nxarray: None\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: 0.2.1\r\ndateutil: 2.5.1\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.7.0\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.3\r\nbs4: 4.2.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n```"""
12713,143363250,jreback,jreback,2016-03-24 21:22:04,2016-03-25 12:54:42,2016-03-25 12:54:42,closed,,0.18.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/12713,b'BUG: Bug in groupby.transform(..) when axis=1 is specified with a non_monotonic indexer',
12702,143019857,jreback,jreback,2016-03-23 16:59:58,2016-03-23 17:56:45,2016-03-23 17:56:45,closed,,0.18.1,0,Bug;Dtypes;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12702,b'BUG: concatenation with a coercable dtype was too aggressive',b'closes #12411\r\ncloses #12045\r\ncloses #11594\r\ncloses #10571'
12701,142975915,emsems,jreback,2016-03-23 14:33:44,2016-04-03 17:52:38,2016-04-03 17:52:22,closed,,0.18.1,1,Bug;Difficulty Novice;Effort Low;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/12701,b'DataFrame.drop() does nothing for non-unique MultiIndex when attempting to drop from a level with DatetimeIndex',"b""Hi,\r\nthere seems to be a bug in DataFrame.drop() when the DataFrame has a non-unique MultiIndex and one of the levels is a DatetimeIndex (labels of which I would like to pass to the drop-method)\r\n\r\n#### Code Sample\r\n```\r\nimport pandas as pd\r\nfrom pandas import DataFrame\r\nimport numpy as np\r\n\r\n# Prepare DataFrame\r\nidx = pd.Index([0, 0, 1, 1, 1, 2, 3, 4, 4, 5], name='id')\r\nidxdt = pd.to_datetime(['201603231200',\r\n                        '201603231200',\r\n                        '201603231300',\r\n                        '201603231300',\r\n                        '201603231400',\r\n                        '201603231400',\r\n                        '201603231500',\r\n                        '201603231600',\r\n                        '201603231600',\r\n                        '201603231700'])\r\ndf = DataFrame(np.arange(30).reshape(10, 3), columns=list('abc'), index=idx)\r\ndf['tstamp'] = idxdt\r\ndf = df.set_index('tstamp', append=True)\r\nprint df\r\n\r\n# Drop the following timestamp\r\nts = pd.Timestamp('201603231600')\r\ndf = df.drop(ts, level='tstamp')\r\nprint df\r\n```\r\n#### Expected Output\r\n```\r\n                         a   b   c\r\nid tstamp\r\n0  2016-03-23 12:00:00   0   1   2\r\n   2016-03-23 12:00:00   3   4   5\r\n1  2016-03-23 13:00:00   6   7   8\r\n   2016-03-23 13:00:00   9  10  11\r\n   2016-03-23 14:00:00  12  13  14\r\n2  2016-03-23 14:00:00  15  16  17\r\n3  2016-03-23 15:00:00  18  19  20\r\n4  2016-03-23 16:00:00  21  22  23\r\n   2016-03-23 16:00:00  24  25  26\r\n5  2016-03-23 17:00:00  27  28  29\r\n                         a   b   c\r\nid tstamp\r\n0  2016-03-23 12:00:00   0   1   2\r\n   2016-03-23 12:00:00   3   4   5\r\n1  2016-03-23 13:00:00   6   7   8\r\n   2016-03-23 13:00:00   9  10  11\r\n   2016-03-23 14:00:00  12  13  14\r\n2  2016-03-23 14:00:00  15  16  17\r\n3  2016-03-23 15:00:00  18  19  20\r\n5  2016-03-23 17:00:00  27  28  29\r\n```\r\n\r\n#### Current Output\r\n```\r\n                         a   b   c\r\nid tstamp\r\n0  2016-03-23 12:00:00   0   1   2\r\n   2016-03-23 12:00:00   3   4   5\r\n1  2016-03-23 13:00:00   6   7   8\r\n   2016-03-23 13:00:00   9  10  11\r\n   2016-03-23 14:00:00  12  13  14\r\n2  2016-03-23 14:00:00  15  16  17\r\n3  2016-03-23 15:00:00  18  19  20\r\n4  2016-03-23 16:00:00  21  22  23\r\n   2016-03-23 16:00:00  24  25  26\r\n5  2016-03-23 17:00:00  27  28  29\r\n                         a   b   c\r\nid tstamp\r\n0  2016-03-23 12:00:00   0   1   2\r\n   2016-03-23 12:00:00   3   4   5\r\n1  2016-03-23 13:00:00   6   7   8\r\n   2016-03-23 13:00:00   9  10  11\r\n   2016-03-23 14:00:00  12  13  14\r\n2  2016-03-23 14:00:00  15  16  17\r\n3  2016-03-23 15:00:00  18  19  20\r\n4  2016-03-23 16:00:00  21  22  23\r\n   2016-03-23 16:00:00  24  25  26\r\n5  2016-03-23 17:00:00  27  28  29\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 0.6\r\nCython: 0.23.3\r\nnumpy: 1.10.4\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.0.3\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.5.1\r\npytz: 2016.2\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.0.2\r\nxlrd: None\r\nxlwt: 1.0.0\r\nxlsxwriter: None\r\nlxml: 3.5.0\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.8\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.7.3\r\nboto: None\r\n```\r\n"""
12698,142831834,ningchi,jreback,2016-03-23 01:44:16,2016-04-03 14:06:15,2016-04-03 14:06:01,closed,,0.18.1,7,Bug;Categorical;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12698,b'Fix #12564 for Categorical: consistent result if comparing as DataFrame',b' - [x] closes #12564 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12685,142522881,markroth8,jorisvandenbossche,2016-03-22 02:01:09,2016-07-05 08:37:17,2016-07-05 08:37:17,closed,,0.19.0,2,Bug;Difficulty Intermediate;Effort Low;Indexing;MultiIndex;Timeseries,https://api.github.com/repos/pydata/pandas/issues/12685,b'BUG: IndexSlice on MultiIndex includes out-of-range rows',"b""#### Code Sample, a copy-pastable example if possible\r\n```python\r\n>>> dft = pd.DataFrame(np.random.randn(100000,1),columns=['A'],index=pd.date_range('20130101',periods=100000,freq='T'))\r\n>>> dft2 = pd.DataFrame(np.random.randn(200000,1),columns=['A'],index=pd.MultiIndex.from_product([dft.index, ['a', 'b']]))\r\n>>> dft2.loc[pd.IndexSlice['2013-03':'2013-03',:],:]\r\n                              A\r\n2013-01-01 00:00:00 a  0.563968\r\n2013-01-01 00:01:00 a -0.439376\r\n2013-01-01 00:02:00 a  1.785202\r\n2013-01-01 00:03:00 a  0.376901\r\n2013-01-01 00:04:00 a -0.977926\r\n2013-01-01 00:05:00 a -0.738415\r\n2013-01-01 00:06:00 a -2.156905\r\n2013-01-01 00:07:00 a -0.016085\r\n2013-01-01 00:08:00 a  1.035935\r\n2013-01-01 00:09:00 a -1.198991\r\n2013-01-01 00:10:00 a -0.867717\r\n2013-01-01 00:11:00 a -0.596923\r\n2013-01-01 00:12:00 a -1.256923\r\n2013-01-01 00:13:00 a  1.042369\r\n2013-01-01 00:14:00 a  1.161365\r\n2013-01-01 00:15:00 a  0.853495\r\n2013-01-01 00:16:00 a  1.398594\r\n2013-01-01 00:17:00 a -0.431314\r\n2013-01-01 00:18:00 a  2.630920\r\n2013-01-01 00:19:00 a -1.031731\r\n2013-01-01 00:20:00 a -0.891799\r\n2013-01-01 00:21:00 a  0.075546\r\n2013-01-01 00:22:00 a -0.163999\r\n2013-01-01 00:23:00 a  0.678027\r\n2013-01-01 00:24:00 a  1.427323\r\n2013-01-01 00:25:00 a  1.426418\r\n2013-01-01 00:26:00 a  0.492158\r\n2013-01-01 00:27:00 a  0.477629\r\n2013-01-01 00:28:00 a -0.703225\r\n2013-01-01 00:29:00 a  0.864293\r\n...                         ...\r\n2013-03-11 10:25:00 a  1.585880\r\n                    b -0.595927\r\n2013-03-11 10:26:00 a  0.259137\r\n                    b -0.718385\r\n2013-03-11 10:27:00 a -0.143240\r\n                    b -0.898806\r\n2013-03-11 10:28:00 a -0.293221\r\n                    b -1.645180\r\n2013-03-11 10:29:00 a -0.790069\r\n                    b -1.075649\r\n2013-03-11 10:30:00 a  0.368399\r\n                    b  2.206858\r\n2013-03-11 10:31:00 a  1.125287\r\n                    b -0.834588\r\n2013-03-11 10:32:00 a  0.740703\r\n                    b -0.587527\r\n2013-03-11 10:33:00 a  0.765259\r\n                    b -0.662232\r\n2013-03-11 10:34:00 a  2.138155\r\n                    b -0.030379\r\n2013-03-11 10:35:00 a -1.510801\r\n                    b  1.200521\r\n2013-03-11 10:36:00 a  0.934974\r\n                    b  1.340875\r\n2013-03-11 10:37:00 a -0.251199\r\n                    b  1.728432\r\n2013-03-11 10:38:00 a  1.651664\r\n                    b -1.032225\r\n2013-03-11 10:39:00 a -0.352417\r\n                    b  0.378273\r\n\r\n[115040 rows x 1 columns]\r\n```\r\n#### Expected Output\r\nData from 2013-01-01 through 2013-03-03 should not be present. Data from 2013-03-04 onward should not be present, either.\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 25cec3adf044982accb78384acab5df94c1435c7\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.2.0-34-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0+31.g25cec3a\r\nnose: 1.3.7\r\npip: 8.0.3\r\nsetuptools: 20.1.1\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.1\r\nsphinx: 1.3.5\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None"""
12675,142156913,sinhrks,jreback,2016-03-20 12:06:58,2016-03-29 20:10:03,2016-03-29 20:09:45,closed,,0.18.1,3,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/12675,b'BUG: .describe lost CategoricalIndex info',"b' - [x] closes #11558\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n#11558 is partially fixed by #12531, this PR let ``.describe()`` to preserve ``CategoricalIndex`` info. And added explicit test cases derived from #11558.\r\n\r\n'"
12659,141588734,gdementen,jreback,2016-03-17 13:50:50,2016-04-22 15:09:57,2016-04-22 15:09:57,closed,,0.18.1,2,Bug;Difficulty Intermediate;Effort Low;IO SAS,https://api.github.com/repos/pydata/pandas/issues/12659,b'BUG: Read SAS fails',"b'cc: @kshedden\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\nhttps://github.com/pydata/pandas/blob/8857008178eb90b59ca67874507ef860b3faf88f/pandas/io/tests/sas/data/test17.sas7bdat\r\n\r\nDo you want a PR with this test case?\r\n\r\n#### Expected Output\r\n\r\npandas reads the file without crashing :)\r\nI tracked the problem to _process_columntext_subheader. It seems like there is an encoding issue...\r\n\r\nYou might need to apply  PR #12658 first to see this issue.\r\n\r\nFWIW, here is an AWFUL HACK to make pandas read the file ""almost"" correctly (I get column names as bytes instead of str in this case).\r\n\r\n```diff\r\n@@ -524,17 +524,17 @@ class SAS7BDATReader(BaseIterator):\r\n \r\n         offset += self._int_length\r\n         text_block_size = self._read_int(offset, _text_block_size_length)\r\n \r\n         buf = self._read_bytes(offset, text_block_size)\r\n-        self.column_names_strings.append(\r\n-            buf[0:text_block_size].rstrip(b""\\x00 "").decode())\r\n+        self.column_names_strings.append(buf)\r\n         if len(self.column_names_strings) == 1:\r\n             column_name = self.column_names_strings[0]\r\n             compression_literal = """"\r\n             for cl in _compression_literals:\r\n+                cl = bytes(cl, \'ascii\')\r\n                 if cl in column_name:\r\n                     compression_literal = cl\r\n             self.compression = compression_literal\r\n             offset -= self._int_length\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.0\r\nsetuptools: 20.2.2\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n'"
12658,141583872,gdementen,jreback,2016-03-17 13:30:36,2016-04-22 15:14:34,2016-04-22 15:14:34,closed,,0.18.1,7,Bug;IO SAS,https://api.github.com/repos/pydata/pandas/issues/12658,b'BUG: fixed SAS7BDATReader._get_properties',b'this fixes a crasher I had in one of my files using SAS7BDATReader.\r\n\r\ncc: @kshedden \r\n\r\nself._path_or_buf.read is an obvious mistake\r\n\r\nalso added + total_align to _os_name_offset and _os_make_offset as they are\r\npresent in the original code from Jared Hobbs:\r\nhttps://bitbucket.org/jaredhobbs/sas7bdat/src/da1faa90d0b15c2c97a2a8eb86c91c58081bdd86/sas7bdat.py?fileviewer=file-view-default#sas7bdat.py-1450'
12650,141460762,OXPHOS,jreback,2016-03-17 02:29:44,2016-04-17 14:31:42,2016-04-17 14:31:40,closed,,0.18.1,5,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12650,b'BUG: pivot_table dropna=False drops columns/index names',"b' - [X] closes #12133 \r\n - [X] closes #12642, closes #12327 (replace PR)\r\n - [X] tests added / passed\r\n - [X] passes ``git diff upstream/master | flake8 --diff``\r\n - [X] whatsnew entry\r\n\r\nBug fixed from pull request #12327.\r\n\r\n'"
12647,141397018,raderaj,jreback,2016-03-16 20:44:32,2016-04-22 15:09:57,2016-04-22 15:09:57,closed,,0.18.1,12,Bug;IO SAS,https://api.github.com/repos/pydata/pandas/issues/12647,b'TypeError with pd.read_sas() (pandas 0.18.0)',"b""#### Code Sample, a copy-pastable example if possible\r\ndf = pd.read_sas('infilename.sas7bdat')\r\n#### Expected Output\r\nto read a sas7bdat file into a pandas data frame.\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.0\r\nsetuptools: 20.2.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.0.3\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.5.0\r\npytz: 2016.1\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.39.0\r\n---------------------------------------------------------------------------\r\n### Error seen\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-3046a8f15981> in <module>()\r\n----> 1 pd.read_sas('mydatainfo.sas7bdat')\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sas\\sasreader.py in read_sas(filepath_or_buffer, format, index, encoding, chunksize, iterator)\r\n     52         reader = SAS7BDATReader(filepath_or_buffer, index=index,\r\n     53                                 encoding=encoding,\r\n---> 54                                 chunksize=chunksize)\r\n     55     else:\r\n     56         raise ValueError('unknown SAS format')\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sas\\sas7bdat.py in __init__(self, path_or_buf, index, convert_dates, blank_missing, chunksize, encoding)\r\n    234             self._path_or_buf = open(self._path_or_buf, 'rb')\r\n    235 \r\n--> 236         self._get_properties()\r\n    237         self._parse_metadata()\r\n    238 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sas\\sas7bdat.py in _get_properties(self)\r\n    333             self.os_name = buf.rstrip(b'\\x00 ').decode()\r\n    334         else:\r\n--> 335             buf = self._path_or_buf.read(_os_maker_offset, _os_maker_length)\r\n    336             self.os_name = buf.rstrip(b'\\x00 ').decode()\r\n    337 \r\n\r\nTypeError: read() takes at most 1 argument (2 given)\r\n\r\n---------------------------------------------------------------------------\r\n## Tracking down the error:\r\n\r\nIt looks like line 335 should either be set up to read a single length from the _path_or_buf bytestream something like:\r\n``` buf = self._path_or_buf.read(_os_maker_length) ``` \r\nor replace it with something like this\r\n``` buf = self._read_bytes(_os_maker_offset, _os_maker_length) ```\r\n\r\n---------------------------------------------------------------------------\r\n** note i tried this under python3.5 and got the same error."""
12643,141306363,OXPHOS,OXPHOS,2016-03-16 15:18:46,2016-03-16 21:09:48,2016-03-16 20:40:49,closed,,,5,Bug;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12643,b'BUG: crosstab with margins=True and dropna=False ',b' - [+] closes #12642 \r\n - [+] tests added / passed\r\n - [+] passes ``git diff upstream/master | flake8 --diff``\r\n - [+] whatsnew entry\r\n'
12642,141272624,jreback,jreback,2016-03-16 13:19:48,2016-04-17 14:31:40,2016-04-17 14:31:40,closed,,0.18.1,3,Bug;Difficulty Intermediate;Effort Low;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12642,b'BUG: crosstab with margins=True and dropna=False',"b""xref #12614 \r\n```\r\nIn [1]: df = pd.DataFrame({'a': [1, 2, 2, 2, 2, np.nan],'b': [3, 3, 4, 4, 4, 4]})\r\n\r\nIn [2]: pd.crosstab(df.a, df.b, margins=True, dropna=True)\r\nOut[2]: \r\nb    3  4  All\r\na             \r\n1.0  1  0    1\r\n2.0  1  3    4\r\nAll  2  3    5\r\n\r\nIn [3]: pd.crosstab(df.a, df.b, margins=True, dropna=False)\r\nKeyError: 'Level None not found'\r\n```"""
12635,141101295,sinhrks,jreback,2016-03-15 21:19:19,2016-03-17 14:17:55,2016-03-17 14:17:46,closed,,0.18.1,2,Bug;Reshaping;Timedelta;Timezones,https://api.github.com/repos/pydata/pandas/issues/12635,b'BUG: Concat with tz-aware and timedelta raises AttributeError',"b"" - [x] closes #12620\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n@jreback 's comment on #12620:\r\n\r\n> should be handled at a slightly higher level.\r\n\r\nI added number of ``dtype`` check logic in ``tseries/common/_concat_compat``. But it should work on ``base/common/_concat_compat``. Lmk which is preferable.\r\n"""
12633,140977798,Xbar,jreback,2016-03-15 13:51:01,2016-03-17 13:54:26,2016-03-17 13:54:20,closed,,0.18.1,20,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12633,b'BUG: #12624 where Panel.fillna() ignores inplace=True',b'Added if condition for fillna when ndim==3 (for Panel) Issue: https://github.com/pydata/pandas/issues/12624'
12630,140870902,PedroMDuarte,jreback,2016-03-15 04:39:51,2016-03-16 00:13:32,2016-03-15 23:49:22,closed,,,4,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12630,b'BUG: #12624 where Panel.fillna() ignores inplace=True',b' - [x] closes #12624 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nFixes #12624\r\n\r\nAuthor: Pedro M Duarte <pmd323@gmail.com>'
12626,140812027,TomAugspurger,jreback,2016-03-14 22:14:24,2016-03-17 14:06:59,2016-03-17 14:06:48,closed,,0.18.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/12626,b'BUG: .rename* not treating Series as mapping',b'Closes https://github.com/pydata/pandas/issues/12623\r\n\r\nI added com.is_dict_like in https://github.com/pydata/pandas/pull/11980\r\nand failed to use it for the `rename` method. Using that now and did\r\nsome refactoring while I was in there. Added more tests for rename.'
12624,140782639,seth-p,jreback,2016-03-14 20:01:35,2016-03-17 13:54:50,2016-03-17 13:54:50,closed,,0.18.1,2,Bug;Difficulty Novice;Effort Low;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12624,b'BUG: Panel.fillna() ignores inplace=True',"b""I'm not sure of ``Panel`` is still being supported, but ``Panel.fillna()`` seems to ignore ``inplace=True``:\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [5]: p1 = pd.Panel([[[0,1],[2,np.nan]], [[10,11],[12,np.nan]]], items=['a','b'], minor_axis=['x','y'])\r\n\r\nIn [7]: p1['a']\r\nOut[7]:\r\n     x    y\r\n0  0.0  1.0\r\n1  2.0  NaN\r\n\r\nIn [8]: p1['b']\r\nOut[8]:\r\n      x     y\r\n0  10.0  11.0\r\n1  12.0   NaN\r\n\r\nIn [9]: p1.fillna(method='ffill', inplace=True)\r\nOut[9]:\r\n<class 'pandas.core.panel.Panel'>\r\nDimensions: 2 (items) x 2 (major_axis) x 2 (minor_axis)\r\nItems axis: a to b\r\nMajor_axis axis: 0 to 1\r\nMinor_axis axis: x to y\r\n\r\nIn [10]: p1['a']\r\nOut[10]:\r\n     x    y\r\n0  0.0  1.0\r\n1  2.0  NaN\r\n\r\nIn [11]: p1['b']\r\nOut[11]:\r\n      x     y\r\n0  10.0  11.0\r\n1  12.0   NaN\r\n\r\nIn [12]: p2 = p1.fillna(method='ffill', inplace=True)\r\n\r\nIn [13]: p2['a']\r\nOut[13]:\r\n     x    y\r\n0  0.0  1.0\r\n1  2.0  1.0\r\n\r\nIn [14]: p2['b']\r\nOut[14]:\r\n      x     y\r\n0  10.0  11.0\r\n1  12.0  11.0\r\n\r\nIn [15]: pd.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.0\r\nsetuptools: 20.2.2\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: 0.7.1\r\nIPython: 4.1.2\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.0\r\npytz: 2016.1\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.5\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.3\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: 1.0b8\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\n```\r\n"""
12623,140777005,TomAugspurger,jreback,2016-03-14 19:38:08,2016-03-17 14:06:48,2016-03-17 14:06:48,closed,,0.18.1,4,Bug;Difficulty Novice;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12623,b'BUG: Series.rename treats Series as list-like instead of mapping',"b'```python\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\nIn [6]: s = pd.Series([0, 1])\r\n\r\nIn [5]: s.rename(pd.Series({0: 10, 1: 20}))\r\nOut[5]:\r\n0    0\r\n1    1\r\nName: [10, 20], dtype: int64\r\n```\r\n\r\nProblem is [here](https://github.com/pydata/pandas/blob/4f5099b4ee5f5c32f6b299de93902c9a074ac228/pandas/core/series.py#L2336) using an `isinstance(,MutableMapping)` instead of `com.is_dict_like`. [`rename_axis`](https://github.com/pydata/pandas/blob/4f5099b4ee5f5c32f6b299de93902c9a074ac228/pandas/core/generic.py#L705) should be fine, but will add tests for both.'"
12620,140640306,sinhrks,jreback,2016-03-14 10:56:01,2016-03-17 14:17:46,2016-03-17 14:17:46,closed,,0.18.1,1,Bug;Difficulty Intermediate;Effort Low;Reshaping;Timedelta;Timezones,https://api.github.com/repos/pydata/pandas/issues/12620,b'BUG: Concat with tz-aware and timedelta raises AttributeError',"b""#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\npd.concat([pd.Series([pd.Timestamp('2011-01-01', tz='US/Eastern')]),\r\n           pd.Series([pd.Timedelta('1 day')])])\r\n# AttributeError: 'numpy.ndarray' object has no attribute 'tz_localize'\r\n```\r\n\r\n#### Expected Output\r\n\r\n```\r\npd.Series([pd.Timestamp('2011-01-01', tz='US/Eastern'), pd.Timedelta('1 day')])\r\n# 0    2011-01-01 00:00:00-05:00\r\n# 1              1 days 00:00:00\r\n# dtype: object\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\ncurrent master.\r\n\r\nIt is caused by the below line, we must check ``typs`` length must be 1, otherwise the result should be ``object`` dtype.\r\n\r\nhttps://github.com/pydata/pandas/blob/master/pandas/tseries/common.py#L278\r\n\r\n"""
12617,140604650,pwaller,jreback,2016-03-14 08:17:11,2016-04-10 14:26:28,2016-04-10 14:26:28,closed,,0.18.1,2,Bug;Difficulty Intermediate;Effort Low;Strings,https://api.github.com/repos/pydata/pandas/issues/12617,b'BUG: Index.str.partition gives ValueError while trying to compute index name (expand=False)',"b'`Index.str.partition`\'s behaviour changed in 3ab35b40274276e4f9b40918a7e260f1abb941c1 in a backwards incompatible way which doesn\'t seem intentional.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\npython3 -c \'import pandas; pandas.Index([""a,b"", ""c,d""], name=""hello"").str.partition("","")\'\r\n```\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/home/pwaller/.local/src/pandas/pandas/core/strings.py"", line 1432, in partition\r\n    return self._wrap_result(result, expand=expand)\r\n  File ""/home/pwaller/.local/src/pandas/pandas/core/strings.py"", line 1348, in _wrap_result\r\n    return MultiIndex.from_tuples(result, names=name)\r\n  File ""/home/pwaller/.local/src/pandas/pandas/indexes/multi.py"", line 889, in from_tuples\r\n    return MultiIndex.from_arrays(arrays, sortorder=sortorder, names=names)\r\n  File ""/home/pwaller/.local/src/pandas/pandas/indexes/multi.py"", line 844, in from_arrays\r\n    names=names, verify_integrity=False)\r\n  File ""/home/pwaller/.local/src/pandas/pandas/indexes/multi.py"", line 92, in __new__\r\n    result._set_names(names)\r\n  File ""/home/pwaller/.local/src/pandas/pandas/indexes/multi.py"", line 446, in _set_names\r\n    raise ValueError(\'Length of names must match number of levels in \'\r\nValueError: Length of names must match number of levels in MultiIndex.\r\n```\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [1]: import pandas; pandas.Index([""a,b"", ""c,d""], name=""hello"").str.partition("","")\r\nOut[1]: \r\nMultiIndex(levels=[[\'a\', \'c\'], [\',\'], [\'b\', \'d\']],\r\n           labels=[[0, 1], [0, 0], [0, 1]])\r\n```\r\n\r\n#### Versions\r\n\r\nI bisected the problem to 3ab35b40274276e4f9b40918a7e260f1abb941c1 (cc @sinhrks). The problem is not present in 0.17.1 and is present in 0.18.0rc2. \r\n\r\nNote: when I use a `name=""foo""`, it happens to work because it has the correct length, the index name ends up being `[""f"", ""o"", ""o""]`, so it\'s possible this is tested somewhere and happens to work even though it\'s not correct for a large number of use cases.'"
12615,140598950,sinhrks,jreback,2016-03-14 07:40:23,2016-03-17 16:56:16,2016-03-17 16:56:09,closed,,0.18.1,6,Bug;Output-Formatting;Period,https://api.github.com/repos/pydata/pandas/issues/12615,b'BUG: Mixed period cannot be displayed with ValueError',"b"" - [x] closes #xxxx (not reported)\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n```\r\n# we can create Series contains Periods with mixed freq\r\ns = pd.Series([pd.Period('2011-01', freq='M'), pd.Period('2011-02-01', freq='D')])\r\n\r\n# print the created Series raises ValueError\r\nprint(s)\r\n# ValueError: Input has different freq=D from PeriodIndex(freq=M)\r\n"""
12614,140594122,OXPHOS,jreback,2016-03-14 07:12:22,2016-03-16 15:14:04,2016-03-16 13:17:21,closed,,0.18.1,14,Bug;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12614,b'BUG: Crosstab margins ignoring dropna',b'To fix bug #12577 : Crosstab margins ignoring dropna\r\n\r\n'
12610,140506850,chinskiy,jreback,2016-03-13 17:11:38,2016-03-25 13:30:15,2016-03-25 13:30:15,closed,,0.18.1,2,Bug;Compat;Difficulty Novice;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12610,b'Bug: Pandas Series name attribute can be array',"b""#### Code Sample\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nndarr = np.array([1,2,3])\r\nseries1 = pd.Series(ndarr, name=['series1'])\r\nseries2 = pd.Series([1,2,3], name='series2')\r\n\r\nseries1.name\r\n##['series1']\r\n\r\nresult_df = pd.concat([series1, series2], axis=1)\r\nresult_df\r\n##give **type error**\r\n\r\n##also\r\n\r\nseries3 = pd.Series(ndarr, name=['series1', 'series2'])\r\nseries3.name\r\n##['series1', 'series2']\r\n```\r\n\r\n#### Expected Output\r\nget error when result_df try to be created with array name attribute,\r\nor give error when try to create Series object with array argument in name attribute.\r\n#### output of ``pd.show_versions()``\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.2.0-30-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: 1.3.7\r\npip: 8.1.0\r\nsetuptools: 20.2.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: 1.3.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.0\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.3\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: 1.0b8\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None"""
12599,140280015,TulyOpt,jreback,2016-03-11 20:13:35,2016-05-23 13:05:43,2016-05-23 13:05:43,closed,,0.19.0,1,Bug;Difficulty Intermediate;Dtypes;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/12599,b'Error : Invalid type promotion',"b'#### Code Sample\r\n\r\nimport pandas as pd\r\n\r\ndef main():\r\n\r\n    df1 = pd.Series() #dtype = float64\r\n    df1[""a""] = pd.Timestamp(""2014-01-02"") #dtype = datetime64\r\n    df1[""b""] = ""z"" # OK ! => dtype = object\r\n    df1[""c""] = 3.0 # OK! => dtype = object\r\n\r\n    df2 = pd.Series() #dtype = float64\r\n    df2[""a""] = pd.Timestamp(""2014-01-02"") #dtype = datetime64\r\n    df2[""c""] = 3.0 #BOOM !! => TypeError: invalid type promotion\r\n    df2[""b""] = ""z""\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 8.1\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 8.0.2\r\nsetuptools: 19.6.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.3\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8'"
12577,139712882,nickeubank,jreback,2016-03-09 21:57:10,2016-03-16 13:17:21,2016-03-16 13:17:21,closed,,0.18.1,4,Bug;Difficulty Novice;Effort Low;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12577,b'BUG: Crosstab margins ignoring dropna',"b""`crosstab` also has a bug -- it counts np.nan in margin totals even when `dropna=True`.\r\n\r\nAppears independent of #12569 and #4003\r\n\r\n    df = pd.DataFrame({'a':[1,2,2,2,2,np.nan],'b':[3,3,4,4,4,4]})\r\n    pd.crosstab(df.a,df.b, margins=True)\r\n    Out[233]: \r\n    b    3  4  All\r\n    a             \r\n    1.0  1  0    1\r\n    2.0  1  3    4\r\n    All  2  4    6\r\n\r\n#### Expected Output\r\n\r\n    Out[233]: \r\n    b    3  4  All\r\n    a             \r\n    1.0  1  0    1\r\n    2.0  1  3    4\r\n    All  2  3    5\r\n\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.4.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 8.1.0\r\nsetuptools: 20.2.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.4.4\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8"""
12576,139709102,nickeubank,nickeubank,2016-03-09 21:41:52,2016-03-14 01:20:23,2016-03-14 01:20:23,closed,,,4,Bug;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12576,"b'BUG: value_count(normalize=True, dropna=True) counting missing in denominator, #12558 '",b'Closes #12558 \r\n\r\n'
12574,139657229,chris-b1,jreback,2016-03-09 18:12:26,2016-03-16 13:59:42,2016-03-16 13:59:42,closed,,Next Major Release,1,Bug;Categorical;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12574,b'BUG: Series with Categorical and dtype',"b""xref https://github.com/pydata/pandas/pull/12573#discussion_r55561598\r\n\r\n```python\r\nIn [37]: pd.Series(pd.Categorical([1,2,3]), dtype='category')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-37-87a1a3228158> in <module>()\r\n----> 1 pd.Series(pd.Categorical([1,2,3]), dtype='category')\r\n\r\nValueError: cannot specify a dtype with a Categorical\r\n```\r\n"""
12566,139435338,kawochen,jreback,2016-03-09 00:29:03,2016-03-12 17:46:56,2016-03-12 17:46:32,closed,,0.18.1,4,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12566,b'BUG: GH12558 where nulls contributed to normalized value_counts',b' - [x] closes #12558\r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\n'
12564,139413351,pganssle,jreback,2016-03-08 22:29:11,2016-04-03 14:06:00,2016-04-03 14:06:00,closed,,0.18.1,7,Bug;Categorical;Difficulty Novice;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/12564,b'Categorical equality check raises ValueError in DataFrame',"b'Apparently there\'s an issue when comparing the equality of a scalar value against a categorical column as part of a DataFrame. In the example below, I\'m checking against `-np.inf`, but comparing to a string or integer gives the same results.\r\n\r\nThis raises `ValueError: Wrong number of dimensions`.\r\n\r\n#### Code Sample\r\n```python\r\nfrom sys import version\r\nimport pandas as pd     # Version 0.17.1 on Linux and Windows\r\nimport numpy as np\r\nprint(version)\r\nprint(pd.__version__)\r\n\r\n# Arbitrary data set\r\ncolumns = [\'Name\', \'Type\', \'Age\', \'Weight (kg)\', \'Cuteness\']\r\ndataset = [[\'Snuggles\', \'Cat\', 5.2, 4.2, 9.7],\r\n           [\'Rex\', \'Dog\', 2.1, 12, 2.1],\r\n           [\'Mrs. Quiggleworth\', \'Cat\', 7.4, 3, 7],\r\n           [\'Squirmy\', \'Snake\', 1.1, 0.2, 0.1],\r\n           [\'Tarantula\', \'Legs\', 0.2, 0.01, -np.inf],\r\n           [\'Groucho\', \'Dog\', 6.9, 8, 5.1]]\r\n\r\ndf = pd.DataFrame(dataset, columns=columns).set_index([\'Name\'])\r\n\r\n# Works fine\r\nprint(""String type - are any of the columns negative infinity?"")\r\nneg_inf = (df == -np.inf)\r\nprint(neg_inf.any(axis=1))\r\n\r\n# Convert \'type\' to a categorical\r\ndf[\'Type\'] = df[\'Type\'].astype(\'category\')\r\n\r\nprint(""Categorical type - is the Type column negative infinity?"")\r\nprint(df[\'Type\'] == -np.inf)    # Works fine\r\n\r\nprint(""Categorical type in dataframe - are any of them negative infinity?"")\r\nprint(df[[\'Type\']] == -np.inf)  # Danger, Will Robinson!\r\n```\r\n\r\n#### Expected Output\r\n```\r\n3.5.1 |Anaconda 2.5.0 (64-bit)| (default, Dec  7 2015, 11:16:01) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\r\n0.17.1\r\nString type - are any of the columns negative infinity?\r\nName\r\nSnuggles             False\r\nRex                  False\r\nMrs. Quiggleworth    False\r\nSquirmy              False\r\nTarantula             True\r\nGroucho              False\r\ndtype: bool\r\nCategorical type - is the Type column negative infinity?\r\nName\r\nSnuggles             False\r\nRex                  False\r\nMrs. Quiggleworth    False\r\nSquirmy              False\r\nTarantula            False\r\nGroucho              False\r\nName: Type, dtype: bool\r\nCategorical type in dataframe - are any of them negative infinity?\r\nTraceback (most recent call last):\r\n  File ""pandas_demo.py"", line 30, in <module>\r\n    print(df[[\'Type\']] == -np.inf)  # Danger, Will Robinson!\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/ops.py"", line 1115, in f\r\n    res = self._combine_const(other, func, raise_on_error=False)\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py"", line 3482, in _combine_const\r\n    new_data = self._data.eval(func=func, other=other, raise_on_error=raise_on_error)\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py"", line 2840, in eval\r\n    return self.apply(\'eval\', **kwargs)\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py"", line 2823, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py"", line 1155, in eval\r\n    fastpath=True,)]\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py"", line 169, in make_block\r\n    return make_block(values, placement=placement, ndim=ndim, **kwargs)\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py"", line 2454, in make_block\r\n    placement=placement)\r\n  File ""~/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py"", line 78, in __init__\r\n    raise ValueError(\'Wrong number of dimensions\')\r\nValueError: Wrong number of dimensions\r\n```\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-51-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 8.0.3\r\nsetuptools: 20.2.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.3\r\nsphinx: 1.3.5\r\npatsy: 0.4.0\r\ndateutil: 2.5.0\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.8.4\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```'"
12558,139171172,nickeubank,jreback,2016-03-08 03:42:59,2016-03-12 17:46:32,2016-03-12 17:46:32,closed,,0.18.1,1,Bug;Difficulty Novice;Effort Low;Numeric,https://api.github.com/repos/pydata/pandas/issues/12558,b'BUG?: value_counts(normalize=True) normalizes over all observations including NaN.',"b'By default, `value_counts` ignores missing values. To get them displayed, one must add the option `dropna=False`. But when one normalizes, they are entering into the denominator even when `dropna=True`. \r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n    s = pd.Series([1,2,3,np.nan, np.nan, np.nan])\r\n    s.value_counts(normalize=True)\r\n\r\n    0.1667\r\n    0.1667\r\n    0.1667\r\n\r\n#### Expected Output\r\n\r\n    0.333\r\n    0.333\r\n    0.333\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.4.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 8.1.0\r\nsetuptools: 20.2.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.4.4\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8'"
12557,139066380,thanasis2028,thanasis2028,2016-03-07 19:16:34,2016-03-08 13:14:29,2016-03-08 08:43:56,closed,,,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/12557,b'BUG: Fix for issue #12553',b'closes #12553 '
12540,138744298,sinhrks,jreback,2016-03-06 02:13:43,2016-03-15 14:22:40,2016-03-15 14:22:14,closed,,0.18.1,1,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/12540,"b""BUG: Can't get period code with frequency alias 'minute' or 'Minute'""","b' - [x] closes #11854 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nAlso, added ``NS`` to normal alias not to show warnings (as the same as ``MS`` and ``NS``)'"
12538,138736370,jreback,jreback,2016-03-06 00:11:36,2016-03-07 18:42:44,2016-03-07 18:42:44,closed,,0.18.1,1,Bug;Difficulty Novice;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12538,b'BUG: window functions count should be an integer dtype',"b""```\r\nIn [1]: df = DataFrame({'A' : pd.date_range('20130101',periods=3),'B' : range(3)})\r\n\r\nIn [2]: df.rolling(window=2).count()\r\nOut[2]: \r\n     A    B\r\n0  1.0  1.0\r\n1  2.0  2.0\r\n2  2.0  2.0\r\n\r\nIn [3]: df.rolling(window=2).count().dtypes\r\nOut[3]: \r\nA    float64\r\nB    float64\r\ndtype: object\r\n```"""
12527,138535037,itcarroll,jreback,2016-03-04 17:33:44,2016-04-20 00:04:17,2016-04-20 00:04:17,closed,,0.18.1,2,Bug;Difficulty Intermediate;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/12527,b'When .loc returns IndexError rather than KeyError',"b'#### Updated\r\n\r\nI have a MultiIndex\'d DataFrame that returns a KeyError for one integer and an IndexError for a different integer, neither integer is in the first level of the index. This only occurs when attempting to access a scalar value, a slice always give KeyError. The behavior does not occur on a cut down version of the (>200M when pickled) data frame, or I would attach a working example. Can send the file if needed though.\r\n\r\n````\r\n>>> isinstance(n, int)\r\nTrue\r\n>>> df.loc[(n, 0), \'dest\']\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1196, in __getitem__\r\n    return self._getitem_tuple(key)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 709, in _getitem_tuple\r\n    return self._getitem_lowerdim(tup)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 817, in _getitem_lowerdim\r\n    return self._getitem_nested_tuple(tup)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 889, in _getitem_nested_tuple\r\n    obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1343, in _getitem_axis\r\n    return self._get_label(key, axis=axis)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 86, in _get_label\r\n    return self.obj._xs(label, axis=axis)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/generic.py"", line 1483, in xs\r\n    drop_level=drop_level)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/index.py"", line 5432, in get_loc_level\r\n    return (self._engine.get_loc(_values_from_object(key)),\r\n  File ""pandas/index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:3979)\r\n  File ""pandas/index.pyx"", line 146, in pandas.index.IndexEngine.get_loc (pandas/index.c:3693)\r\n  File ""pandas/src/util.pxd"", line 41, in util.get_value_at (pandas/index.c:13199)\r\nIndexError: index out of bounds\r\n````\r\n\r\n#### Expected Output\r\n````\r\n>>> df.loc[(m, 0), \'dest\']\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1196, in __getitem__\r\n    return self._getitem_tuple(key)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 709, in _getitem_tuple\r\n    return self._getitem_lowerdim(tup)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 817, in _getitem_lowerdim\r\n    return self._getitem_nested_tuple(tup)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 889, in _getitem_nested_tuple\r\n    obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 1343, in _getitem_axis\r\n    return self._get_label(key, axis=axis)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexing.py"", line 86, in _get_label\r\n    return self.obj._xs(label, axis=axis)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/generic.py"", line 1483, in xs\r\n    drop_level=drop_level)\r\n  File ""/usr/local/lib/python3.5/site-packages/pandas/core/index.py"", line 5432, in get_loc_level\r\n    return (self._engine.get_loc(_values_from_object(key)),\r\n  File ""pandas/index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:3979)\r\n  File ""pandas/index.pyx"", line 147, in pandas.index.IndexEngine.get_loc (pandas/index.c:3719)\r\nKeyError: (300067502, 0)\r\n````\r\n\r\nThe expected behavior occurs for nearly all integers I try that are not in the first level of the index. How could special integers give an IndexError?\r\n\r\n#### output of ``pd.show_versions()``\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 8.0.2\r\nsetuptools: 19.4\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.0\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\n'"
12526,138507736,vlfom,jreback,2016-03-04 15:50:49,2016-05-07 17:48:56,2016-05-07 17:48:56,closed,,,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12526,b'BUG: Add exception when different number of fields present in file lines',"b""Addresses issue in #12519 by raising exception when 'filepath_or_buffer'\r\nin 'read_csv' contains different number of fields in input lines.\r\n\r\nHowever, according to tests the behaviour reported in bug is expected (lines 376-390): https://github.com/pydata/pandas/blob/master/pandas/io/tests/test_cparser.py#L376\r\n\r\nSo should those tests be removed?"""
12512,137901618,gfyoung,jreback,2016-03-02 15:14:52,2016-04-06 19:29:40,2016-04-06 19:17:26,closed,,0.18.1,13,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12512,b'BUG: Fix parse_dates processing with usecols and C engine',"b'closes #9755 \r\ncloses #12678 \r\n\r\nContinuing on my conquest of `read_csv` bugs, this PR fixes a bug brought up in #9755 in processing `parse_dates` with the C engine in which the wrong indices (those of the filtered column names) were being used to determine the date columns to not be dtype-parsed by the C engine. The correct indices are those of the original column names, as they are used later on in the actual data processing.'"
12506,137746702,gfyoung,jreback,2016-03-02 01:45:47,2016-04-13 01:32:13,2016-04-13 01:30:28,closed,,0.18.1,50,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12506,b'BUG: Respect usecols even with empty data',b'closes #12493\r\n\r\n\r\n in which the `usecols` argument was not being respected for empty data.  This is because no filtering was applied when the first (and only) chunk was being read.'
12504,137630532,gfyoung,jreback,2016-03-01 16:59:03,2016-03-03 22:21:48,2016-03-03 21:55:35,closed,,0.18.0,4,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12504,b'BUG: Fixed grow_buffer to grow when capacity is reached',"b'Addresses issue in #12494 by allowing `grow_buffer` to grow the size of the parser buffer when buffer capacity is achieved.  Previously, you had to exceed capacity for this to occur, but that was inconsistent with the `end_field` check later on when handling the EOF terminator, where reached capacity was considered a buffer overflow.'"
12498,137443683,gfyoung,jreback,2016-03-01 02:11:13,2016-03-06 15:43:07,2016-03-06 15:28:43,closed,,0.18.0,15,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12498,b'BUG: Allow assignment by indexing with duplicate column names',"b'closes #12344\r\n\r\n in which assignment to columns in `DataFrame` with duplicate column names caused all columns with the same name to be reassigned.  The bug was located <a href=""https://github.com/pydata/pandas/blob/master/pandas/core/indexing.py#L545"">here</a>.\r\n\r\nWhen you try to index into the DataFrame using `.iloc`, `pandas` will find the corresponding column name (or key) first before setting that key with the given value.  Unfortunately, since all of your columns have the same key name, `pandas` ends up choosing all of the columns corresponding to that name.\r\n\r\nThis PR introduces a `_set_item_by_index` method for `DataFrame` objects that allows you to bypass that issue by using the indices of the columns to set the columns themselves whenever there are duplicates involved.'"
12494,137196439,VelizarVESSELINOV,jreback,2016-02-29 09:19:07,2016-03-08 19:18:20,2016-03-03 22:08:57,closed,,0.18.0,5,Bug;Difficulty Intermediate;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12494,b'BUG: read_csv with empty header row raising ',"b'#### Code Sample, a copy-pastable example if possible\r\n```python\r\n""""""Example of Pandas bug.""""""\r\nfrom pandas import read_csv\r\n\r\ntry:\r\n    from StringIO import StringIO\r\nexcept ImportError:\r\n    from io import StringIO\r\n\r\ns = StringIO(\',,\')\r\n\r\ndf = read_csv(s)\r\n\r\nprint(list(df))\r\n\r\ns = StringIO(\',,,\')\r\n\r\ndf = read_csv(s)\r\n\r\nprint(list(df))\r\n```\r\n#### Expected Output\r\nCurrent output:\r\n```\r\n[\'Unnamed: 0\', \'Unnamed: 1\', \'Unnamed: 2\']\r\nTraceback (most recent call last):\r\n  File ""pandas_bug4.py"", line 17, in <module>\r\n    df = read_csv(s)\r\n  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/io/parsers.py"", line 498, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/io/parsers.py"", line 275, in _read\r\n    parser = TextFileReader(filepath_or_buffer, **kwds)\r\n  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/io/parsers.py"", line 590, in __init__\r\n    self._make_engine(self.engine)\r\n  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/io/parsers.py"", line 731, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/io/parsers.py"", line 1103, in __init__\r\n    self._reader = _parser.TextReader(src, **kwds)\r\n  File ""pandas/parser.pyx"", line 515, in pandas.parser.TextReader.__cinit__ (pandas/parser.c:4948)\r\n  File ""pandas/parser.pyx"", line 632, in pandas.parser.TextReader._get_header (pandas/parser.c:6493)\r\n  File ""pandas/parser.pyx"", line 829, in pandas.parser.TextReader._tokenize_rows (pandas/parser.c:8838)\r\n  File ""pandas/parser.pyx"", line 1833, in pandas.parser.raise_parser_error (pandas/parser.c:22649)\r\npandas.parser.CParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\r\n```\r\nExpected output:\r\n```\r\n[\'Unnamed: 0\', \'Unnamed: 1\', \'Unnamed: 2\']\r\n[\'Unnamed: 0\', \'Unnamed: 1\', \'Unnamed: 2\', \'Unnamed: 3\']\r\n```\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n----------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.3.2\r\nCython: None\r\nnumpy: 1.10.1\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.3.2\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8\r\nNone\r\n'"
12493,137195159,VelizarVESSELINOV,jreback,2016-02-29 09:13:19,2016-04-13 01:30:28,2016-04-13 01:30:28,closed,,0.18.1,4,Bug;Difficulty Novice;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12493,b'usecols is not respected when the file is empty',"b'xref #9755\r\n\r\nIf the file is empty, it is logical to compensate with extra columns.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\n""""""Example of Pandas bug.""""""\r\nfrom pandas import read_csv\r\n\r\ntry:\r\n    from StringIO import StringIO\r\nexcept ImportError:\r\n    from io import StringIO\r\n\r\nname_list = [\'Dummy\', \'X\', \'Dummy_2\']\r\ns = StringIO(\',,\')\r\n\r\ndf = read_csv(s, names=name_list, usecols=name_list[1:2], header=None)\r\n\r\nprint(list(df))\r\n\r\ns = StringIO(\'\')\r\n\r\ndf = read_csv(s, names=name_list, usecols=name_list[1:2], header=None)\r\n\r\nprint(list(df))\r\n```\r\n#### Expected Output\r\nCurrent output:\r\n```\r\n[\'X\']\r\n[\'Dummy\', \'X\', \'Dummy_2\']\r\n```\r\nExpected output:\r\n```\r\n[\'X\']\r\n[\'X\']\r\n```\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.3.2\r\nCython: None\r\nnumpy: 1.10.1\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.3.2\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8\r\nNone\r\n'"
12484,136938067,JoeGermuska,jreback,2016-02-27 16:26:01,2016-04-26 17:22:57,2016-04-26 17:22:44,closed,,No action,1,Bug;Duplicate;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12484,"b""read_csv dtype 'object' doesn't work for index column""","b'I\'m working on a little pandas lesson. The CSV has a column with leading \'0\' which should be treated as text. When I try to read the CSV and indicate that that column is the `index`, the `dtype` argument doesn\'t work. I can use `S7` as the `dtype` instead of `object` but @wesm [suggests using \'object\'](http://stackoverflow.com/questions/16929056/pandas-read-csv-dtype-leading-zeros#comment24510329_16929535)\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\nfrom StringIO import StringIO\r\nimport pandas as pd\r\ndata = """"""\r\nfips,name,popest2014\r\n0800760,""Aguilar town, Colorado"",479\r\n0800925,""Akron town, Colorado"",1694\r\n0801090,""Alamosa city, Colorado"",9531\r\n""""""\r\nerror_case = pd.read_csv(StringIO(data), index_col=0, dtype={\'fips\': object})\r\nprint ""Error case, dtype should be object ""\r\nprint error_case.index.dtype # \'int64\'\r\nexpected1 = pd.read_csv(StringIO(data), dtype={\'fips\': object})\r\nprint ""Works with object, not an index column""\r\nprint expected1.fips.dtype # \'O\'\r\nexpected2 = pd.read_csv(StringIO(data), index_col=0, dtype={\'fips\': \'S7\'})\r\nprint ""Works with \'S7\', as an index column""\r\nprint expected2.index.dtype # \'O\'\r\n\r\n#### Expected Output\r\nThat the dtype for the FIPS column as an index would be `object`\r\n\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 8.0.3\r\nsetuptools: 20.1.1\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 4.1.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```'"
12483,136934357,thejohnfreeman,jreback,2016-02-27 15:53:02,2016-03-06 20:45:41,2016-03-06 20:44:23,closed,,0.18.0,10,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/12483,"b""ENH: optional ':' separator in ISO8601 strings""","b' - [X] closes #10041\r\n - [X] tests added / passed\r\n - [X] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry\r\n\r\nIncludes some refactoring in the ISO8601 parser. If more tests are desired, please let me know where to put them. Same for the whatsnew entry. This is effectively my first pull request for Pandas.\r\n\r\ncc @jreback '"
12477,136856862,kemingts,jreback,2016-02-27 01:00:05,2016-02-27 17:53:27,2016-02-27 17:52:43,closed,,0.18.0,1,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/12477,"b'BUG: indexing operation changes dtype, #10503'",b' - [ ] closes #10503 \r\n - [ ] tests added / passed\r\n - [ ] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n'
12473,136814457,AlJohri,jreback,2016-02-26 21:04:14,2016-04-18 17:17:44,2016-04-18 17:17:44,closed,,0.18.1,2,Bug;Difficulty Intermediate;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12473,b'Pandas datetime64 series no longer has map function when localized',"b'Create test DF.\r\n```\r\ndf = pd.DataFrame({""uuid"": [0,1,2,3,4], ""publication_timestamp"": [""2015-07-28 00:10:05.852"", ""2015-10-03 00:17:43.000"", ""2015-08-20 01:15:52.693"", ""2015-09-09 00:02:03.083"", ""2015-12-08 00:02:41.390""], ""timezone"": [""US/Central"", ""US/Eastern"", ""US/Eastern"", ""US/Pacific"", ""US/Mountain""]}).set_index(\'uuid\')\r\n```\r\n\r\n```\r\n                publication_timestamp     timezone\r\nuuid\r\n0    2015-07-28 04:10:05.852000+00:00   US/Central\r\n1           2015-10-03 04:17:43+00:00   US/Eastern\r\n2    2015-08-20 05:15:52.693000+00:00   US/Eastern\r\n3    2015-09-09 04:02:03.083000+00:00   US/Pacific\r\n4    2015-12-08 05:02:41.390000+00:00  US/Mountain\r\n```\r\n\r\nThis call to `map` works fine:\r\n```\r\ndf.publication_timestamp.map(lambda x: x) # works fine\r\n```\r\n\r\n**Localizing the datetime64 causes it to no longer have the map function**\r\n```\r\ndf[\'publication_timestamp\'] = df.publication_timestamp.astype(""datetime64[ms]"").dt.tz_localize(""UTC"")\r\n```\r\n\r\nDoesn\'t work:\r\n```\r\ndf.publication_timestamp.map(lambda x: x) # no longer works\r\n```\r\n\r\n#### Error Message\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-ba941613799a> in <module>()\r\n----> 1 df.publication_timestamp.map(lambda x: x)\r\n\r\n/Users/johria/.pyenv/versions/3.5.1/lib/python3.5/site-packages/pandas/core/series.py in map(self, arg, na_action)\r\n   2052                                      index=self.index).__finalize__(self)\r\n   2053         else:\r\n-> 2054             mapped = map_f(values, arg)\r\n   2055             return self._constructor(mapped,\r\n   2056                                      index=self.index).__finalize__(self)\r\n\r\nTypeError: Argument \'arr\' has incorrect type (expected numpy.ndarray, got DatetimeIndex)\r\n```\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n```\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.0.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 8.0.3\r\nsetuptools: 19.4\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.1.1\r\nsphinx: 1.3.5\r\npatsy: 0.4.1\r\ndateutil: 2.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.2.0-b1\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: 0.9.2\r\napiclient: 1.4.2\r\nsqlalchemy: 1.0.11\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8\r\n```\r\n\r\n'"
12467,136791919,tsdlovell,jreback,2016-02-26 19:33:34,2016-04-06 13:00:10,2016-04-06 13:00:10,closed,,0.18.1,2,Bug;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/12467,b'Concat of tz-aware and tz-unaware dataframes fails',"b""This snippet\r\n```python\r\nimport pandas as pd\r\ndf1 = pd.DataFrame(dict(time=[pd.Timestamp('2015-01-01', tz=None)]))\r\ndf2 = pd.DataFrame(dict(time=[pd.Timestamp('2015-01-01', tz='UTC')]))\r\npd.concat([df1, df2])\r\n```\r\ncauses\r\n> .../pandas/tseries/common.py in _concat_compat(to_concat, axis)\r\n>     282         if 'datetime' in typs or 'object' in typs:\r\n>     283             to_concat = [convert_to_pydatetime(x, axis) for x in to_concat]\r\n> --> 284             return np.concatenate(to_concat, axis=axis)\r\n>     285 \r\n>     286         # we require ALL of the same tz for datetimetz\r\n> \r\n> ValueError: all the input arrays must have same number of dimensions\r\n\r\nWe would expect it to return something like this\r\n> pd.DataFrame(dict(time=pd.Series([pd.Timestamp('2015-01-01', tz=None), pd.Timestamp('2015-01-01', tz='UTC')], dtype=object)))\r\n> Out[18]: \r\n>                         time\r\n> 0        2015-01-01 00:00:00\r\n> 1  2015-01-01 00:00:00+00:00\r\n\r\noutput of ``pd.show_versions()``\r\n\r\n### INSTALLED VERSIONS\r\ncommit: fe584e7829dc4dd9fa585a7fbf5fd47fc6f4b057\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-53-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0rc1+47.gfe584e7\r\nnose: 1.3.7\r\npip: 8.0.3\r\nsetuptools: 20.1.1\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.1\r\nsphinx: 1.3.5\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 0.9.4\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\n\r\n"""
12462,136774727,tsdlovell,jreback,2016-02-26 18:23:29,2016-04-06 13:00:19,2016-04-06 13:00:10,closed,,0.18.1,11,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/12462,b'BUG: fix df concat containing mix of localized and non-localized Timestamps',b'BUG: fix issue with concat of localized timestamps\r\nTST: test concat of dataframes with non-None timezone columns'
12453,136686241,paulogp,jreback,2016-02-26 12:33:57,2016-06-06 23:20:34,2016-06-06 23:20:34,closed,,0.19.0,12,Bug;Difficulty Novice;Effort Low;IO Excel,https://api.github.com/repos/pydata/pandas/issues/12453,b'BUG: excel export merge margin ',"b'#### Code Sample, a copy-pastable example if possible\r\n```\r\ndf = pd.DataFrame([ {""JOB"": ""Worker"", ""NAME"": ""Bob"", ""YEAR"": 2013, ""MONTH"": 12, ""DAYS"": 3, ""SALARY"": 17},\r\n                    {""JOB"": ""Employ"", ""NAME"": ""Mary"", ""YEAR"": 2013, ""MONTH"": 12, ""DAYS"": 5, ""SALARY"": 23},\r\n                    {""JOB"": ""Worker"", ""NAME"": ""Bob"", ""YEAR"": 2014, ""MONTH"": 1, ""DAYS"": 10, ""SALARY"": 100},\r\n                    {""JOB"": ""Worker"", ""NAME"": ""Bob"", ""YEAR"": 2014, ""MONTH"": 1, ""DAYS"": 11, ""SALARY"": 110},\r\n                    {""JOB"": ""Employ"", ""NAME"": ""Mary"", ""YEAR"": 2014, ""MONTH"": 1, ""DAYS"": 15, ""SALARY"": 200},\r\n                    {""JOB"": ""Worker"", ""NAME"": ""Bob"", ""YEAR"": 2014, ""MONTH"": 2, ""DAYS"": 8, ""SALARY"": 80},\r\n                    {""JOB"": ""Employ"", ""NAME"": ""Mary"", ""YEAR"": 2014, ""MONTH"": 2, ""DAYS"": 5, ""SALARY"": 190}])\r\ndf = df.set_index([""JOB"", ""NAME"", ""YEAR"", ""MONTH""], drop=False, append=False)\r\n\r\ndf = df.pivot_table(index=[""JOB"", ""NAME""], columns=[""YEAR"", ""MONTH""], values=[""DAYS"", ""SALARY""], aggfunc={""DAYS"": ""mean"", ""SALARY"": ""sum""}, margins=True)\r\n\r\nprint(df)\r\n\r\nfile = r""E:\\TESTE.xlsx""\r\nwriter = pd.ExcelWriter(file, engine=""xlsxwriter"", datetime_format=""yyyy-mm-dd"")\r\ndf.to_excel(writer, sheet_name=""TESTE"", index=True)\r\nwriter.save()\r\n```\r\n\r\n#### Expected Output\r\n\r\n     | SALARY || | | DAYS |||||\r\n--- | --- | --- | --- | --- | --- | --- | --- | --- | ---\r\nYEAR || 2013 | 2014 || All | 2013 | 2014 || All\r\nMONTH || 12 | 1 | 2 || 12 | 1 | 2          \r\nJOB | NAME ||||||||\r\nEmploy | Mary | 23 | 200 | 190 | 413 | 5 | 15.0 | 5.0 | 8.333333\r\nWorker | Bob | 17 | 210 | 80 | 307 | 3 | 10.5 | 8.0 | 8.000000\r\nAll || 40 | 410 | 270 | 720 | 4 | 12.0 | 6.5 | 8.142857\r\n\r\nBut when exporting to Excel, the Month 2, on SALARY and DAYS, merges with the next column and All above.\r\n\r\n2014 .......... | All ...........|\r\n1 ..... | 2 ....................... |\r\n15 ... | 5 ..... | 8.333333 | \r\n\r\nWhen exporting to HTML it\'s ok with the header but, if the value cell has no value then shows cuts the columns.\r\n\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 2012ServerR2\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 47 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 8.0.2\r\nsetuptools: 20.1.1\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: 0.17.0\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: 0.8.4\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\nNone\r\n\r\n![image](https://cloud.githubusercontent.com/assets/976878/13352479/73f541f2-dc85-11e5-960e-d6fe6ac85ee8.png)\r\n'"
12408,135234059,VelizarVESSELINOV,jreback,2016-02-21 17:46:54,2016-02-22 14:13:15,2016-02-22 14:13:15,closed,,,1,Bug;Duplicate;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12408,b'index_col result is unexpected when usecols is used to skip a column',"b'#### Code Sample, a copy-pastable example if possible\r\n```python\r\n""""""Example of Pandas bug.""""""\r\nfrom pandas import read_csv\r\ntry:\r\n    from StringIO import StringIO\r\nexcept ImportError:\r\n    from io import StringIO\r\n\r\ncsv = """"""A, B, C\r\n1, 2, 3""""""\r\n\r\n\r\ndef message(msg):\r\n    """"""Print message.""""""\r\n    msg_len = len(msg) + 2\r\n    print(\'-\' * msg_len + \'\\n {}\\n\'.format(msg) + \'-\' * msg_len)\r\n\r\nmessage(\'raw file\')\r\nprint(csv)\r\n\r\n\r\ndf = read_csv(StringIO(csv), names=[\'A\', \'B\', \'C\'], skiprows=1, header=None,\r\n              usecols=[\'B\', \'C\'])\r\n\r\n\r\nmessage(\'expected result after skiping column A\')\r\nprint(df)\r\n\r\ndf = read_csv(StringIO(csv), names=[\'A\', \'B\', \'C\'], skiprows=1, header=None,\r\n              usecols=[\'B\', \'C\'], index_col=\'B\')\r\n\r\nmessage(\'not expected result, if use index with column B\')\r\nprint(df)\r\n```\r\n#### Expected Output\r\nCurrent output:\r\n```\r\n----------\r\n raw file\r\n----------\r\nA, B, C\r\n1, 2, 3\r\n----------------------------------------\r\n expected result after skiping column A\r\n----------------------------------------\r\n   B  C\r\n0  2  3\r\n-------------------------------------------------\r\n not expected result, if use index with column B\r\n-------------------------------------------------\r\n   B\r\nB   \r\n3  2\r\n```\r\nExpected output:\r\n```\r\n----------\r\n raw file\r\n----------\r\nA, B, C\r\n1, 2, 3\r\n----------------------------------------\r\n expected result after skiping column A\r\n----------------------------------------\r\n   B  C\r\n0  2  3\r\n-------------------------------------------------\r\n not expected result, if use index with column B\r\n-------------------------------------------------\r\n   C\r\nB   \r\n2  3\r\n```\r\n#### output of ``pd.show_versions()``\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.3.2\r\nCython: None\r\nnumpy: 1.10.1\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.3.2\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8\r\n'"
12394,134868637,twiecki,twiecki,2016-02-19 13:47:05,2016-02-19 14:32:23,2016-02-19 14:32:23,closed,,0.18.0,4,Bug;Duplicate;Groupby;Timezones,https://api.github.com/repos/pydata/pandas/issues/12394,b'datetime gets converted to int64 in groupby agg',"b""I tried to produce a stand-alone example but couldn't so far. Maybe the answer is already apparent though. This started happening just recently (maybe after updated pandas to 0.17.1) but worked well before. I'm doing a multi-column `agg` in a groupby. One of the columns is a datetime of which I want the first element:\r\n```\r\n(Pdb) t.groupby(['block_dir']).first()\r\n                                 dt    sid  amount     price symbol  \\\r\nblock_dir                                                             \r\n1         2003-01-02 15:56:00+00:00  21719     -62  0.963811   AIRN   \r\n```\r\nWorks fine, however:\r\n```\r\n          order_sign  block_time  \r\nblock_dir                         \r\n1              False           0  \r\n(Pdb) t.groupby(['block_dir']).agg({'dt': 'first'})\r\n                            dt\r\nblock_dir                     \r\n1          1041522960000000000\r\n```\r\n#### Expected Output\r\n\r\n#### output of ``pd.show_versions()``\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-77-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 8.0.2\r\nsetuptools: 19.6.2\r\nCython: 0.23.4\r\nnumpy: 1.10.4\r\nscipy: 0.16.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.8\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext)\r\nJinja2: None\r\n```\r\n"""
12391,134762452,gfyoung,jreback,2016-02-19 03:36:30,2016-02-21 00:41:34,2016-02-20 18:27:34,closed,,0.18.0,5,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/12391,b'BUG: Return UTC DateTimeIndex when specified in to_datetime',b'Title is self-explanatory.  Closes #11934.'
12382,134540744,yosuah,jreback,2016-02-18 10:35:01,2016-02-18 12:48:30,2016-02-18 12:48:30,closed,,,2,Bug;Duplicate;Timezones,https://api.github.com/repos/pydata/pandas/issues/12382,b'apply() silently removes time zone information from DateTime values',"b""Hi,\r\n\r\napply() silently removes time zone information from DateTime values. I think this bug is related to https://github.com/pydata/pandas/issues/10668, but it is apparently not the same.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ndate_range = pd.date_range('1/1/2011', periods=5, freq='H')\r\ndate_range = date_range.tz_localize('Europe/Budapest')\r\ns = pd.Series(date_range, index=date_range)\r\ns\r\n```\r\n\r\n    2011-01-01 00:00:00+01:00   2011-01-01 00:00:00+01:00\r\n    2011-01-01 01:00:00+01:00   2011-01-01 01:00:00+01:00\r\n    2011-01-01 02:00:00+01:00   2011-01-01 02:00:00+01:00\r\n    2011-01-01 03:00:00+01:00   2011-01-01 03:00:00+01:00\r\n    2011-01-01 04:00:00+01:00   2011-01-01 04:00:00+01:00\r\n    Freq: H, dtype: datetime64[ns, Europe/Budapest]\r\n\r\nCreate a simple apply function which does nothing but returns each value. Notice that that values are converted to UTC by default and the timezone information is lost, and a timezone-naive value is returned instead.\r\n\r\n```python\r\ndef apply_function(dt):\r\n    return dt\r\ns.apply(apply_function)\r\n```\r\n\r\n    2011-01-01 00:00:00+01:00   2010-12-31 23:00:00\r\n    2011-01-01 01:00:00+01:00   2011-01-01 00:00:00\r\n    2011-01-01 02:00:00+01:00   2011-01-01 01:00:00\r\n    2011-01-01 03:00:00+01:00   2011-01-01 02:00:00\r\n    2011-01-01 04:00:00+01:00   2011-01-01 03:00:00\r\n    Freq: H, dtype: datetime64[ns]\r\n\r\n#### Expected Output\r\n\r\nApply is expected to return the original series, values unchanged, including the time zone information.\r\n\r\n#### output of ``pd.show_versions()``\r\n\r\n    INSTALLED VERSIONS\r\n    ------------------\r\n    commit: None\r\n    python: 3.4.4.final.0\r\n    python-bits: 64\r\n    OS: Linux\r\n    OS-release: 4.2.0-27-generic\r\n    machine: x86_64\r\n    processor: \r\n    byteorder: little\r\n    LC_ALL: en_US.UTF-8\r\n    LANG: en_US.UTF-8\r\n    \r\n    pandas: 0.17.1\r\n    nose: None\r\n    pip: 8.0.2\r\n    setuptools: 20.1.1\r\n    Cython: 0.23.4\r\n    numpy: 1.10.4\r\n    scipy: 0.17.0\r\n    statsmodels: 0.6.1\r\n    IPython: 4.1.1\r\n    sphinx: None\r\n    patsy: 0.4.0\r\n    dateutil: 2.4.2\r\n    pytz: 2015.7\r\n    blosc: None\r\n    bottleneck: None\r\n    tables: None\r\n    numexpr: None\r\n    matplotlib: 1.5.1\r\n    openpyxl: None\r\n    xlrd: 0.9.4\r\n    xlwt: None\r\n    xlsxwriter: None\r\n    lxml: None\r\n    bs4: None\r\n    html5lib: None\r\n    httplib2: None\r\n    apiclient: None\r\n    sqlalchemy: None\r\n    pymysql: None\r\n    psycopg2: None\r\n    Jinja2: None\r\n"""
12376,134481293,BranYang,jreback,2016-02-18 04:52:09,2016-03-06 01:11:31,2016-03-06 00:45:44,closed,,0.18.0,23,Bug;Dtypes;Stats,https://api.github.com/repos/pydata/pandas/issues/12376,b'Fix #12373: rolling functions raise ValueError on float32 data',b' - [x] closes #12373\r\n - [x] tests added\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [x] whatsnew entry added\r\n - [x] add test for various other dtypes\r\n\r\n'
12373,134427698,jennolsen84,jreback,2016-02-17 23:00:05,2016-03-06 00:45:44,2016-03-06 00:45:44,closed,,0.18.0,1,Bug;Dtypes;Stats,https://api.github.com/repos/pydata/pandas/issues/12373,b'BUG: rolling functions raise ValueError on float32 data',"b'pd version: v0.18.0rc1\r\n\r\nminimal example:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\npd.DataFrame(np.arange(5, dtype=np.float32)).rolling(window=3).max()\r\n```\r\n\r\nstack trace:\r\n\r\n```\r\n<ipython-input-25-795ba6485799> in <module>()\r\n----> 1 pd.DataFrame(np.arange(5, dtype=np.float32)).rolling(window=3).max()\r\n\r\n/venv/lib/python3.4/site-packages/pandas/core/window.py in max(self, **kwargs)\r\n    767     @Appender(_shared_docs[\'max\'])\r\n    768     def max(self, **kwargs):\r\n--> 769         return super(Rolling, self).max(**kwargs)\r\n    770 \r\n    771     @Substitution(name=\'rolling\')\r\n\r\n/venv/lib/python3.4/site-packages/pandas/core/window.py in max(self, how, **kwargs)\r\n    541         if self.freq is not None and how is None:\r\n    542             how = \'max\'\r\n--> 543         return self._apply(\'roll_max\', how=how, **kwargs)\r\n    544 \r\n    545     _shared_docs[\'min\'] = dedent(""""""\r\n\r\n/venv/lib/python3.4/site-packages/pandas/core/window.py in _apply(self, func, window, center, check_minp, how, **kwargs)\r\n    474 \r\n    475             if values.ndim > 1:\r\n--> 476                 result = np.apply_along_axis(calc, self.axis, values)\r\n    477             else:\r\n    478                 result = calc(values)\r\n\r\n/venv/lib/python3.4/site-packages/numpy/lib/shape_base.py in apply_along_axis(func1d, axis, arr, *args, **kwargs)\r\n     89     outshape = asarray(arr.shape).take(indlist)\r\n     90     i.put(indlist, ind)\r\n---> 91     res = func1d(arr[tuple(i.tolist())], *args, **kwargs)\r\n     92     #  if res is a number, then we have a smaller output array\r\n     93     if isscalar(res):\r\n\r\n/venv/lib/python3.4/site-packages/pandas/core/window.py in calc(x)\r\n    471 \r\n    472                 def calc(x):\r\n--> 473                     return func(x, window, min_periods=self.min_periods)\r\n    474 \r\n    475             if values.ndim > 1:\r\n\r\n/venv/lib/python3.4/site-packages/pandas/core/window.py in func(arg, window, min_periods)\r\n    458                 def func(arg, window, min_periods=None):\r\n    459                     minp = check_minp(min_periods, window)\r\n--> 460                     return cfunc(arg, window, minp, **kwargs)\r\n    461 \r\n    462             # calculation function\r\n\r\npandas/algos.pyx in pandas.algos.roll_max (pandas/algos.c:37363)()\r\n\r\nValueError: Buffer dtype mismatch, expected \'float64_t\' but got \'float\'\r\n\r\n```'"
12368,134331256,rinoc,jreback,2016-02-17 16:36:01,2016-02-18 13:15:13,2016-02-18 13:15:08,closed,,0.18.0,3,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/12368,b'BUG #12336: Fix init categorical series with scalar value',b'closes #12336 \r\n\r\nChanged the tests to match the other ones in `test_fromValue`.'
12366,134301031,rinoc,rinoc,2016-02-17 14:54:21,2016-02-17 16:18:04,2016-02-17 16:18:04,closed,,0.18.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/12366,b'BUG #12336: Fix init categorical series with scalar value',b'closes #12336 '
12365,134277451,jreback,jreback,2016-02-17 13:16:49,2016-02-17 14:29:42,2016-02-17 14:29:42,closed,,0.18.0,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/12365,b'BUG: Bug in DataFrame.set_index() with tz-aware Series',b'closes #12358'
12364,134161423,jreback,jreback,2016-02-17 02:49:16,2016-02-17 13:22:18,2016-02-17 13:22:18,closed,,0.18.0,1,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/12364,b'BUG: resample with nunique',b'closes #12352'
12363,134161117,jreback,jreback,2016-02-17 02:48:12,2016-04-26 15:03:02,2016-04-26 15:03:02,closed,,0.18.1,1,Bug;Difficulty Intermediate;Effort Medium;Groupby,https://api.github.com/repos/pydata/pandas/issues/12363,b'BUG: inconsistent name for returned Series in groupby',"b""I would think ``[7]`` and ``[8]`` would be identical. Note this is true for all ops I have looked at, so it is a general issue.\r\n\r\n```\r\nIn [6]:         df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\r\n                              'foo', 'bar', 'foo', 'foo'],\r\n                        'B': ['one', 'one', 'two', 'two',\r\n                              'two', 'two', 'one', 'two'],\r\n                        'C': np.random.randn(8) + 1.0,\r\n                        'D': np.arange(8)})\r\n\r\nIn [7]: df.groupby(['A']).B.count()\r\nOut[7]: \r\nA\r\nbar    3\r\nfoo    5\r\nName: B, dtype: int64\r\n\r\nIn [8]: df.B.groupby(df.A).count()\r\nOut[8]: \r\nA\r\nbar    3\r\nfoo    5\r\ndtype: int64\r\n```"""
12362,134154983,stephen-hoover,jreback,2016-02-17 02:04:45,2016-04-01 13:13:36,2016-04-01 13:13:27,closed,,0.18.1,18,Bug;Dtypes;Groupby;Resample,https://api.github.com/repos/pydata/pandas/issues/12362,b'ENH Consistent apply output when grouping with freq',"b'The `BinGrouper.apply` (used by the `TimeGrouper`) and `BaseGrouper.apply` (used by the `Grouper`) have different output types. To make them consistent, remove `BinGrouper.apply` and let it use the same method as the superclass `BaseGrouper`. This requires changing `BinGrouper.groupings` to return an object which looks sufficiently like a `Grouping` object for `BaseGrouper.apply` to be happy. Use a namedtuple to create an object with the needed attributes.\r\n\r\nI think this change will only affect the output of `apply` with a custom function, and only when grouping by a `TimeGrouper` (although a `Grouper` with a `freq` specified turns into a `TimeGrouper`, so that counts).\r\n\r\nCloses #11742 .'"
12358,134129531,wavexx,jreback,2016-02-16 23:44:34,2016-02-17 14:29:42,2016-02-17 14:29:42,closed,,0.18.0,9,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/12358,b'set_index(DatetimeIndex) unexpectedly shifts tz-aware datetime',"b'This is another issue I\'ve found in code that used to work:\r\n\r\n````\r\nimport pandas as pd\r\ntm = pd.DatetimeIndex(pd.to_datetime([""2014-01-01 10:10:10""]), tz=\'UTC\').tz_convert(\'Europe/Rome\')\r\ndf = pd.DataFrame({\'tm\': tm})\r\ndf.set_index(df.tm, inplace=True)\r\nprint(df.tm[0].hour)\r\nprint(df.index[0].hour)\r\n````\r\n\r\nwrites:\r\n\r\n````\r\n11\r\n10\r\n````\r\n\r\nIt\'s unclear to me why the time is shifted. If we take a pd.DatetimeIndex which is not directly contained in the df, it works as it should:\r\n\r\n````\r\ntm = pd.DatetimeIndex(pd.to_datetime([""2014-01-01 10:10:10""]), tz=\'UTC\').tz_convert(\'Europe/Rome\')\r\ndf = pd.DataFrame({\'tm\': tm})\r\ndf.set_index(tm, inplace=True)\r\nprint(df.tm[0].hour)\r\nprint(df.index[0].hour)\r\n````\r\n````\r\n11\r\n11\r\n````\r\n'"
12352,134028663,wavexx,jreback,2016-02-16 16:34:52,2016-02-17 13:22:18,2016-02-17 13:22:18,closed,,0.18.0,5,Bug;Difficulty Intermediate;Effort Low;Resample,https://api.github.com/repos/pydata/pandas/issues/12352,b'nunique + TimeGrouper error',"b'This used to work in the past:\r\n\r\n````\r\ntmp = pd.DataFrame({\r\n        \'ID\': {pd.Timestamp(\'2015-06-05 00:00:00\'): \'0010100903\', pd.Timestamp(\'2015-06-08 00:00:00\'): \'0010150847\'},\r\n        \'DATE\': {pd.Timestamp(\'2015-06-05 00:00:00\'): \'2015-06-05\', pd.Timestamp(\'2015-06-08 00:00:00\'): \'2015-06-08\'}})\r\ntmp.groupby(pd.TimeGrouper(\'D\')).ID.nunique()\r\n````\r\n\r\nbut now I get the obscure:\r\n\r\n````\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 6, in <module>\r\n    tmp.groupby(pd.TimeGrouper(\'D\')).ID.nunique()\r\n  File ""/usr/lib/python3/dist-packages/pandas/core/groupby.py"", line 2697, in nunique\r\n    name=self.name)\r\n  File ""/usr/lib/python3/dist-packages/pandas/core/series.py"", line 227, in __init__\r\n    data = SingleBlockManager(data, index, fastpath=True)\r\n  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 3736, in __init__\r\n    ndim=1, fastpath=True)\r\n  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 2454, in make_block\r\n    placement=placement)\r\n  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 87, in __init__\r\n    len(self.values), len(self.mgr_locs)))\r\nValueError: Wrong number of items passed 2, placement implies 4\r\n````'"
12348,134004899,JohnSmizz,jreback,2016-02-16 15:07:01,2016-02-16 15:24:58,2016-02-16 15:15:22,closed,,0.18.0,1,Bug;Duplicate;Resample,https://api.github.com/repos/pydata/pandas/issues/12348,b'datetimeindex resample issue',"b""Hi- I am observing odd behavior when trying to use resample on an uneven datetimeindex. See the below code to reproduce:\r\n    \r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nbug_datetimeindex = pd.date_range('2013-12-30', '2014-01-07')\r\nbug_datetimeindex = bug_datetimeindex.drop([ \\\r\npd.Timestamp('2014-01-01'), \\\r\npd.Timestamp('2013-12-31'), \\\r\npd.Timestamp('2014-01-04'), \\\r\npd.Timestamp('2014-01-05')\r\n])\r\ntemp_df = pd.DataFrame(index=bug_datetimeindex, data=np.random.randn(len(bug_datetimeindex), 2))\r\ntemp_df.resample('B')\r\n```\r\n    \r\nthrows up a ValueError: Length mismatch: Expected axis has 5 elements, new values have 7 elements\r\n\r\nUsing asfreq() does not throw up this error on my machine. \r\n\r\nI am running pd 0.17.1 on Python 3.4.4. off Anaconda for Windows."""
12344,133943078,hantusk,jreback,2016-02-16 10:36:18,2016-03-06 15:28:42,2016-03-06 15:28:42,closed,,0.18.0,3,Bug;Difficulty Intermediate;Effort Medium;Indexing;Missing-data,https://api.github.com/repos/pydata/pandas/issues/12344,b'DataFrame.fillna corrupts columns with duplicated names',"b""\r\n```\r\n# Pandas version 0.17.1\r\nimport pandas as pd\r\ndf = pd.DataFrame({'Same': 1.0, ' Same': pd.np.nan, '  Same': pd.np.nan}, index=[0,1,2])\r\ndf.columns = [c.strip() for c in df.columns]\r\ndf.iloc[:, 2] # Returns all 1.0\r\n\r\ndf.iloc[:, 0] = df.iloc[:, 0].fillna(df.iloc[:, 1])\r\n\r\ndf.iloc[:, 2] # Column 2 is corrupted and returns all NaN\r\n```\r\n"""
12336,133839496,toobaz,jreback,2016-02-15 23:08:46,2016-02-18 13:15:08,2016-02-18 13:15:08,closed,,0.18.0,2,Bug;Categorical;Difficulty Intermediate;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12336,b'Initializing category with single value raises AttributeError',"b'        In [2]: pd.Series([0,0,0], dtype=""category"")\r\n        Out[2]: \r\n        0    0\r\n        1    0\r\n        2    0\r\n        dtype: category\r\n        Categories (1, int64): [0]\r\n\r\n        In [3]: pd.Series(0, index=range(3), dtype=""category"")\r\n        ---------------------------------------------------------------------------\r\n        AttributeError                            Traceback (most recent call last)\r\n        <ipython-input-3-2c1d7a241071> in <module>()\r\n        ----> 1 pd.Series(0, index=range(3), dtype=""category"")\r\n\r\n        /home/nobackup/repo/pandas/pandas/core/series.pyc in __init__(self, data, index, dtype, name, copy, fastpath)\r\n            224             else:\r\n            225                 data = _sanitize_array(data, index, dtype, copy,\r\n        --> 226                                        raise_cast_failure=True)\r\n            227 \r\n            228                 data = SingleBlockManager(data, index, fastpath=True)\r\n\r\n        /home/nobackup/repo/pandas/pandas/core/series.pyc in _sanitize_array(data, index, dtype, copy, raise_cast_failure)\r\n           2961             if len(subarr) != len(index) and len(subarr) == 1:\r\n           2962                 subarr = create_from_value(subarr[0], index,\r\n        -> 2963                                            subarr.dtype)\r\n           2964 \r\n           2965     elif subarr.ndim > 1:\r\n\r\n        /home/nobackup/repo/pandas/pandas/core/series.pyc in create_from_value(value, index, dtype)\r\n           2929         else:\r\n           2930             if not isinstance(dtype, (np.dtype, type(np.dtype))):\r\n        -> 2931                 dtype = dtype.dtype\r\n           2932             subarr = np.empty(len(index), dtype=dtype)\r\n           2933             subarr.fill(value)\r\n\r\n        AttributeError: \'CategoricalDtype\' object has no attribute \'dtype\'\r\n\r\n\r\n(Not a particularly brilliant use of categories, but they can then be concatenated with others.)'"
12332,133792909,jcrist,jreback,2016-02-15 19:01:58,2016-02-15 20:29:10,2016-02-15 20:28:58,closed,,0.18.0,4,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/12332,b'Resample with OHLC sometimes results in series on pandas release 0.18.0.rc1',"b""```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: index = pd.date_range('1-1-2000', '2-15-2000', freq='h')\r\n\r\nIn [3]: index = index.union(pd.date_range('4-15-2000', '5-15-2000', freq='h'))\r\n\r\nIn [4]: s = pd.Series(range(len(index)), index=index)\r\n\r\nIn [5]: a = s.loc[:'4-15-2000'].resample('30T').ohlc()\r\n\r\nIn [6]: b = s.loc[:'4-14-2000'].resample('30T').ohlc()\r\n\r\nIn [7]: a.head()\r\nOut[7]:\r\n                     open  high  low  close\r\n2000-01-01 00:00:00   0.0   0.0  0.0    0.0\r\n2000-01-01 00:30:00   NaN   NaN  NaN    NaN\r\n2000-01-01 01:00:00   1.0   1.0  1.0    1.0\r\n2000-01-01 01:30:00   NaN   NaN  NaN    NaN\r\n2000-01-01 02:00:00   2.0   2.0  2.0    2.0\r\n\r\nIn [8]: b.head()\r\nOut[8]:\r\n2000-01-01 00:00:00    0.0\r\n2000-01-01 00:30:00    NaN\r\n2000-01-01 01:00:00    1.0\r\n2000-01-01 01:30:00    NaN\r\n2000-01-01 02:00:00    2.0\r\nFreq: 30T, dtype: float64\r\n\r\nIn [9]: pd.__version__\r\nOut[9]: u'0.18.0rc1'\r\n```"""
12329,133730932,jreback,jreback,2016-02-15 14:50:17,2016-02-15 21:49:01,2016-02-15 20:28:58,closed,,0.18.0,0,Bug;Groupby;Resample,https://api.github.com/repos/pydata/pandas/issues/12329,b'BUG: addtl fix for compat summary of groupby/resample with dicts',b'closes #9052\r\ncloses #12332 '
12327,133634002,Winand,jreback,2016-02-15 06:39:20,2016-04-14 04:03:56,2016-04-14 01:27:52,closed,,,4,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12327,b'FIX: pivot_table dropna=False drops index level names',b'closes #12133 \r\nlevel names are lost on `MultiIndex.from_arrays(cartesian_product(table.index.levels))`'
12306,133153553,kawochen,jreback,2016-02-12 04:06:08,2016-02-12 13:39:21,2016-02-12 13:39:08,closed,,0.18.0,5,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/12306,b'BUG: GH12290 where tz_convert used values of uninitialized arrays',b'closes #12290'
12302,133128050,gfyoung,gfyoung,2016-02-12 00:40:08,2016-02-12 06:37:22,2016-02-12 06:37:21,closed,,,4,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/12302,b'BUG: Fix subtraction with timezone Series',"b'Addresses issue in #12290 with timezone `Series` subtraction in which single element `Series`\r\nobjects containing tz-aware objects would return a timedelta of zero, even though it visually could not be the case.\r\n\r\nThe bug was traced to the conversion of the contained timezones to UTC, in which the method call was somehow returning `NaT`, even though attempts to replicate that behaviour were unsuccessful. This new\r\nmethod call fixes the issue and is in some ways more intuitive given the comment above the conversions.'"
12296,133041486,BranYang,jreback,2016-02-11 17:52:01,2016-02-27 18:42:21,2016-02-27 15:09:05,closed,,0.18.0,6,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/12296,b'Fix #12292 : error when read one empty column from excel file',b'Fix #12292 \r\nalso fix #9002 '
12295,133001807,toobaz,jreback,2016-02-11 15:21:45,2016-02-12 15:39:40,2016-02-12 14:38:36,closed,,0.18.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12295,b'BUG: Refine validation of parameters to RangeIndex.__init__',b'closes #12288'
12294,132999112,scari,jreback,2016-02-11 15:13:40,2016-02-12 14:30:34,2016-02-12 14:30:24,closed,,0.18.0,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/12294,"b""BUG: Series.plot() doesn't work with CustomBusinessDay frequency""","b""Closes #7222\r\nMissing freqstr 'C' in `_period_code_map`\r\nI found `pandas/tseries/tests/test_frequencies.py:TestFrequencyCode.test_freq_code` cover this change."""
12292,132966302,bteplygin,jreback,2016-02-11 12:52:35,2016-02-27 15:09:05,2016-02-27 15:09:05,closed,,0.18.0,3,Bug;Difficulty Intermediate;Effort Low;IO Excel,https://api.github.com/repos/pydata/pandas/issues/12292,b'read_excel crash with empty Exception',"b'If read one empty column from excel file, read_excel raise Exception without error message.\r\nSmall test case: content of excel file:\r\n\r\nA | B | C | D\r\n---|---|---| ---\r\n | 1 | 100 | f\r\n | 2 | 200 | f\r\n | 3 | 300 | f\r\n | 4 | 400 | f\r\n\r\nRun:\r\n```\r\nimport pandas as pd\r\npd.read_excel(""test_excel_.xls"", parse_cols=[0], header=None)\r\n```\r\nResult:\r\n\r\n    File ""/home/.../python2.7/site-packages/pandas/io/excel.py"", line 170, in read_excelskip_footer=skip_footer, converters=converters, **kwds)\r\n    File ""/home/.../python2.7/site-packages/pandas/io/excel.py"", line 436, in _parse_excel**kwds)\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 1346, in TextParserreturn TextFileReader(*args, **kwds)\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 590, in __init__self._make_engine(self.engine)\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 737, in _make_engineself._engine = klass(self.f, **self.options)\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 1452, in __init__self.columns, self.num_original_columns = self._infer_columns()\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 1735, in _infer_columnsline = self._buffered_line()\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 1793, in _buffered_linereturn self._next_line()\r\n    File ""/home/.../python2.7/site-packages/pandas/io/parsers.py"", line 1817, in _next_lineraise StopIteration\r\n\r\nPython 2.7.7\r\npandas 0.17.1\r\nxlrd 0.9.4'"
12290,132945185,polart,jreback,2016-02-11 11:13:31,2016-02-12 13:44:41,2016-02-12 13:39:08,closed,,0.18.0,6,Bug;Difficulty Intermediate;Effort Low;Timezones,https://api.github.com/repos/pydata/pandas/issues/12290,b'Computing time difference with timezone in Series with one row gives wrong result',"b""When there are only one row containing timestamps with timezone in DataFrame, computing time difference between columns gives incorrect result - always '0 days':\r\n```\r\nIn [133]: import pandas as pd\r\n\r\nIn [134]: df = pd.DataFrame(columns=['s', 'f'])\r\n\r\nIn [135]: df.s = [pd.Timestamp('2016-02-08 13:43:14.605000', tz='America/Sao_Paulo')]\r\n\r\nIn [136]: df.f = [pd.Timestamp('2016-02-10 13:43:14.605000', tz='America/Sao_Paulo')]\r\n\r\nIn [137]: df\r\nOut[137]:\r\n                                 s                                f\r\n0 2016-02-08 13:43:14.605000-02:00 2016-02-10 13:43:14.605000-02:00\r\n\r\nIn [138]: df.f - df.s    # should give '2 days'\r\nOut[138]:\r\n0   0 days\r\nName: f, dtype: timedelta64[ns]\r\n\r\nIn [139]: df.s - df.f    # should give '-2 days'\r\nOut[139]:\r\n0   0 days\r\nName: s, dtype: timedelta64[ns]\r\n```\r\nIf there are more than one row, result is correct:\r\n```\r\nIn [140]: import pandas as pd\r\n\r\nIn [141]: df = pd.DataFrame(columns=['s', 'f'])\r\n\r\nIn [142]: df.s = [pd.Timestamp('2016-02-08 13:43:14.605000', tz='America/Sao_Paulo'), pd.Timestamp('2016-02-08 13:43:14.605000', tz='America/Sao_P aulo')]\r\n\r\nIn [143]: df.f = [pd.Timestamp('2016-02-10 13:43:14.605000', tz='America/Sao_Paulo'), pd.Timestamp('2016-02-11 15:43:14.605000', tz='America/Sao_P aulo')]\r\n\r\nIn [144]: df\r\nOut[144]:\r\n                                 s                                f\r\n0 2016-02-08 13:43:14.605000-02:00 2016-02-10 13:43:14.605000-02:00\r\n1 2016-02-08 13:43:14.605000-02:00 2016-02-11 15:43:14.605000-02:00\r\n\r\nIn [145]: df.f - df.s\r\nOut[145]:\r\n0   2 days 00:00:00\r\n1   3 days 02:00:00\r\nName: f, dtype: timedelta64[ns]\r\n```\r\nOne row containing timestamps in naive datetime also gives correct result:\r\n```\r\nIn [147]: import pandas as pd\r\n\r\nIn [148]: df = pd.DataFrame(columns=['s', 'f'])\r\n\r\nIn [149]: df.s = [pd.Timestamp('2016-02-08 13:43:14.605000')]\r\n\r\nIn [150]: df.f = [pd.Timestamp('2016-02-10 13:43:14.605000')]\r\n\r\nIn [151]: df\r\nOut[151]:\r\n                        s                       f\r\n0 2016-02-08 13:43:14.605 2016-02-10 13:43:14.605\r\n\r\nIn [152]: df.f - df.s\r\nOut[152]:\r\n0   2 days\r\ndtype: timedelta64[ns]\r\n```\r\n\r\nInstalled versions:\r\n```\r\nIn [162]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-49-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 8.0.2\r\nsetuptools: 18.2\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 4.0.3\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.9.2\r\napiclient: 1.4.2\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\nJinja2: None\r\n```"""
12288,132924678,toobaz,jreback,2016-02-11 09:24:39,2016-02-12 14:38:36,2016-02-12 14:38:36,closed,,0.18.0,5,Bug;Difficulty Intermediate;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/12288,b'RangeIndex copy behaviour differs from other indices',"b""It is my understanding that the ``copy`` parameter of ``RangeIndex`` is present virtually only for API compatibility with other indices: however the behaviour for ``copy=False`` differs from other indices in what follows:\r\n\r\n    In [2]: i = pd.Int64Index(range(10))\r\n\r\n    In [3]: i.name = 'original'\r\n\r\n    In [4]: j = pd.Int64Index(i)\r\n\r\n    In [5]: j.name\r\n\r\n    In [6]: j.name = 'copy'\r\n\r\n    In [7]: i.name\r\n    Out[7]: 'original'\r\n\r\n    In [8]: i = pd.RangeIndex(10)\r\n\r\n    In [9]: i.name = 'original'\r\n\r\n    In [10]: j = pd.RangeIndex(i)\r\n\r\n    In [11]: j.name\r\n    Out[11]: 'original'\r\n\r\n    In [12]: j.name = 'copy'\r\n\r\n    In [13]: i.name\r\n    Out[13]: 'copy'\r\n\r\nIn other words, since ``RangeIndex`` doesn't have any data to copy, the value of the ``copy`` parameter should be irrelevant. For homogeneity with other indices, we should always act as if ``copy=True``. The docs should also be changed to state that ``copy`` parameter has no effect and is present only for API compatibility. I'll push a PR if you agree."""
12287,132897844,tshauck,jreback,2016-02-11 06:00:43,2016-02-15 06:43:33,2016-02-12 03:02:26,closed,,0.18.0,7,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12287,"b""BUG: Fixes KeyError when indexes don't overlap.""",b'Closes #10291 '
12284,132770161,gfyoung,jreback,2016-02-10 17:47:21,2016-02-12 14:27:12,2016-02-12 13:42:08,closed,,0.18.0,24,Bug;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/12284,"b""BUG: Allow 'apply' to be used with non-numpy-dtype DataFrames""","b'Addresses issue in #12244 in which a non-numpy-dtype for DataFrame.values causes a `TypeError` to be thrown in the `reduce == True` case for `DataFrame.apply`.  Resolved by first passing `DataFrame.values` through `Series` initialization and taking its `values` attribute, which is an `ndarray` and hence will have a valid `dtype`.  Note that the output of `apply` will still have the original `dtype`.'"
12271,132484513,BranYang,jreback,2016-02-09 17:42:12,2016-02-10 23:06:42,2016-02-10 17:29:34,closed,,0.18.0,3,Bug;Reshaping;Timedelta,https://api.github.com/repos/pydata/pandas/issues/12271,b'Fix #12169 - Resample category data with timedelta index',b'closes #12169 '
12270,132474115,BranYang,jreback,2016-02-09 17:02:22,2016-02-10 23:06:41,2016-02-10 17:37:09,closed,,,2,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/12270,b'Fix #12037 Error when Resampling using pd.tseries.offsets.Nano as period',b'Closes #12037 '
12253,131978657,toobaz,jreback,2016-02-07 15:51:57,2016-02-11 06:21:42,2016-02-11 02:55:17,closed,,0.18.0,2,API Design;Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/12253,b'ENH: Make HDFStore iterable',"b'closes #12221\r\n\r\nHDFStore is not an iterator - but being iterable, it can return an iterator of itself (i.e. of ``.keys()``).'"
12244,131883954,jorisvandenbossche,jreback,2016-02-06 18:21:59,2016-02-12 13:42:33,2016-02-12 13:42:33,closed,,0.18.0,1,Bug;Difficulty Intermediate;Effort Low;Timezones,https://api.github.com/repos/pydata/pandas/issues/12244,b'BUG: DataFrame.apply fails on timezone aware datetime data',"b'```\r\nIn [29]: df = pd.DataFrame({\'dt\': pd.date_range(""2015-01-01"", periods=3, tz=\'Europe/Brussels\')})\r\n\r\nIn [30]: df\r\nOut[30]:\r\n                         dt\r\n0 2015-01-01 00:00:00+01:00\r\n1 2015-01-02 00:00:00+01:00\r\n2 2015-01-03 00:00:00+01:00\r\n\r\nIn [31]: df.apply(lambda x: x)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-31-9cd68f0fd3ff> in <module>()\r\n----> 1 df.apply(lambda x: x)\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\frame.py in apply(self, func, a\r\nxis, broadcast, raw, reduce, args, **kwds)\r\n   4029                     if reduce is None:\r\n   4030                         reduce = True\r\n-> 4031                     return self._apply_standard(f, axis, reduce=reduce)\r\n   4032             else:\r\n   4033                 return self._apply_broadcast(f, axis)\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\frame.py in _apply_standard(sel\r\nf, func, axis, ignore_failures, reduce)\r\n   4076             # Create a dummy Series from an empty array\r\n   4077             index = self._get_axis(axis)\r\n-> 4078             empty_arr = np.empty(len(index), dtype=values.dtype)\r\n   4079             dummy = Series(empty_arr, index=self._get_axis(axis),\r\n   4080                            dtype=values.dtype)\r\n\r\nTypeError: data type not understood\r\n\r\n```\r\n(from [SO](http://stackoverflow.com/questions/35241673/pandas-df-apply-typeerror-data-type-not-understood#35244563))'"
12241,131858228,sinhrks,sinhrks,2016-02-06 14:03:34,2016-02-11 23:43:23,2016-02-11 23:35:54,closed,,0.18.0,3,Bug;Missing-data;Timezones,https://api.github.com/repos/pydata/pandas/issues/12241,b'BUG: Timestamp subtraction of NaT with timezones',b'Closes #11718.\r\n\r\n- Ops with ``NaT`` and ``Timestamp`` with ``tz`` results in ``NaT``\r\n- Ops with ``NaT`` and ``DatetimeIndex`` /  ``TimedeltaIndex`` results in corresponding ``Index`` filled with ``NaT``.'
12217,130965001,andyljones,jreback,2016-02-03 09:22:52,2016-02-13 01:15:50,2016-02-13 01:15:50,closed,,0.18.0,2,Bug;Difficulty Intermediate;Effort Low;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/12217,b'Concatenating two localized datetimes returns NaTs  ',"b""Issue:\r\n```python\r\nimport pandas as pd\r\nfrom datetime import datetime\r\n\r\n### Concat'ing two UTC times works\r\nfirst = pd.DataFrame([[datetime(2016, 1, 1)]])\r\nfirst[0] = first[0].dt.tz_localize('UTC')\r\n \r\nsecond = pd.DataFrame([[datetime(2016, 1, 2)]])\r\nsecond[0] = second[0].dt.tz_localize('UTC')\r\n \r\nprint pd.concat([first, second]) # Works\r\n\r\n \r\n### Concat'ing two London times doesn't work\r\nfirst = pd.DataFrame([[datetime(2016, 1, 1)]])\r\nfirst[0] = first[0].dt.tz_localize('Europe/London')\r\n \r\nsecond = pd.DataFrame([[datetime(2016, 1, 2)]])\r\nsecond[0] = second[0].dt.tz_localize('Europe/London')\r\n \r\nprint pd.concat([first, second]) # Doesn't work - returns a DF full of NaTs\r\n\r\n \r\n### Concat'ing 2+1 London times works\r\nfirst = pd.DataFrame([[datetime(2016, 1, 1)], [datetime(2016, 1, 2)]])\r\nfirst[0] = first[0].dt.tz_localize('Europe/London')\r\n \r\nsecond = pd.DataFrame([[datetime(2016, 1, 3)]])\r\nsecond[0] = second[0].dt.tz_localize('Europe/London')\r\n\r\nprint pd.concat([first, second]) # Works\r\n\r\n \r\n### Concat'ing 1+2 London times doesn't work\r\nfirst = pd.DataFrame([[datetime(2016, 1, 1)]])\r\nfirst[0] = first[0].dt.tz_localize('Europe/London')\r\n \r\nsecond = pd.DataFrame([[datetime(2016, 1, 2)], [datetime(2016, 1, 3)]])\r\nsecond[0] = second[0].dt.tz_localize('Europe/London')\r\n\r\nprint pd.concat([first, second]) # Doesn't work - first row is NaT\r\n````\r\n\r\n\r\nDependencies:\r\n```` \r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.17.7-300.fc21.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 8.0.2\r\nsetuptools: 19.4\r\nCython: None\r\nnumpy: 1.10.2\r\nscipy: 0.16.0\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.5.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n````"""
12215,130862686,bennorth,jreback,2016-02-03 00:12:18,2016-02-09 22:15:54,2016-02-09 22:03:11,closed,,0.18.0,17,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12215,"b""BUG: Strings like '2E' are incorrectly parsed as valid floats""","b""A work colleague, David Chase, encountered some surprising behaviour, which can be reduced to the following.  The data-frame\r\n\r\n```\r\nDataFrame({'x': [2.5], 'y': [42], 'z': ['2E']})\r\n```\r\n\r\ndoes not round-trip correctly.  The string '2E' is interpreted as a valid float, but it should not be (according to man strtod(3), which seems a reasonable spec).\r\n\r\nThis PR changes the three variants of `xstrtod()` to reject a string where no digits follow the 'e' or 'E', and includes tests for this case."""
12211,130804016,uzihs,jreback,2016-02-02 20:16:13,2016-04-30 14:32:55,2016-04-30 14:32:55,closed,,0.18.1,9,Bug;Difficulty Intermediate;Effort Low;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/12211,b'str(data_frame) raises an exception when there is a column with both NaT value and Timestamp with timezone info',"b'When you try to print a data frame that has a Timestamp column with both NaT values and Timestamp with timezone, AND the number or rows is more than the display setting - an exception is raised.\r\n\r\n    import pandas as pd\r\n    from datetime import datetime\r\n    from pytz import UTC\r\n    test=pd.DataFrame([{""dt"": pd.Timestamp(datetime.now().replace(tzinfo=UTC)), ""x"": 1}] + [{""dt"": pd.NaT, ""x"": 1}] * 5)\r\n    pd.options.display.max_rows = 5\r\n    str(test)\r\n\r\n\r\nThis is the traceback when running the code above with 0.17.1 (with 0.17.0, the traceback is different, with this error message: AttributeError: \'numpy.ndarray\' object has no attribute \'tz_localize\')\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    339             method = _safe_get_formatter_method(obj, self.print_method)\r\n    340             if method is not None:\r\n--> 341                 return method()\r\n    342             return None\r\n    343         else:\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in _repr_html_(self)\r\n    573             return self.to_html(max_rows=max_rows, max_cols=max_cols,\r\n    574                                 show_dimensions=show_dimensions,\r\n--> 575                                 notebook=True)\r\n    576         else:\r\n    577             return None\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in to_html(self, buf, columns, col_space, colSpace, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, bold_rows, classes, escape, max_rows, max_cols, show_dimensions, notebook)\r\n   1529                                            max_rows=max_rows,\r\n   1530                                            max_cols=max_cols,\r\n-> 1531                                            show_dimensions=show_dimensions)\r\n   1532         # TODO: a generic formatter wld b in DataFrameFormatter\r\n   1533         formatter.to_html(classes=classes, notebook=notebook)\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in __init__(self, frame, buf, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, line_width, max_rows, max_cols, show_dimensions, **kwds)\r\n    378             self.columns = frame.columns\r\n    379 \r\n--> 380         self._chk_truncate()\r\n    381         self.adj = _get_adjustment()\r\n    382 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in _chk_truncate(self)\r\n    444             else:\r\n    445                 row_num = max_rows_adj // 2\r\n--> 446                 frame = concat((frame.iloc[:row_num, :], frame.iloc[-row_num:, :]))\r\n    447             self.tr_row_num = row_num\r\n    448 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\r\n    811                        verify_integrity=verify_integrity,\r\n    812                        copy=copy)\r\n--> 813     return op.get_result()\r\n    814 \r\n    815 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in get_result(self)\r\n    993 \r\n    994             new_data = concatenate_block_managers(\r\n--> 995                 mgrs_indexers, self.new_axes, concat_axis=self.axis, copy=self.copy)\r\n    996             if not self.copy:\r\n    997                 new_data._consolidate_inplace()\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)\r\n   4454                                                 copy=copy),\r\n   4455                          placement=placement)\r\n-> 4456               for placement, join_units in concat_plan]\r\n   4457 \r\n   4458     return BlockManager(blocks, axes)\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\r\n   4454                                                 copy=copy),\r\n   4455                          placement=placement)\r\n-> 4456               for placement, join_units in concat_plan]\r\n   4457 \r\n   4458     return BlockManager(blocks, axes)\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_join_units(join_units, concat_axis, copy)\r\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\r\n   4552                                          upcasted_na=upcasted_na)\r\n-> 4553                  for ju in join_units]\r\n   4554 \r\n   4555     if len(to_concat) == 1:\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\r\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\r\n   4552                                          upcasted_na=upcasted_na)\r\n-> 4553                  for ju in join_units]\r\n   4554 \r\n   4555     if len(to_concat) == 1:\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in get_reindexed_values(self, empty_dtype, upcasted_na)\r\n   4799 \r\n   4800             if self.is_null and not getattr(self.block,\'is_categorical\',None):\r\n-> 4801                 missing_arr = np.empty(self.shape, dtype=empty_dtype)\r\n   4802                 if np.prod(self.shape):\r\n   4803                     # NumPy 1.6 workaround: this statement gets strange if all\r\n\r\nTypeError: data type not understood\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    695                 type_pprinters=self.type_printers,\r\n    696                 deferred_pprinters=self.deferred_printers)\r\n--> 697             printer.pretty(obj)\r\n    698             printer.flush()\r\n    699             return stream.getvalue()\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    381                             if callable(meth):\r\n    382                                 return meth(obj, self, cycle)\r\n--> 383             return _default_pprint(obj, self, cycle)\r\n    384         finally:\r\n    385             self.end_group()\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/lib/pretty.py in _default_pprint(obj, p, cycle)\r\n    501     if _safe_getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\r\n    502         # A user-provided repr. Find newlines and replace them with p.break_()\r\n--> 503         _repr_pprint(obj, p, cycle)\r\n    504         return\r\n    505     p.begin_group(1, \'<\')\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    683     """"""A pprint that just redirects to the normal repr function.""""""\r\n    684     # Find newlines and replace them with p.break_()\r\n--> 685     output = repr(obj)\r\n    686     for idx,output_line in enumerate(output.splitlines()):\r\n    687         if idx:\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/base.py in __repr__(self)\r\n     61         Yields Bytestring in Py2, Unicode String in py3.\r\n     62         """"""\r\n---> 63         return str(self)\r\n     64 \r\n     65 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/base.py in __str__(self)\r\n     40 \r\n     41         if compat.PY3:\r\n---> 42             return self.__unicode__()\r\n     43         return self.__bytes__()\r\n     44 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in __unicode__(self)\r\n    539             width = None\r\n    540         self.to_string(buf=buf, max_rows=max_rows, max_cols=max_cols,\r\n--> 541                        line_width=width, show_dimensions=show_dimensions)\r\n    542 \r\n    543         return buf.getvalue()\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in to_string(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, line_width, max_rows, max_cols, show_dimensions)\r\n   1478                                            max_rows=max_rows,\r\n   1479                                            max_cols=max_cols,\r\n-> 1480                                            show_dimensions=show_dimensions)\r\n   1481         formatter.to_string()\r\n   1482 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in __init__(self, frame, buf, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, line_width, max_rows, max_cols, show_dimensions, **kwds)\r\n    378             self.columns = frame.columns\r\n    379 \r\n--> 380         self._chk_truncate()\r\n    381         self.adj = _get_adjustment()\r\n    382 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in _chk_truncate(self)\r\n    444             else:\r\n    445                 row_num = max_rows_adj // 2\r\n--> 446                 frame = concat((frame.iloc[:row_num, :], frame.iloc[-row_num:, :]))\r\n    447             self.tr_row_num = row_num\r\n    448 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\r\n    811                        verify_integrity=verify_integrity,\r\n    812                        copy=copy)\r\n--> 813     return op.get_result()\r\n    814 \r\n    815 \r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in get_result(self)\r\n    993 \r\n    994             new_data = concatenate_block_managers(\r\n--> 995                 mgrs_indexers, self.new_axes, concat_axis=self.axis, copy=self.copy)\r\n    996             if not self.copy:\r\n    997                 new_data._consolidate_inplace()\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)\r\n   4454                                                 copy=copy),\r\n   4455                          placement=placement)\r\n-> 4456               for placement, join_units in concat_plan]\r\n   4457 \r\n   4458     return BlockManager(blocks, axes)\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\r\n   4454                                                 copy=copy),\r\n   4455                          placement=placement)\r\n-> 4456               for placement, join_units in concat_plan]\r\n   4457 \r\n   4458     return BlockManager(blocks, axes)\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_join_units(join_units, concat_axis, copy)\r\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\r\n   4552                                          upcasted_na=upcasted_na)\r\n-> 4553                  for ju in join_units]\r\n   4554 \r\n   4555     if len(to_concat) == 1:\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\r\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\r\n   4552                                          upcasted_na=upcasted_na)\r\n-> 4553                  for ju in join_units]\r\n   4554 \r\n   4555     if len(to_concat) == 1:\r\n\r\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in get_reindexed_values(self, empty_dtype, upcasted_na)\r\n   4799 \r\n   4800             if self.is_null and not getattr(self.block,\'is_categorical\',None):\r\n-> 4801                 missing_arr = np.empty(self.shape, dtype=empty_dtype)\r\n   4802                 if np.prod(self.shape):\r\n   4803                     # NumPy 1.6 workaround: this statement gets strange if all\r\n\r\nTypeError: data type not understood'"
12202,130633756,m313,jreback,2016-02-02 10:02:05,2016-02-12 20:40:12,2016-02-12 20:40:12,closed,,0.18.0,1,Bug;Resample;Testing,https://api.github.com/repos/pydata/pandas/issues/12202,"b'Resampling converts int to float, but only in group by'","b""```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({'date': pd.date_range(start='2016-01-01', periods=4, freq='W'),\r\n               'group': [1, 1, 2, 2],\r\n               'val': [5, 6, 7, 8]})\r\ndf['val'] = df['val'].astype(np.int32)\r\ndf.set_index('date', inplace=True)\r\n\r\ndf['val'].dtype\r\n#[out] dtype('int32')\r\n```\r\n\r\nCalling resample() on the dataframe does not change the type.\r\n\r\n```python\r\ndf1 = df.resample('1D', fill_method='ffill')\r\ndf1['val'].dtype\r\n#[out] dtype('int32')\r\n```\r\n\r\nHowever, when calling resample() in a group by statement, float type is returned!\r\n\r\n```python\r\ndf2 = df.groupby('group').resample('1D', fill_method='ffill')\r\ndf2['val'].dtype\r\n#[out] dtype('float64')\r\n```\r\n\r\nWhy is val converted to float in the group by statement?\r\nI originally posted this on [stack overflow](http://stackoverflow.com/questions/35085130/resampling-converts-int-to-float-but-only-in-group-by).\r\n\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.2\r\nCython: 0.23.4\r\nnumpy: 1.10.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.5.0\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```"""
12196,130466600,jreback,jreback,2016-02-01 19:56:57,2016-02-01 21:20:39,2016-02-01 21:20:39,closed,,0.18.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/12196,b'BUG: numpy master quantile interpolation method',b'https://github.com/numpy/numpy/issues/7163'
12192,130255115,DGrady,jreback,2016-02-01 04:46:39,2016-02-01 20:09:21,2016-02-01 20:09:16,closed,,0.18.0,1,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/12192,"b""BUG: Handle variables named 'name' in get_dummies, #12180""",b'closes #12180 '
12184,130001145,chris-b1,jreback,2016-01-30 15:47:27,2016-02-01 00:25:12,2016-01-30 19:22:40,closed,,,3,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/12184,b'BUG: read_excel with squeeze=True',b'closes #12157'
12180,129843114,DGrady,jreback,2016-01-29 18:16:15,2016-02-01 20:09:16,2016-02-01 20:09:16,closed,,0.18.0,3,Bug;Difficulty Novice;Effort Low;Strings,https://api.github.com/repos/pydata/pandas/issues/12180,"b""Series.str.get_dummies fails if one of the categorical variables is called 'name'""","b'This works as expected:\r\n\r\n```python\r\n>>> s = pd.Series([\'Name\', \'email|Name|address\', \'address|email\'])\r\n>>> s.str.get_dummies(sep=\'|\')\r\n   Name  address  email\r\n0     1        0      0\r\n1     1        1      1\r\n2     0        1      1\r\n```\r\n\r\nHowever, if any of the categorical variables is named exactly `\'name\'`, then there\'s a problem.\r\n\r\n```python\r\n>>> s = pd.Series([\'name\', \'email|Name|address\', \'address|email\'])\r\n>>> s.str.get_dummies(sep=\'|\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-1a39a6dcd56b> in <module>()\r\n----> 1 s.str.get_dummies(sep=\'|\')\r\n\r\n/Users/dgrady/anaconda/envs/python3/lib/python3.5/site-packages/pandas/core/strings.py in get_dummies(self, sep)\r\n   1377         data = self._orig.astype(str) if self._is_categorical else self._data\r\n   1378         result = str_get_dummies(data, sep)\r\n-> 1379         return self._wrap_result(result, use_codes=(not self._is_categorical))\r\n   1380 \r\n   1381     @copy(str_translate)\r\n\r\n/Users/dgrady/anaconda/envs/python3/lib/python3.5/site-packages/pandas/core/strings.py in _wrap_result(self, result, use_codes, name)\r\n   1100         if not hasattr(result, \'ndim\'):\r\n   1101             return result\r\n-> 1102         name = name or getattr(result, \'name\', None) or self._orig.name\r\n   1103 \r\n   1104         if result.ndim == 1:\r\n\r\n/Users/dgrady/anaconda/envs/python3/lib/python3.5/site-packages/pandas/core/generic.py in __nonzero__(self)\r\n    729         raise ValueError(""The truth value of a {0} is ambiguous. ""\r\n    730                          ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""\r\n--> 731                          .format(self.__class__.__name__))\r\n    732 \r\n    733     __bool__ = __nonzero__\r\n\r\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n```\r\n\r\nThis is presumably related to the `pandas/core/strings.py` code in the stacktrace.\r\n\r\n```python\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 18.5\r\nCython: 0.23.4\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.4.4\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8\r\n```'"
12169,129526690,mapa17,jreback,2016-01-28 17:58:27,2016-02-10 17:29:34,2016-02-10 17:29:34,closed,,0.18.0,3,Bug;Categorical;Difficulty Intermediate;Effort Medium;Resample,https://api.github.com/repos/pydata/pandas/issues/12169,b'Resample category data with timedelta index',"b""Hi,\r\n\r\nI get a very strange behavior when i try to resample categorical data with and timedelta index, as compared to a datetime index. \r\n```pandas\r\n>> d1 = pd.DataFrame({'Group_obj': 'A'}, index=pd.date_range('2000-1-1', periods=20, freq='s'))\r\n>> d1['Group'] = d1['Group_obj'].astype('category')\r\n>> d1\r\n                    Group_obj Group\r\n2000-01-01 00:00:00         A     A\r\n2000-01-01 00:00:01         A     A\r\n2000-01-01 00:00:02         A     A\r\n2000-01-01 00:00:03         A     A\r\n2000-01-01 00:00:04         A     A\r\n2000-01-01 00:00:05         A     A\r\n2000-01-01 00:00:06         A     A\r\n2000-01-01 00:00:07         A     A\r\n2000-01-01 00:00:08         A     A\r\n2000-01-01 00:00:09         A     A\r\n2000-01-01 00:00:10         A     A\r\n2000-01-01 00:00:11         A     A\r\n2000-01-01 00:00:12         A     A\r\n2000-01-01 00:00:13         A     A\r\n2000-01-01 00:00:14         A     A\r\n2000-01-01 00:00:15         A     A\r\n2000-01-01 00:00:16         A     A\r\n2000-01-01 00:00:17         A     A\r\n2000-01-01 00:00:18         A     A\r\n2000-01-01 00:00:19         A     A\r\n\r\n>> corr = d1.resample('10s', how=lambda x: (x.value_counts().index[0]))\r\n>> corr\r\n                    Group_obj Group\r\n2000-01-01 00:00:00         A     A\r\n2000-01-01 00:00:10         A     A\r\n\r\n>> corr.dtypes\r\nGroup_obj    object\r\nGroup        object\r\ndtype: object\r\n\r\n>> d2 = d1.set_index(pd.to_timedelta(list(range(20)), unit='s'))\r\n>> fxx = d2.resample('10s', how=lambda x: (x.value_counts().index[0]))\r\n>> fxx\r\n         Group_obj  Group\r\n00:00:00         A    NaN\r\n00:00:10         A    NaN\r\n\r\n>> fxx.dtypes\r\nGroup_obj     object\r\nGroup        float64\r\ndtype: object\r\n```\r\n\r\nIt seems to me the aggregated result in case of using timedelta as an index for the category is always NaN.\r\nShould this be?\r\n\r\nThx\r\n\r\nPS: is there a way to specify the dtype for the aggregated columns?"""
12158,129204958,nbonnotte,jreback,2016-01-27 16:54:27,2016-01-28 21:46:26,2016-01-28 21:28:09,closed,,0.18.0,2,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/12158,"b'BUG in MultiIndex.drop for not-lexsorted multi-indexes, #12078'",b'Closes #12078'
12157,129178697,rafguns,jreback,2016-01-27 15:33:44,2016-01-30 19:22:40,2016-01-30 19:22:40,closed,,0.18.0,1,Bug;Difficulty Novice;Effort Low;IO Excel;Regression,https://api.github.com/repos/pydata/pandas/issues/12157,"b'read_excel(..., index_col=0, squeeze=True) raises AttributeError'","b""`pd.read_excel` with keyword argument `squeeze=True` raises an AttributeError. This is unexpected, in that:\r\n* I would expect it to work analogously to `pd.read_csv`, which does not exhibit the same problem\r\n* it worked fine in a previous version (0.15 I think?)\r\n\r\nSteps to reproduce:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.17.1'\r\n\r\nIn [3]: df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\r\n\r\nIn [4]: df.to_csv('tmp.csv', index=False)\r\n\r\nIn [5]: df.to_excel('tmp.xlsx', index=False)\r\n\r\nIn [6]: pd.read_csv('tmp.csv', index_col=0, squeeze=True)\r\nOut[6]:\r\na\r\n1    3\r\n2    4\r\nName: b, dtype: int64\r\n\r\nIn [7]: pd.read_excel('tmp.xlsx', index_col=0, squeeze=True)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-ed256b323894> in <module>()\r\n----> 1 pd.read_excel('tmp.xlsx', index_col=0, squeeze=True)\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py in read_excel(io, sheetname, header, skiprows, skip_footer, index_col, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, engine, **kwds)\r\n    168         date_parser=date_parser, na_values=na_values, thousands=thousands,\r\n    169         convert_float=convert_float, has_index_names=has_index_names,\r\n--> 170         skip_footer=skip_footer, converters=converters, **kwds)\r\n    171\r\n    172 class ExcelFile(object):\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py in _parse_excel(self, sheetname, header, skiprows, skip_footer, index_col, has_index_names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, verbose, **kwds)\r\n    437\r\n    438             output[asheetname] = parser.read()\r\n--> 439             output[asheetname].columns = output[asheetname].columns.set_names(header_names)\r\n    440\r\n    441         if ret_dict:\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in __getattr__(self, name)\r\n   2353                 or name in self._metadata\r\n   2354                 or name in self._accessors):\r\n-> 2355             return object.__getattribute__(self, name)\r\n   2356         else:\r\n   2357             if name in self._info_axis:\r\n\r\nAttributeError: 'Series' object has no attribute 'columns'\r\n```"""
12151,128988813,jreback,jreback,2016-01-27 00:41:15,2016-01-27 12:59:13,2016-01-27 12:59:13,closed,,0.18.0,0,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/12151,b'BUG: getitem and a series with a non-ndarray values',b'closes #12089'
12137,128620589,xflr6,xflr6,2016-01-25 19:43:27,2016-02-13 10:33:27,2016-02-13 10:24:44,closed,,0.18.0,2,Bug;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/12137,b'BUG: style.set_precision(0) displays spurious .0',b'Fixes #12134'
12135,128590450,selasley,jreback,2016-01-25 17:13:39,2016-01-27 16:56:42,2016-01-27 15:12:25,closed,,0.18.0,10,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12135,b'BUG: set src->buffer = NULL after garbage collecting it in buffer_rd_\xa1\xad',b'Issue #12098\r\n\r\nAdd src->buffer = NULL; after garbage collecting src->buffer in the buffer_rd_bytes routine in io.c to fix the segfault'
12134,128569266,xflr6,jreback,2016-01-25 16:03:20,2016-02-12 20:42:51,2016-02-12 20:42:51,closed,,0.18.0,6,Bug;Difficulty Novice;Effort Low;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/12134,b'style.set_precision(0) displays spurious .0',b'The expected display format would be as in [2]:\r\n![screenshot](https://cloud.githubusercontent.com/assets/6342379/12556293/77593db2-c385-11e5-89e1-69a2bdd0a049.png)\r\n'
12133,128489800,Winand,jreback,2016-01-25 08:52:00,2016-04-17 14:31:40,2016-04-17 14:31:40,closed,,0.18.1,2,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/12133,b'crosstab dropna=False breaks columns.names',"b'Specifying `dropna=False` drops column level names:\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame({""Col1"": [23], ""Col2"": [45]})\r\nnames1 = pd.crosstab(df[""Col1""], df[""Col2""]).columns.names\r\nnames2 = pd.crosstab(df[""Col1""], df[""Col2""], dropna=False).columns.names\r\nprint(names1, names2)\r\n>>>[\'Col2\'] [None]\r\n```\r\nP.S. At first i thought `dropna=False` will help to count `NaN` values, though it seems that `fillna` is needed. ~~So i don\'t understand, why this parameter is needed at all.~~ If entire column is `NaN` then we\'ll get `KeyError: \'__dummy__\'` anyway.\r\n\r\npandas: 0.17.1'"
12132,128479173,Winand,jreback,2016-01-25 07:32:27,2016-01-30 15:19:56,2016-01-30 15:19:56,closed,,,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12132,b'to_csv locale dependent format {:n} support',b'closes #11812 '
12126,128376058,HHammond,TomAugspurger,2016-01-24 06:05:36,2016-01-24 19:49:26,2016-01-24 19:49:26,closed,,,8,Bug;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/12126,b'Fix Styler._translate failing on numeric columns',b'Addresses https://github.com/pydata/pandas/issues/12125.\r\n\r\nThe previous behaviour was to use a `__getitem__` lookup which caused incorrect behaviour with numeric columns. The adjusted code uses `iloc` numerical indexing to resolve the issue.'
12125,128375394,HHammond,jreback,2016-01-24 05:52:59,2016-02-12 20:42:51,2016-02-12 20:42:51,closed,,0.18.0,0,Bug;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/12125,b'Styler class fails to render numeric columns when 0 not in columns',"b'The `core.style.Styler._translate` method uses `__getitem__` indexing when working with numerical cell locations (https://github.com/pydata/pandas/blob/master/pandas/core/style.py#L240). When using numeric dataframe columns the location based index causes confusion with the column name index, causing an error. \r\n\r\n```python\r\ndf = pd.DataFrame({i: range(10) for i in range(1,10)})\r\ndf.style._translate()\r\n```\r\n\r\nThe expected behaviour should be to always use location based indexing. Changing [line 240](https://github.com/pydata/pandas/blob/master/pandas/core/style.py#L240) to:\r\n\r\n```python\r\n    ""value"": self.data.iloc[r].iloc[c],\r\n```\r\n\r\nFixes the error.\r\n\r\nI do currently have a PR to fix this but wanted to file the bug report first. \r\n\r\n##### Error code\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/usr/local/lib/python3.5/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    339             method = _safe_get_formatter_method(obj, self.print_method)\r\n    340             if method is not None:\r\n--> 341                 return method()\r\n    342             return None\r\n    343         else:\r\n\r\n/usr/local/lib/python3.5/site-packages/pandas/core/style.py in _repr_html_(self)\r\n    158         Hooks into Jupyter notebook rich display system.\r\n    159         \'\'\'\r\n--> 160         return self.render()\r\n    161 \r\n    162     def _translate(self):\r\n\r\n/usr/local/lib/python3.5/site-packages/pandas/core/style.py in render(self)\r\n    259         """"""\r\n    260         self._compute()\r\n--> 261         d = self._translate()\r\n    262         # filter out empty styles, every cell will have a class\r\n    263         # but the list of props may just be [[\'\', \'\']].\r\n\r\n/usr/local/lib/python3.5/site-packages/pandas/core/style.py in _translate(self)\r\n    220                 cs = [DATA_CLASS, ""row%s"" % r, ""col%s"" % c]\r\n    221                 cs.extend(cell_context.get(""data"", {}).get(r, {}).get(c, []))\r\n--> 222                 row_es.append({""type"": ""td"", ""value"": self.data.iloc[r][c],\r\n    223                                ""class"": "" "".join(cs), ""id"": ""_"".join(cs[1:])})\r\n    224                 props = []\r\n\r\n/usr/local/lib/python3.5/site-packages/pandas/core/series.py in __getitem__(self, key)\r\n    555     def __getitem__(self, key):\r\n    556         try:\r\n--> 557             result = self.index.get_value(self, key)\r\n    558 \r\n    559             if not np.isscalar(result):\r\n\r\n/usr/local/lib/python3.5/site-packages/pandas/core/index.py in get_value(self, series, key)\r\n   1788 \r\n   1789         try:\r\n-> 1790             return self._engine.get_value(s, k)\r\n   1791         except KeyError as e1:\r\n   1792             if len(self) > 0 and self.inferred_type in [\'integer\',\'boolean\']:\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_value (pandas/index.c:3204)()\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_value (pandas/index.c:2903)()\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:3843)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6525)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6463)()\r\n\r\nKeyError: 0\r\n```\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 19.4\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: 0.16.0\r\nstatsmodels: None\r\nIPython: 4.0.3\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8'"
12110,127913524,Sereger13,jreback,2016-01-21 12:25:26,2016-01-24 22:40:27,2016-01-24 22:40:27,closed,,0.18.0,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12110,"b'ERR: read_csv with dtype on empty data, #12048.'",b'ERR: read_csv with dtype specified on empty data \r\n\r\ncloses #12048.'
12104,127802321,szs8,jreback,2016-01-20 22:47:28,2016-01-20 23:05:03,2016-01-20 23:04:48,closed,,,1,Bug;Indexing;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/12104,b'Setting values on a DataFrame with tz aware DatetimeIndex fails',"b'```python\r\n\r\ntdf = pd.DataFrame({\'a\': np.arange(10), \'b\': np.arange(10) + 5}, \r\n                   index=pd.date_range(\'20160119 09:00\', freq=\'5T\', \r\n                                       periods=10, tz=\'US/Eastern\'))\r\nnew_values = tdf.sample(2)[\'b\'] + 100\r\ntdf.loc[new_values.index, \'b\'] = new_values.values\r\n```\r\nThis fails with ```KeyError: ""[\'2016-01-19T14:00:00.000000000+0000\' \'2016-01-19T14:05:00.000000000+0000\'] not in index```\r\n\r\nThis used to work. Not sure which version broke it.\r\n'"
12098,127671089,alessiodore,jreback,2016-01-20 12:23:14,2016-01-27 15:12:25,2016-01-27 15:12:25,closed,,0.18.0,18,Bug;Difficulty Intermediate;Effort Medium;IO CSV,https://api.github.com/repos/pydata/pandas/issues/12098,b'Core dumped in read_csv (C engine) when reading multiple corrupted gzip files',"b'I am using read_csv to read some gzip compressed log files. Some of these files are corrupted and they cannot be uncompressed. \r\nAt different iterations in the loop that reads these files my script crashes with a core dumped message:  \r\n*** Error in `/usr/bin/python\': corrupted double-linked list: 0x0000000003836790 ***\r\nor just:\r\nSegmentation fault (core dumped)\r\n\r\nThis is a stripped-down version (just looping over one of the corrupted files) of the code where this error occurs:\r\n    \r\n    import pandas as pd\r\n\r\n    for i in xrange(n):\r\n        try:\r\n             pd.read_csv(fPath,delim_whitespace=True,header=None, compression=\'gzip\')\r\n        except Exception,e:\r\n             continue\r\n\r\nThe traceback of the catched exception is:\r\n\r\nFile ""/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 498, in parser_f \r\n   return _read(filepath_or_buffer, kwds)\r\nFile ""/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 285, in _read \r\n   return parser.read()\r\nFile ""/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 747, in read \r\n   ret = self._engine.read(nrows)\r\nFile ""/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 1197, in read \r\n   data = self._reader.read(nrows)\r\nFile ""pandas/parser.pyx"", line 766, in pandas.parser.TextReader.read (pandas/parser.c:7988)\r\nFile ""pandas/parser.pyx"", line 788, in pandas.parser.TextReader._read_low_memory (pandas/parser.c:8244)\r\nFile ""pandas/parser.pyx"", line 842, in pandas.parser.TextReader._read_rows (pandas/parser.c:8970)\r\nFile ""pandas/parser.pyx"", line 829, in pandas.parser.TextReader._tokenize_rows (pandas/parser.c:8838)\r\nFile ""pandas/parser.pyx"", line 1833, in pandas.parser.raise_parser_error (pandas/parser.c:22649)\r\nCParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine=\'python\'.\r\n\r\nIf I remove the delim_whitespace argument the loop completes without segmentation fault. I tried adding low_memory=False but the program still crashes. \r\nI am using pandas version 0.17.1 on Ubuntu 14.04 OS. \r\n\r\nIt looks like a similar issue to #5664 but the problems should have been resolved in v0.16.1\r\n\r\n\r\n\r\n'"
12089,127409832,JackKelly,jreback,2016-01-19 10:03:21,2016-01-27 13:03:32,2016-01-27 12:59:13,closed,,0.18.0,8,Bug;Regression;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/12089,b'Indexing into Series of tz-aware datetime64s fails using __getitem__',"b'I\'m a huge fan of Pandas.  Thanks for all the hard work!\r\n\r\nI believe I have stumbled across a small bug in Pandas 0.17.1 which was not present in 0.16.2.  Indexing into Series of *timezone-aware* `datetime64`s fails using `__getitem__` but indexing succeeds if the `datetime64`s are timezone-*naive*.  Here is a minimal code example and the exception produced by Pandas 0.17.1:\r\n\r\n```python\r\nIn [37]: dates_with_tz = pd.date_range(""2011-01-01"", periods=3, tz=""US/Eastern"")\r\n\r\nIn [46]: dates_with_tz\r\nOut[46]: \r\nDatetimeIndex([\'2011-01-01 00:00:00-05:00\', \'2011-01-02 00:00:00-05:00\',\r\n               \'2011-01-03 00:00:00-05:00\'],\r\n              dtype=\'datetime64[ns, US/Eastern]\', freq=\'D\')\r\n\r\nIn [38]: s_with_tz = pd.Series(dates_with_tz, index=[\'a\', \'b\', \'c\'])\r\n\r\nIn [39]: s_with_tz\r\nOut[39]: \r\na   2011-01-01 00:00:00-05:00\r\nb   2011-01-02 00:00:00-05:00\r\nc   2011-01-03 00:00:00-05:00\r\ndtype: datetime64[ns, US/Eastern]\r\n\r\nIn [40]: s_with_tz[\'a\']\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-40-81d0bf655282> in <module>()\r\n----> 1 s_with_tz[\'a\']\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc in __getitem__(self, key)\r\n    555     def __getitem__(self, key):\r\n    556         try:\r\n--> 557             result = self.index.get_value(self, key)\r\n    558 \r\n    559             if not np.isscalar(result):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/index.pyc in get_value(self, series, key)\r\n   1778         s = getattr(series,\'_values\',None)\r\n   1779         if isinstance(s, Index) and lib.isscalar(key):\r\n-> 1780             return s[key]\r\n   1781 \r\n   1782         s = _values_from_object(series)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/tseries/base.pyc in __getitem__(self, key)\r\n     98         getitem = self._data.__getitem__\r\n     99         if np.isscalar(key):\r\n--> 100             val = getitem(key)\r\n    101             return self._box_func(val)\r\n    102         else:\r\n\r\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\r\n```\r\n\r\nIf the dates are timezone-aware then we can access them using `loc` but, as far as I\'m aware, we should be able to use `__getitem__` in this situation too:\r\n\r\n```python\r\nIn [41]: s_with_tz.loc[\'a\']\r\nOut[41]: Timestamp(\'2011-01-01 00:00:00-0500\', tz=\'US/Eastern\')\r\n```\r\n\r\nHowever, if the dates are timezone-*naive* then indexing using `__getitem__` works as expected:\r\n\r\n```python\r\nIn [32]: dates_naive = pd.date_range(""2011-01-01"", periods=3)\r\n\r\nIn [33]: dates_naive\r\nOut[33]: DatetimeIndex([\'2011-01-01\', \'2011-01-02\', \'2011-01-03\'], dtype=\'datetime64[ns]\', freq=\'D\')\r\n\r\nIn [34]: s = pd.Series(dates_naive, index=[\'a\', \'b\', \'c\'])\r\n\r\nIn [35]: s\r\nOut[35]: \r\na   2011-01-01\r\nb   2011-01-02\r\nc   2011-01-03\r\ndtype: datetime64[ns]\r\n\r\nIn [36]: s[\'a\']\r\nOut[36]: Timestamp(\'2011-01-01 00:00:00\')\r\n```\r\n\r\nSo indexing into a `Series` using `__getitem__` works if the data is a list of timezone-*naive* `datetime64`s but indexing fails if the `datetime64`s are timezone-*aware*.\r\n\r\n```python\r\nIn [47]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.2.0-23-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 1.5.6\r\nsetuptools: 15.2\r\nCython: 0.23.1\r\nnumpy: 1.10.1\r\nscipy: 0.16.1\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: 0.9.2\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.5.3 (dt dec pq3 ext)\r\nJinja2: None\r\n```'"
12080,127241642,boombard,jreback,2016-01-18 14:48:39,2016-01-20 14:24:23,2016-01-20 14:10:00,closed,,0.18.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12080,b'BUG: GH12071 .reset_index() should create a RangeIndex',b'closes #12071 '
12072,127111442,richjoyce,jreback,2016-01-17 17:45:03,2016-02-02 15:16:18,2016-02-02 15:16:18,closed,,0.18.0,1,Bug;Difficulty Intermediate;Effort Low;Resample,https://api.github.com/repos/pydata/pandas/issues/12072,b'Resampling fails with equal frequency',"b""I've got a DataFrame with a TimedeltaIndex that fails when I try to resample it to the same frequency.\r\n\r\nMinimal working example\r\n```python\r\nimport pandas as pd\r\nindex = pd.timedelta_range('0', periods=9, freq='10L')\r\nseries = pd.Series(range(9), index=index)\r\nseries.resample('10L') # throws ValueError\r\n```\r\n\r\npandas versions\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 1.4.1\r\nsetuptools: 1.1.6\r\nCython: None\r\nnumpy: 1.8.0rc1\r\nscipy: 0.13.0b1\r\nstatsmodels: None\r\nIPython: 2.0.0\r\nsphinx: 1.2.2\r\npatsy: None\r\ndateutil: 1.5\r\npytz: 2013.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```\r\n\r\nThe error thrown is:\r\n```\r\npandas/tseries/resample.pyc in _resample_timestamps(self, kind)\r\n    306                     axlabels.inferred_freq) == self.freq:\r\n    307                     result = obj.copy()\r\n--> 308                     result.index = res_index\r\n    309                 else:\r\n    310                     result = obj.reindex(res_index, method=self.fill_method,\r\n... (more stacks skipped) ...\r\nValueError: Length mismatch: Expected axis has 9 elements, new values have 8 elements\r\n```\r\n\r\nThis appears to be a problem with the logic in `TimeGrouper._resample_timestamps()` lines 298 to 308, the error is being thrown on line 308. It looks like res_index is being set to the wrong thing but I'm not familiar enough with the code to know how to fix properly.\r\n"""
12071,127109217,jreback,jreback,2016-01-17 17:15:05,2016-01-20 14:10:11,2016-01-20 14:10:11,closed,,0.18.0,2,Bug;Difficulty Intermediate;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/12071,b'BUG: .reset_index() should create a RangeIndex',"b""```\r\nIn [10]: df = DataFrame({'A' : range(5)})\r\n\r\nIn [11]: df\r\nOut[11]: \r\n   A\r\n0  0\r\n1  1\r\n2  2\r\n3  3\r\n4  4\r\n\r\nIn [12]: df.index\r\nOut[12]: RangeIndex(start=0, stop=5, step=1)\r\n\r\nIn [13]: df.reset_index()\r\nOut[13]: \r\n   index  A\r\n0      0  0\r\n1      1  1\r\n2      2  2\r\n3      3  3\r\n4      4  4\r\n\r\nIn [14]: df.reset_index().index\r\nOut[14]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\r\n```"""
12063,127046896,nbonnotte,jreback,2016-01-16 19:18:40,2016-01-17 09:04:52,2016-01-17 03:43:47,closed,,0.18.0,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/12063,b'BUG in .groupby for single-row DF',"b'No KeyError was raised when grouping by a non-existant column\r\n\r\nFixes #11741\r\n\r\nXref issue #11640, PR #11717'"
12060,126994706,chris-b1,jreback,2016-01-16 02:24:42,2016-01-26 14:33:47,2016-01-26 14:32:58,closed,,0.18.0,17,Bug;Performance;Timeseries,https://api.github.com/repos/pydata/pandas/issues/12060,b'PERF: more flexible iso8601 parsing',"b'closes #9714\r\ncloses #11899\r\ncloses #11871\r\n\r\nmakes ISO parser in C handle the following. I think 2 & 3 aren\'t actually iso8601 anymore, but close and unambiguous.\r\n\r\n 1. dates without \'-\' separator \r\n 2. dates without space before tz\r\n 3. dates without leading 0s in month/day\r\n    (this ONLY works if the date has separators, eg. `""2015-1-1""` parses, but `""201511""` doesn\'t because it\'s ambiguous)\r\n\r\nasv results - adds a small amount of overhead to the standard case (`\'2014-01-01\'`)\r\n```\r\n    before     after       ratio\r\n  [1ae6384 ] [e922b05 ]\r\n     5.17ms     5.34ms      1.03  timeseries.timeseries_to_datetime_iso8601.time_timeseries_to_datetime_iso8601\r\n     5.15ms     5.50ms      1.07  timeseries.timeseries_to_datetime_iso8601.time_timeseries_to_datetime_iso8601_format\r\n-  111.89ms     5.27ms      0.05  timeseries.timeseries_to_datetime_iso8601.time_timeseries_to_datetime_iso8601_format_no_sep\r\n-     2.02s     5.33ms      0.00  timeseries.timeseries_to_datetime_iso8601.time_timeseries_to_datetime_iso8601_nosep\r\n-     2.97s   218.25ms      0.07  timeseries.timeseries_to_datetime_iso8601.time_timeseries_to_datetime_iso8601_tz_spaceformat\r\n\r\n```'"
12054,126929478,coroa,jreback,2016-01-15 18:28:11,2016-01-19 12:35:00,2016-01-19 12:35:00,closed,,0.18.0,6,Bug;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/12054,b'BUG: GH12050 Setting values on Series using .loc with a TZ-aware DatetimeIndex fails',b'Fixes https://github.com/pydata/pandas/issues/12050 .\r\n\r\nTaking just the values of an index object looses the timezone information.'
12050,126897887,coroa,jreback,2016-01-15 15:44:14,2016-01-19 12:35:13,2016-01-19 12:35:13,closed,,0.18.0,2,Bug;Difficulty Intermediate;Effort Low;Indexing;Regression;Timezones,https://api.github.com/repos/pydata/pandas/issues/12050,b'Setting values on Series using .loc with a TZ-aware DatetimeIndex fails',"b'```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: index = pd.date_range(\'2011-01-01\', \'2011-01-02\', tz=\'utc\')   # without tz=\'utc\' it works\r\n\r\nIn [3]: index\r\nOut[3]: DatetimeIndex([\'2011-01-01\', \'2011-01-02\'], dtype=\'datetime64[ns, UTC]\', freq=\'D\')\r\n\r\nIn [4]: s = pd.Series(range(2), index=index)\r\n\r\nIn [5]: s.loc[index] = 5\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-992315c98f0d> in <module>()\r\n----> 1 s.loc[index] = 5\r\n\r\n/usr/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n    114 \r\n    115     def __setitem__(self, key, value):\r\n--> 116         indexer = self._get_setitem_indexer(key)\r\n    117         self._setitem_with_indexer(indexer, value)\r\n    118 \r\n\r\n/usr/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _get_setitem_indexer(self, key)\r\n    109 \r\n    110         try:\r\n--> 111             return self._convert_to_indexer(key, is_setter=True)\r\n    112         except TypeError:\r\n    113             raise IndexingError(key)\r\n\r\n/usr/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _convert_to_indexer(self, obj, axis, is_setter)\r\n   1148                 mask = check == -1\r\n   1149                 if mask.any():\r\n-> 1150                     raise KeyError(\'%s not in index\' % objarr[mask])\r\n   1151 \r\n   1152                 return _values_from_object(indexer)\r\n\r\nKeyError: ""[\'2011-01-01T01:00:00.000000000+0100\' \'2011-01-02T01:00:00.000000000+0100\'] not in index""\r\n```\r\n\r\nOn python 2.7.11, pandas 0.17.1, linux.'"
12046,126877887,kleingeist,jreback,2016-01-15 14:01:16,2016-05-07 19:06:57,2016-05-07 19:06:57,closed,,,9,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/12046,b'Fix asymmetric error bars for series (closes #9536)',b'closes #9536\r\n \r\nThis fix is for handling asymmetric error bars for series.\r\nIt adapts to the syntax for\r\nhttp://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.errorbar\r\nwhere a sequence of shape 2xN is expected in case of asymmetric error bars.\r\n\r\nIf a single series is to be plotted and the error sequence is of shape 2xN\r\nit will be used as asymmetric error bars.\r\nPreviously a 2xN error sequence was assumed to be 2 symmetric error sequences\r\nfor 2 series. Thus in the end only the first error sequence was used.'
12045,126862093,lvphj,jreback,2016-01-15 12:17:04,2016-03-23 17:56:45,2016-03-23 17:56:45,closed,,0.18.1,3,Bug;Difficulty Novice;Effort Low;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/12045,b'Printing None and NaN values in Pandas dataframe produces confusing results',"b""Printing a dataframe where a variable contains None values produces confusing results. Large dataframes are automatically split to print to screen. If all the values on one side of the splits are None, they are actually displayed as NaN. This can be demonstrated with the following code.\r\n```\r\nimport pandas as pd\r\ntempDF = pd.DataFrame({'id':    np.arange(1,11),\r\n                       'text':  ['some words'] + [None]*9})\r\nprint('Full database\\n-------------\\n',tempDF,'\\n')\r\nwith pd.option_context('display.max_rows', 8, 'display.max_columns', 3):\r\n    print('Split database\\n--------------\\n',tempDF,'\\n')\r\n```\r\nThis produces the following output:\r\n```\r\nFull database\r\n-------------\r\n\r\n    id        text\r\n0   1  some words\r\n1   2        None\r\n2   3        None\r\n3   4        None\r\n4   5        None\r\n5   6        None\r\n6   7        None\r\n7   8        None\r\n8   9        None\r\n9  10        None \r\n\r\nSplit database\r\n--------------\r\n     id        text\r\n0    1  some words\r\n1    2        None\r\n2    3        None\r\n3    4        None\r\n..  ..         ...\r\n6    7         NaN\r\n7    8         NaN\r\n8    9         NaN\r\n9   10         NaN\r\n\r\n[10 rows x 2 columns] \r\n```\r\nAbove the split, the variable 'text' has one cell which has a genuine string ('some words'). All the None values on that side of the split are correctly displayed as 'None'. However, on the bottom part of the split, all the cells contain None values but are confusingly displayed as Nan.\r\n\r\nExpected behaviour: All None values should be displayed as 'None' rather than 'NaN'.\r\n\r\n```\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.2.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 1.5.6\r\nsetuptools: 3.6\r\nCython: None\r\nnumpy: 1.10.2\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.3.2\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: 2.8\r\n```"""
12037,126627505,nothinkelse,jreback,2016-01-14 10:40:40,2016-02-10 17:37:09,2016-02-10 17:37:09,closed,,Next Major Release,5,Bug;Difficulty Intermediate;Effort Medium;Resample;Timeseries,https://api.github.com/repos/pydata/pandas/issues/12037,b'ValueError: Values falls after last bin   when Resampling using pd.tseries.offsets.Nano as period',"b'I have a timeseries in dataframe named dfi with non-eqispaced times as index\r\n``` \r\nprint dfi.value\r\nprint\r\nprint ""len ""+ str(len(dfi))\r\n``` \r\nOutput:\r\n``` \r\ndatetime\r\n2015-10-01 13:58:10.427   -10.072100\r\n2015-10-01 13:58:11.419   -10.072100\r\n2015-10-01 13:58:12.417   -10.072100\r\n2015-10-01 13:58:13.420   -10.072100\r\n2015-10-01 13:58:14.426   -10.072100\r\n2015-10-01 13:58:15.427   -10.072100\r\n2015-10-01 13:58:16.418   -10.072100\r\n2015-10-01 13:58:17.418    -9.753230\r\n2015-10-01 13:58:18.416    -9.753230\r\n2015-10-01 13:58:19.428    -9.753230\r\n2015-10-01 13:58:20.427    -9.753230\r\n2015-10-01 13:58:21.419    -9.753230\r\n2015-10-01 13:58:22.416    -9.753230\r\n2015-10-01 13:58:23.429    -9.753230\r\n2015-10-01 13:58:24.416    -9.753230\r\n2015-10-01 13:58:25.428    -9.753230\r\n2015-10-01 13:58:26.418    -9.753230\r\n2015-10-01 13:58:27.416    -9.396140\r\n2015-10-01 13:58:28.416    -9.396140\r\n2015-10-01 13:58:29.429    -9.396140\r\n2015-10-01 13:58:32.416    -9.396140\r\n2015-10-01 13:58:33.427    -9.396140\r\n2015-10-01 13:58:34.428    -9.396140\r\n2015-10-01 13:58:35.462    -9.396140\r\n2015-10-01 13:58:36.416    -9.396140\r\n2015-10-01 13:58:37.427    -9.010000\r\n2015-10-01 13:58:38.428    -9.010000\r\n2015-10-01 13:58:39.435    -9.010000\r\n2015-10-01 13:58:40.437    -9.010000\r\n2015-10-01 13:58:41.416    -9.010000\r\n                             ...    \r\n2015-10-03 23:59:28.052    -0.759718\r\n2015-10-03 23:59:29.040    -0.759718\r\n2015-10-03 23:59:30.048    -0.759718\r\n2015-10-03 23:59:31.048    -0.759718\r\n2015-10-03 23:59:32.060    -0.759718\r\n2015-10-03 23:59:33.049    -0.759718\r\n2015-10-03 23:59:34.051    -0.759718\r\n2015-10-03 23:59:35.041    -0.759718\r\n2015-10-03 23:59:36.061    -0.759718\r\n2015-10-03 23:59:37.059    -1.010490\r\n2015-10-03 23:59:38.040    -1.010490\r\n2015-10-03 23:59:39.051    -1.010490\r\n2015-10-03 23:59:40.040    -1.010490\r\n2015-10-03 23:59:41.072    -1.010490\r\n2015-10-03 23:59:42.049    -1.010490\r\n2015-10-03 23:59:43.038    -1.010490\r\n2015-10-03 23:59:44.040    -1.010490\r\n2015-10-03 23:59:45.040    -1.010490\r\n2015-10-03 23:59:48.049    -1.133730\r\n2015-10-03 23:59:49.049    -1.133730\r\n2015-10-03 23:59:50.048    -1.133730\r\n2015-10-03 23:59:52.050    -1.133730\r\n2015-10-03 23:59:53.050    -1.133730\r\n2015-10-03 23:59:54.059    -1.133730\r\n2015-10-03 23:59:55.049    -1.133730\r\n2015-10-03 23:59:56.041    -1.133730\r\n2015-10-03 23:59:59.039    -1.296430\r\n2015-10-04 00:00:00.050    -1.296430\r\n2015-10-04 00:00:01.060    -1.296430\r\n2015-10-04 00:00:02.040    -1.296430\r\nName: value, dtype: float64\r\n``` \r\nI get an error when running this code:\r\n```    \r\nprint period_seconds\r\nperiod_nanos=int(period_seconds*(10**9))\r\nprint period_nanos\r\nres= dfi.value.resample(pd.tseries.offsets.Nano(period_nanos), how=[np.min, np.max,\'mean\'])\r\n```\r\n\r\nOutput + error:\r\n```   \r\n4.035752\r\n4035751999\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-14-92e377227823> in <module>()\r\n      5     period_nanos=int(period_seconds*(10**9))\r\n      6     print period_nanos\r\n----> 7     res= dfi.value.resample(pd.tseries.offsets.Nano(period_nanos), how=[np.min, np.max,\'mean\'])\r\n      8 \r\n      9     nullrows=pd.isnull(res).any(1).nonzero()[0]\r\n\r\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base)\r\n   3641                               fill_method=fill_method, convention=convention,\r\n   3642                               limit=limit, base=base)\r\n-> 3643         return sampler.resample(self).__finalize__(self)\r\n   3644 \r\n   3645     def first(self, offset):\r\n\r\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in resample(self, obj)\r\n     80 \r\n     81         if isinstance(ax, DatetimeIndex):\r\n---> 82             rs = self._resample_timestamps()\r\n     83         elif isinstance(ax, PeriodIndex):\r\n     84             offset = to_offset(self.freq)\r\n\r\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in _resample_timestamps(self, kind)\r\n    274         axlabels = self.ax\r\n    275 \r\n--> 276         self._get_binner_for_resample(kind=kind)\r\n    277         grouper = self.grouper\r\n    278         binner = self.binner\r\n\r\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in _get_binner_for_resample(self, kind)\r\n    118             kind = self.kind\r\n    119         if kind is None or kind == \'timestamp\':\r\n--> 120             self.binner, bins, binlabels = self._get_time_bins(ax)\r\n    121         elif kind == \'timedelta\':\r\n    122             self.binner, bins, binlabels = self._get_time_delta_bins(ax)\r\n\r\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in _get_time_bins(self, ax)\r\n    179 \r\n    180         # general version, knowing nothing about relative frequencies\r\n--> 181         bins = lib.generate_bins_dt64(ax_values, bin_edges, self.closed, hasnans=ax.hasnans)\r\n    182 \r\n    183         if self.closed == \'right\':\r\n\r\npandas\\lib.pyx in pandas.lib.generate_bins_dt64 (pandas\\lib.c:20875)()\r\n\r\nValueError: Values falls after last bin\r\n```   \r\n\r\npackages versions:\r\n```   \r\nimport pip\r\ninstalled_packages = pip.get_installed_distributions()\r\ninstalled_packages_list = sorted([""%s==%s"" % (i.key, i.version)\r\n     for i in installed_packages])\r\nfor i in installed_packages_list:\r\n    print i\r\n```   \r\n\r\nOutput:\r\n```   \r\nalabaster==0.7.6\r\nanaconda-client==1.2.1\r\nargcomplete==1.0.0\r\nastropy==1.1.1\r\nbabel==2.1.1\r\nbackports-abc==0.4\r\nbackports.ssl-match-hostname==3.4.0.2\r\nbeautifulsoup4==4.4.1\r\nbitarray==0.8.1\r\nblaze==0.9.0\r\nbokeh==0.11.0\r\nboto==2.38.0\r\nbottleneck==1.0.0\r\ncdecimal==2.3\r\ncffi==1.2.1\r\nclyent==1.2.0\r\ncolorama==0.3.3\r\ncomtypes==1.1.2\r\nconda-build==1.18.2\r\nconda-env==2.4.5\r\nconda==3.19.0\r\nconfigobj==5.0.6\r\ncryptography==0.9.1\r\ncycler==0.9.0\r\ncython==0.23.4\r\ncytoolz==0.7.4\r\ndatashape==0.5.0\r\ndecorator==4.0.6\r\ndocutils==0.12\r\nenum34==1.1.2\r\net-xmlfile==1.0.1\r\nfastcache==1.0.2\r\nflask==0.10.1\r\nfuncsigs==0.4\r\nfutures==3.0.3\r\ngevent-websocket==0.9.3\r\ngevent==1.0.1\r\ngreenlet==0.4.9\r\ngrin==1.2.1\r\nh5py==2.5.0\r\nidna==2.0\r\nipaddress==1.0.14\r\nipykernel==4.1.1\r\nipython-genutils==0.1.0\r\nipython==4.0.1\r\nipywidgets==4.1.0\r\nitsdangerous==0.24\r\njdcal==1.2\r\njedi==0.9.0\r\njinja2==2.8\r\njsonschema==2.4.0\r\njupyter-client==4.1.1\r\njupyter-console==4.0.3\r\njupyter-core==4.0.6\r\njupyter==1.0.0\r\nllvmlite==0.8.0\r\nlxml==3.5.0\r\nmarkupsafe==0.23\r\nmatplotlib==1.5.1\r\nmenuinst==1.3.2\r\nmistune==0.7.1\r\nmultipledispatch==0.4.8\r\nnbconvert==4.1.0\r\nnbformat==4.0.1\r\nnetworkx==1.10\r\nnltk==3.1\r\nnose==1.3.7\r\nnotebook==4.1.0\r\nnumba==0.22.1\r\nnumexpr==2.4.6\r\nnumpy==1.10.1\r\nodo==0.4.0\r\nopenpyxl==2.3.2\r\npandas==0.17.1\r\npath.py==0.0.0\r\npatsy==0.4.0\r\npep8==1.6.2\r\npickleshare==0.5\r\npillow==3.0.0\r\npip==7.1.2\r\nply==3.8\r\npsutil==3.2.2\r\npy==1.4.30\r\npyasn1==0.1.9\r\npycosat==0.6.1\r\npycparser==2.14\r\npycrypto==2.6.1\r\npycurl==7.19.5.3\r\npyflakes==1.0.0\r\npygments==2.0.2\r\npyopenssl==0.15.1\r\npyparsing==2.0.3\r\npyreadline==2.1\r\npytest==2.8.1\r\npython-dateutil==2.4.2\r\npytz==2015.7\r\npywin32==219\r\npyyaml==3.11\r\npyzmq==15.2.0\r\nqtconsole==4.1.1\r\nrequests==2.9.0\r\nrope==0.9.4\r\nscikit-image==0.11.3\r\nscikit-learn==0.17\r\nscipy==0.16.0\r\nsetuptools==19.1.1\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.10.0\r\nsnowballstemmer==1.2.0\r\nsockjs-tornado==1.0.1\r\nsphinx-rtd-theme==0.1.7\r\nsphinx==1.3.1\r\nspyder==2.3.8\r\nsqlalchemy==1.0.11\r\nstatsmodels==0.6.1\r\nsympy==0.7.6.1\r\ntables==3.2.2\r\ntoolz==0.7.4\r\ntornado==4.3\r\ntraitlets==4.0.0\r\nujson==1.33\r\nunicodecsv==0.14.1\r\nwerkzeug==0.11.3\r\nwheel==0.26.0\r\nxlrd==0.9.4\r\nxlsxwriter==0.7.7\r\nxlwings==0.6.1\r\nxlwt==1.0.0\r\n```   '"
12014,125857583,kshedden,jreback,2016-01-11 00:58:10,2016-01-13 13:42:08,2016-01-13 13:42:08,closed,,0.18.0,1,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/12014,b'BUG: Stata value labels',"b'When reading a Stata file incrementally, the value labels are read even when specifying convert_categoricals=False (this does not happen when reading the entire file at once). '"
12013,125853110,kawochen,jreback,2016-01-10 23:25:59,2016-01-15 13:47:24,2016-01-15 13:46:42,closed,,0.18.0,3,Bug;Msgpack,https://api.github.com/repos/pydata/pandas/issues/12013,b'BUG: GH11880 where __contains__ fails in unpacked DataFrame with object cols',b'closes #11880 '
12012,125836524,nbonnotte,jreback,2016-01-10 18:28:57,2016-01-14 12:42:14,2016-01-14 12:36:22,closed,,0.18.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/12012,b'BUG giving modulo operator back to Int64Index',b'Fix issue #9244'
12008,125774767,kyleabeauchamp,jreback,2016-01-09 19:01:04,2016-04-13 01:16:36,2016-04-13 01:15:51,closed,,0.18.1,7,Bug;Compat;Difficulty Novice;Effort Low,https://api.github.com/repos/pydata/pandas/issues/12008,b'Clarify documentation on numexpr in eval / query',"b""So the `query` function calls `eval` under the hood, which ends up meaning that the function call ends failing with the following when numexpr is not present.  This means that the default engine (numexpr) will lead to error on systems without having extra dependencies present.\r\n\r\n```\r\nImportError: 'numexpr' not found. Cannot use engine='numexpr' \r\n```\r\n\r\nI see four solutions, which I list in increasing order of difficulty:\r\n\r\n1.  Make the `numexpr` dependency much clearer in the docstrings.  Explicitly say that an ImportError will occur if you use the default arguments without having numexpr installed.\r\n2.  Make the python engine default\r\n3.  Let the numexpr engine fallback on the python engine in the case of import error\r\n4.  Make numexpr a required dependency\r\n\r\n*edit* It appears that option (3) is preferred here.  """
11996,125589425,kdebrab,jreback,2016-01-08 11:06:50,2016-01-08 13:44:18,2016-01-08 13:44:18,closed,,,2,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/11996,b'Dataframe diff unexpected result',"b""Pandas version 0.17.1:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n   ...: data = pd.DataFrame({'d':[1,2,3],'e':[1.5,2,3],'f':[2,2,3]},index=list('abc'))\r\n   ...: data.diff(axis=1)\r\nOut[1]: \r\n    d   e  f\r\na NaN NaN  1\r\nb NaN NaN  0\r\nc NaN NaN  0\r\n```\r\nHint: the following gives expected result\r\n```python\r\nIn [2]: data.astype(float).diff(axis=1)\r\nOut[2]: \r\n    d    e    f\r\na NaN  0.5  0.5\r\nb NaN  0.0  0.0\r\nc NaN  0.0  0.0\r\n```"""
11995,125513340,shoyer,jreback,2016-01-08 00:08:08,2016-01-08 18:20:15,2016-01-08 18:20:15,closed,,0.18.0,1,Bug;Difficulty Intermediate;Effort Low;Timedelta;Unicode,https://api.github.com/repos/pydata/pandas/issues/11995,b'BUG: pd.Timedelta cannot handle unicode on Python 2',"b""This is on master:\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.17.1+179.g44e4c96'\r\n\r\nIn [3]: pd.Timedelta('1H')\r\nOut[3]: Timedelta('0 days 01:00:00')\r\n\r\nIn [4]: pd.Timedelta(u'1H')\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-9b0d9f7ba675> in <module>()\r\n----> 1 pd.Timedelta(u'1H')\r\n\r\n/Users/shoyer/dev/pandas/pandas/tslib.pyx in pandas.tslib.Timedelta.__new__ (pandas/tslib.c:39786)()\r\n   2329             value = value.value\r\n   2330         elif util.is_string_object(value):\r\n-> 2331             value = np.timedelta64(parse_timedelta_string(value, False))\r\n   2332         elif isinstance(value, timedelta):\r\n   2333             value = convert_to_timedelta64(value,'ns',False)\r\n\r\n/Users/shoyer/dev/pandas/pandas/tslib.pyx in pandas.tslib.parse_timedelta_string (pandas/tslib.c:46542)()\r\n   2784         return NPY_NAT\r\n   2785\r\n-> 2786     for c in ts:\r\n   2787\r\n   2788         # skip whitespace / commas\r\n\r\nTypeError: Expected str, got unicode```"""
11992,125458159,rcarneva,jreback,2016-01-07 18:35:32,2016-01-08 14:04:55,2016-01-08 14:04:55,closed,,0.18.0,10,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/11992,b'BUG: DataFrame.round() drops column index name #11986',b'Fixes #11986. Used the same reconstruction method as `DataFrame.isin()` to ensure that column names are kept through rounding. Works with `np.round` as well.\r\n\r\nPretty minor fix -- does this need a release note or anything else?'
11991,125457773,jseabold,jreback,2016-01-07 18:33:53,2016-01-13 01:01:18,2016-01-12 23:48:10,closed,,0.18.0,11,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11991,"b""BUG: Allow typecode of 'e' to be passed to select_dtypes""",b'Closes #11990.'
11990,125454342,jseabold,jreback,2016-01-07 18:14:05,2016-01-12 23:47:54,2016-01-12 23:47:54,closed,,0.18.0,5,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11990,b'select_dtypes fails on float16 typecode',"b""I'll see if it's an easy fix.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nFLOAT_TYPES = list(np.typecodes['AllFloat'])\r\n\r\ndf = pd.util.testing.makeDataFrame()\r\ndf.select_dtypes(FLOAT_TYPES)\r\n\r\nFLOAT_TYPES.remove('e')\r\ndf.select_dtypes(FLOAT_TYPES)\r\n```"""
11986,125432159,iyer,jreback,2016-01-07 16:44:19,2016-01-08 14:04:28,2016-01-08 14:04:28,closed,,0.18.0,2,Bug;Difficulty Novice;Effort Low;Numeric,https://api.github.com/repos/pydata/pandas/issues/11986,b'BUG: DataFrame.round() drops column index name',"b""- DataFrame.round() drops the index name in the column\r\n- This is also seen when using np.round() on the dataframe. \r\n- Further this is not seen when using other elementwise numpy operations such as np.floor() \r\n- This is only observed in pandas 0.17.* This was working fine in pandas 0.16.*\r\n\r\n```python\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nrows=5\r\ncols=2\r\nindex = pd.Index(range(rows), name='MYINDEX')\r\ncolumns = pd.Index(range(cols), name='MYCOLUMNS')\r\ndf = pd.DataFrame(np.random.rand(rows,cols), index=index, columns=columns)\r\n\r\ndf\r\nOut[2]: \r\nMYCOLUMNS         0         1\r\nMYINDEX                      \r\n0          0.163862  0.828853\r\n1          0.560261  0.980514\r\n2          0.736242  0.055051\r\n3          0.825197  0.783094\r\n4          0.153369  0.069713\r\n\r\ndf.round(2)\r\nOut[3]: \r\n            0     1\r\nMYINDEX            \r\n0        0.16  0.83\r\n1        0.56  0.98\r\n2        0.74  0.06\r\n3        0.83  0.78\r\n4        0.15  0.07\r\n\r\nnp.round(df, 2)\r\nOut[4]: \r\n            0     1\r\nMYINDEX            \r\n0        0.16  0.83\r\n1        0.56  0.98\r\n2        0.74  0.06\r\n3        0.83  0.78\r\n4        0.15  0.07\r\n\r\nnp.floor(df)\r\nOut[5]: \r\nMYCOLUMNS  0  1\r\nMYINDEX        \r\n0          0  0\r\n1          0  0\r\n2          0  0\r\n3          0  0\r\n4          0  0\r\n\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.11.2.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 19.1.1\r\nCython: 0.23.4\r\nnumpy: 1.10.2\r\nscipy: 0.16.1\r\nstatsmodels: None\r\nIPython: 4.0.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.10\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n\r\n```"""
11981,125362395,ChristopherShort,jreback,2016-01-07 10:04:04,2016-05-11 22:18:27,2016-05-11 22:18:27,closed,,0.19.0,7,Bug;Difficulty Novice;Effort Low;Missing-data;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11981,b'interation of set_eng_float_format and pivot_tables',"b""There appears to be an interaction between the option for float display length and pivot_table methods on integers. This issue arose when working on a dataset with both floats and integers - but this arose on operations on integers part of the dataset.\r\n\r\nThe following example throws an error from the `format.py` module.\r\n\r\nSwap the comments on the formatting options  (undoing the setting for floats) and it runs fine.\r\n\r\nIn my dataset - there were no missing values following the pivot table operation and the error still occurs. In the sample code below, NaNs may be causing the formatting issue, but that didn't appear to be the case for me.\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n#pd.set_option('display.float_format', None)\r\npd.set_eng_float_format(accuracy=1)\r\n\r\ndf = pd.DataFrame(data = np.random.randint(25, size=(10, 3)), columns = list('abc'))\r\ndf.pivot_table(values='a', index='b', columns='c')\r\n```\r\n\r\ncheers\r\nChris"""
11964,124978138,Tux1,jreback,2016-01-05 14:33:23,2016-01-12 20:25:09,2016-01-12 20:25:09,closed,,0.18.0,5,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11964,b'BUG: Timestamp rounding wrong implementation fixed #11963',b'closes #11963 '
11963,124957072,Tux1,jreback,2016-01-05 12:20:57,2016-01-12 20:24:46,2016-01-12 20:24:46,closed,,0.18.0,4,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11963,b'Timestamp rounding wrong implementation',"b'Hi,\r\nI was looking at the recent ENH #4314 and there is a wrong implementation of `round` method for datetime.\r\n```cython\r\n    def round(self, freq):\r\n            """"""\r\n            return a new Timestamp rounded to this resolution\r\n            Parameters\r\n            ----------\r\n            freq : a freq string indicating the rouding resolution\r\n            """"""\r\n            cdef int64_t unit\r\n            cdef object result, value\r\n    \r\n            from pandas.tseries.frequencies import to_offset\r\n            unit = to_offset(freq).nanos\r\n            if self.tz is not None:\r\n                value = self.tz_localize(None).value\r\n            else:\r\n                value = self.value\r\n            result = Timestamp(unit*np.floor(value/unit),unit=\'ns\')\r\n            if self.tz is not None:\r\n                result = result.tz_localize(self.tz)\r\n            return result\r\n```\r\nYou are flooring the nano timestamp instead of rounding. You should replace `np.floor` by `np.round`.\r\nAlso I propose to add `floor` and `ceil` method to Timestamp in order to have the same behavior that `float`. (It is very usefull to get the upper/lower bound for a specific freq, likely more usefull than round)\r\n\r\nDo you agree @jreback ? '"
11957,124860068,kawochen,jreback,2016-01-04 23:09:14,2016-01-05 00:39:06,2016-01-05 00:39:03,closed,,0.18.0,1,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/11957,b'BUG: GH11835 where comparison of Timedelta array caused infinite loop',b'closes #11835'
11934,124391988,yueatwork,jreback,2015-12-30 19:46:48,2016-02-20 18:27:34,2016-02-20 18:27:34,closed,,0.18.0,1,Bug;Difficulty Novice;Effort Low;Timezones,https://api.github.com/repos/pydata/pandas/issues/11934,b'Bug: UTC in to_datetime() not working in 0.17.1',"b""In the case below, setting `utc=True` in pandas.to_datetime actually removes the UTC attribute from the input:\r\n\r\n    import pandas as pd\r\n    import pytz\r\n    start = pd.Timestamp('2014-01-01', tz=pytz.UTC)\r\n    end = pd.Timestamp('2014-01-03', tz=pytz.UTC)\r\n    date_range = pd.bdate_range(start, end)\r\n    df = pd.DataFrame({'x':1}, index=date_range)\r\n    df.index\r\n    # DatetimeIndex([...], dtype='datetime64[ns, UTC]', freq='B')\r\n    pd.to_datetime(df.index, utc=True)\r\n    # DatetimeIndex([...], dtype='datetime64[ns]', freq='B')\r\n\r\nthe last function removes the UTC attribute from the input, even though `utc` is set to `True`.\r\n\r\nThis only happens in pandas 0.17.1.  pandas 0.16.x have a different implementation and does not have this issue."""
11925,124278392,jorisvandenbossche,jreback,2015-12-29 22:39:08,2015-12-31 13:46:31,2015-12-31 13:46:31,closed,,0.18.0,0,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11925,b'BUG: subtraction of datetime / timedelta should be allowed',"b""```\r\nIn [12]: s2\r\nOut[12]:\r\n0   1 days\r\n1   2 days\r\n2   3 days\r\ndtype: timedelta64[ns]\r\n\r\nIn [9]: pd.Timestamp('2012-01-01') + s2\r\nOut[9]:\r\n0   2012-01-02\r\n1   2012-01-03\r\n2   2012-01-04\r\ndtype: datetime64[ns]\r\n\r\nIn [10]: pd.Timestamp('2012-01-01') - s2\r\nTypeError: can only operate on a timedelta/DateOffset and a datetime for addition, but the operator [__rsub__] was passed\r\n\r\nIn [11]: pd.Timestamp('2012-01-01') + (-s2)\r\nOut[11]:\r\n0   2011-12-31\r\n1   2011-12-30\r\n2   2011-12-29\r\ndtype: datetime64[ns]\r\n```"""
11923,124252754,kawochen,jreback,2015-12-29 18:51:08,2015-12-29 21:12:59,2015-12-29 21:12:56,closed,,0.18.0,3,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/11923,b'API: GH11885 DataFrame.round() now returns non-numeric columns unchanged',b'closes #11885 '
11895,123796320,IamGianluca,jreback,2015-12-24 12:00:53,2015-12-26 00:29:40,2015-12-26 00:29:36,closed,,0.18.0,2,Bug;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11895,b'BUG: masking empty DataFrame',"b""closes #11859 \r\n\r\nA `ValueError` was raised when trying to mask an empty DataFrame. In addition to solve the bug I've also added a simple new unit test in `test_frame.py`. I didn't add a line in `whatsnew` because this was already done in #10126 """
11894,123743400,evanpw,jreback,2015-12-24 01:23:09,2016-01-07 15:37:11,2016-01-07 15:15:02,closed,,0.18.0,4,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11894,b'BUG: Spurious matches in DataFrame.duplicated when keep=False',b'Fixes GH #11864'
11891,123723910,jorisvandenbossche,jorisvandenbossche,2015-12-23 21:18:33,2015-12-28 10:08:29,2015-12-28 10:08:29,closed,,0.18.0,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/11891,b'Fix passing args in groupby plot (GH11805)',b'Closes https://github.com/pydata/pandas/issues/11805\r\n\r\nSo probably all groupby plot calls that use kwargs are broken in 0.17.x. \r\n@shoyer it was only a very small change actually!\r\n\r\nStill need to add tests.\r\n\r\n'
11882,123408845,sxwang,sxwang,2015-12-22 04:08:07,2016-05-03 05:08:11,2016-05-03 05:08:11,closed,,,24,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/11882,b'BUG: dataframe loading with duplicated columns and usecols #11823',b'Fixes #11823 .\r\n\r\nchanged ``usecols`` from set to list in the C parser to handle duplicated columns and ``usecols``. Update python parser to be consistent with C parser.'
11880,123347417,ikilledthecat,jreback,2015-12-21 20:00:16,2016-01-15 13:46:42,2016-01-15 13:46:42,closed,,0.18.0,5,Bug;Difficulty Intermediate;Effort Low;Msgpack,https://api.github.com/repos/pydata/pandas/issues/11880,b'Msgpack - ValueError: buffer source array is read-only ',"b'I get the Value error when processing data using pandas. I followed the following steps:\r\n1.  convert to msgpack format with compress flag\r\n2. subsequently read file into a dataframe\r\n3. push to sql table with to_sql\r\n\r\nOn the third step i get ValueError: buffer source array is read-only.\r\n\r\nThis problem does not arise if I wrap the read_msgpack call inside a pandas.concat\r\n\r\nExample\r\n```python\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom sqlalchemy import create_engine\r\n\r\neng = create_engine(""sqlite:///:memory:"")\r\n\r\ndf1 = pd.DataFrame({ \'A\' : 1.,\r\n                                   \'B\' : pd.Timestamp(\'20130102\'),\r\n                                   \'C\' : pd.Series(1,index=list(range(4)),dtype=\'float32\'),\r\n                                   \'D\' : np.array([3] * 4,dtype=\'int32\'),\r\n                                   \'E\' : \'foo\' })\r\n\r\ndf1.to_msgpack(\'test.msgpack\', compress=\'zlib\')\r\ndf2 = pd.read_msgpack(\'test.msgpack\')\r\n\r\ndf2.to_sql(\'test\', eng, if_exists=\'append\', chunksize=1000) # throws value error\r\n\r\ndf2 = pd.cooncat([pd.read_msgpack(\'test.msgpack\')])\r\n\r\ndf2.to_sql(\'test\', eng, if_exists=\'append\', chunksize=1000) # works\r\n```\r\n\r\nThis happens with both blosc and zlib compression. While I have found a solution, this behaviour seems very odd and for very large files there is a small performance hit.\r\n\r\nedit: @TomAugspurger changed the sql engine to sqlite'"
11871,123118514,dpinte,jreback,2015-12-20 00:26:29,2016-01-26 14:32:58,2016-01-26 14:32:58,closed,,0.18.0,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11871,b'BUG: to_datetime issue parsing non-zero padded month in 0.17.1',"b'In pandas 0.16.2, the following date (non-zero padded month) was parsing correctly:\r\n\r\n```\r\n>>> import pandas\r\n>>> pandas.__version__\r\n\'0.16.2\'\r\n>>> pandas.to_datetime(\'2005-1-13\', format=\'%Y-%m-%d\')\r\nTimestamp(\'2005-01-13 00:00:00\')\r\n```\r\n\r\nWith 0.17.1, it raises a ValueError:\r\n\r\n```\r\n>>> import pandas\r\n>>> pandas.__version__\r\nu\'0.17.1\'\r\n>>> pandas.to_datetime(\'2005-1-13\', format=\'%Y-%m-%d\')\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/dpinte/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/util/decorators.py"", line 89, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/Users/dpinte/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/tseries/tools.py"", line 276, in to_datetime\r\n    unit=unit, infer_datetime_format=infer_datetime_format)\r\n  File ""/Users/dpinte/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/tseries/tools.py"", line 397, in _to_datetime\r\n    return _convert_listlike(np.array([ arg ]), box, format)[0]\r\n  File ""/Users/dpinte/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/tseries/tools.py"", line 383, in _convert_listlike\r\n    raise e\r\nValueError: time data \'2005-1-13\' does match format specified\r\n```\r\n\r\nEven if `%m` is supposed to be used for zero-padded month definitions, Python\'s strptime function parses them properly.\r\n\r\nIs this a known issue? '"
11864,123011711,capelastegui,jreback,2015-12-18 19:30:22,2016-01-07 15:15:15,2016-01-07 15:15:15,closed,,0.18.0,3,Bug;Difficulty Intermediate;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11864,b'DataFrame .duplicated() / .drop_duplicates() flagging unique rows as duplicated in 0.17.1',"b""Dataframe.duplicated() is flagging rows as duplicates when they are in fact distinct. This happens when using large dataframes, and duplicated(keep=False):\r\n\r\n    import pandas as pd, numpy as np\r\n    df = pd.DataFrame({'a': pd.Series(range(1,100000)),\r\n                       'b': pd.Series(range(10,1000000)),\r\n                       'c': pd.Series(3*range(2,200000,2))})\r\n    df.head()\r\n    \r\n    np.sum(df.duplicated())\r\n\r\n\r\nOut[]: 0\r\n    \r\n    np.sum(df.duplicated(keep=False))\r\n    \r\n\r\nOut[]:110\r\n\r\nChanging column order results in different (but still incorrect) behavior.\r\n    \r\n    np.sum(df[['c','b','a']].duplicated(keep=False))\r\n    \r\n\r\nOut[]:2138\r\n\r\nTested on 0.17.1. Environment details are provided below:\r\n    >> pd.util.print_versions.show_versions()\r\n    INSTALLED VERSIONS\r\n    ------------------\r\n    commit: None\r\n    python: 2.7.11.final.0\r\n    python-bits: 64\r\n    OS: Darwin\r\n    OS-release: 14.5.0\r\n    machine: x86_64\r\n    processor: i386\r\n    byteorder: little\r\n    LC_ALL: None\r\n    LANG: en_GB.UTF-8\r\n\r\n    pandas: 0.17.1\r\n    nose: None\r\n    pip: 7.1.2\r\n    setuptools: 18.5\r\n    Cython: None\r\n    numpy: 1.10.1\r\n    scipy: 0.16.0\r\n    statsmodels: None\r\n    IPython: 4.0.0\r\n    sphinx: 1.3.3\r\n    patsy: None\r\n    dateutil: 2.4.2\r\n    pytz: 2015.7\r\n    blosc: None\r\n    bottleneck: None\r\n    tables: None\r\n    numexpr: None\r\n    matplotlib: 1.4.3\r\n    openpyxl: None\r\n    xlrd: None\r\n    xlwt: None\r\n    xlsxwriter: None\r\n    lxml: None\r\n    bs4: None\r\n    html5lib: None\r\n    httplib2: None\r\n    apiclient: None\r\n    sqlalchemy: 1.0.9\r\n    pymysql: None\r\n    psycopg2: 2.6.1 (dt dec pq3 ext)\r\n    Jinja2: None\r\n    \r\nThis looks like the same kind of problem described in  #11668, though the specific examples provided in that issue work properly in 0.17.1 \r\n"""
11859,122805338,zhaoguixu,jreback,2015-12-17 19:28:02,2015-12-26 00:29:36,2015-12-26 00:29:36,closed,,0.18.0,0,Bug;Difficulty Novice;Effort Low;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11859,b'BUG: masking empty DataFrame',"b' Issue #10126 seems not fixed completely. I still get the same error when running the same example. I have looked up the code and found that _getitem_frame was not fixed yet.  The fix is easy, just the same as _setitem_frame.'"
11858,122749741,diazona,jreback,2015-12-17 14:51:14,2016-05-13 13:19:16,2016-05-13 13:19:16,closed,,0.19.0,6,Bug;Difficulty Novice;Effort Low;Visualization,https://api.github.com/repos/pydata/pandas/issues/11858,b'Index without 0 in xerr/yerr causes KeyError',"b'When creating an errorbar plot, if the data object being used for the errors (`xerr` or `yerr`) doesn\'t have 0 in its index, it produces a `KeyError` from matplotlib code. I can produce this with the following code sample:\r\n\r\n```python\r\nimport pandas as pd, numpy as np\r\ni = np.array([1,2,3])\r\na = pd.DataFrame(i, index=i)\r\na.plot(yerr=a)\r\n```\r\n\r\nThe underlying bug (if it is a bug) in matplotlib is responsible for several other issues, including #4493 and #6127, but this case is different because it uses only Pandas API methods, rather than passing a Pandas object to a matplotlib method. So it\'s a little harder to justify passing this off on matplotlib to fix, as was done in e.g. #6127.\r\n\r\nIf the ""proper"" fix ever is implemented in matplotlib code, it should solve this as well as #4493 and #6127 and all the others of that nature, but until that point, Pandas can work around it by converting the error object (if it is a `Series` or `DataFrame`) to an `ndarray`. I\'m working on this in 4b04f80c41684ba9ab05ce8c87b17b96fde87290 but I\'m not sure if there\'s a better way to fix it, or if this breaks something. (If so, the tests don\'t indicate it.)'"
11856,122623173,DSLituiev,jreback,2015-12-16 23:00:23,2016-05-17 13:25:14,2016-05-17 13:25:14,closed,,0.19.0,15,Bug;Missing-data;Sparse,https://api.github.com/repos/pydata/pandas/issues/11856,b'fixed conversion to sparse for non-numeric index',"b'this fixes  #11633\r\n\r\nthis fix breaks some sparse tests,\r\nstuck on `test_combine_first` : `ValueError: total size of new array must be unchanged`'"
11854,122529784,JCalderan,jreback,2015-12-16 15:31:32,2016-03-15 14:22:14,2016-03-15 14:22:14,closed,,0.18.1,3,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/11854,"b""Can't get period code with frequency alias 'minute' or 'Minute'""","b'Hi,\r\n\r\nWhile playing with the frequency module, I might have found a bug with the function *_period_str_to_code*: \r\nthe function didn\'t return the code 8000 for the frequency string ""minute"", yet it works for frequency strings as \'T\', \'Min\', \'min\'...\r\n\r\nHere\'s the code to reproduce the \'bug\':\r\n```\r\n>>> from pandas.tseries.frequencies import _period_str_to_code\r\n>>> _period_str_to_code(\'Min\')\r\n8000\r\n>>> _period_str_to_code(\'T\')\r\n8000\r\n>>> _period_str_to_code(\'minute\')\r\nsys:1: FutureWarning: Freq ""MINUTE"" is deprecated, use ""Min"" as alternative.\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""***/site-packages/pandas/tseries/frequencies.py"", line 813, in _period_str_to_code\r\n    return _period_code_map[alias]\r\nKeyError: \'Min\'\r\n```\r\n(in pandas/tseries/frequencies.py line 783 to line 813)\r\nIt appears that \'minute\' is converted into the alias \'Min\' during the function ***_period_str_to_code*** (line 807).\r\n\'Min\' is then used as a key in the dictionnary ***_period_code_map***, but this dictionnary doesn\'t hold any value for this key (line 813).\r\nIndeed, the key \'Min\' is stored in the dictionnary ***_lite_rule_alias*** where it indexes the value \'T\' (the correct frequency string for minutes).\r\n\r\nThis particuliar situation could be solved by transforming the return of the function ***_period_str_to_code*** by using a recursive call which allows to \'safely\' handle the converted frequency string (_period_str_to_code(\'minute\') > \'Min\' > _period_str_to_code(\'Min\') > 8000).\r\nThe conversion from \'Min\' to \'T\' occures line 799, and the conversion from \'T\' to 8000 occures line 804.\r\n\r\nBut I\'m not sure of the implication of such a recursive call (performance impact, possibility of an \'infinite\' recursive call in some situation, and so on).\r\nHere is the function with a recursive call (change in pandas/tseries/frequencies.py line 813):\r\n```\r\ndef _period_str_to_code(freqstr):\r\n    # hack\r\n    if freqstr in _rule_aliases:\r\n        new = _rule_aliases[freqstr]\r\n        warnings.warn(_LEGACY_FREQ_WARNING.format(freqstr, new),\r\n                      FutureWarning, stacklevel=3)\r\n        freqstr = new\r\n    freqstr = _lite_rule_alias.get(freqstr, freqstr)\r\n\r\n    if freqstr not in _dont_uppercase:\r\n        lower = freqstr.lower()\r\n        if lower in _rule_aliases:\r\n            new = _rule_aliases[lower]\r\n            warnings.warn(_LEGACY_FREQ_WARNING.format(lower, new),\r\n                          FutureWarning, stacklevel=3)\r\n            freqstr = new\r\n        freqstr = _lite_rule_alias.get(lower, freqstr)\r\n\r\n    try:\r\n        if freqstr not in _dont_uppercase:\r\n            freqstr = freqstr.upper()\r\n        return _period_code_map[freqstr]\r\n    except KeyError:\r\n        try:\r\n            alias = _period_alias_dict[freqstr]\r\n            warnings.warn(_LEGACY_FREQ_WARNING.format(freqstr, alias),\r\n                          FutureWarning, stacklevel=3)\r\n        except KeyError:\r\n            raise ValueError(""Unknown freqstr: %s"" % freqstr)\r\n\r\n        return _period_str_to_code(alias)\r\n```'"
11838,122082544,ciamac,jreback,2015-12-14 16:48:38,2015-12-16 13:13:06,2015-12-16 13:13:06,closed,,0.18.0,1,Bug;Difficulty Intermediate;Effort Low;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/11838,b'REGR: .clip and datetime w/timezone',"b""There seem to be some incompatibilities with timezones and timestamps in 0.17.1.\r\n\r\nFor example, here I'm trying to clip a timezone-aware column with a timezone-aware timestamp:\r\n```\r\nIn [224]: import pandas\r\n\r\nIn [225]: u = pandas.Timestamp('2015-12-01 09:30:30', tz='US/Eastern')\r\n\r\nIn [226]: print u\r\n2015-12-01 09:30:30-05:00\r\n\r\nIn [227]: df = pandas.DataFrame({ 'foo' : [ pandas.Timestamp('2015-12-01 09:30:00', tz='US/Eastern'), pandas.Timestamp('2015-12-01 09:31:00', tz='US/Eastern') ] })\r\n\r\nIn [228]: print df\r\n                        foo\r\n0 2015-12-01 09:30:00-05:00\r\n1 2015-12-01 09:31:00-05:00\r\n\r\nIn [229]: df['foo'].clip(upper=u)\r\n\r\nTypeError: Could not compare [Timestamp('2015-12-01 09:30:30-0500', tz='US/Eastern')] with block values\r\n```\r\n\r\nThis code used to work on 0.16.2."""
11836,121944000,AlbertDeFusco,jreback,2015-12-13 21:11:01,2016-02-13 13:34:34,2016-02-13 13:34:34,closed,,0.18.0,2,Bug;Difficulty Intermediate;Dtypes;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/11836,"b""int64 Index in 0.17 typcasts '0' string to integer""","b""Here's what I got. Is this expected behavior? I could not find a reference for this functionality in the release notes.\r\n\r\n## 0.16\r\n`conda create -n pd pandas=0.16 python=3.4 ipython`\r\n\r\nIn 0.16 the dtype of the index changes to object when adding a row with '0'.\r\n\r\n```\r\nIn [1]: import pandas as pd\r\nIn [2]: import numpy as np\r\nIn [3]: data = np.random.random(10)                                                                                     \r\nIn [4]: m=pd.Series(data)\r\nIn [5]: m[0]=0.4444\r\nIn [6]: m.index\r\nOut[6]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\r\nIn [7]: m['0']=0.5555\r\nIn [8]: m.index\r\nOut[8]: Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, '0'], dtype='object')\r\n```\r\n\r\n## 0.17.1\r\n`conda create -n pd pandas=0.17 python=3.4 ipython`\r\n\r\nIn 0.17.1 The index dtype does not change, but typecasts to the integer 0. Repeated assignment at the integer 0 index appends more 0s to the index.\r\n\r\n```\r\nIn [1]: import pandas as pd\r\nIn [2]: import numpy as np\r\nIn [3]: data = np.random.random(10)                                                                                     \r\nIn [4]: m=pd.Series(data)\r\nIn [5]: m[0]=0.4444\r\nIn [6]: m.index\r\nOut[6]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\r\nIn [7]: m['0']=0.5555\r\nIn [8]: m.index\r\nOut [8]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0], dtype='int64')\r\nIn [9]: m['0']=0.6666\r\nIn [10]: m.index\r\nOut[10]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0], dtype='int64')\r\nIn [11]: m[0]\r\nOut[11]: \r\n0    0.4444\r\n0    0.5555\r\n0    0.6666\r\ndtype: float64\r\n```"""
11835,121939147,WarrenWeckesser,jreback,2015-12-13 19:46:54,2016-01-05 00:39:03,2016-01-05 00:39:03,closed,,0.18.0,1,Bug;Difficulty Novice;Effort Low;Timedelta,https://api.github.com/repos/pydata/pandas/issues/11835,b'ERR: Maximum recursion depth exceeded in comparision when comparing a TimeDelta to numpy object array of TimeDelta',"b'See http://stackoverflow.com/questions/34251068/runtimeerror-from-scipy-stats-mode-on-array-of-timedelta-maximum-recursion-dept\r\n\r\nPython 3.5.1, pandas 0.17.1, numpy 0.10.1:\r\n```\r\nPython 3.5.1 |Continuum Analytics, Inc.| (default, Dec  7 2015, 11:24:55) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 4.0.1 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: np.__version__\r\nOut[2]: \'1.10.1\'\r\n\r\nIn [3]: import pandas as pd\r\n\r\nIn [4]: pd.__version__\r\nOut[4]: \'0.17.1\'\r\n```\r\n\r\nCreate a numpy array of `TimeDelta` objects, and do a comparison of the array to a `TimeDelta` instance:\r\n\r\n```\r\nIn [5]: from pandas import Timedelta\r\n\r\nIn [6]: periods = [Timedelta(\'0 days 01:00:00\'), Timedelta(\'0 days 01:00:00\')]\r\n\r\nIn [7]: p = np.array(periods)\r\n\r\nIn [8]: periods[0] > p\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-8-1c05a376ecc2> in <module>()\r\n----> 1 periods[0] > p\r\n\r\npandas/tslib.pyx in pandas.tslib._Timedelta.__richcmp__ (pandas/tslib.c:38155)()\r\n\r\n<SNIP>\r\n\r\npandas/tslib.pyx in pandas.tslib._Timedelta.__richcmp__ (pandas/tslib.c:38155)()\r\n\r\npandas/tslib.pyx in pandas.tslib._Timedelta.__richcmp__ (pandas/tslib.c:38155)()\r\n\r\nRecursionError: maximum recursion depth exceeded in comparison\r\n\r\nIn [9]: \r\n```'"
11830,121846659,jesrael,jreback,2015-12-12 10:04:38,2016-05-24 13:30:48,2016-05-24 13:30:45,closed,,0.19.0,9,Bug;Difficulty Intermediate;Effort Medium;Groupby,https://api.github.com/repos/pydata/pandas/issues/11830,b'groupby.nth lost multiindex',"b""It is bug or not? Because in function `mean` and `first` it is OK.\r\n[link](http://stackoverflow.com/questions/34237462/selecting-column-before-nth-breaks-group-indices/)\r\n\r\n\r\n    df = pd.DataFrame({'a': [1, 1, 2, 2], 'b': ['b', 'b', 'b', 'a'], 'c': [1, 2, 3, 4]})\r\n    print df\r\n    #   a  b  c\r\n    #0  1  b  1\r\n    #1  1  b  2\r\n    #2  2  b  3\r\n    #3  2  a  4\r\n    \r\n    #lost multiindex\r\n    print df.groupby(['a', 'b']).c.nth(0)\r\n    #0    1\r\n    #2    3\r\n    #3    4\r\n    #Name: c, dtype: int64\r\n    \r\n    print df.groupby(['a', 'b']).c.mean()\r\n    #a  b\r\n    #1  b    1.5\r\n    #2  a    4.0\r\n    #   b    3.0\r\n    #Name: c, dtype: float64\r\n    print df.groupby(['a', 'b']).c.first()\r\n    #a  b\r\n    #1  b    1\r\n    #2  a    4\r\n    #   b    3\r\n    #Name: c, dtype: int64\r\n    print df.groupby(['a', 'b']).nth(0).c\r\n    #a  b\r\n    #1  b    1\r\n    #2  a    4\r\n    #   b    3\r\n    #Name: c, dtype: int64\r\n    \r\n"""
11819,121639504,sxwang,jreback,2015-12-11 05:13:19,2015-12-15 04:34:56,2015-12-12 13:49:39,closed,,0.18.0,5,Bug;Compat;IO Excel,https://api.github.com/repos/pydata/pandas/issues/11819,b'BUG: read_excel fails when empty sheets exist and sheetname=None #11711',b'Fixes issue #11711.\r\n\r\nExisting code prematurely returns an empty dataframe when an empty sheet in the source excel file is encountered.\r\nFix is to store an empty dataframe in the output dict and continue to the next sheet.'
11806,121314316,thrasibule,jreback,2015-12-09 18:54:09,2015-12-18 19:33:16,2015-12-17 22:43:13,closed,,0.18.0,5,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11806,b'be more careful with half-opened date_range',"b""closes #11804 \r\n\r\nThis changes the definition of 'left' and 'right'. If the generated_dates are strictly within (start, end), then changing the value of closed has no impact. Before the code would always remove the leftmost date or rightmost date independently of the value of start and end."""
11804,121285183,thrasibule,jreback,2015-12-09 16:28:35,2015-12-17 22:43:13,2015-12-17 22:43:13,closed,,0.18.0,0,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11804,"b""DatetimeIndex with closed='left' or 'right' drop dates at the boundaries""","b""When either start  or end is at the boundary of a date_range, they get dropped when they shouldn't if we specify the closed argument, see example below:\r\n\r\n```\r\nIn [1]: pd.date_range('2015-09-12', '2015-10-30', freq='QS-MAR', closed='right')\r\nOut[1]: DatetimeIndex([], dtype='datetime64[ns]', freq='QS-MAR')\r\n\r\nIn [2]: pd.date_range('2015-09-01', '2015-10-30', freq='QS-MAR', closed='left')\r\nOut[2]: DatetimeInddex([], dtype='datetime64[ns]', freq='QS-MAR')\r\n```"""
11800,121191101,hcontrast,jreback,2015-12-09 09:05:56,2016-02-06 18:19:36,2015-12-12 14:05:13,closed,,No action,2,Bug;Duplicate;Timezones,https://api.github.com/repos/pydata/pandas/issues/11800,b'Series.apply loses timezone information',"b""apply seems to lose timezone awareness. Would expect the following to be invariant:\r\n\r\n```python\r\nIn [9]: import pandas as pd\r\n\r\nIn [10]: pd.__version__\r\nOut[10]: u'0.17.1'\r\n\r\nIn [11]: ser = pd.Series([pd.Timestamp('2015/01/01', tz='UTC'), pd.Timestamp('2015/01/02', tz='UTC')])\r\n\r\nIn [12]: ser\r\nOut[12]: \r\n0   2015-01-01 00:00:00+00:00\r\n1   2015-01-02 00:00:00+00:00\r\ndtype: datetime64[ns, UTC]\r\n\r\nIn [13]: ser.apply(lambda x: x)\r\nOut[13]: \r\n0   2015-01-01\r\n1   2015-01-02\r\ndtype: datetime64[ns]\r\n```"""
11796,121021053,jreback,jreback,2015-12-08 14:42:16,2015-12-08 20:14:58,2015-12-08 20:14:58,closed,,0.18.0,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11796,"b'BUG: bug in deep copy of datetime tz-aware objects, #11794'",b'closes #11794 \r\n\r\ncleanups in copy / remove warnings '
11794,120969691,hcontrast,jreback,2015-12-08 09:36:53,2015-12-08 20:14:58,2015-12-08 20:14:58,closed,,0.18.0,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11794,b'bug in deep copy of Series with timezone-aware datetime entries',"b""deep copies of Series with timezone-aware datetimes are shallow in 0.17.1:\r\n```python\r\nIn [12]: import pandas as pd\r\nIn [13]: pd.__version__\r\nOut[13]: u'0.17.1'\r\n\r\nIn [14]: ser = pd.Series([pd.Timestamp('2012/01/01', tz='UTC')])\r\nIn [15]: ser2 = ser.copy(deep=True)\r\nIn [18]: ser2[0] = pd.Timestamp('1999/01/01', tz='UTC')\r\n\r\n# unexpected mutation\r\nIn [19]: ser\r\nOut[19]: \r\n0   1999-01-01 00:00:00+00:00\r\ndtype: datetime64[ns, UTC]\r\n```"""
11790,120839608,jdeschenes,jreback,2015-12-07 18:42:04,2016-01-27 13:56:55,2016-01-19 20:02:06,closed,,0.18.0,17,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/11790,b'BUG: GH11786 Thread safety issue with read_csv ',b'closes #11786 \r\n\r\nFixed an issue with thread safety when calling read_csv with a StringIO object.\r\n\r\nThe issue was caused by a misplaced PyGilSate_Ensure()'
11786,120807976,mrocklin,jreback,2015-12-07 16:12:34,2016-01-19 20:02:06,2016-01-19 20:02:06,closed,,0.18.0,4,Bug;Difficulty Intermediate;Effort Medium;IO CSV;Prio-high,https://api.github.com/repos/pydata/pandas/issues/11786,b'read_csv in multiple theads causes segmentation fault',"b""The following script causes a segfault on my machine\r\n\r\n```python\r\nfrom io import BytesIO\r\nfrom multiprocessing.pool import ThreadPool\r\nimport pandas as pd\r\n\r\n# Make many fake CSV files in memory\r\nbytes = ['\\n'.join(['%d,%d,%d' % (i,i,i) for i in range(10000)]).encode()\r\n         for j in range(100)]\r\nfiles = [BytesIO(b) for b in bytes]\r\n\r\n# Read all files in many threads\r\npool = ThreadPool(8)\r\npool.map(pd.read_csv, files)\r\n```\r\n\r\n```\r\n$ python script.py \r\nSegmentation fault (core dumped)\r\n```\r\n\r\nPython 3.4, Pandas 0.17.1, Ubuntu 14.04"""
11779,120691810,srib,jreback,2015-12-07 03:36:26,2015-12-07 18:26:48,2015-12-07 11:11:04,closed,,0.18.0,1,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/11779,b'BUG: [GH 11738] Fix for end_time when multiple of a frequency is requested',"b""closes #11738 \r\n\r\n1. Added a new test with the multiple of a frequency.\r\n2. Rearranged some tests to improve readability of the code.\r\n3. Added bug fix to whatsnew\r\n4. Added a comment in the test\r\n\r\nDidn't follow the contributing guidelines carefully in PR #11771 and hence this new PR. """
11774,120632364,rockg,jreback,2015-12-06 12:49:21,2015-12-07 00:34:07,2015-12-07 00:33:58,closed,,0.18.0,4,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11774,b'BUG: Parsing offset strings with non-zero minutes was incorrect',b'Closes #11708\r\n\r\nMinutes needed to be added to the hour offset rather than subtracted.'
11772,120630846,yoongkang,jreback,2015-12-06 12:21:23,2015-12-07 11:46:05,2015-12-07 11:33:41,closed,,0.18.0,2,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11772,"b'BUG: Fixed DataFrame info() for duplicated columns, GH11761'",b'See #11761.'
11771,120611522,srib,srib,2015-12-06 06:13:56,2015-12-07 11:05:39,2015-12-07 03:06:22,closed,,0.18.0,3,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/11771,b'BUG: [GH 11738] Fix for end_time when multiple of a frequency is requested',b'closes #11738 \r\n\r\n1. Added a new test with the multiple of a frequency. \r\n2. Rearranged some tests to improve readability of the code.'
11761,120435065,wavexx,jreback,2015-12-04 16:21:11,2015-12-07 11:33:53,2015-12-07 11:33:53,closed,,0.18.0,2,Bug;Difficulty Novice;Effort Low;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11761,b'DataFrame.info() issue with duplicated columns',"b""When duplicated columns exists, DataFrame.info() returns broken output:\r\n\r\n````\r\npd.DataFrame([[1, 1]], columns=['x','x']).info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 1 entries, 0 to 0\r\nData columns (total 2 columns):\r\nx    1 non-null x    int64\r\nx    int64\r\ndtype: object\r\nx    1 non-null x    int64\r\nx    int64\r\ndtype: object\r\ndtypes: int64(2)\r\nmemory usage: 24.0 bytes\r\n````\r\n\r\nSee #11754 \r\n\r\n\r\n"""
11758,120403641,bhy,jreback,2015-12-04 13:40:16,2016-04-30 18:47:41,2016-04-30 18:47:41,closed,,0.18.1,9,Bug;Compat;Difficulty Novice;Effort Low;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11758,"b""to_datetime returns NaT for epoch with errors='coerce'""","b""xref #11760 \r\n\r\nParsing unix epoch timestamps give NaT with `errors='coerce'` while they can be parsed correctly without it:\r\n\r\n```python\r\n>>> pd.to_datetime(1420043460, errors='coerce', unit='s')\r\nNaT\r\n>>> pd.to_datetime(1420043460, unit='s')\r\nTimestamp('2014-12-31 16:31:00')\r\n```\r\n\r\npandas: 0.17.1\r\nnumpy: 1.10.1\r\nPython 2.7.7\r\n"""
11755,120272953,varun-kr,jreback,2015-12-03 21:23:04,2016-02-10 18:45:38,2016-02-10 18:45:38,closed,,0.18.1,9,Bug;Difficulty Intermediate;Effort Low;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/11755,b'concat of tz series and no tz results in attribute error.',"b'```python \r\nimport pandas as pd\r\nx = pd.Series(pd.date_range(\'20151124 10:00\', \'20151124 11:00\', freq = \'1h\', tz = ""UTC"") )\r\ny = pd.Series(pd.date_range(\'2012-01-01\', \'2012-01-03\'))\r\npd.concat([x,y])\r\n```\r\n\r\nAttributeError: \'numpy.ndarray\' object has no attribute \'tz_localize\'\r\n\r\nIs it a bug or not supported ?'"
11741,119983283,dwyatte,jreback,2015-12-02 16:37:39,2016-01-19 15:50:56,2016-01-17 03:43:25,closed,,0.18.0,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/11741,b'Unexpected behavior with groupby on single-row dataframe?',"b""Do single-row dataframes (not series) get special treatment in some way? It seems you can do arbitrary groupby operations on them on non-existent columns without errors.\r\n\r\n```\r\ndf1 = pd.DataFrame(np.random.randn(1,4), columns=list('ABCD'))\r\ndf2 = pd.DataFrame(np.random.randn(2,4), columns=list('ABCD'))\r\n\r\nIn [3]: df1.groupby('asdf')\r\nOut[3]: pandas.core.groupby.DataFrameGroupBy object at 0x1139acb10\r\n\r\nIn [4]: df2.groupby('asdf')\r\nKeyError: 'asdf'\r\n```"""
11738,119920855,pageecko,jreback,2015-12-02 11:17:47,2015-12-07 11:11:21,2015-12-07 11:11:21,closed,,0.18.0,3,Bug;Difficulty Novice;Effort Low;Period,https://api.github.com/repos/pydata/pandas/issues/11738,b'BUG: end_time for Period with multiplied freq',"b""end_time of a Period with multiplied freq is the square of the multiplication\r\n\r\n```python\r\n>>> pd.__version__\r\nu'0.17.0'\r\n\r\n>>> p = pd.Period('2015-08-01', freq='3D')\r\n>>> p.end_time, p.to_timestamp(how='E')\r\n(Timestamp('2015-08-09 23:59:59.999999999'), Timestamp('2015-08-03 00:00:00'))\r\n\r\n>>> p = pd.Period('2015-08-01', freq='5D')\r\n>>> p.end_time, p.to_timestamp(how='E')\r\n(Timestamp('2015-08-25 23:59:59.999999999'), Timestamp('2015-08-05 00:00:00'))\r\n```"""
11727,119500564,jorisvandenbossche,jreback,2015-11-30 14:15:33,2016-02-06 19:49:44,2016-02-06 19:49:44,closed,,0.18.0,5,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/11727,"b""BUG: mpl_style 'default' gives AttributeError: Unknown property color_cycle with latest matplotlib ""","b""http://stackoverflow.com/questions/33995707/attributeerror-unknown-property-color-cycle\r\n\r\n```\r\nIn [1]: matplotlib.__version__\r\nOut[1]: '1.5.0'\r\n\r\nIn [2]: df = pd.DataFrame(np.random.randn(5,5))\r\n\r\nIn [4]: df.plot()\r\nOut[4]: <matplotlib.axes._subplots.AxesSubplot at 0x7bc5c50>\r\n\r\nIn [5]: pd.set_option('display.mpl_style', 'default')\r\n\r\nIn [6]: df.plot()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n....\r\n\r\nAttributeError: Unknown property color_cycle\r\n\r\nIn [7]: pd.__version__\r\nOut[7]: '0.17.1'\r\n```"""
11718,119310274,kdebrab,sinhrks,2015-11-28 18:03:48,2016-02-11 23:35:54,2016-02-11 23:35:54,closed,,0.18.0,5,Bug;Difficulty Novice;Effort Low;Missing-data;Timezones,https://api.github.com/repos/pydata/pandas/issues/11718,b'Timestamp subtraction of NaT with timezones',"b'In pandas 0.17.1, a TypeError is returned when trying to subtract a timezone-aware timestamp from a NaT timestamp:\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.Timestamp(None, tz=\'utc\') - pd.Timestamp(\'now\', tz=\'utc\')\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-2-5e0738cec5fa>"", line 1, in <module>\r\n    pd.Timestamp(None, tz=\'utc\') - pd.Timestamp(\'now\', tz=\'utc\')\r\n\r\n  File ""pandas\\tslib.pyx"", line 1099, in pandas.tslib._NaT.__sub__ (pandas\\tslib.c:21618)\r\n\r\n  File ""pandas\\tslib.pyx"", line 1026, in pandas.tslib._Timestamp.__sub__ (pandas\\tslib.c:20036)\r\n\r\nTypeError: Timestamp subtraction must have the same timezones or no timezones\r\n```\r\nSubtracting a timezone unaware timestamp from a NaT timestamp is no problem. Also subtracting a NaT from a timezone aware timestamp works:\r\n```python\r\nIn [3]: pd.Timestamp(None) - pd.Timestamp(\'now\')\r\nOut[3]: NaT\r\n\r\nIn [4]: pd.Timestamp(\'now\', tz=\'utc\') - pd.Timestamp(None, tz=\'utc\')\r\nOut[4]: NaT\r\n\r\nIn [5]: pd.Timestamp(\'now\', tz=\'utc\') - pd.Timestamp(None)\r\nOut[5]: NaT\r\n```\r\n'"
11715,119246669,varun-kr,jreback,2015-11-27 21:14:17,2015-12-03 04:47:39,2015-12-02 15:05:09,closed,,0.18.0,7,Bug;Internals;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11715,b'Fix to BUG GH11698. added default value of mask',b'closes #11698\r\n \r\nPlease review.'
11711,119172808,BreitA,jreback,2015-11-27 10:34:00,2016-01-30 15:38:17,2016-01-30 15:38:17,closed,,0.18.0,7,Bug;Compat;Difficulty Novice;Effort Low;IO Excel,https://api.github.com/repos/pydata/pandas/issues/11711,b'read_excel fails to read excel file when last sheet is empty and sheetname=None',"b""Pandas fails to load an excel file as a dict fo dataframe when the last sheet is empty when sheetname=None.\r\nDeleting the last sheet manually fix the problem. \r\nMaybe Pandas should be improved to be robust to this common case.  \r\nAlso I post this to raise awareness of this issue in case someone is scratching his head wondering why pandas is not loading his excel file\r\n\r\nIn [151]:\r\n\r\ndf_dict=pd.read_excel('D:\\\\trash_test.xlsx',sheetname=None)\r\n\r\nprint df_dict.keys() # pandas fails to load the file\r\n\r\n\r\n\r\ndf=pd.read_excel('D:\\\\trash_test.xlsx',sheetname='a_test')\r\n\r\nprint df # pandas loads first sheet normally\r\n\r\ndf=pd.read_excel('D:\\\\trash_test.xlsx',sheetname='a_test')\r\n\r\nprint df # pandas loads second sheet normally\r\n\r\ndf=pd.read_excel('D:\\\\trash_test.xlsx',sheetname='not_default')\r\n\r\nprint df # pandas fails to load 'not_default' the last sheet which is empty (to be expected?)\r\n\r\nIndex([], dtype='object')\r\nEmpty DataFrame\r\nColumns: [some_stuff, 1]\r\nIndex: []\r\nEmpty DataFrame\r\nColumns: [some_stuff, 1]\r\nIndex: []\r\n\r\n---------------------------------------------------------------------------\r\nXLRDError                                 Traceback (most recent call last)\r\n<ipython-input-151-f10441544ad3> in <module>()\r\n      6 df=pd.read_excel('D:\\\\trash_test.xlsx',sheetname='a_test')\r\n      7 print df # pandas loads second sheet normally\r\n----> 8 df=pd.read_excel('D:\\\\trash_test.xlsx',sheetname='not_default')\r\n      9 print df # pandas fails to load 'not_default' the last sheet which is empty (to be expected?)\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\excel.pyc in read_excel(io, sheetname, header, skiprows, skip_footer, index_col, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, engine, **kwds)\r\n    168         date_parser=date_parser, na_values=na_values, thousands=thousands,\r\n    169         convert_float=convert_float, has_index_names=has_index_names,\r\n--> 170         skip_footer=skip_footer, converters=converters, **kwds)\r\n    171 \r\n    172 class ExcelFile(object):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\excel.pyc in _parse_excel(self, sheetname, header, skiprows, skip_footer, index_col, has_index_names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, verbose, **kwds)\r\n    370 \r\n    371             if isinstance(asheetname, compat.string_types):\r\n--> 372                 sheet = self.book.sheet_by_name(asheetname)\r\n    373             else:  # assume an integer if not a string\r\n    374                 sheet = self.book.sheet_by_index(asheetname)\r\n\r\nC:\\Anaconda\\lib\\site-packages\\xlrd\\book.pyc in sheet_by_name(self, sheet_name)\r\n    439             sheetx = self._sheet_names.index(sheet_name)\r\n    440         except ValueError:\r\n--> 441             raise XLRDError('No sheet named <%r>' % sheet_name)\r\n    442         return self.sheet_by_index(sheetx)\r\n    443 \r\n\r\nXLRDError: No sheet named <'not_default'>\r\n\r\n"""
11709,119089774,behzadnouri,jreback,2015-11-26 18:28:51,2015-11-29 19:51:39,2015-11-29 17:32:18,closed,,0.18.0,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/11709,b'BUG: work around for np.bincount with minlength=0',b'closes https://github.com/pydata/pandas/issues/11699\r\n\r\nxref https://github.com/pydata/pandas/issues/11189#issuecomment-152613636'
11708,119080369,PShiw,jreback,2015-11-26 17:02:50,2015-12-07 00:33:58,2015-12-07 00:33:58,closed,,0.18.0,8,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11708,"b'pd.to_datetime(""2015-11-18 15:30:00+05:30"").tz_localize(\'UTC\').tz_convert(\'Asia/Kolkata\') returns \'2015-11-18 16:30:00+0530\''","b'running ""pd.to_datetime(""2015-11-18 15:30:00+05:30"").tz_localize(\'UTC\').tz_convert(\'Asia/Kolkata\')"" returns following o/p\r\n\r\nTimestamp(\'2015-11-18 16:30:00+0530\', tz=\'Asia/Kolkata\')\r\n\r\nIt should have returned as Timestamp(\'2015-11-18 15:30:00+0530\', tz=\'Asia/Kolkata\') as no daylight saving is applicable in India.'"
11706,118969302,kawochen,jreback,2015-11-26 04:20:15,2015-12-01 12:11:08,2015-12-01 12:10:28,closed,,0.18.0,3,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/11706,b'BUG: GH11697 where memory allocation failed in rolling_median',b'closes #11697 \r\nthanks @ajspeck'
11699,118817123,Sereger13,jreback,2015-11-25 11:31:27,2015-11-29 17:32:18,2015-11-29 17:32:18,closed,,0.18.0,8,Bug;Difficulty Intermediate;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/11699,"b""group_by produces 'minlength must be positive error' when applied to empty DataFrame""","b""This used to work fine in previous versions but appears to be broken in 0.17.1\r\n\r\nThe following code:\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame({'A': [], 'B': []})\r\ngb = df.groupby('A') .size()\r\n```\r\nProduces this error:\r\n```\r\nValueError: minlength must be positive\r\n```\r\nIn v 0.16.2 the same code produced an empty DataFrame. We'd really like to upgrade to 0.17.1 but heavily rely on this functionality so have to hold the upgrade. Checking for empty DataFrame is not going to work for us either as there are too many places where it can actually be empty. \r\n\r\nIf you can suggest any workaround in the meantime so we could upgrade that would be appreciated. \r\n\r\n--\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-238.9.1.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US\r\n\r\npandas: 0.16.2\r\n..."""
11698,118713761,vladu,jreback,2015-11-24 22:07:52,2015-12-02 15:04:44,2015-12-02 15:04:43,closed,,0.18.0,3,Bug;Difficulty Novice;Effort Low;Indexing;Internals;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11698,b'pandas.DataFrame.replace raises UnboundLocalError on mixed data types',"b'This is in 0.17.1. Suppose we have a frame with mixed data types. Suppose also we want to replace a string value with something else. If the frame has only string columns, or maybe string and int columns, this works fine. However, if the frame has string and datetime columns, this raises an exception. Example:\r\n\r\n```\r\n>>> pandas.DataFrame([(\'-\', pandas.to_datetime(\'20150101\')), (\'a\', pandas.to_datetime(\'20150102\')), (\'b\', pandas.to_datetime(\'20150103\'))], columns=[\'a\', \'b\']).replace(\'-\', numpy.nan)\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-11-bce7dc78fc44> in <module>()\r\n----> 1 pandas.DataFrame([(\'-\', pandas.to_datetime(\'20150101\')), (\'a\', pandas.to_datetime(\'20150102\')), (\'b\', pandas.to_datetime(\'20150103\'))], columns=[\'a\', \'b\']).replace(\'-\', numpy.nan)\r\n\r\n/.../pandas/core/generic.pyc in replace(self, to_replace, value, inplace, limit, regex, method, axis)\r\n   3108                 elif not com.is_list_like(value):  # NA -> 0\r\n   3109                     new_data = self._data.replace(to_replace=to_replace, value=value,\r\n-> 3110                                                   inplace=inplace, regex=regex)\r\n   3111                 else:\r\n   3112                     msg = (\'Invalid ""to_replace"" type: \'\r\n\r\n/.../pandas/core/internals.pyc in replace(self, **kwargs)\r\n   2868\r\n   2869     def replace(self, **kwargs):\r\n-> 2870         return self.apply(\'replace\', **kwargs)\r\n   2871\r\n   2872     def replace_list(self, src_list, dest_list, inplace=False, regex=False, mgr=None):\r\n\r\n/.../pandas/core/internals.pyc in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\r\n   2821\r\n   2822             kwargs[\'mgr\'] = self\r\n-> 2823             applied = getattr(b, f)(**kwargs)\r\n   2824             result_blocks = _extend_blocks(applied, result_blocks)\r\n   2825\r\n\r\n/.../pandas/core/internals.pyc in replace(self, to_replace, value, inplace, filter, regex, convert, mgr)\r\n    605\r\n    606             # we can\'t process the value, but nothing to do\r\n--> 607             if not mask.any():\r\n    608                 return self if inplace else self.copy()\r\n    609\r\n\r\nUnboundLocalError: local variable \'mask\' referenced before assignment\r\n```\r\n\r\nAnd the requisite show_versions:\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-348.2.1.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 18.4\r\nCython: 0.23.4\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.1.0\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n```'"
11697,118709250,ajspeck,jreback,2015-11-24 21:41:56,2016-04-21 23:42:43,2015-12-01 12:10:28,closed,,0.18.0,7,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/11697,b'Rolling_median fails with large number of elements',"b'The new version of rolling_median from 0.17.1 fails with:\r\n```python\r\npandas\\algos.pyx in pandas.algos.roll_median_c (pandas\\algos.c:36408)()\r\n\r\nMemoryError: skiplist_insert failed \r\n```\r\nwhen called with series containing large number of values.  In comparison, the previous version of 0.17.0 was robust and never showed this error.\r\n\r\nOn my win-64 computer, I can routinely trigger the error by the following command:\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nn=50000\r\npd.rolling_median(pd.Series(np.random.randn(n)), window=2, center=False)\r\n```\r\nwhile\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nn=5000\r\npd.rolling_median(pd.Series(np.random.randn(n)), window=2, center=False)\r\n```\r\nworks fine.\r\n\r\nThe actual point at which the error begins appears to be non-deterministic but is somewhere around n=35,000. \r\n\r\nVersions are as follows:\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.1\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 18.5\r\nCython: 0.23.4\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.5.0\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.4.4\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None'"
11695,118694452,IamGianluca,jreback,2015-11-24 20:18:45,2015-12-18 18:26:10,2015-12-18 18:26:10,closed,,0.18.0,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/11695,b'BUG: Index does not inherit existing Index or DatatetimeIndex object \xa1\xad',"b""This PR is meant to fix issue #11193 \r\nThe issue prevent, when creating a new `Index` object based on a passed `Index` or `DatetimeIndex` object, to inherit the existing `Index` name if a new name is not provided. \r\n\r\nI've also created two unit test which test the new behaviour on both cases (passing an `Index` or `DatetimeIndex` object) and added a comment in the whatsnew documentation."""
11693,118654562,brendene,jreback,2015-11-24 16:52:53,2016-02-01 21:38:25,2016-02-01 21:38:25,closed,,0.18.0,3,Bug;Difficulty Novice;Effort Low;Reshaping;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/11693,b'Inconsistent concat behavior between datetime64[ns] and tz-aware version in 0.17.1',"b'This edge case appears when concatenating a timezone aware datetime series with another that is filled with only pd.NaT.  This works if the second series is only partially filled with pd.NaT.  It also works if the second series is another timezone (in that case the resulting Series is timezone unaware).\r\n\r\n```python\r\nimport pandas as pd\r\npd.__version__\r\nx = pd.Series( pd.date_range(\'20151124 08:00\', \'20151124 09:00\', freq = \'1h\') )\r\ny = pd.Series( pd.date_range(\'20151124 10:00\', \'20151124 11:00\', freq = \'1h\') )\r\npd.concat([x,y]) # Works\r\n\r\ny[:] = pd.NaT\r\npd.concat([x,y]) # Works\r\n\r\nx = pd.Series( pd.date_range(\'20151124 08:00\', \'20151124 09:00\', freq = \'1h\', tz = ""US/Eastern"") )\r\ny = pd.Series( pd.date_range(\'20151124 10:00\', \'20151124 11:00\', freq = \'1h\', tz = ""US/Eastern"") )\r\npd.concat([x,y])\r\n\r\ny[0] = pd.NaT\r\npd.concat([x,y]) # Works\r\n\r\ny[:] = pd.NaT\r\npd.concat([x,y]) # Fails\r\n```'"
11682,118474145,ajenkins-cargometrics,jreback,2015-11-23 21:17:53,2015-11-27 13:40:22,2015-11-27 13:40:22,closed,,0.18.0,2,Bug;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/11682,b'Timezone info lost when broadcasting scalar datetime to DataFrame',"b""I've encountered a bug in pandas 0.16.2, where when using broadcasting to assign a datetime.datetime value to a whole column of a DataFrame, the timezone info is lost.  Here is an example:\r\n\r\n```python\r\nIn [1]: import pandas, datetime, pytz\r\n\r\nIn [2]: df = pandas.DataFrame({'a': [1,2,3]})\r\n\r\nIn [3]: dt = datetime.datetime.now(pytz.utc)\r\n\r\nIn [4]: dt.tzinfo\r\nOut[4]: <UTC>\r\n\r\nIn [5]: df['b'] = dt\r\n\r\nIn [6]: df\r\nOut[6]: \r\n   a                          b\r\n0  1 2015-11-23 21:02:54.562175\r\n1  2 2015-11-23 21:02:54.562175\r\n2  3 2015-11-23 21:02:54.562175\r\n\r\nIn [7]: df['b'][0].tzinfo\r\n```\r\n\r\nNote how `dt` has a timezone attached, but the values in the 'b' column don't.  The problem only occurs when broadcasting a scalar datetime column, not when assigning an array or series. Also, the problem only occurs when using the builtin datetime.datetime class, not pandas's Timestamp class.\r\n\r\nI've tracked the problem down to the pandas.core.common._infer_dtype_from_scalar function, which is called during the assignment.  It contains this code for handling scalar date times:\r\n\r\n```python\r\n    elif isinstance(val, (np.datetime64, datetime)) and getattr(val,'tz',None) is None:\r\n        val = lib.Timestamp(val).value\r\n        dtype = np.dtype('M8[ns]')\r\n```\r\n\r\nThe problem is that the Timestamp.value property returns an integer value which doesn't contain the timezone information, so the timezone is lost.  The reason this problem occurs for datetime.datetime, but not for pandas.Timestamp, is because the code is looking for the 'tz' attribute, which is specific to Timestamp.  If the gettattr call was changed to look at the 'tzinfo' attribute instead, this code would work correctly for both pandas.Timestamp and datetime.datetime values.  So a fix for this code which works for both datetime and Timestamp would be:\r\n\r\n```python\r\n    elif isinstance(val, (np.datetime64, datetime)) and getattr(val,'tzinfo',None) is None:\r\n        val = lib.Timestamp(val).value\r\n        dtype = np.dtype('M8[ns]')\r\n```\r\n\r\nI checked and this bug still exists in the latest version of the pandas source.   Nevertheless here is the output of show_versions() on my machine:\r\n\r\n```python\r\nIn [8]: pandas.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.7\r\nCython: 0.23.2\r\nnumpy: 1.9.2\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.1.0\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2012c\r\nbottleneck: None\r\ntables: 3.2.1.1\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.8\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\n```"""
11672,118236057,varun-kr,jreback,2015-11-22 03:18:43,2015-11-27 14:43:04,2015-11-27 13:40:00,closed,,0.18.0,7,Bug;Groupby;Timezones,https://api.github.com/repos/pydata/pandas/issues/11672,b'BUG: GH11616 fixes timezone selection error',b'closes #11616 \r\ncloses #11682 '
11653,117926569,jreback,jreback,2015-11-19 22:47:48,2015-11-20 00:38:08,2015-11-20 00:38:08,closed,,0.17.1,0,Bug;Enhancement;Indexing,https://api.github.com/repos/pydata/pandas/issues/11653,"b'BUG: indexing with a range , #11652'",b'closes #11652 '
11652,117920565,nekobon,jreback,2015-11-19 22:14:22,2015-11-20 00:38:08,2015-11-20 00:38:08,closed,,0.17.1,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/11652,"b'BUG: ValueError when indexing using range only when length >= 1,000,000'","b'From SO (http://stackoverflow.com/questions/33814223/strange-error-in-pandas-indexing-with-range-when-length-1-000-000)\r\n\r\nPandas raises a ValueError when assigning multiple values to a Series (or DataFrame) using range(x) where x > 1. This error is raised only when its length is one million or larger.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nfor x in [5, 999999, 1000000]:\r\n    s = pd.Series(index=range(x))\r\n    print(\'series length =\', len(s))\r\n    # assigning value with range(1), always works\r\n    s.loc[range(1)] = 42 \r\n    # reading values with range(x>1), always works\r\n    _ = s.loc[range(2)] \r\n    # assigning values with range(x>1), fails only when len >= 1 million\r\n    s.loc[range(2)] = 42 \r\n```\r\nOutput:\r\n```python\r\nseries length = 5\r\nseries length = 999999\r\nseries length = 1000000\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 9, in <module>\r\n  File ""/home/nekobon/.env_exp/lib/python3.4/site-packages/pandas/core/indexing.py"", line 114, in __setitem__\r\n    indexer = self._get_setitem_indexer(key)\r\n  File ""/home/nekobon/.env_exp/lib/python3.4/site-packages/pandas/core/indexing.py"", line 109, in _get_setitem_indexer\r\n    return self._convert_to_indexer(key, is_setter=True)\r\n  File ""/home/nekobon/.env_exp/lib/python3.4/site-packages/pandas/core/indexing.py"", line 1042, in _convert_to_indexer\r\n    return labels.get_loc(obj)\r\n  File ""/home/nekobon/.env_exp/lib/python3.4/site-packages/pandas/core/index.py"", line 1692, in get_loc\r\n    return self._engine.get_loc(_values_from_object(key))\r\n  File ""pandas/index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:3979)\r\n  File ""pandas/index.pyx"", line 145, in pandas.index.IndexEngine.get_loc (pandas/index.c:3680)\r\n  File ""pandas/index.pyx"", line 464, in pandas.index._bin_search (pandas/index.c:9124)\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n```\r\nTested on pandas 0.17.0 and python 3.4.'"
11649,117896152,varun-kr,jreback,2015-11-19 20:02:39,2015-11-20 01:41:16,2015-11-20 00:40:45,closed,,0.17.1,4,Bug;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11649,b'Bug#11637 fix to_csv',b'This is a fix to Bug #11637. \r\nError Description : incorrect output when a mix of integer and string column names passed as columns parameter in to_csv(). \r\n[format.py] (https://github.com/pydata/pandas/blob/master/pandas/core/format.py#L1330) is the cause of this discrepancy. \r\n\r\nPlease review .'
11644,117733127,varun-kr,jreback,2015-11-19 03:27:19,2015-11-20 01:41:08,2015-11-20 00:59:20,closed,,0.17.1,8,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11644,b'BUG #11638 return correct dtype for int and float',b'closes #11638\r\n Please review. '
11639,117585085,jorisvandenbossche,jorisvandenbossche,2015-11-18 13:19:31,2015-11-18 21:34:56,2015-11-18 21:34:56,closed,,0.17.1,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/11639,b'BUG: remove_unused_categories with NaN values (GH11599)',b'Closes #11599'
11638,117574642,cpaulik,jreback,2015-11-18 12:17:46,2015-11-20 15:18:49,2015-11-20 00:59:32,closed,,0.17.1,12,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/11638,b'Dataframe column dtype changed from int8 to int64 when setting complete column',"b'The following example should explain:\r\n\r\n```python\r\nPython 2.7.10 |Continuum Analytics, Inc.| (default, Oct 19 2015, 18:04:42) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 4.0.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-52-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 18.4\r\nCython: None\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\nIn [4]: df = pd.DataFrame({\'one\': np.full(10, 0, dtype=np.int8)})\r\n\r\nIn [5]: df\r\nOut[5]: \r\n   one\r\n0    0\r\n1    0\r\n2    0\r\n3    0\r\n4    0\r\n5    0\r\n6    0\r\n7    0\r\n8    0\r\n9    0\r\n\r\nIn [6]: df.dtypes\r\nOut[6]: \r\none    int8\r\ndtype: object\r\n\r\nIn [7]: df.loc[1, \'one\'] = 6\r\n\r\nIn [8]: df\r\nOut[8]: \r\n   one\r\n0    0\r\n1    6\r\n2    0\r\n3    0\r\n4    0\r\n5    0\r\n6    0\r\n7    0\r\n8    0\r\n9    0\r\n\r\nIn [9]: df.dtypes\r\nOut[9]: \r\none    int8\r\ndtype: object\r\n\r\nIn [10]: df.one = np.int8(7)\r\n\r\nIn [11]: df.dtypes\r\nOut[11]: \r\none    int64\r\ndtype: object\r\n\r\nIn [12]: df\r\nOut[12]: \r\n   one\r\n0    7\r\n1    7\r\n2    7\r\n3    7\r\n4    7\r\n5    7\r\n6    7\r\n7    7\r\n8    7\r\n9    7\r\n```\r\n\r\nSo it is cast to the correct dtype if a slice of the column is changed but setting the whole column changes the dtype even when explicitly set to `np.int8`'"
11637,117568111,Sereger13,jreback,2015-11-18 11:33:18,2015-11-20 00:40:57,2015-11-20 00:40:57,closed,,0.17.1,2,Bug;Difficulty Novice;Effort Low;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11637,b'DataFrame.to_csv() produces wrong results when mixed-type columns are specified',"b""This used to work OK in version 0.15 but stopped working in versions 0.16+.\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame({0: ['a', 'b', 'c'],\r\n                   1: ['aa', 'bb', 'cc']})\r\ndf['test'] = 'txt'\r\ndf.to_csv('/home/temp/df.csv', columns=['test', 0, 1])\r\n```\r\nThe following output is produced (values from columns 0 and 1 are missing):\r\n```\r\n,test,0,1\r\n0,txt,,\r\n1,txt,,\r\n2,txt,,\r\n```\r\nReplacing 'test' with an integer value or skipping the columns argument works fine.\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-238.9.1.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US\r\n\r\npandas: 0.17.0\r\n..."""
11633,117488583,DSLituiev,jreback,2015-11-18 01:10:01,2016-05-18 13:22:36,2016-05-18 13:22:36,closed,,0.19.0,7,Bug;Difficulty Novice;Effort Low;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/11633,"b""Pivot to SparseDataFrame: TypeError: ufunc 'isnan' not supported in sparse matrix conversion""","b'I want to convert a DataFrame to SparseDataFrame before pivoting it (when it gets really sparse, see also [this discussion](http://stackoverflow.com/questions/31661604/efficiently-create-sparse-pivot-tables-in-pandas) ). I have a textual key, which I need to keep (""chr""):\r\n\r\n    df = pd.DataFrame( list(zip([3,2,4,1,5,3,2],\r\n                 [""chr1"", ""chr1"", ""chr1"",  ""chr1"", ""chr2"", ""chr2"", ""chr3""], \r\n                [100,100, 100, 200, 1,3,1],\r\n                [True, True, True, False, True, False, True],\r\n                [-1,0,1,3, 0,2,1])) ,\r\n                columns = [""counts"", ""chr"", ""pos"", ""strand"", ""distance""])\r\n    \r\n    df.iloc[:,1:].dtypes\r\n    Out[]: \r\n    chr         object\r\n    pos          int64\r\n    strand        bool\r\n    distance     int64\r\n    dtype: object\r\n\r\nFor this small table it works well with regular `DataFrame`:\r\n\r\n    pd.pivot_table(df, index= [ ""chr"", ""pos""], columns= [""strand"",""distance""], values= ""counts"").fillna(0)\r\n\r\n         strand   False    True       \r\n    distance     2  3    -1  0  1\r\n    chr  pos                     \r\n    chr1 100     0  0     3  2  4\r\n         200     0  1     0  0  0\r\n    chr2 1       0  0     0  5  0\r\n         3       3  0     0  0  0\r\n    chr3 1       0  0     0  0  2\r\nBut I need to do it on much larger matrices. So I tried to do following trick:\r\n\r\n    dfpiv = pd.pivot_table(pd.SparseDataFrame(df), index= [ ""chr"", ""pos""], columns= [""strand"",""distance""], values= ""counts"")\r\n\r\nbut I am getting:\r\n\r\n    TypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\r\n\r\nAre there any plans to include a functionality option into `pivot` function for automatic conversion into SparseDataFrame?'"
11627,117336755,jreback,jreback,2015-11-17 11:38:13,2015-11-17 12:18:23,2015-11-17 12:18:23,closed,,0.17.1,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11627,"b'BUG: date_range creation with an ambiguous endpoint, #11619'",b'TST: some tests for datetime tz aware serialized to/from csv/hdf\r\n\r\ncloses #11626 '
11626,117334954,JackKelly,jreback,2015-11-17 11:26:22,2015-11-17 13:12:30,2015-11-17 12:18:23,closed,,0.17.1,1,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11626,b'AmbiguousTimeError when constructing a Series with DST change',"b'In Pandas 0.17, if I construct a `Series` which contains a day light saving transition, then Pandas crashes with an `AmbiguousTimeError`.\r\n\r\nHere\'s the minimal code example to reproduce the bug:\r\n\r\n```python\r\nimport pandas as pd\r\nindex = pd.date_range(""2013-10-26 23:00"", ""2013-10-27 01:00"",\r\n                      tz=""Europe/London"", freq=""H"")\r\nseries = pd.Series(0, index=index)\r\n```\r\n\r\nAnd here\'s the traceback:\r\n\r\n```python\r\nIn [4]: ---------------------------------------------------------------------------\r\nAmbiguousTimeError                        Traceback (most recent call last)\r\n<ipython-input-4-5deb3c83fafd> in <module>()\r\n----> 1 __pyfile = open(\'\'\'/tmp/py7201vZi\'\'\');exec(compile(__pyfile.read(), \'\'\'/home/jack/temp/pandas_dst_bug.py\'\'\', \'exec\'));__pyfile.close(); import os; os.remove(\'\'\'/tmp/py7201vZi\'\'\')\r\n\r\n/home/jack/temp/pandas_dst_bug.py in <module>()\r\n      2 index = pd.date_range(""2013-10-26 23:00"", ""2013-10-27 01:00"",\r\n      3                       tz=""Europe/London"", freq=""H"")\r\n----> 4 series = pd.Series(0, index=index)\r\n\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/tseries/index.pyc in date_range(start, end, periods, freq, tz, normalize, name, closed)\r\n   1912     return DatetimeIndex(start=start, end=end, periods=periods,\r\n   1913                          freq=freq, tz=tz, normalize=normalize, name=name,\r\n-> 1914                          closed=closed)\r\n   1915 \r\n   1916 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/util/decorators.pyc in wrapper(*args, **kwargs)\r\n     87                 else:\r\n     88                     kwargs[new_arg_name] = new_arg_value\r\n---> 89             return func(*args, **kwargs)\r\n     90         return wrapper\r\n     91     return _deprecate_kwarg\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, dtype, **kwargs)\r\n    234             return cls._generate(start, end, periods, name, freq,\r\n    235                                  tz=tz, normalize=normalize, closed=closed,\r\n--> 236                                  ambiguous=ambiguous)\r\n    237 \r\n    238         if not isinstance(data, (np.ndarray, Index, ABCSeries)):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/tseries/index.pyc in _generate(cls, start, end, periods, name, offset, tz, normalize, ambiguous, closed)\r\n    450 \r\n    451                 if end is not None and end.tz is None:\r\n--> 452                     end = end.tz_localize(tz)\r\n    453 \r\n    454             if start and end:\r\n\r\npandas/tslib.pyx in pandas.tslib.Timestamp.tz_localize (pandas/tslib.c:11965)()\r\n\r\npandas/tslib.pyx in pandas.tslib.tz_localize_to_utc (pandas/tslib.c:64516)()\r\n\r\nAmbiguousTimeError: Cannot infer dst time from Timestamp(\'2013-10-27 01:00:00\'), try using the \'ambiguous\' argument\r\n```\r\n\r\nI\'m not certain but I expect this bug is related to #11619\r\n\r\nThis bug report follows on from #11624\r\n\r\nI\'m 99.999% certain that this bug did not exist in Pandas 0.16.2\r\n\r\n```\r\nIn [30]: show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-33-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 1.5.6\r\nsetuptools: 15.2\r\nCython: 0.23.1\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.4.6\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: 0.9.2\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.5.3 (dt dec pq3 ext)\r\n```\r\n\r\nFinally, just to say another huge thank you to everyone who supports Pandas.  It must be a huge amount of work and I am hugely grateful.  Pandas is an awesome tool.  Thank you.'"
11618,117166279,skycaptain,jreback,2015-11-16 16:43:55,2015-11-20 16:26:22,2015-11-20 13:55:24,closed,,0.17.1,4,Bug;Compat;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11618,"b'BUG: fix col iteration in DataFrame.round, #11611'",b'Fixes #11611. The iterator now uses DataFrame.iteritems instead of direct indexing.'
11616,117163999,alexandreyc,jreback,2015-11-16 16:32:52,2015-11-27 13:40:10,2015-11-27 13:40:10,closed,,0.18.0,2,Bug;Difficulty Novice;Effort Low;Groupby;Regression;Timezones,https://api.github.com/repos/pydata/pandas/issues/11616,b'groupby UTC timestamp aggregation',"b""Hi all,\r\n\r\nI've found an inconsistency between pandas 0.17 and 0.16.2 when aggregating on UTC timestamps. Here is a snippet to reproduce the problem:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(42)\r\n\r\ndata = pd.DataFrame({\r\n\t'factor': np.random.randint(0, 3, size=60),\r\n\t'time': pd.date_range('01/01/2000 00:00', periods=60, freq='s', tz='UTC')\r\n})\r\n\r\ngp = data.groupby('factor')\r\n\r\nprint(gp['time'].min())\r\nprint(gp['time'].max())\r\n```\r\n\r\nOn 0.16.2 the output seems correct, i.e it returns timestamps:\r\n\r\n```\r\nIn [1]: %run bug_pandas.py\r\nfactor\r\n0    2000-01-01 00:00:01+00:00\r\n1    2000-01-01 00:00:07+00:00\r\n2    2000-01-01 00:00:00+00:00\r\nName: time, dtype: object\r\nfactor\r\n0    2000-01-01 00:00:57+00:00\r\n1    2000-01-01 00:00:54+00:00\r\n2    2000-01-01 00:00:59+00:00\r\nName: time, dtype: object\r\n```\r\n\r\nHowever on 0.17 it returns timestamps as integers:\r\n\r\n```\r\nIn [1]: %run bug_pandas.py\r\nfactor\r\n0    946684801000000000\r\n1    946684807000000000\r\n2    946684800000000000\r\nName: time, dtype: int64\r\nfactor\r\n0    946684857000000000\r\n1    946684854000000000\r\n2    946684859000000000\r\nName: time, dtype: int64\r\n```\r\nIt should be noted that the problem doesn't appear with `tz=None`.\r\n\r\nThanks for your help,\r\n\r\nAlexandre\r\n"""
11611,117100427,skycaptain,jreback,2015-11-16 11:05:38,2015-11-20 13:55:35,2015-11-20 13:55:35,closed,,0.17.1,2,Bug;Compat;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11611,b'Stack overflow on applying numpy functions to DataFrame with duplicated column entries.',"b'Applying a numpy function, such as `np.round`, to a DataFrame with duplicated column indicies can cause an unrecoverable stack overflow error (Fatal Python error: Cannot recover from stack overflow.), which causes e.g. a ipython kernel to crash. E.g. take the following example, where python crashes at line 4:\r\n\r\n```python\r\nx = pd.DataFrame(np.random.randn(3,3))\r\ny = pd.DataFrame(np.random.randn(3,3))\r\nz = pd.concat((x, y), axis=1)\r\nprint(np.round(z))\r\n```\r\n\r\nHowever, removing the duplicate column entries, works as expected:\r\n```python\r\n...\r\nz = pd.concat((x, y), axis=1, ignore_index=True)\r\nprint(np.round(z))\r\n```\r\n\r\n---\r\npython 3.5.0, numpy 1.10.1, pandas 0.17.0'"
11607,116955410,sinhrks,jreback,2015-11-14 22:12:57,2015-11-23 01:37:17,2015-11-23 01:20:25,closed,,0.18.0,3,Bug;Categorical;Indexing,https://api.github.com/repos/pydata/pandas/issues/11607,b'BUG: loc against CategoricalIndex may results in normal Index',"b""Closes #11586.\r\n\r\nAfter the PR:\r\n\r\n```\r\nimport pandas as pd\r\nindex = pd.CategoricalIndex(list('aabbca'), categories=list('cabe'))\r\ndf = pd.DataFrame({'A' : np.arange(6,dtype='int64')}, index=index)\r\n\r\n# OK (not changed)\r\ndf.loc[['a', 'b']].index\r\n# CategoricalIndex([u'a', u'a', u'a', u'b', u'b'], categories=[u'c', u'a', u'b', u'e'], ordered=False, dtype='category')\r\n\r\n# Fixed to return Categorical Index if value exists in categories\r\ndf.loc[['a', 'b', 'e']].index\r\n# CategoricalIndex([u'a', u'a', u'a', u'b', u'b', u'e'], categories=[u'a', u'b', u'e'], ordered=False, dtype='category')\r\n\r\n# raise KeyError otherwise (not changed)\r\ndf.loc[['a', 'b', 'x']].index\r\n# KeyError: 'a list-indexer must only include values that are in the categories'\r\n```\r\n\r\nThere are separate paths when ``CategoricalIndex`` values (codes) are unique and not-unique and both fixed.\r\n\r\nAlso, ``.reindex`` intentionally returns normal ``Index`` when passed values are not ``Categorical``, I kept the behavior as it is. If ``.reindex`` can always return ``CategoricalIndex``, above 2 separate fixes are not required (fixing ``.reindex`` to return ``CategoricalIndex`` should work both paths).\r\n\r\n```\r\n# return normal Index(not changed)\r\ndf.index.reindex(['a', 'b'])\r\n# (Index([u'a', u'a', u'a', u'b', u'b'], dtype='object'), array([0, 1, 5, 2, 3]))\r\n\r\n# return CategoricalIndex(not changed)\r\ndf.index.reindex(pd.Categorical(['a', 'b']))\r\n# (CategoricalIndex([u'a', u'a', u'a', u'b', u'b'], categories=[u'a', u'b'], ordered=False, dtype='category'), array([0, 1, 5, 2, 3]))\r\n```"""
11606,116951648,Ezekiel-Kruglick,jreback,2015-11-14 20:56:22,2015-11-19 02:28:06,2015-11-19 02:28:06,closed,,0.17.1,7,Bug;MultiIndex;Sparse,https://api.github.com/repos/pydata/pandas/issues/11606,b'BUG GH11600  - MultiIndex column level names lost when to_sparse() called',"b""closes #11600 \r\n\r\nFixed problem with multi-index column level names not propagating into sparse frames or back out to dense on a round trip through sparse. Includes 4 new tests to cover some relevant scenarios. Problem fixed for the conventions to_sparse path, I'm not sure about other paths where something else is passed to SparseDataSeries directly, those would be outside scope of bug.\r\n"""
11600,116880403,Ezekiel-Kruglick,jreback,2015-11-14 00:21:57,2015-11-19 01:13:54,2015-11-19 01:13:54,closed,,0.17.1,8,Bug;Difficulty Intermediate;Effort Medium;MultiIndex;Sparse,https://api.github.com/repos/pydata/pandas/issues/11600,b'BUG - sparse dataframes lose multi-index column names',"b'From SO: http://stackoverflow.com/questions/33702198/do-python-pandas-sparse-dataframes-lose-multi-index-column-names-or-am-i-doing-i\r\n\r\nBug is simple in concept, multi-index with column level names loses those names when going into sparse dataframes.\r\n\r\nMinimal example - first create a multi-index dataframe:\r\n```\r\nIn[2]: import pandas as pd\r\nIn[3]: miindex = pd.MultiIndex.from_product([[""x"",""y""], [""10"",""20""]],names=[\'row-foo\', \'row-bar\'])\r\nmicol = pd.MultiIndex.from_product([[\'a\',\'b\',\'c\'], [""1"",""2""]],names=[\'col-foo\', \'col-bar\'])\r\ndf = pd.DataFrame(index=miindex, columns=micol).sortlevel().sortlevel(axis=1)\r\ndf = df.fillna(value=3.14)\r\ndf\r\nOut[3]: \r\ncol-foo             a           b           c      \r\ncol-bar             1     2     1     2     1     2\r\nrow-foo row-bar                                    \r\nx       10       3.14  3.14  3.14  3.14  3.14  3.14\r\n        20       3.14  3.14  3.14  3.14  3.14  3.14\r\ny       10       3.14  3.14  3.14  3.14  3.14  3.14\r\n        20       3.14  3.14  3.14  3.14  3.14  3.14\r\n```\r\nThis gives us a nice test multi-index with column and row level names. Now if I make a sparse matrix out of that and show it, the column level names are gone.\r\n```\r\nIn[4]: ds = df.to_sparse()\r\nds\r\nOut[4]: \r\n                    a           b           c      \r\n                    1     2     1     2     1     2\r\nrow-foo row-bar                                    \r\nx       10       3.14  3.14  3.14  3.14  3.14  3.14\r\n        20       3.14  3.14  3.14  3.14  3.14  3.14\r\ny       10       3.14  3.14  3.14  3.14  3.14  3.14\r\n        20       3.14  3.14  3.14  3.14  3.14  3.14\r\n```\r\nAnd if I convert the sparse version back to dense those level names are still gone.\r\n```\r\nIn[6]: ds.to_dense()\r\nOut[6]: \r\n                    a           b           c      \r\n                    1     2     1     2     1     2\r\nrow-foo row-bar                                    \r\nx       10       3.14  3.14  3.14  3.14  3.14  3.14\r\n        20       3.14  3.14  3.14  3.14  3.14  3.14\r\ny       10       3.14  3.14  3.14  3.14  3.14  3.14\r\n        20       3.14  3.14  3.14  3.14  3.14  3.14\r\n```\r\nI AM aware that displaying the sparse version calls to_dense() but the loss appears to be happening at the conversion to sparse. I\'m exploring moving to sparse to reduce memory usage for a code base and my attempts to access the levels within the sparse dataframe generate ""KeyError: \'Level not found\'""'"
11599,116868659,jorisvandenbossche,jorisvandenbossche,2015-11-13 22:45:30,2015-11-18 21:34:56,2015-11-18 21:34:56,closed,,0.17.1,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/11599,b'BUG in remove_unused_categories with NaNs in values',"b'From SO: http://stackoverflow.com/questions/33693601/removing-unused-categories-in-series-results-in-duplicated-categories\r\n\r\n`removed_unused_categories` gives a wrong result when there is a NaN in the values:\r\n\r\n```\r\n>>> s = pd.Series([""A"", ""B"", pd.np.nan]).astype(""category"")\r\n>>> s.cat.remove_unused_categories()\r\n0      A\r\n1      B\r\n2    NaN\r\ndtype: category\r\nCategories (3, object): [B, A, B]\r\n```\r\n\r\nI think the `-1` in the codes (`NaN`) duplicates the last item in the categories\r\n'"
11594,116811012,jorisvandenbossche,jreback,2015-11-13 17:18:47,2016-03-23 17:56:45,2016-03-23 17:56:45,closed,,0.18.1,5,Bug;Difficulty Novice;Effort Low;Master Tracker;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11594,b'BUG: displayed dtype of series inferred from shown subset instead of series',"b""All the same issue\r\n\r\n- [ ] #12045 \r\n- [ ] #12211 \r\n\r\n```\r\nIn [1]: import datetime\r\n\r\nIn [2]: s = pd.Series([datetime.datetime(2012, 1, 1)]*10 + [datetime.datetime(1012,1,2)] + [datetime.datetime(2012, 1, 3)]*10)\r\n\r\nIn [8]: s\r\nOut[8]:\r\n0     2012-01-01 00:00:00\r\n1     2012-01-01 00:00:00\r\n2     2012-01-01 00:00:00\r\n3     2012-01-01 00:00:00\r\n4     2012-01-01 00:00:00\r\n5     2012-01-01 00:00:00\r\n6     2012-01-01 00:00:00\r\n7     2012-01-01 00:00:00\r\n8     2012-01-01 00:00:00\r\n9     2012-01-01 00:00:00\r\n10    1012-01-02 00:00:00\r\n11    2012-01-03 00:00:00\r\n12    2012-01-03 00:00:00\r\n13    2012-01-03 00:00:00\r\n14    2012-01-03 00:00:00\r\n15    2012-01-03 00:00:00\r\n16    2012-01-03 00:00:00\r\n17    2012-01-03 00:00:00\r\n18    2012-01-03 00:00:00\r\n19    2012-01-03 00:00:00\r\n20    2012-01-03 00:00:00\r\ndtype: object\r\n\r\nIn [9]: pd.options.display.max_rows = 8\r\n\r\nIn [10]: s\r\nOut[10]:\r\n0    2012-01-01\r\n1    2012-01-01\r\n2    2012-01-01\r\n3    2012-01-01\r\n        ...\r\n17   2012-01-03\r\n18   2012-01-03\r\n19   2012-01-03\r\n20   2012-01-03\r\ndtype: datetime64[ns]\r\n\r\nIn [11]: s.dtype\r\nOut[11]: dtype('O')\r\n```\r\n\r\nSo in some cases, you think you have a `datetime64` series, but actually you don't and eg `dt` properties don't work which can lead to quite some confusion ... :-)"""
11586,116718815,sinhrks,jreback,2015-11-13 07:42:25,2015-11-23 01:20:25,2015-11-23 01:20:25,closed,,0.18.0,1,Bug;Categorical;Indexing,https://api.github.com/repos/pydata/pandas/issues/11586,b'BUG: .loc against CategoricalIndex may result in normal Index',"b""``.loc`` against ``CategoricalIndex`` with values included in its ``categories`` but not appears as ``codes`` results in normal ``Index``\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nindex = pd.CategoricalIndex(list('aabbca'), categories=list('cabe'))\r\ndf = pd.DataFrame({'A' : np.arange(6,dtype='int64')}, index=index)\r\n\r\n# OK\r\ndf.loc[['a', 'b']].index\r\n# CategoricalIndex([u'a', u'a', u'a', u'b', u'b'], categories=[u'c', u'a', u'b', u'e'], ordered=False, dtype='category')\r\n\r\n# NG, must be CategoricalIndex\r\ndf.loc[['a', 'b', 'e']].index\r\n# Index([u'a', u'a', u'a', u'b', u'b', u'e'], dtype='object')\r\n```\r\n\r\nEven though this is tested here, it doesn't check dtype.\r\n\r\n- https://github.com/pydata/pandas/blob/master/pandas/tests/test_indexing.py#L4781"""
11581,116529334,lexual,jreback,2015-11-12 11:11:18,2015-11-15 16:46:47,2015-11-15 16:46:44,closed,,0.17.1,6,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11581,b'ENH: #3335 Pivot table support for setting name of margins column.',"b""closes #3335.\r\n\r\nAdds margin_column parameter to pivot_table so that user can set it to\r\nsomething other than 'All'.\r\nRaises ValueError exception if there is a conflict between the value of\r\nmargin_column and one of the other values appearing in the indices of\r\nthe pivot table.\r\n\r\ntake into account notes from closed pull request: #10296"""
11567,116199899,Evfro,jreback,2015-11-10 20:56:39,2015-11-11 01:39:24,2015-11-11 01:39:17,closed,,,1,Bug;Duplicate,https://api.github.com/repos/pydata/pandas/issues/11567,b'unexpected behaviour of DataFrame.duplicated',"b""At least for large datasets `DataFrame.duplicated` returns incorrect results.\r\n\r\nConsider MovieLens10M data (this code will automatically download the data from grouplens website):\r\n```python\r\nimport pandas as pd\r\nfrom requests import get\r\nfrom StringIO import StringIO\r\nzip_file_url = 'http://files.grouplens.org/datasets/movielens/ml-10m.zip'\r\nzip_response = get(zip_file_url)\r\nzip_contents = StringIO(zip_response.content)\r\n\r\nwith ZipFile(zip_contents) as zfile:\r\n    zdata = zfile.read('ml-10M100K/ratings.dat')\r\n    delimiter = ';'\r\n    zdata = zdata.replace('::', delimiter) # makes data compatible with pandas c-engine\r\n    mldata = pd.read_csv(StringIO(zdata), sep=delimiter, header=None, engine='c',\r\n                              names=['userid', 'movieid', 'rating', 'timestamp'],\r\n                              usecols=['userid', 'movieid', 'rating'])\r\n```\r\nThe data (`mldata` variable) contains no duplicates, which can be verified:\r\n```python\r\n(mldata.groupby(['userid', 'movieid']).size()>1).any()\r\nFalse\r\n\r\nmldata.set_index(['userid', 'movieid']).index.is_unique\r\nTrue\r\n```\r\nHowever,  `DataFrame.duplicated` gives:\r\n```python\r\ndups = mldata.duplicated(['userid', 'movieid'], keep=False)\r\nprint dups.any()\r\nprint dups.sum()\r\n\r\nTrue\r\n12127\r\n```\r\nExpected:\r\n```python\r\nFalse\r\n0\r\n```\r\n\r\n`pd.show_versions()`:\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 8\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 18.3.2\r\nCython: 0.23.3\r\nnumpy: 1.10.0\r\nscipy: 0.16.0\r\nstatsmodels: None\r\nIPython: 3.2.1\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.6\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.0\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.3\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.5\r\npymysql: None\r\npsycopg2: None"""
11564,116033274,kawochen,jreback,2015-11-10 05:53:32,2016-01-05 18:44:24,2015-12-31 13:46:31,closed,,0.18.0,29,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/11564,b'BUG: GH11349 where Series.apply and Series.map did not box timedelta64',b'closes #11349 \r\ncloses #11925 '
11561,115891633,jorisvandenbossche,jreback,2015-11-09 14:48:37,2015-11-15 18:36:39,2015-11-15 18:36:39,closed,,0.17.1,6,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/11561,b'VIS: only apply shared axes handling on actual SubplotAxes',b'closes #11556\r\ncloses #11520\r\n'
11560,115864213,roman-khomenko,jreback,2015-11-09 12:04:53,2015-11-13 15:16:48,2015-11-13 15:06:26,closed,,0.17.1,5,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/11560,b'BUG: Fix bug for kendall corr when in DF num and bool',"b""Hi,\r\n\r\n1. When DataFrame contain Numerics and Booleans, than numpy will have type object,\r\nso `np.isfinite(mat)` will raise Exception.\r\n\r\nI've fixed this by using `com._ensure_float64` like for other correlation.\r\n\r\n2. I've skipped half of computation, because correlation is symmetrical"""
11559,115829201,rserbitar,jreback,2015-11-09 08:21:53,2016-04-17 13:54:53,2016-04-17 13:54:53,closed,,0.18.1,2,Bug;Compat;Difficulty Intermediate;Effort Medium;Indexing,https://api.github.com/repos/pydata/pandas/issues/11559,b'Subclassing not working as expected',"b'Running the code:\r\n\r\n```python\r\nimport pandas\r\n\r\nclass SubclassedSeries(pandas.Series):\r\n    @property\r\n    def _constructor(self):\r\n        return SubclassedSeries\r\n\r\n    @property\r\n    def _constructor_expanddim(self):\r\n        return SubclassedDataFrame\r\n\r\n    def c(self):\r\n        return 1\r\n\r\nclass SubclassedDataFrame(pandas.DataFrame):\r\n    @property\r\n    def _constructor(self):\r\n        return SubclassedDataFrame\r\n\r\n    @property\r\n    def _constructor_sliced(self):\r\n        return SubclassedSeries\r\n\r\n\r\nb = SubclassedDataFrame([[1,2,3,4],[2,3,4,5]])\r\nprint(b[0].c())\r\n```\r\n\r\nworks as expected.\r\n\r\nHowever running:\r\n\r\n```python\r\nprint(b.ix[1].c())\r\n```\r\nthrows an exception as:\r\n\r\n```python\r\nb.ix[1]\r\n```\r\nreturns a Series instance instead of an SublcassedSeries instance as expected.'"
11558,115789910,williamsmj,jreback,2015-11-09 04:32:58,2016-03-29 20:09:45,2016-03-29 20:09:45,closed,,0.18.1,3,Bug;Difficulty Intermediate;Effort Low;Indexing;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/11558,b'groupby categorical column fails with unstack',"b""Replicating example\r\n```\r\nIn [1]: df = pd.DataFrame([[1,2],[3,4]],columns=pd.CategoricalIndex(list('AB')))\r\n\r\nIn [2]: df.describe()\r\nAttributeError: 'DataFrame' object has no attribute 'value_counts'\r\n```\r\n\r\nThe behaviour in [this notebook](https://gist.github.com/williamsmj/c6cbabcb25f27f08407f) seems like a bug to me. This is pandas 0.17.0.\r\n\r\nIn it, `g` and `gcat` are the results of two `df.groupby(['medium', 'artist']).count().unstack()` operations. The only difference is that one of those operations is on `df` where one of the columns that the `groupby` operates over has been converted to Categorical.\r\n\r\n`g` and `gcat` behave very differently. I've tried to pin this down to the exact operation in the split-apply-combine that causes the problem without much luck. \r\n\r\nSlicing a column out of `g` returns a Series as expected, while slicing a column out of `gcat` returns a DataFrame (see cells 4 and 5).\r\n\r\n`g.describe()` works as expected, but `gcat.describe()` raises the exception\r\n\r\n    AttributeError: 'DataFrame' object has no attribute 'value_counts'\r\n\r\nand `g['painting'] + g['sculpture']` works as expected but `g['painting'] + g['sculpture']` raises\r\n\r\n    Exception: Data must be 1-dimensional"""
11556,115772545,jakevdp,jreback,2015-11-09 00:28:00,2015-11-15 18:36:51,2015-11-15 18:36:51,closed,,0.17.1,6,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/11556,b'Plotting fails when using subplot grid + manual axis',"b""The following code raises an error (tested on pandas 0.17.0, matplotlib 1.4.3, and Python 3.4.2)\r\n```\r\n>>> fig, ax = plt.subplots()\r\n>>> fig.add_axes([0.2, 0.2, 0.2, 0.2])\r\n>>> pd.Series(np.random.rand(100)).plot(ax=ax)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-13-80531efe48c4> in <module>()\r\n      8 fig, ax = plt.subplots()\r\n      9 inset = fig.add_axes([0.2, 0.2, 0.2, 0.2])\r\n---> 10 data.plot(ax=ax)\r\n\r\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/tools/plotting.py in __call__(self, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\r\n   3491                            colormap=colormap, table=table, yerr=yerr,\r\n   3492                            xerr=xerr, label=label, secondary_y=secondary_y,\r\n-> 3493                            **kwds)\r\n   3494     __call__.__doc__ = plot_series.__doc__\r\n   3495 \r\n\r\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/tools/plotting.py in plot_series(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\r\n   2581                  yerr=yerr, xerr=xerr,\r\n   2582                  label=label, secondary_y=secondary_y,\r\n-> 2583                  **kwds)\r\n   2584 \r\n   2585 \r\n\r\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/tools/plotting.py in _plot(data, x, y, subplots, ax, kind, **kwds)\r\n   2378         plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\r\n   2379 \r\n-> 2380     plot_obj.generate()\r\n   2381     plot_obj.draw()\r\n   2382     return plot_obj.result\r\n\r\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/tools/plotting.py in generate(self)\r\n    990             self._post_plot_logic_common(ax, self.data)\r\n    991             self._post_plot_logic(ax, self.data)\r\n--> 992         self._adorn_subplots()\r\n    993 \r\n    994     def _args_adjust(self):\r\n\r\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/tools/plotting.py in _adorn_subplots(self)\r\n   1141                                 naxes=nrows * ncols, nrows=nrows,\r\n   1142                                 ncols=ncols, sharex=self.sharex,\r\n-> 1143                                 sharey=self.sharey)\r\n   1144 \r\n   1145         for ax in self.axes:\r\n\r\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/tools/plotting.py in _handle_shared_axes(axarr, nplots, naxes, nrows, ncols, sharex, sharey)\r\n   3378                 layout = np.zeros((nrows+1,ncols+1), dtype=np.bool)\r\n   3379                 for ax in axarr:\r\n-> 3380                     layout[ax.rowNum, ax.colNum] = ax.get_visible()\r\n   3381 \r\n   3382                 for ax in axarr:\r\n\r\nAttributeError: 'Axes' object has no attribute 'rowNum'\r\n```"""
11551,115747697,nbonnotte,jreback,2015-11-08 17:37:41,2015-11-16 09:31:41,2015-11-11 03:20:12,closed,,0.17.1,7,Bug;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11551,b'BUG: multi-index to_native_types is not passing thru parameters',b'closes #7791\r\nxref #6797'
11548,115708166,superChing,jreback,2015-11-08 04:00:58,2015-11-08 16:13:44,2015-11-08 16:13:44,closed,,,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/11548,b'BUG: groupby with datetime columns',"b'```\r\ndf=pd.DataFrame(\r\n    [[75,1,\'2013-07-15\',],\r\n     [44,3,\'2014-04-02\',],\r\n     [15,2,\'2013-05-23\',],\r\n     [93,1,\'2014-04-04\',]] ,columns=[\'Sales\',\'Store\',\'Date\'])\r\n\r\ndf.Date=pd.to_datetime(df.Date)\r\ngb=df.groupby(\'Store\')\r\ngb.apply(lambda df: pd.Series([1,2,3]))\r\n```\r\n**KeyError: ""[\'Date\'] not in index""**\r\nIf I comment out `to_datetime(df.Date)`  to use string type for date, then it works fine.\r\n\r\n----\r\n### more details follows:\r\n\r\nINSTALLED VERSIONS\r\npython: 3.4.3.final.0\r\npandas: 0.17.0\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-109-45118450dd9c> in <module>()\r\n      8 df.Date=pd.to_datetime(df.Date)\r\n      9 gb=df.groupby(\'Store\')\r\n---> 10 gb.apply(lambda df: pd.Series([1,2,3]))\r\n\r\n......I omit the intermediate traces for succinct.\r\n\r\n/Users/apple/miniconda3/lib/python3.4/site-packages/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter)\r\n   1119                 mask = check == -1\r\n   1120                 if mask.any():\r\n-> 1121                     raise KeyError(\'%s not in index\' % objarr[mask])\r\n   1122 \r\n   1123                 return _values_from_object(indexer)\r\n\r\nKeyError: ""[\'Date\'] not in index""\r\n'"
11546,115696337,varun-kr,jreback,2015-11-07 22:30:01,2015-11-10 04:52:18,2015-11-10 02:27:23,closed,,0.17.1,12,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/11546,b'BUG: GH11517 add multiindex column names after describe()',"b'Hi,\r\n\r\nThis fixes the bug mentioned in https://github.com/pydata/pandas/issues/11517\r\nI have added column names after the describe operation. \r\nRequest you to review the change and merge it. \r\n\r\nThanks '"
11545,115695832,nbonnotte,jreback,2015-11-07 22:14:36,2015-11-16 09:31:54,2015-11-11 03:23:40,closed,,0.17.1,7,Bug;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11545,"b""BUG: df.join(df2, how='right') TypeError""","b'Issue #11519\r\n\r\nSomehow right joins had been forgotten in a previous bugfix. There were tests already written that should have seen the problem, but they had been commented out because the expected results were wrong, because of a subtlety in the way non-unique index are handled.\r\n\r\nThanks for the labels ""difficulty novice"" and ""effort low"": I\'m using pandas every day, and I\'m glad I could contribute that easily.'"
11543,115668991,Pekka4444,jreback,2015-11-07 14:08:35,2015-11-07 14:17:21,2015-11-07 14:12:28,closed,,,2,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11543,b'Potential bug: drop_duplicates() and duplicated() fail for multiple integer columns',"b""It seems that drop_duplicates() and duplicated() methods are not working properly for large integer columns. Here is my example data frame http://pastebin.com/KVHxUpgz\r\n\r\n    import pandas as pd\r\n\r\n    pd.read_clipboard(delimiter=',')\r\n    r = x.duplicated(keep=False)\r\n    print(x[r])\r\n\r\nThis gives me:\r\n                 x1       x2\r\n    8   16000010001  8470207\r\n    95  16000010009  8470039\r\n\r\nClearly these are not duplicates but seems like pandas thinks they are!\r\n\r\nAlso drop_duplicates() seems to fail:\r\n\r\n    print(len(x),len(x.drop_duplicates()))\r\n\r\ngives: 101 100\r\n    \r\nWhen I convert my columns to string they are not duplicates anymore:\r\n\r\n    r1 = x.apply(lambda x: '%d-%d' % tuple(x),axis=1).duplicated()\r\n    print(r1.sum())\r\n\r\nis 0 as it should.\r\n\r\n\r\nHere is the versions:\r\n\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fi_FI\r\n\r\npandas: 0.17.0\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.4\r\nCython: 0.23.4\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: None\r\nIPython: 4.0.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.6\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: 2.2.6\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.7\r\nlxml: 3.4.4\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: None\r\n\r\n"""
11531,115481643,jorisvandenbossche,jorisvandenbossche,2015-11-06 10:50:01,2015-11-09 21:11:09,2015-11-09 21:11:09,closed,,0.17.1,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/11531,b'BUG: fix incorrect xticklabels when specifying xticks (GH11529)',"b'Fixes #11529\r\n\r\n@sinhrks in PR #10717 you moved the setting of the ticklabels from `_adorn_subplots` to its own function `_post_plot_logic_common`. But this is called before `_adorn_subplots`, and the specified `xticks` are only set in `_adorn_subplots`, so now it does not use the new xticks to determine the xticklabels. \r\nI just switched the order of the two functions. At first sight this seems OK, but do you know if there could be any side effects of switching the order?\r\n\r\n'"
11529,115452113,Eastsun,jorisvandenbossche,2015-11-06 07:21:39,2015-11-09 21:11:09,2015-11-09 21:11:09,closed,,0.17.1,1,Bug;Regression;Visualization,https://api.github.com/repos/pydata/pandas/issues/11529,b'May a bug of `Series.plot` in pandas 0.17?',"b""The origin post with pictures is on: http://stackoverflow.com/questions/33559772/is-this-a-bug-of-series-plot-in-pandas-0-17\r\n\r\nI always use `ser.plot(xticks=range(0, len(ser), step), ..)` to plot figures with specified xticks. But it doesn't work anymore in pandas 0.17 when the Series with labels. Here are the codes:\r\n```python\r\nIn [1]: from numpy import random as rnd \r\nIn [2]: import pandas as pd\r\nIn [3]: pd.__version__\r\nOut[3]: '0.17.0'\r\nIn [4]: %matplotlib inline\r\nIn [5]: rnd.seed(123)\r\nIn [6]: ser = pd.Series(rnd.randn(73).cumsum(), index=['P%02d' % i for i in range(73)])\r\nIn [7]: ser.plot(figsize=(9, 6), rot=60, title='Figure without xticks')\r\nOut[7]: <matplotlib.axes._subplots.AxesSubplot at 0x8370198\r\nIn [8]: ser.plot(figsize=(9, 6), xticks=list(range(0, 73, 6)), rot=60, title='Figure with xticks')\r\nOut[8]: <matplotlib.axes._subplots.AxesSubplot at 0x83e9940>\r\n```\r\n![image](https://cloud.githubusercontent.com/assets/511719/10991679/4acedd1c-849a-11e5-89d0-f6264b788849.png)\r\n![image](https://cloud.githubusercontent.com/assets/511719/10991682/4d695a16-849a-11e5-9db3-6ff885a22234.png)\r\n\r\nThe figure with xticks specified doesn't display xticklabes correctly."""
11525,115303694,ankurankan,jreback,2015-11-05 15:07:58,2015-11-05 15:20:58,2015-11-05 15:20:37,closed,,,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11525,b'Behavior change in indexing using ix in 0.17.0',"b""I am getting an error when indexing by comparing tuples:\r\n```python\r\nIn [13]: df = pd.DataFrame([[('A', 1), ('B', 0)], [('A', 0), ('B', 1)], [('A', 1), ('B', 0)]], columns=['A', 'B'])\r\n\r\nIn [14]: df\r\nOut[14]: \r\n        A       B\r\n0  (A, 1)  (B, 0)\r\n1  (A, 0)  (B, 1)\r\n2  (A, 1)  (B, 0)\r\n\r\nIn [15]: df[df.A == ('A', 1)]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-2663526e3e28> in <module>()\r\n----> 1 df[df.A == ('A', 1)]\r\n\r\n/home/ankur/anaconda3/envs/py3/lib/python3.4/site-packages/pandas/core/ops.py in wrapper(self, other, axis)\r\n    724     op_desc = _op_descriptions[op_name]\r\n    725     if op_desc['reversed']:\r\n--> 726         equiv = 'other ' + op_desc['op'] + ' series'\r\n    727     else:\r\n    728         equiv = 'series ' + op_desc['op'] + ' other'\r\n\r\n/home/ankur/anaconda3/envs/py3/lib/python3.4/site-packages/pandas/core/ops.py in na_op(x, y)\r\n    642             if isinstance(y, list):\r\n    643                 y = lib.list_to_object_array(y)\r\n--> 644 \r\n    645             if isinstance(y, (np.ndarray, pd.Series)):\r\n    646                 if (x.dtype == np.bool_ and\r\n\r\npandas/lib.pyx in pandas.lib.vec_compare (pandas/lib.c:14361)()\r\n\r\nValueError: Arrays were different lengths: 3 vs 2\r\n```\r\nBut this used to work in 0.16.2, so not sure if this is expected or a bug."""
11519,115167793,dragoljub,jreback,2015-11-04 22:52:03,2015-11-11 03:23:53,2015-11-11 03:23:52,closed,,0.17.1,2,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11519,"b""df.join(df2, how='right')  TypeError: Argument 'left' has incorrect type (expected numpy.ndarray, got Int64Index)""","b""I have found that 'right' joins on index values raise TypeEerror: Argument 'left' has incorrect type (expected numpy.ndarray, got Int64Index). \r\n\r\nBoth index types are the same Int64Index index. This works in pandas 0.13.1.\r\n\r\nPandas: 0.17.0\r\nNumpy: 1.9.2\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\r\n                          'foo', 'bar', 'foo', 'foo'],\r\n                    'B' : ['one', 'one', 'two', 'three',\r\n                          'two', 'two', 'one', 'three'],\r\n                    'C' : np.random.randn(8),\r\n                    'D' : np.random.randn(8)})\r\n\r\ns = pd.Series(np.repeat(np.arange(8),2), index=np.repeat(np.arange(8),2), name='TEST')\r\n\r\nIn []: s.head()\r\n\r\nOut[]:\r\n0    0\r\n0    0\r\n1    1\r\n1    1\r\n2    2\r\ndtype: int32\r\n\r\n# The following all work as expected\r\ndf.join(s, how='inner') \r\ndf.join(s, how='outer')\r\ndf.join(s, how='left')\r\n\r\n# Right Joins Type Error\r\ndf.join(s, how='right') \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-80-26e8bf54fd8f> in <module>()\r\n----> 1 df.join(s, how='right')\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in join(self, other, on, how, lsuffix, rsuffix, sort)\r\n   4218         # For SparseDataFrame's benefit\r\n   4219         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\r\n-> 4220                                  rsuffix=rsuffix, sort=sort)\r\n   4221 \r\n   4222     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _join_compat(self, other, on, how, lsuffix, rsuffix, sort)\r\n   4232             return merge(self, other, left_on=on, how=how,\r\n   4233                          left_index=on is None, right_index=True,\r\n-> 4234                          suffixes=(lsuffix, rsuffix), sort=sort)\r\n   4235         else:\r\n   4236             if on is not None:\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\r\n     33                          right_index=right_index, sort=sort, suffixes=suffixes,\r\n     34                          copy=copy, indicator=indicator)\r\n---> 35     return op.get_result()\r\n     36 if __debug__:\r\n     37     merge.__doc__ = _merge_doc % '\\nleft : DataFrame'\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in get_result(self)\r\n    194             self.left, self.right = self._indicator_pre_merge(self.left, self.right)\r\n    195 \r\n--> 196         join_index, left_indexer, right_indexer = self._get_join_info()\r\n    197 \r\n    198         ldata, rdata = self.left._data, self.right._data\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in _get_join_info(self)\r\n    309         if self.left_index and self.right_index:\r\n    310             join_index, left_indexer, right_indexer = \\\r\n--> 311                 left_ax.join(right_ax, how=self.how, return_indexers=True)\r\n    312         elif self.right_index and self.how == 'left':\r\n    313             join_index, left_indexer, right_indexer = \\\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in join(self, other, how, level, return_indexers)\r\n   2212             if self.is_monotonic and other.is_monotonic:\r\n   2213                 return self._join_monotonic(other, how=how,\r\n-> 2214                                             return_indexers=return_indexers)\r\n   2215             else:\r\n   2216                 return self._join_non_unique(other, how=how,\r\n\r\nD:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in _join_monotonic(self, other, how, return_indexers)\r\n   2463                 join_index, lidx, ridx = self._left_indexer(sv, ov)\r\n   2464             elif how == 'right':\r\n-> 2465                 join_index, ridx, lidx = self._left_indexer(other, self)\r\n   2466             elif how == 'inner':\r\n   2467                 join_index, lidx, ridx = self._inner_indexer(sv, ov)\r\n\r\nTypeError: Argument 'left' has incorrect type (expected numpy.ndarray, got Int64Index)\r\n\r\n```"""
11517,115106890,dragoljub,jreback,2015-11-04 17:37:52,2015-11-10 02:27:58,2015-11-10 02:27:58,closed,,0.17.1,2,Bug;Difficulty Novice;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/11517,b'df.describe() clobbers columns.names',"b""I recently migrated some of my code to Pandas 0.17.0. I found that the df.describe() method is clobbering index names when used after a transpose. Here I'm just using transpose as an easy way to create multi-index column names.\r\n\r\nPandas 0.17.0\r\nNumpy 1.9.2\r\n\r\n````\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\r\n                          'foo', 'bar', 'foo', 'foo'],\r\n                    'B' : ['one', 'one', 'two', 'three',\r\n                          'two', 'two', 'one', 'three'],\r\n                    'C' : np.random.randn(8),\r\n                    'D' : np.random.randn(8)})\r\n\r\n# Create Multi Index For Rows\r\nIn [11]: df.groupby(['A', 'B']).mean().index.names\r\nOut[11]: FrozenList([u'A', u'B'])\r\n\r\n# Transposing the row index to column names works.\r\nIn [31]: df.groupby(['A', 'B']).mean().T.columns.names \r\nOut[31]: FrozenList([u'A', u'B'])\r\n\r\n# After the describe() method the column names are dropped.\r\nIn [23]: df.groupby(['A', 'B']).mean().T.describe().columns.names \r\nOut[23]: FrozenList([None]) # In Pandas 0.13.1 the column names were preserved.\r\n\r\n```"""
11507,114766613,henrystokeley,jreback,2015-11-03 08:17:46,2016-01-06 17:20:51,2016-01-06 17:20:51,closed,,,10,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/11507,b'BUG: GH10355 groupby std() doesnt sqrt grouping cols',"b""New attempt at #10355 Hopefully should address the issues raised in #11300\r\n\r\nPreviously, grouping columns were square rooted when as_index=False\r\nWe now test whether the grouping keys are in the columns, and\r\nif so don't square root those columns.\r\n\r\nNote that we squash TypeError which occurs when self.keys is not\r\nHashable, and so we can't check for existence in columns."""
11497,114466497,sinhrks,jreback,2015-11-01 12:05:05,2015-11-29 19:42:00,2015-11-29 18:01:15,closed,,0.18.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/11497,b'BUG: .loc with duplicated label may have incorrect index dtype',"b""``.loc`` result with duplicated keys may have incorred ``Index`` dtype.\r\n\r\n```\r\nimport pandas as pd\r\n\r\nser = pd.Series([0.1, 0.2], index=pd.Index([1, 2], name='idx'))\r\n\r\n# OK\r\nser.loc[[2, 2, 1]].index\r\n# Int64Index([2, 2, 1], dtype='int64', name=u'idx')\r\n\r\n# NG, Int64Index(dtype=object) \r\nser.loc[[3, 2, 3]].index \r\n# Int64Index([3, 2, 3], dtype='object', name=u'idx')\r\nser.loc[[3, 2, 3, 'x']].index \r\n# Int64Index([3, 2, 3, u'x'], dtype='object', name=u'idx')\r\n\r\nidx = pd.date_range('2011-01-01', '2011-01-02', freq='D', name='idx')\r\nser = pd.Series([0.1, 0.2], index=idx, name='s')\r\n\r\n# OK\r\nser.loc[[pd.Timestamp('2011-01-02'), pd.Timestamp('2011-01-02'), pd.Timestamp('2011-01-01')]].index\r\n# DatetimeIndex(['2011-01-02', '2011-01-02', '2011-01-01'], dtype='datetime64[ns]', name=u'idx', freq=None)\r\n\r\n# NG, ValueError\r\nser.loc[[pd.Timestamp('2011-01-03'), pd.Timestamp('2011-01-02'), pd.Timestamp('2011-01-03')]].index\r\n# ValueError: Inferred frequency None from passed dates does not conform to passed frequency D\r\n```\r\n\r\n## After the PR:\r\n\r\nAbove OK results are unchanged.\r\n\r\n```\r\nimport pandas as pd\r\nser = pd.Series([0.1, 0.2], index=pd.Index([1, 2], name='idx'))\r\n\r\nser.loc[[3, 2, 3]].index \r\n# Int64Index([3, 2, 3], dtype='int64', name=u'idx')\r\nser.loc[[3, 2, 3, 'x']].index \r\n# Index([3, 2, 3, u'x'], dtype='object', name=u'idx')\r\n\r\nidx = pd.date_range('2011-01-01', '2011-01-02', freq='D', name='idx')\r\nser = pd.Series([0.1, 0.2], index=idx, name='s')\r\n\r\nser.loc[[pd.Timestamp('2011-01-03'), pd.Timestamp('2011-01-02'), pd.Timestamp('2011-01-03')]].index\r\n# DatetimeIndex(['2011-01-03', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', name=u'idx', freq=None)\r\n```\r\n"""
11484,114277431,rockg,jreback,2015-10-30 14:11:20,2015-11-14 14:57:29,2015-11-14 14:57:23,closed,,0.17.1,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11484,b'BUG: Holiday observance rules could not be applied',"b'Closes #11477\r\nCloses #11533 \r\n\r\nThere were some other bugs here that I added tests for.  For example, `holiday.dates` for MLK day was returning all holidays from the holiday start date up to the end date rather than just between the range.'"
11477,114139340,vlmercado,jreback,2015-10-29 20:32:09,2015-11-14 14:57:23,2015-11-14 14:57:23,closed,,0.17.1,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11477,b'Time Series / Date functionality Holiday lookup - possible bug',"b""When trying to confirm if a specific date is an observed holiday using the specific date for the start and end range, the attempt fails to confirm a known observed holiday. It works as expected if I change the end date. When I try to use the same start and end date again, it works every time.\r\n\r\nIn 2015, the July 4th holiday was observed on July 3rd. Since a BDay() offset ignores holidays, a confirmation of an observed holiday failed when using July 3, 2015 for the start and end dates. See code below:\r\n\r\n```python\r\nfrom datetime import datetime\r\nfrom pandas.tseries.holiday import get_calendar, HolidayCalendarFactory, GoodFriday\r\n\r\nUSFedCal = get_calendar('USFederalHolidayCalendar')\r\n\r\nUSFedCal.holidays(datetime(2015,7,3), datetime(2015,7,3)) # <-- same start and end dates\r\nDatetimeIndex([], dtype='datetime64[ns]', freq=None)\r\n\r\nUSFedCal.holidays(datetime(2015,7,3), datetime(2015,7,6)) # <-- different start and end dates\r\nDatetimeIndex(['2015-07-03'], dtype='datetime64[ns]', freq=None)\r\n\r\nUSFedCal.holidays(datetime(2015,7,3), datetime(2015,7,3)) # <-- same start and end dates\r\nDatetimeIndex(['2015-07-03'], dtype='datetime64[ns]', freq=None)\r\n```"""
11460,113869383,robdmc,jreback,2015-10-28 17:04:13,2015-11-13 21:48:21,2015-11-13 21:48:21,closed,,0.17.1,3,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11460,"b'implemented fix for groupby date bug, #11324'",b'This is a fix for issue #11324 in which grouping when datetime fields are involved raises an exceptoin\r\n\r\n'
11445,113694360,stahlous,jreback,2015-10-27 21:31:46,2016-01-20 14:12:47,2016-01-20 14:12:24,closed,,,6,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11445,b'BUG: fix Panel.fillna() ignoring axis parameter (re-submission)',"b""closes #3570 \r\ncloses #8251\r\n\r\n\r\nThis is a re-submission of PR #8395 and addresses issue #8251. My apologies for the duplicate PR, but despite rebasing to pydata/pandas master, the original PR would not update with my new commits and I couldn't get rid of the `LooseVersion()` errors in the Travis build.\r\n\r\nThis PR may need some fleshing out still, but I wanted to submit this to see what the thoughts are on this approach. For this PR I've created a new interpolation mechanism at the block level to implement filling across 3 or more dimensions. This avoids the fiddly problems of trying to implement filling of 3+ dimensions at the frame level. However, it isn't possible with this technique to fill across blocks of different dtypes, although that seems like that should be a rare occurrence. \r\n\r\nIncidentally, this will also address #3570, which is one of the issues referenced in #9862."""
11431,113321737,jorisvandenbossche,jorisvandenbossche,2015-10-26 10:00:40,2015-11-16 12:15:32,2015-11-16 12:15:32,closed,,0.17.1,1,Bug;IO SQL;Unicode,https://api.github.com/repos/pydata/pandas/issues/11431,b'UnicodeEncodeError with to_sql and unicode column names',"b""From SO: http://stackoverflow.com/questions/33337798/unicodeencodeerror-when-using-pandas-method-to-sql-on-a-dataframe-with-unicode-c\r\n\r\nTo reproduce:\r\n\r\n```\r\nfrom sqlalchemy import create_engine \r\nengine = create_engine('sqlite:///:memory:')\r\ndf = pd.DataFrame([[1,2],[3,4]], columns = [u'\\xe9',u'b'])\r\ndf.to_sql('data', engine, if_exists='replace', index=False)\r\n```\r\n\r\nwhich gives ``UnicodeEncodeError: 'ascii' codec can't encode character u'\\xe9' in position 0: ordinal not in range(128)``, because of this line: https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L856, where it stringifies the individual columns names with `str` (which fails on python 2 with unicode)."""
11427,113183325,chris-b1,jreback,2015-10-24 21:09:45,2016-01-15 01:03:40,2015-12-13 20:42:04,closed,,0.18.0,5,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/11427,b'BUG: vectorized DateOffset match non-vectorized',"b'closes #11370 \r\n\r\nAlso added a bit of documentation on the semantics for anchored offsets\r\n\r\ncurrently blocked by #11406 - either need to change the vectorized or non-vectorized implementation of `QuarterBegin`  As discussed in that issue, my opinion is the vectorized version is the ""right"" (or at least consistent) one.'"
11422,113093297,cfperez,jreback,2015-10-23 20:26:25,2015-10-27 11:11:42,2015-10-27 11:11:42,closed,,0.17.1,3,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/11422,b'BUG: implement .sort_index(...inplace=True) for #11402',b'closes #11402\r\n \r\nAdds inplace sort_index() for Series.'
11410,112791043,jreback,jreback,2015-10-22 12:19:57,2015-10-23 16:43:22,2015-10-23 16:43:22,closed,,0.17.1,0,Bug;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/11410,"b'Bug in merging datetime64[ns, tz] dtypes #11405'",b'closes #11405 '
11408,112742240,dinya,jreback,2015-10-22 06:54:32,2015-10-25 14:03:18,2015-10-25 14:03:18,closed,,0.17.1,8,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/11408,"b'Pandas exporting to Excel (xls, xlsx) with multilevel columns'","b""Hello all,\r\n\r\nI want collect few tables into the one with pandas. I use the code presented below:\r\n\r\n```python\r\nimport pandas as pd\r\nimport itertools\r\nimport types\r\n\r\ndf = None\r\nfor frame in ['x', 'y']:\r\n    df_ = pd.read_excel(u'%s.xlsx' % frame)\r\n    df_ = df_.set_index([u'time'])\r\n\r\n    parameters = list(df_.columns)\r\n    tuples = []\r\n    for tup in itertools.product([frame,], parameters):\r\n        tuples.append(tup)\r\n\r\n    columns = pd.MultiIndex.from_tuples(tuples, names=[u'Frames',u'Parameters'])\r\n\r\n    df_new = pd.DataFrame(columns=columns, index=df_.index)\r\n    for par in parameters:\r\n        df_new[frame, par] = df_[par]\r\n    del df_\r\n\r\n    if df is None:\r\n        df = df_new\r\n    else:\r\n        df = pd.concat([df, df_new], axis=1)\r\n\r\ndf.to_excel('merged_xlsx.xlsx')\r\ndf.to_excel('merged_xls.xls')\r\n```\r\n\r\nSource data is ``x.xlsx``\r\n![x](https://cloud.githubusercontent.com/assets/2182222/10658681/887a8df8-78b2-11e5-9ee8-8fe3b6057c3a.png)\r\n\r\nand ``y.xlsx`` files.\r\n![y](https://cloud.githubusercontent.com/assets/2182222/10658680/88795dd4-78b2-11e5-9652-7a670ded97a0.png)\r\n\r\nXLS engine works well (``merged_xls.xls``):\r\n![merged_xls](https://cloud.githubusercontent.com/assets/2182222/10658677/885b90ec-78b2-11e5-9e0d-34f9acb6f3a1.png)\r\n\r\nBut something is wrong (cells merging) with XLSX engine (``merged_xlsx.xlsx``):\r\n![merged_xlsx](https://cloud.githubusercontent.com/assets/2182222/10658678/885ba3b6-78b2-11e5-84fb-a60d9664b6f9.png)\r\n\r\nManual cells unmerging works in Excel:\r\n![merged_xlsx_unmerged](https://cloud.githubusercontent.com/assets/2182222/10658679/88603084-78b2-11e5-82d9-6674c480810f.png)\r\n\r\nIs it bug in XLSX pandas engine (openpyxl)? Or what is wrong in my code?\r\n\r\nVersions: ``python-2.7.10``, ``pandas-0.17.0``, ``openpyxl-2.3.0``.\r\n\r\nP.S. This issue is copy of [my question on stackoverflow](http://stackoverflow.com/questions/33274716/pandas-exporting-to-excel-xls-xlsx-with-multilevel-columns). It was suggested as bug and was adviced to post here."""
11405,112702302,miraculixx,jreback,2015-10-21 23:56:12,2015-10-23 16:43:22,2015-10-23 16:43:22,closed,,0.17.1,1,Bug;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/11405,b'pd.merge fails on datetime columns with tzinfo',"b'Since pandas-0.17 a merge on a datetime column fails if the datetime is tz-aware, see example below. Possibly related to #9663?\r\n\r\n```\r\nimport pandas as pd\r\nfrom datetime import datetime\r\nfrom dateutil.tz import gettz\r\nimport sys, os\r\nimport traceback as tbm\r\n# works\r\na = pd.DataFrame({\'created\' : [datetime(2015,10,10), \r\n                               datetime(2015,10,20)], \r\n                  \'count\' : [1,2]})\r\nb = pd.DataFrame({\'created\' : [datetime(2015,10,10), \r\n                               datetime(2015,10,20)], \r\n                  \'count\' : [1,2]})\r\npd.merge(a, b, how=\'outer\')\r\n# doesn\'t work (used to work on pandas-0.16.2)\r\ntry:\r\n    utc = gettz(\'UTC\')\r\n    a = pd.DataFrame({\'created\' : [datetime(2015,10,10, tzinfo=utc), \r\n                                   datetime(2015,10,20, tzinfo=utc)], \r\n                      \'count\' : [1,2]})\r\n    b = pd.DataFrame({\'created\' : [datetime(2015,10,10, tzinfo=utc), \r\n                                   datetime(2015,10,20, tzinfo=utc)], \r\n                      \'count\' : [1,2]})\r\n    pd.merge(a, b, how=\'outer\')\r\nexcept Exception as e:\r\n    print ""Yeah, doesn\'t work: %s"" % e   \r\n    _, _, tb = sys.exc_info()\r\n    stack = lambda n : tbm.extract_tb(tb, 99)[n][0:]\r\n    print ""called from"", stack(0)\r\n    print ""failing statement"", stack(-1)\r\n```\r\n=>\r\n\r\n```\r\nYeah, doesn\'t work: type object argument after * must be a sequence, not itertools.imap\r\ncalled from (\'<ipython-input-194-3c3669b26a55>\', 23, \'<module>\', u""pd.merge(a, b, how=\'outer\')"")\r\nfailing statement (\'/.../local/lib/python2.7/site-packages/pandas/tools/merge.py\', 516, \'_get_join_indexers\', \'llab, rlab, shape = map(list, zip( * map(fkeys, left_keys, right_keys)))\')\r\n```\r\n\r\nthe culprit seems to be in the call to  [`_factorize_keys`](https://github.com/pydata/pandas/blob/master/pandas/tools/merge.py#L513)  though I couldn\'t quite figure out what goes wrong.\r\n\r\nVersion info \r\n\r\n```\r\n$ python --version\r\nPython 2.7.6\r\n$ pip freeze | grep pandas\r\npandas==0.17.0\r\n```'"
11403,112684964,evanpw,jreback,2015-10-21 21:39:13,2015-10-24 00:47:51,2015-10-24 00:18:23,closed,,0.17.1,26,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11403,b'BUG: drop_duplicates drops non-duplicate rows in the presence of integer columns',b'Fixes GH #11376'
11402,112658581,cfperez,jreback,2015-10-21 19:10:40,2015-10-27 11:12:01,2015-10-27 11:12:01,closed,,0.17.1,4,Bug;Compat;Difficulty Novice;Effort Low,https://api.github.com/repos/pydata/pandas/issues/11402,b'Series.sort_index(inplace=True) always returns a new Series',"b""Series.index_sort() _always_ returns a new Series, ignoring the `inplace` keyword argument. This is in contrast to DataFrame's sort_index() which works as expected.\r\n\r\n```python\r\nvals = range(10, 0, -1)\r\nx = pd.Series(vals)\r\n\r\n# Returns a new Series with the sorted index instead of\r\n# changing x in place\r\ny = x.sort_index(ascending=False, inplace=True)\r\n\r\n# Not the same\r\n(x != y).all()\r\n\r\n# sort_index() works as expected for DataFrames\r\nxx = pd.DataFrame({'col': x})\r\nassert xx.sort_index(ascending=False, inplace=True) is None\r\n```\r\n\r\nshow_versions() output:\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 7.1.2\r\nsetuptools: 18.4\r\nCython: None\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 4.0.0\r\nsphinx: None\r\npatsy: 0.4.0\r\ndateutil: 2.4.1\r\npytz: 2015.6\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: 2.2.0-b1\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.3\r\npymysql: 0.6.6.None\r\npsycopg2: None"""
11400,112537936,rekcahpassyla,jreback,2015-10-21 08:40:16,2015-10-27 13:25:20,2015-10-27 11:17:08,closed,,0.17.1,6,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/11400,b'BUG: using .ix with a multi-index indexer',b'closes #11372 \r\n\r\nAdd an optional argument to _NDFrameIndexer to indicate\r\nif the indexer is from a MultiIndex'
11385,112405320,tdhock,sinhrks,2015-10-20 16:15:02,2016-04-10 01:46:47,2016-04-10 01:46:47,closed,,0.18.0,3,Bug;Difficulty Novice;Effort Low;Strings,https://api.github.com/repos/pydata/pandas/issues/11385,"b'str.extract raises ValueError with group named ""name""'","b'For a Series `S`, I find the `S.str.extract` method very useful. It is great how you implemented naming the resulting DataFrame columns according to the names specified in the capturing groups of the regular expression.\r\n\r\nHowever there seems to be a bug when there is a capture group named ""name"" for example\r\n\r\n```python\r\n>>> import re\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> data = {\r\n...     \'Dave\': \'dave@google.com\',\r\n...     \'multiple\': \'rob@gmail.com some text steve@gmail.com\',\r\n...     \'none\': np.nan,\r\n...     }\r\n>>> pattern = r\'\'\'\r\n... (?P<name>[a-z]+)\r\n... @\r\n... (?P<domain>[a-z]+)\r\n... \\.\r\n... (?P<tld>[a-z]{2,4})\r\n... \'\'\'\r\n>>> S = pd.Series(data)\r\n>>> result = S.str.extract(pattern, re.VERBOSE)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/core/strings.py"", line 1370, in extract\r\n    return self._wrap_result(result, name=name)\r\n  File ""pandas/core/strings.py"", line 1088, in _wrap_result\r\n    name = kwargs.get(\'name\') or getattr(result, \'name\', None) or self.series.name\r\n  File ""pandas/core/generic.py"", line 730, in __nonzero__\r\n    .format(self.__class__.__name__))\r\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n>>> from pandas.util.print_versions import show_versions\r\n>>> show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 5d953e3fba420b6721c7f1c5d53e5812fe113bbc\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.8.0-44-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.17.0+73.g5d953e3\r\nnose: 1.1.2\r\npip: None\r\nsetuptools: 0.6\r\nCython: 0.20.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 0.12.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 1.5\r\npytz: 2015.6\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.7.2\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n>>>\r\n```\r\n\r\nThe result I expected was\r\n\r\n```python\r\n>>> exp_list = [\r\n...     (""dave"", ""google"", ""com""),\r\n...     (""rob"", ""gmail"", ""com""),\r\n...     (np.nan, np.nan, np.nan),\r\n...     ]\r\n>>> exp = pd.DataFrame(\r\n...     exp_list,\r\n...     [""Dave"", ""multiple"", ""none""],\r\n...     [""name"", ""domain"", ""tld""])\r\n>>> print exp\r\n          name  domain  tld\r\nDave      dave  google  com\r\nmultiple   rob   gmail  com\r\nnone       NaN     NaN  NaN\r\n>>>\r\n```'"
11376,112246291,RPGillespie6,jreback,2015-10-19 22:31:00,2015-10-25 05:32:36,2015-10-25 05:32:36,closed,,0.17.1,12,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11376,b'drop_duplicates destroys non-duplicated data under 0.17',"b'The `drop_duplicates()` function in Python 3 is broken. Take the following example snippet:\r\n\r\n```\r\nimport pandas as pd\r\n\r\nraw_data = {\'x\': [7,6,3,3,4,8,0],\'y\': [0,6,5,5,9,1,2]}\r\ndf = pd.DataFrame(raw_data, columns = [\'x\', \'y\'])\r\n\r\nprint(""Before:"", df)\r\ndf = df.drop_duplicates()\r\nprint(""After:"", df)\r\n```\r\n\r\nWhen run under python 2, the results are correct, but when running under python 3, pandas removes `6,6` from the frame, which is a completely unique row. When using this function with large CSV files, it causes thousands of lines of unique data loss.\r\n\r\nSee:\r\nhttp://stackoverflow.com/questions/33224356/why-is-pandas-dropping-unique-rows'"
11372,112162699,rekcahpassyla,jreback,2015-10-19 14:57:26,2015-10-27 11:17:08,2015-10-27 11:17:08,closed,,0.17.1,6,Bug;Difficulty Intermediate;Effort Medium;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/11372,"b'DataFrame.ix[idx, :] = value sets wrong values when idx is a MultiIndex and DataFrame.columns is also a MultiIndex'","b'This code is broken in `0.17.0` but not in `0.15.2`:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nnp.random.seed(1)\r\n\r\nfrom itertools import product\r\n\r\nfrom pandas.util.testing import assert_frame_equal\r\n\r\npd.show_versions()\r\n\r\nidx = pd.MultiIndex.from_tuples(\r\n    list(\r\n        product([\'A\', \'B\', \'C\'], \r\n                pd.date_range(\'2015-01-01\', \'2015-04-01\', freq=\'MS\'))\r\n    )\r\n)\r\n\r\nsub = pd.MultiIndex.from_tuples(\r\n    [(\'A\', pd.Timestamp(\'2015-01-01\')), (\'A\', pd.Timestamp(\'2015-02-01\'))]\r\n)\r\n# if cols = [\'foo\', \'bar\', \'baz\', \'quux\'], there is no error. \r\ncols = pd.MultiIndex.from_tuples(\r\n    list(\r\n        product([\'foo\', \'bar\'], \r\n                pd.date_range(\'2015-01-01\', \'2015-02-01\', freq=\'MS\'))\r\n    )\r\n)\r\n\r\ntest = pd.DataFrame(np.random.random((12, 4)), index=idx, columns=cols)\r\nvals = pd.DataFrame(np.random.random((2, 4)), index=sub, columns=cols)\r\ntest.ix[sub, :] = vals\r\n\r\nprint test.ix[sub, :]\r\nprint vals\r\n\r\nassert_frame_equal(test.ix[sub, :], vals)\r\n```\r\n\r\n### 0.17.0\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 7.1.0\r\nsetuptools: 18.0.1\r\nCython: 0.22\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.1\r\npytz: 2015.4\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: 0.7.3\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.7\r\npymysql: None\r\npsycopg2: None\r\n                    foo                   bar\r\n             2015-01-01 2015-02-01 2015-01-01 2015-02-01\r\nA 2015-01-01   0.287775   0.130029   0.019367   0.678836\r\n  2015-02-01   0.287775   0.130029   0.019367   0.678836\r\n                    foo                   bar\r\n             2015-01-01 2015-02-01 2015-01-01 2015-02-01\r\nA 2015-01-01   0.287775   0.130029   0.019367   0.678836\r\n  2015-02-01   0.211628   0.265547   0.491573   0.053363\r\nTraceback (most recent call last):\r\n  File ""c:\\dev\\code\\sandbox\\multiindex.py"", line 41, in <module>\r\n    assert_frame_equal(test.ix[sub, :], vals)\r\n  File ""c:\\python\\envs\\pd017\\lib\\site-packages\\pandas\\util\\testing.py"", line 1028, in assert_frame_equal\r\n    obj=\'DataFrame.iloc[:, {0}]\'.format(i))\r\n  File ""c:\\python\\envs\\pd017\\lib\\site-packages\\pandas\\util\\testing.py"", line 925, in assert_series_equal\r\n    check_less_precise, obj=\'{0}\'.format(obj))\r\n  File ""pandas\\src\\testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:3809)\r\n  File ""pandas\\src\\testing.pyx"", line 147, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:2685)\r\n  File ""c:\\python\\envs\\pd017\\lib\\site-packages\\pandas\\util\\testing.py"", line 798, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (50.0 %)\r\n[left]:  [0.287775338586, 0.287775338586]\r\n[right]: [0.287775338586, 0.211628116]\r\n```\r\n\r\n### 0.15.2\r\n```python\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB\r\n\r\npandas: 0.15.2\r\nnose: 1.3.7\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: 3.2.1\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.1\r\npytz: 2015.4\r\nbottleneck: 1.0.0\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.4\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.7.3\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 1.0.7\r\npymysql: None\r\npsycopg2: None\r\n                    foo                   bar           \r\n             2015-01-01 2015-02-01 2015-01-01 2015-02-01\r\nA 2015-01-01   0.287775   0.130029   0.019367   0.678836\r\n  2015-02-01   0.211628   0.265547   0.491573   0.053363\r\n                    foo                   bar           \r\n             2015-01-01 2015-02-01 2015-01-01 2015-02-01\r\nA 2015-01-01   0.287775   0.130029   0.019367   0.678836\r\n  2015-02-01   0.211628   0.265547   0.491573   0.053363\r\n```'"
11371,112127114,jreback,jreback,2015-10-19 11:39:15,2015-10-20 20:47:02,2015-10-20 17:29:50,closed,,0.17.1,10,API Design;Bug;Categorical;Enhancement;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11371,"b'BUG: pivot table bug with Categorical indexes, #10993'","b""closes #10993 \r\nreplaces #10989 \r\n\r\nSo issue #10993 involves the insertion of a key into a multi-index that has as one of its levels a ``CategoricalIndex``. This causes the semantics to break down because we are inserting an new key.\r\n\r\nExisting\r\n```\r\nIn [16]: df = DataFrame({'A' : [1,2], 'B' : [3,4] })\r\n\r\nIn [17]: df.columns = MultiIndex([pd.CategoricalIndex(list('ab')),[1,2]],[[0,1],[0,1]])\r\n\r\nIn [3]: df.columns.levels[0]\r\nOut[3]: CategoricalIndex([u'a', u'b'], categories=[u'a', u'b'], ordered=False, dtype='category')\r\n\r\nIn [18]: df\r\nOut[18]: \r\n   a  b\r\n   1  2\r\n0  1  3\r\n1  2  4\r\n\r\nIn [19]: df.columns\r\nOut[19]: \r\nMultiIndex(levels=[[u'a', u'b'], [1, 2]],\r\n           labels=[[0, 1], [0, 1]])\r\n\r\nIn [20]: df[('c',3)] = 5\r\nTypeError: cannot insert an item into a CategoricalIndex that is not already an existing category\r\n```\r\n\r\nNew\r\n```\r\nIn [4]: df[('c',3)] = 5\r\n\r\nIn [5]: df\r\nOut[5]: \r\n   a  b  c\r\n   1  2  3\r\n0  1  3  5\r\n1  2  4  5\r\n\r\nIn [8]: df.columns.levels[0]\r\nOut[8]: CategoricalIndex([u'a', u'b', u'c'], categories=[u'a', u'b', u'c'], ordered=False, dtype='category')\r\n```\r\n\r\nThe only issue that was slightly controversial is that ``.insert`` will retain the ``ordered`` attribute (and new categories go to the end). while ``.append`` will always have ``ordered=False``. In theory we could do the same, but ``.append`` is used to generally append *another* ``CategoricalIndex``, so you would have some possiblity of interleaving of the 'ordered' categories (IOW, if self has ``[1,2,3]`` and other has ``[3,2,1,4]``, then the result will be ``[1,2,3,4]``.\r\n\r\nWe *could* raise if we have mixed ordering (e.g. self is ``ordered=False``, other is ``ordered=True``).\r\n\r\nOf course the user is free to reorder and such, but the default should be intuitive.\r\n\r\nFuther note that we can now concat pandas objects with ``CategoricalIndexes`` (I don't think was specified before, certainly not tested), e.g.\r\n\r\n```\r\nIn [1]: df = DataFrame({'A' : np.arange(5)},index=pd.CategoricalIndex(list('aabbc')))\r\n\r\nIn [2]: df2 = DataFrame({'A' : np.arange(5)},index=pd.CategoricalIndex(list('bbcde')))\r\n\r\nIn [3]: pd.concat([df,df2])\r\nOut[3]: \r\n   A\r\na  0\r\na  1\r\nb  2\r\nb  3\r\nc  4\r\nb  0\r\nb  1\r\nc  2\r\nd  3\r\ne  4\r\n\r\nIn [4]: pd.concat([df,df2]).index\r\nOut[4]: CategoricalIndex([u'a', u'a', u'b', u'b', u'c', u'b', u'b', u'c', u'd', u'e'], categories=[u'a', u'b', u'c', u'd', u'e'], ordered=False, dtype='category')\r\n\r\n```\r\n\r\n"""
11370,112124264,rekcahpassyla,jreback,2015-10-19 11:16:27,2015-12-13 20:42:04,2015-12-13 20:42:04,closed,,0.18.0,4,Bug;Difficulty Intermediate;Effort Low;Frequency,https://api.github.com/repos/pydata/pandas/issues/11370,b'Vectorised addition of MonthOffset(n=0) returns different values to item-by-item addition',"b'This code returns different values in `0.17.0` and `0.15.2`\r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas.util.testing import assert_index_equal\r\n\r\npd.show_versions()\r\n\r\noffsets = [\r\n    pd.offsets.Day, pd.offsets.MonthBegin,\r\n    pd.offsets.QuarterBegin, pd.offsets.YearBegin,\r\n]\r\n\r\ndates = pd.date_range(\'2011-01-01\', \'2011-01-05\', freq=\'D\')\r\n\r\nfor offset in offsets:\r\n    # adding each item individually or vectorised should give same answer\r\n    expected_vec = dates + offset(n=0)\r\n    expected = pd.DatetimeIndex([d + offset(n=0) for d in dates])\r\n    \r\n    msg = ""offset: {}, vectorised: {}, individual: {}"".format(\r\n        offset, expected_vec, expected\r\n    )\r\n    try:\r\n        if pd.__version__ == \'0.17.0\':\r\n            assert_index_equal(expected_vec, expected, check_names=False)\r\n        else:\r\n            assert_index_equal(expected_vec, expected)\r\n    except AssertionError as er:\r\n        raise Exception(msg + str(er))\r\n```\r\n\r\n### 0.17.0\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.17.0\r\nnose: 1.3.7\r\npip: 7.1.0\r\nsetuptools: 18.0.1\r\nCython: 0.22\r\nnumpy: 1.10.1\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.1\r\nsphinx: 1.3.1\r\npatsy: 0.4.0\r\ndateutil: 2.4.1\r\npytz: 2015.4\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: 0.9.4\r\nxlwt: None\r\nxlsxwriter: 0.7.3\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.7\r\npymysql: None\r\npsycopg2: None\r\nTraceback (most recent call last):\r\n  File ""c:\\dev\\code\\sandbox\\pandas_17_vs_15_dateoffsets.py"", line 24, in <module>\r\n    raise Exception(msg + str(er))\r\nException: offset: <class \'pandas.tseries.offsets.MonthBegin\'>, vectorised: DatetimeIndex([\'2010-12-01\', \'2011-01-01\', \'2011-01-01\', \'2011-01-01\',\r\n               \'2011-01-01\'],\r\n              dtype=\'datetime64[ns]\', freq=None), individual: DatetimeIndex([\'2011-01-01\', \'2011-02-01\', \'2011-02-01\', \'2011-02-01\',\r\n               \'2011-02-01\'],\r\n              dtype=\'datetime64[ns]\', freq=None)Index are different\r\n\r\nIndex values are different (100.0 %)\r\n[left]:  DatetimeIndex([\'2010-12-01\', \'2011-01-01\', \'2011-01-01\', \'2011-01-01\',\r\n               \'2011-01-01\'],\r\n              dtype=\'datetime64[ns]\', freq=None)\r\n[right]: DatetimeIndex([\'2011-01-01\', \'2011-02-01\', \'2011-02-01\', \'2011-02-01\',\r\n               \'2011-02-01\'],\r\n              dtype=\'datetime64[ns]\', freq=None)\r\n```\r\n\r\n### 0.15.2\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB\r\n\r\npandas: 0.15.2\r\nnose: 1.3.7\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: 3.2.1\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.1\r\npytz: 2015.4\r\nbottleneck: 1.0.0\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.4\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.7.3\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 1.0.7\r\npymysql: None\r\npsycopg2: None\r\n```\r\n'"
11368,112080497,DSLituiev,jreback,2015-10-19 06:23:48,2015-11-18 23:56:16,2015-11-18 20:16:52,closed,,,8,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/11368,"b'interpreting index name in min_itemsize specification #11364, #10381'","b""closes #10381 \r\n\r\nThe fix replaces named index columns in [`Table.index_axes` with their names](https://github.com/pydata/pandas/compare/master...DSLituiev:master#diff-1b15d1477da3a0548d2dd72a5d023d00L3323)  and [corrects `min_itemsize` if it contains 'index' item which is not in `Table.index_axes`](\r\nhttps://github.com/pydata/pandas/compare/master...DSLituiev:master#diff-1b15d1477da3a0548d2dd72a5d023d00R3093)\r\n"""
11366,112076195,kawochen,jreback,2015-10-19 05:40:18,2015-10-23 20:41:17,2015-10-23 20:41:12,closed,,0.17.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/11366,"b""BUG: GH11235 where pd.eval doesn't handle unary ops in lists""",b'closes #11235 '
11364,112041917,DSLituiev,jreback,2015-10-18 21:20:58,2015-10-19 05:50:27,2015-10-18 23:20:02,closed,,,4,Bug;Duplicate;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/11364,b'inconisitent behaviour of hdf5 store.append for MultiIndex vs simple Index',"b'The following example works with MultiIndex, but fails with simple index, which is bad in terms of generalization / intuitive behaviour:\r\n```\r\n#Create example:\r\ncol_nums = [0]\r\ndf = pd.DataFrame({""V1"":[""a"",""b"",""c"",""d"",""e"", ""aaaah!!!""], \r\n                          ""W"":[""c"",""d"",""c"",""d"",""c"",""c""],\r\n                          ""data"":np.arange(6)})\r\ndf.set_index([""V1"",""W""], inplace = True)\r\ndf.to_csv(""testtable.tab"",sep = ""\\t"")\r\n```\r\n\r\n\r\n\r\n```\r\n# MulitIndex -> works:\r\nsep = ""\\t""\r\nindexcols =[0,1]\r\nchunksize=5\r\n\r\nxbed = ""testtable.tab""\r\n%rm \'tempstore.h5\'\r\n# create a store\r\nwith pd.HDFStore(\'tempstore.h5\') as store:\r\n    for nn, chunk in enumerate(pd.read_table(xbed, chunksize=chunksize, sep = sep, index_col= indexcols)):\r\n        group = ""x""\r\n        print(chunk.index.names)\r\n        store.append(group, chunk, format = ""table"", \r\n                     min_itemsize=dict(zip(chunk.index.names, [32]*len(chunk.index.names))))\r\n        print(""chunk #"" , nn, file = sys.stderr)\r\n```\r\n\r\n```\r\n# simple Index -> fails:\r\nsep = ""\\t""\r\nindexcols =[0]    # <==== this is the only difference\r\nchunksize=5\r\n\r\nxbed = ""testtable.tab""\r\n%rm \'tempstore.h5\'\r\n# create a store\r\nwith pd.HDFStore(\'tempstore.h5\') as store:\r\n    for nn, chunk in enumerate(pd.read_table(xbed, chunksize=chunksize, sep = sep, index_col= indexcols)):\r\n        group = ""x""\r\n        print(chunk.index.names)\r\n        store.append(group, chunk, format = ""table"", \r\n                     min_itemsize=dict(zip(chunk.index.names, [32]*len(chunk.index.names))))\r\n        print(""chunk #"" , nn, file = sys.stderr)\r\n```\r\n` -> ...  ValueError: min_itemsize has the key [V1] which is not an axis or data_column`'"
11362,112007885,flying-sheep,jreback,2015-10-18 08:36:40,2015-10-18 16:03:53,2015-10-18 16:02:57,closed,,0.17.1,6,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/11362,b'fixed pathlib tests on windows',b'as requested here: https://github.com/pydata/pandas/issues/11033#issuecomment-148924778\r\n\r\ni can\xa1\xaft test on windows unfortunately.'
11349,111929816,amelio-vazquez-reina,jreback,2015-10-16 23:35:39,2015-12-31 13:46:31,2015-12-31 13:46:31,closed,,0.18.0,2,Bug;Reshaping;Timedelta,https://api.github.com/repos/pydata/pandas/issues/11349,b'BUG: boxing Timedeltas on .apply',"b""Consider the following Series:\r\n\r\n```\r\nobject_id\r\n0CKVYKjyFn    76 days\r\n0CrPL2QKH3   -15 days\r\n0CrVStlVrg    23 days\r\n0Cc5ZvS67u    76 days\r\n0CTOk5OdtI    76 days\r\n0CTSWtTzBa    76 days\r\n0CwBqVeNCX    76 days\r\n0CIRJFIOcD    58 days\r\n0CRQPCxzQe   350 days\r\n0CAq4m9Nru    15 days\r\n0C617yvXBj    76 days\r\n0CzUUJNKX9   -16 days\r\nName: days_left, dtype: timedelta64[ns]\r\n```\r\n\r\nI am hoping to convert the above to hours. \r\n\r\nIf I do:\r\n\r\n```\r\nmy_series.dt.hours\r\n```\r\n\r\nI get:\r\n```\r\nAttributeError: 'Series' object has no attribute 'hours\r\n```\r\n\r\nWhat's even more strange is that if I do:\r\n``` python\r\n> my_series[0].total_seconds()/3600\r\n1824.0\r\n```\r\nit works for one element, but if I do:\r\n```\r\n> my_series.apply(lambda x: x.total_seconds())\r\n```\r\nI get:\r\n```\r\nAttributeError: 'numpy.timedelta64' object has no attribute 'total_seconds'\r\n```\r\n\r\nI thought `apply` would run the function I pass it item by item in the series. Why does `total_seconds()` work for a single item, but not with `apply`?"""
11345,111860745,jreback,jreback,2015-10-16 16:01:39,2015-10-16 21:51:18,2015-10-16 21:51:18,closed,,0.17.1,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11345,"b'BUG: bug in comparisons vs tuples, #11339'",b'closes #11339 '
11339,111728942,svidela,jreback,2015-10-15 23:38:35,2016-07-16 22:40:39,2015-10-16 21:51:18,closed,,0.17.1,6,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11339,b'Boolean comparison vs tuple fails in 0.17.0',"b""Just updated to 0.17.0 (python 2.7.9 on OS X Yosemite) and I found the following:\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\nu'0.17.0'\r\n>>> s = pd.Series([(1,1),(1,2)])\r\n>>> s==(1,2)\r\n0    False\r\n1    False\r\ndtype: bool\r\n```\r\n\r\nMeanwhile in 0.16.2:\r\n```\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n'0.16.2'\r\n>>> s = pd.Series([(1,1),(1,2)])\r\n>>> s==(1,2)\r\n0    False\r\n1     True\r\ndtype: bool\r\n```"""
11331,111520449,chris-b1,jreback,2015-10-15 00:22:32,2015-12-30 15:21:05,2015-12-30 15:21:00,closed,,,7,Bug;Data IO;Dtypes;Duplicate;IO Excel,https://api.github.com/repos/pydata/pandas/issues/11331,"b""BUG: read_excel doesn't respect string data""","b""From [SO](http://stackoverflow.com/questions/33137686/python-loading-zip-codes-into-a-dataframe-as-strings/33137797#33137797)\r\n\r\n```python\r\nIn [16]: df = pd.DataFrame({'a': ['001','002']})\r\n\r\nIn [17]: df.to_excel('temp.xlsx')\r\n\r\nIn [18]: pd.read_excel('temp.xlsx')\r\nOut[18]: \r\n   a\r\n0  1\r\n1  2\r\n```\r\n\r\n\r\nI think it would probably make sense for `read_excel` to not try and convert strings to numeric, or at least have another keyword argument."""
11329,111473547,jreback,jreback,2015-10-14 19:26:29,2015-10-17 15:04:49,2015-10-17 15:04:49,closed,,0.17.1,1,Bug;Dtypes;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11329,"b'BUG: Bug in DataFrame.replace with a datetime64[ns, tz] and a non-compat to_replace #11326'",b'closes #11326\r\nxref #11153 '
11328,111434155,Dr-Irv,jreback,2015-10-14 16:15:06,2015-10-26 14:00:34,2015-10-25 14:02:58,closed,,0.17.1,31,Bug;IO CSV;IO Excel;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/11328,b'Fix for BUG: multi-index excel header fails if all numeric',"b'replaces #11317\r\ncloses #11408\r\n\r\nThis includes updates to 3 Excel files, plus a test in test_excel.py,\r\nplus the fix in parsers.py'"
11327,111433893,ethanluoyc,jreback,2015-10-14 16:13:51,2015-10-17 03:33:38,2015-10-16 16:40:27,closed,,0.17.1,8,Bug;Dtypes;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11327,b'BUG: Fix to_dict() problem when using datetime DataFrame #11247',"b""closes #11247\r\n \r\nThis is my first time contributing through PR so forgive me if I make any silly mistake.\r\nThe issue can be referred to in GH11247\r\nI followed @jreback 's suggestion to box things up in `to_dict()` but I am not sure if this is what he means because I do not know exactly when `_maybe_datetime()` should be used. Initially I tried to tweak `internals.py` but it fails.\r\n\r\nLet me know how I can improve from here. Hope I can really learn from my first PR."""
11326,111418126,joshowen,jreback,2015-10-14 15:03:33,2015-10-17 15:04:49,2015-10-17 15:04:49,closed,,0.17.1,2,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11326,b'DataFrame with TimeSeries column raises TypeError in 0.17.0 when replacing ints or floats',"b'Example:\r\n```\r\n>>> df = extended_df\r\nTraceback (most recent call last):\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File ""<ipython-input-14-3f216cf6821e>"", line 1, in <module>\r\n    df = extended_df\r\nNameError: name \'extended_df\' is not defined\r\n>>> df = expanded\r\n>>> df\r\nOut[16]: \r\n                             pricing_date  exchange_id  close\r\npricing_date_ts                                              \r\n1445025600      2015-10-13 20:00:00+00:00          NaN    NaN\r\n1444939200      2015-10-13 20:00:00+00:00          NaN    NaN\r\n1444852800      2015-10-13 20:00:00+00:00            0       NaN\r\n1444766400      2015-10-13 20:00:00+00:00            6  0.545\r\n1444680000      2015-10-12 20:00:00+00:00            6  0.570\r\n1444420800      2015-10-09 20:00:00+00:00            6  0.580\r\n1444334400      2015-10-08 20:00:00+00:00            6  0.560\r\n1444248000      2015-10-07 20:00:00+00:00            6  0.580\r\n1444161600      2015-10-06 20:00:00+00:00            6  0.620\r\n1444075200      2015-10-05 20:00:00+00:00            6  0.480\r\n1443816000      2015-10-13 20:00:00+00:00          NaN    NaN\r\n>>> df.dtypes\r\nOut[17]: \r\npricing_date    datetime64[ns, UTC]\r\nexchange_id                 float64\r\nclose                       float64\r\ndtype: object\r\n>>> df.replace(0, np.NaN)\r\nTraceback (most recent call last):\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File ""<ipython-input-18-cc26b9ec3ff7>"", line 1, in <module>\r\n    df.replace(0, np.NaN)\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/core/generic.py"", line 2996, in replace\r\n    inplace=inplace, regex=regex)\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/core/internals.py"", line 2761, in replace\r\n    return self.apply(\'replace\', **kwargs)\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/core/internals.py"", line 2710, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/core/internals.py"", line 560, in replace\r\n    mask = com.mask_missing(self.values, to_replace)\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/core/common.py"", line 449, in mask_missing\r\n    mask = arr == x\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/tseries/index.py"", line 84, in wrapper\r\n    other = _ensure_datetime64(other)\r\n  File ""/Users/josh/anaconda/envs/Openfolio/lib/python2.7/site-packages/pandas/tseries/index.py"", line 111, in _ensure_datetime64\r\n    raise TypeError(\'%s type object %s\' % (type(other), str(other)))\r\nTypeError: <type \'int\'> type object 0\r\n```'"
11324,111382182,hadjmic,jreback,2015-10-14 11:52:40,2015-11-13 21:48:37,2015-11-13 21:48:37,closed,,0.17.1,7,Bug;Difficulty Novice;Effort Low;Groupby;Prio-low,https://api.github.com/repos/pydata/pandas/issues/11324,b'groupby.apply datetime bug affecting 0.17',"b""Exception is raised when\r\na) the original dataframe has a datetime column\r\nb) the groupby.apply function returns a series object with a new datetime column\r\n\r\nCode to reproduce:\r\n```\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndf = pd.DataFrame([['1', datetime.datetime.today()], \r\n                   ['2', datetime.datetime.today()],\r\n                   ['2', datetime.datetime(2010, 1, 1)]],\r\n                   columns=['record', 'date'])\r\ndd = df.groupby('record').apply(lambda x: pd.Series({'max_date': x['date'].max()}))\r\n```\r\n\r\nThis is a new issue affecting 0.17\r\n"""
11322,111373902,jreback,jreback,2015-10-14 10:55:15,2015-10-14 11:43:50,2015-10-14 11:43:50,closed,,0.17.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/11322,"b'BUG: Bug in list-like indexing with a mixed-integer Index, #11320'",b'closes #11320 '
11320,111301598,DSLituiev,jreback,2015-10-14 01:01:44,2015-10-14 11:43:50,2015-10-14 11:43:50,closed,,0.17.1,6,Bug;Difficulty Intermediate;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/11320,b'Negative interger indices lead to  IndexError: indices are out-of-bounds ',"b'The following test case produces `IndexError: indices are out-of-bounds` with negative integers, while positive work fine:\r\n\r\n```\r\ntestdf = pd.DataFrame({""rna"": (1.5,2.2,3.2,4.5),  -1000: [11,21,36,40],0: [10,22,43,34], 1000:[0, 10, 20, 30]})\r\ntestdf[[1000]] #this works\r\ntestdf[[-1000]] #this does not\r\n```'"
11302,111021364,marcomayer,marcomayer,2015-10-12 17:36:27,2015-10-13 13:53:26,2015-10-13 13:53:26,closed,,0.17.1,18,Bug;Numeric;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11302,b'Different precision calling .astype(str) on float numbers',"b""With pandas 0.16.2:\r\n\r\n>>> import pandas as pd\r\n>>> pd.DataFrame([1.12345678901234567890]).astype(str)\r\n               0\r\n0  1.12345678901\r\n\r\n\r\nWith pandas 0.17:\r\n\r\n>>> import pandas as pd\r\n>>> pd.DataFrame([1.12345678901234567890]).astype(str)\r\n                    0\r\n0  1.1234567890123457\r\n\r\nI read the 0.17 release log but couldn't figure out why that is. Is it a bug or a new feature, and if it's a new feature how can I re-activate the old behavior?"""
11301,111002906,jreback,jreback,2015-10-12 15:45:45,2015-10-13 12:25:28,2015-10-13 12:25:28,closed,,0.17.1,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/11301,"b'BUG: Bug in tz-conversions with an ambiguous time and .dt accessors, #11295'",b'closes #11295 '
11300,110987537,henrystokeley,jreback,2015-10-12 14:40:57,2015-11-05 12:57:18,2015-11-05 12:57:18,closed,,,6,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/11300,b'BUG: GH10355 groupby std() no longer sqrts grouping cols',b'closes #10355 \r\n\r\nPreviously grouping columns were square rooted when as_index=False\r\nNew method closely follows the format of var() method.'
11295,110956168,witosx,jreback,2015-10-12 11:30:23,2015-10-13 12:25:28,2015-10-13 12:25:28,closed,,0.17.1,2,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/11295,b'datetime like series .dt properties fail after ts.dt.tz_localize(tz1).dt.tz_convert(tz2) sequence',"b'Second tz_convert fails with AmbiguousTimeError (\'UTC\' in the example, but other timezones fail too).\r\n\r\nEDIT: Actually it\'s not only second tz_convert that fails, it looks like most (if not all) of the .dt accessed properties are affected\r\n\r\n```python\r\nt = pd.Series(pd.date_range(\'2015-01-01\', \'2016-01-01\', freq=\'T\'))\r\nt = t.dt.tz_localize(\'UTC\').dt.tz_convert(\'America/Chicago\')\r\nt.head(10)\r\n```\r\n\r\n\r\n\r\n\r\n    0   2014-12-31 18:00:00-06:00\r\n    1   2014-12-31 18:01:00-06:00\r\n    2   2014-12-31 18:02:00-06:00\r\n    3   2014-12-31 18:03:00-06:00\r\n    4   2014-12-31 18:04:00-06:00\r\n    5   2014-12-31 18:05:00-06:00\r\n    6   2014-12-31 18:06:00-06:00\r\n    7   2014-12-31 18:07:00-06:00\r\n    8   2014-12-31 18:08:00-06:00\r\n    9   2014-12-31 18:09:00-06:00\r\n    dtype: datetime64[ns, America/Chicago]\r\n\r\n\r\n\r\n\r\n```python\r\nt.dt.tz_convert(\'UTC\')\r\n```\r\n\r\n\r\n    ---------------------------------------------------------------------------\r\n\r\n    AmbiguousTimeError                        Traceback (most recent call last)\r\n\r\n    /usr/lib/python3.5/site-packages/pandas/core/series.py in _make_dt_accessor(self)\r\n       2639         try:\r\n    -> 2640             return maybe_to_datetimelike(self)\r\n       2641         except Exception:\r\n\r\n\r\n    /usr/lib/python3.5/site-packages/pandas/tseries/common.py in maybe_to_datetimelike(data, copy)\r\n         48     if is_datetime64_dtype(data.dtype) or is_datetime64tz_dtype(data.dtype):\r\n    ---> 49         return DatetimeProperties(DatetimeIndex(data, copy=copy, freq=\'infer\'), index, name=data.name)\r\n         50     elif is_timedelta64_dtype(data.dtype):\r\n\r\n\r\n    /usr/lib/python3.5/site-packages/pandas/util/decorators.py in wrapper(*args, **kwargs)\r\n         88                     kwargs[new_arg_name] = new_arg_value\r\n    ---> 89             return func(*args, **kwargs)\r\n         90         return wrapper\r\n\r\n\r\n    /usr/lib/python3.5/site-packages/pandas/tseries/index.py in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, dtype, **kwargs)\r\n        343                     subarr = tslib.tz_localize_to_utc(ints, tz,\r\n    --> 344                                                       ambiguous=ambiguous)\r\n        345 \r\n\r\n\r\n    pandas/tslib.pyx in pandas.tslib.tz_localize_to_utc (pandas/tslib.c:64516)()\r\n\r\n\r\n    AmbiguousTimeError: Cannot infer dst time from Timestamp(\'2015-11-01 01:00:00\'), try using the \'ambiguous\' argument\r\n\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n\r\n\r\n    AttributeError                            Traceback (most recent call last)\r\n\r\n    <ipython-input-2-eb75730cd6fc> in <module>()\r\n    ----> 1 t.dt.tz_convert(\'UTC\')\r\n    \r\n\r\n    /usr/lib/python3.5/site-packages/pandas/core/generic.py in __getattr__(self, name)\r\n       2239                 or name in self._metadata\r\n       2240                 or name in self._accessors):\r\n    -> 2241             return object.__getattribute__(self, name)\r\n       2242         else:\r\n       2243             if name in self._info_axis:\r\n\r\n\r\n    /usr/lib/python3.5/site-packages/pandas/core/base.py in __get__(self, instance, owner)\r\n        186             # this ensures that Series.str.<method> is well defined\r\n        187             return self.accessor_cls\r\n    --> 188         return self.construct_accessor(instance)\r\n        189 \r\n        190     def __set__(self, instance, value):\r\n\r\n\r\n    /usr/lib/python3.5/site-packages/pandas/core/series.py in _make_dt_accessor(self)\r\n       2640             return maybe_to_datetimelike(self)\r\n       2641         except Exception:\r\n    -> 2642             raise AttributeError(""Can only use .dt accessor with datetimelike ""\r\n       2643                                  ""values"")\r\n       2644 \r\n\r\n\r\n    AttributeError: Can only use .dt accessor with datetimelike values\r\n\r\n\r\n\r\n```python\r\n\r\n```\r\n'"
11291,110902598,BrenBarn,jreback,2015-10-12 03:13:05,2015-10-13 11:49:29,2015-10-13 11:49:26,closed,,0.17.1,4,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/11291,b'Fix mistake in Pytables querying with numpy scalar value.  Fixes #11283',"b'Fixes #11283\r\n\r\nFirst PR to this project.  The fix is minuscule, but let me know if I did anything awry.'"
11283,110822073,BrenBarn,jreback,2015-10-10 23:11:39,2015-10-13 11:49:26,2015-10-13 11:49:26,closed,,0.17.1,3,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/11283,b'Mistake in BinOp.conform?',"b'in BinOp.conform in pandas.computation,pytables there is this code:\r\n\r\n````\r\n    def conform(self, rhs):\r\n        """""" inplace conform rhs """"""\r\n        if not com.is_list_like(rhs):\r\n            rhs = [rhs]\r\n        if hasattr(self.rhs, \'ravel\'):\r\n            rhs = rhs.ravel()\r\n        return rhs\r\n````\r\n\r\nI think this is wrong.  The first test checks `rhs`, but the second checks `self.rhs`.  They should both check one or the other.  I *think* both should check local `rhs` but I\'m not really familiar with all the nuts and bolts here so I\'m not sure.\r\n\r\nThis causes failures when doing HDF5 queries making use of a local variable whose value is a single Numpy value (e.g., a float64).  Because even a single numpy value has `ravel`, but it is not list-like, both `if` blocks execute, meaning that it sets `rhs` to a list and then tries to call `ravel` on it.  This leads to a difficult-to-debug error because the exception is caught at a higher level and replaced with an ""Invalid query syntax"" error message.'"
11259,110296765,jreback,jreback,2015-10-07 19:13:42,2015-10-07 21:05:33,2015-10-07 21:05:33,closed,,0.17.0,0,Bug;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/11259,"b'BUG/ERR: raise when trying to set a subset of values in a datetime64[ns, tz] columns with another tz'",
11247,109894948,depristo,jreback,2015-10-05 21:59:17,2015-10-16 16:40:40,2015-10-16 16:40:40,closed,,0.17.1,10,Bug;Difficulty Novice;Dtypes;Effort Low;Output-Formatting;Prio-low,https://api.github.com/repos/pydata/pandas/issues/11247,"b""DataFrame.to_dict(orient='records') datetime conversion inconsistency ""","b""My original code used:\r\n\r\n```\r\ndata = df.to_dict(orient='records')\r\n```\r\n\r\nbut I had to replace with a different to_dict conversion because to_dict(orient='records') doesn't properly convert numpy.datetime64 to Timestamps but rather returns numpy.datetime64 values which are hard to work with downstream.  If you include any other non-datetime fields in the dataframe the conversion to a Timestamp happens with 'records' mode, so I think this is some kind of edge-case where converters aren't being triggered when everything is a datetime.\r\n\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame({\r\n        'c1': pd.to_datetime(['1-1-2015', '1-2-2015', '1-3-2015']),\r\n        'c2': pd.to_datetime(['2-2-2015', '2-3-2015', '2-4-2015']),\r\n    })\r\nprint('raw dataframe')\r\nprint(df)\r\nprint('\\nconverted to records')\r\nprint(df.to_dict(orient='records'))\r\nprint('\\nconverted to dict')\r\nprint(df.to_dict(orient='dict'))\r\nprint('\\nconverted to list')\r\nprint(df.to_dict(orient='list'))\r\n```\r\n\r\nproduces\r\n\r\n```\r\nraw dataframe\r\n          c1         c2\r\n0 2015-01-01 2015-02-02\r\n1 2015-01-02 2015-02-03\r\n2 2015-01-03 2015-02-04\r\n\r\nconverted to records\r\n[{'c2': numpy.datetime64('2015-02-01T16:00:00.000000000-0800'), 'c1': numpy.datetime64('2014-12-31T16:00:00.000000000-0800')}, {'c2': numpy.datetime64('2015-02-02T16:00:00.000000000-0800'), 'c1': numpy.datetime64('2015-01-01T16:00:00.000000000-0800')}, {'c2': numpy.datetime64('2015-02-03T16:00:00.000000000-0800'), 'c1': numpy.datetime64('2015-01-02T16:00:00.000000000-0800')}]\r\n\r\nconverted to dict\r\n{'c2': {0: Timestamp('2015-02-02 00:00:00'), 1: Timestamp('2015-02-03 00:00:00'), 2: Timestamp('2015-02-04 00:00:00')}, 'c1': {0: Timestamp('2015-01-01 00:00:00'), 1: Timestamp('2015-01-02 00:00:00'), 2: Timestamp('2015-01-03 00:00:00')}}\r\n\r\nconverted to list\r\n{'c2': [Timestamp('2015-02-02 00:00:00'), Timestamp('2015-02-03 00:00:00'), Timestamp('2015-02-04 00:00:00')], 'c1': [Timestamp('2015-01-01 00:00:00'), Timestamp('2015-01-02 00:00:00'), Timestamp('2015-01-03 00:00:00')]}\r\n```"""
11245,109859561,llllllllll,jreback,2015-10-05 18:41:09,2015-10-11 15:17:56,2015-10-11 15:17:51,closed,,0.17.1,16,Bug;Compat;Dtypes;Numeric;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11245,b'BUG: datetime64 series reduces to nan when empty instead of nat',"b""I ran into some strange behavior with a series of dtype datetime64[ns] where I called max and got back a `nan`. I think the correct behavior here is to return `nat`. I looked through test_nanops but I am not sure where the correct place to put the test for this is.\r\n\r\nThe new behavior is:\r\n\r\n```python\r\nIn [1]: pd.Series(dtype='datetime64[ns]').max()\r\nOut[1]: NaT\r\n```\r\nwhere the old behavior was:\r\n```python\r\nIn [1]: pd.Series(dtype='datetime64[ns]').max()\r\nOut[1]: nan\r\n```"""
11240,109714512,TomAugspurger,jreback,2015-10-05 00:15:36,2015-10-09 13:33:02,2015-10-09 13:33:02,closed,,0.17.1,2,Bug;IO HDF5;Unicode,https://api.github.com/repos/pydata/pandas/issues/11240,b'BUG: HDFStore.append with encoded string itemsize',"b'Closes https://github.com/pydata/pandas/issues/11234\r\n\r\nFailure came when the maximum length of the unencoded string\r\nwas smaller than the maximum encoded length.\r\n\r\nNeed to run a perf check still. We end up having to call `_convert_string_array` twice, once before we know the min_itemsize, and a second time just before appending once we do know the min itemsize.'"
11237,109694935,chris-b1,jreback,2015-10-04 18:21:15,2015-10-11 23:36:11,2015-10-10 00:23:39,closed,,0.17.1,6,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/11237,b'BUG: to_excel duplicate columns',b'closes #11007 \r\ncloses #10970 (data in wrong order)\r\ncloses #10982 (columns lost).    \r\n\r\nUsing the approach suggested [here](https://github.com/pydata/pandas/issues/11007#issuecomment-141019207)\r\n\r\nAll three occurred when using `to_excel` with duplicate columns in the `DataFrame`\r\n\r\n\r\n'
11235,109686658,tobgu,jreback,2015-10-04 15:30:08,2015-10-23 20:41:12,2015-10-23 20:41:12,closed,,0.17.1,3,Bug;Difficulty Intermediate;Effort Low,https://api.github.com/repos/pydata/pandas/issues/11235,"b""Dataframe.eval(): Negative number in list passed to 'in'-expression causes crash on python 3.4.0""","b'The following crashes on python 3.4.0. It works fine on Python 2.7.5.\r\n\r\n```\r\n>>> import pandas\r\n>>> from io import StringIO\r\n>>> data = ""foo,bar\\n11,12""\r\n>>> df = pandas.read_csv(StringIO(data))\r\n>>> df.eval(\'foo in [11, 32]\')\r\n0    True\r\ndtype: bool\r\n>>> df.eval(\'foo in [11, -32]\')\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/core/frame.py"", line 1987, in eval\r\n    return _eval(expr, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/eval.py"", line 230, in eval\r\n    truediv=truediv)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 635, in __init__\r\n    self.terms = self.parse()\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 652, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 314, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 320, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 314, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 323, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 314, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 560, in visit_Compare\r\n    return self.visit(binop)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 314, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 404, in visit_BinOp\r\n    op, op_class, left, right = self._possibly_transform_eq_ne(node)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 357, in _possibly_transform_eq_ne\r\n    right = self.visit(node.right, side=\'right\')\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 314, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 430, in visit_List\r\n    name = self.env.add_tmp([self.visit(e).value for e in node.elts])\r\n  File ""/home/tobias/Envs/qcache-py3/lib/python3.4/site-packages/pandas/computation/expr.py"", line 430, in <listcomp>\r\n    name = self.env.add_tmp([self.visit(e).value for e in node.elts])\r\nAttributeError: \'UnaryOp\' object has no attribute \'value\'\r\n>>> \r\n````'"
11234,109686223,FilipDusek,jreback,2015-10-04 15:18:43,2015-10-09 13:33:13,2015-10-09 13:33:13,closed,,0.17.1,7,Bug;IO HDF5;Unicode,https://api.github.com/repos/pydata/pandas/issues/11234,b'HDFStore fails to read non-ascii characters',"b'When I try to save some non-ascii character like \xa8\xa6 and then load it again, I end up with UnicodeDecodeError. If you add some more data to the string (like \'a\xa8\xa6e\'), the data gets stored and retrieved without error, but the result is missing the last character.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(columns=[""A""])\r\ntoAppend = {""A"": ""\xa8\xa6""}\r\ndf = df.append(toAppend, ignore_index = True)\r\n\r\nstore = pd.HDFStore(r\'thiswillcrash.h5\')\r\nstore.put(\'df\', df, format=\'table\', encoding=""utf-8"")\r\nd = store[""df""]\r\nprint(d)\r\n\r\nstore.close()\r\n```\r\nVersions\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 8\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.2\r\nnose: 1.3.4\r\nCython: 0.22\r\nnumpy: 1.9.3\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.6\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.4.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.6.7\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 0.9.9\r\npymysql: None\r\npsycopg2: None\r\n```'"
11233,109667079,taeold,jreback,2015-10-04 06:55:29,2015-10-09 13:04:03,2015-10-09 13:03:57,closed,,0.17.1,2,Bug;IO LaTeX;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/11233,b'BUG: to_latex() output broken when the index has a name (GH10660)',b'closes #10660\r\n \r\nfirst try at contributing to pandas. feedbacks will be greatly appreciated.'
11230,109636435,MaximilianR,jreback,2015-10-03 18:05:48,2015-12-08 03:52:34,2015-10-09 13:15:07,closed,,0.17.1,9,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11230,b'squeeze works on 0 length arrays',"b""fixes: https://github.com/pydata/pandas/issues/11229.\r\nfixes: https://github.com/pydata/pandas/issues/8999.\r\n\r\nAlso a better implementation that avoids `ix`\r\n\r\nShould I add to What's new for 0.17? Or is that closed now?"""
11229,109636324,MaximilianR,jreback,2015-10-03 18:04:36,2015-10-09 13:15:20,2015-10-09 13:15:20,closed,,0.17.1,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11229,b'Squeeze fails on zero length arrays',"b'```python\r\nIn [77]: empty=pd.DataFrame(pd.np.empty(shape=(0,1)))\r\n\r\nIn [78]: empty\r\nOut[78]: \r\nEmpty DataFrame\r\nColumns: [0]\r\nIndex: []\r\n\r\nIn [79]: empty.squeeze()\r\nOut[79]: \r\nEmpty DataFrame\r\nColumns: [0]\r\nIndex: []\r\n```\r\n...but should return a `Series`\r\n\r\nPR coming now'"
11226,109568951,jreback,jreback,2015-10-02 20:44:21,2016-02-13 13:48:30,2016-02-13 13:48:30,closed,,Next Major Release,1,Bug;Difficulty Advanced;Effort Low;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/11226,b'BUG: multilevel indexing with new values',"b""```\r\nIn [2]: df = pd.DataFrame(data=[[0, 2, 1], [np.nan, 5, 4]], index=pd.date_range('2014-01-01', '2014-01-2', name='timestamp'), columns=pd.MultiIndex.from_product([['value'], [24, 3766, 5061]], names=[None, 'sid']))\r\n\r\nIn [3]: df\r\nOut[3]: \r\n           value          \r\nsid         24   3766 5061\r\ntimestamp                 \r\n2014-01-01     0    2    1\r\n2014-01-02   NaN    5    4\r\n\r\nIn [4]: assets = [3766,5061,123]\r\n\r\nIn [5]: df.reindex(columns=assets,level=1)\r\nOut[5]: \r\n           value     \r\nsid         3766 5061\r\ntimestamp            \r\n2014-01-01     2    1\r\n2014-01-02     5    4\r\n```\r\n\r\nSo this should include 123 as the last column (and all nans) as well.\r\nThe indexer returned in ``np.array([1,2])``, but should be ``np.array([1,2,-1])``, see [here](https://github.com/pydata/pandas/blob/master/pandas/core/index.py#L2406)\r\n\r\nonly 1 test I could find actually tests this behavior, see [here](https://github.com/pydata/pandas/blob/master/pandas/tests/test_multilevel.py#L141)\r\n\r\nbut only tests that the SAME values are returned, not less or more (which is the point of this issue)"""
11216,109417385,jreback,jreback,2015-10-02 01:55:57,2015-10-03 15:48:50,2015-10-03 15:48:50,closed,,0.17.0,6,Bug;IO SQL;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/11216,b'BUG: edge case when reading from postgresl with read_sql_query and datetime with tz and chunksize',"b""- When we don't specifiy a chunksize we get an object dtype which is ok\r\n- We create a propery datetime64[ns, tz] type, but its a pytz.FixedOffset(....)\r\n  which ATM is not really a useful/palatable type and is mostly confusing for now.\r\n  In the future could attempt to coerce this to a nice tz, e.g. US/Eastern, ,not sure if\r\n  this is possible. \r\n- Note that this is w/o parse_dates specified"""
11206,109066095,jreback,jreback,2015-09-30 11:21:29,2015-11-20 14:51:21,2015-11-20 14:51:21,closed,,0.17.1,0,Bug;Difficulty Intermediate;Effort Low;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/11206,"b""BUG: accept np.datetime64('NaT') in an object dtyped as a null""","b""from [SO](http://stackoverflow.com/questions/32863674/python-pandas-isnull-does-not-work-on-nat-in-object-dtype)\r\n\r\nthis should work\r\n\r\n```\r\nOut[3]: array([11, 22, 33, nan, numpy.datetime64('NaT')], dtype=object)\r\n\r\nIn [4]: pd.isnull(ser)\r\nOut[4]: \r\n0    False\r\n1    False\r\n2    False\r\n3     True\r\n4    False\r\nName: my_series, dtype: bool\r\n```\r\n\r\nThis could be updated: https://github.com/pydata/pandas/blob/master/pandas/tslib.pyx#L711, then ``lib.isnullobj`` then could use for null detection"""
11195,108484869,121onto,jreback,2015-09-26 18:14:11,2015-09-26 18:32:56,2015-09-26 18:29:34,closed,,,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11195,b'DataFrame.shift unexpected result',"b""Consider an arbitrary DataFrame:\r\n\r\n```{python}\r\ndf = pd.DataFrame([1,2,4,35,356,3425,24,24,12,24,6,7,8,8,4], columns=['one'])\r\ndf['two'] = np.log(df['one'])\r\ndf['three'] = np.log10(df['one'])\r\ndf['four'] = np.exp(df['one'])\r\ndf['five'] = df['two'] * df['three']\r\n```\r\n\r\nThis gives:\r\n\r\n```\r\nIn [8]: df\r\nOut[8]:\r\n     one       two     three           four       five\r\n0      1  0.000000  0.000000   2.718282e+00   0.000000\r\n1      2  0.693147  0.301030   7.389056e+00   0.208658\r\n2      4  1.386294  0.602060   5.459815e+01   0.834632\r\n3     35  3.555348  1.544068   1.586013e+15   5.489699\r\n4    356  5.874931  2.551450  4.062895e+154  14.989592\r\n5   3425  8.138857  3.534661            inf  28.768096\r\n6     24  3.178054  1.380211   2.648912e+10   4.386386\r\n7     24  3.178054  1.380211   2.648912e+10   4.386386\r\n8     12  2.484907  1.079181   1.627548e+05   2.681665\r\n9     24  3.178054  1.380211   2.648912e+10   4.386386\r\n10     6  1.791759  0.778151   4.034288e+02   1.394260\r\n11     7  1.945910  0.845098   1.096633e+03   1.644485\r\n12     8  2.079442  0.903090   2.980958e+03   1.877923\r\n13     8  2.079442  0.903090   2.980958e+03   1.877923\r\n14     4  1.386294  0.602060   5.459815e+01   0.834632\r\n``` \r\nNow shift columns:\r\n\r\n```\r\nIn [9]: df.shift(axis=1)\r\nOut[9]:\r\n    one  two  three  four  five\r\n0   NaN  NaN    NaN   NaN   NaN\r\n1   NaN  NaN    NaN   NaN   NaN\r\n2   NaN  NaN    NaN   NaN   NaN\r\n3   NaN  NaN    NaN   NaN   NaN\r\n4   NaN  NaN    NaN   NaN   NaN\r\n5   NaN  NaN    NaN   NaN   NaN\r\n6   NaN  NaN    NaN   NaN   NaN\r\n7   NaN  NaN    NaN   NaN   NaN\r\n8   NaN  NaN    NaN   NaN   NaN\r\n9   NaN  NaN    NaN   NaN   NaN\r\n10  NaN  NaN    NaN   NaN   NaN\r\n11  NaN  NaN    NaN   NaN   NaN\r\n12  NaN  NaN    NaN   NaN   NaN\r\n13  NaN  NaN    NaN   NaN   NaN\r\n14  NaN  NaN    NaN   NaN   NaN\r\n```\r\n\r\nIt appears this issue has been around for over a year:\r\n\r\nhttp://stackoverflow.com/questions/23105197/shift-entire-column-on-a-pandas-dataframe\r\n"""
11193,108435857,sinhrks,jreback,2015-09-26 03:02:37,2015-12-18 18:26:22,2015-12-18 18:26:22,closed,,0.18.0,12,API Design;Bug;Difficulty Novice;Needs Discussion,https://api.github.com/repos/pydata/pandas/issues/11193,b'API/BUG: Index creation with named Index resets name',"b""```\r\nimport pandas as pd\r\nidx = pd.Index([1, 2 ,3], name='x')\r\npd.Index(idx)\r\n# Int64Index([1, 2, 3], dtype='int64')\r\n```\r\n\r\nA point is whether to reset existing Index name in below case. Current impl can't distinguish whether name is not passed, or passed ``None`` explicitly.\r\n\r\n```\r\npd.Index(idx, name=None)\r\n# -> should reset name?\r\n```"""
11192,108430016,sinhrks,jreback,2015-09-26 00:50:51,2015-12-26 00:28:22,2015-12-26 00:28:22,closed,,0.18.0,2,Bug;Difficulty Intermediate;Effort Low;Frequency,https://api.github.com/repos/pydata/pandas/issues/11192,b'BUG: pd.frequency.get_offset cache may be overwritten',"b""Because ``frequencies.get_offset`` may be broken by user operation because of its cache.\r\n\r\nhttps://github.com/pydata/pandas/blob/master/pandas/tseries/frequencies.py#L515\r\n\r\n```\r\nimport pandas as pd\r\nx = pd.tseries.frequencies.get_offset('D')\r\nx.__dict__\r\n# {'normalize': False, 'kwds': {}, 'n': 1, '_offset': datetime.timedelta(1), '_named': 'D', '_use_relativedelta': False}\r\n\r\n# modify its property\r\nx.n = 5\r\n\r\n# NG, modified instance is returned\r\ny = pd.tseries.frequencies.get_offset('D')\r\ny.__dict__\r\n# {'normalize': False, 'kwds': {}, 'n': 5, '_offset': datetime.timedelta(1), '_named': 'D', '_use_relativedelta': False}\r\n```\r\n\r\nIt should return a copy of the cache."""
11189,108401278,jorisvandenbossche,jreback,2015-09-25 20:20:29,2015-11-01 18:44:44,2015-09-25 23:51:09,closed,,0.17.0,13,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/11189,b'BUG: groupby().size gives casting error on 32 bit platform',"b'```\r\nIn [1]: df = pd.DataFrame({""id"":[1,2,3,4,5,6], ""grade"":[\'a\', \'b\', \'b\', \'a\', \'a\', \'e\']})\r\n\r\nIn [2]: df.groupby(""grade"").size()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-d8e387418f9d> in <module>()\r\n----> 1 df.groupby(""grade"").size()\r\n\r\n/home/joris/scipy/pandas/pandas/core/groupby.pyc in size(self)\r\n    818 \r\n    819         """"""\r\n--> 820         return self.grouper.size()\r\n    821 \r\n    822     sum = _groupby_function(\'sum\', \'add\', np.sum)\r\n\r\n/home/joris/scipy/pandas/pandas/core/groupby.pyc in size(self)\r\n   1380         """"""\r\n   1381         ids, _, ngroup = self.group_info\r\n-> 1382         out = np.bincount(ids[ids != -1], minlength=ngroup)\r\n   1383         return Series(out, index=self.result_index)\r\n   1384 \r\n\r\nTypeError: Cannot cast array data from dtype(\'int64\') to dtype(\'int32\') according to the rule \'safe\'\r\n\r\nIn [4]: pd.__version__\r\nOut[4]: \'0.17.0rc1+108.g3fb802a\'\r\n```'"
11175,107842082,pradyu1993,pradyu1993,2015-09-23 03:15:23,2015-09-24 07:11:23,2015-09-23 17:03:53,closed,,No action,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/11175,b'issue #11119',b'Closes #11119'
11153,107360214,sinhrks,jreback,2015-09-19 23:01:47,2015-09-26 00:27:01,2015-09-20 18:52:13,closed,,0.17.0,3,Blocker;Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/11153,b'(WIP) BUG: DatetimeTZBlock.fillna raises TypeError',"b'```\r\nimport pandas as pd\r\nidx = pd.DatetimeIndex([\'2011-01-01\', pd.NaT, \'2011-01-03\'], tz=\'Asia/Tokyo\')\r\ns = pd.Series(idx)\r\ns\r\n# 0   2011-01-01 00:00:00+09:00\r\n# 1                         NaT\r\n# 2   2011-01-03 00:00:00+09:00\r\n# dtype: datetime64[ns, Asia/Tokyo]\r\n\r\n# NG, the result must be DatetimeTZBlock\r\ns.fillna(pd.Timestamp(\'2011-01-02\', tz=\'Asia/Tokyo\'))\r\n# TypeError: putmask() argument 1 must be numpy.ndarray, not DatetimeIndex\r\n\r\n# NG, it should be object dtype, as the same as when Series creation with mixed tz\r\ns.fillna(pd.Timestamp(\'2011-01-02\'))\r\n# TypeError: putmask() argument 1 must be numpy.ndarray, not DatetimeIndex\r\n```\r\n\r\nAlso, found existing ``DatetimeBlock`` doesn\'t handle tz properly. Closes #7095.\r\n\r\n```\r\nidx = pd.DatetimeIndex([\'2011-01-01\', pd.NaT, \'2011-01-03\'])\r\ns = pd.Series(idx)\r\n\r\n# OK, result must be DatetimeBlock\r\ns.fillna(pd.Timestamp(\'2011-01-02\'))\r\n# 0   2011-01-01\r\n# 1   2011-01-02\r\n# 2   2011-01-03\r\n# dtype: datetime64[ns]\r\n\r\n# NG, it should be object dtype, as the same as when Series creation with mixed tz\r\ns.fillna(pd.Timestamp(\'2011-01-02\', tz=\'Asia/Tokyo\'))\r\n# 0   2011-01-01 00:00:00\r\n# 1   2011-01-01 15:00:00\r\n# 2   2011-01-03 00:00:00\r\n# dtype: datetime64[ns]\r\n\r\n# NG, unable to fill different dtypes\r\ns.fillna(\'AAA\')\r\n# ValueError: Error parsing datetime string ""AAA"" at position 0\r\n```'"
11150,107351905,Tux1,jreback,2015-09-19 19:31:05,2015-11-13 16:41:57,2015-11-13 16:41:57,closed,,0.17.1,12,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/11150,b'BUG: axis kw not propogating on pct_change',b'axis option was not correctly set on fillna in pct_change'
11134,106895602,chris-b1,jreback,2015-09-17 01:34:38,2015-09-17 22:31:12,2015-09-17 11:45:05,closed,,0.17.0,6,Bug;Dtypes;Reshaping;Timedelta,https://api.github.com/repos/pydata/pandas/issues/11134,b'BUG: nested construction with timedelta #11129',"b""Closes #11129\r\n\r\nPer discussion in issue this adds similar hash equality to `_Timedelta` / `datetime.timedelta`\r\nthat exists for `_Timestamp` / `datetime.datime`.\r\n\r\nI tried inlining `_Timedelta._ensure_components` and it actually hurt performance a bit, so I left it\r\nas a plain `def` but did build a seperate `_has_ns` check for hashing.\r\n\r\nAdds a little overhead to hashing:\r\n\r\n    In [1]: tds = pd.timedelta_range('1 day', periods=100000)\r\n\r\n    # PR\r\n    In [2]: %timeit [hash(td) for td in tds]\r\n    1 loops, best of 3: 810 ms per loop\r\n\r\n    # Master\r\n    In [2]: %timeit [hash(td) for td in tds]\r\n    1 loops, best of 3: 765 ms per loop\r\n"""
11129,106882944,chris-b1,jreback,2015-09-16 23:13:18,2015-09-17 11:45:05,2015-09-17 11:45:05,closed,,0.17.0,8,Bug;Difficulty Novice;Dtypes;Effort Low;Reshaping;Timedelta,https://api.github.com/repos/pydata/pandas/issues/11129,b'BUG: nested dict construction timedelta',"b""Related to #10160 / PR https://github.com/pydata/pandas/pull/10269\r\n\r\n    In [2]: from datetime import timedelta\r\n    ...: from pandas import * \r\n    ...: td_as_int = [1, 2, 3, 4]\r\n    ...: \r\n    ...: def create_data(constructor):\r\n    ...:     return dict((i, {constructor(s): 2*i}) for i, s in enumerate(td_as_int))\r\n    ...: \r\n    ...: data_timedelta64 = create_data(lambda x: np.timedelta64(x, 'D'))\r\n    ...: data_timedelta = create_data(lambda x: timedelta(days=x))\r\n    ...: \r\n    ...: result_timedelta64 = DataFrame(data_timedelta64)\r\n    ...: result_timedelta = DataFrame(data_timedelta)\r\n    ...: \r\n\r\n    In [3]: result_timedelta64\r\n    Out[3]: \r\n            0   1   2   3\r\n    1 days   0 NaN NaN NaN\r\n    2 days NaN   2 NaN NaN\r\n    3 days NaN NaN   4 NaN\r\n    4 days NaN NaN NaN   6\r\n\r\n    In [4]: result_timedelta\r\n    Out[4]: \r\n            0   1   2   3\r\n    1 days NaN NaN NaN NaN\r\n    2 days NaN NaN NaN NaN\r\n    3 days NaN NaN NaN NaN\r\n    4 days NaN NaN NaN NaN\r\n\r\n\r\nThis seems to happen because `Timedelta` doesn't have hash equality with `datetime.timedelta`  It could easily be worked around here, but I was wondering if there's any reason not to make `Timedelta` work  like `Timestamp` and match hashes if above a certain resolution?\r\n\r\n\r\n"""
11082,106188961,sinhrks,jreback,2015-09-12 22:50:09,2016-04-10 14:22:42,2016-04-10 14:22:42,closed,,0.18.1,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11082,b'BUG: empty Series concat has no effect',"b""```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ns1 = pd.Series([1, 2, 3], name='x')\r\ns2 = pd.Series(name='y')\r\n\r\n# NG, no columns is added\r\npd.concat([s1, s2], axis=1)\r\n#    x\r\n# 0  1\r\n# 1  2\r\n# 2  3\r\n\r\n# If padded by value, OK\r\npd.concat([s1, pd.Series([np.nan]*3, name='z')], axis=1)\r\n#    x   z\r\n# 0  1 NaN\r\n# 1  2 NaN\r\n# 2  3 NaN\r\n\r\n# If converted to frame, OK.\r\npd.concat([s1.to_frame(), s2.to_frame()], axis=1)\r\n#    x   y\r\n# 0  1 NaN\r\n# 1  2 NaN\r\n# 2  3 NaN\r\n```"""
11079,106177603,cpcloud,cpcloud,2015-09-12 18:31:35,2015-09-14 22:11:28,2015-09-14 22:09:02,closed,cpcloud,0.17.0,11,Blocker;Bug,https://api.github.com/repos/pydata/pandas/issues/11079,b'BUG: Fix Series nunique groupby with object dtype',b'closes #11077'
11077,106175299,cpcloud,cpcloud,2015-09-12 18:04:09,2015-09-14 22:09:01,2015-09-14 22:09:01,closed,cpcloud,0.17.0,14,Blocker;Bug,https://api.github.com/repos/pydata/pandas/issues/11077,b'Broken nunique on Series group by',"b""The following code works in 0.16.2 and not in latest master:\r\n\r\n```python\r\ndata = pd.DataFrame(\r\n    [[100, 1, 'Alice'],\r\n     [200, 2, 'Bob'],\r\n     [300, 3, 'Charlie'],\r\n     [-400, 4, 'Dan'],\r\n     [500, 5, 'Edith']],\r\n    columns=['amount', 'id', 'name']\r\n)\r\n\r\nexpected = data.groupby(['id', 'amount'])['name'].nunique()\r\n```\r\n\r\nGoing to bisect this today unless someone beats me to it."""
11065,106048025,d1manson,jorisvandenbossche,2015-09-11 16:38:44,2015-11-15 21:09:40,2015-11-15 21:09:28,closed,,No action,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/11065,b'BUG: matplotlib inset-axis #10407',b'This is the issue: https://github.com/pydata/pandas/issues/10407 \r\nI just added a bunch of try-catch AttributeError around the offending code - it seemed to do the trick for me.\r\n\r\n'
11061,106022822,hhuuggoo,jreback,2015-09-11 14:34:54,2015-09-12 13:16:30,2015-09-12 13:16:30,closed,,Next Major Release,2,Bug;Difficulty Intermediate;Dtypes;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11061,b'shifting across blocks results in NaNs if dataframe has multiple blocks (mixed types)',"b""variant of #10907 \r\n\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame({'a' : [1],'b':[2]})\r\n\r\nIn [3]: df\r\nOut[3]: \r\n   a  b\r\n0  1  2\r\n\r\nIn [4]: df['c'] = 3.5\r\n\r\nIn [5]: df.shift(axis=1)\r\nOut[5]: \r\n    a  b   c\r\n0 NaN  1 NaN\r\n```"""
11055,105932107,terrytangyuan,jreback,2015-09-11 02:15:01,2015-09-11 14:15:01,2015-09-11 14:12:16,closed,,0.17.0,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/11055,"b'BUG: Fixed bug in groupby(axis=1) with filter() throws IndexError, #11041'",b'Fixed #11041 '
11049,105789427,kawochen,jreback,2015-09-10 11:42:38,2015-09-10 22:12:02,2015-09-10 22:11:55,closed,,0.17.0,3,Bug;Indexing;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/11049,b'BUG: GH10645 and GH10692 where operation on large Index would error',b'closes #10645\r\ncloses #10692 \r\nreplaces #10675 \r\nDo I need `@slow` for these tests?'
11041,105730669,tdihp,jreback,2015-09-10 04:44:15,2015-09-11 14:12:48,2015-09-11 14:12:48,closed,,0.17.0,6,Bug;Difficulty Novice;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/11041,b'grouped.filter with axis=1 gave filtered rows',"b'using pandas 0.16.2\r\nminimum example here:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nindex = pd.MultiIndex.from_product([range(10), [0, 1]])\r\ndata = pd.DataFrame(np.arange(100).reshape(-1, 20), columns=index)\r\ndata.groupby(level=0, axis=1).filter(lambda x: x.iloc[0, 0] > 10)\r\n```\r\n\r\nexpected: get all filtered (grouped) columns as a DataFrame\r\nresult: `IndexError`\r\n\r\nwhen change `np.arange(100)` to `np.arange(1000)` (more rows than columns)\r\nresults data frame with missing rows.'"
11039,105704428,behzadnouri,jreback,2015-09-09 23:38:43,2016-04-25 14:22:20,2016-04-25 14:21:45,closed,,0.18.1,34,Bug;Groupby;Performance,https://api.github.com/repos/pydata/pandas/issues/11039,b'PERF: improves performance in GroupBy.cumcount',"b""closes #12839 \r\n\r\n```\r\n    -------------------------------------------------------------------------------\r\n    Test name                                    | head[ms] | base[ms] |  ratio   |\r\n    -------------------------------------------------------------------------------\r\n    groupby_ngroups_10000_cumcount               |   3.9790 |  74.9559 |   0.0531 |\r\n    groupby_ngroups_100_cumcount                 |   0.6043 |   0.9940 |   0.6079 |\r\n    -------------------------------------------------------------------------------\r\n    Test name                                    | head[ms] | base[ms] |  ratio   |\r\n    -------------------------------------------------------------------------------\r\n\r\n    Ratio < 1.0 means the target commit is faster then the baseline.\r\n    Seed used: 1234\r\n\r\n    Target [2a1c935] : PERF: improves performance in GroupBy.cumcount\r\n    Base   [5b1f3b6] : reverts 'from .pandas_vb_common import *'\r\n```"""
11036,105641405,cpcloud,jreback,2015-09-09 17:05:23,2015-09-09 21:09:17,2015-09-09 21:09:17,closed,cpcloud,0.17.0,1,Blocker;Bug,https://api.github.com/repos/pydata/pandas/issues/11036,b'BUG: Fix Timestamp __ne__ comparison issue',b'closes #11034 '
11034,105599200,jcrist,jreback,2015-09-09 13:43:38,2015-09-09 21:09:28,2015-09-09 21:09:28,closed,cpcloud,0.17.0,3,Blocker;Bug,https://api.github.com/repos/pydata/pandas/issues/11034,b'Timestamp equality broadcasts incorrectly',"b""Always returns true for equality and inequality:\r\n\r\n```python\r\nIn [168]: t = pd.Timestamp('2000-01-29 01:59:00')\r\n\r\nIn [169]: a = pd.Series([t])\r\n\r\nIn [170]: b = pd.Series([t])\r\n\r\nIn [171]: a\r\nOut[171]:\r\n0   2000-01-29 01:59:00\r\ndtype: datetime64[ns]\r\n\r\nIn [172]: a == b\r\nOut[172]:\r\n0    True\r\ndtype: bool\r\n\r\nIn [173]: a != b\r\nOut[173]:\r\n0    True\r\ndtype: bool\r\n\r\nIn [174]: t == t\r\nOut[174]: True\r\n\r\nIn [175]: t != t\r\nOut[175]: False\r\n```"""
11031,105501785,terrytangyuan,jreback,2015-09-09 01:51:42,2015-09-09 12:40:45,2015-09-09 11:16:52,closed,,0.17.0,6,Bug;Groupby;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11031,"b""BUG: Fixed bug in len(DataFrame.groupby) causing IndexError when there's a NaN-only column (issue11016)""",b'This is the new PR for Fixed issue #11016 '
11030,105498259,terrytangyuan,terrytangyuan,2015-09-09 01:18:09,2015-09-09 01:52:44,2015-09-09 01:52:42,closed,,,3,Bug;Groupby;Missing-data,https://api.github.com/repos/pydata/pandas/issues/11030,"b""BUG: Fixed bug in len(DataFrame.groupby) causing IndexError when there's a NaN-only column (issue11016)""",b'Fixed #11016 '
11024,105376756,sinhrks,sinhrks,2015-09-08 13:07:49,2015-09-09 08:47:50,2015-09-09 08:47:32,closed,,0.17.0,1,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/11024,b'BUG: DatetimeIndex.freq can defer by ufunc',"b""Follow up of #10638. ``DatetimeIndex.freq`` also can be changed when other arg is an ndarray.\r\n\r\nAs the same as #10638, freq is re-infered only when it exists. Otherwise, leave it as ``None`` even if the result can be on specific freq. This is based on other functions behavior as below:\r\n\r\n```\r\nidx = pd.DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'])\r\nidx\r\n# DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None)\r\nidx.take([0, 2])\r\n# DatetimeIndex(['2011-01-01', '2011-01-03'], dtype='datetime64[ns]', freq=None)\r\n\r\nidx = pd.DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], freq='D')\r\nidx.take([0, 2])\r\n# DatetimeIndex(['2011-01-01', '2011-01-03'], dtype='datetime64[ns]', freq='2D')\r\n```"""
11018,105238541,sinhrks,jreback,2015-09-07 15:44:46,2015-09-07 20:23:10,2015-09-07 20:19:58,closed,,0.17.0,1,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/11018,b'BUG: Unable to infer negative freq',"b""Fixed 2 issues:\r\n\r\n- ``date_range`` can't handle negative calendar-based freq (like ``A``, ``Q`` and ``M``).\r\n\r\n```\r\nimport pandas as pd\r\n\r\n# OK\r\npd.date_range('2011-01-01', freq='-1D', periods=3)\r\n# DatetimeIndex(['2011-01-01', '2010-12-31', '2010-12-30'], dtype='datetime64[ns]', freq='-1D')\r\n\r\n# NG\r\npd.date_range('2011-01-01', freq='-1M', periods=3)\r\n# DatetimeIndex([], dtype='datetime64[ns]', freq='-1M')\r\n```\r\n\r\n- Unable to infer negative freq.\r\n\r\n```\r\npd.DatetimeIndex(['2011-01-05', '2011-01-03', '2011-01-01'], freq='infer')\r\n# DatetimeIndex(['2011-01-05', '2011-01-03', '2011-01-01'], dtype='datetime64[ns]', freq=None)\r\n\r\npd.TimedeltaIndex(['-1 days', '-3 days', '-5 days'], freq='infer')\r\n# TimedeltaIndex(['-1 days', '-3 days', '-5 days'], dtype='timedelta64[ns]', freq=None)\r\n```\r\n\r\n\r\n\r\n"""
11017,105228564,sinhrks,jreback,2015-09-07 14:46:47,2015-09-11 14:36:06,2015-09-11 14:24:25,closed,,0.17.0,10,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/11017,b'BUG: Index dtype may not be applied properly',"b'Fixed 2 problems:\r\n\r\n- Specified ``dtype`` is not applied to other iterables.\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\npd.Index([1, 2, 3], dtype=int)\r\n# Index([1, 2, 3], dtype=\'object\')\r\n```\r\n\r\n- Specifying ``category`` to ndarray-like results in ``TypeError``\r\n\r\n```\r\npd.Index(np.array([1, 2, 3]), dtype=\'category\')\r\n# TypeError: data type ""category"" not understood\r\n```\r\n'"
11016,105225302,marcelm,jreback,2015-09-07 14:27:19,2015-09-09 12:39:53,2015-09-09 11:17:16,closed,,0.17.0,3,Bug;Difficulty Novice;Effort Low;Groupby;Missing-data;Prio-low,https://api.github.com/repos/pydata/pandas/issues/11016,b'groupby on NaN-only column gives IndexError',"b""Using pandas from master (0.16.2+590.g81b647f) in Python 3.4.2, the following code gives an `IndexError: index out of bounds`:\r\n```\r\nimport pandas as pd, numpy as np\r\ndf = pd.DataFrame(dict(a=[np.nan]*3, b=[1,2,3]))\r\ng = df.groupby(('a', 'b'))\r\nlen(g)  # IndexError\r\n```\r\nThe same problem occurs when calling `list(g)` instead. Since NaN values are skipped according to the documentation, I guess the correct answer would be zero for `len(g)` and an empty list for `list(g)`.\r\n\r\nStrangely, iteration works, so `for x in g: pass` (or `[x for x in g]`) does not give an error (and iterates zero times). Also, `g.count()`, `g.sum()` etc. work (and return an empty DataFrame).\r\n\r\nTo add to the confusion, `g.groups` gives the dictionary `{(nan, 1): [0], (nan, 2): [1], (nan, 3): [2]}`. Shouldn\xa1\xaft this be empty because group keys with NaNs are dropped?\r\n\r\nGrouping only by column 'a' or 'b' works and results in a length of 0 or 3, respectively."""
11015,105143718,MaximilianR,jreback,2015-09-07 04:21:55,2015-12-08 03:52:34,2015-09-09 21:40:50,closed,,0.17.0,3,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/11015,b'BUG: .ix with PeriodIndex is fixed',"b""Solves https://github.com/pydata/pandas/issues/4125\r\n\r\nIt doesn't feel like an elegant solution, but I think it works. \r\nI think when Period gets its own dtype, these will all go away?"""
11007,105043411,jorisvandenbossche,jreback,2015-09-05 18:56:36,2015-10-10 00:23:39,2015-10-10 00:23:39,closed,,0.17.1,11,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/11007,b'BUG: to_excel swaps order of values of duplicate columns',"b""On master, as a small example:\r\n\r\n```\r\nIn [1]: df = pd.DataFrame([[1,2,3,4],[5,6,7,8]], columns=['A','B','A','B'])\r\n\r\nIn [2]: df\r\nOut[2]:\r\n   A  B  A  B\r\n0  1  2  3  4\r\n1  5  6  7  8\r\n\r\nIn [4]: df.to_excel('test_excel_duplicate_columns.xlsx')\r\n```\r\n\r\ngives:\r\n\r\n![capture](https://cloud.githubusercontent.com/assets/1020496/9700751/4cacf18e-540f-11e5-923b-e6d6689d569f.PNG)\r\n\r\nSo the values of columns 2 and 3 are swapped (not the column names)\r\n\r\nBTW, this happens both with .xlsx as .xls (openpyxl / xlsxwriter / xlwt)\r\n\r\nPossibly related: #10982, #10970"""
11006,105041899,jreback,jreback,2015-09-05 18:27:44,2015-09-05 23:22:57,2015-09-05 23:22:57,closed,,0.17.0,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/11006,b'BUG: Bug in pickling of a non-regular freq DatetimeIndex #11002',b'closes #11002 '
11002,105039813,ifmihai,jreback,2015-09-05 17:46:24,2015-09-05 23:22:57,2015-09-05 23:22:57,closed,,0.17.0,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/11002,b'pd.to_pickle() and pd.read_pickle() bug?',"b""I'm on windows 7, anaconda, python 2.7.9, 32bit\r\npandas 0.16.2\r\n\r\ntry this:\r\n\r\n```python\r\nimport pandas as pd\r\ndf4 = pd.DataFrame(index=P.date_range('1750-1-1', '2050-1-1', freq='7D')\r\npd.to_pickle(df4, '7d.test')\r\npd.read_pickle('7d.test')\r\n\r\n\r\nIn [84]: P.read_pickle('7d.test')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-84-0108eecfbea7> in <module>()\r\n----> 1 P.read_pickle('7d.test')\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\io\\pickle.pyc in read_pickle(path)\r\n     58 \r\n     59     try:\r\n---> 60         return try_read(path)\r\n     61     except:\r\n     62         if PY3:\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\io\\pickle.pyc in try_read(path, encoding)\r\n     55             except:\r\n     56                 with open(path, 'rb') as fh:\r\n---> 57                     return pc.load(fh, encoding=encoding, compat=True)\r\n     58 \r\n     59     try:\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\compat\\pickle_compat.pyc in load(fh, encoding, compat, is_verbose)\r\n    114         up.is_verbose = is_verbose\r\n    115 \r\n--> 116         return up.load()\r\n    117     except:\r\n    118         raise\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\pickle.pyc in load(self)\r\n    856             while 1:\r\n    857                 key = read(1)\r\n--> 858                 dispatch[key](self)\r\n    859         except _Stop, stopinst:\r\n    860             return stopinst.value\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\compat\\pickle_compat.pyc in load_reduce(self)\r\n     18 \r\n     19     try:\r\n---> 20         stack[-1] = func(*args)\r\n     21         return\r\n     22     except Exception as e:\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\tseries\\index.pyc in _new_DatetimeIndex(cls, d)\r\n    113     # data are already in UTC\r\n    114     tz = d.pop('tz',None)\r\n--> 115     result = cls.__new__(cls, **d)\r\n    116     result.tz = tz\r\n    117     return result\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\util\\decorators.pyc in wrapper(*args, **kwargs)\r\n     86                 else:\r\n     87                     kwargs[new_arg_name] = new_arg_value\r\n---> 88             return func(*args, **kwargs)\r\n     89         return wrapper\r\n     90     return _deprecate_kwarg\r\n\r\nC:\\Users\\LV\\Miniconda\\lib\\site-packages\\pandas\\tseries\\index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, **kwargs)\r\n    334                     if not np.array_equal(subarr.asi8, on_freq.asi8):\r\n    335                         raise ValueError('Inferred frequency {0} from passed dates does not'\r\n--> 336                                          'conform to passed frequency {1}'.format(inferred, freq.freqstr))\r\n    337 \r\n    338         if freq_infer:\r\n\r\nValueError: Inferred frequency W-THU from passed dates does notconform to passed frequency 7D\r\n\r\n```"""
10993,104960190,jakevdp,jreback,2015-09-04 20:00:17,2015-10-20 17:29:50,2015-10-20 17:29:50,closed,,0.17.1,12,Bug;Categorical;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10993,"b'BUG: pivot_table with margins=True fails for categorical dtype, #10989'","b'This is a fix for the issue reported in #10989. I suspect this is an example of ""fixing the symptom"" rather than ""fixing the problem"", but I think it makes clear what the source of the problem is: to compute margins, the pivot table must add a row and/or column to the result. If the index or column is categorical, a new value cannot be added.\r\n\r\nLet me know if you think there are better approaches to this.'"
10989,104923169,jakevdp,jreback,2015-09-04 16:26:01,2015-10-19 13:54:03,2015-10-19 11:39:44,closed,,0.17.1,5,Bug;Categorical;Difficulty Novice;Effort Low;Prio-medium;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10989,b'BUG: pivot_table with margins=True fails for categorical dtype',"b'First, an example that works as expected (non-categorical):\r\n\r\n```python\r\nIn [22]: pd.__version__\r\nOut[22]: \'0.16.2\'\r\n\r\nIn [23]: data = pd.DataFrame({\'x\': np.arange(99),\r\n                     \'y\': np.arange(99) // 50,\r\n                     \'z\': np.arange(99) % 3})\r\n\r\nIn [24]: data.pivot_table(\'x\', \'y\', \'z\')\r\nOut[24]: \r\nz     0     1     2\r\ny                  \r\n0  24.0  25.0  24.5\r\n1  73.5  74.5  74.0\r\n\r\nIn [25]: data.pivot_table(\'x\', \'y\', \'z\', margins=True)\r\nOut[25]: \r\nz       0     1     2   All\r\ny                          \r\n0    24.0  25.0  24.5  24.5\r\n1    73.5  74.5  74.0  74.0\r\nAll  48.0  49.0  50.0  49.0\r\n```\r\n\r\nNow convert ``y`` and ``z`` to categories; pivot table works without margins but fails with:\r\n\r\n```python\r\nIn [27]: data.y = data.y.astype(\'category\')\r\n\r\nIn [28]: data.z = data.z.astype(\'category\')\r\n\r\nIn [29]: data.pivot_table(\'x\', \'y\', \'z\')\r\nOut[29]: \r\nz     0     1     2\r\ny                  \r\n0  24.0  25.0  24.5\r\n1  73.5  74.5  74.0\r\n\r\nIn [32]: data.pivot_table(\'x\', \'y\', \'z\', margins=True)\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/internals.py in set(self, item, value, check)\r\n   2979         try:\r\n-> 2980             loc = self.items.get_loc(item)\r\n   2981         except KeyError:\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/index.py in get_loc(self, key, method)\r\n   5072             key = tuple(map(_maybe_str_to_time_stamp, key, self.levels))\r\n-> 5073             return self._engine.get_loc(key)\r\n   5074 \r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:3824)()\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:3704)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12280)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12231)()\r\n\r\nKeyError: (\'x\', \'All\')\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-32-7436e0e1c9bb> in <module>()\r\n----> 1 data.pivot_table(\'x\', \'y\', \'z\', margins=True)\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/tools/pivot.py in pivot_table(data, values, index, columns, aggfunc, fill_value, margins, dropna)\r\n    141     if margins:\r\n    142         table = _add_margins(table, data, values, rows=index,\r\n--> 143                              cols=columns, aggfunc=aggfunc)\r\n    144 \r\n    145     # discard the top level\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/tools/pivot.py in _add_margins(table, data, values, rows, cols, aggfunc)\r\n    167 \r\n    168     if values:\r\n--> 169         marginal_result_set = _generate_marginal_results(table, data, values, rows, cols, aggfunc, grand_margin)\r\n    170         if not isinstance(marginal_result_set, tuple):\r\n    171             return marginal_result_set\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/tools/pivot.py in _generate_marginal_results(table, data, values, rows, cols, aggfunc, grand_margin)\r\n    236                 # we are going to mutate this, so need to copy!\r\n    237                 piece = piece.copy()\r\n--> 238                 piece[all_key] = margin[key]\r\n    239 \r\n    240                 table_pieces.append(piece)\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/frame.py in __setitem__(self, key, value)\r\n   2125         else:\r\n   2126             # set column\r\n-> 2127             self._set_item(key, value)\r\n   2128 \r\n   2129     def _setitem_slice(self, key, value):\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/frame.py in _set_item(self, key, value)\r\n   2203         self._ensure_valid_index(value)\r\n   2204         value = self._sanitize_column(key, value)\r\n-> 2205         NDFrame._set_item(self, key, value)\r\n   2206 \r\n   2207         # check if we are modifying a copy\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/generic.py in _set_item(self, key, value)\r\n   1194 \r\n   1195     def _set_item(self, key, value):\r\n-> 1196         self._data.set(key, value)\r\n   1197         self._clear_item_cache()\r\n   1198 \r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/internals.py in set(self, item, value, check)\r\n   2981         except KeyError:\r\n   2982             # This item wasn\'t present, just insert at end\r\n-> 2983             self.insert(len(self.items), item, value)\r\n   2984             return\r\n   2985 \r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/internals.py in insert(self, loc, item, value, allow_duplicates)\r\n   3100             self._blknos = np.insert(self._blknos, loc, len(self.blocks))\r\n   3101 \r\n-> 3102         self.axes[0] = self.items.insert(loc, item)\r\n   3103 \r\n   3104         self.blocks += (block,)\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/index.py in insert(self, loc, item)\r\n   5583                 # other labels\r\n   5584                 lev_loc = len(level)\r\n-> 5585                 level = level.insert(lev_loc, k)\r\n   5586             else:\r\n   5587                 lev_loc = level.get_loc(k)\r\n\r\n/Users/jakevdp/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/core/index.py in insert(self, loc, item)\r\n   3217         code = self.categories.get_indexer([item])\r\n   3218         if (code == -1):\r\n-> 3219             raise TypeError(""cannot insert an item into a CategoricalIndex that is not already an existing category"")\r\n   3220 \r\n   3221         codes = self.codes\r\n\r\nTypeError: cannot insert an item into a CategoricalIndex that is not already an existing category\r\n```\r\n'"
10982,104714589,balzer82,jreback,2015-09-03 14:55:26,2015-10-10 00:23:39,2015-10-10 00:23:39,closed,,0.17.1,2,Bug;IO Excel;Prio-high,https://api.github.com/repos/pydata/pandas/issues/10982,b'.to_excel() cuts off columns',"b""Today I discovered a strange behavior: When I am writing a DataFrame with `.to_excel()`, it cuts columns. Compared with the same DataFrame with `.to_csv()` or `.head()`, you can see the difference, that the last 8 columns are missing.\r\n\r\nYou can reproduce this by downloading `Features.pkl` from [here](https://dl.dropboxusercontent.com/u/16985007/Features.pkl) and then:\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.read_pickle('Features.pkl')\r\ndf.head() # see the last 8 columns!\r\ndf.to_excel('Features.xlsx', index=False, header=False)\r\n# see the Excel, you do not have these last 8 columns\r\n# in a .to_csv() you have them\r\n```\r\n\r\nFunny part: If you `df.ix[:,-71:].to_excel('Features.xlsx', index=False, header=False)` you have one of the missing columns. If you do `df.ix[:,-70:].to_excel('Features.xlsx', index=False, header=False)` you have two and so on...\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: de_DE.UTF-8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.7\r\nCython: 0.22.1\r\nnumpy: 1.9.2\r\nscipy: 0.16.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 1.0.0\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.6\r\nxlrd: 0.9.3\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.3\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: 0.9\r\napiclient: None\r\nsqlalchemy: 1.0.5\r\npymysql: None\r\npsycopg2: None\r\n```"""
10962,104296627,sinhrks,sinhrks,2015-09-01 15:52:12,2015-09-05 02:14:56,2015-09-05 02:14:54,closed,,0.17.0,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10962,b'BUG: DataFrame subplot with duplicated columns output incorrect result',"b""When ``DataFrame`` has duplicated column name, each subplot will contain all the lines with same name.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame(np.random.rand(5, 5), columns=list('aaaaa'))\r\ndf.plot(subplots=True)\r\n```\r\n\r\n![figure_1](https://cloud.githubusercontent.com/assets/1696302/9609047/7acba4d0-510c-11e5-872d-1fcc1fcf8ce6.png)\r\n"""
10961,104271599,cbuton,jreback,2015-09-01 13:52:22,2015-09-01 14:23:14,2015-09-01 14:22:44,closed,,,1,Bug;Duplicate;Numeric,https://api.github.com/repos/pydata/pandas/issues/10961,b'Panel.clip() does not work ',"b'Hello,\r\nI tried to use the clip function on a basic Panel but it seems that the axis management is not working as expected. I believe this is an update to issue https://github.com/pydata/pandas/issues/8344\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n \r\n    data = np.random.randn(3, 4, 5)\r\n    panel = pd.Panel(data)\r\n    panel.clip(0, 1)\r\n\r\nIf I run the script with pandas 0.16.2 on python 2.7.10 and 3.4.3, I\'m getting the following error:\r\n\r\n    .../pandas/core/generic.pyc in clip(self, lower, upper, out, axis)\r\n       3052         result = self\r\n       3053         if lower is not None:\r\n    -> 3054             result = result.clip_lower(lower, axis)\r\n       3055         if upper is not None:\r\n       3056             result = result.clip_upper(upper, axis)\r\n\r\n    .../pandas/core/generic.pyc in clip_lower(self, threshold, axis)\r\n       3103             raise ValueError(""Cannot use an NA value as a clip threshold"")\r\n       3104 \r\n    -> 3105         subset = self.ge(threshold, axis=axis) | isnull(self)\r\n       3106         return self.where(subset, threshold, axis=axis)\r\n       3107 \r\n\r\n    TypeError: f() got an unexpected keyword argument \'axis\'\r\n\r\n**Thanks a lot for all your efforts!**'"
10960,104258009,terrytangyuan,jreback,2015-09-01 12:46:37,2015-09-01 19:39:57,2015-09-01 19:38:53,closed,,0.17.0,5,Bug;Indexing;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10960,b'BUG: Fixed bug that Timedelta raises error when slicing from 0s (issue #10583)',b'closes #10583 '
10957,104167340,terrytangyuan,terrytangyuan,2015-09-01 01:52:22,2015-09-01 13:22:24,2015-09-01 12:47:11,closed,,0.17.0,6,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10957,b'BUG: Fixed bug that Timedelta raises error when slicing from 0s (issue #10583)',b'closes #10583 '
10945,103960528,chris-b1,jreback,2015-08-30 22:43:26,2015-08-31 23:37:24,2015-08-31 12:18:36,closed,,0.17.0,2,Bug;Indexing;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10945,b'BUG: Index name lost in conv #10875',"b'Addresses #10875, when `Index` is converted via `to_datetime`, `to_timedelta`\r\npart of #9862 master issue'"
10944,103929047,glyg,jreback,2015-08-30 14:48:18,2015-09-01 09:17:48,2015-08-31 11:07:48,closed,,0.17.0,6,Bug;IO LaTeX,https://api.github.com/repos/pydata/pandas/issues/10944,"b'Fixing column_format argument passing, #9402'",b'closes #9402 '
10930,103862035,terrytangyuan,jreback,2015-08-29 16:31:30,2015-09-11 13:15:00,2015-09-01 11:01:58,closed,,0.17.0,25,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/10930,b'fixed bug in DataFrame.diff - issue #10907',b'Fixed issue #10907 '
10926,103821270,jreback,jreback,2015-08-29 02:03:26,2015-08-29 13:12:39,2015-08-29 13:12:39,closed,,0.17.0,0,Bug;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10926,b'BUG: Bug in incorrection computation of .mean() on timedelta64[ns] because of overflow #9442',b'closes #9442'
10923,103777325,briangerke,briangerke,2015-08-28 19:03:58,2015-08-28 20:22:18,2015-08-28 20:18:43,closed,,,2,Bug;Groupby;Missing-data,https://api.github.com/repos/pydata/pandas/issues/10923,b'groupby.transform inconsistent behavior when grouping by columns containing NaN',"b""This is similar to #9697, which was fixed in 0.16.1. I give a (very) slightly modified example here to show some related behavior which is at least inconsistent and should probably be handled cleanly.\r\n\r\nIt's not entirely clear to me what the desired behavior is in this case; it's possible that transform should not work here at all, since it spits out unexpected values. But at minimum it seems like it should do the same thing no matter how I invoke it below.\r\n\r\nExample:\r\n```python\r\nimport numpy as np\r\ndf = pd.DataFrame({'col1':[1,1,2,2], 'col2':[1,2,3,np.nan])\r\n#Let's try grouping on 'col2', which contains a NaN.\r\n\r\n# Works and gives arguably reasonable results, with one unpredictable value\r\ndf.groupby('col2').transform(sum)['col1']\r\n\r\n# Throws an unhelpful error\r\ndf.groupby('col2')['col1'].transform(sum)\r\n```\r\nError is similar to the one encountered in the previous issue:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-54-2d4b83df6487> in <module>()\r\n----> 1 df.groupby('col2')['col1'].transform(sum)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in transform(self, func, *args, **kwargs)\r\n   2442         cyfunc = _intercept_cython(func)\r\n   2443         if cyfunc and not args and not kwargs:\r\n-> 2444             return self._transform_fast(cyfunc)\r\n   2445 \r\n   2446         # reg transform\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in _transform_fast(self, func)\r\n   2488             values = self._try_cast(values, self._selected_obj)\r\n   2489 \r\n-> 2490         return self._set_result_index_ordered(Series(values))\r\n   2491 \r\n   2492     def filter(self, func, dropna=True, *args, **kwargs):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in _set_result_index_ordered(self, result)\r\n    503             result = result.sort_index()\r\n    504 \r\n--> 505         result.index = self.obj.index\r\n    506         return result\r\n    507 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc in __setattr__(self, name, value)\r\n   2159         try:\r\n   2160             object.__getattribute__(self, name)\r\n-> 2161             return object.__setattr__(self, name, value)\r\n   2162         except AttributeError:\r\n   2163             pass\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/lib.so in pandas.lib.AxisProperty.__set__ (pandas/lib.c:42548)()\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc in _set_axis(self, axis, labels, fastpath)\r\n    273         object.__setattr__(self, '_index', labels)\r\n    274         if not fastpath:\r\n--> 275             self._data.set_axis(axis, labels)\r\n    276 \r\n    277     def _set_subtyp(self, is_all_dates):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc in set_axis(self, axis, new_labels)\r\n   2217         if new_len != old_len:\r\n   2218             raise ValueError('Length mismatch: Expected axis has %d elements, '\r\n-> 2219                              'new values have %d elements' % (old_len, new_len))\r\n   2220 \r\n   2221         self.axes[axis] = new_labels\r\n\r\nValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements\r\n\r\n```\r\n"""
10922,103773266,jreback,jreback,2015-08-28 18:36:05,2015-08-28 23:28:06,2015-08-28 23:28:06,closed,,0.17.0,1,Bug;Internals;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10922,b'BUG: Bug in clearing the cache on DataFrame.pop and a subsequent inplace op #10912',b'closes #10912'
10912,103537160,s-wakaba,jreback,2015-08-27 15:36:23,2015-08-28 23:28:06,2015-08-28 23:28:06,closed,,0.17.0,1,Bug;Internals;Prio-high,https://api.github.com/repos/pydata/pandas/issues/10912,b'inplace operation for Series causes reappearance popped column',"b'Hi, Thanks for all efforts for the great tool!\r\n\r\nI\'ve found a strange behavior like this...\r\n\r\n```python\r\nIn [1]: a = pd.DataFrame([[1,2,3],[4,5,6]], columns=[\'A\',\'B\',\'C\'], index=[\'X\',\'Y\'])\r\n\r\nIn [2]: a\r\nOut[2]: \r\n   A  B  C\r\nX  1  2  3\r\nY  4  5  6\r\n\r\nIn [3]: b = a.pop(\'B\')\r\n\r\nIn [4]: a\r\nOut[4]: \r\n   A  C\r\nX  1  3\r\nY  4  6\r\n\r\nIn [5]: b\r\nOut[5]: \r\nX    2\r\nY    5\r\nName: B, dtype: int64\r\n\r\nIn [6]: b += 1\r\n\r\nIn [7]: a # Why does a popped column ""B"" appear again?\r\nOut[7]: \r\n   A  C  B\r\nX  1  3  3\r\nY  4  6  6\r\n\r\nIn [8]: b\r\nOut[8]: \r\nX    3\r\nY    6\r\nName: B, dtype: int64\r\n```\r\n\r\nIs it a bug of pandas?\r\nThis behavior can be avoided by using `b = b+1` instead of implace operations. '"
10907,103281892,dadkins,jreback,2015-08-26 14:23:21,2015-09-01 11:02:13,2015-09-01 11:02:13,closed,,0.17.0,5,Bug;Difficulty Novice;Effort Low;Internals;Prio-medium;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10907,b'diff(axis=1) after insert results in unexpected NaN column',"b""The following code in pandas 0.16.2 demonstrates the problem:\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({'y': pd.Series([2]), 'z': pd.Series([3])})\r\n>>> df\r\n   y  z\r\n0  2  3\r\n>>> df.insert(0, 'x', 1)\r\n>>> df.diff(axis=1)\r\n    x   y  z\r\n0 NaN NaN  1\r\n```\r\nThe following workaround produces the expected result:\r\n\r\n```python\r\n>>> df.T.diff().T\r\n    x  y  z\r\n0 NaN  1  1\r\n```\r\n\r\nVersions:\r\n```python\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-400.1.1.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: C\r\n\r\npandas: 0.16.2\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.2\r\nsphinx: None\r\npatsy: 0.4.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```"""
10889,102564950,jreback,jreback,2015-08-22 20:20:30,2015-08-28 02:29:10,2015-08-28 02:29:04,closed,,0.17.0,3,Bug;Categorical;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10889,b'BUG: encoding of categoricals in hdf serialization',b'closes #10366 \r\nreplaces #10454 '
10887,102542391,kawochen,jreback,2015-08-22 14:48:11,2015-08-24 13:59:27,2015-08-24 11:37:59,closed,,0.17.0,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10887,b'BUG: GH10885 where an edge case in date_range produces an extra point',b'closes #10885 '
10885,102525488,kochede,jreback,2015-08-22 11:05:50,2015-08-24 11:37:58,2015-08-24 11:37:58,closed,,0.17.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10885,b'BUG: pd.date_range(...) produces timestamps outside the specified range',"b""Hi Pandas devs!\r\n\r\nThis call produces 3 timestamps when it has to produce only two:\r\n    pd.date_range('2005-01-12 10:00', '2005-01-12 16:00', freq='345min')\r\nthis gives:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2005-01-12 10:00:00, ..., 2005-01-12 **21:30:00**]\r\nLength: 3, Freq: 345T, Timezone: None\r\n\r\nstrangely, for another date function works as expected:\r\n    pd.date_range('2005-01-13 10:00', '2005-01-13 16:00', freq='345min')\r\nthis gives:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2005-01-13 10:00:00, 2005-01-13 **15:45:00**]\r\nLength: 2, Freq: 345T, Timezone: None\r\n\r\nCould you please take a look? Thanks!\r\n\r\nPandas versions:\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 30 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.1\r\npytz: 2015.2\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.6.7\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.9\r\npymysql: None\r\npsycopg2: None"""
10879,102409178,sinhrks,jreback,2015-08-21 15:54:55,2015-09-01 12:16:17,2015-09-01 12:06:24,closed,,0.17.0,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10879,b'BUG: DataFrame.plot may raise IndexError / show unnessesary minor ticklabels',"b'\r\nCloses #10657. Closes #10819. Both are related to the logic how ``pandas`` detects axes layout. Use current logic as much to support cases like #7457, and added error handing logic for gridspec.\r\n\r\nFollowing is an example notebook how the fix works.\r\n\r\n- https://gist.github.com/sinhrks/2d29e2e8ef26f2757e92\r\n\r\nCC @TomAugspurger @JanSchulz @heelancd @williamsmj\r\n'"
10875,102288239,soupault,jreback,2015-08-21 03:23:08,2015-08-31 12:19:06,2015-08-31 12:19:06,closed,,0.17.0,2,Bug;Difficulty Novice;Effort Low;Indexing;Prio-low,https://api.github.com/repos/pydata/pandas/issues/10875,b'BUG: Index name is not preserved during some conversions',"b""The issue is existing name of `Index` is dropped during conversion to either `TimedeltaIndex` or `DatetimeIndex`.\r\n\r\n    In [42]: vec = ['01:02:03', '01:02:04']\r\n    In [43]: i = pd.Index(vec, name='label')\r\n    In [44]: i\r\n    Out[44]: Index(['01:02:03', '01:02:04'], dtype='object', name='label')\r\n\r\n    In [45]: pd.to_datetime(i)\r\n    Out[45]: DatetimeIndex(['2015-08-21 01:02:03', '2015-08-21 01:02:04'], dtype='datetime64[ns]', freq=None, tz=None)\r\n\r\n    In [46]: pd.to_timedelta(i)\r\n    Out[46]: TimedeltaIndex(['01:02:03', '01:02:04'], dtype='timedelta64[ns]', freq=None)\r\n"""
10872,102260643,djgagne,jreback,2015-08-20 23:35:29,2015-09-01 11:49:25,2015-09-01 11:49:25,closed,,0.17.0,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10872,"b'BUG: import of maybe_convert_indices in pandas.core.index.py, #10610'",b'closes #10610 \r\n\r\nI fixed the import statement and added a test to check for proper behavior when accessing a mixed-integer index with a list of values.'
10845,101736184,soupault,jreback,2015-08-18 19:55:42,2015-08-21 14:10:00,2015-08-21 14:09:48,closed,,0.17.0,4,Bug;IO CSV;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10845,b'BUG: Error while saving DataFrame with TimedeltaIndex to .csv #10833',b'Fix in accordance with https://github.com/pydata/pandas/issues/10833'
10838,101535242,jreback,jreback,2015-08-18 00:03:35,2015-08-18 10:44:03,2015-08-18 10:44:03,closed,,0.17.0,1,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10838,b'BUG: Panel setitem with a multiindex #10360 (partial)',b'partial on #10360 '
10833,101451670,soupault,jreback,2015-08-17 15:45:54,2015-08-21 14:10:24,2015-08-21 14:10:24,closed,,0.17.0,4,Bug;IO CSV;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10833,b'BUG: Error while saving DataFrame with TimedeltaIndex to .csv',"b""I'm trying to save pd.DataFrame using `.to_csv` method.\r\nIf DataFrame has TimedeltaIndex the error is risen:\r\n\r\n    >>> dt = pd.Timedelta(seconds=10)\r\n    >>> timestamps = [dt, 2*dt, 3*dt]\r\n    >>> df = pd.DataFrame({'obs': [11,22,33]}, index=timestamps)\r\n    >>> df\r\n              obs\r\n    00:00:10   11\r\n    00:00:20   22\r\n    00:00:30   33\r\n\r\n    >>> df.to_csv('test')\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-27-9b2e5ea53beb> in <module>()\r\n    ----> 1 df.to_csv('test')\r\n    \r\n    C:\\Python34\\lib\\site-packages\\pandas\\core\\frame.py in to_csv(self, path_or_buf,\r\n    sep, na_rep, float_format, columns, header, index, index_label, mode, encoding,\r\n    quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doub\r\n    lequote, escapechar, decimal, **kwds)\r\n       1187                                      escapechar=escapechar,\r\n       1188                                      decimal=decimal)\r\n    -> 1189         formatter.save()\r\n       1190\r\n       1191         if path_or_buf is None:\r\n    \r\n    C:\\Python34\\lib\\site-packages\\pandas\\core\\format.py in save(self)\r\n       1465\r\n       1466             else:\r\n    -> 1467                 self._save()\r\n       1468\r\n       1469         finally:\r\n    \r\n    C:\\Python34\\lib\\site-packages\\pandas\\core\\format.py in _save(self)\r\n       1565                 break\r\n       1566\r\n    -> 1567             self._save_chunk(start_i, end_i)\r\n       1568\r\n       1569     def _save_chunk(self, start_i, end_i):\r\n    \r\n    C:\\Python34\\lib\\site-packages\\pandas\\core\\format.py in _save_chunk(self, start_i\r\n    , end_i)\r\n       1592                                         quoting=self.quoting)\r\n       1593\r\n    -> 1594         lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.\r\n    writer)\r\n       1595\r\n       1596 # from collections import namedtuple\r\n    \r\n    TypeError: Argument 'data_index' has incorrect type (expected numpy.ndarray, got\r\n     list)\r\n\r\nBut if Timedelta is not an Index, but a regular column, everything is OK:\r\n\r\n    >>> df['timedelta'] = df.index\r\n    >>> df.reset_index(drop=True, inplace=True)\r\n    >>> df.to_csv('test')\r\n\r\nPython 3.4.2 x64 Windows version.\r\npandas (0.16.2), numpy (1.9.2)"""
10827,101189111,evanpw,jreback,2015-08-15 16:45:00,2015-09-19 00:38:10,2015-08-15 22:22:36,closed,,0.17.0,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10827,b'Bug in read_csv when using nrows or chunksize on a file containing only a header',b'Fixes GH #9535'
10826,101181083,sinhrks,jreback,2015-08-15 14:25:10,2015-08-20 13:48:48,2015-08-20 12:54:55,closed,,0.17.0,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10826,b'BUG: Merge with empty dataframe may raise IndexError',b'Closes #10824.'
10825,101181038,evanpw,jreback,2015-08-15 14:23:49,2015-09-19 00:38:10,2015-08-17 11:09:50,closed,,0.17.0,6,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10825,"b""Fix handling of EOF in 'c' csv parser""","b""Fixes GH #10728\r\nfixes #10548.\r\n\r\nAlso fixes:\r\n* '\\r' followed by EOF should be considered a blank line\r\n* Escape character followed by EOF should produce an exception\r\n* Line containing only whitespace should be skipped if ``skip_blank_lines`` and ``delim_whitespace`` are both ``True``"""
10824,101153739,sinhrks,jreback,2015-08-15 08:57:49,2015-08-20 12:54:55,2015-08-20 12:54:55,closed,,0.17.0,0,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10824,b'BUG: Merge with empty DataFrame raise IndexError',"b""```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf1 = pd.DataFrame([], columns=['a', 'b', 'c'])\r\ndf2 = pd.DataFrame(np.random.randn(3, 3), columns=['x', 'y', 'z'])\r\n\r\n# OK\r\npd.merge(df1, df2, how='right', left_index=True, right_index=True)\r\n#      a    b    c         x         y         z\r\n# 0  NaN  NaN  NaN  0.665359  0.087728 -0.608138\r\n# 1  NaN  NaN  NaN -0.730847  0.882151  0.175648\r\n# 2  NaN  NaN  NaN  2.370834 -1.347337 -0.478547\r\n\r\npd.merge(df1, df2, how='outer', left_index=True, right_index=True)\r\n#      a    b    c         x         y         z\r\n# 0  NaN  NaN  NaN  0.665359  0.087728 -0.608138\r\n# 1  NaN  NaN  NaN -0.730847  0.882151  0.175648\r\n# 2  NaN  NaN  NaN  2.370834 -1.347337 -0.478547\r\n\r\n# NG, this should be the same as above\r\npd.merge(df1, df2, how='right', left_on='a', right_index=True)\r\n# IndexError: cannot do a non-empty take from an empty axes.\r\n\r\npd.merge(df1, df2, how='outer', left_on='a', right_index=True)\r\n# IndexError: cannot do a non-empty take from an empty axes.\r\n```\r\n\r\nThis works if both ``DataFrame`` are empty.\r\n\r\n```\r\ndf1 = pd.DataFrame([], columns=['a', 'b', 'c'])\r\ndf2 = pd.DataFrame([], columns=['x', 'y', 'z'])\r\n\r\npd.merge(df1, df2, how='right', left_on='a', right_index=True)\r\n# Empty DataFrame\r\n# Columns: [a, b, c, x, y, z]\r\n# Index: []\r\n\r\npd.merge(df1, df2, how='outer', left_on='a', right_index=True)\r\n# Empty DataFrame\r\n# Columns: [a, b, c, x, y, z]\r\n# Index: []\r\n```"""
10819,100910677,heelancd,jreback,2015-08-14 01:54:26,2015-09-01 12:06:24,2015-09-01 12:06:24,closed,,0.17.0,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10819,b'plotting with Gridspec : index is out of bounds',"b""I came across the same error described in [this stack post.](https://stackoverflow.com/questions/30623162/plotting-with-gridspec-index-is-out-of-bounds) I didn't experience the error until upgrading to 0.16.2. Like others who commented, downgrading to 0.15.2 fixed the issue. I couldn't find it reported anywhere else so I wanted to submit an issue."""
10818,100898484,francescoferroni,sinhrks,2015-08-14 00:11:59,2016-04-03 07:45:23,2016-04-03 07:45:23,closed,,Next Major Release,5,Bug;Difficulty Intermediate;Effort Medium;Sparse,https://api.github.com/repos/pydata/pandas/issues/10818,b'non-NDFFrame object error using pandas.SparseSeries.from_coo() function',"b'There appears to be an issue with the .from_coo() sparse function. If the frame is viewed, it gives a ""non-NDFFrame error"". It could potentially be due to overlapping entries on the same index (this is handled in sparse.coo_matrix by adding the entries by default).\r\n\r\nMore details here: \r\nhttp://stackoverflow.com/questions/31970070/non-ndfframe-object-error-using-pandas-sparseseries-from-coo-function\r\n\r\nExample:\r\n\r\n```\r\nimport pandas as pd \r\nimport scipy.sparse as ss \r\nimport numpy as np \r\nrow = (np.random.random(100)*100).astype(int) \r\ncol = (np.random.random(100)*100).astype(int) \r\nval = np.random.random(100)*100 \r\nsparse = ss.coo_matrix((val,(row,col)),shape=(100,100)) \r\npd.SparseSeries.from_coo(sparse)\r\nTypeError: cannot concatenate a non-NDFrame object\r\n```'"
10814,100798445,agolbin,jreback,2015-08-13 15:25:58,2015-08-13 15:43:59,2015-08-13 15:32:39,closed,,,3,Bug;Duplicate;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10814,b'pandas 0.16.2 groupby and transform does not properly work with datetime objects',"b""In the 0.16.2 version, transform returns original values, while in the 0.15.2 version it properly transforms the values into group counts.  The problem seems to be with how pandas deal with datetime objects.  \r\n\r\nThe only difference is the pandas version, and everything else is the same.\r\n\r\n----\r\n```\r\nimport pandas as pd\r\nimport datetime\r\ndf = pd.DataFrame([[pd.to_datetime(datetime.date(2015,8,10)), 'A', 10],\r\n                   [pd.to_datetime(datetime.date(2015,8,10)), 'A', 20],\r\n                   [pd.to_datetime(datetime.date(2015,8,11)), 'B', 30],\r\n                   [pd.to_datetime(datetime.date(2015,8,10)), 'B', 40],\r\n                  ], columns=['timestamp', 'name', 'value'])\r\ngrp = df.groupby(['timestamp', 'name'])['value']\r\ngrp.transform(lambda x: x.count())\r\n\r\n```\r\n----\r\n0.15.2 version\r\n----\r\n0    2\r\n1    2\r\n2    1\r\n3    1\r\nName: value, dtype: int64\r\n\r\n------\r\n0.16.2 version\r\n------\r\n\r\n0    10\r\n1    20\r\n2    30\r\n3    40\r\nName: value, dtype: int64\r\n\r\n\r\n---\r\nPandas versions \r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.3.5.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.4\r\nCython: 0.22\r\nnumpy: 1.9.1\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 2.4.1\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.1\r\npytz: 2014.9\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.2\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.6.6\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: 2.5.5 (dt dec pq3 ext)"""
10808,100631745,ajcr,jreback,2015-08-12 20:47:31,2015-08-15 12:46:01,2015-08-14 18:40:52,closed,,0.17.0,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10808,b'BUG: fix bounds for negative ints when using iloc (GH 10779)',"b'This fixes the issues raised in [GH 10779](https://github.com/pydata/pandas/issues/10779).\r\ncloses #10547 \r\n\r\nPreviously, negative integers used in `iloc` were not stopped from going beyond the bounds of the Series. For instance, if `s = pd.Series([1,2,3])` then `s.iloc[-4]` returned bytes from memory outside `s`. This will now raise an IndexError instead.\r\n\r\nAdditionally, it was not possible to access the first row (index 0) of a Series or DataFrame with a negative integer in a list. For example, `s.iloc[[-3]]` raised an IndexError instead of returning a Series containing the first row of `s`. The behaviour of `iloc` has been changed to allow this.'"
10791,100190894,sinhrks,sinhrks,2015-08-10 23:30:11,2015-08-12 13:51:53,2015-08-12 13:51:50,closed,,0.17.0,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10791,b'BUG: Index.take may add unnecessary freq attribute',"b""``Index.freq`` should raise ``AttributeError`` in non-datetime-like index. But ``Index.take`` adds ``freq=None`` and broke the behavior. \r\n\r\n```\r\nimport pandas as pd\r\nidx = pd.Index([1, 2, 3])\r\n\r\nidx.freq\r\n# AttributeError: 'Int64Index' object has no attribute 'freq'\r\n\r\nidx.take([True, False, True]).freq\r\n# None\r\n```\r\n"""
10779,99917716,sergeny,jreback,2015-08-09 18:18:52,2015-08-14 18:41:14,2015-08-14 18:41:14,closed,,0.17.0,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10779,"b""Series.iloc[ negative number ] can access memory it doesn't own or cause Segmentation Fault""","b'In other words, there is no bounds checking for Series.iloc[] with a negative argument. It just accesses whatever is in the memory there. Also a security breach.\r\n\r\nIt does appear to check on write, just not on read.\r\n\r\nPython 2.7.10 |Anaconda 2.1.0 (64-bit)| (default, May 28 2015, 17:02:03) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://binstar.org\r\n>>> import pandas as pd\r\npd>>> pd.__version__\r\n\'0.16.2\'\r\n>>> s = pd.Series([1,2,3])\r\n>>> s\r\n0    1\r\n1    2\r\n2    3\r\ndtype: int64\r\n>>> s.iloc[-2]\r\n2\r\n>>> s.iloc[-4]\r\n33\r\n>>> s.iloc[-12345]\r\n0\r\n>>> s.iloc[-123123123123123]\r\nSegmentation fault (core dumped)\r\n'"
10776,99871274,kawochen,jreback,2015-08-09 06:52:20,2015-08-12 13:44:37,2015-08-12 13:44:32,closed,,0.17.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10776,b'BUG: GH10747 where cast_to_nanoseconds from NaT fails',b'related to first of #10747 \r\n'
10757,99424029,jbuyl,jreback,2015-08-06 12:27:07,2015-08-06 15:17:07,2015-08-06 15:16:59,closed,,0.17.0,4,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/10757,b'BUG: Fix dtypes order when ordering is different from original file in pandas.io.stata.read_stata',"b'This fixes http://stackoverflow.com/questions/31783392/pandas-adding-columns-to-filter-will-mess-with-data-structure\r\nThe dtypes of the DataFrame returned by read_stata have the same order as the original stata file, even if columns are specified with a different order.'"
10747,99020384,jreback,jreback,2015-08-04 17:37:50,2015-08-24 11:36:49,2015-08-24 11:36:49,closed,,0.17.0,1,Bug;Difficulty Intermediate;Effort Low;IO JSON;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10747,b'JSON: handling missing values in date coercion',"b'xref https://github.com/pydata/pandas/issues/10746\r\n\r\n```\r\nIn [1]: data = \'\'\'\r\n   ...: [{""id"": 1, ""timestamp"": 1036713600000},\r\n   ...:  {""id"": 2}]\r\n   ...: \'\'\'\r\n```\r\n\r\nafter #10776 this is ok\r\n```\r\nIn [2]: pd.read_json(StringIO(data), orient=\'records\', dtype={\'id\': \'int32\', \'timestamp\': \'datetime64[ms]\'})\r\nOut[2]: \r\n   id     timestamp\r\n0   1  1.036714e+12\r\n1   2           NaN\r\n\r\nIn [2]: pd.read_json(StringIO(data), orient=\'records\', dtype={\'id\': \'int32\', \'timestamp\': \'datetime64[ms]\'})\r\nOut[10]: \r\n   id  timestamp\r\n0   1 2002-11-08\r\n1   2        NaT\r\n```\r\n\r\nstill open\r\n```\r\nIn [7]: pd.read_json(StringIO(data), orient=\'records\', date_unit=\'ms\')\r\nOut[7]: \r\n   id     timestamp\r\n0   1  1.036714e+12\r\n1   2           NaN\r\n```'"
10741,98834450,warmlogic,jreback,2015-08-03 20:39:39,2015-08-18 10:51:57,2015-08-18 10:51:57,closed,,0.17.0,4,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10741,b'BUG: Joining single-index DataFrame to multiindex DF incorrect for how=left and how=right',"b""When joining DataFrames where the calling frame is a multiindex DF and the input frame is a single-index DF, `how='left'` and `how='right'` produce results that should be swapped (i.e., `'left'` returns what `'right'` should return, and vice versa). To give a single example using `how='left'`:\r\n\r\n```\r\ndf1 = pd.DataFrame([['a', 'x', 0.471780], ['a','y', 0.774908], ['a', 'z', 0.563634],\r\n                    ['b', 'x', -0.353756], ['b', 'y', 0.368062], ['b', 'z', -1.721840],\r\n                    ['c', 'x', 1], ['c', 'y', 2], ['c', 'z', 3],\r\n                   ],\r\n                   columns=['first', 'second', 'value1']\r\n                   ).set_index(['first', 'second'])\r\ndf2 = pd.DataFrame([['a', 10], ['b', 20]],\r\n                   columns=['first', 'value2']).set_index(['first'])\r\n\r\nprint(df1.join(df2, how='left'))\r\n```\r\n\r\nExpected (for `how='left'`):\r\n```\r\n                value1  value2\r\nfirst second                  \r\na     x       0.471780      10\r\n      y       0.774908      10\r\n      z       0.563634      10\r\nb     x      -0.353756      20\r\n      y       0.368062      20\r\n      z      -1.721840      20\r\nc     x       1.000000     NaN\r\n      y       2.000000     NaN\r\n      z       3.000000     NaN\r\n```\r\n\r\nActual (for `how='left'`):\r\n```\r\n                value1  value2\r\nfirst second                  \r\na     x       0.471780      10\r\n      y       0.774908      10\r\n      z       0.563634      10\r\nb     x      -0.353756      20\r\n      y       0.368062      20\r\n      z      -1.721840      20\r\n```\r\n\r\nHowever, the correct behavior occurs if the single-index DF is the calling frame and the multiindex DF is the input frame (`df2.join(df1, how='left')`). Behavior for `how='inner'` and `how='outer'` is correct in both situations.\r\n\r\npandas version 0.16.2"""
10740,98828576,ringw,jreback,2015-08-03 20:05:56,2015-08-28 18:17:41,2015-08-28 18:17:41,closed,,0.17.0,12,Algos;Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10740,"b'ENH: Fixed DF.apply for functions returning a dict, #8735'","b'closes #8735 \r\n\r\nPreviously, when the function argument to DataFrame.apply returned a dict, the reduction code would mistake its ""values"" property for the values of a Pandas Series, and return a Series of ""values"" instance methods. The new check ensures that the ""values"" property is an np.ndarray.\r\n\r\nPrevious behavior:\r\n\r\n     In [1]: A = DataFrame([[\'foo\', \'bar\'], [\'spam\', \'eggs\']])\r\n\r\n     In [2]: A.apply(lambda c: c.to_dict(), reduce=True)\r\n     Out[2]: \r\n     0    <built-in method values of dict object at 0x7f...\r\n     1    <built-in method values of dict object at 0x7f...\r\n     dtype: object\r\n\r\nNew behavior:\r\n\r\n     In [1]: A = DataFrame([[\'foo\', \'bar\'], [\'spam\', \'eggs\']])\r\n\r\n     In [2]: A.apply(lambda c: c.to_dict(), reduce=True)\r\n     Out[2]:\r\n     0    {0: u\'foo\', 1: u\'spam\'}\r\n     1    {0: u\'bar\', 1: u\'eggs\'}\r\n     dtype: object\r\n\r\nIf reduce=False, the result is a DataFrame (this did not change):\r\n\r\n     In [3]: A.apply(lambda c: c.to_dict(), reduce=False)\r\n     Out[3]:\r\n           0     1\r\n     0   foo   bar\r\n     1  spam  eggs'"
10739,98820689,jbuyl,jbuyl,2015-08-03 19:25:04,2015-08-06 12:17:42,2015-08-06 12:17:29,closed,,0.17.0,8,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/10739,b'BUG: Fix dtypes order when ordering is different from original file in pandas.io.stata.read_stata',"b'This fixes http://stackoverflow.com/questions/31783392/pandas-adding-columns-to-filter-will-mess-with-data-structure\r\nThe dtypes of the DataFrame returned by `read_stata` have the same order as the original stata file, even if `columns` are specified with a different order. '"
10732,98726501,ruidc,jreback,2015-08-03 11:49:00,2015-08-04 11:47:19,2015-08-04 11:47:19,closed,,0.17.0,14,Bug;Msgpack,https://api.github.com/repos/pydata/pandas/issues/10732,b'DataFrame msgpack objects written from linux cannot be read on windows',"b'eg. on linux:\r\n```\r\ndf2 = pandas.DataFrame([[\'100\']], index=[1], columns=[\'v\'])\r\ndf2.to_msgpack(r""/mnt/C/temp/p2.msgpack"")\r\n```\r\nthen on windows:\r\n```\r\ndf = pandas.read_msgpack(r""C:\\temp\\p2.msgpack"")\r\nValueError: Shape of passed values is (1, 1), indices imply (1, 2)\r\n```'"
10724,98605407,ajcr,jreback,2015-08-02 12:05:21,2015-08-04 17:58:00,2015-08-03 22:03:11,closed,,0.17.0,5,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10724,b'BUG: pd.unique should respect datetime64 and timedelta64 dtypes (GH9431)',"b'To fix [GH9431](https://github.com/pydata/pandas/issues/9431).\r\n\r\nPreviously `pd.unique` would return an array of  `object` dtype when passed a 1D array, Series or Index with a `datetime64` or `timedelta64` dtype. This PR should remedy that behaviour.'"
10723,98604288,IamGianluca,jreback,2015-08-02 11:42:29,2015-09-02 11:54:22,2015-09-02 11:54:14,closed,,0.17.0,18,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10723,b'BUG: concat of Series w/o names #10698',"b""closes #10698 \r\n\r\nLet the result of 'concat' to inherit the parent Series' names. The Series' name (if present) will be used as the resulting DataFrame column name. When only one of the Series has a valid name, the resulting DataFrame will inherit the name only, and use a column name for the other columns the column index value."""
10718,98541976,sinhrks,sinhrks,2015-08-01 14:48:50,2015-08-08 19:44:00,2015-08-08 19:43:55,closed,,0.17.0,3,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10718,"b""BUG: Categorical doesn't show tzinfo properly""","b""Closes #10713.\r\n\r\n- [x] Check timedelta\r\n- [x] Refactor not to import ``DatetimeIndex`` and ``PeriodIndex`` if possible.\r\n- [x] Fix ``categories`` line break location\r\n- [x] Check ``Categorical`` ``Series`` formatting\r\n- [x] Check ``Categorical`` ``DataFrame`` formatting \r\n- [x] Fix ``CategoricalIndex`` formatting \r\n\r\n## CategoricalIndex problem (befor this PR)\r\n\r\n```\r\nimport pandas as pd\r\nidx = pd.period_range('2011-01-01 09:00', freq='H', periods=5)\r\nidx = pd.Index(pd.Categorical(idx))\r\nidx\r\n# CategoricalIndex([359409, 359410, 359411, 359412, 359413], categories=[2011-01-01 09:00, 2011-01-01 10:00, 2011-01-01 11:00, 2011-01-01 12:00, 2011-01-01 13:00], ordered=False, dtype='category')\r\n```\r\n\r\n## Format differences (after this PR)\r\n\r\nI left ordered category representation of ``CategoricalIndex`` as below because it has ``ordered`` repr separately.\r\n\r\n```\r\npd.Categorical([1, 2, 3], ordered=True)\r\n# [1, 2, 3]\r\n# Categories (3, int64): [1 < 2 < 3]\r\n\r\npd.Series(pd.Categorical([1, 2, 3], ordered=True))\r\n# 0    1\r\n# 1    2\r\n# 2    3\r\n# dtype: category\r\n# Categories (3, int64): [1 < 2 < 3]\r\n\r\npd.CategoricalIndex([1, 2, 3], ordered=True)\r\n# CategoricalIndex([1, 2, 3], categories=[1, 2, 3], ordered=True, dtype='category')\r\n```"""
10716,98498248,sinhrks,jreback,2015-08-01 01:56:20,2015-08-18 11:22:55,2015-08-18 10:51:57,closed,,0.17.0,1,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10716,b'BUG: Series.align with MultiIndex may be inverted',"b""Closes #10665. Closes #10741.\r\n\r\nI'm adding some more tests for ``Index.join``, and found a skeptic behavior.\r\n\r\nFor ``right`` join, all levels / labels of right side remains (OK).\r\n\r\n```\r\nimport pandas as pd\r\nmidx = pd.MultiIndex.from_product([[0, 1], [0, 1, 2]], names=['a', 'b'])\r\nidx = pd.Index([1, 2 ,3], name='b')\r\n\r\nidx.join(midx, how='right')\r\n# MultiIndex(levels=[[0, 1], [0, 1, 2]],\r\n#            labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\r\n#            names=[u'a', u'b'])\r\nidx.join(midx, how='right').values\r\n# array([(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)], dtype=object)\r\n```\r\n\r\nFor ``left`` join, all levels are remain but not used as levels. Thus, values only on left side is dropped (NG).\r\n\r\n```\r\nidx.join(midx, how='left')\r\n# MultiIndex(levels=[[0, 1], [1, 2, 3]],\r\n#            labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\r\n#            names=[u'a', u'b'])\r\nidx.join(midx, how='left').values\r\narray([(0, 1), (0, 2), (1, 1), (1, 2)], dtype=object)\r\n```\r\n\r\nI understand the last result must be the following. If my understanding is correct, I'll include the fix to this PR.\r\n\r\n```\r\nidx.join(midx, how='left')\r\n# MultiIndex(levels=[[0, 1], [1, 2, 3]],\r\n#            labels=[[0, 0, 1, 1], [0, 1, 2, 0, 1, 2]],\r\n#            names=[u'a', u'b'])\r\nidx.join(midx, how='left').values\r\n# array([(0, 1), (0, 2), (0, 3), (1, 1), (1, 2), (1, 3)], dtype=object)\r\n```\r\n"""
10713,98418455,sinhrks,sinhrks,2015-07-31 15:54:00,2015-08-08 19:43:55,2015-08-08 19:43:55,closed,,0.17.0,4,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10713,"b""BUG/API: Categorical doesn't support categories with tz""","b""Creating ``Categorical`` from ``DatetimeIndex`` with tz results in GMT.\r\n\r\n```\r\nimport pandas as pd\r\nidx = pd.date_range('2011-01-01', periods=5, freq='M', tz='US/Eastern')\r\nidx\r\n# DatetimeIndex(['2011-01-31 00:00:00-05:00', '2011-02-28 00:00:00-05:00',\r\n#                '2011-03-31 00:00:00-04:00', '2011-04-30 00:00:00-04:00',\r\n#                '2011-05-31 00:00:00-04:00'],\r\n#               dtype='datetime64[ns]', freq='M', tz='US/Eastern')\r\n\r\npd.Categorical(idx)\r\n# [2011-01-31 05:00:00, 2011-02-28 05:00:00, 2011-03-31 04:00:00, 2011-04-30 04:00:00, 2011-05-31 # 04:00:00]\r\n# Categories (5, datetime64[ns]): [2011-01-31 05:00:00, 2011-02-28 05:00:00, 2011-03-31 04:00:00\r\n#                                 , 2011-04-30 04:00:00, 2011-05-31 04:00:00]\r\n```"""
10712,98417470,sinhrks,jreback,2015-07-31 15:50:00,2016-02-27 15:09:04,2016-02-27 15:09:04,closed,,0.18.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10712,b'BUG: Series.dt ops reset name',"b""``DatetimeIndex.tz_localize`` preserves name.\r\n\r\n```\r\nimport pandas as pd\r\nidx = pd.date_range('2011-01-01', periods=5, freq='M', name='x')\r\nidx\r\n# DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-30', '2011-05-31'],\r\n#               dtype='datetime64[ns]', name=u'x', freq='M', tz=None)\r\nidx.tz_localize('US/Eastern')\r\n# DatetimeIndex(['2011-01-31 00:00:00-05:00', '2011-02-28 00:00:00-05:00',\r\n#                '2011-03-31 00:00:00-04:00', '2011-04-30 00:00:00-04:00',\r\n#                '2011-05-31 00:00:00-04:00'],\r\n#               dtype='datetime64[ns]', name=u'x', freq='M', tz='US/Eastern')\r\n```\r\n\r\nBut ``Series`` doesn't. I don't check all methods  which can return ``Series`` yet.\r\n\r\n```\r\ns = pd.Series(idx)\r\ns\r\n# 0   2011-01-31\r\n# 1   2011-02-28\r\n# 2   2011-03-31\r\n# 3   2011-04-30\r\n# 4   2011-05-31\r\n# Name: x, dtype: datetime64[ns]\r\n\r\n# NG, name is reset\r\ns2 = s.dt.tz_localize('US/Eastern')\r\ns2\r\n# 0    2011-01-31 00:00:00-05:00\r\n# 1    2011-02-28 00:00:00-05:00\r\n# 2    2011-03-31 00:00:00-04:00\r\n# 3    2011-04-30 00:00:00-04:00\r\n# 4    2011-05-31 00:00:00-04:00\r\n# dtype: object\r\n\r\n# NG, name is reset\r\ns2.name = 'x'\r\ns2.dt.tz_convert('US/Pacific')\r\n# 0    2011-01-30 21:00:00-08:00\r\n# 1    2011-02-27 21:00:00-08:00\r\n# 2    2011-03-30 21:00:00-07:00\r\n# 3    2011-04-29 21:00:00-07:00\r\n# 4    2011-05-30 21:00:00-07:00\r\n# dtype: object\r\n\r\n# NG, name is reset\r\ns.dt.normalize()\r\n# 0   2011-01-31\r\n# 1   2011-02-28\r\n# 2   2011-03-31\r\n# 3   2011-04-30\r\n# 4   2011-05-31\r\n# dtype: datetime64[ns]\r\n```"""
10703,98157572,jreback,jreback,2015-07-30 12:25:15,2015-07-30 14:36:19,2015-07-30 14:36:19,closed,,0.17.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10703,b'BUG: Bug in Index construction with a mixed list of tuples #10697)',b'closes #10697 '
10698,97988683,hcontrast,jreback,2015-07-29 17:34:42,2015-09-02 11:54:14,2015-09-02 11:54:14,closed,,0.17.0,4,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10698,b'Loss of series name attributes in pd.concat ',"b""`pd.concat` nicely preserves name attributes when given a list of series. However, this only works if all series have the name attribute set, otherwise all names are lost:\r\n    \r\n    In [1]: import pandas as pd\r\n    In [2]: pd.__version__\r\n    Out[2]: '0.16.2'\r\n\r\n    In [3]: foo = pd.Series([1,2], name='foo')\r\n    In [4]: bar = pd.Series([1,2])\r\n    \r\n    In [6]: pd.concat([foo, bar], 1)\r\n    Out[6]: \r\n       0  1\r\n    0  1  1\r\n    1  2  2\r\n\r\nNot sure this qualifies as a bug.\r\n\r\n """
10697,97925920,Alexis-benoist,jreback,2015-07-29 12:37:26,2015-07-30 14:36:19,2015-07-30 14:36:19,closed,,0.17.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10697,b'BUG: sorting with tuples and Multi-Index creation',"b'Hello,\r\n\r\nI\'m using python 2.7.9 and pandas 0.15.2.\r\nI\'m trying to concatenate two DataFrames (to_concat is a list of 2 DataFrames) and I have the following error.\r\n```\r\n  File ""/Users/alexisbenoist/Documents/CoTH/AFDS/app/core.py"", line 67, in train_dataflow\r\n    df = pd.concat(to_concat)\r\n  File ""/Users/alexisbenoist/.virtualenvs/afds/lib/python2.7/site-packages/pandas/tools/merge.py"", line 725, in concat\r\n    return op.get_result()\r\n  File ""/Users/alexisbenoist/.virtualenvs/afds/lib/python2.7/site-packages/pandas/tools/merge.py"", line 896, in get_result\r\n    mgrs_indexers, self.new_axes, concat_axis=self.axis, copy=self.copy)\r\n  File ""/Users/alexisbenoist/.virtualenvs/afds/lib/python2.7/site-packages/pandas/core/internals.py"", line 4046, in concatenate_block_managers\r\n    for placement, join_units in concat_plan]\r\n  File ""/Users/alexisbenoist/.virtualenvs/afds/lib/python2.7/site-packages/pandas/core/internals.py"", line 4135, in concatenate_join_units\r\n    empty_dtype, upcasted_na = get_empty_dtype_and_na(join_units)\r\n  File ""/Users/alexisbenoist/.virtualenvs/afds/lib/python2.7/site-packages/pandas/core/internals.py"", line 4124, in get_empty_dtype_and_na\r\n    raise AssertionError(""invalid dtype determination in get_concat_dtype"")\r\nAssertionError: invalid dtype determination in get_concat_dtype\r\n```\r\n\r\nI put the pickle of the DataFrames in a [gist](https://gist.github.com/Alexis-benoist/ba7f38deccc70d46c32f) using (I can upload it somewhere else if it\'s more convenient).\r\n \r\n```\r\nimport pickle\r\nwith open(path, \'w+\') as f:\r\n    pickle.dump(to_concat, f)\r\n```\r\n\r\nI feel that I\'m doing something wrong or it\'s a bug from pandas because if do: \r\n```\r\npd.DataFrame(np.concatenate(to_concat), columns=to_concat[0].columns.tolist())\r\n```\r\nIt gives a result which seems logical.\r\n\r\nAny input is welcome.\r\n\r\nCheers,\r\nAlexis.\r\n\r\n'"
10692,97792697,Pekka4444,jreback,2015-07-28 20:16:54,2016-06-19 06:14:46,2015-09-10 22:11:55,closed,,0.17.0,7,Bug;Difficulty Intermediate;Effort Low;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10692,b'setting with enlargement fails for large DataFrames',"b""Setting with enlargement seems to fail for DataFrames longer than `10**6 - 1`\r\n`10**6` seems to be the exact treshold for me. That and anything bigger fails. Anything smaller works.\r\n\r\nExample:\r\n\r\n    import pandas as pd\r\n\r\n    #works \r\n    X = pd.DataFrame(dict(x=range(10**6-1)))\r\n    X.loc[len(X)] = 42\r\n    \r\n    #doesn't work\r\n    Y = pd.DataFrame(dict(y=range(10**6)))\r\n    Y.loc[len(Y)] = 42   \r\n\r\n    >>> IndexError: index out of bounds\r\n\r\npd.show_versions() returns:\r\n\r\n\tINSTALLED VERSIONS\r\n\t------------------\r\n\tcommit: None\r\n\tpython: 2.7.9.final.0\r\n\tpython-bits: 64\r\n\tOS: Windows\r\n\tOS-release: 7\r\n\tmachine: AMD64\r\n\tprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\n\tbyteorder: little\r\n\tLC_ALL: None\r\n\tLANG: fi_FI\r\n\r\n\tpandas: 0.16.2\r\n\tnose: 1.3.6\r\n\tCython: 0.22\r\n\tnumpy: 1.9.2\r\n\tscipy: 0.15.1\r\n\tstatsmodels: 0.6.1\r\n\tIPython: 3.0.0\r\n\tsphinx: 1.2.3\r\n\tpatsy: 0.3.0\r\n\tdateutil: 2.4.2\r\n\tpytz: 2015.4\r\n\tbottleneck: None\r\n\ttables: 3.1.1\r\n\tnumexpr: 2.3.1\r\n\tmatplotlib: 1.4.3\r\n\topenpyxl: 1.8.5\r\n\txlrd: 0.9.3\r\n\txlwt: 0.7.5\r\n\txlsxwriter: 0.6.7\r\n\tlxml: 3.4.2\r\n\tbs4: 4.3.2\r\n\thtml5lib: None\r\n\thttplib2: None\r\n\tapiclient: None\r\n\tsqlalchemy: 0.9.9\r\n\tpymysql: None\r\n\tpsycopg2: None"""
10679,97337966,jvkersch,jreback,2015-07-26 18:00:31,2015-09-04 19:14:29,2015-09-04 12:15:22,closed,,0.17.0,13,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/10679,b'BUG: Use stable algorithm for _nanvar.',"b""closes #10242\r\n\r\nThis PR replaces the sum-of-squares algorithm used to compute the variance by a more stable algorithm. The algorithm here is essentially the same as used in numpy 1.8 and up, and I've added a TODO to replace the implementation with a direct call to numpy when that version is the default.\r\n\r\nSomewhat counter to the discussion in #10242, I chose not to go with the Welford algorithm, for two reasons: numpy, and the fact that `_nanvar` needs to be able to deal with arrays of different shape, which is tricky to get right in Cython."""
10675,97322560,scari,jreback,2015-07-26 14:26:58,2015-10-12 02:45:53,2015-09-10 11:59:39,closed,,,12,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10675,b'BUG: #10645 in using MultiIndex.__contains__',b'This PR fix a BUG #10645 \r\n@sinhrks would you review my PR?'
10672,97236823,TomAugspurger,jreback,2015-07-25 16:25:03,2015-08-26 01:30:07,2015-08-26 01:30:07,closed,,0.17.0,10,Bug;Build;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10672,b'BUG: DataFrame.to_hdf segfault',"b""```\r\nimport pandas as pd\r\n\r\n# import dask.dataframe as dd\r\nN = 5000\r\ndf = pd.DataFrame({'A': [1, 2] * N, 'B': [3, 4] * N, 'C': ['a', 'b'] * N})\r\n\r\ndf.to_hdf('foo.h5', '/data', format='table', mode='a')\r\n\r\nfor _ in range(3):\r\n    df.to_hdf('foo.h5', '/data', format='table', mode='a', append=True)\r\n```\r\n\r\nThis actually works when `N` is smaller, perhaps something with a pytables chunksize"""
10665,97011645,gdementen,jreback,2015-07-24 09:21:46,2015-08-18 10:51:57,2015-08-18 10:51:57,closed,,0.17.0,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10665,"b""BUG: Series.align(join='left') and join='right' are inverted""","b""join='left' does a right join and 'right' does a left join\r\n\r\nSee the notebook for details\r\nhttps://nbviewer.jupyter.org/gist/gdementen/fa4d388993ee981b5698"""
10663,96849366,zbak,jreback,2015-07-23 16:13:01,2015-07-24 17:22:08,2015-07-24 17:21:46,closed,,Next Major Release,5,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/10663,b'pandas.tslib.normalize_date()  can put Timestamp into inconsistent state',"b""### Code and Error description\r\nIf I try to normalize a date on daylight saving date day, the normalize_date() can put a Timestamp object into inconsistent state.\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> original_midnight = pd.Timestamp('20121104', tz='US/Eastern')\r\n>>> original_midday = pd.Timestamp('20121104T120000', tz='US/Eastern')\r\n\r\n>>> str(pd.tslib.normalize_date(original_midday))\r\n'2012-11-04 00:00:00-05:00'\r\n\r\n>>> str(original_midnight)\r\n'2012-11-04 00:00:00-04:00'\r\n\r\n>>> pd.tslib.normalize_date(original_midday) == original_midnight\r\nFalse\r\n\r\n>>> pd.tslib.normalize_date(original_midday).tzinfo\r\n<DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>\r\n>>> original_midnight.tzinfo\r\n<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>\r\n```\r\naccording to the [implementation](https://github.com/pydata/pandas/blob/master/pandas/tslib.pyx#L4303-L4317) It only replaces the with zeroes the time part.\r\n\r\nI believe either the replace should be timezone aware or the function should look like this:\r\n```python\r\nif PyDateTime_Check(dt):\r\n    return pd.Timestamp(dt.date(), tz=dt.tz)\r\nelif PyDate_Check(dt):\r\n    return datetime(dt.year, dt.month, dt.day)\r\nelse:\r\n    raise TypeError('Unrecognized type: %s' % type(dt))\r\n```\r\n\r\n#### INSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-44-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.0\r\nCython: 0.22.1\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.0\r\nsphinx: None\r\npatsy: 0.2.1\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 0.8.0\r\ntables: 3.0.0\r\nnumexpr: 2.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None"""
10660,96765091,jakbaum,jreback,2015-07-23 08:55:09,2015-10-09 13:03:57,2015-10-09 13:03:57,closed,,0.17.1,11,Bug;Difficulty Novice;Effort Low;IO LaTeX,https://api.github.com/repos/pydata/pandas/issues/10660,b'BUG: to_latex() output broken when the index has a name',"b""Hey folks,\r\n\r\nI posted this on [SO](https://stackoverflow.com/questions/31582114/groupby-to-latex-output-broken) and was asked to file a report here as well. \r\n\r\nI'm trying to export `pandas.DataFrame.describe()` to `LaTex` using the `to_latex()`-method. This works all fine as long as I don't apply the `groupby()`-method beforehand. With a grouped DataFrame, the first row has no values, even though its label is `count`. Note that the first row of a grouped dataframe is used to mark down the variable used for grouping in iPython notebook.\r\n\r\nI'm using pandas 0.16.2, python 3.\r\nIs this a bug or am I doing something wrong? \r\n\r\nCheers,\r\nJakob\r\n\r\nHere some examples:\r\n\r\nWithout `groupby`:\r\n\r\n    \\begin{tabular}{lr}\r\n    \\toprule\r\n    {} &    IS\\_FEMALE \\\\\r\n    \\midrule\r\n    count &  2267.000000 \\\\\r\n    mean  &     0.384649 \\\\\r\n    ...\r\n    ...\r\n    75\\%   &     1.000000 \\\\\r\n    max   &     1.000000 \\\\\r\n    \\bottomrule\r\n    \\end{tabular}\r\n\r\n\r\n[![enter image description here][1]][2]\r\n\r\nWith `groupby`:\r\n\r\n    \\begin{tabular}{llr}\r\n    \\toprule\r\n      &       &    IS\\_FEMALE \\\\\r\n    \\midrule\r\n    0 & count &              \\\\     % <-- note missing value here\r\n      & mean &  1134.000000 \\\\\r\n      & std &     0.554674 \\\\\r\n    ...\r\n    ...\r\n      & 75\\% &     0.000000 \\\\\r\n      & max &     0.000000 \\\\\r\n    \\bottomrule\r\n    \\end{tabular}\r\n\r\n[![enter image description here][3]][4]\r\n\r\nOutput in the notebook:\r\n\r\n[![enter image description here][5]][5]\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/gK10V.png\r\n  [2]: http://i.stack.imgur.com/gK10V.png\r\n  [3]: http://i.stack.imgur.com/lt9z9.png\r\n  [4]: http://i.stack.imgur.com/lt9z9.png\r\n  [5]: http://i.stack.imgur.com/wT8Tp.png"""
10659,96696871,chris-b1,jreback,2015-07-23 00:21:58,2015-08-17 23:42:53,2015-07-23 11:16:39,closed,,0.17.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/10659,b'BUG: #10565 Series.name lost in rolling_* funcions',b'Addresses #10565\r\n'
10658,96677732,ajcr,jreback,2015-07-22 22:13:01,2015-07-27 20:45:02,2015-07-25 14:52:12,closed,,0.17.0,10,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/10658,b'BUG: GH9428 promote string dtype to object dtype for empty DataFrame',"b'Addresses [GH9428](https://github.com/pydata/pandas/issues/9428).\r\n\r\nWhen constructing an empty DataFrame with a string dtype (e.g. `str`, `np.unicode_`, `U5`), the dtype is now promoted to the `object` dtype. This is now consistent with Series and avoids the confusing behaviour described in the original issue.'"
10656,96644016,fdellavedova,jreback,2015-07-22 19:20:42,2015-09-13 16:16:13,2015-09-13 16:16:13,closed,,0.17.0,7,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/10656,b'BUG: #10652 google-api-python-client minimum version check',b'Fixes [bug](https://github.com/pydata/pandas/issues/10652) with version check for google-api-python-client'
10655,96621085,ebolyen,jreback,2015-07-22 17:23:09,2015-11-10 01:21:15,2015-11-10 01:21:15,closed,,Next Major Release,5,Bug;Dtypes;Sparse,https://api.github.com/repos/pydata/pandas/issues/10655,b'BUG: to_dense now preserves dtype in SparseArray',b'Also fixes values and get_values.\r\n\r\nfixes #10648'
10652,96549470,fdellavedova,jreback,2015-07-22 11:50:00,2015-09-13 20:20:52,2015-09-13 20:20:52,closed,,,1,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/10652,b'google-api-python-client Version Check is broken',"b""Hi All, I cannot use io.gbq due to an issue with the version check of the installed google library.\r\n\r\n    >>> pkg_resources.get_distribution('google-api-python-client').version\r\n    '1.2'\r\n\r\nThe [offending line](https://github.com/pydata/pandas/blob/master/pandas/io/gbq.py#L29) executes the version check as follows:\r\n\r\n    >>> pkg_resources.get_distribution('google-api-python-client').version < '1.2.0'\r\n    True\r\n\r\nbut\r\n\r\n    >>> pkg_resources.get_distribution('google-api-python-client').version < '1.2'\r\n    False\r\n"""
10648,96378563,ebolyen,jreback,2015-07-21 18:02:20,2016-04-03 14:24:21,2016-04-03 14:24:21,closed,,0.18.1,3,Bug;Dtypes;Sparse,https://api.github.com/repos/pydata/pandas/issues/10648,b'to_dense does not preserve dtype in SparseArray',"b""This isn't a huge deal, but it seems a little odd:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: a = pd.SparseArray([True, False, False, False, True], fill_value=False, dtype=bool)\r\n\r\nIn [3]: a\r\nOut[3]: \r\n[True, False, False, False, True]\r\nFill: False\r\nIntIndex\r\nIndices: array([0, 4], dtype=int32)\r\n\r\nIn [4]: a.dtype\r\nOut[4]: dtype('bool')\r\n\r\nIn [5]: d = a.to_dense()\r\n\r\nIn [6]: d\r\nOut[6]: array([ 1.,  0.,  0.,  0.,  1.])\r\n\r\nIn [7]: d.dtype\r\nOut[7]: dtype('float64')\r\n```\r\n\r\nI would have expected `d` to retain the dtype of `bool`. I can cast down, but I am still wasting 7 bytes per element in the process. """
10645,96317929,iyer,jreback,2015-07-21 13:43:35,2015-09-10 22:11:55,2015-09-10 22:11:55,closed,,0.17.0,1,Bug;Difficulty Intermediate;Effort Low;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10645,b'MultiIndex __contains__()/in operator throws an IndexError for large multiindices',"b'# For a small multiindex the in operator works\r\nmi = pd.MultiIndex.from_arrays([range(100),range(100)])\r\n(1000001,0) in mi\r\n\r\n# For a large multiindex the in operator throws an IndexError\r\nmi = pd.MultiIndex.from_arrays([range(1000000),range(1000000)])\r\n(1000001,0) in mi\r\n\r\n'"
10644,96248884,schettino72,jreback,2015-07-21 07:26:53,2015-07-24 13:50:22,2015-07-24 13:50:18,closed,,0.17.0,2,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10644,"b'BUG: (GH10408, GH10412) in vectorised setting of timestamp columns'",b'closes #10408  \r\ncloses #10412 \r\n\r\nFix setting values with python datetime.date and numpy datetime64.'
10642,96237559,captainsafia,jreback,2015-07-21 06:13:46,2015-07-21 17:33:46,2015-07-21 10:53:01,closed,,0.17.0,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10642,b'BUG: Fixed typo-related bug to resolve #9266',b'Resubmitting pull request #10576 to resolve issue #9266 .\r\n\r\n@jreback \xa1\xaa This should be good to merge.'
10626,95892610,kawochen,jreback,2015-07-19 06:54:19,2016-07-21 15:45:17,2015-11-10 01:27:48,closed,,No action,13,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/10626,b'BUG: GH10536 in concat for SparseSeries',"b""To address #10536, but it's clearly not enough. What should be done for `SparseSeries` of different `kind`s and different `fill` values?\r\n"""
10619,95870435,agijsberts,jreback,2015-07-19 00:11:26,2015-07-31 06:42:06,2015-07-30 15:49:15,closed,,0.17.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10619,b'Fix bug in outer_indexer where the special case of an empty right array resulted in bogus return data.',b'Fixes bug described in issue #10618.'
10618,95865650,agijsberts,jreback,2015-07-18 22:45:29,2015-08-26 01:30:47,2015-08-26 01:30:47,closed,,0.17.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10618,b'BUG: incorrect empty list handling in Cython outer_join_indexer',"b'The low level Cython routine for outer_join_indexer uses the wrong size to handle the special case that the right array is empty (see: https://github.com/pydata/pandas/blob/master/pandas/src/generate_code.py#L2134). The fix is simple: if the right array is empty (`nright == 0`), it should of course loop and copy the left array (`for i in range(nleft)`).\r\n\r\nDue to this bug, the merged index and both indexers are never written when the right array is empty:\r\n```\r\nIn [91]: from pandas.algos import outer_join_indexer_float64\r\n\r\nIn [92]: outer_join_indexer_float64(array([1.,2.,3.]), array([]))\r\nOut[92]: \r\n(array([  6.92981993e-310,   2.56210349e-316,   4.27255699e+180]),\r\n array([140261116804248, 140261116804248, 140260374492336]),\r\n array([140261116804232, 140261116804232,        50534992]))\r\n```\r\n`Index.union` already preempts this special case separately, so this bug will only affect a handful of people that interact directly with the Cython algos. If desired, I could prepare a PR tomorrow, though it might be overkill for such a small change.'"
10612,95732225,rubyfin,jreback,2015-07-17 19:57:13,2015-07-17 19:59:11,2015-07-17 19:59:02,closed,,,1,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10612,"b""object of type 'bool' has no len() on read_table() with index_col=False""","b""If you try to read a csv file into a dataframe, and the csv file has just the header and no rows, it throws 'object of type 'bool' has no len()'. This issue does not seem to exist in 0.16.1.\r\n\r\nThe sample csv file is blank:\r\nid,strategyId,symbol,side,target,time,reason,epochTime\r\n\r\nCode that causes the issue:\r\n```\r\nimport pandas as pd\r\ndf = pd.read_table(path_to_csv, sep=',', index_col=False)\r\n```\r\nCode that does not have the issue:\r\n```\r\nimport pandas as pd\r\ndf = pd.read_table(path_to_csv, sep=',')\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\n\r\nINSTALLED VERSIONSOS: Linux\r\nOS-release: 2.6.32-431.3.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\n\r\n------------------LC_ALL: None\r\n\r\nLANG: en_US.utf8\r\ncommit: None\r\n\r\npandas: 0.16.2\r\npython: 2.7.7.final.0\r\nnose: 1.3.4\r\npython-bits: 64Cython: None\r\n\r\nOS: Linuxnumpy: 1.9.2\r\n\r\nOS-release: 2.6.32-431.3.1.el6.x86_64scipy: 0.15.1\r\n\r\nmachine: x86_64\r\nstatsmodels: None\r\nprocessor: x86_64\r\nIPython: None\r\nbyteorder: little\r\nsphinx: 1.3.1\r\nLC_ALL: None\r\npatsy: None\r\nLANG: en_US.utf8\r\ndateutil: 2.4.2\r\n\r\npandas: 0.16.2pytz: 2015.4\r\n\r\nnose: 1.3.4bottleneck: 1.0.0\r\n\r\nCython: Nonetables: None\r\n\r\nnumpy: 1.9.2numexpr: 2.4\r\n\r\nscipy: 0.15.1matplotlib: 1.3.1\r\n\r\nstatsmodels: Noneopenpyxl: None\r\n\r\nIPython: Nonexlrd: None\r\n\r\nxlwt: Nonesphinx: 1.3.1\r\n\r\npatsy: Nonexlsxwriter: None\r\n\r\ndateutil: 2.4.2lxml: None\r\n\r\npytz: 2015.4bs4: None\r\n\r\nbottleneck: 1.0.0html5lib: None\r\n\r\ntables: Nonehttplib2: None\r\n\r\nnumexpr: 2.4apiclient: None\r\n\r\nsqlalchemy: 0.9.8matplotlib: 1.3.1\r\n\r\npymysql: Noneopenpyxl: None\r\n\r\npsycopg2: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nNonehttplib2: None\r\n\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\nNone\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.3.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.4\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 1.0.0\r\ntables: None\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\nNone\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.3.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.4\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 1.0.0\r\ntables: None\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None"""
10610,95710731,djgagne,jreback,2015-07-17 18:03:18,2015-09-01 11:49:40,2015-09-01 11:49:40,closed,,0.17.0,13,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10610,b'Naming inconsistency in import of maybe_convert_indices in pandas.core.index.py',"b'In lines 930-931 of pandas.core.index.py, the following code states\r\n```python\r\nfrom pandas.core.indexing import _maybe_convert_indices\r\nreturn _maybe_convert_indices(indexer, len(self))\r\n```\r\nIn pandas.core.indexing.py, the function is called `maybe_convert_indices`. In the 3 other places where the function is called, it is called using the correct name. The leading underscore should be removed from the function name in index.py.'"
10602,95452235,bashtage,jreback,2015-07-16 15:00:18,2016-02-16 16:30:00,2015-09-06 17:14:50,closed,,0.17.0,20,Bug;Deprecate,https://api.github.com/repos/pydata/pandas/issues/10602,b'BUG: Fix issue with old-style usage in convert_objects',"b""Fix to temporary allow passing 'coerce' to variables\r\n\r\ncloses #10601"""
10601,95387146,jorisvandenbossche,jreback,2015-07-16 09:08:28,2015-09-06 17:14:50,2015-09-06 17:14:50,closed,,0.17.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/10601,b'BUG in dealing with deprecated argument in convert_objects ',"b'cc @bashtage\r\n\r\nThe deprecation warning is triggered, but the value of the keyword argument should also be adapted so it still works with the new code I think (from a failure in the docs)\r\n\r\n```\r\nIn [1]: from datetime import datetime\r\n\r\nIn [2]: s = pd.Series([datetime(2001,1,1,0,0), \'foo\', 1.0, 1,\r\n   ...:                pd.Timestamp(\'20010104\'), \'20010105\'], dtype=\'O\')\r\n\r\nIn [5]: s.convert_objects(convert_dates=\'coerce\')\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\util\\decorators.py:81: FutureWarning\r\n: the \'convert_dates\' keyword is deprecated, use \'datetime\' instead\r\n  warnings.warn(msg, FutureWarning)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-b962b638ac86> in <module>()\r\n----> 1 s.convert_objects(convert_dates=\'coerce\')\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\util\\decorators.pyc in wrapper(*args\r\n, **kwargs)\r\n     86                 else:\r\n     87                     kwargs[new_arg_name] = new_arg_value\r\n---> 88             return func(*args, **kwargs)\r\n     89         return wrapper\r\n     90     return _deprecate_kwarg\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\util\\decorators.pyc in wrapper(*args\r\n, **kwargs)\r\n     86                 else:\r\n     87                     kwargs[new_arg_name] = new_arg_value\r\n---> 88             return func(*args, **kwargs)\r\n     89         return wrapper\r\n     90     return _deprecate_kwarg\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\util\\decorators.pyc in wrapper(*args\r\n, **kwargs)\r\n     86                 else:\r\n     87                     kwargs[new_arg_name] = new_arg_value\r\n---> 88             return func(*args, **kwargs)\r\n     89         return wrapper\r\n     90     return _deprecate_kwarg\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\generic.py in convert_objects(s\r\nelf, datetime, numeric, timedelta, coerce, copy)\r\n   2468                                timedelta=timedelta,\r\n   2469                                coerce=coerce,\r\n-> 2470                                copy=copy)).__finalize__(self)\r\n   2471\r\n   2472     #-------------------------------------------------------------------\r\n---\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\internals.py in convert(self, *\r\n*kwargs)\r\n   3459         """""" convert the whole block as one """"""\r\n   3460         kwargs[\'by_item\'] = False\r\n-> 3461         return self.apply(\'convert\', **kwargs)\r\n   3462\r\n   3463     @property\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\internals.py in apply(self, f,\r\naxes, filter, do_integrity_check, **kwargs)\r\n   2467                                                  copy=align_copy)\r\n   2468\r\n-> 2469             applied = getattr(b, f)(**kwargs)\r\n   2470\r\n   2471             if isinstance(applied, list):\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\internals.py in convert(self, d\r\natetime, numeric, timedelta, coerce, copy, by_item)\r\n   1493                 timedelta=timedelta,\r\n   1494                 coerce=coerce,\r\n-> 1495                 copy=copy\r\n   1496             ).reshape(self.values.shape)\r\n   1497             blocks.append(make_block(values,\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\common.py in _possibly_convert_\r\nobjects(values, datetime, numeric, timedelta, coerce, copy)\r\n   1897     """""" if we have an object dtype, try to coerce dates and/or numbers ""\r\n""""\r\n   1898\r\n-> 1899     conversion_count = sum((datetime, numeric, timedelta))\r\n   1900     if conversion_count == 0:\r\n   1901         import warnings\r\n\r\nTypeError: unsupported operand type(s) for +: \'int\' and \'str\'\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
10590,95293744,larvian,jreback,2015-07-15 21:23:28,2015-09-03 13:09:57,2015-09-03 13:09:57,closed,,0.17.0,5,Bug;Groupby;Missing-data;Prio-medium;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10590,"b'Inconsistency, NaT included in result of groupby method first but not NaN'","b""`NaT` is included in result of `groupby` method `first` while `NaN`. I am expecting that  first should skip both `NaN` and `NaT` and include the first value where `pandas.isnull` is False.\r\nDemonstration of the inconsistency. (note that both `NaT` and `NaN` in the data frame are produced by `np.nan`, the difference is that the d_t column contains date values).\r\n\r\n```Python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom datetime import datetime as dt\r\n\r\ntestFrame=DataFrame({'IX':['A','A'],'num':[np.nan,100],'d_t':[np.nan,dt.now()]})\r\n```\r\nResulting data frame:\r\n```\r\n  IX                     d_t  num\r\n0  A                     NaT  NaN\r\n1  A 2015-07-15 22:47:10.635  100\r\n```\r\nGrouping this data frame on the `IX` column and executing the `first` method results in this data frame which shows the inconsistency between the `d_t` and `num` columns.\r\n```\r\ntestFrame.groupby('IX').first()\r\n```\r\nResulting dataframe:\r\n```\r\n        d_t  num\r\nIX              \r\nA       NaT  100\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.2\r\nnose: 1.3.4\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.6.7\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 0.9.9\r\npymysql: None\r\npsycopg2: None\r\n```"""
10583,95224153,JonasAbernot,jreback,2015-07-15 15:32:38,2015-09-01 19:38:53,2015-09-01 19:38:53,closed,,0.17.0,5,Bug;Difficulty Novice;Effort Low;Prio-low;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10583,"b""TimedeltaIndex raises error when slicing from '0s'""","b""```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame(np.random.normal(size=(10,4)))\r\ndf.index = pd.timedelta_range(start='0s', periods=10, freq='s')\r\n\r\nprint(df.loc['0s':,:])\r\n```\r\nraises a `ValueError: invalid resolution`, while `print(df.loc['5s':,:])` works.\r\n\r\nWorkaround : `print(df.loc[pd.Timedelta('0s'):,:])`\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 6246cc1bb5149863de09b470530b69f7e22cad87\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-57-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\n\r\npandas: 0.16.2-138-g6246cc1\r\nnose: 1.3.1\r\nCython: 0.20.1post0\r\nnumpy: 1.8.2\r\nscipy: 0.13.3\r\nstatsmodels: None\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: None\r\ndateutil: 2.4.1\r\npytz: 2013b\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n```"""
10581,95170751,ruidc,jreback,2015-07-15 11:36:53,2015-08-18 11:09:49,2015-08-18 11:09:49,closed,,0.17.0,1,Bug;Msgpack;Unicode,https://api.github.com/repos/pydata/pandas/issues/10581,b'encoding not respected on read_msgpack',"b'as discussed on https://groups.google.com/forum/#!topic/pydata/ngROaML_hLI\r\nencoding does not seem to be respected on reading a msgpack, below i am expecting to get back what \r\nI put in as utf8\r\n```\r\nIn [17]: s\r\nOut[17]: u\'\\u2019\'\r\n\r\nIn [18]: s = pd.Series({\'a\' : u""\\u2019"" })\r\n\r\nIn [19]: s.values[0]\r\nOut[19]: u\'\\u2019\'\r\n\r\nIn [20]: pd.read_msgpack(s.to_msgpack(encoding=\'utf8\')).values[0]\r\nOut[20]: u\'\\xe2\\x80\\x99\'\r\n```\r\nin stepping through, part of the problem seems to be that in the call to unpack on https://github.com/pydata/pandas/blob/master/pandas/io/packers.py#L134 that there is no encoding argument passed and so it defaults to latin1 in https://github.com/pydata/pandas/blob/master/pandas/io/packers.py#L558\r\n\r\nchanging L134 to :\r\n```\r\nl = list(unpack(fh, **kwargs))\r\n```\r\nand passing the encoding like: \r\n```\r\npandas.read_msgpack(m, encoding=\'utf8\') \r\n```\r\nmakes it work for me, however i don\'t have en environment set up to submit this as a pull request via GH, and we\'re still using 0.14.1 due to compatibility issues.\r\n'"
10576,95104119,captainsafia,captainsafia,2015-07-15 04:31:58,2015-07-21 06:09:49,2015-07-21 05:27:32,closed,,0.17.0,5,Bug;Dtypes;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10576,b'BUG: Fix typo-related bug to resolve #9266',b'closes #9266 \r\n\r\nTried replicating the test case provided in the original issue. Let me know if it needs any changes or fixes.'
10565,94944690,cpcloud,jreback,2015-07-14 13:26:08,2016-02-02 19:53:54,2016-02-02 19:53:46,closed,,Next Major Release,2,Bug;Difficulty Novice;Effort Low;Prio-low,https://api.github.com/repos/pydata/pandas/issues/10565,b'pandas.rolling_* functions lose the name of the input Series',"b""```python\r\nIn [10]: s = pd.Series(np.arange(1000.0), name='foo')\r\n\r\nIn [11]: s.name\r\nOut[11]: 'foo'\r\n\r\nIn [12]: pd.rolling_mean(s, 30).name is None\r\nOut[12]: True\r\n\r\nIn [13]: pd.__version__\r\nOut[13]: '0.16.2'\r\n```"""
10560,94640745,mcwitt,jreback,2015-07-13 05:12:40,2016-04-03 14:20:30,2016-04-03 14:20:30,closed,,Next Major Release,1,Bug;Difficulty Intermediate;Effort Low;Output-Formatting;Prio-medium;Sparse,https://api.github.com/repos/pydata/pandas/issues/10560,b'TypeError in SparseSeries.__repr__ when series longer than max_rows',"b""```python\r\nIn [2]: pd.__version__\r\nOut[2]: '0.16.2-123-gdf1f5cf'\r\n\r\nIn [3]: pd.options.display.max_rows = 3\r\n\r\nIn [4]: pd.Series(randn(3)).to_sparse()\r\nOut[4]: \r\n0    1.100684\r\n1   -0.924482\r\n2   -0.106069\r\ndtype: float64\r\nBlockIndex\r\nBlock locations: array([0], dtype=int32)\r\nBlock lengths: array([3], dtype=int32)\r\n\r\nIn [5]: pd.Series(randn(4)).to_sparse()\r\nOut[5]: ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n...\r\nTypeError: cannot concatenate a non-NDFrame object\r\n```\r\n\r\nSet `max_rows=3` for demonstration, but occurs also for the default `max_rows=60`."""
10558,94601173,sinhrks,sinhrks,2015-07-12 21:35:36,2015-07-20 21:19:12,2015-07-20 21:19:08,closed,,0.17.0,2,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/10558,b'BUG: pd.eval with numexpr engine coerces 1 element numpy array to scalar',"b'Closes #10546. Used ``assert_numpy_atray_equivalent`` to guarantee array comparison, not scalar. This should be prior to #10542 and being changed to use ``assert_numpy_array_equal`` in #10542.'"
10557,94596349,kjordahl,jreback,2015-07-12 20:17:27,2015-07-13 11:31:29,2015-07-13 02:40:09,closed,,0.17.0,6,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/10557,b'Pickle subclass metadata',b'Fixes #10553'
10553,94509278,kjordahl,jreback,2015-07-11 21:36:54,2015-07-13 02:40:09,2015-07-13 02:40:09,closed,,0.17.0,2,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/10553,b'Subclassed attributes are not serializable',"b""In a subclass of pandas objects, pickling an object doesn't serialize properties of an instance of that subclass, even if the attribute has been added to `_metadata`. It would be hard to override this behavior entirely in the subclass, because it will require updates to `__getstate__` and `__setstate__`, and probably also an addition or subclass of `BlockManager`. It would be nice if the `_metadata` serialization was handled in the base pandas class.\r\n\r\nExample:\r\n\r\n    class SubDataFrame(DataFrame):\r\n\r\n        _metadata = ['my_data']\r\n\r\n        @property\r\n        def _constructor(self):\r\n            return SubDataFrame\r\n\r\n    sdf = SubDataFrame()\r\n    sdf.my_data = 'foo'\r\n    sdf.to_pickle('tmp.pkl')\r\n    new_sdf = read_pickle('tmp.pkl')\r\n    new_sdf.my_data\r\n\r\nraises `AttributeError: 'SubDataFrame' object has no attribute 'my_data'`\r\n\r\n(edited original example for correctness)"""
10548,94498384,foobarbecue,jreback,2015-07-11 18:45:44,2015-08-17 11:09:50,2015-08-17 11:09:50,closed,,0.17.0,3,Bug;Difficulty Intermediate;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10548,b'bug: read_csv inserts NaN row if file ends in comment line',"b""xref #4623\r\n \r\nReading this file produces the expected behavior:\r\n```\r\n1,2,3\r\n3,8,3\r\n#comment\r\n5,5,5\r\n```\r\n```\r\nIn [52]: pandas.read_csv('commentBugTest.csv',comment='#')\r\nOut[52]: \r\n   1  2  3\r\n0  3  8  3\r\n1  5  5  5\r\n```\r\nHowever, if the last line in the file is a comment, you get unexpected NaNs:\r\n```\r\n1,2,3\r\n3,8,3\r\n#comment\r\n```\r\n```\r\nIn [51]: pandas.read_csv('commentBugTest2.csv',comment='#')\r\nOut[51]: \r\n    1   2   3\r\n0   3   8   3\r\n1 NaN NaN NaN\r\n```"""
10547,94483363,seth-p,jreback,2015-07-11 15:28:16,2015-08-14 18:40:52,2015-08-14 18:40:52,closed,,0.17.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10547,b'BUG: Series.iloc[[-1]] error when Series is of length 1',"b'When a ``Series`` is of length one, I would expect ``iloc[[-1]]`` to produce the same result as ``iloc[[0]]``.  However ``iloc[[-1]]`` produces an error, even though ``iloc[[0]]`` and ``iloc[-1]`` work.\r\n\r\n```\r\nIn [178]: import pandas as pd\r\n\r\nIn [179]: pd.Series([\'a\', \'b\'], index=[\'A\', \'B\']).iloc[[-1]]\r\nOut[179]:\r\nB    b\r\ndtype: object\r\n\r\nIn [180]: pd.Series([\'a\'], index=[\'A\']).iloc[[-1]]  # BUG: This should return the same as [181].\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-180-47228385ee31> in <module>()\r\n----> 1 pd.Series([\'a\'], index=[\'A\']).iloc[[-1]]\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key)\r\n   1187             return self._getitem_tuple(key)\r\n   1188         else:\r\n-> 1189             return self._getitem_axis(key, axis=0)\r\n   1190\r\n   1191     def _getitem_axis(self, key, axis=0):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis)\r\n   1463\r\n   1464                 # validate list bounds\r\n-> 1465                 self._is_valid_list_like(key, axis)\r\n   1466\r\n   1467                 # force an actual list\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _is_valid_list_like(self, key, axis)\r\n   1402         l = len(ax)\r\n   1403         if len(arr) and (arr.max() >= l or arr.min() <= -l):\r\n-> 1404             raise IndexError(""positional indexers are out-of-bounds"")\r\n   1405\r\n   1406         return True\r\n\r\nIndexError: positional indexers are out-of-bounds\r\n\r\nIn [181]: pd.Series([\'a\'], index=[\'A\']).iloc[[0]]\r\nOut[181]:\r\nA    a\r\ndtype: object\r\n\r\nIn [182]: pd.Series([\'a\'], index=[\'A\']).iloc[-1]\r\nOut[182]: \'a\'\r\n\r\nIn [183]: pd.__version__\r\nOut[183]: \'0.16.2\'\r\n```'"
10546,94476903,sinhrks,sinhrks,2015-07-11 14:09:15,2015-07-20 21:19:08,2015-07-20 21:19:08,closed,,0.17.0,0,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/10546,b'BUG: pd.eval against single element array using numexpr engine coerces to scalar',"b""```\r\nimport pandas as pd\r\n\r\n# scalar (OK)\r\ns = 1\r\npd.eval('s', engine='numexpr')\r\n# 1L\r\npd.eval('s', engine='python')\r\n# 1\r\n\r\n# array\r\na = np.array([1])\r\n\r\n# OK\r\npd.eval('a', engine='python')\r\n# array([1])\r\n\r\n# NG\r\npd.eval('a', engine='numexpr')\r\n# 1L\r\n```\r\n\r\nInternally, we can distinguish them using returned shape.\r\n\r\n```\r\nimport numexpr as ne\r\nne.evaluate('s')\r\n# array(1L)\r\nne.evaluate('a')\r\n# array([1])\r\nne.evaluate('s').shape\r\n# ()\r\nne.evaluate('a').shape\r\n# (1,)\r\n```"""
10543,94436016,captainsafia,jreback,2015-07-11 04:41:10,2015-08-15 22:41:23,2015-08-15 22:41:23,closed,,0.17.0,14,Bug;Resample;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10543,b'ENH: Added functionality in resample to resolve #10530',"b""closes #10530 \r\n\r\nI'm not 100% sure that I implemented this per the documentation or that my tests are complete but I'd be glad to make any necessary changes."""
10537,94126777,kzielnicki,jreback,2015-07-09 19:00:52,2015-07-14 15:52:23,2015-07-14 15:52:23,closed,,0.17.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10537,b'BUG: Inconsistent date parsing of month/year',"b'\t>>> pd.Timestamp(\'2014-06\')\r\n\tTimestamp(\'2014-06-01 00:00:00\')\r\n\t>>> pd.Timestamp(\'06-2014\')\r\n\tTimestamp(\'2014-06-09 00:00:00\')\r\n\r\nWhen parsing a string ""YYYY-MM"", the day defaults to the 1st of the month (the behavior I would expect, consistent with defaulting hour minute and second to 0), while ""MM-YYYY"" defaults the day to today\'s day. If the string format is specified manually as in `pd.to_datetime(\'06-2014\',format=\'%m-%Y\')`, the day consistently defaults to the first of the month.'"
10536,94116779,artemyk,jreback,2015-07-09 18:16:36,2016-04-18 19:55:34,2016-04-18 19:55:34,closed,,0.18.1,1,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/10536,b'Concat does not work for sparse series',"b""Concatenating two sparse series does not return sparse data structures as expected:\r\n\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: ts = pd.Series([0, 1, 1, 2, 3, 0 ,0 ,0])\r\n\r\nIn [3]: sts = ts.to_sparse()\r\n\r\nIn [4]: print type(pd.concat([sts, sts], axis=0))\r\n<class 'pandas.core.series.Series'>\r\n\r\nIn [5]: print type(pd.concat([sts, sts], axis=1))\r\n<class 'pandas.core.frame.DataFrame'>\r\n```\r\n\r\nThe above *does* work correctly for SparseDataFrames."""
10535,94116416,artemyk,jreback,2015-07-09 18:14:28,2015-07-21 10:34:24,2015-07-21 10:34:24,closed,,0.17.0,5,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/10535,b'BUG: get_dummies not returning SparseDataFrame',b'Fixes #10531 .'
10531,93857728,tgarc,jreback,2015-07-08 18:24:19,2015-07-21 10:34:53,2015-07-21 10:34:53,closed,,0.17.0,6,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/10531,"b'get_dummies(df,sparse=True) does not return sparse DataFrame'","b""Just like it says in the subject. Here's an example:\r\n\r\n```python\r\nIn [216]: pd.version.version\r\nOut[216]: '0.16.2'\r\n\r\nIn [217]: df = pd.DataFrame(np.random.randint(10,size=(10000,5)),columns=list('abcde'))\r\n\r\nIn [218]: df.head()\r\nOut[218]: \r\n   a  b  c  d  e\r\n0  2  6  1  4  0\r\n1  5  2  6  8  5\r\n2  0  8  7  3  1\r\n3  9  2  6  3  1\r\n4  4  8  6  8  6\r\n\r\nIn [219]: ddf = pd.get_dummies(df,columns=df.columns,sparse=True)\r\n\r\nIn [220]: ddf.head()\r\nOut[220]: \r\n   a_0  a_1  a_2  a_3  a_4  a_5  a_6  a_7  a_8  a_9 ...   e_0  e_1  e_2  e_3  \\\r\n0    0  NaN    1  NaN    0    0  NaN  NaN  NaN    0 ...     1    0  NaN  NaN   \r\n1    0  NaN    0  NaN    0    1  NaN  NaN  NaN    0 ...     0    0  NaN  NaN   \r\n2    1  NaN    0  NaN    0    0  NaN  NaN  NaN    0 ...     0    1  NaN  NaN   \r\n3    0  NaN    0  NaN    0    0  NaN  NaN  NaN    1 ...     0    1  NaN  NaN   \r\n4    0  NaN    0  NaN    1    0  NaN  NaN  NaN    0 ...     0    0  NaN  NaN   \r\n\r\n   e_4  e_5  e_6  e_7  e_8  e_9  \r\n0  NaN    0    0  NaN  NaN  NaN  \r\n1  NaN    1    0  NaN  NaN  NaN  \r\n2  NaN    0    0  NaN  NaN  NaN  \r\n3  NaN    0    0  NaN  NaN  NaN  \r\n4  NaN    0    1  NaN  NaN  NaN  \r\n\r\n[5 rows x 50 columns]\r\n\r\nIn [221]: type(ddf)\r\nOut[221]: pandas.core.frame.DataFrame\r\n\r\nIn [222]: hasattr(ddf,'density')\r\nOut[222]: False\r\n\r\nIn [223]: ddf = ddf.to_sparse()\r\n\r\nIn [224]: type(ddf)\r\nOut[224]: pandas.sparse.frame.SparseDataFrame\r\n\r\nIn [225]: ddf.density\r\nOut[225]: 0.1\r\n```\r\n\r\nI notice the NaN encoding in the DataFrame returned by `get_dummies` when `sparse=True` but the datatype is not sparse. Is this expected behavior?"""
10529,93770462,BotoKopo,jreback,2015-07-08 11:53:41,2015-10-25 15:29:19,2015-10-25 15:29:12,closed,,Next Major Release,3,Bug;Data IO;Unicode,https://api.github.com/repos/pydata/pandas/issues/10529,b'BUG : read_csv() twice decodes stream on URL file #10424',"b""As described in #10424 , reading non-utf-8 csv files from URL leads to decoding problems, i.e. a decoding may first be made in io.common.get_filepath_or_buffer() when file is URL.\r\nModification done makes this function read stream from URL without decoding (this is done later, at the same place as for local files). \r\nIt's also used by io.common.read_stata(), io.common.read_json() and io.common.read_msgpack(). Similar problems  reading stata and msgpack URL files may also be solved using this modification.\r\n\r\nThanks for reviewing."""
10527,93687621,kawochen,jreback,2015-07-08 04:04:17,2015-07-18 13:30:47,2015-07-18 13:30:43,closed,,0.17.0,10,Bug;Msgpack,https://api.github.com/repos/pydata/pandas/issues/10527,b'BUG: GH9618 in read_msgpack where DataFrame has duplicate column names',"b""To close #9618\r\nNote I modified ``encode``, so it's not backward compatible."""
10520,93379069,vincentdavis,jreback,2015-07-06 21:15:58,2015-07-07 10:19:46,2015-07-07 10:19:46,closed,,0.17.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10520,b'Fix and tests for issue #10154 inconsistent behavior with invalid dates',b'closes #10154 \r\n\r\nThis addresses the original issue and another discovered with the test cases. \r\nPlease review the tests to be sure you agree with the assertions.'
10514,93239499,alan-wong,jreback,2015-07-06 10:09:46,2015-07-06 10:31:52,2015-07-06 10:31:30,closed,,,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10514,"b""DataFrame constructor ignores key order when data is an OrderedDict and orient is 'columns'""","b""Whilst answering this question: http://stackoverflow.com/questions/31242021/python-3-4-pandas-dataframe-not-responding-to-ordered-dictionary\r\n\r\nI had look at this and found the error is in index.py line 5746 on pandas 0.16.2:\r\n\r\n```\r\ndef _union_indexes(indexes):\r\n    if len(indexes) == 0:\r\n        raise AssertionError('Must have at least 1 Index to union')\r\n    if len(indexes) == 1:\r\n        result = indexes[0]\r\n        if isinstance(result, list):\r\n            result = Index(sorted(result)) #<---- here\r\n        return result\r\n```\r\n\r\nminimal example:\r\n\r\n```\r\nIn [38]:\r\nimport pandas as pd\r\nfrom collections import OrderedDict\r\nd = OrderedDict([('XXX', OrderedDict([('B', 1), ('A', 2)]))])\r\npd.DataFrame(d)\r\n\r\nOut[38]:\r\n   XXX\r\nA    2\r\nB    1\r\n```\r\n\r\nThe same thing occurs if you do:\r\n\r\n```\r\npd.DataFrame.from_dict(d)\r\n```\r\n\r\nNote that passing 'orient='index'' preserves the order:\r\n\r\n```\r\nIn [40]:\r\npd.DataFrame.from_dict(d, orient='index')\r\n\r\nOut[40]:\r\n     B  A\r\nXXX  1  2\r\n```"""
10513,93156476,rosnfeld,jreback,2015-07-05 23:26:35,2015-08-02 21:31:20,2015-08-02 21:26:30,closed,,0.17.0,12,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10513,b'BUG: display.precision options seems off-by-one  (GH10451)',"b'Closes #10451 \r\n\r\nI made a call here in response to my questions on #10451 and hopefully people like it. I made it clear that ""precision"" refers to places after the decimal, not significant figures, and changed the default value to match so that for many pandas users no change would be detected. I updated the Options docs and also What\'s New. For tests I basically updated the precision setting to the new semantics, so that the expected strings wouldn\'t need to change.\r\n\r\nThe one question I have is the code that computes ``too_long``. This compares the longest formatted string against what looks to be an arbitrary constant of ""number of digits + 5"". Changing the 5 to a 4 or 6 doesn\'t trip up any unit tests. If it\'s desired, this could be increased by 1 as the ""new"" digits value is effectively 1 less than what the old value in terms of its effects on formatting, so the 5 would need to change to a 6 to maintain behavior. I could write tests on this as well.'"
10508,92976536,sinhrks,sinhrks,2015-07-04 06:14:30,2015-07-28 21:02:20,2015-07-28 15:28:20,closed,,0.17.0,6,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10508,b'BUG: Groupby(sort=False) with datetime-like Categorical raises ValueError',b'Closes #10505.'
10505,92946695,sinhrks,sinhrks,2015-07-03 23:03:17,2015-07-28 15:28:20,2015-07-28 15:28:20,closed,,0.17.0,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10505,b'BUG: Groupby(sort=False) with datetime-like Categorical raises ValueError',"b""Related to #10501, but not the same. ``groupby`` can accept ``Categorical`` and ``sort`` keyword.\r\n\r\n```\r\ndf = pd.DataFrame({'A': [1, 2, 3 ,4], 'B': [5, 6, 7, 8]})\r\n\r\n# OK\r\ndf.groupby(pd.Categorical(['A', 'B', 'A', 'B'])).groups\r\n# {'A': [0, 2], 'B': [1, 3]}\r\n\r\n# OK\r\ndf.groupby(pd.Categorical(['A', 'B', 'A', 'B']), sort=False).groups\r\n# {'A': [0, 2], 'B': [1, 3]}\r\n```\r\n\r\nIf ``Categorical`` has datetime-like categories, groupby fails if ``sort=False`` is specified.\r\n\r\n```\r\n# OK\r\ndf.groupby(pd.Categorical(pd.DatetimeIndex(['2011', '2012', '2011', '2012']))).groups\r\n# {numpy.datetime64('2011-01-01T09:00:00.000000000+0900'): [0, 2], \r\n#  numpy.datetime64('2012-01-01T09:00:00.000000000+0900'): [1, 3]}\r\n\r\n# NG\r\ndf.groupby(pd.Categorical(pd.DatetimeIndex(['2011', '2012', '2011', '2012'])), sort=False).groups\r\n# ValueError: items in new_categories are not the same as in old categories\r\n```\r\n\r\n"""
10503,92902147,jeggleston,jreback,2015-07-03 16:05:33,2016-02-27 17:52:43,2016-02-27 17:52:43,closed,,0.18.0,3,Bug;Difficulty Intermediate;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/10503,b'Simple operation unexpectedly changes dtype',"b""Hi all,\r\nI can't find any documentation that says this should happen, so I think it's a bug. But maybe something's happening that I don't understand. When I do a simple operation (adding 1 to a slice), suddenly the dtype of the columns changes from uint32 to int64.\r\nAny ideas why this is happening? Bug?\r\nThanks\r\n\r\nMake a sample dataframe. Columns are dtype uint32.\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame({'a':[0, 1, 1], 'b':[100, 200, 300]}, dtype='uint32')\r\n\r\nIn [3]: df.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 3 entries, 0 to 2\r\nData columns (total 2 columns):\r\na    3 non-null uint32\r\nb    3 non-null uint32\r\ndtypes: uint32(2)\r\nmemory usage: 48.0 bytes\r\n```\r\n\r\nTake a slice of a column. Adding 1 to that slice still results in dtype uint32.\r\n```\r\nIn [4]: ix = df['a'] == 1\r\n\r\nIn [5]: z = df.loc[ix, 'b']\r\n\r\nIn [6]: z + 1\r\nOut[6]: \r\n1    201\r\n2    301\r\nName: b, dtype: uint32\r\n```\r\n\r\nBut, if I modify that slice in the original dataframe, suddenly both columns of the dataframe are int64.\r\n```\r\nIn [7]: df.loc[ix, 'b'] = z + 1\r\n\r\nIn [8]: df.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 3 entries, 0 to 2\r\nData columns (total 2 columns):\r\na    3 non-null int64\r\nb    3 non-null int64\r\ndtypes: int64(2)\r\nmemory usage: 72.0 bytes\r\n```\r\n\r\nI've seen this in 0.16, 0.16.1, and 0.16.2.\r\n```\r\nIn [9]: pd.__version__\r\nOut[9]: '0.16.2'\r\n```\r\n"""
10498,92879181,cottrell,jreback,2015-07-03 13:42:13,2015-07-06 12:45:10,2015-07-06 12:45:10,closed,,0.17.0,3,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10498,b'Add test and fix for categorical series .shift #10495.',b'#10495 \r\n\r\nWaiting for Travis and will review further.'
10497,92879144,bwillers,jreback,2015-07-03 13:42:01,2015-07-18 00:03:30,2015-07-18 00:03:27,closed,,0.17.0,4,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10497,b'BUG: CategoricalBlock shift GH9416',"b""Should resolve #9416.\r\n\r\nCategoricalBlocks always seem to have ndim=1, even if multiple\r\ncategoricals are in a frame with the same categories. This simplifies\r\nthe axis shift logic somewhat.\r\n\r\nNote that dataframe shift with axis=1 still doesn't work with multiple\r\ncategorical columns, since they are each a different block."""
10495,92871047,cottrell,jreback,2015-07-03 13:01:56,2015-07-06 12:44:38,2015-07-06 12:44:38,closed,,0.17.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10495,"b'.shift() on Categorical raises ""\'Categorical\' object has no attribute \'flags\'""'","b'Not sure if this is related to #10324 .\r\n\r\nRunning master with python 3.4.3.\r\n\r\n``` python\r\nimport pandas\r\n\r\nvalues = [\'a\', \'b\', \'c\', \'d\']\r\na = pandas.Series(values)\r\nb = pandas.Series(values, dtype=\'category\')\r\n\r\nprint(a.shift()) # ok\r\nprint(b.shift()) # error\r\n```\r\n\r\nError looks like this:\r\n```\r\n$ python f.py\r\n0    NaN\r\n1      a\r\n2      b\r\n3      c\r\ndtype: object\r\nTraceback (most recent call last):\r\n  File ""f.py"", line 8, in <module>\r\n    print(b.shift()) # error\r\n  File ""/Users/davidcottrell/dev/pandas.git/pandas/core/series.py"", line 2165, in shift\r\n    axis=axis, **kwargs)\r\n  File ""/Users/davidcottrell/dev/pandas.git/pandas/core/generic.py"", line 3688, in shift\r\n    new_data = self._data.shift(periods=periods, axis=block_axis)\r\n  File ""/Users/davidcottrell/dev/pandas.git/pandas/core/internals.py"", line 2498, in shift\r\n    return self.apply(\'shift\', **kwargs)\r\n  File ""/Users/davidcottrell/dev/pandas.git/pandas/core/internals.py"", line 2462, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""/Users/davidcottrell/dev/pandas.git/pandas/core/internals.py"", line 890, in shift\r\n    f_ordered = new_values.flags.f_contiguous\r\nAttributeError: \'Categorical\' object has no attribute \'flags\'\r\n```'"
10486,92510433,DSLituiev,jreback,2015-07-01 23:50:49,2016-05-07 17:12:30,2016-05-07 17:12:30,closed,,0.19.0,1,Bug;Difficulty Intermediate;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/10486,b'BUG: query with invalid dtypes should fallback to python engine',"b'The following call of `df.query()` produces an error:\r\n\r\n    ...\r\n    File ""/Applications/anaconda/lib/python3.4/site-packages/numexpr/necompiler.py"", line 629, in getType\r\n    raise ValueError(""unkown type %s"" % a.dtype.name)\r\n\r\n    ValueError: unkown type str128\r\nin pandas 0.15.2\r\n\r\n    # -*- coding: utf-8 -*-\r\n    import re\r\n    import pandas as pd\r\n    \r\n    def get_gene_site(tp):\r\n        site = re.sub(\'T.$\', \'\',tp)\r\n        gene = site.split(\'%%\')[0]\r\n        t = int(re.search(r\'T(\\d+)$\',tp).group(1))\r\n        return gene, site, t\r\n        \r\n    sub_sample_list = [\'Actb%%qeqwT0\',\r\n     \'Actb%%qeqwT1\',\r\n     \'Actb%%qeqwT2\',\r\n     \'Actb%%qeqwT3\',\r\n     \'Actb%%tralalaT0\',\r\n     \'Actb%%tralalaT1\',\r\n     \'Actb%%tralalaT2\',\r\n     \'Actb%%tralalaT3\',\r\n     \'Dummy%%rrrrT0\',\r\n     \'Dummy%%rrrrT1\',\r\n     \'Dummy%%rrrrT2\',\r\n     \'Dummy%%rrrrT3\',\r\n     ]\r\n     \r\n    dict_samples = {\'sample\':[], \'gene\':[], \'site\':[], \'timepoint\':[]}\r\n    for tp in sub_sample_list:\r\n        gene, site, t = get_gene_site(tp)\r\n        dict_samples[\'sample\'].append(tp)\r\n        dict_samples[\'gene\'].append(str(gene))\r\n        dict_samples[ \'site\'].append(site)\r\n        dict_samples[\'timepoint\'].append(t) \r\n        \r\n    df = pd.DataFrame(dict_samples , \r\n                    columns = [\'sample\', \'gene\', \'site\', \'timepoint\'])\r\n    \r\n    df.query(\'gene = ""Actb""\')'"
10483,92470164,richlewis42,jreback,2015-07-01 19:47:21,2015-07-07 15:52:31,2015-07-07 15:52:31,closed,,0.17.0,4,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10483,b'Series.from_csv not loading header names',"b'Series.from_csv at the moment does not load the series.name and series.index.name when the header keyword argument is stated.\r\n\r\nThis was introduced in 6078fba9410918baa486ca008cc9e3ba066c03ec.  The issue was that `Series.from_csv` uses `DataFrame.from_csv`, which automatically indexes columns (in practice, the index column gets labelled 0 and the values column gets labelled 1).  Converting this to a series will then cause the `series.name` to be 1, and the `series.index.name` to be 0.  The fix was to explicitly set the names in Series.from_csv to `None`.\r\n\r\nThis caused the headers to be deleted even if they were provided by setting the header kwarg.\r\n\r\nThe fix is simple, to check if the headers are provided, and only setting the names to None if they are not.\r\n\r\nI included some tests, please let me know if this is enough as I am new to open-source and pandas.\r\n\r\nThanks!'"
10482,92400857,sinhrks,sinhrks,2015-07-01 14:34:48,2015-07-20 01:46:53,2015-07-20 01:46:53,closed,,0.17.0,5,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10482,"b""BUG: Index doesn't take over the Categorical name""","b""``Series`` can take over the ``Categorical.name``.\r\n\r\n```\r\nimport pandas as pd\r\nc = pd.Categorical([1, 2, 3], name='xxx')\r\npd.Series(c)\r\n# 0    1\r\n# 1    2\r\n# 2    3\r\n# Name: xxx, dtype: category\r\n# Categories (3, int64): [1, 2, 3]\r\n```\r\n\r\nBut ``Index`` doesn't.\r\n\r\n```\r\npd.Index(c)\r\n# CategoricalIndex([1, 2, 3], categories=[1, 2, 3], ordered=False, dtype='category')\r\npd.CategoricalIndex(c)\r\n# CategoricalIndex([1, 2, 3], categories=[1, 2, 3], ordered=False, dtype='category')\r\n```"""
10476,92127551,michaelaye,jreback,2015-06-30 15:40:42,2016-05-25 12:09:25,2016-05-25 12:09:25,closed,,0.19.0,4,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10476,b'read_csv python engine errors',"b'Only thing I changed from my usually working reduction pipeline is to try `engine=""python""` (because I wanted to use `nrows` for a smaller test-read, but that fails as well, and I thought maybe the python engine is buggy currently):\r\n\r\n```python\r\n$ python reduction.py ~/data/planet4/2015-06-21_planet_four_classifications.csv\r\nINFO:Starting reduction.\r\nTraceback (most recent call last):\r\n  File ""reduction.py"", line 258, in <module>\r\n    args.test_n_rows, args.remove_duplicates)\r\n  File ""reduction.py"", line 182, in main\r\n    data = [chunk for chunk in reader]\r\n  File ""reduction.py"", line 182, in <listcomp>\r\n    data = [chunk for chunk in reader]\r\n  File ""/Users/klay6683/miniconda3/lib/python3.4/site-packages/pandas-0.16.2_58_g01995b2-py3.4-macosx-10.5-x86_64.egg/pandas/io/parsers.py"", line 697, in __iter__\r\n    yield self.read(self.chunksize)\r\n  File ""/Users/klay6683/miniconda3/lib/python3.4/site-packages/pandas-0.16.2_58_g01995b2-py3.4-macosx-10.5-x86_64.egg/pandas/io/parsers.py"", line 721, in read\r\n    ret = self._engine.read(nrows)\r\n  File ""/Users/klay6683/miniconda3/lib/python3.4/site-packages/pandas-0.16.2_58_g01995b2-py3.4-macosx-10.5-x86_64.egg/pandas/io/parsers.py"", line 1556, in read\r\n    content = self._get_lines(rows)\r\n  File ""/Users/klay6683/miniconda3/lib/python3.4/site-packages/pandas-0.16.2_58_g01995b2-py3.4-macosx-10.5-x86_64.egg/pandas/io/parsers.py"", line 2007, in _get_lines\r\n    for _ in range(rows):\r\nTypeError: \'float\' object cannot be interpreted as an integer\r\n```\r\n\r\nMy function call is this:\r\n```python\r\n# as chunksize and nrows cannot be used together yet, i switch chunksize\r\n# to None if I want test_n_rows for a small test database:\r\nif test_n_rows:\r\n    chunks = None\r\nelse:\r\n    chunks = 1e6\r\n# creating reader object with pandas interface for csv parsing\r\n# doing this in chunks as its faster. Also, later will do a split\r\n# into multiple processes to do this.\r\nreader = pd.read_csv(fname, chunksize=chunks, na_values=[\'null\'],\r\n                                   usecols=analysis_cols, nrows=test_n_rows,\r\n                                   engine=\'c\')\r\n```\r\n\r\nUsing pandas-0.16.2_58_g01995b2-py3.4\r\n'"
10472,91861641,jvkersch,jreback,2015-06-29 17:23:27,2015-07-08 21:19:54,2015-07-08 14:19:41,closed,,0.17.0,6,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/10472,"b""ENH: Make group_var_ use Welford's algorithm.""","b""closes #10448 \r\n\r\nThis PR reimplements the Cython functions `group_var_float64` and `group_var_float32` to use Welford's algorithm, rather than the sum-of-squares method, which is numerically unstable. This came up in #10448; see also #10242 for more discussion."""
10469,91806906,santegoeds,jreback,2015-06-29 13:45:00,2015-07-07 09:30:45,2015-07-07 09:30:45,closed,,0.17.0,6,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10469,b'BUG: Fix csv_read bugs when using empty input. GH10467 & GH10413',b'closes #10413 \r\ncloses #10467 '
10467,91767392,santegoeds,jreback,2015-06-29 10:28:49,2015-07-07 09:30:45,2015-07-07 09:30:45,closed,,0.17.0,3,Bug;IO CSV;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10467,b'BUG: read_csv: invalid index from header-only input file with multiindex.',"b'```Python\r\nimport pandas as pd\r\nimport cStringIO as stringio\r\n\r\ndf = pd.read_csv(stringio.StringIO(""x,y,z""), index_col=[""x"", ""y""])\r\nprint df.columns\r\nprint df.index.names\r\n```\r\n\r\nOutput:\r\n```\r\n$ python x.py\r\nIndex([u\'y\'], dtype=\'object\')\r\n[u\'x\', u\'y\']\r\n```\r\n\r\nExpected column name is ""z"".\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 01995b2c2c759c66b208dc0bd8a35f5f23c39b9f\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-400.1.1.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB\r\n\r\npandas: 0.16.2-58-g01995b2\r\nnose: 1.3.0\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.9.0\r\nstatsmodels: None\r\nIPython: 3.2.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.2\r\nbottleneck: 0.6.0\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.2.1\r\nhtml5lib: 0.90\r\nhttplib2: $Rev$\r\napiclient: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n'"
10464,91586068,sinhrks,sinhrks,2015-06-28 11:03:24,2015-07-01 13:47:52,2015-07-01 13:46:10,closed,,0.17.0,8,Bug;Categorical;Compat,https://api.github.com/repos/pydata/pandas/issues/10464,b'BUG: Series.map using categorical Series raises AttributeError',"b'Closes #10324. Closes #10460.\r\n\r\nBased on #9848, using ``.get_values`` should be avoided?'"
10462,91559942,Garrett-R,jreback,2015-06-28 06:11:29,2015-10-12 01:32:14,2015-10-11 16:00:58,closed,,Next Major Release,10,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/10462,b'BUG: #10445 cannot add DataFrame to empty Panel',b'closes #10445 \r\n \r\nI based this bugfix off how DataFrame handles the analogous situation  being intialized to empty and then having a series added (cf. [DataFrame._ensure_valid_index](https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L2167))'
10454,91478138,cottrell,jreback,2015-06-27 14:47:44,2015-08-22 20:21:50,2015-08-22 20:21:50,closed,,0.17.0,9,Bug;IO HDF5;Unicode,https://api.github.com/repos/pydata/pandas/issues/10454,b'Attempt to fix issue #10366 encoding and categoricals hdf serialization.',b'closes #10366 . \r\n\r\nProbably not quite the right approach but want to run travis. Tests are a bit weak at this point (just testing for non-exceptions). Encoding issues might be relevant to other types besides categoricals (but categoricals raise exceptions when strings to mangled to non-uniqueness).'
10452,91465788,sinhrks,jreback,2015-06-27 13:43:35,2016-03-30 12:39:02,2016-03-30 12:39:02,closed,,0.18.1,3,API Design;Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/10452,b'API/BUG: SparseSeries.shape ignores fill_value ',"b'``SparseSeries.shape`` seems to return incorrect result ignoring shapes filled by ``fill_value``.\r\n\r\n```\r\ns = pd.SparseSeries([0, 0, 1, 0], fill_value=0)\r\ns\r\n# 0    0\r\n# 1    0\r\n# 2    1\r\n# 3    0\r\n# dtype: int64\r\n# BlockIndex\r\n# Block locations: array([2], dtype=int32)\r\n# Block lengths: array([1], dtype=int32)\r\n\r\nlen(s)\r\n# 4\r\n\r\n# must be (4, )?\r\ns.shape\r\n# (1,)\r\n\r\n# OK\r\ns._data.shape\r\n# (4,)\r\n```'"
10451,91459326,rosnfeld,jreback,2015-06-27 12:39:39,2015-08-02 21:26:30,2015-08-02 21:26:30,closed,,0.17.0,9,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10451,b'BUG: display.precision option seems off-by-one',"b'I may very well be wrong on this, given how common an option it seems, but I am surprised that the ""display.precision"" option seems to limit the digits after the decimal to one less than specified. \r\n\r\n```python\r\nIn [1]: x = pd.Series(np.random.randn(5))\r\n\r\nIn [2]: x\r\nOut[2]: \r\n0   -0.163960\r\n1    1.016273\r\n2    0.861317\r\n3   -0.521916\r\n4   -0.069322\r\ndtype: float64\r\n\r\nIn [3]: pd.set_option(\'display.precision\', 3)\r\n\r\nIn [4]: x\r\nOut[4]: \r\n0   -0.16\r\n1    1.02\r\n2    0.86\r\n3   -0.52\r\n4   -0.07\r\ndtype: float64\r\n```\r\n\r\nI can\'t see where in the code this is happening. At first glance, this looks like what the code is doing:\r\n\r\n```\r\nIn [13]: fmt_str = \'%% .%dg\' % 3\r\n\r\nIn [14]: fmt_str % x[0]\r\nOut[14]: \'-0.164\'\r\n```\r\n\r\nbut clearly something else is happening.\r\n\r\nnumpy\'s precision seems fine/meets my expectations:\r\n```python\r\nIn [10]: np.set_printoptions(precision=3)\r\n\r\nIn [11]: np.random.randn(5)\r\nOut[11]: array([ 0.569, -2.638,  0.707,  0.675,  1.191])\r\n```\r\n\r\nSo is this a bug? (if so it\'s been around for a long time) Or are my expectations off?\r\n\r\nThis was tested on current pandas master (as of writing) with numpy 1.9.2 and python 3.4.'"
10433,90828851,behzadnouri,jreback,2015-06-25 02:08:32,2015-08-30 02:30:09,2015-08-08 12:18:12,closed,,0.17.0,13,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/10433,b'BUG: closes bug in stack when index is not unique',b'closes https://github.com/pydata/pandas/issues/10417'
10429,90672224,jreback,jreback,2015-06-24 13:14:21,2015-06-24 20:46:26,2015-06-24 20:46:26,closed,,0.17.0,0,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10429,"b'BUG: Timedeltas with no specified units (and frac) should raise, #10426'",b'closes #10426 '
10428,90668244,jreback,jreback,2015-06-24 12:52:36,2015-06-24 20:45:55,2015-06-24 20:45:55,closed,,0.17.0,0,Bug;Indexing;Multithreading,https://api.github.com/repos/pydata/pandas/issues/10428,"b'BUG: using .loc[:,column] fails type coercion when the object is a multi-index'",b'from [SO](http://stackoverflow.com/questions/31024821/pandas-dataframe-casting-to-timedelta-fails-with-loc)'
10426,90664213,jreback,jreback,2015-06-24 12:39:34,2015-06-24 20:46:26,2015-06-24 20:46:26,closed,,0.17.0,0,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10426,"b""ERR: Timedelta('3.1415') should raise""","b""These are currently accepted, but parsed incorrectly. These don't have units, nor are of the standard format, so should raise as unparseable."""
10422,90569390,kawochen,jreback,2015-06-24 05:25:20,2015-06-30 10:53:45,2015-06-30 10:53:45,closed,,0.17.0,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10422,b'BUG: GH9907 generate_range when start/end has higher resolution than offset',b'To close #9907'
10419,90494564,sinhrks,sinhrks,2015-06-23 21:27:25,2015-07-04 12:52:08,2015-06-27 03:51:56,closed,,0.17.0,2,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/10419,b'BUG: Fix value_counts name handling',"b'Closes #10150. Also, made ``test_base`` have name attributes.'"
10417,90467700,vumaasha,jreback,2015-06-23 19:10:49,2015-08-08 12:18:12,2015-08-08 12:18:12,closed,,0.17.0,1,Bug;Difficulty Intermediate;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/10417,b'BUG: groupby with levels on duplicated multi-index',"b""\r\n```\r\nx = pd.DataFrame({'x':[1,1,3,3],'y':[3,3,5,5]},index=[11,11,12,12])\r\ny = x.stack()\r\nprint(y)\r\nprint(y.groupby(level=[0,1]).sum())\r\n```\r\nOuput\r\n```\r\n11  x    1\r\n    y    3\r\n    x    1\r\n    y    3\r\n12  x    3\r\n    y    5\r\n    x    3\r\n    y    5\r\ndtype: int64\r\n11  x    1\r\n    y    3\r\n    x    1\r\n    y    3\r\n12  x    3\r\n    y    5\r\n    x    3\r\n    y    5\r\ndtype: int64\r\n```\r\nThe stack and group by sum are just the same.\r\n\r\nExpected output:\r\n\r\n```\r\n11  x    2\r\n11  y    6\r\n12  x    6\r\n12  y    10\r\n```"""
10412,90363980,jorisvandenbossche,jreback,2015-06-23 11:40:02,2015-07-24 13:50:18,2015-07-24 13:50:18,closed,,0.17.0,1,Bug;Difficulty Novice;Effort Low;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10412,b'BUG: setting a slice with datetime64 ignores datetime64 resolution',"b""Resulting in 1970 datetimes when using eg `datetime64[D]` resolution (splitted from #10408):\r\n\r\n```\r\nIn [46]: df=pd.DataFrame({'c':pd.Timestamp('2010-10-01')}, index=range(5))\r\n\r\nIn [47]: df\r\nOut[47]:\r\n           c\r\n0 2010-10-01\r\n1 2010-10-01\r\n2 2010-10-01\r\n3 2010-10-01\r\n4 2010-10-01\r\n\r\nIn [48]: df.loc[0:2,'c'] = np.datetime64('2010-10-12')\r\n\r\nIn [49]: df\r\nOut[49]:\r\n                              c\r\n0 1970-01-01 00:00:00.000014894\r\n1 1970-01-01 00:00:00.000014894\r\n2 1970-01-01 00:00:00.000014894\r\n3 2010-10-01 00:00:00.000000000\r\n4 2010-10-01 00:00:00.000000000\r\n```"""
10408,90221776,seanv507,jreback,2015-06-22 21:49:54,2015-07-24 13:50:18,2015-07-24 13:50:18,closed,,0.17.0,13,Bug;Difficulty Novice;Dtypes;Effort Low;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10408,b'vectorised setting of timestamp columns fails with python datetime and numpy datetime64',"b""    import pandas as pd\r\n    import numpy as np\r\n    import datetime as dt\r\n    z=dt.date(2010,11,1)\r\n    zs=[z+dt.timedelta(days=r) for r in range(5)]\r\n    df=pd.DataFrame({'obj':zs, 'b':pd.Timestamp('2010-10-01'),'c':pd.Timestamp('2010-10-01')})\r\n    df.dtypes\r\n\r\n    #df.loc[0:2,'c']=dt.date(2010,10,12) # causes error: long() argument must be a string or a number, not 'datetime.date\r\n    df.loc[0:2,'c']=np.datetime64('2010-10-12') # sets to 1970...\r\n    df.at[4,'c']=np.datetime64('2010-10-12') # works\r\n    df.loc[0:2,'obj']=np.datetime64('2010-10-12') #works\r\n\r\n    df.loc[0:2,'obj']=dt.date(2010,10,12)\r\n\r\n    df\r\n\r\n| ind | b          | c          | obj        |\r\n|-----|------------|------------|------------|\r\n| 0   | 2010-10-01 | 1970-01-01 | 2010-10-12 |\r\n| 1   | 2010-10-01 | 1970-01-01 | 2010-10-12 |\r\n| 2   | 2010-10-01 | 1970-01-01 | 2010-10-12 |\r\n| 3   | 2010-10-01 | 2010-10-01 | 2010-11-04 |\r\n| 4   | 2010-10-01 | 2010-10-12 | 2010-11-05 |\r\n\r\nI am using Pandas 0.16.2"""
10407,90189163,floriangeigl,jorisvandenbossche,2015-06-22 19:30:44,2015-11-27 12:35:05,2015-11-15 21:05:24,closed,,0.17.1,6,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10407,b'Pandas stopped working with matplotlib inset-axis',"b""Hy,\r\n\r\nin older versions pandas was able to plot onto inset-axis of matplotlib. In new versions this support seems to be broken. I was able to execute the following code without any exceptions using an old pandas version. However, the current pandas version (0.16.2) throws an exception. Since the last update of matplotlib is a while back, I think the bug is caused by a recent pandas update. (As far as i remember, I was able to plot onto inset-axis using pandas 0.16.0.)\r\n\r\n```python\r\nimport matplotlib.pylab as plt\r\nimport pandas as pd\r\nfrom mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\r\n\r\ndf = pd.DataFrame(range(10))\r\nax = df.plot() # OK\r\naxins = zoomed_inset_axes(ax, 2)\r\ndf.plot(ax=axins) # Throws exception\r\n```\r\n\r\nThe thrown exception:\r\n```\r\n/usr/local/lib/python2.7/dist-packages/pandas/tools/plotting.pyc in _handle_shared_axes(axarr, nplots, naxes, nrows, ncols, sharex, sharey)\r\n   3324         layout = np.zeros((nrows+1,ncols+1), dtype=np.bool)\r\n   3325         for ax in axarr:\r\n-> 3326             layout[ax.rowNum, ax.colNum] = ax.get_visible()\r\n   3327 \r\n   3328         if sharex and nrows > 1:\r\n\r\nAttributeError: 'Axes' object has no attribute 'rowNum'\r\n```\r\n\r\nOutput of pd.show_version():\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-53-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.7\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.0\r\nsphinx: 1.2.2\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: None\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: 2.0.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.3\r\nbs4: 4.3.2\r\nhtml5lib: 0.99999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: 0.6.2.None\r\npsycopg2: None"""
10401,89925697,kawochen,jreback,2015-06-21 15:43:36,2015-06-23 11:00:19,2015-06-23 11:00:14,closed,,0.17.0,2,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10401,b'BUG: GH10392 bug where Table.select_column does not preserve column name',b'To close #10392'
10400,89871354,kawochen,jreback,2015-06-21 06:24:00,2015-06-23 14:00:48,2015-06-23 14:00:40,closed,,0.17.0,2,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/10400,b'BUG: GH10395 bug in DataFrame.interpolate with axis=1 and inplace=True',b'To close #10395.'
10395,89689967,JianxunLi,jreback,2015-06-20 00:20:55,2015-06-23 14:00:40,2015-06-23 14:00:40,closed,,0.17.0,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/10395,"b""bug: interpolate() row-by-row doesn't work when setting inplace=True""","b""```python\r\n\r\n# pandas version 0.16.2\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(12345)\r\n# simulate some data\r\ndf = pd.DataFrame(np.random.randn(250, 10), columns=np.arange(1, 11), index=pd.date_range('2015-01-01', periods=250, freq='B'))\r\n# set 20% data to NaN\r\nsel = np.random.choice([True, False], size=(250, 10), p=[0.2, 0.8])\r\ndf.values[sel] = np.nan\r\n\r\n# row-wise interpolation doesn't work when setting inplace=True\r\ndf1 = df.copy()\r\ndf1.interpolate(method='linear', axis=1, inplace=True)\r\n\r\n# inplace doesn't work\r\nimport pandas.util.testing as pdt\r\npdt.assert_frame_equal(df, df1)\r\n\r\n# dropping inplace=True, it works\r\ndf2 = df.copy()\r\ndf2 = df2.interpolate(method='linear', axis=1)\r\npdt.assert_frame_equal(df, df2)\r\n\r\n```"""
10392,89633876,jreback,jreback,2015-06-19 18:49:33,2015-06-23 11:00:14,2015-06-23 11:00:14,closed,,0.17.0,0,Bug;Difficulty Novice;Effort Low;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10392,b'BUG: select_column does not preserve the column name',"b""```\r\nIn [4]: df = DataFrame({'A' : np.random.randn(10), 'B' : 'foo'})\r\n\r\nIn [5]: df.to_hdf('test.h5','df',mode='w',format='table',data_columns=True)\r\n\r\n# this should have a name of B\r\nIn [7]: with pd.HDFStore('test.h5') as store:\r\n    print store.select_column('df','B')\r\n   ...:     \r\n0    foo\r\n1    foo\r\n2    foo\r\n3    foo\r\n4    foo\r\n5    foo\r\n6    foo\r\n7    foo\r\n8    foo\r\n9    foo\r\ndtype: object\r\n```"""
10389,89432413,behzadnouri,jreback,2015-06-19 00:29:40,2015-06-20 12:17:35,2015-06-20 11:09:45,closed,,0.17.0,1,Bug;Indexing;Missing-data;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10389,b'BUG: closes bug in reset_index when index contains NaT',b'closes https://github.com/pydata/pandas/issues/10388'
10388,89302139,jstritar,jreback,2015-06-18 14:02:14,2015-06-20 11:09:44,2015-06-20 11:09:44,closed,,0.17.0,2,Bug;Difficulty Intermediate;Effort Low;Indexing;Missing-data;MultiIndex;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10388,b'NaT in MultiIndex throws ValueError when reset_index() is called.',"b""Please see this test case:\r\n\r\n```\r\nx = pp.DataFrame({\r\n    'other': ['a', np.nan],\r\n    'date': ['2015-03-22', np.nan],\r\n    'amount': [2, 3]\r\n  })\r\nx.date = pp.to_datetime(x.date)\r\n```\r\n\r\nindex | amount | date | other\r\n------- | ---------- | ------ | ------\r\n0 | 2 | 2015-03-22 | a\r\n1 | 3 | NaT | NaN\r\n\r\nThe reset_index here then fails with the error below:\r\n```\r\nx.set_index(['other', 'date']).reset_index()\r\n\r\nValueError: Converting an integer to a NumPy datetime requires a specified unit\r\n```\r\n\r\npp.version.version\r\n'0.16.1'\r\npp.np.version.version\r\n'1.7.1'\r\n"""
10387,89297729,sinhrks,jorisvandenbossche,2015-06-18 13:42:29,2015-07-01 13:47:12,2015-07-01 13:21:04,closed,,0.17.0,10,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10387,b'BUG: DataFrame.plot raises ValueError when color name is specified by multiple characters ',"b'Derived from #9894. Passing color name with multiple characters results in ``ValueError``. Below is the bahavior of current master.\r\n\r\n```\r\n# OK\r\ndf = pd.DataFrame(np.random.randn(3, 3))\r\ndf[0].plot(color=\'green\')\r\n# single green line\r\n\r\n# OK\r\ndf.plot(color=[\'green\'])\r\n# triple green lines\r\n\r\n# NG\r\ndf.plot(color=\'green\')\r\n# ValueError: to_rgba: Invalid rgba arg ""e""\r\n# -> This should be triple green lines\r\n```\r\n\r\nIf passed str can be parsed as both single color and color cycle, following error will be raised.\r\n\r\n- *""\'green\' can be parsed as both single color and color cycle. Specify each color using a list like [\'green\'] or [\'g\', \'r\', \'e\', \'e\', \'n\']""*\r\n\r\nCurrently, there is no color name which can meet above condition (thus cannot tested).\r\n\r\n- http://matplotlib.org/examples/color/named_colors.html\r\n\r\n'"
10386,89273917,jorisvandenbossche,jorisvandenbossche,2015-06-18 12:00:03,2015-07-03 13:33:49,2015-07-03 13:33:49,closed,,0.17.0,2,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/10386,b'BUG: fix multiple columns as primary key in io.sql.get_schema (GH10385)',b'Closes #10385'
10385,89267295,jorisvandenbossche,jorisvandenbossche,2015-06-18 11:25:27,2015-07-03 13:33:49,2015-07-03 13:33:49,closed,,0.17.0,0,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/10385,b'BUG: specifying multiple columns as primary keys broken in io.sql.get_schema',"b'See http://stackoverflow.com/questions/30911299/get-schema-multiple-primary-keys\r\n\r\nI think this still works for the fallback sqlite version, but not for the new sqlalchemy implementation. in `PrimaryKeyConstraint`, the columns should be feeded as separate positional arguments instead of a list (https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L837). So just needs an unacking `*`.'"
10379,89078879,rekcahpassyla,jreback,2015-06-17 17:38:44,2015-06-18 15:43:28,2015-06-18 15:36:21,closed,,0.17.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10379,b'Check for size=0 before setting item',"b""This is a second try at fixing #10193; the first try is in #10194. There is some useful discussion in that PR, so I didn't want to clobber that- not sure of the etiquette of multiple PRs for the same bug??\r\n\r\nThe discussion around setting values in views on #10194 is separate from the bug itself- I find that setting an item on a newly-constructed empty series with a frequency (thus, no views are involved) raises the same error. \r\n\r\nI couldn't cherry pick from my old branch easily as some commits contained more code than is necessary for this set of changes, so I have done a new one.\r\n\r\n"""
10376,89035501,bashtage,jreback,2015-06-17 14:37:38,2015-06-20 16:20:16,2015-06-20 16:20:11,closed,,0.17.0,9,Bug;IO Excel;Performance,https://api.github.com/repos/pydata/pandas/issues/10376,b'ENH: Enable ExcelWriter to construct in-memory sheets',b'Add support for StringIO/BytesIO to ExcelWriter\r\nAdd vbench support for writing excel files\r\nAdd support for serializing lists/dicts to strings\r\nFix bug when reading blank excel sheets\r\n\r\ncloses #8188\r\ncloses #7074\r\ncloses #6403\r\ncloses #7171'
10367,88850645,sinhrks,sinhrks,2015-06-16 21:58:38,2015-06-23 14:12:49,2015-06-23 14:11:23,closed,,0.17.0,4,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10367,b'BUG: drop_duplicates drops name(s).',"b""Closes #10115. Closes #10116.\r\n\r\nBased on @seth-p  and @jreback 's fix, added tets for datetime-likes."""
10366,88776630,cottrell,jreback,2015-06-16 16:33:23,2015-08-28 02:29:03,2015-08-28 02:29:03,closed,,0.17.0,8,Bug;Difficulty Intermediate;Effort Low;IO HDF5;Unicode,https://api.github.com/repos/pydata/pandas/issues/10366,b'Irregular errors when reading certain categorical strings from hdf',"b""It seems that there is something bad happening when we use certain strings with special characters AND the empty string with categoricals:\r\n\r\n```python\r\n    # -*- coding: latin-1 -*-\r\n    import pandas\r\n    import os\r\n\r\n    examples = [\r\n            pandas.Series(['E, 17', '', 'a', 'b', 'c'], dtype='category'),\r\n            pandas.Series(['E, 17', 'a', 'b', 'c'], dtype='category'),\r\n            pandas.Series(['', 'a', 'b', 'c'], dtype='category'),\r\n            pandas.Series(['EE, 17', '', 'a', 'b', 'c'], dtype='category'),\r\n            pandas.Series(['\xa8\xb9', 'a', 'b', 'c'], dtype='category'),\r\n            pandas.Series(['A\xa8\xb9', '', 'a', 'b', 'c'], dtype='category'),\r\n            pandas.Series(['E, 17', '\xa8\xb9', 'a', 'b', 'c'], dtype='category')\r\n            ]\r\n\r\n    def test_hdf(s):\r\n        f = 'testhdf.h5'\r\n        if os.path.exists(f):\r\n            os.remove(f)\r\n        s.to_hdf(f, 'data', format='table')\r\n        return pandas.read_hdf(f, 'data')\r\n\r\n    for i, s in enumerate(examples):\r\n        flag = True\r\n        e = ''\r\n        try:\r\n            test_hdf(s)\r\n        except Exception as ex:\r\n            e = ex\r\n            flag = False\r\n        print('%d: %s\\t%s\\t%s' % (i, 'pass' if flag else 'fail', s.tolist(), e))\r\n```\r\nResults in:\r\n\r\n```\r\n    0: fail ['E, 17', '', 'a', 'b', 'c']   Categorical categories must be unique\r\n    1: pass ['E, 17', 'a', 'b', 'c']\r\n    2: pass ['', 'a', 'b', 'c']\r\n    3: pass ['EE, 17', '', 'a', 'b', 'c']\r\n    4: pass ['\xa8\xb9', 'a', 'b', 'c']\r\n    5: fail ['A\xa8\xb9', '', 'a', 'b', 'c']  Categorical categories must be unique\r\n    6: pass ['E, 17', '\xa8\xb9', 'a', 'b', 'c']\r\n```\r\n\r\nNot sure if I am using this incorrectly or if this is actually a corner case."""
10363,88536091,naftaliharris,jreback,2015-06-15 20:46:32,2015-06-15 21:08:49,2015-06-15 21:01:34,closed,,,4,Bug,https://api.github.com/repos/pydata/pandas/issues/10363,"b""Inconsistent Series/scalar comparison behavior based upon the scalar's type""","b'It\'s pretty unexpected that the behavior of `x <= pd.Series(...)` depends on whether `x` is a python float or a `np.float64`:\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> 5 <= pd.Series(range(10))\r\n0    False\r\n1    False\r\n2    False\r\n3    False\r\n4    False\r\n5     True\r\n6     True\r\n7     True\r\n8     True\r\n9     True\r\ndtype: bool\r\n>>> np.float64(5) <= pd.Series(range(10))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/ops.py"", line 588, in wrapper\r\n    if len(self) != len(other):\r\nTypeError: len() of unsized object\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.2.0-54-virtual\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.2\r\nnose: 1.3.3\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 0.12.1\r\nsphinx: None\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.6\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.3.2\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.7.2\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.5.2 (dt dec pq3 ext)\r\n>>>\r\n```'"
10354,88297259,behzadnouri,jreback,2015-06-15 02:53:41,2015-08-22 11:33:42,2015-06-15 10:44:29,closed,,0.17.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10354,b'BUG: closes bug in apply when function returns categorical',b'closes https://github.com/pydata/pandas/issues/9573\r\n'
10350,88078932,sinhrks,sinhrks,2015-06-13 22:54:48,2015-06-20 02:09:55,2015-06-15 12:57:04,closed,,0.17.0,2,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/10350,b'BUG: frequencies.get_freq_code raises an error against offset with n != 1',"b""Fixed a ``frequencies.get_freq_code`` raises ``ValueError`` against offset instance with n!=1. This is needed for #7832.\r\n\r\n```\r\n# OK\r\n>>> import pandas.tseries.frequencies as f\r\n>>> f.get_freq_code(('D', 3))\r\n(6000, 3)\r\n>>> import pandas.tseries.offsets as offsets\r\n>>> f.get_freq_code(offsets.Day())\r\n(6000, 1)\r\n\r\n# NG!\r\n>>> f.get_freq_code(offsets.Day(3))\r\nValueError: Unknown freqstr: 3D\r\n```\r\n\r\nAlso, added docstrings and tests which make frequencies behavior understandable."""
10346,87987004,sinhrks,sinhrks,2015-06-13 11:14:58,2015-08-05 10:30:19,2015-08-05 10:29:55,closed,,0.17.0,3,Bug;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10346,b'BUG: df.to_html(index=False) renders index.name',b'Closes #10344.'
10344,87912403,sinhrks,sinhrks,2015-06-13 02:50:28,2015-08-05 10:29:55,2015-08-05 10:29:55,closed,,0.17.0,0,Bug;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10344,b'BUG: DataFrame.to_html(index=False) with named Index rendered incorrectly',"b""As below capture, index name is drawn even if ``index=False`` is specified, and it corrupts the layout.\r\n\r\n![2015-06-13 11 47 56](https://cloud.githubusercontent.com/assets/1696302/8142720/1be7dc10-11c2-11e5-894b-8b27c2c62048.png)\r\n\r\nFor copy/paste to Jupyter:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom IPython.display import HTML\r\n\r\ndf = pd.DataFrame(np.random.randn(2, 2), index=pd.Index(['A', 'B'], name='idx'))\r\nHTML(df.to_html())\r\n\r\nHTML(df.to_html(index=False))\r\n```\r\n"""
10341,87824558,bashtage,jreback,2015-06-12 19:17:00,2015-06-15 13:56:20,2015-06-12 21:49:39,closed,,0.16.2,4,Bug;Error Reporting;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10341,b'BUG: Check complib values',b'Add check for complib when opening a HDFStore\r\n\r\ncloses #4582\r\ncloses #8874'
10337,87651037,Garrett-R,jreback,2015-06-12 08:13:02,2015-06-27 18:29:01,2015-06-26 23:24:27,closed,,0.17.0,6,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/10337,b'BUG #10228: segfault due to out-of-bounds in binning',"b""Closes #10228.   I also deleted some duplicated code while I was at it.\r\n\r\nSo, I wasn't sure if I should be including a unit test for this.  This issue was a segfault, which was happening when you do:\r\n\r\n    s = pd.Series([], index=pd.DatetimeIndex([]), dtype=np.object)\r\n    s.resample('d', how='count')\r\n\r\nso I suppose I could add this code into a test, and just verify it doesn't segfault, but that seemed like a bad idea.  It would be testing for something that's undefined behavior and therefore be a non-deterministic test."""
10335,87568002,kawochen,jreback,2015-06-12 01:35:25,2015-06-12 14:28:13,2015-06-12 14:28:08,closed,,0.16.2,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10335,b'BUG: GH10332 where Panel.apply does not handle result with ndim=0',b'This is to close #10332'
10332,87419103,iriyaasagao,jreback,2015-06-11 17:17:45,2015-06-12 14:28:08,2015-06-12 14:28:08,closed,,0.16.2,1,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10332,"b""BUG: Panel.apply (2d) doesn't work when function returns a scalar of NumPy datatype in 0.16.1""","b""Panel.apply (2d) doesn't work when function returns a scalar of NumPy datatype in 0.16.1.\r\n\r\ntest code is as follows:\r\n\r\n    import pandas\r\n    import numpy\r\n\r\n    panel = pandas.Panel(numpy.random.randn(5, 5, 5))\r\n\r\n    panel.apply(lambda df: 0, axis=[1, 2]) # OK\r\n\r\n    panel.apply(lambda df: 0.0, axis=[1, 2]) # OK\r\n\r\n    panel.apply(lambda df: numpy.int64(0), axis=[1, 2]) # NG\r\n\r\n    panel.apply(lambda df: numpy.float64(0.0), axis=[1, 2]) # NG\r\n\r\nresult is:\r\n\r\n    yIn [1]: import pandas\r\n\r\n    In [2]: import numpy\r\n\r\n    In [3]: panel = pandas.Panel(numpy.random.randn(5, 5, 5))\r\n\r\n    In [4]: panel.apply(lambda df: 0, axis=[1, 2]) # OK\r\n    Out[4]:\r\n    0    0\r\n    1    0\r\n    2    0\r\n    3    0\r\n    4    0\r\n    dtype: int64\r\n\r\n    In [5]: panel.apply(lambda df: 0.0, axis=[1, 2]) # OK\r\n    Out[5]:\r\n    0    0\r\n    1    0\r\n    2    0\r\n    3    0\r\n    4    0\r\n    dtype: float64\r\n\r\n    In [6]: panel.apply(lambda df: numpy.int64(0), axis=[1, 2]) # NG\r\n    ---------------------------------------------------------------------------\r\n    PandasError                               Traceback (most recent call last)\r\n    <ipython-input-6-fa750da7769d> in <module>()\r\n    ----> 1 panel.apply(lambda df: numpy.int64(0), axis=[1, 2]) # NG\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\panel.py in apply(self, func, axis, **kwargs)\r\n        967         # 2d-slabs\r\n        968         if isinstance(axis, (tuple,list)) and len(axis) == 2:\r\n    --> 969             return self._apply_2d(f, axis=axis)\r\n        970\r\n        971         axis = self._get_axis_number(axis)\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\panel.py in _apply_2d(self, func, axis)\r\n       1066             results.append((e,obj))\r\n       1067\r\n    -> 1068         return self._construct_return_type(dict(results))\r\n       1069\r\n       1070     def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\panel.py in _construct_return_type(self, result, axes)\r\n       1122\r\n       1123         raise PandasError('invalid _construct_return_type [self->%s] '\r\n    -> 1124                           '[result->%s]' % (self, result))\r\n       1125\r\n       1126     def _wrap_result(self, result, axis):\r\n\r\n    PandasError: invalid _construct_return_type [self-><class 'pandas.core.panel.Panel'>\r\n    Dimensions: 5 (items) x 5 (major_axis) x 5 (minor_axis)\r\n    Items axis: 0 to 4\r\n    Major_axis axis: 0 to 4\r\n    Minor_axis axis: 0 to 4] [result->{0: 0, 1: 0, 2: 0, 3: 0, 4: 0}]\r\n\r\n    In [7]: panel.apply(lambda df: numpy.float64(0.0), axis=[1, 2]) # NG\r\n    ---------------------------------------------------------------------------\r\n    PandasError                               Traceback (most recent call last)\r\n    <ipython-input-7-e7ea1dec29c2> in <module>()\r\n    ----> 1 panel.apply(lambda df: numpy.float64(0.0), axis=[1, 2]) # NG\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\panel.py in apply(self, func, axis, **kwargs)\r\n        967         # 2d-slabs\r\n        968         if isinstance(axis, (tuple,list)) and len(axis) == 2:\r\n    --> 969             return self._apply_2d(f, axis=axis)\r\n        970\r\n        971         axis = self._get_axis_number(axis)\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\panel.py in _apply_2d(self, func, axis)\r\n       1066             results.append((e,obj))\r\n       1067\r\n    -> 1068         return self._construct_return_type(dict(results))\r\n       1069\r\n       1070     def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\r\n\r\n    C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\panel.py in _construct_return_type(self, result, axes)\r\n       1122\r\n       1123         raise PandasError('invalid _construct_return_type [self->%s] '\r\n    -> 1124                           '[result->%s]' % (self, result))\r\n       1125\r\n       1126     def _wrap_result(self, result, axis):\r\n\r\n    PandasError: invalid _construct_return_type [self-><class 'pandas.core.panel.Panel'>\r\n    Dimensions: 5 (items) x 5 (major_axis) x 5 (minor_axis)\r\n    Items axis: 0 to 4\r\n    Major_axis axis: 0 to 4\r\n    Minor_axis axis: 0 to 4] [result->{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}]\r\n\r\n    In [8]:\r\n"""
10324,87098278,tnilanon,sinhrks,2015-06-10 20:34:49,2015-07-01 13:46:10,2015-07-01 13:46:10,closed,,0.17.0,2,Bug;Categorical;Compat,https://api.github.com/repos/pydata/pandas/issues/10324,b'mapping with category data does not work',"b'```\r\nimport pandas as pd\r\na = pd.Series([1, 2, 3, 4])\r\nb = pd.Series([""even"", ""odd"", ""even"", ""odd""], dtype=""category"")\r\nc = pd.Series([""even"", ""odd"", ""even"", ""odd""])\r\n\r\nprint(""-- map by category"")\r\ntry:\r\n    print(a.map(b))\r\nexcept AttributeError as e:\r\n    print(e)\r\n\r\nprint(""-- map by string"")\r\nprint(a.map(c))\r\n```\r\n\r\nhas the following output:\r\n\r\n> -- map by category\r\n> \'Categorical\' object has no attribute \'flags\'\r\n> -- map by string\r\n> 0     odd\r\n> 1    even\r\n> 2     odd\r\n> 3     NaN\r\n> dtype: object\r\n'"
10322,86789275,evanpw,jreback,2015-06-10 01:04:09,2015-09-19 00:38:31,2015-06-10 10:28:59,closed,,0.16.2,5,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/10322,b'Bug in to_json causing segfault with a CategoricalIndex (GH #10317)',b'Fixed GH #10317\r\nStole tests from #10321\r\n'
10321,86785199,jreback,jreback,2015-06-10 00:42:11,2015-06-10 01:07:41,2015-06-10 01:07:41,closed,,0.16.2,1,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/10321,"b'BUG: Bug in to_json with certain orients and a CategoricalIndex would segfault, closes #10317'","b'xref #10317 \r\n\r\n```\r\nIn [17]: df = DataFrame({ \'A\' : pd.Series([3,2,2],index=pd.Categorical([1,2,3],categories=[1,2,3])), \'B\' : pd.Categorical(list(\'aab\')) })\r\n\r\nIn [18]: df\r\nOut[18]: \r\n   A  B\r\n1  3  a\r\n2  2  a\r\n3  2  b\r\n\r\nIn [19]: df.index\r\nOut[19]: CategoricalIndex([1, 2, 3], categories=[1, 2, 3], ordered=False, dtype=\'category\')\r\n\r\nIn [20]: df.dtypes\r\nOut[20]: \r\nA       int64\r\nB    category\r\ndtype: object\r\n\r\nIn [31]: def f(orient):\r\n    print ""orient->%s"" % orient\r\n    print df.to_json(orient=orient)\r\n   ....:     \r\n\r\nIn [32]: f(\'columns\')\r\norient->columns\r\n{""A"":{""1"":3,""2"":2,""3"":2},""B"":{""1"":""a"",""2"":""a"",""3"":""b""}}\r\n\r\nIn [33]: f(\'index\')\r\norient->index\r\n{""1"":{""A"":3,""B"":""a""},""2"":{""A"":2,""B"":""a""},""3"":{""A"":2,""B"":""b""}}\r\n\r\nIn [34]: f(\'split\')\r\norient->split\r\n{""columns"":[""A"",""B""],""index"":[1,2,3],""data"":[[3,""a""],[2,""a""],[2,""b""]]}\r\n\r\nIn [35]: f(\'records\')\r\norient->records\r\n[{""A"":3,""B"":""a""},{""A"":2,""B"":""a""},{""A"":2,""B"":""b""}]\r\n\r\nIn [36]: f(\'values\')\r\norient->values\r\n[[3,""a""],[2,""a""],[2,""b""]]\r\n```'"
10317,86672829,sborgeson,jreback,2015-06-09 17:25:42,2015-06-10 16:51:15,2015-06-10 10:30:12,closed,,0.16.2,10,Bug;Categorical;Difficulty Intermediate;Effort Low;Enhancement;Error Reporting;IO JSON,https://api.github.com/repos/pydata/pandas/issues/10317,b'df.to_json segfaults with categorical index',"b""DataFrame.to_json is reliably segfaulting python when the DataFrame has an index of type CategoricalIndex.\r\n\r\n```python\r\nimport pandas as pd\r\nidx = pd.Categorical([1,2,3], categories=[1,2,3])\r\ndf = pd.DataFrame( {  'count' : pd.Series([3,2,2],index=idx) } )\r\n# this will crash python (2.6.X or 2.7.X on linux 64 or win 64 with pandas 0.16.1)\r\nprint df.to_json(orient='split')\r\n```\r\nIf I call with orient='index', I get a value error instead:\r\n```python\r\n# this throws a ValueError\r\nprint df.to_json(orient='index')\r\n```\r\n```\r\nValueError: Label array sizes do not match corresponding data shape\r\n```\r\nFor what it's worth, my work around, which is acceptable in my application, is to convert my index to strings:\r\n```python\r\ndf.index = df.index.astype(str)\r\nprint df.to_json(orient='split')\r\n```\r\nWindows config:\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.1\r\nnose: None\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: 2.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.0.3\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\nlinux config:\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.6.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-274.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.1\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: None\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n"""
10308,85963299,behzadnouri,jreback,2015-06-07 23:34:30,2015-06-09 23:30:23,2015-06-09 14:43:27,closed,,0.16.2,5,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/10308,b'BUG: bug in setitem where type promotion is applied to entire block',"b""closes https://github.com/pydata/pandas/issues/10280\r\n\r\non master:\r\n\r\n```python\r\n>>> df\r\n   foo  bar  baz\r\na    0    1    2\r\nb    3    4    5\r\n>>> df.dtypes\r\nfoo    int64\r\nbar    int64\r\nbaz    int64\r\ndtype: object\r\n>>> df.loc['a', 'bar'] = 3.14\r\n>>> df.dtypes\r\nfoo    float64\r\nbar    float64\r\nbaz    float64\r\ndtype: object\r\n```\r\n\r\non branch:\r\n```python\r\n>>> df.dtypes\r\nfoo    int64\r\nbar    int64\r\nbaz    int64\r\ndtype: object\r\n>>> df.loc['a', 'bar'] = 3.14\r\n>>> df.dtypes\r\nfoo      int64\r\nbar    float64\r\nbaz      int64\r\ndtype: object\r\n>>> df\r\n   foo   bar  baz\r\na    0  3.14    2\r\nb    3  4.00    5\r\n```"""
10306,85834582,behzadnouri,jreback,2015-06-07 01:41:01,2015-08-22 11:36:04,2015-06-07 23:05:24,closed,,0.16.2,1,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/10306,b'BUG: bug in json serialization when frame has mixed types',"b""closes https://github.com/pydata/pandas/issues/10289\r\n\r\non master:\r\n\r\n```python\r\n>>> df\r\n   1st  2nd  3rd  4th   5th\r\na   10    1  foo  0.1  0.01\r\nb   20    2  bar  0.2  0.02\r\nc   30    3  baz  0.3  0.03\r\nd   40    4  qux  0.4  0.04\r\n\r\n>>> read_json(df.to_json(orient='index'), orient='index', convert_axes=False)\r\n   1st  2nd  3rd  4th   5th\r\na   40    4  qux  0.4  0.04\r\n```\r\n\r\non branch:\r\n```python\r\n>>> read_json(df.to_json(orient='index'), orient='index', convert_axes=False)\r\n   1st  2nd  3rd  4th   5th\r\na   10    1  foo  0.1  0.01\r\nb   20    2  bar  0.2  0.02\r\nc   30    3  baz  0.3  0.03\r\nd   40    4  qux  0.4  0.04\r\n```\r\n\r\n"""
10304,85800125,evanpw,jreback,2015-06-06 18:53:16,2015-06-10 13:40:04,2015-06-09 23:45:58,closed,,0.16.2,8,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10304,b'BUG: Categorical.remove_categories(np.nan) fails when underlying dtype is float',"b'Fixes GH #10156. This also makes different null values indistinguishable inside of remove_categories, but they\'re already indistinguishable in most other contexts:\r\n\r\n``` .python\r\n>>> pd.Categorical([], categories=[np.nan, None])\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/core/categorical.py"", line 289, in __init__\r\n    categories = self._validate_categories(categories)\r\n  File ""pandas/core/categorical.py"", line 447, in _validate_categories\r\n    raise ValueError(\'Categorical categories must be unique\')\r\nValueError: Categorical categories must be unique\r\n```'"
10302,85781211,evanpw,jreback,2015-06-06 16:57:35,2015-09-19 00:38:30,2015-06-07 23:06:15,closed,,0.16.2,1,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10302,b'BUG: read_csv does not set index name on an empty DataFrame',b'Fixes GH #10184.'
10298,85702150,scls19fr,jreback,2015-06-06 05:29:13,2015-06-09 11:54:08,2015-06-09 11:54:08,closed,,0.16.2,8,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10298,b'BUG: plotting grouped_hist with a single row frame #10214',b'closes #10214 '
10292,85656615,sinhrks,jreback,2015-06-05 23:31:04,2015-06-08 13:20:31,2015-06-07 23:02:54,closed,,0.16.2,1,Bug;Frequency;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10292,b'BUG: TimedeltaIndex slicing may reset freq',"b""``DatetimeIndex`` preserve freq after slicing.\r\n\r\n```\r\ndi = pd.date_range('2001', '2005', freq='D')\r\ndi[1:5:2]\r\n# DatetimeIndex(['2001-01-02', '2001-01-04'], dtype='datetime64[ns]', freq='2D', tz=None)\r\n```\r\n\r\nBut ``TimedeltaIndex`` doesn't.\r\n```\r\ntdi = pd.timedelta_range('1day', '5day', freq='D')\r\ntdi[1:5:2]\r\n# TimedeltaIndex(['2 days', '4 days'], dtype='timedelta64[ns]', freq=None)\r\n```"""
10291,85655291,songhuiming,jreback,2015-06-05 23:20:24,2016-02-12 03:02:26,2016-02-12 03:02:26,closed,,0.18.0,12,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10291,"b""KeyError: '__dummy__'  for pd.crosstab in pandas""","b""get ~~ KeyError: '__dummy__' ~~ when I run the following:\r\n\r\n    np.random.seed(seed = 99)\r\n    s = np.random.randint(1,10,200)\r\n    s = pd.Series(np.where(s > 9, np.nan, s))\r\n    s1 = s[:100]\r\n    s2 = s[100:]\r\n    pd.crosstab(s1, s2)\r\n\r\n    KeyError: '__dummy__\r\n\r\n\r\n"""
10289,85583919,sborgeson,jreback,2015-06-05 17:09:53,2015-07-02 19:04:02,2015-06-07 23:05:24,closed,,0.16.2,3,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/10289,"b""mixed data type to_json(orient='index') repeats only the first index value""","b""```python\r\nfrom pandas import DataFrame\r\ndf = DataFrame({\r\n    'one' : pandas.Series(['foo','bar','bee'],index=['a','b','c']),\r\n    'two' : pandas.Series([1,2,3],index=['a','b','c'])   } )\r\ndf.to_json(orient='index')\r\n```\r\nObserve that the first index value 'a' is used for all the record returned, rather than for just the first one. I've reproduced this issue on a linux machine with the config below, but it was first observed on OSX and can reproduce it on Windows, all with pandas 0.16.1.\r\n\r\nThis issue was not present in pandas version 0.14.x\r\n\r\n>>> pandas.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.6.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-274.el5\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.1\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: None\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n"""
10283,85395767,mortada,jreback,2015-06-05 03:00:16,2015-08-30 18:08:39,2015-08-26 01:41:14,closed,,0.17.0,36,Bug;Indexing;Needs Discussion,https://api.github.com/repos/pydata/pandas/issues/10283,b'BUG: DataFrame.where does not handle Series slice correctly (#10218)',b'closes #10218 '
10280,85240315,jreback,jreback,2015-06-04 19:25:04,2015-06-09 14:43:27,2015-06-09 14:43:27,closed,,0.16.2,0,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/10280,b'BUG: indexers coerce partial assignment of float->int',"b""xref #10264\r\n \r\nconverts int -> floats for the entire block.\r\n\r\n```\r\nIn [1]: x = pd.DataFrame(data=np.zeros((5,5),dtype=np.int),columns=['a','b','c','d','e'], index=['one','two','three','four','five'])\r\n\r\nIn [3]: x.loc['three','e'] = 3.1\r\n\r\nIn [4]: x\r\nOut[4]: \r\n       a  b  c  d    e\r\none    0  0  0  0  0.0\r\ntwo    0  0  0  0  0.0\r\nthree  0  0  0  0  3.1\r\nfour   0  0  0  0  0.0\r\nfive   0  0  0  0  0.0\r\n\r\nIn [5]: x.dtypes\r\nOut[5]: \r\na    float64\r\nb    float64\r\nc    float64\r\nd    float64\r\ne    float64\r\ndtype: object\r\n```\r\n\r\nok for other kinds of assignments\r\n```\r\nIn [6]: x = pd.DataFrame(data=np.zeros((5,5),dtype=np.int),columns=['a','b','c','d','e'], index=['one','two','three','four','five'])\r\n\r\n [8]: x.loc['three','e'] = 'foo'\r\n\r\nIn [9]: x\r\nOut[9]: \r\n       a  b  c  d    e\r\none    0  0  0  0    0\r\ntwo    0  0  0  0    0\r\nthree  0  0  0  0  foo\r\nfour   0  0  0  0    0\r\nfive   0  0  0  0    0\r\n\r\nIn [10]: x.dtypes\r\nOut[10]: \r\na     int64\r\nb     int64\r\nc     int64\r\nd     int64\r\ne    object\r\ndtype: object\r\n\r\n```"""
10274,85123584,ssaumitra,jreback,2015-06-04 14:30:17,2015-09-02 10:41:49,2015-08-29 00:08:16,closed,,0.17.0,25,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/10274,b'BUG: Unhandled ValueError when Bigquery called through io.gbq returns zero rows #10273',b'closes #10273 '
10273,85117597,ssaumitra,jreback,2015-06-04 14:14:22,2015-08-29 00:08:29,2015-08-29 00:08:29,closed,,0.17.0,3,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/10273,b'BUG: Unhandled ValueError when Bigquery called through io.gbq returns zero rows',"b'Sample code below throws ValueError. Ideally it should return a DataFrame with zero rows.\r\n\r\n```python\r\n\r\nimport pandas as pd\r\n#This query returns zero rows\r\nquery = ""SELECT * FROM [publicdata:samples.wikipedia] where timestamp=-9999999""\r\ndf = pd.io.gbq.read_gbq(query, project_id=""my_bigquery_project_id"")\r\n```\r\n```\r\nJob not yet complete...\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/io/gbq.py"", line 341, in read_gbq\r\n    final_df = concat(dataframe_list, ignore_index = True)\r\n  File ""pandas/tools/merge.py"", line 754, in concat\r\n    copy=copy)\r\n  File ""pandas/tools/merge.py"", line 799, in __init__\r\n    raise ValueError(\'All objects passed were None\')\r\nValueError: All objects passed were None\r\n```\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: b144cc1285e8f353112075b3d4eff75a68ad1699\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.1-64-gb144cc1\r\nnose: 1.3.6\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.1\r\npytz: 2015.4\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.9.1\r\napiclient: 1.4.0\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\n'"
10272,85067673,jreback,jreback,2015-06-04 12:10:26,2015-06-05 21:41:05,2015-06-05 21:41:05,closed,,0.16.2,0,Bug,https://api.github.com/repos/pydata/pandas/issues/10272,b'BUG: bug in cache updating when consolidating #10264',b'closes #10264 '
10269,84899304,kawochen,jreback,2015-06-04 04:06:49,2015-06-10 20:01:05,2015-06-10 20:01:04,closed,,0.16.2,8,Bug;Dtypes;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10269,b'BUG: GH10160 in  DataFrame construction from dict with datetime64 index',b'closes #10160\r\ncloses #9456\r\n where ``DataFrame/Series`` construction from nested ``dict`` with ``datetime64`` index returns a ``DataFrame\\Series`` of ``NaN``s.\r\n\r\n'
10265,84787632,bashtage,jreback,2015-06-03 21:53:02,2015-07-15 21:14:39,2015-07-14 12:47:25,closed,,0.17.0,40,API Design;Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/10265,"b""BUG: Ensure 'coerce' actually coerces datatypes""","b""Changes behavior of convert objects so that passing 'coerce' will\r\nensure that data of the correct type is returned, even if all\r\nvalues are null-types (NaN or NaT).\r\n\r\ncloses #9589"""
10264,84771124,jbezaire,jreback,2015-06-03 21:05:16,2015-06-05 21:41:05,2015-06-05 21:41:05,closed,,0.16.2,15,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10264,b'view into modified dataframe of ints causes subsequent set_value to not work properly',"b""The following code snippet produces unexpected output:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nx = pd.DataFrame(data=np.zeros((5,5),dtype=np.int),columns=['a','b','c','d','e'])\r\nx['f'] = 0\r\nx.f.values[3]=1\r\ny = x.iloc[np.arange(2,len(x))]\r\nx.f.values[3]=2\r\nprint x\r\nprint x.f.values\r\n```\r\n```\r\nOut[130]: \r\n   a  b  c  d  e  f\r\n0  0  0  0  0  0  0\r\n1  0  0  0  0  0  0\r\n2  0  0  0  0  0  0\r\n3  0  0  0  0  0  1\r\n4  0  0  0  0  0  0\r\n[0 0 0 2 0]\r\n```\r\n\r\nThe expected output is:\r\n```\r\nOut[131]: \r\n   a  b  c  d  e  f\r\n0  0  0  0  0  0  0\r\n1  0  0  0  0  0  0\r\n2  0  0  0  0  0  0\r\n3  0  0  0  0  0  2\r\n4  0  0  0  0  0  0\r\n[0 0 0 2 0]\r\n```\r\n\r\nThe behavior only manifests if the dataframe has had a column added to it (f in this case), if the columns are ints, and if the view is constructed with certain kinds of indexing (y=x.iloc[2:] works fine for instance).\r\n\r\nHere is my version info:\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-229.4.2.el7.jump.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2.dev\r\nnose: 1.3.4\r\nCython: 0.20.2\r\nnumpy: 1.9.0\r\nscipy: 0.14.1rc1.dev-Unknown\r\nstatsmodels: None\r\nIPython: 3.1.0\r\nsphinx: 1.3b1\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.10\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.0\r\nopenpyxl: 2.1.4\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: 2.4.3\r\nsqlalchemy: None\r\npymysql: 0.6.2.None\r\npsycopg2: 2.5.4 (dt dec pq3 ext)\r\n"""
10258,84524831,sinhrks,sinhrks,2015-06-03 12:07:26,2015-06-04 13:16:14,2015-06-03 21:21:22,closed,,0.16.2,2,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/10258,b'BUG: SparseSeries constructor ignores input data name',"b""When constructing ``Series`` from ``Series``, name attribute is preserved otherwise specified.\r\n\r\n```\r\nimport pandas as pd\r\ns = pd.Series([1, 2, 3], name='a')\r\npd.Series(s)\r\n# 0    1\r\n# 1    2\r\n# 2    3\r\n# Name: a, dtype: int64\r\n\r\npd.Series(s, name='x')\r\n# 0    1\r\n# 1    2\r\n# 2    3\r\n# Name: x, dtype: int64\r\n```\r\n\r\nBut ``SparseSeries`` doesn't preserve its name.\r\n\r\n```\r\ns = pd.SparseSeries([1, 2, 3], name='a')\r\npd.SparseSeries(s)\r\n# 0    1\r\n# 1    2\r\n# 2    3\r\n# dtype: int64\r\n# BlockIndex\r\n# Block locations: array([0], dtype=int32)\r\n# Block lengths: array([3], dtype=int32)\r\n```"""
10251,84119177,mortada,jreback,2015-06-02 17:32:47,2015-06-05 21:29:03,2015-06-05 21:28:59,closed,,0.16.2,2,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/10251,b'ENH: make sure return dtypes for nan funcs are consistent',"b'this is a follow-up to https://github.com/pydata/pandas/pull/10172\r\n\r\nIn addition to `mean()`, this PR also makes sure the returned dtype is consistent for `std()`, `var()`, `skew()`, and `kurt()`'"
10249,83990269,cmeeren,jreback,2015-06-02 12:00:16,2015-06-08 13:50:57,2015-06-08 13:28:39,closed,,0.16.2,15,Bug;IO CSV;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10249,b'Datetime resolution coercion',"b'Fixes #10245 \r\n\r\nWithout this fix, if the ``date_parse`` function passed to e.g. ``read_csv`` outputs a ``np.datetime64`` array, then it must be of ``ns`` resolution. With this fix, all time resolutions may be used.'"
10245,83373915,cmeeren,jreback,2015-06-01 08:05:39,2015-06-08 13:28:39,2015-06-08 13:28:39,closed,,0.16.2,31,Bug;IO CSV;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10245,b'numpy error using read_csv with parse_dates=[...] and index_col=[...]',"b'Consider a file of the following format:\r\n```\r\nweek,sow,prn,rxstatus,az,elv,l1_cno,s4,s4_cor,secsigma1,secsigma3,secsigma10,secsigma30,secsigma60,code_carrier,c_cstdev,tec45,tecrate45,tec30,tecrate30,tec15,tecrate15,tec00,tecrate00,l1_loctime,chanstatus,l2_locktime,l2_cno\r\n1765,68460.00,126,00E80000,0.00,0.00,39.38,0.118447,0.107595,0.252663,0.532384,0.600540,0.603073,0.603309,-13.255543,0.114,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,1692.182,8C023D84,0.000,0.00\r\n1765,68460.00,23,00E80000,0.00,0.00,53.48,0.034255,0.021177,0.035187,0.042985,0.061142,0.061738,0.061801,-22.760003,0.015,24.955111,0.112239,25.115330,-0.119774,25.146603,-0.065852,24.747576,-0.243804,10426.426,08109CC4,10409.660,44.52\r\n1765,68460.00,13,00E80000,0.00,0.00,54.28,0.046218,0.019314,0.037818,0.056421,0.060602,0.060698,0.060735,-20.679035,0.090,25.670250,-0.070761,25.752224,-0.055089,26.045048,-0.180056,25.360369,-0.062119,7553.020,18109CA4,7202.660,47.27\r\n```\r\n\r\nI try to read that with the following code\r\n\r\n```python\r\ndata = pd.read_csv(FILE, date_parser=GPStime2datetime,\r\n                   parse_dates={\'datetime\': [\'week\', \'sow\']},\r\n                   index_col=[\'datetime\', \'prn\'])\r\n```\r\n\r\nHere I\'m parsing ``week`` and ``sow`` into a ``datetime`` column using a custom function (this works properly) and using ``datetime`` and the ``prn`` column as a ``MultiIndex``. The file is read successfully when ``index_col=\'datetime\'``, but not when trying to create the ``MultiIndex`` using ``index_col=[\'datetime\', \'prn\']`` (or when using column numbers instead of names). I get the following traceback:\r\n\r\n```\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py"", line 474, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py"", line 260, in _read\r\n    return parser.read()\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py"", line 721, in read\r\n    ret = self._engine.read(nrows)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1223, in read\r\n    index, names = self._make_index(data, alldata, names)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py"", line 898, in _make_index\r\n    index = self._agg_index(index, try_parse_dates=False)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py"", line 984, in _agg_index\r\n    index = MultiIndex.from_arrays(arrays, names=self.index_names)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\index.py"", line 4410, in from_arrays\r\n    cats = [Categorical.from_array(arr, ordered=True) for arr in arrays]\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\categorical.py"", line 355, in from_array\r\n    return Categorical(data, **kwargs)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\categorical.py"", line 271, in __init__\r\n    codes, categories = factorize(values, sort=False)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py"", line 131, in factorize\r\n    (hash_klass, vec_klass), vals = _get_data_algo(vals, _hashtables)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py"", line 412, in _get_data_algo\r\n    mask = com.isnull(values)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\common.py"", line 230, in isnull\r\n    return _isnull(obj)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\common.py"", line 240, in _isnull_new\r\n    return _isnull_ndarraylike(obj)\r\n\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\common.py"", line 330, in _isnull_ndarraylike\r\n    result = np.isnan(values)\r\n\r\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\r\n```\r\n\r\nI am using Python 2.7, Pandas 0.16.1 and numpy 1.9.2.'"
10242,82933958,nleehone,jreback,2015-05-30 22:37:57,2015-09-04 12:15:33,2015-09-04 12:15:33,closed,,0.17.0,14,Bug;Docs;Numeric,https://api.github.com/repos/pydata/pandas/issues/10242,b'Variance is not calculated correctly in some cases + inconsistent definition',"b""The var method acts inconsistently when used on the Series vs the numpy array returned from values:\r\n```python\r\ndat = pd.DataFrame({'x': [1,2,3,4,0,0,0]})\r\nprint(dat['x'].values.var())\r\nprint(dat['x'].var())\r\n```\r\n```\r\nPrints:\r\n2.24489795918\r\n2.61904761905\r\n```\r\nThis is due to numpy using ddof=0 as its default (calculates the biased variance), whereas pandas calculates the unbiased variance by default. These two should be consistent, so either pandas should adapt or numpy should (maybe it should be numpy since we should calculate unbiased by default?).\r\n\r\n\r\nThe other problem is that pandas does not calculate the variance of this DataFrame properly.\r\nInconsistent definition aside, the variance should clearly not be zero when calculated from pandas.\r\n```python\r\ndat = pd.DataFrame({'x': [9.0692124699,9.0692124692,9.0692124702,9.0692124686,\r\n                          9.0692124687,9.0692124707,9.0692124679,9.0692124685,\r\n                          9.0692124698,9.0692124719,9.0692124698,9.0692124692,\r\n                          9.0692124689,9.0692124673,9.0692124707,9.0692124714,\r\n                          9.0692124714,9.0692124734,9.0692124719,9.0692124710,\r\n                          9.0692124694,9.0692124705,9.0692124713,9.0692124717\r\n]})\r\nprint(dat['x'].values.var())\r\nprint(dat['x'].var())\r\n```\r\n```\r\nPrints:\r\n2.06817742558e-18\r\n0.0\r\n```\r\n\r\nHere is the system information:\r\nINSTALLED VERSIONS:\r\ncommit: None\r\npython: 3.4.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-52-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.16.1\r\nnumpy: 1.9.2"""
10241,82922347,sinhrks,jreback,2015-05-30 21:47:52,2015-06-02 19:26:12,2015-06-02 10:39:04,closed,,0.16.2,4,Bug,https://api.github.com/repos/pydata/pandas/issues/10241,b'BUG: SparseSeries.abs() resets name',"b""```\r\n# OK: name is preserved\r\ns = pd.Series([1, 2, -3], name='a')\r\ns.abs()\r\n# 0    1\r\n# 1    2\r\n# 2    3\r\n# Name: a, dtype: int64\r\n\r\n# NG: name is reset\r\ns = pd.SparseSeries([1, 0, -3], name='a')\r\ns.name\r\n# 'a'\r\n\r\ns.abs()\r\n# 0    1\r\n# 1    0\r\n# 2    3\r\n# dtype: int64\r\n# BlockIndex\r\n# Block locations: array([0], dtype=int32)\r\n# Block lengths: array([3], dtype=int32)\r\n```"""
10240,82914553,sinhrks,jreback,2015-05-30 21:14:49,2015-06-02 19:26:12,2015-06-02 10:38:48,closed,,0.16.2,1,Bug,https://api.github.com/repos/pydata/pandas/issues/10240,b'BUG: Series arithmetic methods incorrectly hold name',"b'Closes #10068.\r\n\r\nShould handle the case ``_maybe_match_name`` is ``None``, because of continuous ``__finalize__``.'"
10233,82507495,scls19fr,scls19fr,2015-05-29 17:09:10,2015-06-07 06:12:25,2015-06-07 06:12:25,closed,,0.16.2,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10233,b'BUG: plotting grouped_hist with a single row frame #10214',b'Should closes https://github.com/pydata/pandas/issues/10214'
10228,82271589,kelvin22,jreback,2015-05-29 05:29:41,2015-06-26 23:24:40,2015-06-26 23:24:40,closed,,0.17.0,4,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/10228,b'resample() with how=count causes Segmentation Fault',"b""Runing the code as is below results in a segmentation fault when it gets to resample()\r\nThe first pass runs, and then either the 2nd or 3rd run produces:\r\n\r\nSegmentation fault: 11\r\n\r\nIt happens on:\r\n- Python 2.7.9 and Pandas 0.16.1\r\n- Python 3.4.3 and Pandas 0.16.1\r\n\r\nIt does not happen on:\r\n- Python 2.7.9 with Pandas 0.12.0\r\n\r\nThose are the only ones I've tested\r\n\r\n```\r\nimport pandas as pd\r\n\r\nframe = pd.DataFrame({'delay_category': {pd.Timestamp('2014-05-18 08:50:00'): 'yes',\r\n  pd.Timestamp('2014-05-19 14:35:00'): 'yes',\r\n  pd.Timestamp('2014-05-22 08:45:00'): 'yes',\r\n  pd.Timestamp('2014-05-27 10:00:00'): 'no',\r\n  pd.Timestamp('2014-05-28 11:05:00'): 'no'}})\r\n\r\nframe_filter = frame[frame['delay_category'] == 'da']['delay_category']\r\n\r\nfor i in range(1,10):\r\n    frame_filter.resample('d', how='count')\r\n\r\n```\r\nSetting how=sum also does not result in the issue.\r\n\r\nIt only happens after the frame has been selected with a parameter that doesn't work, resulting in an empty series.\r\nEg. this works fine:\r\n```\r\nframe_filter = frame[frame['delay_category'] == 'yes']['delay_category']\r\n\r\nfor i in range(1,10):\r\n    frame_filter.resample('d', how='count')\r\n\r\n```\r\n\r\n"""
10220,82019885,scls19fr,TomAugspurger,2015-05-28 16:39:33,2015-06-02 19:29:23,2015-05-29 17:10:25,closed,,No action,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10220,b'Update plotting.py',b'Should fix https://github.com/pydata/pandas/issues/10214#issuecomment-106091734'
10218,81519720,dunpealer,jreback,2015-05-27 16:28:14,2015-08-26 01:41:14,2015-08-26 01:41:14,closed,,0.17.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10218,b'DataFrame#where broken in latest 0.16.1 release',"b'Simple usecase of `DataFrame#where` fails on a freshly installed latest Pandas release:\r\n\r\n    In [1]: df = DataFrame([1, 2]); df\r\n    Out[1]: \r\n       0\r\n    0  1\r\n    1  2\r\n    \r\n    In [2]: df.where(df[0]==1)\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-2-7b64bb424f7b> in <module>()\r\n    ----> 1 df.where(df[0]==1)\r\n    \r\n    /home/dunpeal/.env/lib/python3.4/site-packages/pandas/core/generic.py in where(self, cond, other, inplace, axis, level, try_cast, raise_on_error)\r\n       3445 \r\n       3446         if isinstance(cond, NDFrame):\r\n    -> 3447             cond = cond.reindex(**self._construct_axes_dict())\r\n       3448         else:\r\n       3449             if not hasattr(cond, \'shape\'):\r\n    \r\n    /home/dunpeal/.env/lib/python3.4/site-packages/pandas/core/series.py in reindex(self, index, **kwargs)\r\n       2142     @Appender(generic._shared_docs[\'reindex\'] % _shared_doc_kwargs)\r\n       2143     def reindex(self, index=None, **kwargs):\r\n    -> 2144         return super(Series, self).reindex(index=index, **kwargs)\r\n       2145 \r\n       2146     @Appender(generic._shared_docs[\'fillna\'] % _shared_doc_kwargs)\r\n    \r\n    /home/dunpeal/.env/lib/python3.4/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs)\r\n       1747         if kwargs:\r\n       1748             raise TypeError(\'reindex() got an unexpected keyword \'\r\n    -> 1749                     \'argument ""{0}""\'.format(list(kwargs.keys())[0]))\r\n       1750 \r\n       1751         self._consolidate_inplace()\r\n    \r\n    TypeError: reindex() got an unexpected keyword argument ""columns""\r\n'"
10216,81435635,vincentdavis,vincentdavis,2015-05-27 13:04:55,2015-07-06 17:41:14,2015-07-06 17:41:14,closed,,0.17.0,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10216,"b'add try / except to address issue #10154 to_datetime, Inconsistent be\xa1\xad'","b'closes #10154 to_datetime, Inconsistent behavior with invalid dates.\r\n\r\nresults are now\r\n```\r\nIn [2]: pd.to_datetime(\'2015-02-29\', format=""%Y-%m-%d"", coerce=True)\r\nOut[2]: NaT\r\n\r\nIn [3]: pd.to_datetime(\'2015-03-32\', format=""%Y-%m-%d"", coerce=True)\r\nOut[3]: NaT\r\n\r\nIn [4]: pd.to_datetime(\'2015-02-32\', format=""%Y-%m-%d"", coerce=True)\r\nOut[4]: NaT\r\n\r\nIn [5]: pd.to_datetime(\'2015-04-31\', format=""%Y-%m-%d"", coerce=True)\r\nOut[5]: NaT\r\n```\r\n'"
10214,81423486,scls19fr,jreback,2015-05-27 12:12:06,2015-06-09 11:54:20,2015-06-09 11:54:19,closed,,0.16.2,6,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/10214,"b""hist raises AttributeError: 'AxesSubplot' object has no attribute 'ndim' with a one row DataFrame""","b'Hello,\r\n\r\nthis code works fine:\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    bins = np.arange(80, 100 + 2, 1)\r\n    df = pd.DataFrame({""Name"": [""AAA"", ""BBB""], ""ByCol"": [1, 2], ""Mark"": [85, 89]})\r\n    df\r\n\r\n       ByCol  Mark Name\r\n    0      1    85  AAA\r\n    1      2    89  BBB\r\n\r\n    df[""Mark""].hist(by=df[""ByCol""], bins=bins)\r\n    plt.show()\r\n\r\nbut same code with a one row DataFrame such as\r\n\r\n    df = pd.DataFrame({""Name"": [""AAA""], ""ByCol"": [1], ""Mark"": [85]})\r\n    df\r\n\r\n       ByCol  Mark Name\r\n    0      1    85  AAA\r\n\r\nraises:\r\n\r\n    ---------------------------------------------------------------------------\r\n    AttributeError                            Traceback (most recent call last)\r\n    <ipython-input-35-93bcd47d24f2> in <module>()\r\n    ----> 1 df[""Mark""].hist(by=df[""Mark""], bins=bins)\r\n\r\n    //anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in hist_series(self, by, ax, grid, xlabelsize, xrot, ylabelsize, yrot, figsize, bins, **kwds)\r\n       2847                             **kwds)\r\n       2848\r\n    -> 2849     if axes.ndim == 1 and len(axes) == 1:\r\n       2850         return axes[0]\r\n       2851     return axes\r\n\r\n    AttributeError: \'AxesSubplot\' object has no attribute \'ndim\'\r\n\r\nKind regards'"
10196,79402706,rekcahpassyla,shoyer,2015-05-22 10:46:45,2015-06-04 11:17:25,2015-06-03 19:02:45,closed,,0.16.2,10,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10196,b'BUG: Raise TypeError only if key DataFrame is not empty #10126',b'Proposed fix for #10126'
10194,79386999,rekcahpassyla,jreback,2015-05-22 09:59:41,2015-06-18 12:56:00,2015-06-18 12:56:00,closed,,0.17.0,10,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/10194,b'BUG: Check for size != 0 before trying to insert #10193',b'Proposed fix for #10193'
10193,79381307,rekcahpassyla,jreback,2015-05-22 09:44:57,2015-06-18 15:36:30,2015-06-18 15:36:30,closed,,0.17.0,2,API Design;Bug;Difficulty Intermediate;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/10193,b'BUG: indexing error when setting item in empty Series which has a frequency',"b'Another somewhat degenerate case that works in 0.15.2, fails in 0.16.1\r\n\r\n```python\r\nimport pandas as pd\r\n\r\npd.show_versions()\r\n\r\n\r\nts = pd.TimeSeries()\r\n\r\nts[pd.datetime(2012, 1, 1)] = 47\r\n\r\n\r\nts2 = pd.TimeSeries(0, pd.date_range(\'2011-01-01\', \'2011-01-01\'))[:0]\r\n\r\nts2[pd.datetime(2012, 1, 1)] = 47\r\n```\r\n\r\n0.15.2 output:\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.21\r\nnumpy: 1.9.2\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 3.0.0-dev\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.7\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\n0.16.1 output:\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.1-46-g0aceb38\r\nnose: 1.3.6\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.14.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.1.0\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.7.2\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.4\r\npymysql: None\r\npsycopg2: None\r\n\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\nc:\\test_set_empty_series_with_freq.py in <module>()\r\n     11 ts2 = pd.TimeSeries(0, pd.date_range(\'2011-01-01\', \'2011-01-01\'))[:0]\r\n     12\r\n---> 13 ts2[pd.datetime(2012, 1, 1)] = 47\r\n\r\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\series.pyc in __setitem__(self, key, value)\r\n    687         # do the setitem\r\n    688         cacher_needs_updating = self._check_is_chained_assignment_possible()\r\n--> 689         setitem(key, value)\r\n    690         if cacher_needs_updating:\r\n    691             self._maybe_update_cacher()\r\n\r\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\series.pyc in setitem(key, value)\r\n    660                             pass\r\n    661                 try:\r\n--> 662                     self.loc[key] = value\r\n    663                 except:\r\n    664                     print """"\r\n\r\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\indexing.pyc in __setitem__(self, key, value)\r\n    113     def __setitem__(self, key, value):\r\n    114         indexer = self._get_setitem_indexer(key)\r\n--> 115         self._setitem_with_indexer(indexer, value)\r\n    116\r\n    117     def _has_valid_type(self, k, axis):\r\n\r\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n    272                 if self.ndim == 1:\r\n    273                     index = self.obj.index\r\n--> 274                     new_index = index.insert(len(index),indexer)\r\n    275\r\n    276                     # this preserves dtype of the value\r\n\r\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\tseries\\index.pyc in insert(self, loc, item)\r\n   1523             # check freq can be preserved on edge cases\r\n   1524             if self.freq is not None:\r\n-> 1525                 if (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\r\n   1526                     freq = self.freq\r\n   1527                 elif (loc == len(self)) and item - self.freq == self[-1]:\r\n\r\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\tseries\\index.pyc in __getitem__(self, key)\r\n   1351         getitem = self._data.__getitem__\r\n   1352         if np.isscalar(key):\r\n-> 1353             val = getitem(key)\r\n   1354             return Timestamp(val, offset=self.offset, tz=self.tz)\r\n   1355         else:\r\n\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n\r\n```\r\n\r\nProposed https://github.com/pydata/pandas/pull/10194'"
10184,78955747,corr723,jorisvandenbossche,2015-05-21 11:38:34,2015-06-09 12:19:32,2015-06-09 12:19:32,closed,,0.16.2,5,Bug;Difficulty Intermediate;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10184,b'read_csv does not set index name on empty dataframe',"b'If foo.csv contains\r\n```csv\r\nx,y\r\n1,2\r\n```\r\nthen you get the normal behavior:\r\n```py\r\n>>> df = pd.read_csv(\'foo.csv\', index_col=0)\r\n>>> list(df.columns)\r\n[\'y\']\r\n>>> df.index.name\r\n\'x\'\r\n```\r\nbut if you remove the data row ""1,2"" in the CSV the index name gets dropped:\r\n```py\r\n>>> df = pd.read_csv(\'foo.csv\', index_col=0)\r\n>>> list(df.columns)\r\n[\'y\']\r\n>>> df.index.name\r\nNone\r\n```\r\nThis means round-tripping with to_csv/read_csv fails for empty dataframes.'"
10183,78950327,jorisvandenbossche,jorisvandenbossche,2015-05-21 11:20:38,2015-06-10 06:47:12,2015-06-10 06:47:12,closed,,0.16.2,11,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10183,b'FIX printing index with display.max_seq_items=None (GH10182)',b'Fixes #10182 '
10182,78917570,JWarmenhoven,jorisvandenbossche,2015-05-21 09:44:11,2015-06-10 06:47:12,2015-06-10 06:47:12,closed,,0.16.2,5,Bug;Output-Formatting;Regression,https://api.github.com/repos/pydata/pandas/issues/10182,"b""Setting option 'display.max_seq_items' to None results in TypeError when pprinting index""","b""According to the docstring, the following is allowed:\r\n```Python\r\npd.set_option('display.max_seq_items', None)\r\n```\r\nWith 0.16.1 this results in a TypeError when pretty printing an index. This is because the None value is compared against lenght of the index to see if we need to limit output.\r\n```Python\r\nTypeError: unorderable types: int() > NoneType()\r\n```"""
10179,78572359,sebp,jreback,2015-05-20 15:34:57,2015-06-27 22:04:27,2015-06-27 22:04:22,closed,,0.17.0,5,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10179,b'BUG: concat on axis=0 with categorical (GH10177)',b'Contains a proposed fix for #10177'
10177,78495486,jreback,jreback,2015-05-20 11:57:53,2015-06-27 22:04:40,2015-06-27 22:04:40,closed,,0.17.0,4,Bug;Categorical;Difficulty Intermediate;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10177,b'BUG: concat on axis=0 with categorical',"b'xref https://github.com/pydata/pandas/issues/9426, fixed in #9597 \r\n``axis=1`` is ok\r\n\r\n```\r\ndf1 = pandas.DataFrame(numpy.random.randn(6, 3), columns=[""a"", ""b"", ""c""])\r\ndf2 = pandas.DataFrame(numpy.random.randn(7, 4), columns=[""g"", ""h"", ""a"", ""c""])\r\ndf2[\'h\'] = pandas.Series(pandas.Categorical([""one"", ""one"", ""two"", ""one"", ""two"", ""two"", ""one""]))\r\ndf = pandas.concat((df1, df2))\r\n```'"
10172,78164170,mortada,shoyer,2015-05-19 16:43:57,2015-06-02 19:26:12,2015-05-30 20:41:19,closed,,0.16.2,11,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/10172,b'BUG: mean overflows for integer dtypes (fixes #10155)',b'closes #10155 '
10171,78142327,jreback,jorisvandenbossche,2015-05-19 15:46:04,2015-06-02 19:26:12,2015-05-20 22:02:19,closed,,0.16.2,5,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10171,b'BUG: consistent datetime display format with < ms #10170',"b""closes #10170\r\n```\r\nIn [3]: Series(date_range('20130101 09:00:00',periods=5,freq='D'))\r\nOut[3]: \r\n0   2013-01-01 09:00:00\r\n1   2013-01-02 09:00:00\r\n2   2013-01-03 09:00:00\r\n3   2013-01-04 09:00:00\r\n4   2013-01-05 09:00:00\r\ndtype: datetime64[ns]\r\n\r\nIn [4]: Series(date_range('20130101 09:00:00',periods=5,freq='s'))\r\nOut[4]: \r\n0   2013-01-01 09:00:00\r\n1   2013-01-01 09:00:01\r\n2   2013-01-01 09:00:02\r\n3   2013-01-01 09:00:03\r\n4   2013-01-01 09:00:04\r\ndtype: datetime64[ns]\r\n\r\nIn [2]: s = Series(date_range('20130101 09:00:00',periods=5,freq='ms'))\r\n\r\nIn [3]: s.iloc[1] = np.nan\r\n\r\nIn [4]: s\r\nOut[4]: \r\n0   2013-01-01 09:00:00.000\r\n1                       NaT\r\n2   2013-01-01 09:00:00.002\r\n3   2013-01-01 09:00:00.003\r\n4   2013-01-01 09:00:00.004\r\ndtype: datetime64[ns]\r\n\r\nIn [6]: Series(date_range('20130101 09:00:00',periods=5,freq='us'))\r\nOut[6]: \r\n0   2013-01-01 09:00:00.000000\r\n1   2013-01-01 09:00:00.000001\r\n2   2013-01-01 09:00:00.000002\r\n3   2013-01-01 09:00:00.000003\r\n4   2013-01-01 09:00:00.000004\r\ndtype: datetime64[ns]\r\n\r\nIn [7]: Series(date_range('20130101 09:00:00',periods=5,freq='N'))\r\nOut[7]: \r\n0   2013-01-01 09:00:00.000000000\r\n1   2013-01-01 09:00:00.000000001\r\n2   2013-01-01 09:00:00.000000002\r\n3   2013-01-01 09:00:00.000000003\r\n4   2013-01-01 09:00:00.000000004\r\ndtype: datetime64[ns]\r\n```"""
10170,78118888,jorisvandenbossche,jorisvandenbossche,2015-05-19 14:46:09,2015-06-02 19:25:20,2015-05-20 22:02:19,closed,,0.16.2,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10170,b'Display of a datetime column/index with both second and sub-second resolution',"b'Should a datetime column should use the same precision for all, padding with zeros, just like a float column does?\r\n\r\n```\r\nIn [121]: df = pd.DataFrame({\'datetime\': pd.date_range(\'1/1/2000 09:00:00\', freq=""5ms"",periods=10), \'float\': np.arange(0,0.05,0.005)})\r\n\r\nIn [122]: df\r\nOut[122]:\r\n                    datetime  float\r\n0        2000-01-01 09:00:00  0.000\r\n1 2000-01-01 09:00:00.005000  0.005\r\n2 2000-01-01 09:00:00.010000  0.010\r\n3 2000-01-01 09:00:00.015000  0.015\r\n4 2000-01-01 09:00:00.020000  0.020\r\n5 2000-01-01 09:00:00.025000  0.025\r\n6 2000-01-01 09:00:00.030000  0.030\r\n7 2000-01-01 09:00:00.035000  0.035\r\n8 2000-01-01 09:00:00.040000  0.040\r\n9 2000-01-01 09:00:00.045000  0.045\r\n```\r\n\r\nso that the first row would show `2000-01-01 09:00:00.000000`?'"
10160,77224825,JohnNapier,jreback,2015-05-17 05:48:48,2015-06-10 20:01:31,2015-06-10 20:01:31,closed,,0.16.2,2,Bug;Difficulty Intermediate;Dtypes;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/10160,b'pd.DataFrame.from_dict fails when index is numpy.datetime',"b'Suppose that we want to create a dataframe from a dictionary.\r\n\r\n```python\r\nimport pandas as pd,numpy as np\r\n#1) Form dataframe\r\nix=pd.date_range(\'1/1/2015\',periods=5,freq=\'D\')\r\ndf0=pd.DataFrame(np.random.normal(size=(len(ix),5)),index=ix)\r\n#2) Split dataframe into 2 dictionaries, based on sign\r\nd={\'+\':{},\'-\':{}}\r\nfor loc in df0.index:\r\n    d[\'+\'][loc]=df0.loc[loc][df0.loc[loc]>=0]\r\n    d[\'-\'][loc]=df0.loc[loc][df0.loc[loc]<0]\r\n#3) Convert dictionary to dataframe\r\nprint pd.DataFrame.from_dict(d[\'+\'],orient=\'index\')\r\n```\r\n\r\nThis returns a dataframe of positive values, with NaN in place of the negative values. Now, it appears the exact same operation fails if we use ""df.index.values"" instead of ""df.index"".\r\n\r\n```python\r\nd={\'+\':{},\'-\':{}}\r\nfor loc in df0.index.values: # NOTE .values!\r\n    d[\'+\'][loc]=df0.loc[loc][df0.loc[loc]>=0]\r\n    d[\'-\'][loc]=df0.loc[loc][df0.loc[loc]<0]\r\nprint pd.DataFrame.from_dict(d[\'+\'],orient=\'index\')\r\n```\r\n\r\nNow the outcome is a dataframe with all None. I think the problem is some inconsistency between pandas\' datetime and numpy\'s datetime formats.\r\n\r\nPandas version 0.16.1, Numpy version 1.9.2. Thanks.'"
10158,77137402,sinhrks,jreback,2015-05-16 21:31:49,2015-06-02 19:26:12,2015-05-18 11:59:45,closed,,0.16.2,3,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10158,b'BUG: Index.name is lost during timedelta ops',"b""Closes #9926.\r\n\r\nCC @hsperr I've changed ``common._maybe_match_names`` to work with not-named objects, and hopefully can use in #9965 also."""
10157,77136426,sinhrks,jreback,2015-05-16 21:26:19,2015-06-02 19:26:12,2015-06-01 11:45:10,closed,,0.16.2,5,Bug,https://api.github.com/repos/pydata/pandas/issues/10157,b'BUG: Index.union cannot handle array-likes',"b""Closes #10149.\r\n\r\n- Added explicit tests for array-likes in set-ops, ``intersection``, ``union``, ``difference`` and ``sym_diff``.\r\n   - ``TimedeltaIndex`` previously did additional input check which is inconsistent with others. Made it work as the same manner as others.\r\n   -  ``MultiIndex`` set-ops should only accept list-likes of tuples. Implement the logic and add explicit test.\r\n- Changed to understandable error message for ``str`` which has ``__iter__`` in py3.\r\n\r\n```\r\n# current error msg\r\nidx.intersection('aaa')\r\n# TypeError: Index(...) must be called with a collection of some kind, 'aaa' was passed\r\n```\r\n\r\nNOTE: This DOESN'T care the ``name`` attribute checks, which is being worked in #9965."""
10156,77133838,wikiped,jreback,2015-05-16 21:12:36,2015-06-09 23:46:30,2015-06-09 23:46:30,closed,,0.16.2,2,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/10156,b'.remove_category(np.nan) fails on Categorical with floats',"b'Trying to remove a `nan` category from Categorical series fails if categories are made of floats.\r\nIn the docs it says:\r\n\r\n> Note: As integer Series can\xa1\xaft include NaN, the categories were converted to _object_. \r\n\r\nSo it is probably linked to this with `float` remaining `float` and `nan` != `nan`.\r\n\r\nIf this is intended behavior perhaps would be useful to add this to the docs?\r\n\r\n\timport pandas as pd\r\n\tdf = pd.DataFrame({\'a\': pd.Categorical([1,2,3]),\r\n\t\t\t\t\t   \'b\': pd.Categorical(list(\'abc\')),\r\n\t\t\t\t\t   \'c\': pd.Categorical([1.1,2.1,3.1])})\r\n\tfor col in df.columns:\r\n\t\tdf[col].cat.add_categories(pd.np.nan, inplace=True)\r\n\t\tprint df[col]\r\n\t\tdf[col].cat.remove_categories(pd.np.nan)\r\n\t\t\r\n\t0    1\r\n\t1    2\r\n\t2    3\r\n\tName: a, dtype: category\r\n\tCategories (4, object): [1, 2, 3, NaN]\r\n\t0    a\r\n\t1    b\r\n\t2    c\r\n\tName: b, dtype: category\r\n\tCategories (4, object): [a, b, c, NaN]\r\n\t0    1.1\r\n\t1    2.1\r\n\t2    3.1\r\n\tName: c, dtype: category\r\n\tCategories (4, float64): [1.1, 2.1, 3.1, NaN]\r\n\r\n\t---------------------------------------------------------------------------\r\n\tValueError                                Traceback (most recent call last)\r\n\td:\\Anaconda\\envs\\py2k\\lib\\site-packages\\pandas\\core\\categorical.pyc in _delegate_method(self, name, *args, **kwargs)\r\n\t   1643         from pandas import Series\r\n\t   1644         method = getattr(self.categorical, name)\r\n\t-> 1645         res = method(*args, **kwargs)\r\n\t   1646         if not res is None:\r\n\t   1647             return Series(res, index=self.index)\r\n\r\n\td:\\Anaconda\\envs\\py2k\\lib\\site-packages\\pandas\\core\\categorical.pyc in remove_categories(self, removals, inplace)\r\n\t\t753         not_included = removals - set(self._categories)\r\n\t\t754         if len(not_included) != 0:\r\n\t--> 755             raise ValueError(""removals must all be in old categories: %s"" % str(not_included))\r\n\t\t756         new_categories = [ c for c in self._categories if c not in removals ]\r\n\t\t757         return self.set_categories(new_categories, ordered=self.ordered, rename=False,\r\n\r\n\tValueError: removals must all be in old categories: set([nan])\r\n\r\n____________________________________________________________________\r\n\tINSTALLED VERSIONS\r\n\t------------------\r\n\tcommit: None\r\n\tpython: 2.7.9.final.0\r\n\tpython-bits: 64\r\n\tpandas: 0.16.1\r\n\tnumpy: 1.9.2\r\n        ...'"
10155,77092567,qqsusu,shoyer,2015-05-16 18:00:20,2015-06-02 19:25:20,2015-05-30 20:41:19,closed,,0.16.2,20,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/10155,b'Mean overflows for integer dtypes',"b""Say there is a array of type int64\r\nfor convenience, that me just put is some large number\r\n\r\ntest1 = pd.Series(20150515061816532, index=list(range(500)), dtype='int64')\r\ntest1.describe()\r\nOut[152]: \r\ncount    5.000000e+02\r\n**mean    -1.674297e+16**\r\nstd      0.000000e+00\r\nmin      2.015052e+16\r\n25%      2.015052e+16\r\n50%      2.015052e+16\r\n75%      2.015052e+16\r\nmax      2.015052e+16\r\n\r\nLook at the mean, it overflow, and become negative. Obviously the mean should be 20150515061816532\r\n\r\nIn [153]: test1.sum()\r\n**Out[153]: -8371486542801285616** This is wrong.\r\n\r\nThe computation should have been sum them up as **float**, and devided by total count.\r\n\r\nI think we need to examine other parts of code that involve similar situation."""
10154,77091519,vincentdavis,jreback,2015-05-16 17:52:41,2015-07-07 10:19:58,2015-07-07 10:19:58,closed,,0.17.0,12,Bug;Difficulty Novice;Effort Low;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10154,"b'to_datetime, inconsistent behavior with invalid dates.'","b'Consider Feb 29 1991 (2291991) and March 32 1991 (3321991), Both are invalid dates.\r\n```\r\npd.to_datetime(2291991, format=""%m%d%Y"", coerce=True, exact=True)\r\n```\r\nReturns a TypeError: ValueError: day is out of range for month\r\n\r\nWhile\r\n```\r\npd.to_datetime(3321991, format=""%m%d%Y"", coerce=True, exact=True)\r\n```\r\nReturns NaT. Which is what I would expect.\r\n\r\nIn any case they should return the same error or value\r\n\r\nJoris Van den Bossche pointed out on the mailing list:\r\n""It has something to do with the number of the day, only values above 31 convert to NaT, 31 or lower raises the error (eg also 31 April raise error instead of giving NaT)""\r\n'"
10150,76839233,sinhrks,sinhrks,2015-05-15 19:59:59,2015-06-27 03:51:56,2015-06-27 03:51:56,closed,,0.17.0,0,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/10150,b'BUG/API: inconsistent name handling in value_counts ',"b""Both ``Series.value_counts`` and ``Index.value_counts`` should preserve its name in resulted ``Series.name``?  Current behaviors are below:\r\n\r\n```\r\ns = pd.Series([1, 2, 1], name='a')\r\nidx = pd.Index([1, 2, 1], name='a')\r\n\r\nresult = s.value_counts()\r\nresult.name, result.index.name\r\n# (None, None)      # should be ('a', None)?\r\n\r\nresult = idx.value_counts()\r\nresult.name, result.index.name\r\n# (None, None)      # should be ('a', None)?\r\n\r\ndidx = pd.date_range('2011-01-01', freq='D', periods=3, name='a')\r\nresult = didx.value_counts()\r\n# (None, 'a')       # should be ('a', None)?\r\n```\r\n```"""
10149,76837115,sinhrks,jreback,2015-05-15 19:51:00,2015-06-02 19:25:20,2015-06-01 11:45:10,closed,,0.16.2,0,Bug,https://api.github.com/repos/pydata/pandas/issues/10149,b'BUG: Index.union cannot handle array-likes ',"b""Though API doc says ``Index.union`` can accept ``Index`` or array-like, it raises errors.\r\n\r\n- http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.union.html\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nidx = pd.Index([1, 2, 3])\r\n\r\nidx.union([2, 3, 4])\r\n# AttributeError: 'list' object has no attribute 'dtype'\r\n\r\nidx.union(pd.Series([2, 3, 4]))\r\n# AttributeError: 'Series' object has no attribute 'is_monotonic'\r\n\r\nidx.union(np.array([2, 3, 4]))\r\n# AttributeError: 'numpy.ndarray' object has no attribute 'is_monotonic'\r\n```\r\n\r\nNOTE: ``intersection`` looks OK.\r\n\r\n```\r\nidx.intersection([2, 3, 4])\r\n# Int64Index([2, 3], dtype='int64')\r\n\r\nidx.intersection(pd.Series([2, 3, 4]))\r\n# Int64Index([2, 3], dtype='int64')\r\n\r\nidx.union([2, 3, 4])\r\n# Int64Index([2, 3], dtype='int64')\r\n```"""
10133,76350971,evanpw,jreback,2015-05-14 12:57:00,2015-06-10 13:40:52,2015-06-09 10:49:18,closed,,0.16.2,7,Bug;Dtypes;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10133,b'BUG: Strings with exponent but no decimal point parsed as integers in python csv engine',b'Fixes GH #9565. I also made the handling of out-of-range integers consistent with the C engine: parse as a string rather than a double.\r\n'
10132,76317672,evanpw,jreback,2015-05-14 10:42:15,2015-09-19 00:38:07,2015-06-05 22:03:42,closed,,0.16.2,8,Bug;Categorical;Groupby,https://api.github.com/repos/pydata/pandas/issues/10132,b'BUG: get_group fails when multi-grouping with a categorical',"b'Example:\r\n``` .python\r\n>>> df = pd.DataFrame({\'a\' : pd.Categorical(\'xyxy\'), \'b\' : 1, \'c\' : 2})\r\n>>> df.groupby([\'a\', \'b\']).get_group((\'x\', 1))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/groupby.py"", line 601, in get_group\r\n    inds = self._get_index(name)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/groupby.py"", line 429, in _get_index\r\n    sample = next(iter(self.indices))\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/groupby.py"", line 414, in indices\r\n    return self.grouper.indices\r\n  File ""pandas/src/properties.pyx"", line 34, in pandas.lib.cache_readonly.__get__ (pandas/lib.c:41912)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/groupby.py"", line 1305, in indices\r\n    return _get_indices_dict(label_list, keys)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/groupby.py"", line 3762, in _get_indices_dict\r\n    return lib.indices_fast(sorter, group_index, keys, sorted_labels)\r\n  File ""pandas/lib.pyx"", line 1385, in pandas.lib.indices_fast (pandas/lib.c:23843)\r\nTypeError: Cannot convert Categorical to numpy.ndarray\r\n```\r\n\r\nThe problem is that ``Grouping.group_index`` is a CategoricalIndex, so calling ``get_values()`` gives you a ``Categorical``, which needs one more application of ``get_values()`` to get an ``ndarray``'"
10126,76042551,kdebrab,shoyer,2015-05-13 16:21:21,2015-12-17 19:09:51,2015-06-03 19:03:09,closed,,0.16.2,6,Bug;Difficulty Novice;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/10126,b'masking empty DataFrame',"b'In pandas 0.16.1, I get a ValueError when trying to mask an empty DataFrame:\r\n```python\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame()\r\n>>> df[df>0]\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-3-efe84c9ebabc>"", line 1, in <module>\r\n    df[df>0]\r\n\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 1787, in __getitem__\r\n    return self._getitem_frame(key)\r\n\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 1859, in _getitem_frame\r\n    raise ValueError(\'Must pass DataFrame with boolean values only\')\r\n\r\nValueError: Must pass DataFrame with boolean values only\r\n```\r\nUsing \'where\' works as expected:\r\n```python\r\n>>> df.where(df>0)\r\nOut[4]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: []\r\n```\r\nAlso, masking a Series works as expected:\r\n```python\r\n>>> ts = pd.Series()\r\n>>> ts[ts>0]\r\nOut[6]: Series([], dtype: float64)\r\n```\r\n'"
10124,75960058,evanpw,jreback,2015-05-13 11:58:09,2015-07-30 17:14:50,2015-07-30 16:09:29,closed,,0.17.0,5,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/10124,b'BUG: Filter/transform fail in some cases when multi-grouping with a datetime-like key',"b'Fixes GH #10114, and a related problem in transform which was already present in 0.16.0'"
10116,75738342,seth-p,sinhrks,2015-05-12 21:05:54,2015-08-07 00:15:40,2015-06-23 14:11:23,closed,,0.17.0,3,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10116,b'BUG: drop_duplicates drops name(s).',b'Closes #10115.'
10115,75720160,seth-p,sinhrks,2015-05-12 20:00:01,2015-06-23 14:11:23,2015-06-23 14:11:23,closed,,0.17.0,1,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/10115,b'BUG: MultiIndex.drop_duplicates() drops names',"b""```\r\nIn [12]: import pandas as pd\r\n\r\nIn [13]: mi = pd.MultiIndex.from_tuples([('A','a',1), ('A','a',2)],\r\n                                        names=['Upper','Lower','Num'])\r\n\r\nIn [14]: mi.drop_duplicates()\r\nOut[14]:\r\nMultiIndex(levels=[['A'], ['a'], [1, 2]],\r\n           labels=[[0, 0], [0, 0], [0, 1]])\r\n\r\nIn [16]: mi.names\r\nOut[16]: FrozenList(['Upper', 'Lower', 'Num'])\r\n\r\nIn [18]: mi.drop_duplicates().names\r\nOut[18]: FrozenList([None, None, None])\r\n```"""
10114,75704982,jreback,jreback,2015-05-12 19:05:06,2015-08-15 23:41:09,2015-08-15 23:41:09,closed,,0.17.0,2,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/10114,b'BUG: filter with a multi-grouping including a datetimelike fails',"b""this was changed in #9921 (worked prior to 0.16.0)\r\ncc @evanpw\r\n\r\nso using a multiple grouper where one item is a datetimelike fails as the ``name in self.indices`` fails. Instead ``.get_group()`` can be used to fix\r\n\r\n```\r\nIn [1]: df = DataFrame({'A' : np.arange(5), 'B' : ['foo','bar','foo','bar','bar'], 'C' : Timestamp('20130101') })\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A    B          C\r\n0  0  foo 2013-01-01\r\n1  1  bar 2013-01-01\r\n2  2  foo 2013-01-01\r\n3  3  bar 2013-01-01\r\n4  4  bar 2013-01-01\r\n\r\nIn [3]: df.groupby(['B','C']).filter(lambda x: True)\r\nOut[3]: \r\nEmpty DataFrame\r\nColumns: [A, B, C]\r\nIndex: []\r\n\r\nIn [4]: df.groupby(['B']).filter(lambda x: True)\r\nOut[4]: \r\n   A    B          C\r\n0  0  foo 2013-01-01\r\n1  1  bar 2013-01-01\r\n2  2  foo 2013-01-01\r\n3  3  bar 2013-01-01\r\n4  4  bar 2013-01-01\r\n\r\nIn [5]: df.groupby(['C']).filter(lambda x: True)\r\nOut[5]: \r\n   A    B          C\r\n0  0  foo 2013-01-01\r\n1  1  bar 2013-01-01\r\n2  2  foo 2013-01-01\r\n3  3  bar 2013-01-01\r\n4  4  bar 2013-01-01\r\n```"""
10108,75376773,rosnfeld,jreback,2015-05-11 22:15:45,2015-06-02 19:26:11,2015-05-12 10:47:23,closed,,0.16.2,5,2/3 Compat;Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10108,"b""BUG: categorical doesn't handle display.width of None in Python 3 (GH10087)""","b""closes #10087\r\n\r\nI think this is fairly straightforward, however I am not sure if the prior code comparing ```display.width``` to 0 was correct... is ```display.width``` ever supposed to be set to 0? It looks suspiciously similar to a snippet from ```Series.__unicode__()```, but that is looking at ```display.max_rows```. Maybe it was brought over incorrectly?\r\n\r\nIf it should just be a simple comparison to None, not 0 as it was before, just let me know and I'll amend this."""
10098,75088488,jameshiebert,jreback,2015-05-11 05:08:57,2015-06-07 22:28:40,2015-06-07 22:28:38,closed,,0.16.2,11,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10098,b'BUG: invalid column names in a HDF5 table format #9057',b'Have DataFrame.to_hdf() raise an error when using pytables with non-string\r\ncolumn types\r\n\r\ncloses #9057'
10096,74772629,sinhrks,sinhrks,2015-05-09 22:26:31,2015-06-02 19:26:11,2015-05-12 19:58:49,closed,,0.16.2,4,Bug;Dtypes;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10096,b'BUG: Timestamp properties may return np.int',b'Closes #10050.\r\n\r\nAlso added ``daysinmonth`` and ``dayofweek`` properties to ``NaT`` and perform tests.'
10092,74742935,mortada,jreback,2015-05-09 19:36:35,2015-05-10 01:53:25,2015-05-10 01:29:59,closed,,0.16.1,5,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/10092,b'BUG: Series.fillna() raises if given a numerically convertible string',"b""First reported here but it's been dormant for a long time: https://github.com/pydata/pandas/pull/9043 \r\n\r\nI think it'd be good to patch this bug. @jreback please let me know which release this should go to and I can add a release note. Thanks. """
10087,74525823,rosnfeld,jreback,2015-05-08 23:13:05,2015-06-02 19:25:20,2015-05-12 10:47:23,closed,,0.16.2,2,2/3 Compat;Bug;Difficulty Novice;Effort Low;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/10087,"b""BUG: categorical doesn't handle display.width of None in Python 3 ""","b'Categorical Series have a special repr that looks at display.width, which can be None if following the Options and Settings docs. Unlike Python 2, in Python 3 an integer vs None comparison throws an exception.\r\n\r\n(on current master, and has been true for several releases now)\r\n\r\n```python\r\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \r\n[GCC 4.8.2] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas as pd\r\n>>> pd.core.config.set_option(\'display.width\', None)\r\n>>> import numpy as np\r\n>>> x = pd.Series(np.random.randn(100))\r\n>>> pd.cut(x, 10)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/base.py"", line 67, in __repr__\r\n    return str(self)\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/base.py"", line 46, in __str__\r\n    return self.__unicode__()\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/series.py"", line 897, in __unicode__\r\n    max_rows=max_rows)\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/series.py"", line 962, in to_string\r\n    name=name, max_rows=max_rows)\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/series.py"", line 992, in _get_repr\r\n    result = formatter.to_string()\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/format.py"", line 222, in to_string\r\n    footer = self._get_footer()\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/format.py"", line 196, in _get_footer\r\n    level_info = self.tr_series.values._repr_categories_info()\r\n  File ""/home/andrew/git/pandas-rosnfeld-py3/pandas/core/categorical.py"", line 1323, in _repr_categories_info\r\n    if max_width != 0 and cur_col_len + sep_len + len(val) > max_width:\r\nTypeError: unorderable types: int() > NoneType()\r\n```'"
10074,73946710,twu,jreback,2015-05-07 11:19:02,2015-06-02 10:53:55,2015-06-02 10:53:55,closed,,,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/10074,b'BUG: Fix typo in parsers.py',b'closes #9266 \r\n\r\nnp.uin8 should be np.uint8'
10072,73886033,mortada,jreback,2015-05-07 07:55:30,2015-05-07 21:03:23,2015-05-07 20:53:20,closed,,0.16.1,5,Bug;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10072,b'BUG: median() not correctly handling non-float null values (fixes #10\xa1\xad',b'closes #10040 '
10068,73518989,sinhrks,jreback,2015-05-06 05:24:39,2015-06-02 19:25:20,2015-06-02 10:38:48,closed,,0.16.2,6,Bug,https://api.github.com/repos/pydata/pandas/issues/10068,b'BUG/API: Series arithmetic ops inconsistently hold names',"b""Based on ``Index`` behavior, ``Series`` arithmetic should reset name when names are different?\r\n\r\n```\r\nimport pandas as pd\r\ns1 = pd.Series([1, 2, 3], name='a')\r\ns2 = pd.Series([3, 4, 5], name='b')\r\n\r\n(s1 + s2).name\r\n# None\r\n\r\ns1.add(s2).name\r\n# a\r\n```"""
10067,73498417,sinhrks,sinhrks,2015-05-06 03:09:14,2015-06-02 19:26:59,2015-05-30 13:41:38,closed,,0.16.2,3,Bug;Reshaping;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10067,b'BUG: Series.align resets name when fill_value is specified',"b""```\r\nimport pandas.util.testing as tm\r\n\r\nts = tm.makeTimeSeries()\r\nts.name = 'ts'\r\na, b = ts[2:], ts[:5]\r\n\r\n# OK (names are preserved)\r\naa, ba = a.align(b)\r\naa.name\r\n# ts\r\nba.name\r\n# ts\r\n\r\n# NG, specifying fill_value resets names\r\naa, ba = a.align(b, fill_value=-1)\r\naa.name\r\n# None\r\nba.name\r\n# None\r\n```"""
10064,73332217,jreback,jreback,2015-05-05 14:34:47,2015-05-05 16:48:28,2015-05-05 16:48:28,closed,,0.16.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/10064,b'BUG: Bug in grouping with multiple pd.Grouper where one is non-time based (GH10063)',b'closes #10063'
10063,73313216,neiseman,jreback,2015-05-05 13:20:02,2015-05-13 20:02:08,2015-05-05 16:48:28,closed,,0.16.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/10063,b'Grouped timeseries operations fail on multi-indexed DataFrames',"b""This might be a potential bug: doing grouped timeseries operations fails silently on a multi-indexed DataFrame.\r\n```python\r\n    import pandas as pd\r\n    import pandas.io.data as web\r\n\r\n    # Get some market data\r\n    df = web.DataReader(['AAPL', 'GOOG'], 'yahoo', pd.Timestamp('2013'), pd.Timestamp('2014')).to_frame()\r\n    df.index.names = ('dt', 'symbol')\r\n\r\n    In [21]: df.head()\r\n    Out[21]: \r\n                            Open       High        Low      Close     Volume  \\\r\n    dt         symbol                                                          \r\n    2013-01-02 AAPL    553.82001  555.00000  541.62994  549.03003  140129500   \r\n    2013-01-03 AAPL    547.88000  549.67004  541.00000  542.10004   88241300   \r\n    2013-01-04 AAPL    536.96997  538.63000  525.82996  527.00000  148583400   \r\n    2013-01-07 AAPL    522.00000  529.30005  515.20001  523.90002  121039100   \r\n    2013-01-08 AAPL    529.21002  531.89001  521.25000  525.31000  114676800   \r\n\r\n                       Adj Close  \r\n    dt         symbol             \r\n    2013-01-02 AAPL     74.63931  \r\n    2013-01-03 AAPL     73.69719  \r\n    2013-01-04 AAPL     71.64438  \r\n    2013-01-07 AAPL     71.22294  \r\n    2013-01-08 AAPL     71.41463 \r\n```\r\nLet's say we want to resample this to monthly data. This fails and returns an empty DataFrame:\r\n```python\r\n    df_M = df.groupby(level='symbol').resample('M', how='mean')\r\n    In [23]: df_M\r\n    Out[23]: \r\n    Empty DataFrame\r\n    Columns: []\r\n    Index: []\r\n```\r\nThis, however, works, but requires a seemingly-unnecessary re-indexing:\r\n```python\r\n    df_M = df.reset_index().set_index('dt').groupby('symbol').resample('M', how='mean')\r\n    In [26]: df_M.head()\r\n    Out[26]: \r\n                       Adj Close       Close        High         Low        Open  \\\r\n    symbol dt                                                                      \r\n    AAPL   2013-01-31  67.677750  497.822382  504.407623  492.969997  500.083329   \r\n           2013-02-28  62.388477  456.808942  463.231056  452.106325  458.503692   \r\n           2013-03-31  60.417287  441.841000  446.803495  437.337996  442.011512   \r\n           2013-04-30  57.398619  419.765001  425.553183  414.722271  419.766820   \r\n           2013-05-31  61.340151  446.452734  451.658190  441.495455  446.400919   \r\n    \r\n                             Volume  \r\n    symbol dt                        \r\n    AAPL   2013-01-31  1.562312e+08  \r\n           2013-02-28  1.229478e+08  \r\n           2013-03-31  1.147110e+08  \r\n           2013-04-30  1.245851e+08  \r\n           2013-05-31  1.073583e+08  \r\n```\r\nThe fact that you need to do the `reset_index().set_index('dt')` and then `groupby('symbol')` instead of `groupby(level='symbol')` seems to defeat the purpose of a multi-index! What gives?\r\n\r\nI also realize that data like this is perhaps more well-suited to a Panel than a DataFrame, but when dealing with very large amounts of (often sparse) data, the 3D Panel structure presents performance and memory issues compared to the flat DataFrame."""
10055,72908707,ajamian,shoyer,2015-05-04 03:07:47,2015-05-26 04:11:28,2015-05-26 04:06:27,closed,,Next Major Release,10,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/10055,b'BUG: read_hdf modifying passed columns list (GH7212)',"b""Closes #7212. If this object is a list, make a copy and then pass to process_axes - otherwise, just pass the object directly (in this case it is most likely none).\r\n\r\nA test for this behavior seems complex and somewhat contrived when compared with what exists in test_pytables.py, so I didn't add one --- let me know any thoughts on this."""
10050,72651783,miraculixx,sinhrks,2015-05-02 14:16:40,2015-05-12 19:58:49,2015-05-12 19:58:49,closed,,Next Major Release,2,Bug;Dtypes;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10050,"b""Type error on using datetime64's microseconds in datetime.timedelta""","b""Using Pandas' datetime64 dtype to convert to a datetime.timedelta object results in this type error:\r\n\r\n```python\r\nTypeError: unsupported type for timedelta microseconds component: numpy.int32\r\n```\r\n\r\nReproduce:\r\n\r\n```python\r\n# set up a dataframe with a datetime value in it\r\nfrom datetime import datetime\r\nimport pandas as pd\r\ndf = pd.DataFrame({'foo' : [datetime.now()] })\r\nprint df.dtypes\r\n# =>\r\nfoo    datetime64[ns]\r\ndtype: object\r\n# let's convert to timedelta\r\ndf.foo.apply(lambda r : timedelta(hours = r.hour, minutes = r.minute, seconds = r.second, microseconds = r.microseconds))\r\n=> TypeError: unsupported type for timedelta microseconds component: numpy.int32\r\n```\r\n\r\nExpected:\r\n\r\n```python\r\n# no excepetions, whatever the correct value\r\ndf.foo.apply(lambda r : timedelta(hours = r.hour, minutes = r.minute, seconds = r.second, microseconds = r.microseconds))\r\n# => \r\n0   16:08:25.347571\r\nName: foo, dtype: timedelta64[ns]\r\n```\r\n\r\nWork around:\r\n\r\n```python\r\n# if you can't control the timedelta conversion (i.e. it is somewhere embedded, e.g. in a library)\r\ndf.foo = df.foo.astype(datetime)\r\n# if you can\r\ndf.foo.apply(lambda r : timedelta(hours = r.hour, minutes = r.minute, seconds = r.second, microseconds = int(r.microseconds))\r\n```\r\n\r\nVersion information\r\n\r\n```python\r\n> pd.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-24-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.0\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 3.0.0\r\nsphinx: 1.2.2\r\npatsy: None\r\ndateutil: 2.3\r\npytz: 2014.10\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: 2.1.4\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: 1.2\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```"""
10044,72517470,scari,jreback,2015-05-01 20:35:54,2015-05-04 19:09:36,2015-05-04 19:09:36,closed,,0.16.1,3,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/10044,b'BUG: Resample BM/BQ adds extra index point #9756',b'closes #9756 '
10042,72463130,jreback,jreback,2015-05-01 16:03:24,2015-05-04 21:00:39,2015-05-04 12:45:51,closed,,0.16.1,4,Bug;Categorical;Deprecate;Indexing,https://api.github.com/repos/pydata/pandas/issues/10042,b'BUG: numericlike set ops on unsupported Indexes',"b""closes #10038 \r\ncloses #10039\r\n\r\n- ``CategoricalIndex`` now raise ``TypeError`` on ops like : ``idx - idx`` (we are moving all non-numeric Indexes to this, but just deprecated for now on existing index types)\r\n- Index ops with lists will now show the same warning (e.g. ``idx - ['a','b']``)\r\n"""
10041,72462604,hcontrast,jreback,2015-05-01 15:59:58,2016-03-06 20:44:23,2016-03-06 20:44:23,closed,,0.18.0,1,Bug;Difficulty Intermediate;Effort Low;Timeseries,https://api.github.com/repos/pydata/pandas/issues/10041,b'Loss of nanosecond resolution when constructing Timestamps from str',"b""Looks like a bug: When passing a YYYYMMDDTHHMMSS.f string to `pd.Timestamp`, the resolution seems limited to micro- rather than nano-seconds:\r\n\r\n```python\r\n>>> pd.Timestamp('20130101T000000.000001+0000')\r\nTimestamp('2013-01-01 00:00:00.000001+0000', tz='UTC')\r\n\r\n>>> pd.Timestamp('20130101T000000.0000001+0000')\r\nTimestamp('2013-01-01 00:00:00+0000', tz='UTC')\r\n\r\n>>> pd.Timestamp('20130101T000000.00000001+0000')\r\nTimestamp('2013-01-01 00:00:00+0000', tz='UTC')\r\n```\r\nThe behavior seems correct when using a different string format:\r\n```python\r\n>>> pd.Timestamp('2013-01-01 00:00:00.00000001+0000')\r\nTimestamp('2013-01-01 00:00:00.000000010+0000', tz='UTC')\r\n```\r\nUsing pandas 0.16.0, python 2.7.6"""
10040,72445433,AndreaBravi,jreback,2015-05-01 14:39:15,2015-05-07 20:53:20,2015-05-07 20:53:20,closed,,0.16.1,2,Bug;Difficulty Novice;Effort Low;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10040,b'Median returns odd value when applied to timedeltas',"b""```python\r\nfrom pandas import DataFrame\r\nfrom numpy import datetime64\r\ndata = ['2015-02-03', '2015-02-07']\r\ndata = DataFrame(data, dtype=datetime64)\r\ndata.diff().median()\r\n```\r\n\r\nThis code returns \r\n```python\r\n0   -53374 days +00:06:21.572612\r\ndtype: timedelta64[ns]\r\n```\r\nwhile I would expect it to return the same as data.diff().mean()\r\n```python\r\n0   4 days\r\ndtype: timedelta64[ns]\r\n```\r\n\r\nHere is my setup\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.16.0\r\nnose: 1.3.4\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2015.2\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: 2.1.3\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.5.4 (dt dec pq3 ext)\r\n```\r\n\r\n\r\n"""
10028,72102599,jorisvandenbossche,jreback,2015-04-30 08:49:02,2015-04-30 10:03:27,2015-04-30 10:03:07,closed,,,1,Bug;Categorical;Duplicate;Groupby,https://api.github.com/repos/pydata/pandas/issues/10028,b'Pivot_table with categorical data gives ValueError',"b""This errors for me on master as well: http://stackoverflow.com/questions/29945342/pandas-error-with-pivot-table-and-categorical-data\r\n\r\n```\r\n  category1   category2  numeric\r\n0      grey    complete        1\r\n1      grey    complete        2\r\n2       NaN  incomplete        3\r\n3      blue    complete        4\r\n```\r\n\r\n```\r\nIn [13]: df = pd.read_clipboard()\r\n\r\n# with plain strings it works\r\n\r\nIn [16]: pd.pivot_table(df, index = 'category1', columns = 'category2', values = 'numeric', aggfunc = 'count')\r\nOut[16]:\r\ncategory2  complete\r\ncategory1\r\nblue              1\r\ngrey              2\r\n\r\nIn [17]: df['category1'] = df['category1'].astype('category')\r\n\r\nIn [18]: df['category2'] = df['category2'].astype('category')\r\n\r\nIn [19]: pd.pivot_table(df, index = 'category1', columns = 'category2', values =\r\n 'numeric', aggfunc = 'count')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n\r\n...\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\common.py in _astype_nansafe(ar\r\nr, dtype, copy)\r\n   2726\r\n   2727         if np.isnan(arr).any():\r\n-> 2728             raise ValueError('Cannot convert NA to integer')\r\n   2729     elif arr.dtype == np.object_ and np.issubdtype(dtype.type, np.intege\r\nr):\r\n   2730         # work around NumPy brokenness, #1987\r\n\r\nValueError: Cannot convert NA to integer\r\n```\r\n"""
10025,72042031,shoyer,jreback,2015-04-30 02:33:53,2015-05-04 10:48:26,2015-05-04 10:48:26,closed,,0.16.1,0,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/10025,b'BUG: Index constructor does not set name when given a TimedeltaIndex as data',"b""```\r\nIn [46]: tdi = pd.timedelta_range(start=0, periods=4, freq='D')\r\n\r\nIn [47]: pd.Index(tdi, name='foo').name is None\r\nOut[47]: True\r\n\r\nIn [48]: pd.Index(tdi.values, name='foo').name is None\r\nOut[48]: False\r\n```\r\n\r\nOther index types seem to work fine:\r\n```\r\nIn [51]: dti = pd.date_range(start='2000-01-01', periods=4, freq='D')\r\n\r\nIn [52]: pd.Index(dti, name='foo').name is None\r\nOut[52]: False\r\n```"""
10014,71854019,evanpw,shoyer,2015-04-29 11:48:50,2015-04-29 23:08:00,2015-04-29 23:05:42,closed,,0.16.1,3,Bug;Categorical;Groupby,https://api.github.com/repos/pydata/pandas/issues/10014,b'BUG: null group spills into final group when grouping on a categorical',b'Fixes GH #9603'
10001,71385135,a59,jreback,2015-04-27 19:58:18,2016-05-06 12:10:40,2016-05-06 12:10:40,closed,,0.19.0,8,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/10001,b'pandas.ExcelFile ignore parse_dates=False',"b'I am trying to read an Excel file which someone else created and the wrongly formatted a column as ""date"" when it is not. It has a large integer in it, which triggers an error\r\n\r\nOverflowError: normalized days too large to fit in a C int\r\n\r\nBut I have ""parse_dates=False"" so I thought pandas.ExcelFile would not try to parse the dates and return a string instead. Is this a bug?'"
9994,71106296,evanpw,jreback,2015-04-26 19:04:19,2015-09-19 00:38:08,2015-04-29 10:31:53,closed,,0.16.1,4,Bug;Categorical;Groupby,https://api.github.com/repos/pydata/pandas/issues/9994,b'BUG: transform and filter misbehave when grouping on categorical data',b'Fixes GH #9921\r\ncloses #9700 '
9983,70844774,artemyk,jreback,2015-04-25 03:10:19,2015-04-30 10:04:40,2015-04-30 10:04:04,closed,,0.16.1,9,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9983,b'BUG: .iloc and .loc behavior not consistent on empty dataframe',"b'Fixes #9964 .\r\n\r\nNotice that `assert_frame_equal` now fails for empty dataframes with different dtypes (as, I think, it should).  However, this means some tests need to be patched now.'"
9974,70499563,josteinbf,jreback,2015-04-23 20:00:42,2015-08-05 21:36:43,2015-08-05 21:36:42,closed,,0.17.0,6,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/9974,b'Change of index values dtype should be reflected in index type',b'This PR should eventually address issue #9966. I will look at testing first to make sure I have understood how pandas ought to work in these situations.'
9973,70464756,hhuuggoo,jreback,2015-04-23 17:18:27,2015-04-24 10:47:19,2015-04-24 10:47:01,closed,,,1,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/9973,b'dividing by integers wierd in 0.14.1',"b'In [1]: import pandas as pd; pd.Series ([-49.2, 0.0], index=[0,1]) / pd.Series([49,0], index=[0,1])                                                                                  \r\nOut[1]: \r\n0   -inf\r\n1    inf\r\ndtype: float64\r\n\r\nThis appears to be resolved in 0.15, and 0.16 - so not sure that this is worth reporting \r\n\r\n'"
9966,70097553,josteinbf,jreback,2015-04-22 11:12:43,2015-09-06 16:31:07,2015-09-06 16:31:07,closed,,0.17.0,3,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/9966,b'Int64Index with dtype=float and slicing issues',"b""I have stumbled onto problems with slicing a pandas DataFrame when an index that originally contained integers has been manipulated such that it contains floats. Consider the following example (somewhat contrived to make it stand on its own; in practice, data are loaded from file):\r\n\r\n    df = pd.DataFrame({'i': np.linspace(0, 5, 6).astype(np.int64), \r\n                   'j': np.linspace(2, 3, 6)})\r\n    df.set_index('i', inplace=True)\r\n    print(df.index)   # Int64Index([0, 1, 2, 3, 4, 5], dtype='int64')\r\n    df.index = df.index / 10.\r\n    print(df.index)   # Int64Index([0.0, 0.1, 0.2, 0.3, 0.4, 0.5], dtype='float64')\r\n    df[:0.3]\r\n\r\nThe final line raises this exception:\r\n\r\n    TypeError: the slice stop value [None] is not a proper indexer \r\n    for this index type (Int64Index)\r\n\r\nWhile it was relatively easy to work around this using an explicit cast along the lines of\r\n\r\n    df.index = np.asarray(df.index, dtype=np.float64) / 10.\r\n\r\nit took me quite a while to figure out what was going on. What I expected to happen in the original code was that the index would become a Float64Index or something similar. When I first noticed the Int64Index with dtype=float I thought that was wierd. Is it a good idea to change this behaviour so that there is not this mismatch between index type and dtype?\r\n\r\nMy pandas version is 0.15.2. User EdChum at StackOverflow[1] has kindly tested this on 0.16, finding the same behaviour with the code above. Slicing with .ix works on 0.16 but not on 0.15.2; slicing with .loc works for both versions.\r\n\r\n[1] http://stackoverflow.com/questions/29795225/pandas-int64index-with-dtype-float"""
9965,69987139,hsperr,jreback,2015-04-22 00:32:36,2015-08-06 01:21:48,2015-08-05 21:39:50,closed,,0.17.0,16,Bug;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9965,b'FIX: interesction and union changed index names. fixes #9943 partly #9862',b'had some time this morning and started working\r\n\r\ncloses #9943 \r\n\r\nfor now I just uncommented the explicit setting to None of the index names.\r\nlet me know if I should remove it completely.'
9964,69985755,artemyk,jreback,2015-04-22 00:23:43,2015-04-30 10:04:04,2015-04-30 10:04:04,closed,,0.16.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9964,b'.loc and .iloc returns different dtypes on empty dataframe',"b""`.iloc` returns `object` dtype for a non-object empty series:\r\n\r\n```\r\nIn [1]: import pandas as pd\r\nIn [2]: df = pd.DataFrame({'a':[1,2,3],'b':['b','b2','b3']})\r\n\r\nIn [3]: df.ix[[],:].loc[:,'a']\r\nOut[3]: Series([], name: a, dtype: int64)\r\n\r\nIn [4]: df.ix[[],:].iloc[:,0]\r\nOut[4]: Series([], name: a, dtype: object)\r\n```\r\n\r\n(came across this as I think it is related to the issue reported http://stackoverflow.com/questions/29749356/python-pandas-export-structure-only-no-rows-of-a-dataframe-to-sql/ , i.e. in sql code which is trying to infer series datatypes)\r\n\r\n```\r\n >>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.6\r\nCython: 0.21.1\r\nnumpy: 1.9.2\r\nscipy: 0.14.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.1.0\r\nsphinx: 1.2.3\r\npatsy: 0.2.1\r\ndateutil: 2.4.2\r\npytz: 2015.2\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: 0.9\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.8\r\npymysql: 0.6.2.None\r\npsycopg2: 2.5.4 (dt dec pq3 ext)\r\n```"""
9958,69692067,bembom,jreback,2015-04-20 22:05:38,2016-06-13 02:06:01,2016-06-12 21:04:40,closed,,Next Major Release,5,Bug;Difficulty Intermediate;Dtypes;Duplicate;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9958,b'Left join on an integer column casts the column to float if it is in one of the indexes',"b""When I left-join two data frames on an int64 column, this column is cast to a float if it is the index of one of the two data frames:\r\n```python\r\n>>> id = [1, 143349558619761320]\r\n>>> a = pd.DataFrame({'a': [1, 2]}, index=pd.Int64Index(id, name='id'))\r\n>>> b = pd.DataFrame({'id': id[1], 'b': [2]})\r\n>>> merge_index = pd.merge(a, b, left_index=True, right_on='id', how='left')\r\n>>> print(merge_index.dtypes)\r\na       int64\r\nb     float64\r\nid    float64\r\ndtype: object\r\n```\r\nIn this case this is a problem because casting back to an int changes the value of this column:\r\n```python\r\n>>> int(float(143349558619761320))\r\n143349558619761312\r\n```\r\nIf the int64 column is not in the index, the column is not cast to a float: \r\n```python\r\n>>> merge_column = pd.merge(a.reset_index(), b, on='id', how='left')\r\n>>> print(merge_column.dtypes)\r\nid      int64\r\na       int64\r\nb     float64\r\ndtype: object\r\n```\r\nI understand that integer columns get cast to a float if NaNs are introduced (like column b here), but in this case the final column contains no missing values, so casting to a float can be avoided.\r\n\r\n\r\nOutput from pd.show_versions():\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.0\r\nnose: 1.3.4\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.0.0\r\nsphinx: None\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.2\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None"""
9939,69451293,evanpw,jreback,2015-04-19 18:27:44,2015-04-28 23:16:52,2015-04-28 12:16:00,closed,,0.16.1,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9939,b'BUG: DataFrame constructor fails when columns is set and data=[]',b'From this StackOverflow question: http://stackoverflow.com/questions/29730488/setting-columns-for-an-empty-pandas-dataframe'
9936,69347002,sinhrks,jreback,2015-04-18 20:56:54,2015-04-22 13:55:10,2015-04-20 11:08:51,closed,,0.16.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9936,b'BUG: GroupBy.size doesnt attach index name properly if grouped by TimeGr...',b'Closes #9925'
9934,69329157,evanpw,jreback,2015-04-18 17:22:20,2015-04-18 20:52:21,2015-04-18 20:42:41,closed,,0.16.1,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9934,b'BUG: Exception when setting an empty range using DataFrame.loc',b'Fixes #9596\r\n'
9929,69278048,mortada,jreback,2015-04-18 07:01:35,2015-04-22 00:08:07,2015-04-21 23:38:48,closed,,0.16.1,3,API Design;Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9929,b'Allow add_categories() to accept Series/np.array and make raising on dup...',"b'...licates optional, fixes #9927'"
9926,69247223,sinhrks,jreback,2015-04-18 00:36:11,2015-06-02 19:25:19,2015-05-18 11:59:45,closed,,0.16.2,4,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9926,b'BUG: index name lost with timedelta ops',"b""```\r\nimport pandas as pd \r\n\r\ndtidx = pd.DatetimeIndex(['2011-01-01'], freq='D', name='dtidx')\r\n(dtidx + 1).name\r\n# dtidx\r\n\r\n# NG\r\n(dtidx + pd.Timedelta('1 day')).name\r\n# None\r\n\r\ntdidx = pd.TimedeltaIndex(['1 day'], freq='D', name='tdidx')\r\n(tdidx + 1).name\r\n# tdidx\r\n\r\n# NG\r\n(tdidx + pd.Timedelta('1 day')).name\r\n# None\r\n```\r\n\r\nref: #9862"""
9925,69246724,sinhrks,jreback,2015-04-18 00:29:48,2015-04-20 11:09:39,2015-04-20 11:08:51,closed,,0.16.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9925,"b""BUG: GroupBy.size doesn't attach index name properly if grouped by TimeGrouper""","b""```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndf = pd.DataFrame({'A': [1, 1, 1, 2, 2, 2], 'B': [1, 2, 3, 4, 5, 6]})\r\ngrouped = df.groupby('A')\r\n\r\nn = 20\r\ndata = np.random.randn(n, 4)\r\ndt_df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D'])\r\ndt_df['key'] = [datetime.datetime(2013, 1, 1), datetime.datetime(2013, 1, 2), pd.NaT,\r\n                datetime.datetime(2013, 1, 4), datetime.datetime(2013, 1, 5)] * 4\r\n\r\n# OK, index is named as 'key'\r\ngrouped = dt_df.groupby('key')\r\ngrouped.size()\r\n# key\r\n# 2013-01-01    4\r\n# 2013-01-02    4\r\n# 2013-01-04    4\r\n# 2013-01-05    4\r\n# dtype: int64\r\n\r\n# NG (no index name)\r\ngrouper = dt_df.groupby(pd.TimeGrouper(key='key', freq='D'))\r\ngrouper.size()\r\n# 2013-01-01    4\r\n# 2013-01-02    4\r\n# 2013-01-03    0\r\n# 2013-01-04    4\r\n# 2013-01-05    4\r\n# dtype: int64\r\n\r\n# other agg methods looks ok\r\ngrouper.count()\r\n#             A  B  C  D\r\n# key                   \r\n# 2013-01-01  4  4  4  4\r\n# 2013-01-02  4  4  4  4\r\n# 2013-01-03  0  0  0  0\r\n# 2013-01-04  4  4  4  4\r\n# 2013-01-05  4  4  4  4\r\n```\r\n\r\nref: #9862"""
9924,69214467,patrickfournier,jreback,2015-04-17 20:45:31,2015-06-10 20:02:11,2015-06-10 20:02:06,closed,,0.16.2,3,Bug;Dtypes;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9924,b'BUG: need better inference for path in Series construction (GH9456)',b'closes  #9456.\r\n'
9921,69160002,dsm054,jreback,2015-04-17 15:24:21,2015-04-29 10:32:15,2015-04-29 10:32:15,closed,,0.16.1,14,Bug;Categorical;Difficulty Intermediate;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/9921,b'groupby transform misbehaving with categoricals',"b'Starting from [this SO question](http://stackoverflow.com/questions/29699352/pandas-keyerror-on-nan-group-for-ints-but-not-floats), we can see that something is odd when doing a groupby using Series of category dtype.  (Today\'s trunk, 0.16.0-163-gf9f88b2.)\r\n\r\n```\r\n>>> df = pd.DataFrame({""a"": [5,15,25]})\r\n>>> c = pd.cut(df.a, bins=[0,10,20,30,40])\r\n>>> c\r\n0     (0, 10]\r\n1    (10, 20]\r\n2    (20, 30]\r\nName: a, dtype: category\r\nCategories (4, object): [(0, 10] < (10, 20] < (20, 30] < (30, 40]]\r\n>>> df.groupby(c).sum()\r\n           a\r\na           \r\n(0, 10]    5\r\n(10, 20]  15\r\n(20, 30]  25\r\n(30, 40] NaN\r\n>>> df.groupby(c).transform(sum)\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-81-1eb61986eff3>"", line 1, in <module>\r\n    df.groupby(c).transform(sum)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.16.0_163_gf9f88b2-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 3017, in transform\r\n    indexer = indices[name]\r\nKeyError: \'(30, 40]\'\r\n>>> df.a.groupby(c).transform(max)\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-87-9759e8c1e070>"", line 1, in <module>\r\n    df.a.groupby(c).transform(max)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.16.0_163_gf9f88b2-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 2422, in transform\r\n    return self._transform_fast(cyfunc)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.16.0_163_gf9f88b2-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 2461, in _transform_fast\r\n    values = np.repeat(values, com._ensure_platform_int(counts))\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 393, in repeat\r\n    return repeat(repeats, axis)\r\nValueError: count < 0\r\n```\r\n\r\nIt looks like we\'re somehow using the categories themselves, not the values.  Not sure if this is a consequence of the special-casing of grouping on categories.. \r\n\r\nISTM if we have a Series with categorical dtype passed to groupby, we should group on the values, and not return the missing elements-- if you want the missing elements, you should pass the Categorical object *itself*.  Admittedly I haven\'t thought through all the consequences, but I was pretty surprised when I read the docs and saw this was the intended behaviour.\r\n'"
9915,68972930,brownan,jreback,2015-04-16 17:36:44,2016-04-17 20:37:34,2016-04-17 20:37:34,closed,,0.18.1,3,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/9915,b'Resampling produces all NaN values for a particular dataset',"b'I ran into a particular dataset that seems to cause Series.resample() to produce incorrect results. Each datapoint is almost but not exactly a minute apart. (This is trimmed down from a larger dataset, but this still reproduces the problem)\r\n\r\n```python\r\n>>> import pandas; from pandas import Timestamp\r\n>>> data1 = pandas.Series(\r\n... {\r\n...  Timestamp(\'2015-03-31 21:48:52.672000\'): 2,\r\n...  Timestamp(\'2015-03-31 21:49:52.739000\'): 1,\r\n...  Timestamp(\'2015-03-31 21:50:52.806000\'): 2}\r\n... )\r\n>>> data1\r\n2015-03-31 21:48:52.672000    2\r\n2015-03-31 21:49:52.739000    1\r\n2015-03-31 21:50:52.806000    2\r\ndtype: int64\r\n>>> data1.resample(""10S"")\r\n2015-03-31 21:48:50   NaN\r\n2015-03-31 21:49:00   NaN\r\n2015-03-31 21:49:10   NaN\r\n2015-03-31 21:49:20   NaN\r\n2015-03-31 21:49:30   NaN\r\n2015-03-31 21:49:40   NaN\r\n2015-03-31 21:49:50   NaN\r\n2015-03-31 21:50:00   NaN\r\n2015-03-31 21:50:10   NaN\r\n2015-03-31 21:50:20   NaN\r\n2015-03-31 21:50:30   NaN\r\n2015-03-31 21:50:40   NaN\r\n2015-03-31 21:50:50   NaN\r\nFreq: 10S, dtype: float64\r\n```\r\n\r\nExpected output:\r\n```python\r\n>>> data1.resample(""10S"")\r\n2015-03-31 21:48:50   2\r\n2015-03-31 21:49:00   NaN\r\n2015-03-31 21:49:10   NaN\r\n2015-03-31 21:49:20   NaN\r\n2015-03-31 21:49:30   NaN\r\n2015-03-31 21:49:40   NaN\r\n2015-03-31 21:49:50   1\r\n2015-03-31 21:50:00   NaN\r\n2015-03-31 21:50:10   NaN\r\n2015-03-31 21:50:20   NaN\r\n2015-03-31 21:50:30   NaN\r\n2015-03-31 21:50:40   NaN\r\n2015-03-31 21:50:50   2\r\nFreq: 10S, dtype: float64\r\n```\r\n\r\nChanging any one of the timestamps or taking one of the two datapoints out works correctly. Something about this particular dataset causes resample to fail.\r\n\r\nI have more examples on this ipython notebook: http://nbviewer.ipython.org/gist/brownan/1000ba324ad7df917e32\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-123.20.1.el7.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.0\r\nnose: 1.3.4\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 3.0.0\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.1\r\npytz: 2015.2\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\nedit: I can also reproduce this with the latest version on master: 0.16.0-157-g161f38d'"
9907,68733491,ptrcarta,jreback,2015-04-15 16:48:54,2015-06-30 10:53:54,2015-06-30 10:53:54,closed,,0.17.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9907,b'DatetimeIndex behaves inconsistently when start and end have different precisions',"b""When creating a DatetimeIndex where start has a greater precision than end for a given frequency the output DatetimeIndex will be shorter than expected\r\n\r\n```python\r\n>>>pd.__version__\r\n0.15.2-318-g549b72f\r\n\r\n\r\n>>>pd.DatetimeIndex(start='2015-04-15 00:00:03', end='2016-04-22 00:00:04', freq='Q')\r\n2015-06-30 00:00:03\r\n2015-09-30 00:00:03\r\n2015-12-31 00:00:03\r\n2016-03-31 00:00:03\r\n\r\n\r\n>>>pd.DatetimeIndex(start='2015-04-15 00:00:03', end='2016-04-22 00:00', freq='Q')\r\n2015-06-30 00:00:03\r\n2015-09-30 00:00:03\r\n2015-12-31 00:00:03\r\n\r\n\r\n>>>pd.DatetimeIndex(start='2015-04-15 00:00:03', end='2015-6-22 00:00:04', freq='W')\r\n2015-04-19 00:00:03\r\n2015-04-26 00:00:03\r\n2015-05-03 00:00:03\r\n2015-05-10 00:00:03\r\n2015-05-17 00:00:03\r\n2015-05-24 00:00:03\r\n2015-05-31 00:00:03\r\n2015-06-07 00:00:03\r\n2015-06-14 00:00:03\r\n2015-06-21 00:00:03\r\n\r\n\r\n>>>pd.DatetimeIndex(start='2015-04-15 00:00:03', end='2015-6-22 00:00', freq='W')\r\n2015-04-19 00:00:03\r\n2015-04-26 00:00:03\r\n2015-05-03 00:00:03\r\n2015-05-10 00:00:03\r\n2015-05-17 00:00:03\r\n2015-05-24 00:00:03\r\n2015-05-31 00:00:03\r\n2015-06-07 00:00:03\r\n2015-06-14 00:00:03\r\n```"""
9905,68672112,jorisvandenbossche,TomAugspurger,2015-04-15 12:44:27,2015-04-16 18:55:42,2015-04-16 18:55:42,closed,,0.16.1,2,Bug;Regression;Visualization,https://api.github.com/repos/pydata/pandas/issues/9905,b'BUG: barplot with log=True not working for values smaller than 1',"b""```\r\ns = pd.Series([0.1, 0.01, 0.001], index=['a', 'b', 'c'])\r\ns.plot(kind='bar', log=True)\r\n```\r\n\r\ngives:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-75b989d29042> in <module>()\r\n      1 s = pd.Series([0.1, 0.01, 0.001], index=['a', 'b', 'c'])\r\n----> 2 s.plot(kind='bar', log=True)\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc in plot_series(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\r\n   2517                  yerr=yerr, xerr=xerr,\r\n   2518                  label=label, secondary_y=secondary_y,\r\n-> 2519                  **kwds)\r\n   2520 \r\n   2521 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc in _plot(data, x, y, subplots, ax, kind, **kwds)\r\n   2323         plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\r\n   2324 \r\n-> 2325     plot_obj.generate()\r\n   2326     plot_obj.draw()\r\n   2327     return plot_obj.result\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc in generate(self)\r\n    921         self._compute_plot_data()\r\n    922         self._setup_subplots()\r\n--> 923         self._make_plot()\r\n    924         self._add_table()\r\n    925         self._make_legend()\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc in _make_plot(self)\r\n   1884                 w = self.bar_width / K\r\n   1885                 rect = bar_f(ax, self.ax_pos + (i + 0.5) * w, y, w,\r\n-> 1886                              start=start, label=label, **kwds)\r\n   1887             self._add_legend_handle(rect, label, index=i)\r\n   1888 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc in f(ax, x, y, w, start, **kwds)\r\n   1823         if self.kind == 'bar':\r\n   1824             def f(ax, x, y, w, start=None, **kwds):\r\n-> 1825                 start = start + self.bottom\r\n   1826                 return ax.bar(x, y, w, bottom=start,log=self.log, **kwds)\r\n   1827         elif self.kind == 'barh':\r\n\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n```\r\n\r\nThis was working in 0.14.1"""
9902,68483894,jameshiebert,jameshiebert,2015-04-14 20:06:43,2015-06-02 19:29:58,2015-05-11 05:10:00,closed,,No action,8,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/9902,"b""str'ified column names in DataFrame.to_hdf(); Fixes #9057""","b""closes #9057\r\n \r\nConverted column names to strings before creating pytables metadata object\r\n\r\nThis brute force `str()`ing method *might* not be the best approach, but AFAIK it hasn't broken any of the tests."""
9890,68247802,mortada,shoyer,2015-04-14 03:05:52,2015-04-15 18:38:52,2015-04-15 17:33:15,closed,,0.16.1,5,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9890,b'ENH: allow Panel.shift on items axis',"b""I'm not sure why `Panel.shift` currently raises with `axis='items'`, while `axis=0` works.\r\n\r\nThis PR enables `axis='items'` and also updates the docstring to show the parameter `periods` instead of the deprecated `lags`"""
9888,68191176,jameshiebert,cpcloud,2015-04-13 20:49:58,2015-04-14 13:15:43,2015-04-14 13:15:43,closed,,0.16.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/9888,b'Ensured that the index gets retained on Frame.apped(). Fixes #9857',
9884,68183362,hsperr,jreback,2015-04-13 20:12:28,2015-04-14 13:56:34,2015-04-14 13:56:34,closed,,0.16.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/9884,"b'FIX: timeseries asfreq would drop the name of the index, closes #9854'",b'it seems that it is open discussion whether reindex should preserve the name of the index.\r\n(Towards the end of https://github.com/pydata/pandas/issues/6552)\r\n\r\nMy proposal for now is to set the name of the new index created by the asfreq function.\r\n'
9877,68176713,cpcloud,cpcloud,2015-04-13 19:40:07,2015-05-27 20:26:50,2015-04-14 14:15:14,closed,cpcloud,0.16.1,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9877,b'Fix right hand side dict assignment for DataFrames',b'closes #9874'
9876,68176574,ksanghai,jreback,2015-04-13 19:39:18,2015-04-14 13:14:03,2015-04-14 13:14:03,closed,,0.16.1,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9876,b'BUG: fix for index name lost #9857 and also added the corresponding test in test_index.py',b'closes issue #9857 '
9875,68144772,scari,cpcloud,2015-04-13 17:04:10,2015-04-14 12:59:54,2015-04-14 12:59:48,closed,,0.16.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/9875,b'BUG: unstack on unicode name level breaks #9856',b'This PR fixes #9856 \r\n\r\n[PYCON 2015 Sprints]'
9874,68144103,cpcloud,cpcloud,2015-04-13 16:59:52,2015-04-14 14:15:14,2015-04-14 14:15:14,closed,,0.16.1,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9874,"b""Dictionary assignment with loc with existing index doesn't properly match column names in dict""","b""uses the example DataFrame from #9857.\r\n\r\n```python\r\nIn [3]: df = pd.DataFrame({'x': [1,2,6], 'y': [2,2,8], 'z':[-5,0,5]})\r\n\r\nIn [4]: df = df.set_index('z')\r\n\r\nIn [5]: rhs = {'x': 9, 'y': 99}\r\n\r\nIn [6]: df.loc[5]\r\nOut[6]:\r\nx    6\r\ny    8\r\nName: 5, dtype: int64\r\n\r\nIn [7]: df.loc[5] = rhs\r\n\r\nIn [8]: df\r\nOut[8]:\r\n                      x                    y\r\nz\r\n-5                    1                    2\r\n 0                    2                    2\r\n 5  {u'y': 99, u'x': 9}  {u'y': 99, u'x': 9}\r\n```\r\n\r\n"""
9873,68141362,jcrist,jreback,2015-04-13 16:45:27,2015-04-14 13:21:23,2015-04-14 13:21:23,closed,,0.16.1,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9873,b'Groupby transform preserves output dtype',b'Previously `transform` output was always the same dtype as the groupby\r\nobject. This allows the output dtype to differ from the input. Fixes #9807.'
9872,68137674,pykler,jreback,2015-04-13 16:25:59,2015-04-14 13:40:41,2015-04-14 13:40:41,closed,,0.16.1,1,Bug;Compat;Error Reporting,https://api.github.com/repos/pydata/pandas/issues/9872,b'Fixing == __eq__ operator for MultiIndex ... closes #9785',b'closes #9785  '
9869,68126498,jcrist,cpcloud,2015-04-13 15:28:38,2015-04-14 14:26:45,2015-04-14 14:26:45,closed,,0.16.1,5,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/9869,b'Period accepts datetime64 value',b'Added support for `datetime64` value for Period. Fixes #9054.'
9864,67963643,janschulz,jreback,2015-04-12 21:45:29,2015-04-13 13:40:24,2015-04-13 13:40:24,closed,,0.16.1,4,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9864,b'Fix for comparisons of categorical and an scalar not in categories',"b'Up to now, a comparison of categorical data and a scalar, which\r\nis not in the categories would return `False` for all elements when\r\nit should raise a `TypeError`, which it now does.\r\n\r\nAlso fix that `!=` comparisons would return `False` for all elements\r\nwhen the more logical choice would be `True`.\r\n\r\nFixes the raised issue in https://github.com/pydata/pandas/issues/9836#issuecomment-92123057'"
9857,67832165,stared,jreback,2015-04-11 20:54:21,2015-04-14 13:14:14,2015-04-14 13:14:14,closed,,0.16.1,3,Bug;Difficulty Novice;Effort Low;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9857,b'Adding new rows drops index name',"b""    df = pd.DataFrame({'x': [1,2,6], 'y': [2,2,8], 'z':[-5,0,5]})\r\n    df = df.set_index('z')\r\n    \r\nThen `df.index.name` is `'z'`. After modifying an existing row\r\n\r\n    df.loc[5] = {'x': 9, 'y': 99}\r\n\r\nit's still fine. But once we add a new index:\r\n\r\n    df.loc[10] = {'x': 7, 'y': 77}\r\n\r\n`df.index.name` is `None`.\r\n\r\nI use Pandas 0.16.0 on Python 3.4.\r\n\r\nIt might be related to another issue with disappearing index name: https://github.com/pydata/pandas/issues/9854"""
9856,67816607,jreback,cpcloud,2015-04-11 18:29:13,2015-04-14 12:59:48,2015-04-14 12:59:48,closed,,Next Major Release,1,Bug;Difficulty Novice;Effort Low;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9856,b'BUG: unstack on unicode name level breaks',"b""xref #9650\r\n\r\n```\r\nIn [62]: i = pd.MultiIndex(\r\n    levels=[[u'foo', u'bar'], [u'one', u'two'], [u'a', u'b']],\r\n    labels=[[0, 0, 1, 1], [0, 1, 0, 1], [1, 0, 1, 0]],\r\n    names=[u'first', u'second', u'third'])\r\n\r\nIn [63]: s = pd.Series(0, index=i)\r\n\r\nIn [64]: s.unstack([1,2]).stack(0)\r\nKeyError: 'Level t not found'\r\n```"""
9854,67806709,amelio-vazquez-reina,jreback,2015-04-11 16:50:54,2015-04-14 13:56:16,2015-04-14 13:56:16,closed,,Next Major Release,1,Bug;Difficulty Novice;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/9854,b'asfreq drops the name of the index',"b""Here's a simple repro\r\n```\r\nIn [19]: df = DataFrame(range(3),columns=['foo'],index=pd.date_range('20130101',periods=3,name='bar'))\r\n\r\nIn [20]: df\r\nOut[20]: \r\n            foo\r\nbar            \r\n2013-01-01    0\r\n2013-01-02    1\r\n2013-01-03    2\r\n\r\nIn [21]: df.index.name\r\nOut[21]: 'bar'\r\n\r\nIn [22]: df.asfreq('10D')\r\nOut[22]: \r\n            foo\r\n2013-01-01    0\r\n\r\nIn [24]: df.asfreq('10D').index.name\r\n\r\n```\r\n-------------------------\r\nThis bug was originally reported on StackOverflow [here](http://stackoverflow.com/questions/29356412/efficiently-re-indexing-one-level-with-forward-fill-in-a-multi-index-dataframe/29404203?noredirect=1#comment47161025_29404203).  \r\n\r\n### Original question\r\nThe question was asking how to efficiently re-index one level of a multi-index DataFrame with \xa1\xb0forward-fill\xa1\xb1 using the following input DataFrame as an example:\r\n\r\n```\r\n                          value\r\nitem_uid   created_at          \r\n\r\n0S0099v8iI 2015-03-25  10652.79\r\n0F01ddgkRa 2015-03-25   1414.71\r\n0F02BZeTr6 2015-03-20  51505.22\r\n           2015-03-23  51837.97\r\n           2015-03-24  51578.63\r\n           2015-03-25       NaN\r\n           2015-03-26       NaN\r\n           2015-03-27  50893.42\r\n0F02BcIzNo 2015-03-17   1230.00\r\n           2015-03-23   1130.00\r\n0F02F4gAMs 2015-03-25   1855.96\r\n0F02Vwd6Ou 2015-03-19   5709.33\r\n0F04OlAs0R 2015-03-18    321.44\r\n0F05GInfPa 2015-03-16    664.68\r\n0F05PQARFJ 2015-03-18   1074.31\r\n           2015-03-26   1098.31\r\n0F06LFhBCK 2015-03-18    211.49\r\n0F06ryso80 2015-03-16     13.73\r\n           2015-03-20     12.00\r\n0F07gg7Oth 2015-03-19   2325.70\r\n```\r\n\r\n### Andy's answer:\r\n\r\n> You have a couple of options, the easiest IMO is to simply unstack the first level and then ffill. I think this make it much clearer about what's going on than a groupby/resample solution (I suspect it will also be faster, depending on the data):\r\n\r\n    In [11]: df1['value'].unstack(0)\r\n    Out[11]:\r\n    item_uid    0F01ddgkRa  0F02BZeTr6  0F02BcIzNo  0F02F4gAMs  0F02Vwd6Ou  0F04OlAs0R  0F05GInfPa  0F05PQARFJ  0F06LFhBCK  0F06ryso80  0F07gg7Oth  0S0099v8iI\r\n    created_at\r\n    2015-03-16         NaN         NaN         NaN         NaN         NaN         NaN      664.68         NaN         NaN       13.73         NaN         NaN\r\n    2015-03-17         NaN         NaN        1230         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-18         NaN         NaN         NaN         NaN         NaN      321.44         NaN     1074.31      211.49         NaN         NaN         NaN\r\n    2015-03-19         NaN         NaN         NaN         NaN     5709.33         NaN         NaN         NaN         NaN         NaN      2325.7         NaN\r\n    2015-03-20         NaN    51505.22         NaN         NaN         NaN         NaN         NaN         NaN         NaN       12.00         NaN         NaN\r\n    2015-03-23         NaN    51837.97        1130         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-24         NaN    51578.63         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-25     1414.71         NaN         NaN     1855.96         NaN         NaN         NaN         NaN         NaN         NaN         NaN    10652.79\r\n    2015-03-26         NaN         NaN         NaN         NaN         NaN         NaN         NaN     1098.31         NaN         NaN         NaN         NaN\r\n    2015-03-27         NaN    50893.42         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n\r\n> If you're missing some dates you have to reindex (assuming the start and end are present, otherwise you can do this manually e.g. with `pd.date_range`):\r\n\r\n    In [12]: df1['value'].unstack(0).asfreq('D')\r\n    Out[12]:\r\n    item_uid    0F01ddgkRa  0F02BZeTr6  0F02BcIzNo  0F02F4gAMs  0F02Vwd6Ou  0F04OlAs0R  0F05GInfPa  0F05PQARFJ  0F06LFhBCK  0F06ryso80  0F07gg7Oth  0S0099v8iI\r\n    2015-03-16         NaN         NaN         NaN         NaN         NaN         NaN      664.68         NaN         NaN       13.73         NaN         NaN\r\n    2015-03-17         NaN         NaN        1230         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-18         NaN         NaN         NaN         NaN         NaN      321.44         NaN     1074.31      211.49         NaN         NaN         NaN\r\n    2015-03-19         NaN         NaN         NaN         NaN     5709.33         NaN         NaN         NaN         NaN         NaN      2325.7         NaN\r\n    2015-03-20         NaN    51505.22         NaN         NaN         NaN         NaN         NaN         NaN         NaN       12.00         NaN         NaN\r\n    2015-03-21         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-22         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-23         NaN    51837.97        1130         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-24         NaN    51578.63         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n    2015-03-25     1414.71         NaN         NaN     1855.96         NaN         NaN         NaN         NaN         NaN         NaN         NaN    10652.79\r\n    2015-03-26         NaN         NaN         NaN         NaN         NaN         NaN         NaN     1098.31         NaN         NaN         NaN         NaN\r\n    2015-03-27         NaN    50893.42         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\r\n\r\n**Note: `asfreq` drops the name of the index (which is most likely a bug!)**\r\n"""
9853,67779897,sinhrks,jreback,2015-04-11 12:24:14,2015-04-11 15:58:20,2015-04-11 15:18:00,closed,,0.16.1,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/9853,b'BUG: plot(kind=hist) results in TypeError if it contains non-numeric data',"b""```\r\ndf = pd.DataFrame(np.random.rand(20, 5), columns=['A', 'B', 'C', 'D', 'E'])\r\ndf['C'] = ['A', 'B', 'C', 'D'] * 5\r\ndf['D'] = df['D'] * 100\r\n\r\ndf.plot(kind='hist')\r\n# TypeError: cannot concatenate 'str' and 'float' objects\r\n```"""
9852,67779375,sinhrks,jreback,2015-04-11 12:18:15,2015-04-18 14:26:06,2015-04-12 13:39:44,closed,,0.16.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/9852,b'BUG/CLN: Repeated time-series plot may raise TypeError',"b""This repeated time-series plotting works:\r\n\r\n```\r\nimport pandas.util.testing as tm\r\ns1 = tm.makeTimeSeries()\r\ns2 = s1[[0, 5, 10, 11, 12, 13, 14, 15]]\r\nax = s1.plot()\r\nax2 = s2.plot(style='g')\r\n```\r\n\r\n![line_incorrectsecondary](https://cloud.githubusercontent.com/assets/1696302/7101339/f74b566a-e08f-11e4-8b7f-5ed58327cc5c.png)\r\n\r\nBut if converted to ``DateFrame``, it doesn't:\r\n\r\n```\r\ns1 = s1.to_frame()\r\ns2 = s1.iloc[[0, 5, 10, 11, 12, 13, 14, 15]]\r\nprint(s2.index.freq)\r\nax = s1.plot()\r\nax2 = s2.plot(style='g', ax=ax)\r\n# TypeError: expected string or buffer\r\n```\r\n\r\nFixed the problem, and cleaned up the code to merge ``Series`` and ``DataFrame`` flows.\r\n\r\n#### After the Fix:\r\n\r\n```\r\nimport pandas.util.testing as tm\r\nfig, axes = plt.subplots(2, 1)\r\ns1 = tm.makeTimeSeries()\r\ns2 = s1[[0, 5, 10, 11, 12, 13, 14, 15]]\r\nax = s1.plot(ax=axes[0])\r\nax2 = s2.plot(style='g', ax=axes[0])\r\n\r\ns1 = s1.to_frame(name='x')\r\ns2 = s1.iloc[[0, 5, 10, 11, 12, 13, 14, 15]]\r\nax = s1.plot(ax=axes[1])\r\nax2 = s2.plot(style='g', ax=axes[1])\r\n```\r\n\r\n![line_incorrectsecondary](https://cloud.githubusercontent.com/assets/1696302/7103495/55f95316-e0e4-11e4-8086-6aedfae82006.png)\r\n\r\n"""
9851,67735100,hayd,jreback,2015-04-11 05:08:05,2015-04-11 13:06:48,2015-04-11 13:06:10,closed,,,1,Bug;Duplicate;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9851,b'timedelta from string requires leading 0',"b'http://stackoverflow.com/questions/29573441/calculate-difference-between-times-rows-in-dataframe-pandas#comment47297436_29573471\r\n\r\n```\r\nIn [11]: pd.to_timedelta(""06:40:00"")\r\nOut[11]: Timedelta(\'0 days 06:40:00\')\r\n\r\nIn [12]: pd.to_timedelta(""6:40:00"")\r\nValueError: cannot create timedelta string converter for [6:40:00]\r\n```'"
9850,67721481,sinhrks,jreback,2015-04-11 02:20:38,2016-04-09 18:01:56,2016-04-09 18:01:56,closed,,0.18.1,1,API Design;Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/9850,b'API: SparseSeries.to_frame and SparseDataFrame.to_panel result in dense structures',"b""Related to #9802. I think these should return sparse.\r\n\r\n```\r\nimport pandas as pd\r\n\r\nss = pd.SparseSeries([1, 0, 1])\r\ntype(ss.to_frame())\r\n# <class 'pandas.core.frame.DataFrame'>\r\n\r\nindex = pd.MultiIndex.from_tuples([(0, 0), (0, 1), (1, 1)])\r\nsdf = pd.SparseDataFrame([[1, 0, 1], [0, 0, 0], [1, 0, 0]], index=index)\r\ntype(sdf.to_panel())\r\n# <class 'pandas.core.panel.Panel'>\r\n```"""
9849,67624344,julienvienne,jreback,2015-04-10 15:49:50,2015-04-11 15:21:46,2015-04-11 15:20:43,closed,,,1,Bug;Dtypes;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9849,b'read_csv : string type not used for multiindex column',"b'Hello,\r\nI have a CSV like this :\r\n```csv\r\nPOSTE;DATE;RR;TN;TX;SIGMA\r\n06088001;20150316;8,1;7,4;14,1;11\r\n06088001;20150317;0,6;9,8;15,6;0\r\n```\r\nFirst column designate a 8 character code which should be handled as a string. \r\nSecond column is a datetime. The two columns are designated as index column with index_col argument :\r\n```python\r\n>>>df = read_csv(fileame, sep="";"",\r\n              header=0, \r\n              parse_dates=[1],\r\n              decimal="","",  \r\n              index_col=[0,1],\r\n              dtype={""POSTE"":str})\r\n>>>df \r\n                       RR    TN    TX  SIGMA\r\nPOSTE    DATE                               \r\n6088001  2015-03-16   8.1   7.4  14.1     11\r\n         2015-03-17   0.6   9.8  15.6      0\r\n```\r\n\r\nAs you can see, there is a missing ""0"" for POSTE column because it has been casted to integer, despite of `dtype={""POSTE"":str}` argument. I also tried with `dtype={""POSTE"":object}` instruction as indicated on forums it still gives the same result.\r\n\r\nApparently this only occurs only if column is used in an index column.\r\nColumn is not converted if I only use DATE for the index :\r\n```python\r\n>>> df = read_csv(fileame, sep="";"",\r\n              header=0, \r\n              parse_dates=[1],\r\n              decimal="","",  \r\n              index_col=[1],\r\n              dtype={""POSTE"":str})\r\n\r\n>>> df \r\n            POSTE       RR    TN    TX  SIGMA\r\nDATE                                         \r\n2015-03-16  06088001   8.1   7.4  14.1     11\r\n2015-03-17  06088001   0.6   9.8  15.6      0\r\n```\r\nI\'m using Pandas v 0.14.1 so it may have been fixed recently.\r\nDo you confirm this is not the correct behaviour ?\r\n\r\nRegards\r\n\r\n\r\n\r\n\r\n'"
9848,67608444,janschulz,jreback,2015-04-10 14:30:38,2015-05-11 22:21:16,2015-04-11 20:06:52,closed,,0.16.1,27,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9848,b'Fix for unequal comparisons of categorical and scalar',b'\r\nFixes: #9836 '
9846,67494241,behzadnouri,jreback,2015-04-10 02:41:58,2015-04-12 13:57:14,2015-04-12 13:37:27,closed,,0.16.1,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9846,b'BUG: memory access bug in read_csv causing segfault',"b'closes https://github.com/pydata/pandas/issues/5664\r\n\r\nthe added test covers the original issue, though I cannot reproduce that on master; [the one mentioned in the comments](https://github.com/pydata/pandas/issues/5664#issuecomment-49332533) still segfaults on master (if repeated few times) and is fixed by this pr.'"
9845,67494239,davidastephens,jreback,2015-04-10 02:41:56,2015-04-12 13:38:26,2015-04-12 13:38:22,closed,,0.16.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/9845,b'BUG: raw_locales unreachable in util.testing.get_locales',b'Fixes #9744 '
9838,67265081,evanpw,jreback,2015-04-09 02:29:07,2015-08-18 12:09:34,2015-08-18 11:20:12,closed,,0.17.0,9,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9838,b'BUG: DataFrame.where does not respect axis parameter when shape is symmetric',"b'Fixes GH #9736.\r\n\r\nThe problem was that at the ``Block`` level, ``where`` and ``putmask`` did a lot of guessing about how the block and the other parameters needed to be transposed in order to align correctly. It guessed wrongly when the DataFrame was symmetric because the shapes match up in both directions. This change tries to make everything explicit and remove the guessing.'"
9837,67256210,evanpw,jreback,2015-04-09 01:20:03,2015-04-28 10:30:23,2015-04-28 01:02:18,closed,,0.16.1,4,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9837,b'BUG: read_csv skips lines with initial whitespace + one non-space character',b'Fixes GH #9710'
9834,67123037,evanpw,jreback,2015-04-08 12:54:05,2015-09-19 00:38:21,2015-04-08 18:05:47,closed,,0.16.1,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9834,"b""BUG: skiprows doesn't handle blank lines properly when engine='c'""",b'Fixes GH #9832'
9832,67028392,lexual,jreback,2015-04-08 01:45:17,2015-04-09 14:45:33,2015-04-09 14:45:33,closed,,0.16.1,5,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9832,"b'Regression: read_csv call with skiprows fails on pandas versions 0.15.2, 0.16.0, works on 0.15.0 and 0.15.1'","b'    # passes for 0.15.0\r\n    # passes for 0.15.1\r\n    # fails for 0.15.2\r\n    # fails for 0.16.0\r\n    # This is for a ""csv"" file where there are a number of initial rows to be skipped at file start.\r\n    import pandas as pd\r\n    import urllib\r\n    \r\n    test_data_url = \'http://www.bom.gov.au/fwo/IDV60901/IDV60901.95936.axf\'\r\n    test_file = \'test.csv\'\r\n    \r\n    ROWS_TO_SKIP_AT_THE_START = 19\r\n    \r\n    \r\n    def main():\r\n        urllib.urlretrieve(test_data_url, test_file)\r\n        print(\'pandas version: {}\'.format(pd.version.version))\r\n        data = pd.read_csv(test_file, skiprows=ROWS_TO_SKIP_AT_THE_START)\r\n        assert len(data) == 145\r\n        assert \'sort_order\' in data\r\n        print(\'file successfully read by read_csv()\')\r\n    \r\n    \r\n    \r\n    if __name__ == \'__main__\':\r\n        main()\r\n'"
9807,66263082,hayd,jreback,2015-04-04 02:05:10,2015-04-14 13:19:45,2015-04-14 13:19:45,closed,,0.16.1,0,Bug;Difficulty Intermediate;Dtypes;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/9807,b'Groupby mean transform not converting to float',"b""```\r\nIn [11]: df = pd.DataFrame([[1, 3], [2, 3]])\r\n\r\nIn [12]: df.groupby(1).transform('mean')\r\nOut[12]:\r\n   0\r\n0  1\r\n1  1\r\n\r\nIn [13]: df.groupby(1).mean()  # converts when it's a normal groupby mean\r\nOut[13]:\r\n     0\r\n1\r\n3  1.5\r\n```\r\nhttp://stackoverflow.com/a/29437332/1240268\r\n"""
9805,66202142,behzadnouri,shoyer,2015-04-03 18:19:26,2015-05-06 02:50:31,2015-04-03 22:41:42,closed,,0.16.1,5,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/9805,b'BUG: bug in json lib when frame has length zero',"b'closes https://github.com/pydata/pandas/issues/9781\r\n\r\non master:\r\n\r\n```\r\n>>> df = DataFrame(columns=[\'jim\', \'joe\'])\r\n>>> df\r\nEmpty DataFrame\r\nColumns: [jim, joe]\r\nIndex: []\r\n>>> df.to_json()\r\n\'{}\'\r\n```\r\nmixed type will segfault:\r\n\r\n```\r\n>>> df[\'joe\'] = df[\'joe\'].astype(\'i8\')\r\n>>> df.to_json()\r\nSegmentation fault (core dumped)\r\n```\r\n\r\non branch:\r\n```\r\n>>> df.to_json()\r\n\'{""jim"":{},""joe"":{}}\'\r\n>>> df[\'joe\'] = df[\'joe\'].astype(\'i8\')\r\n>>> read_json(df.to_json())\r\nEmpty DataFrame\r\nColumns: [jim, joe]\r\nIndex: []\r\n```'"
9804,66192484,evanpw,jreback,2015-04-03 17:18:07,2015-09-19 00:38:19,2015-04-14 15:24:45,closed,,0.16.1,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9804,"b'BUG: where behaves badly when dtype of self is datetime or timedelta, and dtype of other is not'","b'There are a few weird behaviors that this fixes:\r\n\r\n1. If ``other`` is an ``int`` or ``float``, then ``where`` throws an Exception while trying to call ``other.view``\r\n2. If ``other`` is an np.int64, it works fine\r\n3. If ``other`` is an np.float64, there is no error, but the result is bizarre (it reinterprets the bits of the float as an integer, rather than casting it)\r\n4. If ``other`` is list-like and has a numerical dtype, then it throws an exception while try to call ``all`` on the value False (for some reason, comparing an ndarray with dtype datetime64 and an ndarray with an integer dtype just returns a scalar False rather than a boolean ndarray)\r\n\r\n```\r\n>>> s = Series(date_range(\'20130102\', periods=2))\r\n>>> s.where(s.isnull(), 0)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/generic.py"", line 3399, in where\r\n    try_cast=try_cast)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/internals.py"", line 2469, in where\r\n    return self.apply(\'where\', **kwargs)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/internals.py"", line 2451, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/internals.py"", line 1080, in where\r\n    result = func(cond, values, other)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/internals.py"", line 1063, in func\r\n    v, o = self._try_coerce_args(v, o)\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/internals.py"", line 1819, in _try_coerce_args\r\n    other = other.view(\'i8\')\r\nAttributeError: \'int\' object has no attribute \'view\'\r\n>>> s.where(s.isnull(), np.int64(10))\r\n0   1970-01-01 00:00:00.000000010\r\n1   1970-01-01 00:00:00.000000010\r\ndtype: datetime64[ns]\r\n>>> s.where(s.isnull(), np.float64(10))\r\n0   2116-06-17 06:38:37.588971520\r\n1   2116-06-17 06:38:37.588971520\r\ndtype: datetime64[ns]\r\n>>> s.where(s.isnull(), [0, 1])\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/evanpw/Workspace/pandas/pandas/core/generic.py"", line 3326, in where\r\n    if not (new_other == np.array(other)).all():\r\nAttributeError: \'bool\' object has no attribute \'all\'\r\n```'"
9800,66140131,kshedden,jreback,2015-04-03 12:44:36,2015-11-12 23:44:34,2015-04-07 10:28:47,closed,,0.16.1,3,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/9800,b'Closes #9795 (Stata writer changes input frame)',b'closes #9795 \r\n\r\nJust needed to move the data frame copy earlier in the execution path.'
9795,66033108,tzye,jreback,2015-04-02 23:39:12,2015-04-07 10:28:15,2015-04-07 10:28:15,closed,,0.16.1,1,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/9795,b'Unexpected in-place changes when saving a DataFrame to Stata with write_index=False',"b""After executing `df.to_stata()` with `write_index=False`, all `NaNs` in `df` were automatically replaced by `8.988466e+307`. Please see the following code:\r\n\r\n`df = pd.DataFrame(np.random.randn(5,4), columns=list('abcd'))`\r\n`df.ix[2, 'a':'c'] = np.nan`\r\n`print df`\r\n`df.to_stata('test.dta', write_index=False)`\r\n`print df`\r\nI am using `pandas v0.16.0`. Thanks."""
9788,65910190,katastrofa999,jreback,2015-04-02 11:49:45,2015-04-02 12:00:07,2015-04-02 11:59:47,closed,,,1,Bug;Compat;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9788,b'groupby() changes values of pandas.Timestamp (pandas 0.15.2)',"b""    import datetime as dt; import pandas as pd\r\n    d2 = dt.datetime(2015, 4, 2, 12, 23, 12, 567000)\r\n    df = pd.DataFrame([['Bar', d2]], columns=['name', 'date'])\r\n    df2 = df.sort('date').groupby('name').last().reset_index()\r\n\r\n`df2` is\r\n\r\n\r\nname\tdate\r\n0\tBar\t2015-04-02 12:23:12.567000064\r\n\r\ni.e. 64 nanoseconds were added to the timestamp. The amounts added seem to be always the powers of 2, perhaps it is caused by some internal conversion to double?"""
9783,65808087,invisibleroads,cpcloud,2015-04-02 00:52:27,2015-04-13 16:01:48,2015-04-13 16:01:48,closed,,0.16.1,21,Bug;Compat;Msgpack,https://api.github.com/repos/pydata/pandas/issues/9783,b'Fix zlib and blosc imports in to_msgpack',"b""6717aa06dcaa1950ffb46fef454f5df9404209bd removed zlib and blosc from the global namespace.\r\n\r\n    from pandas import read_csv\r\n    table = read_csv('aadhaar_data.csv')\r\n    table.to_msgpack('d.msg')\r\n\r\n    # NameError: global name 'blosc' is not defined\r\n    table.to_msgpack('d-blosc.msg', compress='blosc')\r\n\r\n    # NameError: global name 'zlib' is not defined\r\n    table.to_msgpack('d-zlib.msg', compress='zlib')\r\n\r\nThis pull request restores zlib and blosc compression in to_msgpack via local imports."""
9781,65768497,nevermindewe,shoyer,2015-04-01 20:06:37,2015-04-03 22:42:19,2015-04-03 22:42:19,closed,,0.16.1,6,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/9781,b'bug in dataframe.to_json causing segfault with 0.16.0',"b'calling to_json on an apparently empty dataframe may segfault.\r\nusing the same dataframe with 0.15.2 works.\r\n\r\nA git bisect says that commit a67bef49e0b8e89e785c08011e8331e9eb7e54b8 is when the problem first appeared.\r\n\r\nsegfault:\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0  0x00007fd42aae8735 in PdBlock_iterBegin (_obj=0x7fd423d22410, tc=0x7fff1bf57310) at pandas/src/ujson/python/objToJSON.c:1057\r\n(gdb) list\r\n1052\t        // init a dedicated context for this column\r\n1053\t        NpyArr_iterBegin(obj, tc);\r\n1054\t        npyarr = GET_TC(tc)->npyarr;\r\n1055\t\r\n1056\t        // set the dataptr to our desired column and initialise\r\n1057\t        npyarr->dataptr += npyarr->stride * idx;\r\n1058\t        NpyArr_iterNext(obj, tc);\r\n1059\t        GET_TC(tc)->itemValue = NULL;\r\n1060\t        ((PyObjectEncoder*) tc->encoder)->npyCtxtPassthru = NULL;\r\n1061\t\r\n\r\n\r\nFollowing is the test code and ascii-pickle file that reproduces the behavior:\r\n```\r\nimport pickle\r\np = pickle.load(open(""p.pkl"", ""rb""))\r\np.to_json(""/tmp/foo.json"", date_format=\'iso\', orient=\'records\')\r\n```\r\n\r\nbelow is the pickled dataframe that causes the problem.\r\n(I\'ve been unable to construct another dataframe that exhibits this behavior)\r\n```\r\nccopy_reg\r\n_reconstructor\r\np0\r\n(cpandas.core.frame\r\nDataFrame\r\np1\r\nc__builtin__\r\nobject\r\np2\r\nNtp3\r\nRp4\r\ng0\r\n(cpandas.core.internals\r\nBlockManager\r\np5\r\ng2\r\nNtp6\r\nRp7\r\n((lp8\r\ncpandas.core.index\r\n_new_Index\r\np9\r\n(cpandas.core.index\r\nIndex\r\np10\r\n(dp11\r\nS\'data\'\r\np12\r\ncnumpy.core.multiarray\r\n_reconstruct\r\np13\r\n(cnumpy\r\nndarray\r\np14\r\n(I0\r\ntp15\r\nS\'b\'\r\np16\r\ntp17\r\nRp18\r\n(I1\r\n(I5\r\ntp19\r\ncnumpy\r\ndtype\r\np20\r\n(S\'O8\'\r\np21\r\nI0\r\nI1\r\ntp22\r\nRp23\r\n(I3\r\nS\'|\'\r\np24\r\nNNNI-1\r\nI-1\r\nI63\r\ntp25\r\nbI00\r\n(lp26\r\nS\'symbol\'\r\np27\r\naS\'side\'\r\np28\r\naS\'shares\'\r\np29\r\naS\'entry_price\'\r\np30\r\naS\'entry_time\'\r\np31\r\natp32\r\nbsS\'name\'\r\np33\r\nNstp34\r\nRp35\r\nag9\r\n(cpandas.core.index\r\nInt64Index\r\np36\r\n(dp37\r\ng12\r\ng13\r\n(g14\r\n(I0\r\ntp38\r\ng16\r\ntp39\r\nRp40\r\n(I1\r\n(I0\r\ntp41\r\ng20\r\n(S\'i8\'\r\np42\r\nI0\r\nI1\r\ntp43\r\nRp44\r\n(I3\r\nS\'<\'\r\np45\r\nNNNI-1\r\nI-1\r\nI0\r\ntp46\r\nbI00\r\nS\'\'\r\np47\r\ntp48\r\nbsg33\r\nNstp49\r\nRp50\r\na(lp51\r\ng13\r\n(g14\r\n(I0\r\ntp52\r\ng16\r\ntp53\r\nRp54\r\n(I1\r\n(I3\r\nI0\r\ntp55\r\ng44\r\nI00\r\ng47\r\ntp56\r\nbag13\r\n(g14\r\n(I0\r\ntp57\r\ng16\r\ntp58\r\nRp59\r\n(I1\r\n(I2\r\nI0\r\ntp60\r\ng23\r\nI00\r\n(lp61\r\ntp62\r\nba(lp63\r\ng9\r\n(g10\r\n(dp64\r\ng12\r\ng13\r\n(g14\r\n(I0\r\ntp65\r\ng16\r\ntp66\r\nRp67\r\n(I1\r\n(I3\r\ntp68\r\ng23\r\nI00\r\n(lp69\r\ng28\r\nag29\r\nag31\r\natp70\r\nbsg33\r\nNstp71\r\nRp72\r\nag9\r\n(g10\r\n(dp73\r\ng12\r\ng13\r\n(g14\r\n(I0\r\ntp74\r\ng16\r\ntp75\r\nRp76\r\n(I1\r\n(I2\r\ntp77\r\ng23\r\nI00\r\n(lp78\r\ng27\r\nag30\r\natp79\r\nbsg33\r\nNstp80\r\nRp81\r\na(dp82\r\nS\'0.14.1\'\r\np83\r\n(dp84\r\nS\'axes\'\r\np85\r\ng8\r\nsS\'blocks\'\r\np86\r\n(lp87\r\n(dp88\r\nS\'mgr_locs\'\r\np89\r\ng13\r\n(g14\r\n(I0\r\ntp90\r\ng16\r\ntp91\r\nRp92\r\n(I1\r\n(I3\r\ntp93\r\ng44\r\nI00\r\nS\'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\'\r\np94\r\ntp95\r\nbsS\'values\'\r\np96\r\ng54\r\nsa(dp97\r\ng89\r\nc__builtin__\r\nslice\r\np98\r\n(I0\r\nI6\r\nI3\r\ntp99\r\nRp100\r\nsg96\r\ng59\r\nsasstp101\r\nbb.\r\n```\r\n'"
9778,65708727,frubino,shoyer,2015-04-01 15:11:18,2015-07-23 11:01:52,2015-04-16 06:15:12,closed,,0.16.1,4,Bug;Difficulty Novice;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/9778,b'Wrong number of row in to_latex output for MultiIndex Dataframe',"b""If I try to run this code:\r\n\r\n```python\r\n\r\npd.DataFrame.from_dict({\r\n    ('c1', 0): pd.Series({x: x for x in range(4)}),\r\n    ('c1', 1): pd.Series({x: x+4 for x in range(4)}),    \r\n    ('c2', 0): pd.Series({x: x for x in range(4)}),\r\n    ('c2', 1): pd.Series({x: x+4 for x in range(4)}),\r\n    ('c3', 0): pd.Series({x: x for x in range(4)}),\r\n}).T.to_latex()\r\n```\r\n\r\nThe result is 2 rows instead of 5 in the latex output. If I use a csv it works.\r\n\r\nI'm running pandas 0.16.0 under python 2.7 on Mac OS X 10.10"""
9765,65532244,RasterBurn,jreback,2015-03-31 19:51:46,2016-04-25 13:42:20,2016-04-25 13:42:20,closed,,0.18.1,1,Bug;Reshaping;Sparse,https://api.github.com/repos/pydata/pandas/issues/9765,b'concat erroneously sets series to NaN',"b'problem\r\n======\r\n\r\n`concat` fails when:\r\n\r\n1. Using a SparseDataFrame\r\n2. AND that SparseDataFrame has a column of all `0.0` and a `fill_value` of `0.0`\r\n\r\nIf that SparseDataFrame doesn\'t have an all `0.0` column, it works beautifully.\r\n\r\n```python\r\n\r\nimport unittest\r\nimport pandas as pd\r\nfrom pandas.util.testing import assert_frame_equal\r\nimport sys\r\n\r\nclass TestSparseConcat(unittest.TestCase):\r\n    def setUp(self):\r\n        self.orig = {""A"": [1.0, 0.0, 1.0, 0.0]}\r\n        self.to_add = {\r\n            ""B"": [0.0, 0.0, 0.0, 0.0],\r\n            ""C"": [0.0, 0.0, 1.0, 0.0]\r\n        }\r\n        merged = self.orig.copy()\r\n        merged.update(self.to_add)\r\n        self.expected = pd.DataFrame(data=merged)\r\n\r\n    def test_concat_df_to_df(self):\r\n        A = pd.DataFrame(data=self.orig)\r\n        B = pd.DataFrame(data=self.to_add)\r\n        C = pd.concat([A, B], axis=1)\r\n        assert_frame_equal(C, self.expected)  # this works\r\n\r\n    def test_concat_sparse_to_df(self):\r\n        A = pd.DataFrame(data=self.orig)\r\n        B = pd.DataFrame(data=self.to_add).to_sparse(fill_value=0.0)\r\n        C = pd.concat([A, B], axis=1)\r\n        sys.stderr.write(""\\nExpected:\\n{}\\nGot:\\n{}\\n"".format(self.expected, C))\r\n        assert_frame_equal(C, self.expected)  # this DOESN\'T work\r\n\r\nif __name__ == ""__main__"":\r\n    unittest.main()\r\n\r\n```\r\n\r\noutput\r\n=====\r\n\r\n```\r\n.\r\nExpected:\r\n   A  B  C\r\n0  1  0  0\r\n1  0  0  0\r\n2  1  0  1\r\n3  0  0  0\r\nGot:\r\n   A   B  C\r\n0  1 NaN  0\r\n1  0 NaN  0\r\n2  1 NaN  1\r\n3  0 NaN  0\r\nF\r\n======================================================================\r\nFAIL: test_concat_sparse_to_df (__main__.TestSparseConcat)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""example.py"", line 28, in test_concat_sparse_to_df\r\n    assert_frame_equal(C, self.expected)\r\n  File ""/home/vagrant/.virtualenvs/ai-modeling/lib/python2.7/site-packages/pandas/util/testing.py"", line 748, in assert_frame_equal\r\n    check_exact=check_exact)\r\n  File ""/home/vagrant/.virtualenvs/ai-modeling/lib/python2.7/site-packages/pandas/util/testing.py"", line 692, in assert_series_equal\r\n    assert_almost_equal(left.values, right.values, check_less_precise)\r\n  File ""das/src/testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2758)\r\n  File ""das/src/testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas/src/testing.c:1843)\r\n  File ""das/src/testing.pyx"", line 102, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2010)\r\nAssertionError: First object is null, second isn\'t: nan != 0.0\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.024s\r\n\r\nFAILED (failures=1)\r\n```\r\n\r\nmetadata\r\n=======\r\n\r\n```\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-300.3.1.el6uek.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.16.0\r\nnose: 1.3.3\r\nCython: None\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: 3.0.0\r\nsphinx: 1.2\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.2.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\n```'"
9763,65496185,zegres,jreback,2015-03-31 16:45:07,2015-08-12 15:14:02,2015-08-12 15:14:02,closed,,0.17.0,7,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9763,b'BUG: incorrectly defined Memoridal Day holiday',"b""closes #9760 \r\n\r\nMemorial Day was incorrectly defined.\r\nIt is the last Monday in May, so the earliest it could be is the 25th (not the 24th).\r\nThis change fixes the problem.\r\n\r\n    >>> from pandas.tseries.holiday import AbstractHolidayCalendar, USMemorialDay\r\n    >>> class MemorialDayCalendar(AbstractHolidayCalendar):rules=[USMemorialDay]\r\n    >>> MemorialDayCalendar().holidays('2021','2022')\r\n\r\nWill now return Timestamp('2021-05-31 00:00:00') instead of Timestamp('2021-05-24 00:00:00')"""
9760,65450274,zegres,jreback,2015-03-31 13:27:19,2015-08-18 11:09:30,2015-08-18 11:09:30,closed,,0.17.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/9760,b'USMemorialDay defined incorrectly.',"b""Memorial day is incorrectly defined. \r\n\r\n    >>> from pandas.tseries.holiday import AbstractHolidayCalendar, USMemorialDay\r\n    >>> class MemorialDayCalendar(AbstractHolidayCalendar): rules=[USMemorialDay]\r\n    >>> MemorialDayCalendar().holidays('2021','2022')\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2021-05-24]\r\n    Length: 1, Freq: None, Timezone: None\r\n\r\nThe [actual date](http://www.calendar-12.com/holidays/memorial_day/2021) of Memorial day in 2021 is 5/31.\r\n\r\nMemorial day should be defined as:\r\n`Holiday('Memorial Day'    , month=5 , day=25, offset=DateOffset(weekday=MO(1)))`"""
9756,65394968,bjonen,jreback,2015-03-31 08:15:01,2015-05-04 19:09:49,2015-05-04 19:09:49,closed,,0.16.1,6,Bug;Difficulty Novice;Effort Low;Resample,https://api.github.com/repos/pydata/pandas/issues/9756,b'Resample BM adds extra index point',"b""```\r\nIn [1]: index = pd.DatetimeIndex(start='20150101', end='20150331', freq='B')\r\n\r\nIn [2]: df = pd.DataFrame(index=index, data=len(index)*[0])\r\n\r\nIn [3]: df.resample('BM', how='last')\r\nOut[3]:\r\n             0\r\n2015-01-30   0\r\n2015-02-27   0\r\n2015-03-31   0\r\n2015-04-30 NaN\r\n```\r\nThis is both in 0.14.1 and 0.15.2"""
9755,65392950,brechtm,jreback,2015-03-31 08:01:57,2016-04-06 19:17:26,2016-04-06 19:17:26,closed,,0.18.1,2,Bug;Difficulty Intermediate;Effort Medium;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9755,"b'read_csv with names, usecols and parse_dates'","b""xref #12203 \r\n\r\nThe arrays passed to the date_parser function is different when `names` and `use_cols` are specified to limit the number of parsed columns.\r\n\r\nWhen running the example code below, the date_parser function receives two arguments, one array with '20140101' strings, and one array with integers. The default `date_parser` fails to process this input.\r\n\r\nWhen assigning an empty list to `DROPPED_COLUMNS` (so that all columns are parsed), the second array contains strings instead of integers, and the datetimes are parsed correctly.\r\n\r\nThe problem doesn't occur with `engine='python'`. I haven't tested the influence of the `header` and `index_cols` options.\r\n\r\nPython script:\r\n\r\n\tfrom __future__ import print_function, division\r\n\timport pandas as pd\r\n\r\n\tCSV = '2014.csv'\r\n\r\n\tDROPPED_COLUMNS = ['NCDC', 'I', 'QCP']\r\n\t# DROPPED_COLUMNS = []\r\n\r\n\twith open(CSV) as csv:\r\n\t    csv.readline()\r\n\t    column_names = csv.readline().split()\r\n\t    used_columns = [i for i, column_name in enumerate(column_names)\r\n\t                    if column_name not in DROPPED_COLUMNS]\r\n\t    used_col_names = [column_name for i, column_name in enumerate(column_names)\r\n\t                      if i in used_columns]\r\n\t    parse_dates = [[i for i, column_name in enumerate(used_col_names)\r\n\t                    if column_name in ('Date', 'HrMn')]]\r\n\t    print(parse_dates)\r\n\r\n\t    data = pd.read_csv(csv, header=None, names=used_col_names, index_col=False, engine='python',\r\n\t                       parse_dates=parse_dates, usecols=used_columns)\r\n\t    print(data)\r\n\r\nContents of 2014.csv:\r\n\r\n\tIdentification                          TEMP\r\n\tUSAF   NCDC  Date     HrMn I Type  QCP  Temp   Q\r\n\t062693,99999,20140101,0025,4,FM-15,    ,   7.0,1,\r\n\t062693,99999,20140101,0055,4,FM-15,    ,   6.0,1,\r\n\t062693,99999,20140101,0125,4,FM-15,    ,   6.0,1,\r\n\t062693,99999,20140101,0155,4,FM-15,    ,   6.0,1,\r\n\t062693,99999,20140101,0225,4,FM-15,    ,   6.0,1,\r\n\t062693,99999,20140101,0255,4,FM-15,    ,   6.0,1,\r\n\t062693,99999,20140101,0325,4,FM-15,    ,   6.0,1,\r\n\t062693,99999,20140101,0355,4,FM-15,    ,   6.0,1,"""
9743,64946114,evanpw,jreback,2015-03-28 14:08:51,2015-06-10 13:41:24,2015-04-02 21:25:39,closed,,0.16.1,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9743,b'BUG: where gives incorrect results when upcasting (GH 9731)',"b'closes #9731\r\n \r\nThe main issue is when the destination and source arrays have different lengths, ``np.putmask`` doesn\'t behave like ``arr[mask] = values``:\r\n\r\n""Sets ``a.flat[n] = values[n]`` for each n where ``mask.flat[n]==True``""\r\n\r\nWe have to use ``np.place`` instead. A secondary issue is that ``np.place`` doesn\'t automatically convert an integer to a ``datetime64`` like ``np.putmask`` does (I created a numpy issue for this), so we need an additional check for that case. The rest of the commit is just cleaning up ``_maybe_upcast_putmask``, which had some parameters that were never used, and a confusing docstring.'"
9736,64550576,evanpw,jreback,2015-03-26 15:06:50,2015-08-18 11:20:29,2015-08-18 11:20:29,closed,,0.17.0,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9736,b'DataFrame.where does not respect axis parameter for n x n frames',"b""```\r\nIn [3]: df = pd.DataFrame(np.random.randn(5, 5))\r\n\r\nIn [4]: s = pd.Series(range(5))\r\n\r\nIn [5]: df.where(df.isnull(), s, axis=0)\r\nOut[5]:\r\n   0  1  2  3  4\r\n0  0  1  2  3  4\r\n1  0  1  2  3  4\r\n2  0  1  2  3  4\r\n3  0  1  2  3  4\r\n4  0  1  2  3  4\r\n\r\nIn [6]: df.where(df.isnull(), s, axis=1)\r\nOut[6]:\r\n   0  1  2  3  4\r\n0  0  1  2  3  4\r\n1  0  1  2  3  4\r\n2  0  1  2  3  4\r\n3  0  1  2  3  4\r\n4  0  1  2  3  4\r\n```\r\n\r\nThe right axis _is_ used when aligning indices, but it's not used when broadcasting the aligned Series. You also get the right result if the DataFrame is not square, because core.internals.where will transpose if necessary to the get the dimensions to fit."""
9731,64366318,skellys,jreback,2015-03-25 20:34:47,2015-04-02 21:25:39,2015-04-02 21:25:39,closed,,0.16.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9731,b'[] (__getitem__) boolean indexing assignment bug with nans',"b""See repro below:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ntemp = pd.Series(np.random.randn(10))\r\ntemp[3:6] = np.nan\r\ntemp[8] = np.nan\r\nnan_index = np.isnan(temp)\r\n\r\n# this works\r\ntemp1 = temp.copy()\r\ntemp1[nan_index] = [99, 99, 99, 99]\r\ntemp1[nan_index]\r\n\r\n3    99\r\n4    99\r\n5    99\r\n8    99\r\ndtype: float64\r\n\r\n# this doesn't - values look like they're being assigned in a different order?\r\ntemp2 = temp.copy()\r\ntemp2[nan_index] = [99, 99, 99, np.nan]\r\n\r\n3   NaN\r\n4    99\r\n5    99\r\n8    99\r\ndtype: float64\r\n\r\n# ... but it works properly when using .loc\r\ntemp2 = temp.copy()\r\ntemp2.loc[nan_index] = [99, 99, 99, np.nan]\r\n\r\n3    99\r\n4    99\r\n5    99\r\n8   NaN\r\ndtype: float64\r\n\r\n```\r\n\r\noutput of show_versions():\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.0\r\nnose: 1.3.4\r\nCython: 0.21.2\r\nnumpy: 1.9.2\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 3.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.2.1\r\ndateutil: 2.4.1\r\npytz: 2015.2\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.0\r\nopenpyxl: 2.0.2\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.6.6\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n```"""
9726,64252679,ghost,jreback,2015-03-25 11:58:31,2015-12-11 15:21:16,2015-12-11 15:21:10,closed,,,2,Bug;Duplicate;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9726,"b'""c""-engine read_table sigsegv when chunksize close to multiple of file length?'","b'I have the following script:\r\n\r\n```\r\ntable_generator = pd.io.parsers.read_table(chromosome_file, sep=""\\t"", engine=""c"",\r\n                                               chunksize=50000, names=[""Start"", ""End"", ""Strand""],\r\n                                               usecols=[1, 2, 5])\r\n\r\n\r\nfor chunk in table_generator:\r\n    print chunk\r\n    print chromosome_file\r\n```\r\n\r\nI get an error when the file length is close to a multiple of the chunksize:\r\n\r\n```\r\nfish: \'python exorcised.py\' terminated by signal SIGSEGV (Address boundary error)\r\n```\r\n\r\nChanging the chunksize or the lengths of the files allows me to avoid the error.\r\n\r\nThe error seems to happen before or when the tiny leftover chunk is read (since the previous `print` was displayed.)\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.22\r\nnumpy: 1.9.2\r\nscipy: 0.14.1\r\nstatsmodels: 0.6.1\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.3.0\r\ndateutil: 2.4.1\r\npytz: 2015.2\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.2\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\nThe files read look like the following:\r\n\r\n```\r\nchrY\t59052460\t59052659\tKeratinocyte_H3K27me3_03\t1\t+\r\nchrY\t246219\t246418\tMelanocyte_H3K27me3_02\t0\t-\r\nchrY\t9978094\t9978293\tMelanocyte_H3K27me3_03\t1\t+\r\nchrY\t2472778\t2472977\tFibroblast_H3K27me3_01\t0\t+\r\nchrY\t13266277\t13266476\tKeratinocyte_H3K27me3_03\t1\t-\r\nchrY\t699049\t699248\tFibroblast_H3K27me3_02\t0\t-\r\nchrY\t23986624\t23986823\tMelanocyte_H3K27me3_01\t0\t-\r\nchrY\t562143\t562342\tFibroblast_H3K27me3_03\t1\t+\r\nchrY\t23026706\t23026905\tMelanocyte_H3K27me3_01\t0\t-\r\nchrY\t17509636\t17509835\tMelanocyte_H3K27me3_03\t1\t+\r\n```\r\n\r\nWith special symbols showing:\r\n\r\n```\r\nchrY^I10500^I10699^IMelanocyte_H3K27me3_01^I0^I+$\r\n```\r\n'"
9713,63822502,cgrin,jreback,2015-03-23 20:19:33,2015-05-01 16:45:21,2015-05-01 16:45:21,closed,,0.16.1,10,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/9713,b'On demand imports failing in pandas.io.gbq',"b""After upgrading to 0.16.0 BigQuery functionality is not working. Upon invoking pd.read_gbq(), the _test_imports() function is running, which in turn runs _importers(). Everything is all well and good up to here, but per pdb, once _importers() returns, the imported modules are not accessible and read_gbq() throws ```NameError: global name 'OAuth2WebServerFlow' is not defined```.\r\n"""
9710,63724227,aarimond,jreback,2015-03-23 13:34:23,2015-04-28 01:02:30,2015-04-28 01:02:30,closed,,0.16.1,4,Bug;IO CSV;Usage Question,https://api.github.com/repos/pydata/pandas/issues/9710,b'read_csv skips rows with value 0 if having initial space',"b""Hi,\r\n\r\nI have something like the following csv file:\r\n\r\n```\r\nMyColumn\r\n   0\r\n   1\r\n   0\r\n   1\r\n```\r\nNote the initial space in each row.\r\nUpgrading from 0.14.1 to 0.16 I recognized that read_csv started throwing away the 0 rows\r\n\r\n```\r\nIn [28]: import pandas\r\nIn [29]: from StringIO import StringIO\r\nIn [30]: data = 'MyColumn\\n   0\\n   1\\n   0\\n   1'\r\nIn [31]: pandas.read_csv(StringIO(data))\r\nOut[31]:\r\n   MyColumn\r\n0         1\r\n1         1\r\n```\r\nskipinitialspace=True did not help:\r\n\r\n```\r\nIn [32]: pandas.read_csv(StringIO(data), skipinitialspace=True)\r\nOut[32]:\r\n   MyColumn\r\n0         1\r\n1         1\r\n```\r\nhowever, skip_blank_lines=False would help:\r\n```\r\nIn [34]: pandas.read_csv(StringIO(data), skip_blank_lines=False)\r\nOut[34]:\r\n   MyColumn\r\n0         0\r\n1         1\r\n2         0\r\n3         1\r\n```\r\nNot sure if this is working as intended.\r\n\r\nCheers,\r\nAlex\r\n\r\n\r\nPS:\r\nHaving a second columns works as expected:\r\n```\r\nIn [40]: data = 'MyColumn,SecondColumn\\n   0, 2\\n   1, 3\\n   0, 0\\n   1, 4'\r\nIn [41]: pandas.read_csv(StringIO(data))\r\nOut[41]:\r\n   MyColumn  SecondColumn\r\n0         0             3\r\n1         1             4\r\n2         0             0\r\n3         1             6\r\n```\r\n\r\nUPDATE:\r\nMade code more reproducable.\r\n"""
9703,63595386,mcwitt,jreback,2015-03-23 00:51:44,2015-03-23 01:26:44,2015-03-23 01:26:31,closed,,,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9703,b'groupby with multiindex behaves differently for series and single-column dataframe',"b""This behavior seems strange to me. Starting with a multiindex dataframe with unbalanced levels:\r\n\r\n```python\r\nIn [3]: pd.__version__\r\nOut[3]: '0.15.2'\r\n\r\nIn [4]: df = pd.read_csv(StringIO('''\r\nA,B,x\r\nthree,a,1\r\nthree,b,2\r\nthree,c,3\r\ntwo,a,4\r\ntwo,b,5\r\none,a,6'''), index_col=list('AB'))\r\n\r\nIn [5]: df\r\nOut[5]: \r\n         x\r\nA     B   \r\nthree a  1\r\n      b  2\r\n      c  3\r\ntwo   a  4\r\n      b  5\r\none   a  6\r\n```\r\n\r\nGroupby aggregations on this dataframe seem to revert the index to the cross product of the levels, potentially leaving many NAs in the result:\r\n\r\n```python\r\nIn [6]: df.groupby(level=[0,1]).mean()\r\nOut[6]: \r\n          x\r\nA     B    \r\none   a   6\r\n      b NaN\r\n      c NaN\r\nthree a   1\r\n      b   2\r\n      c   3\r\ntwo   a   4\r\n      b   5\r\n      c NaN\r\n```\r\n\r\nBut with the series this doesn't occur (no NAs in the result):\r\n\r\n```python\r\nIn [7]: df.x.groupby(level=[0,1]).mean()\r\nOut[7]: \r\nA      B\r\none    a    6\r\nthree  a    1\r\n       b    2\r\n       c    3\r\ntwo    a    4\r\n       b    5\r\nName: x, dtype: int64\r\n```\r\n\r\nI'm wondering if this is a bug or intended behavior? I haven't been able to find any mention of different behavior in the docs."""
9700,63554841,dsm054,jreback,2015-03-22 17:17:25,2015-04-29 10:31:53,2015-04-29 10:31:53,closed,,0.16.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9700,b'BUG: groupby.transform confused about index',"b'\r\nUnder some circumstances, `transform` is getting confused about what the output index should be:\r\n\r\n```\r\n>>> df = pd.DataFrame({""c1"": [1], ""c2"": [2]})\r\n>>> df\r\n   c1  c2\r\n0   1   2\r\n>>> df.groupby(\'c1\').transform(sum)\r\n   c2\r\n0 NaN\r\n1   2\r\n```\r\n\r\nI haven\'t chased down why, but since we have\r\n\r\n```\r\n>>> df.groupby(\'c1\').transform(lambda x: sum(x))\r\n   c2\r\n0   2\r\n```\r\n\r\nit must be another fast-path problem, like #9697.'"
9699,63553600,dsm054,jreback,2015-03-22 17:05:19,2015-03-23 10:55:42,2015-03-23 10:55:36,closed,,0.16.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9699,"b'BUG: ensure we use group sizes, not group counts, in transform (GH9697)'","b""Switch count() to size(), so that when we build the expanded values we're using the size of the groups and not simply the number of non-null values they have.  Fixes #9697."""
9697,63471029,nickeubank,jreback,2015-03-22 00:31:06,2015-05-18 15:20:38,2015-03-23 10:55:36,closed,,0.16.1,10,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9697,b'BUG: groupby.transform length mismatch under certain specifications',"b'Replicating Example (pandas 15.2 and 0.16.0rc1-32-g5a417ec):\r\n\r\n    import numpy as np\r\n    df = pd.DataFrame({\'col1\':[1,1,2,2], \'col2\':[1,2,3,np.nan]})\r\n    \r\n    # Works fine\r\n    df.groupby(\'col1\').transform(sum)[\'col2\']\r\n    \r\n    # Throws error\r\n    df.groupby(\'col1\')[\'col2\'].transform(sum)\r\n\r\nError:\r\n    Traceback (most recent call last):\r\n    \r\n      File ""<ipython-input-7-f969e26273d4>"", line 8, in <module>\r\n        df.groupby(\'col1\')[\'col2\'].transform(sum)\r\n    \r\n      File ""/Users/Nick/GitHub/pandas/pandas/core/groupby.py"", line 2418, in transform\r\n        return self._transform_fast(cyfunc)\r\n    \r\n      File ""/Users/Nick/GitHub/pandas/pandas/core/groupby.py"", line 2459, in _transform_fast\r\n        return self._set_result_index_ordered(Series(values))\r\n    \r\n      File ""/Users/Nick/GitHub/pandas/pandas/core/groupby.py"", line 497, in _set_result_index_ordered\r\n        result.index = self.obj.index\r\n    \r\n      File ""/Users/Nick/GitHub/pandas/pandas/core/generic.py"", line 2061, in __setattr__\r\n        return object.__setattr__(self, name, value)\r\n    \r\n      File ""pandas/src/properties.pyx"", line 65, in pandas.lib.AxisProperty.__set__ (pandas/lib.c:41404)\r\n        obj._set_axis(self.axis, value)\r\n    \r\n      File ""/Users/Nick/GitHub/pandas/pandas/core/series.py"", line 268, in _set_axis\r\n        self._data.set_axis(axis, labels)\r\n    \r\n      File ""/Users/Nick/GitHub/pandas/pandas/core/internals.py"", line 2211, in set_axis\r\n        \'new values have %d elements\' % (old_len, new_len))\r\n    \r\n    ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements'"
9695,63404143,TomAugspurger,jreback,2015-03-21 13:40:01,2015-08-18 12:45:09,2015-05-09 16:16:26,closed,,,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/9695,b'BUG: fix for interp with axis=1 and inplace',"b""Closes https://github.com/pydata/pandas/issues/9687\r\n\r\nThere's still some weird issues with dtypes. In my test case there's a DataFrame with dtypes [int, float, int]. After the interpolation they should be downcasts to [int, int, int] when the missing value is filled. But along the way a transpose changes things to `[float, float, float]` before filling. And then (I think) `self._update_inplace` doesn't change dtypes, so the actual result is `[float, float, float]`."""
9694,63343481,samzhang111,jreback,2015-03-21 01:39:57,2015-03-25 23:03:40,2015-03-25 23:03:40,closed,,0.16.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/9694,b'BUG: datetime/timedelta Series quantile() call',b'Changes to be committed:\r\n\tmodified:   pandas/core/series.py\r\n\tmodified:   pandas/tests/test_series.py\r\n\r\nFixes global reference to iNaT (should be tslib.iNaT) in series._maybe_box.\r\nAdds test `test_datetime_timedelta_quantiles` to check for proper return value\r\nin test_series.py.\r\n\r\nIssue #9675'
9683,63011434,JonasAbernot,jreback,2015-03-19 15:10:08,2015-04-28 00:53:57,2015-04-28 00:53:52,closed,,0.16.1,3,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9683,b'Bug: Access unknown attribute of timedeltaindexed series used to raise ValueError',b'closes #9680\r\n\r\nAnd now it raises AttributeError.'
9681,62964608,jreback,jreback,2015-03-19 11:53:16,2015-03-19 20:39:15,2015-03-19 20:39:15,closed,,0.16.0,7,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/9681,"b'BUG: workaround PyTables 319, but not setting expected rows (GH8265, GH9676)'",b'closes #8265\r\ncloses #9676 \r\n'
9680,62944418,JonasAbernot,jreback,2015-03-19 10:17:38,2015-04-28 00:54:11,2015-04-28 00:54:11,closed,,0.16.1,4,API Design;Bug;Difficulty Novice;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9680,b'TimedeltaIndexed Series raises ValueError instead of AttributeError',"b""```python\r\nts = pd.Series(np.random.normal(size=10),index=pd.timedelta_range(start=0,periods=10,freq='1s'))\r\nts.foo\r\n```\r\nraises\r\n```\r\nValueError: cannot create timedelta string converter for [foo]\r\n```\r\nShouldn't it raise \r\n```\r\nAttributeError: 'Series' object has no attribute 'foo'\r\n```\r\n?\r\n```\r\npd.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-46-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\n\r\npandas: 0.16.0rc1-28-gd7d868f\r\nnose: 1.3.1\r\nCython: 0.20.1post0\r\nnumpy: 1.8.2\r\nscipy: 0.13.3\r\nstatsmodels: None\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: None\r\ndateutil: 2.4.1\r\npytz: 2013b\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n```"""
9676,62774931,alexfields,jreback,2015-03-18 19:09:37,2015-05-07 00:20:21,2015-05-07 00:20:21,closed,,0.16.1,21,Bug;Docs;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/9676,"b'BUG: entries missing when reading from pytables hdf store using ""where"" statement'","b'When I select from a HDF store using a ""where"" string (locating entries in which one field matches a particular string value), the function returns fewer rows than when I load the entire dataframe into memory and then match on that field. Below is some code that reproduces the problem; unfortunately, I can\'t easily provide the code that generates the source HDF store, but I\'m happy to provide the kept_tids_20150310.h5 file if it would help. There are no nan values in the dataframe.\r\n\r\nRunning ptrepack on the dataframe solves the problem, but I don\'t believe this should happen in the first place.\r\n\r\nI am using pandas 0.15.2 but have not tried 0.16.0.\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-46-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.19.2\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.5.0\r\nIPython: 2.4.0\r\nsphinx: 1.2.3\r\npatsy: 0.2.1\r\ndateutil: 2.4.1\r\npytz: 2014.10\r\nbottleneck: 0.8.0\r\ntables: 3.1.0\r\nnumexpr: 2.3\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: 0.7.2\r\nxlsxwriter: None\r\nlxml: 2.3.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.7.2\r\napiclient: None\r\nrpy2: 2.4.2\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n>>> kept_tids = pd.read_hdf(\'kept_tids_20150310.h5\', \'kept_tids\', mode=\'r\')\r\n>>> kept_tids.to_hdf(\'kept_tids_20150310_resave.h5\', \'kept_tids\', mode=\'w\', format=\'t\', data_columns=True)\r\n>>> chroms = kept_tids[\'chrom\'].drop_duplicates().order().tolist()\r\n>>> print chroms\r\n[\'chr1\', \'chr10\', \'chr11\', \'chr12\', \'chr13\', \'chr14\', \'chr15\', \'chr16\', \'chr17\', \'chr18\', \'chr19\', \'chr2\', \'chr20\', \'chr21\', \'chr22\', \'chr3\', \'chr4\', \'chr5\', \'chr6\', \'chr7\', \'chr8\', \'chr9\', \'chrM\', \'chrX\', \'chrY\']\r\n>>> len(kept_tids)\r\n202836\r\n>>> sum(len(pd.read_hdf(\'kept_tids_20150310_resave.h5\', \'kept_tids\', mode=\'r\', where=""chrom == \'%s\'""%x)) for x in chroms)\r\n193757\r\n>>> (kept_tids[\'chrom\']==\'chr16\').sum()\r\n10157\r\n>>> len(pd.read_hdf(\'kept_tids_20150310_resave.h5\', \'kept_tids\', mode=\'r\', where=""chrom == \'chr16\'""))\r\n6278\r\n```'"
9675,62745414,samzhang111,jreback,2015-03-18 17:16:56,2015-03-25 23:03:52,2015-03-25 23:03:52,closed,,0.16.1,3,Bug;Difficulty Novice,https://api.github.com/repos/pydata/pandas/issues/9675,"b""global name 'iNaT' is not defined""","b""Trying to perform this operation on a groupby object called clusters:\r\n\r\n`clusters.agg({'diffs': lambda x: x.quantile(.5)})`\r\n\r\nwhere diffs is a column of Timedelta objects.\r\n\r\nGetting the error:\r\n\r\n```\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\r\n   2305                 return self._python_agg_general(func_or_funcs, *args, **kwargs)\r\n   2306             except Exception:\r\n-> 2307                 result = self._aggregate_named(func_or_funcs, *args, **kwargs)\r\n   2308 \r\n   2309             index = Index(sorted(result), name=self.grouper.names[0])\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in _aggregate_named(self, func, *args, **kwargs)\r\n   2392         for name, group in self:\r\n   2393             group.name = name\r\n-> 2394             output = func(group, *args, **kwargs)\r\n   2395             if isinstance(output, (Series, Index, np.ndarray)):\r\n   2396                 raise Exception('Must produce aggregated value')\r\n\r\n<ipython-input-174-5be8bb847fd6> in <lambda>(x)\r\n----> 1 clusters.agg({'diffs': lambda x: x.quantile(.5)})\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc in quantile(self, q)\r\n   1264                 return _quantile(values, qs*100)\r\n   1265 \r\n-> 1266         return self._maybe_box(lambda values: multi(values, q), dropna=True)\r\n   1267 \r\n   1268     def ptp(self, axis=None, out=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc in _maybe_box(self, func, dropna)\r\n   2105 \r\n   2106             if len(values) == 0:\r\n-> 2107                 return boxer(iNaT)\r\n   2108 \r\n   2109             values = values.view('i8')\r\n\r\nNameError: global name 'iNaT' is not defined\r\n```\r\n\r\nLooks like other references to iNaT are all tslib.iNaT.\r\n\r\nhttps://github.com/pydata/pandas/blob/master/pandas/core/series.py#L2098"""
9672,62400731,JonasAbernot,jreback,2015-03-17 13:41:37,2015-04-04 18:40:35,2015-04-04 18:40:35,closed,,0.16.1,2,Bug;IO HDF5;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9672,b'BUG: support TimedeltaIndex serialization in fixed stores (GH9635)',"b'closes #9635\r\n\r\nHop, revival of PR9662, after some bad git moves.'"
9670,62345524,musically-ut,jreback,2015-03-17 09:35:21,2015-03-17 10:25:44,2015-03-17 10:25:24,closed,,,1,Bug;Duplicate;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9670,b'Numerically unstable mean calculation for Timedeltas.',"b""I am not sure whether I should report this here or on `numpy`. But this is what lead me to the problem:\r\n\r\n     In [11]: dAllTags.describe()\r\n    Out [11]:\r\n                         finalPeriod\r\n    count                      74501\r\n    mean    -1 days +02:40:08.792662\r\n    std     500 days 06:32:37.640848\r\n    min       2 days 00:51:49.730000\r\n    25%     498 days 19:11:28.576000\r\n    50%     846 days 00:46:56.656000\r\n    75%    1245 days 17:11:58.493000\r\n    max    2224 days 07:03:26.593000\r\n\r\nAll the values are positive (the minimum is `2 days`) but the `mean` calculated is negative. This happens because the underlying type of `np.timedelta64` is `int64` which overflows while calculating the mean.\r\n\r\nNow the issue of numerical stability in `numpy` has had a long history:\r\n\r\n - https://github.com/numpy/numpy/issues/4694\r\n - https://github.com/numpy/numpy/issues/1033\r\n - https://github.com/numpy/numpy/issues/1063\r\n - https://github.com/numpy/numpy/issues/2448\r\n\r\nAnd though some steps have been taken to introduce precision accuracy (e.g. by providing `fsum` and using pairwise summation), there doesn't seem to be a consensus for using a numerically stable method for `mean`. \r\n\r\nI was wondering if something could be done on the Pandas level to resolve this issue.\r\n\r\n----\r\n\r\nCurrently, I am working around the issue by using the rather elaborate scheme:\r\n\r\n    df.finalPeriod.view(int).astype(float).mean()\r\n\r\nsince `timedelta64` cannot be directly converted to `float64`. Is there a better/more intuitive way to do this?"""
9669,62286022,diehl,diehl,2015-03-17 04:46:50,2015-03-17 23:55:36,2015-03-17 23:19:49,closed,,,5,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9669,b'Type inference problem with read_csv',"b""I have a large CSV file that contains a single column of integers. When loading the CSV file with read_csv, nearly 2/3rds of the approximately 1.5 million values are loaded as ints while the remaining values are loaded as strings. I see no obvious problem with the file that would lead to this behavior.\r\nThe file I used is available here: https://drive.google.com/file/d/0ByZvgdTf0yfAT2dPdHRvc2hLVkU/view?usp=sharing\r\n\r\n```\r\ntest_df = pd.read_csv('test.csv',header=0)\r\nfrom collections import Counter\r\nc = Counter()\r\nels = []\r\nfor el in test_df['SERIALNO'].values:\r\n    c[type(el)] += 1\r\nprint c\r\nCounter({<type 'int'>: 952026, <type 'str'>: 524288})\r\n```\r\n\r\nIt appears that a continguous block of rows were read as strings - a block in the middle of the file. Very curious. \r\n\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.21\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.5.0\r\nIPython: 3.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.1\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.0\r\nopenpyxl: 2.1.0\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.7\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None"""
9662,62055115,JonasAbernot,JonasAbernot,2015-03-16 11:49:28,2015-03-17 12:41:39,2015-03-17 12:41:39,closed,,0.16.1,5,Bug;Dtypes;IO HDF5;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9662,b'BUG: support TimedeltaIndex serialization in fixed stores (GH9635)',"b""closes https://github.com/pydata/pandas/issues/9635\r\n\r\nSmall change to fix issue 9635. I'm a beginner contributor, feel free to crush it out."""
9659,61847612,jreback,jreback,2015-03-15 15:46:51,2015-03-15 19:52:37,2015-03-15 19:52:37,closed,,0.16.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9659,b'API: spurious setting_with_copy warning (GH8730)',"b'closes #8730 \r\n\r\nwas tricky to distinguish this case from others, but I think this is a reasonably robust soln.'"
9647,61135368,evanpw,jreback,2015-03-13 16:55:41,2015-09-19 00:38:04,2015-04-28 11:56:04,closed,,0.16.1,7,Bug,https://api.github.com/repos/pydata/pandas/issues/9647,"b'Allow clip{,_lower,_upper} to use array-like thresholds (GH 6966)'",b'closes #6966 '
9635,60799613,JonasAbernot,jreback,2015-03-12 10:40:11,2015-04-04 18:40:47,2015-04-04 18:40:47,closed,,0.16.1,2,Bug;Dtypes;IO HDF5;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9635,b'Hdf5/PyTables support for TimedeltaIndex',"b""When stored in HDFStore, TimedeltaIndex are converted into Int64Index.\r\n\r\n```python\r\ndf = pd.DataFrame(np.random.normal(size=(10,5)))\r\ndf.index = pd.timedelta_range(start='0s',periods=10,freq='1s')\r\nstore = pd.HDFStore('test.h5')\r\nstore['df'] = df\r\ndf = store['df']\r\nprint df.index\r\n```\r\n```\r\nInt64Index([0, 1000000000, 2000000000, 3000000000, 4000000000, 5000000000, 6000000000, 7000000000, 8000000000, 9000000000], dtype='int64')\r\n```\r\nAlthough the backward conversion is easy (pd.TimedeltaIndex(df.index)) , it's surprising.\r\n\r\n```python\r\npd.show_versions()\r\n```\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-46-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\n\r\npandas: 0.15.2-269-ge3d1b0e\r\nnose: 1.3.1\r\nCython: 0.20.1post0\r\nnumpy: 1.8.2\r\nscipy: 0.13.3\r\nstatsmodels: None\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: None\r\ndateutil: 2.4.1\r\npytz: 2013b\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n\r\n```\r\n\r\nI would have been happy to help and add this myself, but I really have no idea were is this hiding. I don't even know if I should have looked toward PyTables instead of here.\r\n\r\nThanks !\r\n"""
9623,60437112,rockg,jreback,2015-03-10 01:24:57,2015-03-11 14:17:14,2015-03-11 12:10:07,closed,,0.16.0,17,Bug;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/9623,b'Fixes to resample over DST boundaries',"b""closes #5172 \r\ncloses #8744\r\ncloses #8653 \r\ncloses #9173\r\ncloses #9468\r\ncloses #8794\r\n\r\nfixes all issues but one attached to #5172.  There were several issues as mentioned by my comments in the master list.  They are:\r\n\r\n1. Offset classes were not working properly over DST boundaries.  I added flags on the class to handle specially.\r\n2. `normalize_date` does not work with DST change dates as the tz offset is not adjusted. `normalize` is added to Timestamp as discussed in #8794 (I don't like how this was done, but it seemed the easiest rather than duplicating lots of code in `tslib.date_normalize`.\r\n3. `_adjust_dates_anchored` was not localizing properly once times were adjusted. """
9618,60326756,jmavila,jreback,2015-03-09 10:55:31,2015-07-18 13:30:43,2015-07-18 13:30:43,closed,,0.17.0,4,Bug;Difficulty Advanced;Effort Low;Msgpack,https://api.github.com/repos/pydata/pandas/issues/9618,b'msgpack unpack dataframe error: InvalidIndexError',"b'This happen with some Pandas dataframes with indexes. \r\n\r\n```python\r\nfrom pandas import read_msgpack\r\nmsg = df.to_msgpack()\r\nread_msgpack(msg) #==> exception!\r\n```\r\n\r\nFull Traceback (most recent call last):\r\n\r\nFile ""/srv/python/venv/local/lib/python2.7/site-packages/pandas/io/packers.py"", line 163, in read_msgpack\r\nreturn read(fh)\r\nFile ""/srv/python/venv/local/lib/python2.7/site-packages/pandas/io/packers.py"", line 141, in read\r\nl = list(unpack(fh))\r\nFile ""pandas/msgpack.pyx"", line 662, in pandas.msgpack.Unpacker.__next__ (pandas/msgpack.cpp:8303)\r\nFile ""pandas/msgpack.pyx"", line 591, in pandas.msgpack.Unpacker._unpack (pandas/msgpack.cpp:7328)\r\nFile ""/srv/python/venv/local/lib/python2.7/site-packages/pandas/io/packers.py"", line 490, in decode\r\nblocks = [create_block(b) for b in obj[\'blocks\']]\r\nFile ""/srv/python/venv/local/lib/python2.7/site-packages/pandas/io/packers.py"", line 488, in create_block\r\nplacement=axes[0].get_indexer(b[\'items\']))\r\nFile ""/srv/python/venv/local/lib/python2.7/site-packages/pandas/core/index.py"", line 1488, in get_indexer\r\nraise InvalidIndexError(\'Reindexing only valid with uniquely\'\r\nInvalidIndexError: Reindexing only valid with uniquely valued Index objects'"
9603,60126706,tmoroz,jreback,2015-03-06 16:45:01,2015-04-29 23:48:14,2015-04-29 23:48:14,closed,,0.16.1,2,Bug;Categorical;Difficulty Intermediate;Effort Low;Groupby,https://api.github.com/repos/pydata/pandas/issues/9603,b'unexpected result with groupby apply on categorical data',"b""```python\r\ndf = pandas.DataFrame({'a': [1, 0, 0, 0]})\r\ndf.groupby(pandas.cut(df.a, [0, 1, 2, 3, 4])).apply(lambda x: len(x))\r\n\r\nOut[3]:\r\na\r\n(0, 1]    1\r\n(1, 2]    0\r\n(2, 3]    0\r\n(3, 4]    3\r\ndtype: int64\r\n```\r\nfinal group length should be 0 not 3\r\n"""
9597,60040520,jreback,jreback,2015-03-06 00:32:55,2015-03-06 03:18:11,2015-03-06 03:18:10,closed,,0.16.0,0,Bug;Categorical;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9597,b'BUG: Regression in merging Categorical and object dtypes (GH9426)',b'closes #9426 '
9596,59970070,Sereger13,jreback,2015-03-05 15:47:10,2015-04-20 09:13:32,2015-04-18 20:42:41,closed,,0.16.1,9,Bug;Difficulty Intermediate;Effort Low;Error Reporting;Indexing,https://api.github.com/repos/pydata/pandas/issues/9596,b'Error when updating dataframe with empty filter',"b""```python\r\ndf = pd.DataFrame({'a': ['1', '2', '3'], \r\n               'b': ['11', '22', '33'], \r\n               'c': ['111', '222', '333']})\r\ndf.loc[df.b.isnull(), 'a'] = df.b\r\n```\r\nThe filtering condition is always false (b is never null) and the above code produces this error:\r\n`Array is not broadcastable to correct shape`.\r\n\r\nIf, however, I change the type for column 'c' - which is not even mentioned in the assignment expression (!) - from str to int,\r\n```python\r\ndf = pd.DataFrame({'a': ['1', '2', '3'], \r\n               'b': ['11', '22', '33'], \r\n               'c': [111, 222, 333]}) # Type changed to int\r\ndf.loc[df.b.isnull(), 'a'] = df.b\r\n```\r\nNo errors are produced.\r\n\r\nWould be nice to have consistent behaviour and not have any errors - could not find anything in the documentation saying that applying .loc on empty filter is considered illegal?\r\n"""
9589,59858439,bashtage,jreback,2015-03-04 20:40:53,2015-07-14 12:47:25,2015-07-14 12:47:25,closed,,0.17.0,32,API Design;Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9589,b'BUG: convert_objects(convert_numeric=True) fails with all strings',"b""```\r\ns=pd.Series(['a','a','a'])\r\n\r\ns.convert_objects(convert_numeric=True)\r\nOut[78]: \r\n0    a\r\n1    a\r\n2    a\r\ndtype: object\r\n\r\ns[0]=1.0\r\n\r\ns\r\nOut[80]: \r\n0    1\r\n1    a\r\n2    a\r\ndtype: object\r\n\r\ns.convert_objects(convert_numeric=True)\r\nOut[81]: \r\n0     1\r\n1   NaN\r\n2   NaN\r\ndtype: float64\r\n```\r\n\r\nHaving a single number changes behavior.  Makes `convert_objects` unreliable when the data type must be numeric."""
9583,59726498,cottrell,jreback,2015-03-03 23:44:05,2015-03-07 12:43:55,2015-03-05 23:21:12,closed,,0.16.0,2,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/9583,b'Test added and patch to fix python-version-dependent issues when len ro...',"b'...w/col_levels is 1.\r\n\r\nThe docs for sparse to_coo methods failed to build. There was some case (row_levels len 1) that failed in python 2.7 only that I failed to test (and I have been building docs in python 3). Have added test and patched. Also needed to expand the interator (list(map(... ) in the ""# squish"" line as there was some tupleizing differences between python 3 and 2.\r\n\r\nPerhaps there is a better way to avoid these issues? Waiting for Travis.'"
9578,59664298,gpoulin,jreback,2015-03-03 16:13:23,2015-08-31 12:13:27,2015-08-31 12:13:27,closed,,0.17.0,23,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/9578,b'BUG: Raise TypeError when serializing 0d ndarray',"b""Previously pandas ujson Segfault. see #9576 \r\n\r\nThis is an initial fix. I'll try to make sens out of ujson and see if 0d array can be handle in a better way. However, At the moment raising an exception is still better than a Segfault."""
9576,59598319,gpoulin,jreback,2015-03-03 05:50:07,2015-08-31 12:13:52,2015-08-31 12:13:52,closed,,0.17.0,1,Bug;IO JSON,https://api.github.com/repos/pydata/pandas/issues/9576,b'Segfault when json serializing a 0d `numpy.array`',"b'Pandas json serializer Segfault when trying to serialize a 0d `numpy.array`. Ideally, it should able to serialize this structure as a scalar or at least raise a `TypeError`.\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\npd.json.dumps(np.array(1)) # Segfault\r\npd.Series([np.array(x) for x in range(5)]).to_json() # Segfault\r\n```\r\n\r\npandas version\r\n(also tested with python2.7/pandas0.15.2/gentoo and python3.3/pandas0.15.2/gentoo and python3.4/pandas0.15.2/Ubuntu/anaconda)\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-gentoo\r\nmachine: x86_64\r\nprocessor: Intel(R) Core(TM) i7-3632QM CPU @ 2.20GHz\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_CA.utf8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.22\r\nnumpy: 1.9.1\r\nscipy: 0.15.0\r\nstatsmodels: 0.6.1\r\nIPython: 2.4.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.4.0\r\npytz: 2014.10\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.3\r\nopenpyxl: 2.1.0\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.6\r\nlxml: 3.4.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: 2.5.6\r\nsqlalchemy: 0.9.8\r\npymysql: 0.6.3.None\r\npsycopg2: 2.6 (dt dec pq3 ext lo64)\r\n```'"
9574,59590669,schmohlio,TomAugspurger,2015-03-03 03:39:15,2015-05-01 16:06:11,2015-03-31 01:29:08,closed,,0.16.1,23,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/9574,b'fixing pandas.DataFrame.plot(): labels do not appear in legend and label kwd',"b""Closes https://github.com/pydata/pandas/issues/9542, #8905\r\n\r\nThe following behavior has been tested: \r\n\r\n* `df.plot(y='sin(x)')` -> gives a legend with label 'None' -> this should give no legend instead (as it plots one series, and then we don't automatically add a legend, see behaviour of df['sin(x)'].plot())\r\n* `df.plot(y='sin(x)', legend=True)` -> gives a legend with label 'None' -> this should of course give a legend with label 'sin(x)' (behaviour as df['sin(x)'].plot(legend=True))\r\n* `df.plot(y='sin(x)', label='something else', legend=True)` -> gives a legend with label 'None' -> should be a legend with label 'something else', as we want that the label kwarg overwrites the column name.\r\n\r\nbased on following data:\r\n`x=np.linspace(-10,10,201)`\r\n`y,z=np.sin(x),np.cos(x)`\r\n`x,y,z=pd.Series(x),pd.Series(y),pd.Series(z)`\r\n`df=pd.concat([x,y,z],axis=1)`\r\n`df.columns=['x','sin(x)','cos(x)']`\r\n`df=df.set_index('x')`"""
9573,59578391,ruoyu0088,jreback,2015-03-03 00:53:08,2015-06-15 10:44:29,2015-06-15 10:44:29,closed,,0.17.0,3,Bug;Categorical;Difficulty Advanced;Effort Medium;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9573,b'DataFrame.apply() with function that return category series',"b'```\r\nimport pandas as pd\r\ndf = pd.DataFrame({""c0"":[""A"",""A"",""B"",""B""], ""c1"":[""C"",""C"",""D"",""D""]})\r\ndf.apply(lambda s:s.astype(""category""))\r\n```\r\nthe resut is a series with series as element, not a dataframe:\r\n\r\n```\r\nc0    [A, A, B, B]\r\nCategories (2, object): [A < B]\r\nc1    [C, C, D, D]\r\nCategories (2, object): [C < D]\r\ndtype: object\r\n```\r\n\r\n```\r\nHere is the output of `show_vershons()`:\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: x86\r\nprocessor: x86 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.2.dev\r\nnose: 1.3.4\r\nCython: 0.21.2\r\nnumpy: 1.9.1\r\nscipy: 0.15.0\r\nstatsmodels: 0.6.1\r\nIPython: 3.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.3\r\npytz: 2014.10\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.6.5\r\nlxml: 3.4.1\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: 2.5.4\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n```'"
9566,59358177,jreback,jreback,2015-02-28 21:01:15,2015-03-13 17:47:35,2015-03-04 20:46:25,closed,,0.16.0,21,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9566,b'API: consistency with .ix and .loc for getitem operations (GH8613)',"b""closes #8613 \r\n\r\n```\r\nIn [1]:      df = DataFrame(np.random.randn(5,4), columns=list('ABCD'), index=date_range('20130101',periods=5))\r\n\r\nIn [2]:      df\r\nOut[2]: \r\n                   A         B         C         D\r\n2013-01-01 -1.380065  1.221596  0.279703 -0.119258\r\n2013-01-02  1.684202 -0.202251 -0.961145 -1.595015\r\n2013-01-03 -0.623634 -2.377577 -3.024554 -0.298809\r\n2013-01-04 -1.251699  0.456356  1.257002  1.465878\r\n2013-01-05  0.687818 -2.125079  1.454911  1.914207\r\n\r\nIn [5]:      s = Series(range(5),[-2,-1,1,2,3])\r\n\r\nIn [6]:      s\r\nOut[6]: \r\n-2    0\r\n-1    1\r\n 1    2\r\n 2    3\r\n 3    4\r\ndtype: int64\r\n\r\nIn [7]:      df.loc['2013-01-02':'2013-01-10']\r\nOut[7]: \r\n                   A         B         C         D\r\n2013-01-02  1.684202 -0.202251 -0.961145 -1.595015\r\n2013-01-03 -0.623634 -2.377577 -3.024554 -0.298809\r\n2013-01-04 -1.251699  0.456356  1.257002  1.465878\r\n2013-01-05  0.687818 -2.125079  1.454911  1.914207\r\n\r\nIn [9]:      s.loc[-10:3]\r\nOut[9]: \r\n-2    0\r\n-1    1\r\n 1    2\r\n 2    3\r\n 3    4\r\ndtype: int64\r\n```\r\n\r\n```\r\n# this used to be a KeyError\r\nIn [15]: df.loc[2:3]\r\nTypeError: Cannot index datetime64 with <type 'int'> keys\r\n```\r\n\r\nSlicing with float indexers; now works for ix (loc worked before)\r\n```\r\nIn [3]: s.loc[-1.0:2]\r\nOut[3]: \r\n-1    1\r\n 1    2\r\n 2    3\r\ndtype: int64\r\n\r\nIn [4]: s.ix[-1.0:2]\r\nOut[4]: \r\n-1    1\r\n 1    2\r\n 2    3\r\ndtype: int64\r\n\r\n# [] raises\r\nIn [5]: s[-1.0:2]\r\nTypeError: cannot do slice start value indexing on <class 'pandas.core.index.Int64Index'> with these indexers [-1.0] of <type 'float'>\r\n```"""
9565,59282252,SvenWambecq,jreback,2015-02-27 19:28:28,2015-06-09 10:49:31,2015-06-09 10:49:31,closed,,0.16.2,1,Bug;Dtypes;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9565,b'Difference between C and Python parser engine for pandas.read_csv',"b'In the code snippet below, I expect that both the values in the c1 and c2 column both are 4.5. \r\nWhen the Python parser engine is used, this gives me 4 iso 4.5. When using the C parser engine, I get 4.5. \r\n\r\nIs this expected behaviour? \r\n\r\n```python\r\n\r\nimport StringIO\r\nimport pandas\r\n\r\ncsv = """"""\r\nc1,c2\r\n45e-1,45.0e-1\r\n""""""\r\nstream = StringIO.StringIO(csv)\r\npandas.read_csv(stream, engine=\'python\')\r\n\r\n  ```\r\nI used the following versions: (pandas.show_versions)\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.2\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.1\r\npytz: 2014.7\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.3.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n\r\n'"
9560,59162933,dakoner,jreback,2015-02-26 23:30:36,2015-02-27 11:54:15,2015-02-27 11:51:04,closed,,,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9560,b'Difference in groupby behavior between Pandas 0.13.1 and 0.15.2',"b""Hi, I am seeing a difference in behavior on this groupby between  Pandas 0.13.1 and 0.15.2.  Specifically, it's like 0.15.2 is doing a cross join while 0.13.1 isn't.\r\n\r\n```\r\nprint pandas.DataFrame([\r\n  {'a': 1, 'b': 2, 'c': 3},\r\n  {'a': 4, 'b': 5, 'c': 6}, ]).set_index(\r\n    list('ab')).groupby(level=list('ab')).mean()\r\n```\r\n0.13.1 produces:\r\n```\r\n     c\r\na b   \r\n1 2  3\r\n4 5  6\r\n[2 rows x 1 columns]\r\n```\r\nwhile 0.15.2 produces\r\n```\r\n      c\r\na b    \r\n1 2   3\r\n  5 NaN\r\n4 2 NaN\r\n  5   6\r\n```\r\nbasically, the same matrix, but with extra cross NaN entries.\r\n\r\nWe're wondering if this behavior is intentional, or a bug.  It wasn't entirely clear from the set of release notes that the groupby behavior changed so much."""
9555,59041556,dhirschfeld,jreback,2015-02-26 09:11:20,2015-04-14 17:19:27,2015-04-14 16:40:34,closed,,0.16.1,7,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9555,b'Moved caching in AbstractHolidayCalendar to the instance level',b'Closes #9552'
9554,58986087,coroa,jreback,2015-02-25 23:12:59,2015-02-25 23:46:23,2015-02-25 23:46:14,closed,,0.16.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9554,b'Setting a value in a categorical column using boolean indexing fails for large index values',"b'Consider\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(dict(A=np.array(range(0, 200)),\r\n                       B=pd.Categorical([\'a\']*200, categories=[\'a\', \'b\'])))\r\n\r\n# Setting a categorical with boolean indexing works fine for small indices\r\nprint df.loc[df.A == 5]\r\ndf.loc[df.A == 5, \'B\'] = \'b\'\r\nprint df.loc[df.A == 5]\r\n\r\nprint """"\r\n\r\n# ... but fails for large indices\r\nprint df.loc[df.A == 150]\r\ndf.loc[df.A == 150, \'B\'] = \'b\'\r\nprint df.loc[df.A == 150]\r\n```\r\noutputs at my pandas 0.15.1.dev installation (note that the value is correctly updated for A==5, but not for A==150):\r\n```\r\n   A  B\r\n5  5  a\r\n   A  B\r\n5  5  b\r\n\r\n       A  B\r\n150  150  a\r\n       A  B\r\n150  150  a'"
9553,58961400,msdrahcir,jreback,2015-02-25 20:07:30,2015-02-25 23:54:44,2015-02-25 23:54:44,closed,,0.16.0,1,Bug;Missing-data;Period,https://api.github.com/repos/pydata/pandas/issues/9553,b'BUG: isnull of a PeriodIndex object array failing when NaT',"b""Pandas isnull failing for NaT period index\r\n\r\n>s= pd.PeriodIndex([pd.NaT,'2014-01-01'],freq='M')\r\n>pd.isnull(s)\r\n>\r\n>array([False, False], dtype=bool)\r\n\r\n"""
9552,58939087,dhirschfeld,jreback,2015-02-25 17:29:33,2015-04-14 16:40:34,2015-04-14 16:40:34,closed,,0.16.1,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9552,b'Disable class level caching in AbstractHolidayCalendar',"b""By caching at the *class* level it prevents instances from defining different rules - e.g.\r\n```python\r\nimport numpy as np\r\nfrom pandas.tseries.holiday import AbstractHolidayCalendar, Holiday\r\n\r\nclass HolidayCalendar(AbstractHolidayCalendar):\r\n    def __init__(self, rules):\r\n        self.rules = list(np.atleast_1d(rules))\r\n\r\njan1 = HolidayCalendar(Holiday('jan1', year=2015, month=1, day=1))\r\njan2 = HolidayCalendar(Holiday('jan2', year=2015, month=1, day=2))\r\n\r\n\r\njan1.holidays()\r\nOut[2]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2015-01-01]\r\nLength: 1, Freq: None, Timezone: None\r\n\r\njan2.holidays()\r\nOut[3]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2015-01-01]    <--------------------------- uses cached result from *other* instance!\r\nLength: 1, Freq: None, Timezone: None\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nfrom pandas.tseries.holiday import AbstractHolidayCalendar, Holiday\r\n\r\nclass HolidayCalendar(AbstractHolidayCalendar):\r\n    def __init__(self, rules):\r\n        self.rules = list(np.atleast_1d(rules))\r\n\r\njan1 = HolidayCalendar(Holiday('jan1', year=2015, month=1, day=1))\r\njan2 = HolidayCalendar(Holiday('jan2', year=2015, month=1, day=2))\r\n\r\n\r\njan2.holidays()\r\nOut[2]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2015-01-02]\r\nLength: 1, Freq: None, Timezone: None\r\n\r\njan1.holidays()\r\nOut[3]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2015-01-02]    <--------------------------- uses cached result from *other* instance!\r\nLength: 1, Freq: None, Timezone: None\r\n```\r\nhttps://github.com/pydata/pandas/blob/a72d95163b4d268012709255d7a52bbe5c1a7eb6/pandas/tseries/holiday.py#L351-L357"""
9544,58806265,fbrundu,jreback,2015-02-24 21:12:07,2015-06-07 22:54:38,2015-06-07 22:54:38,closed,,0.16.2,9,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/9544,b'ENH + BUG: insert of new values for axis parameter on pandas.DataFrame.quantile',"b'closes #9543\r\n \r\nI added the values ""index"" and ""columns"" for the axis parameter of the method pandas.DataFrame.quantile().\r\nAlso, there was no check on the value inserted for axis. E.g. if 2 or ""foo"" was inserted, no ValueError() was raised.\r\nI added a basic check.\r\n\r\nThanks '"
9543,58772112,fbrundu,jreback,2015-02-24 17:28:51,2015-06-07 22:54:49,2015-06-07 22:54:49,closed,,0.16.2,4,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/9543,b'String value for DataFrame.quantile() axis parameter',"b""Hi all,\r\nI don't know if this could be an enhancement but: why do not use string values such as `'index'` or `'columns'` in the axis parameter for `DataFrame.quantile()` function? Such as in `DataFrame.div()`..\r\nI think that it would be more straightforward to use.\r\nThanks\r\n\r\nFrancesco  """
9542,58767143,JohnNapier,TomAugspurger,2015-02-24 16:54:05,2015-06-01 13:02:44,2015-03-31 11:55:16,closed,,0.16.1,16,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/9542,b'pandas.DataFrame.plot(): Labels do not appear in legend',"b""The following code plots two lines. The column names appear in the legend.\r\n\r\n```python\r\nx=np.linspace(-10,10,201)\r\ny,z=np.sin(x),np.cos(x)\r\nx,y,z=pd.Series(x),pd.Series(y),pd.Series(z)\r\ndf=pd.concat([x,y,z],axis=1)\r\ndf.columns=['x','sin(x)','cos(x)']\r\ndf=df.set_index('x')\r\ndf.plot()\r\nplt.show()\r\nplt.clf();plt.close()\r\n```\r\n\r\n![figure_1](https://cloud.githubusercontent.com/assets/10504477/6354154/a5b15c66-bc1b-11e4-9811-3ad39c033a85.png)\r\n\r\nHowever, the following equivalent code shows **None** as legend, even though the labels are explicitly set.\r\n\r\n```python\r\nax=df.plot(y='sin(x)',label='sin(x)')\r\ndf.plot(y='cos(x)',label='cos(x)',ax=ax)\r\nplt.show()\r\n```\r\n\r\nOf course there are alternative ways to make this work, but it appears to me that passing the labels should suffice. In particular, I don't think their values should be replaced with None, or printed as x-label.\r\n\r\n![figure_2](https://cloud.githubusercontent.com/assets/10504477/6354161/b137068a-bc1b-11e4-96b0-ba4c093d8c67.png)\r\n"""
9527,58274272,KevinLourd,jreback,2015-02-19 21:41:41,2015-02-19 22:50:53,2015-02-19 22:16:02,closed,,,2,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/9527,b'BUG: Resample downsampling return NaN',"b'Pandas resample bugs when trying to resample a time serie with same size splits :\r\n\r\nI have a time serie of **size 10**:\r\n```\r\nrng = pd.date_range(\'20130101\',periods=10,freq=\'T\')\r\nts=pd.Series(np.random.randn(len(rng)), index=rng)\r\n```\r\n`print(ts)`\r\n```\r\n2013-01-01 00:00:00   -1.811999\r\n2013-01-01 00:01:00   -0.890837\r\n2013-01-01 00:02:00   -0.363520\r\n2013-01-01 00:03:00   -0.026245\r\n2013-01-01 00:04:00    1.515072\r\n2013-01-01 00:05:00    0.920129\r\n2013-01-01 00:06:00   -0.125954\r\n2013-01-01 00:07:00    0.588933\r\n2013-01-01 00:08:00   -1.278408\r\n2013-01-01 00:09:00   -0.172525\r\nFreq: T, dtype: float64\r\n```\r\n\r\nWhen trying to resample in **8 equal parts it works fine:**\r\n```\r\nlength = 8\r\n\r\nprint(ts)\r\ntimeSpan = (ts.index[-1]-ts.index[0]+timedelta(minutes=1))\r\nrule = int(timeSpan.total_seconds()/length)\r\ntsNew=ts.resample(str(rule)+""S"")\r\n```\r\n`print(tsNew)`\r\n```\r\n2013-01-01 00:00:00    0.124147\r\n2013-01-01 00:01:15    0.558947\r\n2013-01-01 00:02:30    0.076321\r\n2013-01-01 00:03:45    0.178429\r\n2013-01-01 00:05:00   -1.357948\r\n2013-01-01 00:06:15    0.931305\r\n2013-01-01 00:07:30    0.984052\r\n2013-01-01 00:08:45   -1.758608\r\nFreq: 75S, dtype: float64\r\n```\r\n\r\nBut when tying to split into **9 parts** there are nan arriving :\r\n\r\n```\r\nlength = 9\r\n\r\nprint(ts)\r\ntimeSpan = (ts.index[-1]-ts.index[0]+timedelta(minutes=1))\r\nrule = int(timeSpan.total_seconds()/length)\r\ntsNew=ts.resample(str(rule)+""S"")\r\n```\r\n`print(tsNew)`\r\n```\r\n2013-01-01 00:00:00   -0.264389\r\n2013-01-01 00:01:06         NaN\r\n2013-01-01 00:02:12         NaN\r\n2013-01-01 00:03:18         NaN\r\n2013-01-01 00:04:24         NaN\r\n2013-01-01 00:05:30         NaN\r\n2013-01-01 00:06:36         NaN\r\n2013-01-01 00:07:42         NaN\r\n2013-01-01 00:08:48         NaN\r\n2013-01-01 00:09:54         NaN\r\nFreq: 66S, dtype: float64\r\n```\r\n\r\nDo you have an idea how to solve this issue ?\r\n\r\nPS: with length > 10 it doesn\'t works neither... '"
9525,58268753,shoyer,shoyer,2015-02-19 20:58:07,2015-02-20 00:04:47,2015-02-20 00:04:45,closed,,0.16.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/9525,b'Squashed version of #9515.',b'Fixes #9512\r\nCloses #9515'
9524,58241545,ocefpaf,TomAugspurger,2015-02-19 17:36:57,2015-08-16 13:50:46,2015-08-16 13:50:01,closed,,0.17.0,5,Bug;Usage Question;Visualization,https://api.github.com/repos/pydata/pandas/issues/9524,"b'Bad x-axis dates when plotting two time-series with different indexes (start, end, and freq)'","b""The two DataFrames are from 2014, but from the figure you can see that pandas throws one of them to the year 2050!\r\n\r\n![bad_pandas](https://cloud.githubusercontent.com/assets/950575/6272178/7ec299a6-b844-11e4-81a7-c47c1a3293bf.png)\r\n\r\nI am using pandas 0.15.2 and the notebook below describes the problem and show a few workarounds that might help debug this:\r\n\r\nhttp://nbviewer.ipython.org/gist/ocefpaf/e7e085c84fd3ea7dbc24\r\n\r\nHere is a quick SSCCE to generate the figure above:\r\n```python\r\nimport numpy as np                                                                               \r\nimport pandas as pd                                                                              \r\n                                          \r\nindex = pd.date_range(start='2014-06-01 00:00:00', end='2014-09-01 00:00:00', freq='H')          \r\ndata = np.random.rand(len(index))                                                                \r\ndf0 = pd.DataFrame(data=data, index=index, columns=['DF0'])                                      \r\nindex = pd.date_range(start='2014-04-07 10:57:00', end='2014-10-29 09:32:00', freq='D')          \r\ndata = np.random.rand(len(index))                                                                \r\ndf1 = pd.DataFrame(data=data, index=index, columns=['DF1'])                                      \r\n                                                                                                 \r\nax = df0.plot(legend=True, figsize=(12, 4))                                                      \r\nax = df1.plot(ax=ax)                                                                             \r\n```\r\n\r\nThanks!"""
9522,58201625,jreback,jreback,2015-02-19 12:24:15,2015-02-19 13:19:46,2015-02-19 13:19:45,closed,,0.16.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9522,b'BUG: Bug in .loc partial setting with a np.datetime64 (GH9516)',b'closes #9516 '
9521,58200517,jreback,jreback,2015-02-19 12:12:12,2015-02-19 12:13:14,2015-02-19 12:13:14,closed,,Next Major Release,1,API Design;Bug;Dtypes;Performance,https://api.github.com/repos/pydata/pandas/issues/9521,b'API/PERF/BUG: infer dtypes when enlarging',"b""So this can be fixed by inferring after the set. We need to do this because we first set the value to a null-type (nan/NaT), then set the value. This works fine for datetime/timedelta/floats/strings, but not for integers which get set as ``float``.\r\n\r\nHowever, this *can* be an expensive operation as potentially the entire column needs to be scaned for nulls.\r\n\r\n\r\n```\r\nIn [4]: df = pd.DataFrame()\r\n\r\nIn [5]: df.loc[1,'foo'] = 2\r\n\r\nIn [6]: df\r\nOut[6]: \r\n   foo\r\n1    2\r\n\r\nIn [7]: df.dtypes\r\nOut[7]: \r\nfoo    float64\r\ndtype: object\r\n```"""
9517,58161648,behzadnouri,jreback,2015-02-19 02:50:53,2015-03-06 02:52:19,2015-03-05 23:25:34,closed,,0.16.0,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9517,b'BUG: multiple level unstack with nulls',"b""closes https://github.com/pydata/pandas/issues/9497\r\n\r\n    >>> mi\r\n                            jim    joe\r\n    1st 2nd        3rd\r\n    1   2014-02-01 -1 days  100 -20.87\r\n    2   NaT        NaT      101   5.76\r\n    1   2014-02-03 1 days   102  -4.94\r\n    2   NaT        2 days   103  -0.79\r\n    1   2014-02-05 NaT      104 -12.51\r\n    2   2014-02-06 4 days   105  -9.89\r\n\r\n    >>> mi.unstack(['2nd', '3rd']).fillna('.')\r\n               jim                                                     joe\r\n    2nd 2014-02-01  NaT 2014-02-03    NaT 2014-02-05 2014-02-06 2014-02-01   NaT 2014-02-03    NaT 2014-02-05 2014-02-06\r\n    3rd    -1 days  NaT     1 days 2 days        NaT     4 days    -1 days   NaT     1 days 2 days        NaT     4 days\r\n    1st\r\n    1          100    .        102      .        104          .     -20.87     .      -4.94      .     -12.51          .\r\n    2            .  101          .    103          .        105          .  5.76          .  -0.79          .      -9.89\r\n"""
9516,58142485,dashesy,jreback,2015-02-18 22:55:23,2015-02-20 00:25:06,2015-02-19 13:19:45,closed,,0.16.0,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9516,b'DateTimeIndex appending with loc inconsistency in handling numpy datetime64',"b""This is in Pandas `0.15.2` (but I also tried it on '0.15.2-148-g484f668'):\r\n\r\n```python\r\ndf = pd.DataFrame()\r\ndf.loc[np.datetime64(datetime.datetime.now()),'one'] = 100\r\ndf.loc[np.datetime64(datetime.datetime.now()),'one'] = 100\r\n```\r\n\r\nThis is the output:\r\n```text\r\n                               one\r\n2015-02-18 14:50:05.606510     100\r\n1970-01-17 11:37:51.007748755  100\r\n```\r\n\r\nIt seems the the first usage of `loc` correctly works around limitations of `datetime64` but I cannot explain the second one."""
9515,58136754,ischwabacher,shoyer,2015-02-18 22:10:59,2015-02-20 16:11:04,2015-02-20 00:04:45,closed,,0.16.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/9515,b'FIX: Fix some instances where idx[0] not in idx',b'`DatetimeIndex.__contains__` and `TimedeltaIndex.__contains__` were failing to see duplicated elements in some circumstances.\r\n\r\nFixes #9512'
9512,58109953,ischwabacher,shoyer,2015-02-18 18:47:30,2015-02-20 00:04:45,2015-02-20 00:04:45,closed,,,4,Bug,https://api.github.com/repos/pydata/pandas/issues/9512,b'Non-monotonic-increasing DatetimeIndex claims not to __contain__ duplicate entries',"b""This was fun to debug.\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: 0 in pd.Int64Index([0, 0, 1])\r\nOut[2]: True\r\n\r\nIn [3]: 0 in pd.Int64Index([0, 1, 0])\r\nOut[3]: True\r\n\r\nIn [4]: 0 in pd.Int64Index([0, 0, -1])\r\nOut[4]: True\r\n\r\nIn [5]: pd.Timestamp(0) in pd.DatetimeIndex([0, 1, -1])\r\nOut[5]: True\r\n\r\nIn [6]: pd.Timestamp(0) in pd.DatetimeIndex([0, 1, 0])\r\nOut[6]: False   # BAD\r\n\r\nIn [7]: pd.Timestamp(0) in pd.DatetimeIndex([0, 0, 1])\r\nOut[7]: True\r\n\r\nIn [8]: pd.Timestamp(0) in pd.DatetimeIndex([0, 0, -1])\r\nOut[8]: False   # BAD\r\n```\r\n\r\nTimedeltaIndex is also broken.\r\n\r\nThe problem is in [`DatetimeIndexOpsMixin.__contains__`](https://github.com/pydata/pandas/blob/v0.15.2/pandas/tseries/base.py#L68), which checks the type of `idx.get_loc(key)` to determine whether the key was found in the index.  If the index contains duplicate entries and is not monotonic increasing (for some reason, monotonic decreasing doesn't cut it), `get_loc` eventually falls back to [`Int64Engine._maybe_get_bool_indexer`](https://github.com/pydata/pandas/blob/v0.15.2/pandas/index.pyx#L376), which returns an ndarray of bools if the key is duplicated.  Since the original `__contains__` method is looking for scalars or slices, it reports that the duplicated entry is not present."""
9505,57954211,skellys,jreback,2015-02-17 17:25:34,2015-02-18 00:22:49,2015-02-18 00:22:49,closed,,0.16.0,2,Bug;Dtypes;Duplicate,https://api.github.com/repos/pydata/pandas/issues/9505,b'Result of DataFrameGroupBy.apply(lambda x: pd.Series(...)) can have values incorrectly cast from object (string) to pd.Timestamp for a DataFrame with at least one datetime64[ns] dtype column',"b""See repro below:\r\n\r\n```python\r\nimport pandas as pd\r\ndf_with_ts = pd.DataFrame(data=[['BL', pd.Timestamp('2015-02-01')], ['TH', pd.Timestamp('2015-02-02')]], columns=['item', 'date'])\r\nres = df_with_ts.groupby(['date']).apply(lambda x: pd.Series(x['item'].unique()[0]))\r\nIn []: res                                                                                                                                                            \r\nOut[]:                                                                                                                                                                \r\n                    0                                                                                                                                                   \r\ndate                                                                                                                                                                    \r\n2015-02-01        NaT                                                                                                                                                   \r\n2015-02-02 2015-02-17                                                                                                                                                   \r\n                                                                                                                                                                        \r\nIn []: res.dtypes                                                                                                                                                     \r\nOut[]:                                                                                                                                                                \r\n0    datetime64[ns]                                                                                                                                                     \r\ndtype: object              \r\n```\r\n\r\nIt looks like this is happening because pd.Timestamp('TH') is valid and results in no exceptions. If you replace the string 'TH' with something else then the resulting column's dtype is object, as expected. The order of columns in the above example does not matter.\r\n\r\nFor comparison, this case works as expected:\r\n\r\n```python\r\ndf = pd.DataFrame(data=[['BL', '2015-02-01'], ['TH', '2015-02-02']], columns=['item', 'date'])\r\nres = df.groupby(['date']).apply(lambda x: pd.Series(x['item'].unique()[0]))                                                                                                                                            \r\n                                                                                                                                                                        \r\nIn []: res                                                                                                                                                            \r\nOut[]:                                                                                                                                                                \r\n             0                                                                                                                                                          \r\ndate                                                                                                                                                                    \r\n2015-02-01  BL                                                                                                                                                          \r\n2015-02-02  TH                                                                                          \r\n\r\nIn []: res.dtypes                                                                                                                                                     \r\nOut[]:                                                                                                                                                                \r\n0    object                                                                                                                                                             \r\ndtype: object                                                                              \r\n```\r\n\r\noutput of show_versions() below (using Miniconda):\r\n```python\r\nIn []: from pandas.util.print_versions import show_versions                                                                                                            \r\n                                                                                                                                                                        \r\nIn []: show_versions()                                                                                                                                                 \r\n                                                                                                                                                                        \r\nINSTALLED VERSIONS                                                                                                                                                      \r\n------------------                                                                                                                                                      \r\ncommit: None                                                                                                                                                            \r\npython: 2.7.9.final.0                                                                                                                                                   \r\npython-bits: 64                                                                                                                                                         \r\nOS: Windows                                                                                                                                                             \r\nOS-release: 7                                                                                                                                                           \r\nmachine: AMD64                                                                                                                                                          \r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel                                                                                                           \r\nbyteorder: little                                                                                                                                                       \r\nLC_ALL: None                                                                                                                                                            \r\nLANG: None                                                                                                                                                              \r\n                                                                                                                                                                        \r\npandas: 0.15.2                                                                                                                                                          \r\nnose: 1.3.4                                                                                                                                                             \r\nCython: 0.21.2                                                                                                                                                          \r\nnumpy: 1.8.2                                                                                                                                                            \r\nscipy: 0.14.0                                                                                                                                                           \r\nstatsmodels: 0.5.0                                                                                                                                                      \r\nIPython: 2.3.0                                                                                                                                                          \r\nsphinx: 1.2.3                                                                                                                                                           \r\npatsy: 0.2.1                                                                                                                                                            \r\ndateutil: 2.2                                                                                                                                                           \r\npytz: 2014.9                                                                                                                                                            \r\nbottleneck: 0.8.0                                                                                                                                                       \r\ntables: 3.1.1                                                                                                                                                           \r\nnumexpr: 2.3.1                                                                                                                                                          \r\nmatplotlib: 1.4.0                                                                                                                                                       \r\nopenpyxl: 2.0.2                                                                                                                                                         \r\nxlrd: 0.9.3                                                                                                                                                             \r\nxlwt: 0.7.5                                                                                                                                                             \r\nxlsxwriter: 0.6.6                                                                                                                                                       \r\nlxml: 3.4.1                                                                                                                                                             \r\nbs4: 4.3.2                                                                                                                                                              \r\nhtml5lib: 0.999                                                                                                                                                         \r\nhttplib2: None                                                                                                                                                          \r\napiclient: None                                                                                                                                                         \r\nrpy2: None                                                                                                                                                              \r\nsqlalchemy: 0.9.8                                                                                                                                                       \r\npymysql: None                                                                                                                                                           \r\npsycopg2: None \r\n```"""
9501,57865335,jreback,jreback,2015-02-16 23:44:53,2015-02-24 13:49:22,2015-02-24 13:49:22,closed,,0.16.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9501,b'BUG: Incorrect dtypes inferred on datetimelike looking series & on xs slices (GH9477)',b'closes #9477'
9497,57753602,seth-p,jreback,2015-02-16 00:19:15,2015-03-05 23:25:33,2015-03-05 23:25:33,closed,,0.16.0,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9497,b'BUG: DataFrame.unstack() with two levels containing NaN',"b'With latest master code (including #9061 and #9292), ``DataFrame.unstack()`` produces incorrect results when ``level`` contains two levels one of which contains ``NaN``:\r\n\r\n```\r\n        df = pd.DataFrame({\'A\': list(\'aaaabbbb\'),\'B\':range(8), \'C\':range(8)})\r\n        df.iloc[3, 1] = np.NaN\r\n        dfs = df.set_index([\'A\', \'B\'])\r\n        left = dfs.unstack([0, 1)\r\n        data = [3, 0, 1, 2, 4, 5, 6, 7]\r\n        idx = MultiIndex(levels=[[\'C\'], [\'a\', \'b\'], [0, 1, 2, 4, 5, 6, 7]],\r\n                         labels=[[0, 0, 0, 0, 0, 0, 0, 0],\r\n                                 [0, 0, 0, 0, 1, 1, 1, 1],\r\n                                 [-1, 0, 1, 2, 3, 4, 5, 6]],\r\n                         names=[None, \'A\', \'B\'])\r\n        right = Series(data, index=idx, dtype=dfs.dtypes[0])\r\n        print(""dfs="")\r\n        print(dfs)\r\n        print(""\\nleft="")\r\n        print(left)\r\n        print(""\\nright="")\r\n        print(right)\r\n        assert_series_equal(left, right)\r\n```\r\nproduces:\r\n```\r\n======================================================================\r\nFAIL: test_unstack_nan_index (pandas.tests.test_frame.TestDataFrame)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\Users\\seth\\github\\pandas2\\build\\lib.win-amd64-3.4\\pandas\\tests\\test_frame.py"", line 12439, in test_unstack_nan_index\r\n    assert_series_equal(left, right)\r\n  File ""c:\\Users\\seth\\github\\pandas2\\build\\lib.win-amd64-3.4\\pandas\\util\\testing.py"", line 674, in assert_series_equal\r\n    assert_almost_equal(left.values, right.values, check_less_precise)\r\n  File ""das\\src\\testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:2745)\r\n  File ""das\\src\\testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:1830)\r\n  File ""das\\src\\testing.pyx"", line 135, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:2514)\r\nnose.proxy.AssertionError: (very low values) expected 3.00000 but got 0.00000, with decimal 5\r\n-------------------- >> begin captured stdout << ---------------------\r\ndfs=\r\n       C\r\nA B     \r\na 0    0\r\n  1    1\r\n  2    2\r\n  NaN  3\r\nb 4    4\r\n  5    5\r\n  6    6\r\n  7    7\r\n\r\nleft=\r\n   A  B\r\nC  a  1    0\r\n      2    1\r\n      4    2\r\n      0    3\r\n   b  6    4\r\n      7    5\r\n   a  0    6\r\n      1    7\r\ndtype: int32\r\n\r\nright=\r\n   A  B  \r\nC  a  NaN    3\r\n      0      0\r\n      1      1\r\n      2      2\r\n   b  4      4\r\n      5      5\r\n      6      6\r\n      7      7\r\ndtype: int32\r\n\r\n--------------------- >> end captured stdout << ----------------------\r\n```'"
9491,57694413,behzadnouri,jreback,2015-02-14 14:21:58,2015-03-06 02:53:33,2015-02-16 12:30:41,closed,,0.16.0,5,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9491,b'BUG: unstack with nulls & Timedelta/DateTime index',"b""xref https://github.com/pydata/pandas/pull/9292#discussion_r24620887\r\n\r\non branch:\r\n\r\n    >>> df\r\n               a       b    c\r\n    0 2014-02-01 -1 days  100\r\n    1        NaT     NaT  101\r\n    2 2014-02-03  1 days  102\r\n    3        NaT  2 days  103\r\n    4 2014-02-05     NaT  104\r\n    5 2014-02-06  4 days  105\r\n\r\n    >>> df.pivot('a', 'b', 'c').fillna('-')\r\n    b           NaT -1 days 1 days 2 days 4 days\r\n    a\r\n    NaT         101       -      -    103      -\r\n    2014-02-01    -     100      -      -      -\r\n    2014-02-03    -       -    102      -      -\r\n    2014-02-05  104       -      -      -      -\r\n    2014-02-06    -       -      -      -    105\r\n\r\n    >>> df.pivot('b', 'a', 'c').fillna('-')\r\n    a        NaT 2014-02-01 2014-02-03 2014-02-05 2014-02-06\r\n    b\r\n    NaT      101          -          -        104          -\r\n    -1 days    -        100          -          -          -\r\n    1 days     -          -        102          -          -\r\n    2 days   103          -          -          -          -\r\n    4 days     -          -          -          -        105\r\n"""
9480,57538094,cottrell,jreback,2015-02-12 23:44:58,2015-02-21 13:03:58,2015-02-18 11:27:50,closed,,,3,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9480,b'BUG: bug in sort for grouping with a categorical columns (GH8868)',b'Closes #8868\r\n\r\nhack. I can add tests if this passes the CI and we think this should be taken further.'
9479,57529876,jreback,jreback,2015-02-12 22:29:18,2015-02-13 20:22:24,2015-02-13 20:22:23,closed,,0.16.0,0,Bug;Indexing;Missing-data,https://api.github.com/repos/pydata/pandas/issues/9479,b'BUG: bug in partial setting of with a DatetimeIndex (GH9478)',b'closes #9478 '
9478,57519652,alan-wong,jreback,2015-02-12 21:07:53,2015-02-13 21:02:07,2015-02-13 20:22:23,closed,,0.16.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9478,b'DateTimeIndex values are assigned across entire df when using .loc',"b""I posted a 2-part answer to this question on SO: http://stackoverflow.com/questions/28482553/pandas-set-value-of-column-to-value-of-index-based-on-condtion\r\n\r\nWhat I noticed is that if your index is a datetimeindex then assigning the values is not respecting the column selection and is blatting all rows.\r\n\r\nI am using pandas 0.15.2 using numpy 1.9.1 and python 3.4 64-bit\r\n\r\nexample:\r\n\r\n```\r\nIn [46]:\r\n\r\nrows = 3\r\ndf = pd.DataFrame(np.random.randn(rows,2), columns=list('AB'), index=pd.date_range('1/1/2000', periods=rows, freq='1H'))\r\nprint(df)\r\ndf.loc[df.A > 0.5, 'LAST_TIME_A_ABOVE_X'] = df.loc[df.A > 0.5].index\r\ndf\r\n                            A         B\r\n2000-01-01 00:00:00 -0.761643  0.969167\r\n2000-01-01 01:00:00  0.050335 -1.346953\r\n2000-01-01 02:00:00  0.663857 -0.272247\r\nOut[46]:\r\n                             A                             B  \\\r\n2000-01-01 00:00:00 1970-01-01           1970-01-01 00:00:00   \r\n2000-01-01 01:00:00 1970-01-01 1969-12-31 23:59:59.999999999   \r\n2000-01-01 02:00:00 1970-01-01           1970-01-01 00:00:00   \r\n\r\n                    LAST_TIME_A_ABOVE_X  \r\n2000-01-01 00:00:00                 NaT  \r\n2000-01-01 01:00:00                 NaT  \r\n2000-01-01 02:00:00 2000-01-01 02:00:00 \r\n```\r\n\r\nIf the index is an Int64 type then this doesn't happen.\r\n\r\nIf you reset the index and then assign the values using .loc then it works correctly"""
9477,57514661,ashishsingal1,jreback,2015-02-12 20:27:08,2015-02-24 13:49:22,2015-02-24 13:49:22,closed,,0.16.0,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9477,b'.apply converting string to date',"b""The .apply on DataFrame is converting my string to a date. I have posted this on StackOverflow:\r\n\r\nhttp://stackoverflow.com/questions/28487105/pandas-on-apply-passing-wrong-value\r\n\r\nI'm not sure if I'm doing something wrong or it is a bug, but it seems quite strange behavior. \r\n\r\nExample [here](http://stackoverflow.com/questions/28550339/interesting-pandas-loc-behavior?noredirect=1#comment45415876_28550339)\r\n\r\n```\r\nIn [8]: df = pd.DataFrame({'wing1':wing1, 'wing2':wing2, 'mat':mat}, index=belly)\r\n\r\nIn [9]: df\r\nOut[9]: \r\n            mat wing1 wing2\r\n216  2016-01-22  2T15   416\r\n3T19 2019-09-07  4H19  4T20\r\n\r\nIn [17]: df.loc['3T19']\r\nOut[17]: \r\nmat     2019-09-07 00:00:00\r\nwing1   2015-02-16 04:19:00\r\nwing2   2015-04-20 00:00:00\r\nName: 3T19, dtype: datetime64[ns]\r\n\r\nIn [18]: df.loc['216']\r\nOut[18]: \r\nmat      2016-01-22 00:00:00\r\nwing1                   2T15\r\nwing2                    416\r\nName: 216, dtype: object\r\n```"""
9475,57484076,jorisvandenbossche,jorisvandenbossche,2015-02-12 16:34:48,2015-02-23 11:46:54,2015-02-23 11:46:54,closed,,0.16.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/9475,b'BUG: binary operator method alignment with integer level (GH9463)',"b""Closes #9463\r\n\r\n@jreback also related to #6682 (I removed 'join compat' introduced in that PR, as this worked only for level names, and passing `level=level` works for both level names and integer level numbers)"""
9473,57416819,shoyer,jreback,2015-02-12 04:44:08,2015-02-16 21:53:34,2015-02-16 21:53:33,closed,,0.16.0,1,2/3 Compat;Bug,https://api.github.com/repos/pydata/pandas/issues/9473,b'BUG: fix common.is_hashable for NumPy scalars on Python 3',"b""Fixes #9276\r\n\r\nThis now relies entirely on the result of calling ``hash`` on the argument.\r\n\r\nNote: I had to change the test for old style classes on Python 2 -- these are\r\nnow considered hashable by `is_hashable`, because they don't fail `hash`.\r\n\r\nCC @aevri"""
9470,57380477,jreback,jreback,2015-02-11 21:32:22,2015-02-12 19:23:50,2015-02-12 19:23:50,closed,,0.16.0,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9470,b'BUG: Bug in Categorical.__getitem__/__setitem__ with listlike input getting incorrect result from indexer coercion (GH9469)',b'closes #9469'
9469,57375627,jseabold,jreback,2015-02-11 20:53:55,2015-02-12 20:15:50,2015-02-12 19:23:50,closed,,0.16.0,15,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9469,b'Categorical[idx].codes != Categorical.codes[idx]',"b""I'm unable to isolate this, but maybe this rings a bell for someone. It took me a long time to find the source of error in my code. I don't think this is intentional, and if so, is pretty dangerous.\r\n\r\n    [~/]\r\n    [22]: y[idx].codes[:15]\r\n    [22]: array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4], dtype=int8)\r\n\r\n    [~/]\r\n    [23]: y.codes[idx][:15]\r\n    [23]: array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int8)\r\n\r\n    [~/]\r\n    [24]: idx[:15]\r\n    [24]:\r\n    array([ 74172,  53885, 128953,  28125, 136548, 136700,  93633,  61147,\r\n            56535,  90577, 115719,  58038, 111711,  53399,  77475])\r\n\r\n    [~/]\r\n    [25]: type(y)\r\n    [25]: pandas.core.categorical.Categorical\r\n\r\n    [~/]\r\n    [27]: pd.version.version\r\n    [27]: '0.15.2'"""
9468,57368417,jeremywhelchel,jreback,2015-02-11 19:56:17,2015-03-11 12:10:55,2015-03-11 12:10:55,closed,,0.16.0,7,Bug;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/9468,b'BUG: Series.resample across daylight saving boundary causes segfault.',"b""The following snippet of code is causing a segfault. Here it's failing at head, but I've seen it fail in the 0.15.2 release as well, with both NumPy 1.7.1 and NumPy 1.9.1.\r\nInterestingly this doesn't happen with Pandas 0.13.1.\r\n\r\n```\r\n>>> import pandas\r\n>>> import pytz\r\n>>> pandas.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-30-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\npandas: 0.15.2-182-gbb9c311\r\nnose: None\r\nCython: 0.20.1post0\r\nnumpy: 1.9.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.4.0\r\npytz: 2013b\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n>>> LA_TZ = pytz.timezone('America/Los_Angeles')\r\n>>> \r\n>>> s = pandas.Series({\r\n...     pandas.Timestamp('2010-1-1', tz=LA_TZ): 1,\r\n...     pandas.Timestamp('2011-4-1', tz=LA_TZ): 1})\r\n>>> s\r\n2010-01-01 00:00:00-08:00    1\r\n2011-04-01 00:00:00-07:00    1\r\ndtype: int64\r\n>>> s.resample('D', how='max', fill_method='pad')\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nOther observations:\r\n- The start/end need to cross the daylight savings switch. Notice utc-08:00 vs utc-07:00 above\r\n- Only fails with how=max and fill_method=pad\r\n- Needs a large enough resampled window. 2011-1-1 to 2011-4-1 won't do it. But 2010-1-1 to 2011-4-1 will. """
9467,57362106,shoyer,jreback,2015-02-11 19:12:29,2016-04-10 14:32:46,2016-04-10 14:32:46,closed,,0.18.1,0,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/9467,"b'BUG/CLN: SparseSeries __getitem__ uses questionable, buggy logic'","b""SparseSeries currently relies on the assumption that all non-scalar input will raise a `TypeError` (because it won't be hashable) when used as input to `Index.get_loc`:\r\nhttps://github.com/pydata/pandas/blob/bb9c311f36fbf508d11f2eabe3fdbdd010abf8c0/pandas/sparse/series.py#L361\r\n\r\nHowever, some non-scalar input is hashable (e.g., `Ellipsis`):\r\n```\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: ss = pd.Series(range(10)).to_sparse()\r\n\r\nIn [4]: ss[...]\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-4-f0ccf654123b> in <module>()\r\n----> 1 ss[...]\r\n\r\n/Users/shoyer/dev/pandas/pandas/sparse/series.pyc in __getitem__(self, key)\r\n    369             if isinstance(key, (int, np.integer)):\r\n    370                 return self._get_val_at(key)\r\n--> 371             raise Exception('Requested index not in this series!')\r\n    372\r\n    373         except TypeError:\r\n\r\nException: Requested index not in this series!\r\n```\r\n(this should return the entire sparse series)\r\n\r\nThis logic is also problematic because it relies on `get_loc` always throwing `TypeError` for invalid input. So this could use some cleanup, ideally checking directly if the input is hashable. And it probably shouldn't be raising a generic `Exception`, either.\r\n\r\nxref tslib.pyx change/discussion in #9258"""
9463,57294946,jorisvandenbossche,jorisvandenbossche,2015-02-11 09:50:20,2015-02-23 11:46:54,2015-02-23 11:46:54,closed,,0.16.0,2,Bug;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9463,"b""BUG: binary operator methods (eg mul) don't handle integer level""","b'Suppose I have the following dataframe and series:\r\n\r\n```\r\nIn [20]: df = pd.DataFrame(np.ones((3,4)), columns=pd.MultiIndex.from_product([[\'A\', \'B\'],[\'a\', \'b\']]))\r\n\r\nIn [21]: s = pd.Series({\'a\':1, \'b\':2})\r\n\r\nIn [22]: df\r\nOut[22]: \r\n   A     B   \r\n   a  b  a  b\r\n0  1  1  1  1\r\n1  1  1  1  1\r\n2  1  1  1  1\r\n\r\nIn [23]: s\r\nOut[23]: \r\na    1\r\nb    2\r\ndtype: int64\r\n```\r\n\r\nTrying to multiply this by the columns, and matching on the second level, gives the following error:\r\n\r\n```\r\nIn [24]: df.mul(s, axis=1, level=1)\r\n...\r\n  File ""C:\\Anaconda\\lib\\site-packages\\pandas\\core\\index.py"", line 1786, in _join_multi\r\n    raise ValueError(""cannot join with no level specified and no overlapping names"")\r\n\r\nValueError: cannot join with no level specified and no overlapping names\r\n```\r\n\r\nIn any case, this is a wrong error message, as I *did* specify a level. \r\nFurther, this does work and gives the expected output with a named level instead of a integer level:\r\n\r\n```\r\nIn [25]: df2 = df.copy()\r\n    ...: df2.columns.names = [\'l0\', \'l1\']\r\n\r\nIn [35]: df2.mul(s, axis=1, level=\'l1\') # <-- works\r\n\r\nOut[35]: \r\nl0  A     B   \r\nl1  a  b  a  b\r\n0   1  2  1  2\r\n1   1  2  1  2\r\n2   1  2  1  2\r\n```\r\n\r\nAm I correct that this should also work with integer levels?\r\n'"
9461,57247168,cottrell,jreback,2015-02-10 22:23:29,2015-02-11 14:19:38,2015-02-11 14:19:34,closed,,0.16.0,6,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9461,b'Fix bug in multiindex series groupby where sort argument is ignored',b'PR as per. \r\n\r\nOne test added. Test fails without the fix.\r\n\r\ncloses https://github.com/pydata/pandas/issues/9444'
9459,57236918,Kodiologist,jreback,2015-02-10 21:09:23,2015-03-08 16:12:52,2015-03-08 16:12:47,closed,,0.16.0,15,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9459,"b""BUG: improve handling of Series.value_counts's argument 'dropna' (GH9443)""",b'closes #9443 '
9456,57172776,jreback,jreback,2015-02-10 13:13:29,2015-08-20 15:23:45,2015-06-10 20:01:43,closed,,0.16.2,12,Bug;Difficulty Novice;Dtypes;Effort Low,https://api.github.com/repos/pydata/pandas/issues/9456,b'BUG: need better inference for path in Series construction',"b""This hits a path in ``Series.__init__`` which I think needs some better inference\r\n\r\nhttps://github.com/pydata/pandas/blob/master/pandas/core/series.py#L178\r\n\r\n```\r\nIn [1]: d = {numpy.datetime64('2015-01-07T02:00:00.000000000+0200'): 42544017.198965244,\r\n   ...:      numpy.datetime64('2015-01-08T02:00:00.000000000+0200'): 40512335.181958228,\r\n   ...:      numpy.datetime64('2015-01-09T02:00:00.000000000+0200'): 39712952.781494237,\r\n   ...:      numpy.datetime64('2015-01-12T02:00:00.000000000+0200'): 39002721.453793451}\r\n\r\nIn [2]: Series(d)\r\nOut[2]: \r\n2015-01-07   NaN\r\n2015-01-08   NaN\r\n2015-01-09   NaN\r\n2015-01-12   NaN\r\ndtype: float64\r\n\r\nIn [3]: Series(d.values(),d.keys())\r\nOut[3]: \r\n2015-01-07    42544017.198965\r\n2015-01-08    40512335.181958\r\n2015-01-09    39712952.781494\r\n2015-01-12    39002721.453793\r\ndtype: float64\r\n```\r\n\r\nThe problem is the index is already converted at this point and its not easy to get the keys/values out (except to do so explicity which is better IMHO).\r\n\r\nNeed a review of what currently hits this path (can simply put a halt in here and see what tests hit this). Then figure out a better method."""
9444,56955610,cottrell,jreback,2015-02-08 15:40:42,2015-02-11 14:19:34,2015-02-11 14:19:34,closed,,0.16.0,5,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9444,b'sort=False ignored in Series groupby on MultiIndex levels',"b""Example:\r\n\r\n    import pandas\r\n    def geta():\r\n        i = pandas.MultiIndex.from_tuples([(1, 2, 'a', 0),\r\n                                  (1, 2, 'a', 1),\r\n                                  (1, 1, 'b', 0),\r\n                                  (1, 1, 'b', 1),\r\n                                  (2, 1, 'b', 0),\r\n                                  (2, 1, 'b', 1)], names=['a', 'b', 'c', 'd'])\r\n        a = pandas.Series([0, 1, 2, 3, 4, 5], index=i)\r\n        return(a)\r\n\r\n    for dosort in [True, False]:\r\n        a = geta()\r\n        b = a.groupby(level=['a', 'b'], sort=dosort).first()\r\n        a = None\r\n        print('%s, sort=%s, \\n%s' % (geta.__name__, dosort, b))\r\n\r\nOutput:\r\n\r\n    geta, sort=True,\r\n    a  b\r\n    1  1    2\r\n       2    0\r\n    2  1    4\r\n    dtype: int64\r\n    geta, sort=False,\r\n    a  b\r\n    1  1    2\r\n       2    0\r\n    2  1    4\r\n    dtype: int64\r\n\r\nAlso see: https://github.com/pydata/pandas/pull/9076\r\nPossibly related to: https://github.com/pydata/pandas/issues/8868"""
9443,56953916,Kodiologist,jreback,2015-02-08 14:50:01,2015-03-08 16:13:04,2015-03-08 16:13:04,closed,,0.16.0,5,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9443,"b""Series.value_counts doesn't respect dropna = False for categorical series""","b'Right:\r\n\r\n    $ python -c \'import pandas; print pandas.Series([1, 2, None, 1, 1, 3, None, 3]).value_counts(dropna = False)\'\r\n     1     3\r\n     3     2\r\n    NaN    2\r\n     2     1\r\n    dtype: int64\r\n\r\nWrong, because there is no row for NaN:\r\n\r\n    $ python -c \'import pandas; print pandas.Series([1, 2, None, 1, 1, 3, None, 3], dtype = ""category"").value_counts(dropna = False)\'\r\n    1    3\r\n    3    2\r\n    2    1\r\n    dtype: int64\r\n\r\n`pandas.show_versions()` yields:\r\n\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-30-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.18\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.3.0\r\nsphinx: None\r\npatsy: 0.2.1\r\ndateutil: 2.4.0\r\npytz: 2014.10\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: None\r\nrpy2: 2.4.2\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n'"
9442,56921839,tjcrone,jreback,2015-02-07 19:07:25,2015-08-29 13:12:39,2015-08-29 13:12:39,closed,,0.17.0,6,Bug;Difficulty Intermediate;Effort Low;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9442,b'Strange Behavior of mean() with timedelta64',"b""xref #6549 \r\n\r\nWhen a dataframe with a timedelta64 is very large, the mean() function does not work as expected. In the following code, the mean is incorrect but all the other stats are fine:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndates1 = pd.date_range(start='20000101T000000', end='20150101T230000', freq='1H').values\r\ndates2 = dates1 + np.random.randint(0*60*60*1000000000, 10*24*60*60*1000000000, len(dates1))\r\ndf = pd.DataFrame({'dates1':dates1, 'dates2':dates2})\r\ndf['tdiff'] = dates2-dates1\r\ndf['fdiff'] = df['tdiff'].apply(lambda x: float(x.item())/1000000000/3600/24)\r\ndf.describe()\r\n```\r\n\r\nBy making the length smaller, by changing the start date in the above example:\r\n\r\n```python\r\ndates1 = pd.date_range(start='20140101T000000', end='20150101T230000', freq='1H').values\r\n```\r\n\r\nThe correct result is obtained. (The mean of the timedelta should be about 5 days.) Is this an open bug?\r\n\r\nVersion:\r\n```python\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.21.2\r\nnumpy: 1.9.1\r\nscipy: 0.15.1\r\nstatsmodels: None\r\nIPython: 2.3.1\r\nsphinx: 1.2.3\r\npatsy: None\r\ndateutil: 2.4.0\r\npytz: 2014.10\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```"""
9431,56761312,hayd,jreback,2015-02-06 02:21:56,2015-08-03 22:03:11,2015-08-03 22:03:11,closed,,0.17.0,1,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9431,b'Unique on the datetimeindex / datetime64 array',"b""http://stackoverflow.com/a/28357483/1240268\r\n\r\n```\r\nIn [11]: dti = pd.DatetimeIndex([pd.Timestamp('2015-02-05 22:24:00+0000', tz='UTC')])\r\n\r\nIn [12]: dti.unique()\r\nOut[12]:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2015-02-05 22:24:00+00:00]\r\nLength: 1, Freq: None, Timezone: UTC\r\n\r\nIn [13]: pd.unique(dti)\r\nOut[13]: array([1423175040000000000L], dtype=object)\r\n\r\nIn [14]: pd.unique(dti.values)\r\nOut[14]: array([1423175040000000000L], dtype=object)\r\n```\r\n\r\n*This is on 0.15.2, not checked on master.*"""
9430,56754361,pdiffley,jorisvandenbossche,2015-02-06 00:52:41,2015-04-07 09:07:40,2015-04-07 09:07:33,closed,,0.16.0,3,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/9430,b'ValueError in io.data Options get_all_data function. Could not convert string to float.',"b'When making the request\r\n\r\n    opt = Options(\'PCLN\', \'yahoo\')\r\n    data = opt.get_all_data()\r\n\r\nI get the error:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-f49f9e288226> in <module>()\r\n      1 opt = Options(\'PCLN\', \'yahoo\')\r\n----> 2 data = opt.get_all_data()\r\n\r\n/usr/local/lib/python3.4/dist-packages/pandas/io/data.py in get_all_data(self, call, put)\r\n   1088             expiry_dates, _ = self._get_expiry_dates_and_links()\r\n   1089 \r\n-> 1090         return self._get_data_in_date_range(dates=expiry_dates, call=call, put=put)\r\n   1091 \r\n   1092     def _get_data_in_date_range(self, dates, call=True, put=True):\r\n\r\n/usr/local/lib/python3.4/dist-packages/pandas/io/data.py in _get_data_in_date_range(self, dates, call, put)\r\n   1102                     frame = getattr(self, nam)\r\n   1103                 except AttributeError:\r\n-> 1104                     frame = self._get_option_data(expiry=expiry_date, name=name)\r\n   1105                 data.append(frame)\r\n   1106 \r\n\r\n/usr/local/lib/python3.4/dist-packages/pandas/io/data.py in _get_option_data(self, expiry, name)\r\n    721             frames = getattr(self, frame_name)\r\n    722         except AttributeError:\r\n--> 723             frames = self._get_option_frames_from_yahoo(expiry)\r\n    724 \r\n    725         option_data = frames[name]\r\n\r\n/usr/local/lib/python3.4/dist-packages/pandas/io/data.py in _get_option_frames_from_yahoo(self, expiry)\r\n    653     def _get_option_frames_from_yahoo(self, expiry):\r\n    654         url = self._yahoo_url_from_expiry(expiry)\r\n--> 655         option_frames = self._option_frames_from_url(url)\r\n    656         frame_name = \'_frames\' + self._expiry_to_string(expiry)\r\n    657         setattr(self, frame_name, option_frames)\r\n\r\n/usr/local/lib/python3.4/dist-packages/pandas/io/data.py in _option_frames_from_url(self, url)\r\n    682         if not hasattr(self, \'underlying_price\'):\r\n    683             try:\r\n--> 684                 self.underlying_price, self.quote_time = self._get_underlying_price(url)\r\n    685             except IndexError:\r\n    686                 self.underlying_price, self.quote_time = np.nan, np.nan\r\n\r\n/usr/local/lib/python3.4/dist-packages/pandas/io/data.py in _get_underlying_price(self, url)\r\n    700         root = self._parse_url(url)\r\n    701         underlying_price = float(root.xpath(\'.//*[@class=""time_rtq_ticker Fz-30 Fw-b""]\')[0]\\\r\n--> 702             .getchildren()[0].text)\r\n    703 \r\n    704         #Gets the time of the quote, note this is actually the time of the underlying price.\r\n\r\nValueError: could not convert string to float: \'1,044.60\'\r\n\r\nIt looks like commas just need to be stripped from the text before converting to a float.\r\n\r\n'"
9428,56753148,ajcr,jreback,2015-02-06 00:36:42,2015-07-25 14:52:34,2015-07-25 14:52:34,closed,,0.17.0,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9428,"b'DataFrame populated with the letter ""n"" when no data passed and dtype is specified as str'","b'This is discussed on Stack Overflow [here](http://stackoverflow.com/questions/28354207/why-is-an-empty-dataframe-of-dtype-str-filled-of-n/28355909).\r\n\r\nWhen constructing a DataFrame with the `dtype` as `str` but not passing in any data (just an index), the resulting DataFrame is filled with the letter `n`:\r\n\r\n```python\r\nIn [61]: pd.DataFrame(index=range(2), columns=[0], dtype=str)\r\nOut[61]: \r\n   0\r\n0  n\r\n1  n\r\n```\r\n\r\nThis is contrary to the behaviour of Series when no data is passed in:\r\n\r\n```python\r\nIn [62]: pd.Series(index=range(2), dtype=str)\r\nOut[62]: \r\n0    NaN\r\n1    NaN\r\ndtype: object\r\n```\r\n\r\nIt would be logical for the DataFrame to be populated with `NaN` in this instance too.\r\n\r\n\r\n\r\n'"
9426,56742972,lminer,jreback,2015-02-05 22:54:07,2015-05-20 11:59:03,2015-03-06 03:18:10,closed,,0.16.0,13,Bug;Categorical;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9426,b'Merge fails when dataframe contains categoricals',"b'Trying to perform a left merge between two dataframes using a column of type object. If I include categoricals in the right dataframe, I get the following error. Trying to reproduce with a toy dataset but no luck so far.\r\n```python\r\nout = pd.merge(left, right, how=\'left\', left_on=\'left_id\', right_on=\'right_id\')\r\nTraceback (most recent call last):\r\n  File "".../pandas/tools/merge.py"", line 39, in merge return op.get_result()\r\n  File "".../pandas/tools/merge.py"", line 201, in get_result concat_axis=0, copy=self.copy)\r\n  File "".../pandas/core/internals.py"", line 4046, in concatenate_block_managers for placement, join_units in concat_plan]\r\n  File "".../pandas/core/internals.py"", line 4135, in concatenate_join_units empty_dtype, upcasted_na = get_empty_dtype_and_na(join_units)\r\n  File "".../pandas/core/internals.py"", line 4074, in get_empty_dtype_and_na dtypes[i] = unit.dtype\r\n  File "".../pandas/src/properties.pyx"", line 34, in pandas.lib.cache_readonly.__get__ (pandas/lib.c:40664)\r\n  File "".../pandas/core/internals.py"", line 4349, in dtype self.block.fill_value)[0])\r\n  File "".../pandas/core/common.py"", line 1128, in _maybe_promote if issubclass(np.dtype(dtype).type, compat.string_types):\r\nTypeError: data type not understood\r\n```\r\n'"
9424,56738943,stevenmanton,jreback,2015-02-05 22:20:07,2016-05-23 21:43:04,2016-05-23 21:43:04,closed,,0.19.0,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9424,b'read_csv clobbers values of columns with duplicate names',"b'xref #10577 (has test for duplicates with empty data)\r\n\r\nI don\'t expect this is the correct behavior, although it\'s always possible I\'m doing something wrong. Importing data using the `names` keyword will clobber the values of columns where the name is duplicated. For example:\r\n\r\n```python\r\nfrom StringIO import StringIO\r\nimport pandas as pd\r\n\r\ndata = """"""a,1\r\nb,2\r\nc,3""""""\r\nnames = [\'field\', \'field\']\r\n\r\nprint pd.read_csv(StringIO(data), names=names, mangle_dupe_cols=True)\r\nprint pd.read_csv(StringIO(data), names=names, mangle_dupe_cols=False)\r\n```\r\n\r\nreturns \r\n\r\n```\r\n   field  field\r\n0      1      1\r\n1      2      2\r\n2      3      3\r\n   field  field\r\n0      1      1\r\n1      2      2\r\n2      3      3\r\n```\r\n\r\nHowever, this produces the correct result:\r\n```python\r\ndf = pd.read_csv(StringIO(data), header=None)\r\ndf.columns = names\r\nprint df\r\n```\r\n```\r\n   field  field\r\n0      a      1\r\n1      b      2\r\n2      c      3\r\n```\r\n\r\nInterestingly, it works if the field names are in the header:\r\n```python\r\ndata_with_header = ""field,field\\n"" + data\r\nprint pd.read_csv(StringIO(data_with_header))\r\n```\r\n```\r\n  field  field.1\r\n0     a        1\r\n1     b        2\r\n2     c        3\r\n```\r\n\r\nIs this a bug or am I doing something wrong?'"
9416,56596430,lminer,jreback,2015-02-04 22:17:39,2015-07-18 00:03:27,2015-07-18 00:03:27,closed,,0.17.0,2,Bug;Categorical;Enhancement,https://api.github.com/repos/pydata/pandas/issues/9416,"b""Series.shift() doesn't work for categorical type""","b'Not sure if this is intentional, but Series.shift() won\'t run with categorical dtypes:\r\n\r\n```python\r\nser = pd.Series([\'a\', \'b\', \'c\', \'d\'], dtype=""category"")\r\nser.shift(1)\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-15-1a7536b0af06>"", line 1, in <module>\r\n    ser.shift(1)\r\n\r\n  File ""/.../pandas/core/generic.py"", line 3394, in shift\r\n    new_data = self._data.shift(periods=periods, axis=block_axis)\r\n\r\n  File ""/.../pandas/core/internals.py"", line 2533, in shift\r\n    return self.apply(\'shift\', **kwargs)\r\n\r\n  File ""/.../pandas/core/internals.py"", line 2497, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n\r\n  File ""/.../pandas/core/internals.py"", line 893, in shift\r\n    new_values, fill_value = com._maybe_upcast(self.values)\r\n\r\n  File ""/.../pandas/core/common.py"", line 1218, in _maybe_upcast\r\n    new_dtype, fill_value = _maybe_promote(dtype, fill_value)\r\n\r\n  File ""/.../pandas/core/common.py"", line 1124, in _maybe_promote\r\n    if issubclass(np.dtype(dtype).type, compat.string_types):\r\n\r\nTypeError: data type not understood\r\n\r\n```'"
9380,56106638,behzadnouri,jreback,2015-01-31 01:34:08,2015-02-07 13:07:06,2015-01-31 16:23:42,closed,,0.16.0,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9380,b'bug in groupby when key space exceeds int64 bounds',b'closes https://github.com/pydata/pandas/issues/9096\r\n\r\nalso improves performance when there is `int64` overflow; complete groupby benchmarks [here](https://gist.github.com/behzadnouri/e142fbd65f41357a7360).\r\n\r\n    -------------------------------------------------------------------------------\r\n    Test name                                    | head[ms] | base[ms] |  ratio   |\r\n    -------------------------------------------------------------------------------\r\n    groupby_int64_overflow                       | 637.6967 | 3021.1190 |   0.2111 |\r\n    -------------------------------------------------------------------------------\r\n\r\n    Ratio < 1.0 means the target commit is faster then the baseline.\r\n    Seed used: 1234\r\n\r\n    Target [d6e3cb7] : bug in groupby when key space exceeds int64 bounds\r\n    Base   [391f46a] : BUG: allow for empty SparseSeries SparsePanel constructors (GH9272)\r\n'
9360,55588425,selasley,jreback,2015-01-27 08:19:25,2015-02-05 11:34:52,2015-02-05 11:34:48,closed,,0.16.0,8,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9360,b'BUG: Fix buffer overflows in tokenizer.c that caused python to segfault with certain',b'closes #9205 '
9350,55385082,CoolRanch29,jreback,2015-01-24 21:16:23,2015-07-15 12:42:57,2015-07-15 12:42:57,closed,,0.17.0,14,Bug;Dtypes;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9350,b'Changed uin8 to uint8 in response to issue #9266',b'closes #9266 '
9345,55302506,iwschris,jreback,2015-01-23 16:36:04,2015-02-14 04:48:51,2015-02-14 03:10:28,closed,,0.16.0,42,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/9345,b'BUG: Fixes GH9311 groupby on datetime64',b'datetime64 columns were changing at the nano-second scale when\r\napplying a groupby aggregator.\r\n\r\ncloses #9311 \r\ncloses #6620 '
9341,55222651,CodeSturgeon,jreback,2015-01-22 23:09:06,2015-01-22 23:17:37,2015-01-22 23:16:44,closed,,,1,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/9341,b'to_datetime does not handle timezones correctly for timestamps',"b""It seems that the `to_datetime` function always assumes that a timestamp is in local time and converts it to UTC. This does not seem to be the case with converting from strings.\r\n\r\nThis code:\r\n```python\r\nfrom datetime import datetime\r\nimport time\r\nimport pandas as pd\r\nnow = datetime.now()\r\nts = time.mktime(now.timetuple())\r\nprint ts\r\nprint now.strftime('%Y-%m-%d %H:%M')\r\nprint pd.to_datetime(ts, unit='s')\r\nprint pd.to_datetime(ts, unit='s', utc=True)\r\nprint pd.to_datetime(now.strftime('%Y-%m-%d %H:%M'))\r\n```\r\n\r\nGive this result (pandas 0.15.1):\r\n```\r\n1421967906.0\r\n2015-01-22 18:05\r\n2015-01-22 23:05:06\r\n2015-01-22 23:05:06+00:00\r\n2015-01-22 18:05:00\r\n```\r\n\r\nNote that when `to_datetime` is converting a timestamp five hours are added no matter the `utc=` arg value."""
9331,55107321,shoyer,shoyer,2015-01-22 02:36:03,2015-02-13 00:42:40,2015-02-13 00:42:02,closed,,0.16.0,20,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/9331,"b""BUG: don't sort unique values from categoricals""",b'This should resolve the inconsistency @mwaskom reported in #9148.\r\n\r\nCC @jreback @TomAugspurger @JanSchulz'
9330,55078406,wikiped,jreback,2015-01-21 21:18:00,2015-04-05 23:10:37,2015-04-05 23:10:37,closed,,0.16.1,10,API Design;Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/9330,b'False negative on .equals() after read_hdf()',"b'I have strange results from `.equals` appearing when DataFrame is written to HDF Store and then read back:\r\n\r\n\timport pandas as pd\r\n\tdf = pd.DataFrame({\'B\':[1,2], \'A\':[str(\'x\'), str(\'y\')]})  # str() - just to be sure this is not linked to unicode \r\n\tprint \'df:\'\r\n\tprint df\r\n\tdf.to_hdf(\'hdf_file\', \'key\', format=\'t\', mode=\'w\')\r\n\tdf_out = pd.read_hdf(\'hdf_file\', \'key\')\r\n\tprint \'\\ndf_out:\'\r\n\tprint df_out\r\n\tprint \'\\ndf equals df_out:\', df.equals(df_out)\r\n\tprint \'\\ndf_out equals df:\', df_out.equals(df)\r\n\tprint \'\\ndf.shape == df_out.shape:\', df.shape == df_out.shape\r\n\tprint \'\\narray_equivalent(df.values, df_out.values):\', pd.core.common.array_equivalent(df.values, df_out.values)\r\n\tprint \'\\ndf.index equals df_out.index:\', df.index.equals(df_out.index)\r\n\tprint \'\\ndf.columns equals df_out.columns:\', df.columns.equals(df_out.columns)\r\n\tfor col in df.columns:\r\n\t\tprint \'\\ndf.{0} equals df_out.{0}: {1}\'.format(col, df[col].equals(df_out[col]))\r\n\r\noutput:\r\n\r\n\tdf:\r\n\t   A  B\r\n\t0  x  1\r\n\t1  y  2\r\n\r\n\tdf_out:\r\n\t   A  B\r\n\t0  x  1\r\n\t1  y  2\r\n\r\n\tdf equals df_out: False\r\n\r\n\tdf_out equals df: False\r\n\r\n\tdf.shape == df_out.shape: True\r\n\r\n\tarray_equivalent(df.values, df_out.values): True\r\n\r\n\tdf.index equals df_out.index: True\r\n\r\n\tdf.columns equals df_out.columns: True\r\n\r\n\tdf.A equals df_out.A: True\r\n\r\n\tdf.B equals df_out.B: True\r\n\r\n\r\nThe interesting thing is that if DataFrame is initialized with different columns ""order"" in the dictionary the results are ALL True (i.e. correct):\r\n\r\n    df = pd.DataFrame({\'A\':[1,2], \'B\':[str(\'x\'),str(\'y\')]})  # in the code above\r\n\r\nwill give:\r\n\r\n    df equals df_out: True\r\n    df_out equals df: True\r\n\r\nI have seen similar issues ([#8437](https://github.com/pydata/pandas/issues/8437) and [#7605](https://github.com/pydata/pandas/issues/7605)), which are marked as closed, but seeing this strange results... might be something different?\r\n\r\npython 2.7.9, pandas 0.15.2\r\n\r\nMy apologies in advance for potential duplicate.'"
9311,54908220,iwschris,jreback,2015-01-20 16:52:32,2015-02-14 03:10:28,2015-02-14 03:10:28,closed,,0.16.0,19,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/9311,b'BUG: first() changes datetime64 data',"b""I have a dataframe that contains a datetime64 column that seems to be losing some digits after a groupby.first(). I know that under the hood these are stored at nanosecond precision, but I need them to remain equal, since I'm merging back onto the original frame later on. Is this expected behavior?\r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas import Timestamp\r\n\r\ndata = [\r\n {'a': 1,\r\n  'dateCreated': Timestamp('2011-01-20 12:50:28.593448')},\r\n {'a': 1,\r\n  'dateCreated': Timestamp('2011-01-15 12:50:28.502376')},\r\n {'a': 1,\r\n  'dateCreated': Timestamp('2011-01-15 12:50:28.472790')},\r\n {'a': 1,\r\n  'dateCreated': Timestamp('2011-01-15 12:50:28.445286')}]\r\n\r\ndf = pd.DataFrame(data)\r\n```\r\n\r\nOutput is:\r\n\r\n```python\r\nIn [6]: df\r\nOut[6]:\r\n   a                dateCreated\r\n0  1 2011-01-20 12:50:28.593448\r\n1  1 2011-01-15 12:50:28.502376\r\n2  1 2011-01-15 12:50:28.472790\r\n3  1 2011-01-15 12:50:28.445286\r\n\r\nIn [7]: df.groupby('a').first()\r\nOut[7]:\r\n                    dateCreated\r\na\r\n1 2011-01-20 12:50:28.593447936\r\n```\r\n\r\nWhen I compare the datetime64 in the first row to the datetime64 after the groupby.first(), the two are not equal."""
9292,54714642,behzadnouri,jreback,2015-01-18 23:28:06,2015-02-13 01:59:02,2015-01-26 01:29:07,closed,,0.16.0,4,Bug;Dtypes;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9292,"b'TST: tests for GH4862, GH7401, GH7403, GH7405'",b'closes https://github.com/pydata/pandas/issues/4862\r\ncloses https://github.com/pydata/pandas/issues/7401\r\ncloses https://github.com/pydata/pandas/issues/7403\r\ncloses https://github.com/pydata/pandas/issues/7405\r\n\r\nminor code change to https://github.com/pydata/pandas/pull/9061; otherwise only tests. the code change is to avoid https://github.com/pydata/pandas/issues/9170 issue.\r\n\r\n'
9291,54696655,sinhrks,jreback,2015-01-18 13:34:15,2015-01-31 02:57:19,2015-01-25 18:41:40,closed,,0.16.0,5,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/9291,b'BUG: Adding nano offset raises TypeError',b'Closes #9284.'
9286,54669252,JohnNapier,jreback,2015-01-17 17:54:36,2015-01-17 19:07:01,2015-01-17 18:01:59,closed,,0.16.0,5,Bug;Duplicate;Numeric,https://api.github.com/repos/pydata/pandas/issues/9286,"b'Pandas returns 0/y=inf, even when y is a non-zero dataframe or series.'","b""Pandas returns the wrong result for the operation **0/y** where **y** is a dataframe or series of non-zero values:\r\n\r\n```python\r\nimport pandas as pd\r\ny=pd.Series(range(1,10))\r\nprint 0/y # wrong\r\n```\r\n\r\nHowever, this operation would work:\r\n\r\n```python\r\nfor i in y:print 0/i # correct\r\n```\r\n\r\nI'm using Pandas 0.15.2-1."""
9284,54662962,tvyomkesh,jreback,2015-01-17 14:17:51,2015-01-25 18:41:40,2015-01-25 18:41:40,closed,,0.16.0,0,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/9284,b'Adding tseries.offsets.Nano to other offsets throws error',"b""xref #9226 \r\n\r\n<pre>\r\nIn [11]: from pandas.tseries import offsets\r\n\r\nIn [12]: t1 = offsets.Nano(100)\r\n\r\nIn [13]: t2 = offsets.Micro(1)\r\n\r\nIn [14]: t3 = t1 + t2\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-14-4ee6d99529e1> in <module>()\r\n----> 1 t3 = t1 + t2\r\n\r\n/Users/tvyomkesh/pandas/pandas/tseries/offsets.pyc in __add__(self, other)\r\n   2025                 return type(self)(self.n + other.n)\r\n   2026             else:\r\n-> 2027                 return _delta_to_tick(self.delta + other.delta)\r\n   2028         try:\r\n   2029             return self.apply(other)\r\n\r\nTypeError: ufunc add cannot use operands with types dtype('&lt;m8[ns]') and dtype('O')\r\n</pre>"""
9283,54658016,sinhrks,jreback,2015-01-17 10:35:48,2015-01-19 14:31:31,2015-01-18 20:36:40,closed,,0.16.0,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9283,b'BUG: where coerces numeric to str incorrectly',b'Closes #9280.'
9280,54650540,sinhrks,jreback,2015-01-17 04:41:29,2015-01-18 20:36:40,2015-01-18 20:36:40,closed,,0.16.0,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9280,b'BUG: where incorrectly coerces to str',"b""Replacing some numeric values using `where` affects to dtype unrelated to the condition.\r\n```\r\ns = pd.Series(np.random.randn(5))\r\nu = s.where(s<0, 'X', axis=1)\r\nu\r\n# 0     -1.01421184508\r\n# 1                  X\r\n# 2                  X\r\n# 3    -0.228117839063\r\n# 4    -0.393903873887\r\n# dtype: object\r\n```\r\n\r\nThe result looks OK, but internally coerced to all `str`.\r\n\r\n```\r\nu.apply(type)\r\n# 0    <type 'str'>\r\n# 1    <type 'str'>\r\n# 2    <type 'str'>\r\n# 3    <type 'str'>\r\n# 4    <type 'str'>\r\n# dtype: object\r\n```"""
9276,54622173,shoyer,jreback,2015-01-16 20:38:34,2015-02-16 21:53:33,2015-02-16 21:53:33,closed,,0.16.0,0,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/9276,b'BUG: common.is_hashable returns False for np.float64 on Python 3',"b""xref #8929\r\nxref #9193\r\n\r\nHere are [some test results](https://travis-ci.org/shoyer/pandas/builds/47184451) on Travis-CI illustrating the issue and my [commit](https://github.com/shoyer/pandas/commit/f8eb1c687164274a6b9a9640e565de60293ad2c1)\r\n\r\nAny ideas for the fix? This is quite likely a numpy bug, but it's one we need to work around. The simplest thing would be revert back to just calling `hash`, but I expect that would cause some other test cases to fail.\r\n\r\nCC @aevri\r\n"""
9266,54541329,rmorgans,jreback,2015-01-16 05:33:51,2015-07-21 10:52:45,2015-07-21 10:52:45,closed,,0.17.0,8,Bug;Difficulty Novice;Dtypes;Effort Low;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9266,b'uin8 should be uint8 in io/parsers.py',"b""while using read_fwf I had a bug that had this line from parsers.py\r\n\r\nhttps://github.com/pydata/pandas/blob/072e40b0b5beab36c2adda9e0bcbb26755e1928f/pandas/io/parsers.py#L989\r\n\r\nin which there is a np.uin8 which I'm pretty sure should be a np.uint8\r\n\r\napologise if this has already been submitted - I did a search on uin8 and only got the offending line in the code.\r\n\r\nI just fixed on my arch source code to keep working on my problem (I'm an engineer not a software guy!). I will try and find time for a patch and test (I failed on my last one)\r\n\r\nCHeers\r\n\r\nRick"""
9256,54400633,behzadnouri,jreback,2015-01-15 01:16:58,2015-01-17 11:33:53,2015-01-16 12:51:42,closed,,0.16.0,3,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9256,b'BUG: bug in multi-index where insert fails',"b""closes https://github.com/pydata/pandas/issues/9250\r\n\r\non master:\r\n\r\n    >>> df\r\n             3rd\r\n    1st 2nd     \r\n    a   b      0\r\n    b   d      1\r\n    >>> df.loc[('b', 'x'), '3rd'] = 2  # this works!\r\n    >>> df\r\n             3rd\r\n    1st 2nd     \r\n    a   b      0\r\n    b   d      1\r\n        x      2\r\n    >>> df.loc[('b', 'a'), '3rd'] = -1  # fails! sets everything to -1\r\n    >>> df\r\n             3rd\r\n    1st 2nd     \r\n    a   b     -1\r\n    b   d     -1\r\n        x     -1\r\n    >>> df.loc[('b', 'b'), '3rd'] = 3  # erros!\r\n    NotImplementedError: Index._join_level on non-unique index is not implemented\r\n\r\non branch:\r\n\r\n    >>> df\r\n             3rd\r\n    1st 2nd     \r\n    a   b      0\r\n    b   d      1\r\n    >>> df.loc[('b', 'x'), '3rd'] = 2\r\n    >>> df.loc[('b', 'a'), '3rd'] = -1\r\n    >>> df.loc[('b', 'b'), '3rd'] = 3\r\n    >>> df\r\n             3rd\r\n    1st 2nd     \r\n    a   b      0\r\n    b   d      1\r\n        x      2\r\n        a     -1\r\n        b      3\r\n"""
9250,54338376,FedericoV,jreback,2015-01-14 15:56:18,2015-01-16 12:51:42,2015-01-16 12:51:42,closed,,0.16.0,6,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9250,b'Setting with enlargement fails in presence of MultiIndex',"b""```\r\nIn [1]:\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n \r\nprint pd.__version__\r\n \r\na = np.linspace(0, 10, 11)\r\nidx = [('test1', i) for i in range(5)]\r\nidx.extend([('test2', i) for i in range(6)])\r\n \r\nidx = pd.MultiIndex.from_tuples(idx)\r\n0.15.2-80-ge69deae\r\nIn [2]:\r\n\r\na = pd.Series(a, idx)\r\na\r\nOut[2]:\r\ntest1  0     0\r\n       1     1\r\n       2     2\r\n       3     3\r\n       4     4\r\ntest2  0     5\r\n       1     6\r\n       2     7\r\n       3     8\r\n       4     9\r\n       5    10\r\ndtype: float64\r\nIn [3]:\r\n\r\na.loc[('test1', 2)] = 13\r\na\r\nOut[3]:\r\ntest1  0     0\r\n       1     1\r\n       2    13\r\n       3     3\r\n       4     4\r\ntest2  0     5\r\n       1     6\r\n       2     7\r\n       3     8\r\n       4     9\r\n       5    10\r\ndtype: float64\r\nIn [4]:\r\n\r\na.loc[('test', 17)] = 13\r\n---------------------------------------------------------------------------\r\nIndexingError                             Traceback (most recent call last)\r\n<ipython-input-4-157acf13152f> in <module>()\r\n----> 1 a.loc[('test', 17)] = 13\r\n\r\n/home/federico/Python_Libraries/pandas/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n    114                 if len(key) > self.ndim:\r\n    115                     raise IndexingError('only tuples of length <= %d supported' %\r\n--> 116                                         self.ndim)\r\n    117                 indexer = self._convert_tuple(key, is_setter=True)\r\n    118             else:\r\n\r\nIndexingError: only tuples of length <= 1 supported\r\n\r\nIn [5]:\r\n\r\na.ix[('test', 17)] = 13\r\n---------------------------------------------------------------------------\r\nIndexingError                             Traceback (most recent call last)\r\n<ipython-input-5-89467846f8af> in <module>()\r\n----> 1 a.ix[('test', 17)] = 13\r\n\r\n/home/federico/Python_Libraries/pandas/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n    114                 if len(key) > self.ndim:\r\n    115                     raise IndexingError('only tuples of length <= %d supported' %\r\n--> 116                                         self.ndim)\r\n    117                 indexer = self._convert_tuple(key, is_setter=True)\r\n    118             else:\r\n\r\nIndexingError: only tuples of length <= 1 supported\r\n```\r\n\r\nSomehow - the enlarging fails.  Any clue how to get it to work?  I have also noticed that the 'fast' indexing methods, iat and at fail with DataFrames that are multi indexed."""
9248,54324364,jreback,jreback,2015-01-14 13:55:39,2015-01-14 14:39:57,2015-01-14 14:39:57,closed,,0.16.0,1,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9248,b'BUG: Bug in the returned Series.dt.components index was reset to the default index (GH9247)',b'closes #9247 '
9247,54299311,jorisvandenbossche,jreback,2015-01-14 09:14:13,2015-01-14 14:39:57,2015-01-14 14:39:57,closed,,0.16.0,1,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9247,b'API: .dt.components attribute on timedelta series does not preserve the index',"b""```\r\nIn [70]: tds = pd.Series(pd.timedelta_range('1 days', periods=5, freq='2 h'), index=list('ABCDE'))\r\n\r\nIn [71]: tds\r\nOut[71]:\r\nA   1 days 00:00:00\r\nB   1 days 02:00:00\r\nC   1 days 04:00:00\r\nD   1 days 06:00:00\r\nE   1 days 08:00:00\r\ndtype: timedelta64[ns]\r\n\r\nIn [72]: tds.dt.hours\r\nOut[72]:\r\nA    0\r\nB    2\r\nC    4\r\nD    6\r\nE    8\r\ndtype: int64\r\n\r\nIn [73]: tds.dt.components.hours\r\nOut[73]:\r\n0    0\r\n1    2\r\n2    4\r\n3    6\r\n4    8\r\nName: hours, dtype: int64\r\n```\r\nSo the direct attributes via `.dt` like `.dt.hours` above does preserve the index, but the `components` not. \r\n\r\n@jreback I don't know if this was done on purpose or is an oversight, I can't remember, and I looked briefly to the long discussion in #8184, but didn't find something directly."""
9222,53961832,jreback,jreback,2015-01-10 15:48:22,2015-01-10 17:23:12,2015-01-10 17:23:12,closed,,0.16.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9222,b'BUG: Bug in using grouper functions that need passed thru arguments (GH9221)',b'closes #9221 '
9221,53959022,prebenbang,jreback,2015-01-10 14:04:00,2015-01-10 17:23:12,2015-01-10 17:23:12,closed,,0.16.0,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9221,"b'DataFrame.groupby.fillna for axis=1, no filling happens'","b""groupby.fillna not working along column axis.\r\n\r\nThe bug exists for 15.1 and 15.2\r\nWorks for 13.1\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nprint pd.__version__\r\n\r\ntest = pd.DataFrame(index=pd.MultiIndex.from_product([['value1','value2'],pd.date_range('2014-01-01','2014-01-06')]),\r\n                    columns=pd.Index(['1','2'], name='id'))\r\ntest['1'] = [np.nan, 1, np.nan, np.nan, 11, np.nan, np.nan, 2, np.nan, np.nan, 22, np.nan]\r\ntest['2'] = [np.nan, 3, np.nan, np.nan, 33, np.nan, np.nan, 4, np.nan, np.nan, 44, np.nan]\r\ntestT = test.T\r\n\r\nprint '*' * 30\r\nprint test\r\nprint '*' * 30\r\nprint test.groupby(level=0, axis=0).fillna(method='ffill')\r\nprint '*' * 30\r\nprint testT\r\nprint '*' * 30\r\nprint testT.groupby(level=0, axis=1).fillna(method='ffill')\r\nprint '*' * 30\r\n```"""
9215,53819553,jtorcasso,jreback,2015-01-09 00:25:22,2015-01-18 20:59:05,2015-01-18 20:59:05,closed,,0.16.0,6,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9215,b'BUG FIX: wide_to_long modifies stubnames',"b'Simple fix that prevents modification of the list `stubnames`, which is an argument to `wide_to_long`. \r\n\r\nCloses #9204'"
9212,53730230,tlmaloney,jreback,2015-01-08 09:28:20,2016-07-20 20:22:00,2016-06-12 16:50:05,closed,,No action,28,API Design;Bug;Duplicate;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9212,b'sort_index behavior differs for the same DataFrame?',"b""I feel like I've encountered a bug. In the following scenario, the first `sort_index` call behaves as expected, but the second does not. Does someone know what the difference is here?\r\n\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.15.2'\r\n\r\nIn [3]: tuples = [(' foo', 'bar'), ('foo', 'bar'), (' foo ()', 'bar')]\r\n\r\nIn [4]: cols = pd.MultiIndex.from_tuples(tuples)\r\n\r\nIn [5]: df = pd.DataFrame(index=cols, data={'baz': [0, 1, 2]})\r\n\r\nIn [6]: df\r\nOut[6]: \r\n             baz\r\n foo    bar    0\r\nfoo     bar    1\r\n foo () bar    2\r\n\r\nIn [7]: df.sort_index()\r\nOut[7]: \r\n             baz\r\n foo    bar    0\r\n foo () bar    2\r\nfoo     bar    1\r\n\r\nIn [8]: tuples = [(' foo', 'bar'), ('foo', 'bar')]\r\n\r\nIn [9]: cols = pd.MultiIndex.from_tuples(tuples)\r\n\r\nIn [10]: df = pd.DataFrame(index=cols, data={'baz': [0, 1]})\r\n\r\nIn [11]: df\r\nOut[11]: \r\n          baz\r\n foo bar    0\r\nfoo  bar    1\r\n\r\nIn [12]: df.ix[(' foo ()', 'bar'), 'baz'] = 2\r\n\r\nIn [13]: df\r\nOut[13]: \r\n             baz\r\n foo    bar    0\r\nfoo     bar    1\r\n foo () bar    2\r\n\r\nIn [14]: df.sort_index()\r\nOut[14]: \r\n             baz\r\n foo    bar    0\r\nfoo     bar    1\r\n foo () bar    2\r\n```"""
9210,53704864,behzadnouri,jreback,2015-01-08 01:36:53,2015-01-10 20:17:03,2015-01-10 18:07:10,closed,,0.16.0,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9210,b'BUG: bug in left join on multi-index with sort=True or nulls',"b""on master:\r\n\r\n    In [8]: left\r\n    Out[8]:\r\n      1st 2nd  3rd\r\n    0   c   c   13\r\n    1   b   b   79\r\n    2   a   a   27\r\n    3   b   b   27\r\n    4   c   a   86\r\n\r\n    In [9]: right\r\n    Out[9]:\r\n             4th\r\n    1st 2nd\r\n    c   a    -86\r\n    b   b    -79\r\n    c   c    -13\r\n    b   b    -27\r\n    a   a    -27\r\n\r\n`sort=True` is ignored, and the result is not sorted by the join key:\r\n\r\n    In [10]: left.join(right, on=['1st', '2nd'], how='left', sort=True)\r\n    Out[10]:\r\n      1st 2nd  3rd  4th\r\n    0   c   c   13  -13\r\n    1   b   b   79  -79\r\n    1   b   b   79  -27\r\n    2   a   a   27  -27\r\n    3   b   b   27  -79\r\n    3   b   b   27  -27\r\n    4   c   a   86  -86\r\nin addition:\r\n\r\n    In [44]: left\r\n    Out[44]:\r\n       1st  2nd  3rd\r\n    0  NaN    a   14\r\n    1    a  NaN   10\r\n    2    a    b   19\r\n    3  NaN  NaN   62\r\n    4    a    c   90\r\n\r\n    In [45]: right\r\n    Out[45]:\r\n             4th\r\n    1st 2nd\r\n    NaN a    -14\r\n    a   c    -90\r\n        NaN  -10\r\n        b    -19\r\n    NaN NaN  -62\r\n\r\nthis works:\r\n\r\n    In [46]: merge(left, right.reset_index(), on=['1st', '2nd'], how='left')\r\n    Out[46]:\r\n       1st  2nd  3rd  4th\r\n    0  NaN    a   14  -14\r\n    1    a  NaN   10  -10\r\n    2    a    b   19  -19\r\n    3  NaN  NaN   62  -62\r\n    4    a    c   90  -90\r\n\r\nbut this does not:\r\n\r\n    In [47]: left.join(right, on=['1st', '2nd'], how='left')\r\n    Out[47]:\r\n       1st  2nd  3rd  4th\r\n    0  NaN    a   14  NaN\r\n    1    a  NaN   10  NaN\r\n    2    a    b   19  -19\r\n    3  NaN  NaN   62  NaN\r\n    4    a    c   90  -90\r\n\r\nalso, [`get_group_index`](https://github.com/pydata/pandas/blob/b62754d4de0b60fdbfd67e0d0216ad7cd51d3c5f/pandas/core/groupby.py#L3493) called in [these lines](https://github.com/pydata/pandas/blob/b62754d4de0b60fdbfd67e0d0216ad7cd51d3c5f/pandas/tools/merge.py#L536) is subject to overflow, and should be avoided.\r\n\r\n`r 'join|merge'` benchmarks:\r\n\r\n    -------------------------------------------------------------------------------\r\n    Test name                                    | head[ms] | base[ms] |  ratio   |\r\n    -------------------------------------------------------------------------------\r\n    join_dataframe_index_multi                   |  35.4387 |  36.5883 |   0.9686 |\r\n    join_dataframe_index_single_key_bigger_sort  |  24.7660 |  24.8604 |   0.9962 |\r\n    strings_join_split                           |  57.6473 |  57.6183 |   1.0005 |\r\n    join_dataframe_index_single_key_small        |  16.6840 |  16.6461 |   1.0023 |\r\n    merge_2intkey_sort                           |  61.2427 |  60.5460 |   1.0115 |\r\n    join_non_unique_equal                        |   0.9513 |   0.9391 |   1.0130 |\r\n    left_outer_join_index                        | 2887.7623 | 2839.2557 |   1.0171 |\r\n    i8merge                                      | 1534.6023 | 1506.8540 |   1.0184 |\r\n    join_dataframe_index_single_key_bigger       |  25.3410 |  24.8287 |   1.0206 |\r\n    merge_2intkey_nosort                         |  21.6643 |  21.2137 |   1.0212 |\r\n    join_dataframe_integer_key                   |   3.0307 |   2.9414 |   1.0304 |\r\n    join_dataframe_integer_2key                  |   7.7363 |   7.4220 |   1.0423 |\r\n    -------------------------------------------------------------------------------\r\n    Test name                                    | head[ms] | base[ms] |  ratio   |\r\n    -------------------------------------------------------------------------------\r\n\r\n    Ratio < 1.0 means the target commit is faster then the baseline.\r\n    Seed used: 1234\r\n\r\n    Target [f02ef89] : bug in left join on multi-index with sort=True or nulls\r\n    Base   [b62754d] : Merge pull request #9206 from robertdavidwest/9203_resubmitted_in_single_commit\r\n\r\n    9203 SQUASHED - DOCS: doc string edited pandas/core/frame.duplicated()\r\n"""
9209,53696918,mortada,jreback,2015-01-07 23:36:46,2015-04-29 15:31:04,2015-03-12 12:08:53,closed,,0.16.0,10,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/9209,b'added mising numeric_only option for DataFrame.std/var/sem',"b'closes https://github.com/pydata/pandas/issues/9201, the `numeric_only` option is missing for `DataFrame.std()` (and also `DataFrame.var()` and `DataFrame.sem()`), this is a fix for it'"
9205,53574184,michaelaye,jreback,2015-01-06 22:52:45,2015-02-05 11:35:06,2015-02-05 11:35:06,closed,,0.16.0,20,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9205,b'pandas error kills IPython kernel',"b""Doing this simple thing can kill an IPython 2 notebook kernel (version:GH master):\r\n\r\n```python\r\nurl = 'http://nssdc.gsfc.nasa.gov/planetary/factsheet/index.html'\r\npd.read_table(url)\r\n```\r\nNote that I know this will fail, I just don't expect it to kill a notebook kernel?\r\n\r\nVersion:\r\npandas: 128ce857f1cd\r\nIPython: 13facaf0206240a7301e045666143d68305d0119\r\n"""
9204,53568148,jtorcasso,jreback,2015-01-06 21:51:56,2015-01-18 20:59:16,2015-01-18 20:59:16,closed,,0.16.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/9204,b'BUG: wide_to_long modifies stubnames',"b""This is not a serious issue, but the list of stubnames passed into `wide_to_long` is modified by the function call. \r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-43-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: 1.3.1\r\nCython: None\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\ndateutil: 2.3\r\npytz: 2014.10\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.7.0\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.3\r\nbs4: 4.2.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: None\r\nrpy2: 2.5.2\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\n```python\r\n\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame([[0,1,2,3,8],[4,5,6,7,9]])\r\n>>> df.columns = ['id', 'inc1', 'inc2', 'edu1', 'edu2']\r\n>>> df\r\n   id  inc1  inc2  edu1  edu2\r\n0   0     1     2     3     8\r\n1   4     5     6     7     9\r\n>>> stubs = ['inc', 'edu']\r\n>>> df_long = pd.wide_to_long(df, stubs, i='id', j='age')\r\n>>> df_long\r\n        inc  edu\r\nid age          \r\n0  1      1    3\r\n4  1      5    7\r\n0  2      2    8\r\n4  2      6    9\r\n>>> stubs\r\n['edu']\r\n```"""
9201,53476956,mortada,jreback,2015-01-06 03:25:17,2015-03-12 12:08:53,2015-03-12 12:08:53,closed,,0.16.0,4,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/9201,b'Issues with numeric_only for DataFrame.std()',"b""The docstring shows a `numeric_only` option for `DataFrame.std()` but it does not seem to actually be implemented. I'm happy to take a crack at fixing it but I'm not sure whether it's the doc or the implementation that needs fixing.\r\n\r\nTo see this consider a mixed-type `DataFrame` where I'm setting one entry to be a `str` of `'100'` while all other entries are `float`. For `std()` It does not matter whether `numeric_only` is `True` or `False`, but for `max()` it clearly makes a difference. \r\n```\r\nIn [1]: import pandas as pd\r\nIn [2]: import numpy as np\r\nIn [3]: df = pd.DataFrame(np.random.randn(5, 2), columns=['foo', 'bar'])\r\nIn [4]: df.ix[0, 'foo'] = '100'\r\n\r\nIn [5]: df\r\nOut[5]:\r\n         foo       bar\r\n0        100 -1.958036\r\n1   0.221049  0.309971\r\n2   1.200093 -0.103244\r\n3  -2.475388 -2.279483\r\n4  0.1623936 -1.185682\r\n\r\nIn [6]: df.std(numeric_only=True)\r\nOut[6]:\r\nfoo    44.841828\r\nbar     1.129182\r\ndtype: float64\r\n\r\nIn [7]: df.std(numeric_only=False)\r\nOut[7]:\r\nfoo    44.841828\r\nbar     1.129182\r\ndtype: float64\r\n\r\nIn [8]: df.max(numeric_only=False)\r\nOut[8]:\r\nfoo    100.000000\r\nbar      0.309971\r\ndtype: float64\r\n\r\nIn [9]: df.max(numeric_only=True)\r\nOut[9]:\r\nbar    0.309971\r\ndtype: float64\r\n```"""
9198,53362180,PollyP,jorisvandenbossche,2015-01-05 03:58:04,2015-03-04 10:41:26,2015-03-04 10:40:47,closed,,0.16.0,7,Bug;Compat;Data Reader,https://api.github.com/repos/pydata/pandas/issues/9198,b'BUG: fix for GH9010',b'Superseded by #9358 \r\n\r\n---\r\n\r\n\r\nThis is an update of PR #9024.\r\n\r\n\r\ncloses #9010 '
9197,53327492,mortada,shoyer,2015-01-04 06:33:03,2015-04-29 15:32:55,2015-01-05 06:37:52,closed,,,7,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/9197,b'fixes division by zero error for kurt()',"b'Currently if `kurt()` is called on a `Series` with equal values, it throws a `ZeroDivisionError`\r\n\r\n```\r\nIn [1]: import pandas as pd\r\nIn [2]: import numpy as np\r\nIn [3]: s = pd.Series(np.ones(5))\r\nIn [4]: s.kurt()\r\nZeroDivisionError: float division by zero\r\n```\r\nThis is not consistent with the case when `kurt()` is called on a `DataFrame` of equal values\r\n\r\n```\r\nIn [5]: df = pd.DataFrame(np.ones((5, 5)))\r\nIn [6]: df.kurt()\r\nOut[6]:\r\n0    0\r\n1    0\r\n2    0\r\n3    0\r\n4    0\r\ndtype: float64\r\n```\r\n\r\nwith this patch `s.kurt()` will return `0` instead of throwing.'"
9194,53312835,boarpig,jreback,2015-01-03 19:25:58,2016-04-24 20:19:20,2015-05-09 20:10:25,closed,,No action,14,Bug;IO HTML,https://api.github.com/repos/pydata/pandas/issues/9194,b'BUG: read_html with a single column table #9178',"b'This commit fixes a crash on read_html(html, flavor=""bs4"") if table only has one column.\r\n\r\ncloses #9178 '"
9182,53235628,bjonen,jreback,2015-01-02 03:08:18,2015-03-17 00:19:21,2015-03-17 00:18:39,closed,,0.16.0,28,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/9182,b'FIX: Fix problems with Series text representation. ',"b'This PR harmonizes the way DataFrame and Series are printed. \r\ncloses #8532 \r\ncloses #7508 \r\nBefore \r\n\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.options.display.max_rows = 10\r\n\r\nIn [3]: s = pd.Series([1,1,1,1,1,1,1,1,1,1,0.9999,1,1]*10)\r\n\r\nIn [4]: s\r\nOut[4]:\r\n0    1\r\n1    1\r\n2    1\r\n...\r\n127    0.9999\r\n128    1.0000\r\n129    1.0000\r\nLength: 130, dtype: float64\r\n```\r\nNow\r\n```\r\n0      1.0000\r\n1      1.0000\r\n2      1.0000\r\n3      1.0000\r\n4      1.0000\r\n        ...  \r\n125    1.0000\r\n126    1.0000\r\n127    0.9999\r\n128    1.0000\r\n129    1.0000\r\ndtype: float64\r\n```'"
9178,53200037,boarpig,jreback,2014-12-31 19:39:48,2016-04-25 21:57:21,2016-04-25 21:57:21,closed,,0.18.1,5,Bug;IO HTML,https://api.github.com/repos/pydata/pandas/issues/9178,"b'Crash on read_html(url, flavor=""bs4"") if table has only one column'","b'I was trying to read a package tracking table from finnish post office\'s website and I got\r\n\r\n    Traceback (most recent call last):\r\n      File ""./posti.py"", line 69, in <module>\r\n        dfs = read_html(html)\r\n      File ""/usr/lib/python3.4/site-packages/pandas/io/html.py"", line 851, in read_html\r\n        parse_dates, tupleize_cols, thousands, attrs, encoding)\r\n      File ""/usr/lib/python3.4/site-packages/pandas/io/html.py"", line 721, in _parse\r\n        infer_types, parse_dates, tupleize_cols, thousands))\r\n      File ""/usr/lib/python3.4/site-packages/pandas/io/html.py"", line 609, in _data_to_frame\r\n        _expand_elements(body)\r\n      File ""/usr/lib/python3.4/site-packages/pandas/io/html.py"", line 586, in _expand_elements\r\n        lens = Series(lmap(len, body))\r\n      File ""/usr/lib/python3.4/site-packages/pandas/compat/__init__.py"", line 87, in lmap\r\n        return list(map(*args, **kwargs))\r\n    TypeError: len() of unsized object\r\n\r\nI isolated the offending table into this script:\r\n\r\nhttps://gist.github.com/boarpig/de4044f4188fac700c68\r\n\r\nThe problem seems to be related to [`parse_raw_thead`](https://github.com/pydata/pandas/blob/master/pandas/io/html.py#L344) function\r\n\r\n    def _parse_raw_thead(self, table):\r\n        thead = self._parse_thead(table)\r\n        res = []\r\n        if thead:\r\n            res = lmap(self._text_getter, self._parse_th(thead[0]))\r\n        return np.array(res).squeeze() if res and len(res) == 1 else res\r\n\r\nWhere `res` contains `[\'Tapahtumat\']` which comes out of numpy array creation as \r\n\r\n`array(\'Tapahtumat\', dtype=\'<U10\')`\r\n\r\nwhich then produces previously mentioned error because you cannot take a len from that.'"
9176,53168119,selasley,jreback,2014-12-31 05:32:02,2015-01-10 17:33:28,2015-01-10 17:33:28,closed,,0.16.0,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9176,"b'BUG: ""index_col=False"" not working when ""usecols"" is specified in read_csv (GH9082)'","b""Fixes #9082 \r\nMy access to the internet is spotty for the next few days. This fixes 9082 and does not break any of the existing tests so hopefully it won't break anyone's existing code.  Happy New Year!"""
9175,53164314,jmcnamara,jreback,2014-12-31 03:04:58,2015-01-15 02:35:28,2015-01-15 02:35:28,closed,,0.16.0,5,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/9175,b'BUG: Fix for Timestamp handling in xlwt and xlsxwriter engines.',b'Fix for writing Timestamp objects using the xlwt and xlsxwriter\r\nengines. Both modules write Excel dates and times using\r\ndatetime.timedelta which differs from pandas.Timedelta. This\r\nfix coerces Timestamp objects to datetime objects.\r\n\r\nfixes #9139.'
9173,53118626,fantabolous,jreback,2014-12-30 14:24:52,2015-03-11 12:10:42,2015-03-11 12:10:42,closed,,0.16.0,3,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/9173,"b'BUG: ""ValueError: Values falls after last bin"" on resample(\'1D\') of df with DST index, new in 0.15.2'","b'I just upgraded to pandas 0.15.2 and now hit this when I do resample. It worked fine in 0.15.1 (I rolled back to 0.15.1 to confirm). As far as I can tell it occurs when the first date in the index is during daylight savings and the last is not, or vice versa. \r\n\r\nFor example, with a date just before and another just after the DST change date of April 6:\r\n\r\n    df = pd.DataFrame(index=pd.DatetimeIndex([\'2014-04-04 10:00:00\', \'2014-04-07 10:00:00\']), data={\'open\':0.5}).tz_localize(\'Australia/Sydney\')\r\n    print(df.resample(\'1D\', how={\'open\':\'first\'}))\r\n\r\nTraceback (most recent call last):\r\n  File ""D:/Documents/Programming/Projects/IB/play.py"", line 1332, in <module>\r\n    main()\r\n  File ""D:/Documents/Programming/Projects/IB/play.py"", line 48, in main\r\n    print(df.resample(\'1D\', how={\'open\':\'first\'}))\r\n  File ""C:\\Users\\Tom\\Anaconda\\envs\\py2test\\lib\\site-packages\\pandas\\core\\generic.py"", line 3005, in resample\r\n    return sampler.resample(self).__finalize__(self)\r\n  File ""C:\\Users\\Tom\\Anaconda\\envs\\py2test\\lib\\site-packages\\pandas\\tseries\\resample.py"", line 85, in resample\r\n    rs = self._resample_timestamps()\r\n  File ""C:\\Users\\Tom\\Anaconda\\envs\\py2test\\lib\\site-packages\\pandas\\tseries\\resample.py"", line 275, in _resample_timestamps\r\n    self._get_binner_for_resample(kind=kind)\r\n  File ""C:\\Users\\Tom\\Anaconda\\envs\\py2test\\lib\\site-packages\\pandas\\tseries\\resample.py"", line 123, in _get_binner_for_resample\r\n    self.binner, bins, binlabels = self._get_time_bins(ax)\r\n  File ""C:\\Users\\Tom\\Anaconda\\envs\\py2test\\lib\\site-packages\\pandas\\tseries\\resample.py"", line 184, in _get_time_bins\r\n    bins = lib.generate_bins_dt64(ax_values, bin_edges, self.closed, hasnans=ax.hasnans)\r\n  File ""pandas\\lib.pyx"", line 1064, in pandas.lib.generate_bins_dt64 (pandas\\lib.c:17790)\r\nValueError: Values falls after last bin\r\n\r\nOn the other hand if I roll back to 0.15.1 I get what I expect:\r\n                               open\r\n    2014-04-04 00:00:00+11:00   0.5\r\n    2014-04-05 00:00:00+11:00   NaN\r\n    2014-04-06 00:00:00+11:00   NaN\r\n    2014-04-07 00:00:00+10:00   0.5\r\n\r\nAlternatively in 0.15.2 if I add another date in the previous year, such that the first and last dates are either both in daylight savings (even in a different year) or both not in daylight savings, it also seems to work fine.'"
9171,53092855,jmcnamara,jreback,2014-12-30 04:40:18,2015-01-06 00:13:06,2015-01-06 00:13:06,closed,,0.16.0,5,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/9171,b'BUG: Fix for extraneous default cell format in xlsxwriter files.',"b""Fix for issue in the xlsxwriter engine where is adds a default\r\n'General' format to cells if no other format is applied. This\r\nisn't a bug, per se, but it prevents other row or column formatting.\r\n\r\ncloses #9167"""
9167,53052848,jmcnamara,jreback,2014-12-29 16:36:50,2015-01-06 00:12:48,2015-01-06 00:12:48,closed,,0.16.0,0,Bug;Data IO;IO Excel,https://api.github.com/repos/pydata/pandas/issues/9167,b'BUG: ExcelWriter with xlsxwriter adds default format to cells preventing column formats',"b""The `xlsxwriter` engine (which I helped implement) erroneously adds a default `General` format to cells that don't require it.\r\n\r\nThis doesn't affect spreadsheets created with `to_excel()`. However, it does prevent the user from overwriting the cell format with a column format.\r\n\r\nFor example consider the following program:\r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas import ExcelWriter\r\n\r\ndf = pd.DataFrame([[123456, 123456], \r\n                   [987654, 987654]],\r\n                  columns=['First', 'Second'])\r\n\r\nwriter = ExcelWriter('test.xlsx', engine='xlsxwriter')\r\n\r\ndf.to_excel(writer)\r\n\r\nworkbook = writer.book\r\nworksheet = writer.book.worksheets()[0]\r\n\r\n# Add a column format.\r\nmyformat = workbook.add_format({'num_format': '#,##0'})\r\nworksheet.set_column(1, 1, None, myformat)\r\n\r\nwriter.save()\r\n\r\n```\r\nThe gives the following output:\r\n\r\n![screenshot 2](https://cloud.githubusercontent.com/assets/94267/5570393/7e848cd2-8f78-11e4-9259-98f40ef53cbd.png)\r\n\r\n\r\nHowever, it should look like this (note the format in the B column cells):\r\n\r\n![screenshot](https://cloud.githubusercontent.com/assets/94267/5570400/93733f9e-8f78-11e4-8af3-8ece50d9f144.png)\r\n\r\nI'll submit a PR to fix this as soon as I get a working test.\r\n\r\nVersions:\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 4aa0e0a674a0823b5a81944f087e744f27c4e21d\r\npython: 2.7.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.0.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_IE.UTF-8\r\n\r\npandas: 0.15.2-50-g4aa0e0a\r\nnose: 1.3.0\r\nCython: 0.19.1\r\nnumpy: 1.7.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 1.1.0\r\nsphinx: 1.2b3\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2013b\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.x\r\nopenpyxl: 2.0.2\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.6.4\r\nlxml: 3.2.3\r\nbs4: None\r\nhtml5lib: 1.0b3\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\n"""
9144,52794133,skinnymonkey,jreback,2014-12-24 03:57:14,2015-02-16 12:35:24,2015-02-16 12:35:24,closed,,0.16.0,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/9144,b'Wired behavior: 0 / dataframe = inf while 0.0 / dataframe = 0.0',"b'```\r\nIn [3]: s = pd.Series([1.0, 2.0])\r\n\r\nIn [4]: 0 / s\r\nOut[4]:\r\n0    inf\r\n1    inf\r\ndtype: float64\r\n\r\nIn [5]: 0.0 / s\r\nOut[5]:\r\n0    0\r\n1    0\r\ndtype: float64\r\n```\r\n\r\nThis is seems quite wired. Should not 0/s be 0 as well?\r\n\r\n'"
9143,52790202,jreback,jreback,2014-12-24 02:04:20,2014-12-24 15:50:19,2014-12-24 15:50:18,closed,,0.16.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9143,b'BUG: Bug in Panel indexing with an object-like (GH9140)',b'closes #9140 '
9141,52769270,cgrin,jreback,2014-12-23 19:46:31,2015-01-18 21:09:18,2015-01-18 21:09:11,closed,,0.16.0,8,Bug;Duplicate;IO Google,https://api.github.com/repos/pydata/pandas/issues/9141,b'Check whether GBQ Job is finished',"b""xref #8728 \r\n\r\njobComplete can be False in query_reply. Simply checking for the existence of the field isn't enough, as a False value will cause a KeyError when checking for query_reply['totalRows']"""
9140,52762010,echu79,jreback,2014-12-23 18:09:11,2014-12-24 15:50:18,2014-12-24 15:50:18,closed,,0.16.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/9140,b'Referencing a panel with an object causes an inf loop in 0.15.2',"b'The snippet of code works fine in 0.15.0.  Not sure about 0.15.1 as I did not try it on that version.  However on 0.15.2 it causes an infinite loop\r\n\r\n\r\n    import pandas\r\n    import numpy\r\n\r\n    class TestObject:\r\n            def __str__(self):\r\n                    return ""TestObject""\r\n\r\n    obj = TestObject()\r\n\r\n    wp = pandas.Panel(numpy.random.randn(1,5,4), items=[obj],\r\n            major_axis = pandas.date_range(\'1/1/2000\', periods=5),\r\n            minor_axis=[\'A\', \'B\', \'C\', \'D\'])\r\n\r\n\r\n    print(wp)\r\n\r\n    print(wp[obj])\r\n\r\n\r\n'"
9139,52758265,scls19fr,jreback,2014-12-23 17:18:39,2015-02-17 14:03:47,2015-01-16 16:54:46,closed,,0.16.0,17,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/9139,b'to_excel incorrectly sets time to midnight for exported datetime column',"b'Hello,\r\n\r\nI try this:\r\n\r\n    import requests\r\n    from pandas.io.json import json_normalize\r\n    import json\r\n    import pandas as pd\r\n    response = requests.get(""http://api.openweathermap.org/data/2.5/history/station?start=1356220800&end=1356307200&type=hour&id=5530"")\r\n    df = json_normalize(json.loads(response.text)[\'list\'])\r\n    df[\'dt\'] = pd.to_datetime(df[\'dt\'], unit=\'s\')\r\n    #df = df.set_index(\'dt\')\r\n    print(df)\r\n    df.to_excel(""out.xls"")\r\n\r\nIt displays:\r\n\r\n                        dt  humidity.c      ...       pressure.v  temp.c  temp.ma  \\\r\n    0  2012-12-23 00:00:00           2      ...           1017.0       2   286.15\r\n    1  2012-12-23 01:00:00           2      ...           1017.0       2   286.15\r\n    2  2012-12-23 02:00:00           2      ...           1017.0       2   285.15\r\n    3  2012-12-23 03:00:00           2      ...           1017.0       2   284.15\r\n    4  2012-12-23 04:00:00           2      ...           1017.0       2   283.15\r\n    ...\r\n    19 2012-12-23 19:00:00           1      ...           1016.0       1   282.15\r\n    20 2012-12-23 20:00:00           1      ...           1015.0       1   281.15\r\n    21 2012-12-23 21:00:00           1      ...           1015.0       1   282.15\r\n    22 2012-12-23 22:00:00           1      ...           1015.0       1   281.15\r\n    23 2012-12-23 23:00:00           1      ...           1005.0       1   286.15\r\n    24 2012-12-24 00:00:00           2      ...           1013.5       2   280.15\r\n\r\nbut if we look at Excel file `dt` column looks like\r\n\r\n    dt\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    ...\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    2012-12-23 00:00:00\r\n    2012-12-24 00:00:00\r\n\r\n`to_csv` method is ok\r\n\r\n    In [1]: pd.__version__\r\n    Out[1]: \'0.15.2\'\r\n\r\nKind regards'"
9129,52649462,unutbu,jreback,2014-12-22 13:29:35,2014-12-23 03:22:57,2014-12-23 03:22:57,closed,,0.16.0,2,Bug;Missing-data;Period,https://api.github.com/repos/pydata/pandas/issues/9129,b'isnull not detecting NaT in PeriodIndex',"b""```\r\nIn [158]: x = pd.to_datetime([np.nan, '2000-1-1'])\r\n\r\nIn [159]: y = x.to_period(freq='m')\r\n\r\nIn [160]: y\r\nOut[160]: \r\n<class 'pandas.tseries.period.PeriodIndex'>\r\n[NaT, 2000-01]\r\nLength: 2, Freq: M\r\n\r\nIn [161]: pd.isnull(x)\r\nOut[161]: array([ True, False], dtype=bool)\r\n\r\nIn [162]: pd.isnull(y)\r\nOut[162]: array([False, False], dtype=bool)\r\n```\r\n\r\nI think `pd.isnull(y)` should be returning `array([True, False], dtype=bool)`."""
9114,52557767,shoyer,jreback,2014-12-20 08:56:33,2016-07-24 13:54:58,2016-07-24 13:54:50,closed,,0.19.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9114,b'BUG: datetime64[us] arrays with NaT cannot be cast to DatetimeIndex',"b""```\r\nIn [11]: t = pd.to_datetime(['2000-01-01T00:00', 'NaT']).values.astype('datetime64[us]')\r\n\r\nIn [12]: pd.DatetimeIndex(t)\r\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 294247-01-09 19:59:05\r\n\r\nIn [13]: pd.to_datetime(t)\r\nOut[13]:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2000-01-01, NaT]\r\nLength: 2, Freq: None, Timezone: None\r\n```\r\n\r\nThe second line should give the same result as the last one."""
9105,52366881,mairas,jreback,2014-12-18 14:04:11,2014-12-20 20:06:40,2014-12-20 20:05:52,closed,,0.16.0,3,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/9105,b'Refs BUG-9099: Support simultaneous copy and dtype args in DataFrame init',"b'closes #9099\r\nThis is a somewhat brute-force fix for supporting copy and dtype arguments in DataFrame constructor. Some data might be copied twice if both arguments are set, but if I did the operations in a different order, one other unit test would fail.'"
9104,52353420,jreback,jreback,2014-12-18 11:28:40,2014-12-19 21:03:11,2014-12-19 21:03:11,closed,,0.16.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9104,"b'BUG: Bug in DatetimeIndex iteration, related to (GH8890), fixed in (GH9100)'",b'closes #9100'
9102,52320807,selasley,jreback,2014-12-18 03:09:51,2014-12-20 20:31:55,2014-12-20 20:31:45,closed,,0.16.0,1,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9102,b'BUG in read_csv when using skiprows on a file with CR line endings #9079',b'closes #9079 \r\nFix the read_csv bug introduced in 6bf83c5d when using skiprows on a file with CR line terminators'
9101,52314081,behzadnouri,jreback,2014-12-18 01:10:15,2014-12-21 13:52:03,2014-12-18 11:40:22,closed,,0.16.0,3,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9101,b'overflow bug in multi-index when checking for duplicates',b'closes https://github.com/pydata/pandas/issues/9075'
9100,52273870,Oxtay,jreback,2014-12-17 18:26:03,2014-12-19 21:03:11,2014-12-19 21:03:11,closed,,0.16.0,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9100,b'0.15.2: Iteration over datetime index',"b'When iterating over a Series of datetime objects that have timezone info on them, the returned value has the timezone info but also alters the time according to that timezone info, as seen in example below:\r\n\r\n`time_points` is the index of a dataframe with values:\r\n\r\n    [2014-12-01 03:32:39.987000-08:00, ..., 2014-12-01 04:12:34.987000-08:00]\r\n    Length: 4, Freq: None, Timezone: tzoffset(None, -28800)\r\n    2014-12-01 03:32:39.987000-08:00\r\n    2014-12-01 03:52:38.987000-08:00\r\n    2014-12-01 04:02:36.987000-08:00\r\n    2014-12-01 04:12:34.987000-08:00 \r\n\r\nHowever, for the following command:\r\n\r\n    for time in time_points:\r\n        print time\r\n\r\nthe output is:\r\n\r\n    2014-12-01 11:32:39.987000-08:00\r\n    2014-12-01 11:52:38.987000-08:00\r\n    2014-12-01 12:02:36.987000-08:00\r\n    2014-12-01 12:12:34.987000-08:00'"
9099,52243312,mairas,jreback,2014-12-17 14:05:44,2014-12-20 20:05:52,2014-12-20 20:05:52,closed,,0.16.0,2,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/9099,b'BUG: DataFrame constructor ignores copy=True argument if dtype is set',"b""I just noticed that `DataFrame` constructor ignores the `copy=True` argument if `dtype` is set. In the code snippet below, the `orig` dataframe should stay unmodified after any modification of `new1` and `new2`. Instead, the columns of `new2` (or at least the first one as shown in the snippet) are references to the same data, as highlighted by the modification shown on statement 13 and onwards.\r\n\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-25-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.4.3\r\nIPython: 2.3.1\r\nsphinx: 1.1.2\r\npatsy: 0.3.0\r\ndateutil: 2.3\r\npytz: 2014.10\r\nbottleneck: 0.6.0\r\ntables: None\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.7.4\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None\r\n\r\nIn [4]: orig_data = {\r\n   ...:     'col1': [1.],\r\n   ...:     'col2': [2.],\r\n   ...:     'col3': [3.],}\r\n\r\nIn [5]: orig = pd.DataFrame(orig_data)\r\n\r\nIn [6]: new1 = pd.DataFrame(orig, copy=True)\r\n\r\nIn [7]: new2 = pd.DataFrame(orig, dtype=float, copy=True)\r\n\r\nIn [8]: new1\r\nOut[8]: \r\n   col1  col2  col3\r\n0     1     2     3\r\n\r\nIn [9]: new2\r\nOut[9]: \r\n   col1  col2  col3\r\n0     1     2     3\r\n\r\nIn [10]: new1['col1'] = 100.\r\n\r\nIn [11]: new1\r\nOut[11]: \r\n   col1  col2  col3\r\n0   100     2     3\r\n\r\nIn [12]: orig\r\nOut[12]: \r\n   col1  col2  col3\r\n0     1     2     3\r\n\r\nIn [13]: new2['col1'] = 200.\r\n\r\nIn [14]: new2\r\nOut[14]: \r\n   col1  col2  col3\r\n0   200     2     3\r\n\r\nIn [15]: orig\r\nOut[15]: \r\n   col1  col2  col3\r\n0   200     2     3\r\n\r\nIn [16]:\r\n```"""
9096,52217764,jordeu,jreback,2014-12-17 09:26:06,2015-02-02 15:16:15,2015-01-31 16:23:42,closed,,0.16.0,6,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9096,"b'Groupby ""negative dimensions are not allowed"" error and bad key behaviour when there are NaNs values.'","b'On a groupby with a composed key if the product of all possible values is bigger than 2^63 we get a `ValueError ""negative dimensions are not allowed""` when we call `len(grouped_data)`.\r\n\r\nA simple version to reproduce it:\r\n```python\r\nvalues = range(55109)\r\ndata = pd.DataFrame.from_dict({\'a\': values, \'b\': values, \'c\': values, \'d\': values})\r\ngrouped = data.groupby([\'a\', \'b\', \'c\', \'d\'])\r\nlen(grouped)\r\n```\r\n\r\nA side effect of this error is that if there are NaN values as possible keys it won\'t ignore them, it will replace the NaN values with some other values present in the index.\r\n\r\nHere there is a complete IPython notebook example to reproduce it:\r\nhttp://nbviewer.ipython.org/gist/jordeu/cd86fc99f5f89451cf93\r\n\r\n\r\n'"
9090,52059006,benjschiller,jreback,2014-12-16 00:37:56,2015-01-02 16:29:36,2015-01-02 16:29:36,closed,,0.16.0,5,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9090,b'Fixes .nth() with groupby multiple columns',b'closes #8979\r\n\r\nLet me know if this makes sense\r\n\r\nProblem mentioned in issue 8979 too'
9083,51998905,pilotstew,jorisvandenbossche,2014-12-15 15:29:25,2016-06-14 13:36:54,2014-12-24 10:36:06,closed,,0.16.0,12,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/9083,"b'BUG: error in handling a sqlalchemy type with arguments (instantiated type, not class)'","b""I'm trying to use DataFrame().to_sql to input a time aware dataframe series. Here is an example of my code.\r\n\r\n    times = ['201412120154', '201412110254']\r\n\r\n    df = pd.DataFrame()\r\n    df['time'] = pd.to_datetime(times, utc=True)\r\n\r\n    df.time.to_sql('test', engine, dtype={'time': sqlalchemy.TIMESTAMP(timezone=True)})\r\nThe error I recieve is:\r\n\r\n    TypeError: issubclass() arg 1 must be a class\r\nThe following code works but obviously results in a postgresql column that is not timezone aware.\r\n\r\n    times = ['201412120154', '201412110254']\r\n\r\n    df = pd.DataFrame()\r\n    df['time'] = pd.to_datetime(times, utc=True)\r\n\r\n    df.time.to_sql('test', engine, dtype={'time': sqlalchemy.TIMESTAMP})\r\nI'm using python 2.7, pandas 0.15.2, postsgresql 9.3 and SQLAlchemy 0.9.7. This same issue also occurs with sqlalchemy.DATETIME(timezone=True) vs sqlalchemy.DATETIME\r\n\r\nFull Traceback:\r\n\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-161-ec79a553e6d0> in <module>()\r\n    ----> 1 df.time.to_sql('test', w.engine, dtype={'time': sqlalchemy.TIMESTAMP(timezone=True)})\r\n\r\n    /home/stew/.pyenv/versions/p276/lib/python2.7/site-packages/pandas/core/generic.pyc in to_sql(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\r\n        964             self, name, con, flavor=flavor, schema=schema, if_exists=if_exists,   \r\n        965             index=index, index_label=index_label, chunksize=chunksize,\r\n    --> 966             dtype=dtype)\r\n        967 \r\n        968     def to_pickle(self, path):\r\n\r\n    /home/stew/.pyenv/versions/p276/lib/python2.7/site-packages/pandas/io/sql.pyc in to_sql(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\r\n        536     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\r\n        537                       index_label=index_label, schema=schema,\r\n    --> 538                       chunksize=chunksize, dtype=dtype)\r\n        539 \r\n        540 \r\n\r\n    /home/stew/.pyenv/versions/p276/lib/python2.7/site-packages/pandas/io/sql.pyc in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)\r\n       1162             import sqlalchemy.sql.type_api as type_api\r\n       1163             for col, my_type in dtype.items():\r\n    -> 1164                 if not issubclass(my_type, type_api.TypeEngine):\r\n       1165                     raise ValueError('The type of %s is not a SQLAlchemy '\r\n       1166                                      'type ' % col)\r\n\r\n    TypeError: issubclass() arg 1 must be a class"""
9082,51996222,jseabold,jreback,2014-12-15 15:08:08,2016-07-07 06:21:15,2015-01-10 17:33:40,closed,,0.16.0,4,Bug;IO CSV;Regression,https://api.github.com/repos/pydata/pandas/issues/9082,"b'BUG: ""index_col=False"" not working when ""usecols"" is specified in read_csv'","b'Wes\' old [blog post](http://wesmckinney.com/blog/?p=635) indicates that you can read the malformed FEC data by passing `index_col=False`, the docstring for read_csv seems to also say this. It doesn\'t look like this works anymore?\r\n\r\n    [9]: cat test.csv\r\n    cmte_id,cand_id,cand_nm,contbr_nm,contbr_city,contbr_st,contbr_zip,contbr_employer,contbr_occupation,contb_receipt_amt,contb_receipt_dt,receipt_desc,memo_cd,memo_text,form_tp,file_num,tran_id,election_tp\r\n    C00410118,""P20002978"",""Bachmann, Michele"",""HARVEY, WILLIAM"",""MOBILE"",""AL"",""366010290"",""RETIRED"",""RETIRED"",250,20-JUN-11,"""","""","""",""SA17A"",""736166"",""A1FDABC23D2D545A1B83"",""P2012"",\r\n    C00410118,""P20002978"",""Bachmann, Michele"",""HARVEY, WILLIAM"",""MOBILE"",""AL"",""366010290"",""RETIRED"",""RETIRED"",50,23-JUN-11,"""","""","""",""SA17A"",""736166"",""A899B9B0E223743EFA63"",""P2012"",\r\n\r\n    [10]: cols = [\'cand_nm\', \'contbr_st\', \'contbr_employer\', \'contb_receipt_amt\', \r\n                  \'contbr_occupation\', \'contb_receipt_amt\', \'contb_receipt_dt\']\r\n\r\n\r\n    # raises an error\r\n    pd.read_csv(""test.csv"", usecols=cols, index_col=False)\r\n\r\n    # gets ""incorrect"" columns\r\n    pd.read_csv(""test.csv"", usecols=cols) '"
9079,51983535,cpaulik,jreback,2014-12-15 13:03:45,2014-12-20 20:31:45,2014-12-20 20:31:45,closed,,0.16.0,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9079,"b'Pandas v0.15.2 breaks read_csv with skiprows, delim_whitespace=True and explicit naming of columns'","b""Hi,\r\n\r\nThe latest version of pandas 0.15.2 can no longer read a file that was no problem before.\r\n\r\nThe file has the following format.\r\n\r\n```\r\nSMOSMANIA  SMOSMANIA       Narbonne          43.15000     2.95670  112.00    0.05    0.05 ThetaProbe-ML2X \r\n2007/01/01 01:00   0.2140 U M \r\n2007/01/01 02:00   0.2140 U M \r\n2007/01/01 03:00   0.2140 U M \r\n```\r\nThe file can be found [here](https://raw.githubusercontent.com/TUW-GEO/pytesmo/master/tests/test_ismn/test_data/format_header_values/SMOSMANIA/SMOSMANIA_SMOSMANIA_Narbonne_sm_0.050000_0.050000_ThetaProbe-ML2X_20070101_20070131.stm)\r\n\r\nSince the first line is not directly related to the number of columns below I use skiprows=1 and specify the names explicitly.\r\n```\r\nfname='https://raw.githubusercontent.com/TUW-GEO/pytesmo/master/tests/test_ismn/test_data/format_header_values/SMOSMANIA/SMOSMANIA_SMOSMANIA_Narbonne_sm_0.050000_0.050000_ThetaProbe-ML2X_20070101_20070131.stm'\r\n\r\npd.read_csv(fname, skiprows=1, delim_whitespace=True, names=['date', 'time', 'variable','flag','orig_flag'])\r\n```\r\n\r\nPlease compare the two code sample below. The first using pandas 0.15.2, the second one 0.15.1\r\n\r\n### 0.15.2\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: from pandas.util.print_versions import show_versions\r\n\r\nIn [3]: show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-24-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.2\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.6.0\r\nIPython: 2.3.1\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\nIn [4]: pd.read_csv('https://raw.githubusercontent.com/TUW-GEO/pytesmo/master/tests/test_ismn/test_data/format_header_values/SMOSMANIA/SMOSMANIA_SMOSMANIA_Narbonne_sm_0.050000_0.050000_ThetaProbe-ML2X_20070101_20070131.stm', skiprows=1, delim_whitespace=True, names=['date', 'time', 'variable','flag','orig_flag'])\r\nOut[4]: \r\nEmpty DataFrame\r\nColumns: [date, time, variable, flag, orig_flag]\r\nIndex: []\r\n```\r\n### 0.15.1\r\n\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: from pandas.util.print_versions import show_versions\r\n\r\nIn [3]: show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-24-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.6.0\r\nIPython: 2.3.1\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\nIn [4]: pd.read_csv('https://raw.githubusercontent.com/TUW-GEO/pytesmo/master/tests/test_ismn/test_data/format_header_values/SMOSMANIA/SMOSMANIA_SMOSMANIA_Narbonne_sm_0.050000_0.050000_ThetaProbe-ML2X_20070101_20070131.stm', skiprows=1, delim_whitespace=True, names=['date', 'time', 'variable','flag','orig_flag'])\r\nOut[4]: \r\n           date   time  variable flag orig_flag\r\n0    2007/01/01  01:00    0.2140    U         M\r\n1    2007/01/01  02:00    0.2140    U         M\r\n2    2007/01/01  03:00    0.2140    U         M\r\n3    2007/01/01  04:00    0.2140    U         M\r\n4    2007/01/01  05:00    0.2140    U         M\r\n5    2007/01/01  06:00    0.2140    U         M\r\n6    2007/01/01  07:00    0.2135    U         M\r\n7    2007/01/01  08:00    0.2135    U         M\r\n8    2007/01/01  09:00    0.2135    U         M\r\n9    2007/01/01  10:00    0.2140    U         M\r\n10   2007/01/01  11:00    0.2140    U         M\r\n11   2007/01/01  12:00    0.2145    U         M\r\n12   2007/01/01  13:00    0.2149    U         M\r\n13   2007/01/01  14:00    0.2149    U         M\r\n14   2007/01/01  15:00    0.2149    U         M\r\n15   2007/01/01  16:00    0.2145    U         M\r\n16   2007/01/01  17:00    0.2135    U         M\r\n17   2007/01/01  18:00    0.2130    U         M\r\n18   2007/01/01  19:00    0.2130    U         M\r\n19   2007/01/01  20:00    0.2126    U         M\r\n20   2007/01/01  21:00    0.2121    U         M\r\n21   2007/01/01  22:00    0.2121    U       NaN\r\n22   2007/01/01  23:00    0.2116    U         M\r\n23   2007/01/02  00:00    0.2116    U         M\r\n24   2007/01/02  01:00    0.2112    U         M\r\n25   2007/01/02  02:00    0.2107    U         M\r\n26   2007/01/02  03:00    0.2107    U         M\r\n27   2007/01/02  04:00    0.2102    U         M\r\n28   2007/01/02  05:00    0.2098    U         M\r\n29   2007/01/02  06:00    0.2098    U         M\r\n..          ...    ...       ...  ...       ...\r\n711  2007/01/30  18:00    0.1538    U         M\r\n712  2007/01/30  19:00    0.1534    U         M\r\n713  2007/01/30  20:00    0.1534    U         M\r\n714  2007/01/30  21:00    0.1534    U         M\r\n715  2007/01/30  22:00    0.1534    U         M\r\n716  2007/01/30  23:00    0.1534    U         M\r\n717  2007/01/31  00:00    0.1534    U         M\r\n718  2007/01/31  01:00    0.1531    U         M\r\n719  2007/01/31  02:00    0.1531    U         M\r\n720  2007/01/31  03:00    0.1527    U         M\r\n721  2007/01/31  04:00    0.1527    U         M\r\n722  2007/01/31  05:00    0.1524    U         M\r\n723  2007/01/31  06:00    0.1524    U         M\r\n724  2007/01/31  07:00    0.1524    U         M\r\n725  2007/01/31  08:00    0.1521    U         M\r\n726  2007/01/31  09:00    0.1521    U         M\r\n727  2007/01/31  10:00    0.1521    U         M\r\n728  2007/01/31  11:00    0.1524    U         M\r\n729  2007/01/31  12:00    0.1527    U         M\r\n730  2007/01/31  13:00    0.1534    U         M\r\n731  2007/01/31  14:00    0.1541    U         M\r\n732  2007/01/31  15:00    0.1545    U         M\r\n733  2007/01/31  16:00    0.1541    U         M\r\n734  2007/01/31  17:00    0.1538    U         M\r\n735  2007/01/31  18:00    0.1534    U         M\r\n736  2007/01/31  19:00    0.1531    U         M\r\n737  2007/01/31  20:00    0.1527    U         M\r\n738  2007/01/31  21:00    0.1527    U         M\r\n739  2007/01/31  22:00    0.1524    U         M\r\n740  2007/01/31  23:00    0.1524    U         M\r\n\r\n[741 rows x 5 columns]\r\n```\r\n\r\n"""
9077,51937288,jreback,jreback,2014-12-14 23:55:41,2014-12-18 11:39:07,2014-12-18 11:39:07,closed,,0.16.0,6,API Design;Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9077,b'BUG: Bug in MultiIndex.has_duplicates when having many levels causes an indexer overflow (GH9075)',b'closes #9075 '
9075,51929421,lebedov,jreback,2014-12-14 19:51:04,2014-12-18 11:40:22,2014-12-18 11:40:22,closed,,0.16.0,1,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/9075,b'change in MultiIndex.has_duplicates behavior from 0.15.0 -> 0.15.2',"b""The following code prints False when run with Pandas 0.15.0, but True with 0.15.2; I'm using Python 2.7.6 on Ubuntu 14.04.1 in both cases. Is this change in behavior expected? (I was using has_duplicates to detect duplicate tuples in the index.)\r\n\r\n```\r\nimport pandas as pd\r\nt = [(u'x', u'out', u'z', 5, u'y', u'in', u'z', 169),\r\n     (u'x', u'out', u'z', 7, u'y', u'in', u'z', 119),\r\n     (u'x', u'out', u'z', 9, u'y', u'in', u'z', 135),\r\n     (u'x', u'out', u'z', 13, u'y', u'in', u'z', 145),\r\n     (u'x', u'out', u'z', 14, u'y', u'in', u'z', 158),\r\n     (u'x', u'out', u'z', 16, u'y', u'in', u'z', 122),\r\n     (u'x', u'out', u'z', 17, u'y', u'in', u'z', 160),\r\n     (u'x', u'out', u'z', 18, u'y', u'in', u'z', 180),\r\n     (u'x', u'out', u'z', 20, u'y', u'in', u'z', 143),\r\n     (u'x', u'out', u'z', 21, u'y', u'in', u'z', 128),\r\n     (u'x', u'out', u'z', 22, u'y', u'in', u'z', 129),\r\n     (u'x', u'out', u'z', 25, u'y', u'in', u'z', 111),\r\n     (u'x', u'out', u'z', 28, u'y', u'in', u'z', 114),\r\n     (u'x', u'out', u'z', 29, u'y', u'in', u'z', 121),\r\n     (u'x', u'out', u'z', 31, u'y', u'in', u'z', 126),\r\n     (u'x', u'out', u'z', 32, u'y', u'in', u'z', 155),\r\n     (u'x', u'out', u'z', 33, u'y', u'in', u'z', 123),\r\n     (u'x', u'out', u'z', 12, u'y', u'in', u'z', 144)]\r\nidx = pd.MultiIndex.from_tuples(t)\r\nprint idx.has_duplicates\r\n```\r\n\r\nReference on StackOverflow [here](http://stackoverflow.com/questions/27473093/understanding-the-pandas-multiindex-has-duplicates-property)."""
9057,51676364,jreback,jreback,2014-12-11 11:36:40,2015-06-07 22:28:37,2015-06-07 22:28:37,closed,,0.16.2,1,Bug;Difficulty Novice;Dtypes;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/9057,b'BUG: to_hdf failing with integer columns and data_columns=True',"b""```\r\nIn [39]: N = 1000\r\n\r\nIn [40]: df = DataFrame(np.random.randint(0,8,size=N*400).astype('uint8').reshape(N,-1),index=pd.date_range('20130206',periods=N,freq='ms'))\r\n\r\nIn [41]: df.to_hdf('test.h5','df',mode='w',format='table')\r\n\r\nIn [42]: df.to_hdf('test.h5','df',mode='w',format='table',data_columns=True)\r\nAttributeError: 'numpy.int64' object has no attribute 'startswith'\r\n```"""
9047,51376349,jreback,jreback,2014-12-09 01:21:31,2014-12-10 11:11:03,2014-12-10 11:11:03,closed,,0.15.2,0,Bug;Compat;Timezones,https://api.github.com/repos/pydata/pandas/issues/9047,b'TST: dateutil fixes (GH8639)',b'xref #8639 '
9044,51314088,cpcloud,cpcloud,2014-12-08 15:29:40,2014-12-09 00:51:22,2014-12-08 16:32:56,closed,cpcloud,0.15.2,1,Bug;Windows,https://api.github.com/repos/pydata/pandas/issues/9044,b'Fix timedelta json on windows',b'some fixes for different int sizes on windows'
9043,51309073,jlowin,jreback,2014-12-08 14:44:13,2015-05-09 16:05:40,2015-05-09 16:05:40,closed,,Next Major Release,3,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/9043,b'BUG: series.fillna() fails with quoted numbers',"b'Here\'s a simple example:\r\n\r\n``` python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nx = pd.Series(range(5))\r\nx.iloc[1] = np.nan\r\nx.fillna(0) # works\r\nx.fillna(\'foo\') # works\r\nx.fillna(\'0\') # crash\r\n```\r\n\r\nin the function `_putmask_smart()`, two numpy arrays are compared to determine if they have compatible dtypes. However if one array contains quoted numbers, then the test does NOT resolve as element wise compare, but rather as a ""normal"" python comparison. This means the result is `False`, rather than an array, and the attempt to get `False.all()` obviously fails.\r\n\r\nNot sure exactly why the arrays don\'t compare as expected -- perhaps there are other criteria that would trigger it -- but an easy fix is to add `AttributeError` to the try/except clause as a ""valid"" error. '"
9040,51265725,ahjulstad,jreback,2014-12-08 07:38:37,2014-12-12 14:37:30,2014-12-09 21:23:19,closed,,0.15.2,18,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/9040,b'Return from to_timedelta is forced to dtype timedelta64[ns]. ',"b""This is a revised change, fixing the issue I experienced (issue #9011). The proper change seems to be setting the dtype to ```'timedelta64[ns]'```, as was already done for the other cases in the ```to_timedelta``` routine. \r\n\r\nI have also added more tests, and moved them to the ```test_timedeltas``` function (next to the other tests of the functionality of the constructor method). I have not looked too much at #8886, it is difficult for me to understand the issue."""
9029,51199327,papaloizouc,papaloizouc,2014-12-06 22:47:34,2014-12-07 12:33:36,2014-12-07 12:33:36,closed,,,1,Bug;Error Reporting;IO CSV,https://api.github.com/repos/pydata/pandas/issues/9029,b'BUG: Fixed maximum columns limit when using usecols for csv. #8985',b'fixes  #8985'
9028,51198650,cpcloud,cpcloud,2014-12-06 22:27:31,2014-12-07 20:25:39,2014-12-07 20:25:34,closed,cpcloud,0.15.2,3,Bug;Data IO;IO JSON,https://api.github.com/repos/pydata/pandas/issues/9028,b'Implement timedeltas for to_json',b'Closes #9027'
9026,51195400,davidastephens,davidastephens,2014-12-06 20:45:54,2015-03-07 21:17:28,2015-03-07 21:11:28,closed,,0.16.0,9,Bug;Data Reader;IO Google;Unicode,https://api.github.com/repos/pydata/pandas/issues/9026,b'BUG: Datareader index name shows unicode characters',b'Fixes #8967 '
9025,51195024,davidastephens,jreback,2014-12-06 20:34:01,2014-12-07 00:07:27,2014-12-07 00:07:23,closed,,0.15.2,2,Bug;Data Reader;IO Google,https://api.github.com/repos/pydata/pandas/issues/9025,b'BUG: Fix Datareader dtypes if there are missing values from Google.',b'Fixes #8980 '
9024,51193769,PollyP,jreback,2014-12-06 19:52:43,2015-01-05 23:51:07,2015-01-05 23:51:07,closed,,0.16.0,11,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/9024,b'Gh9010 yahoo options parsing bug',"b'closes #9010. Fix for Yahoo Options parse error for underlying prices of 1,000.00 or larger.'"
9022,51191416,rockg,jreback,2014-12-06 18:27:51,2014-12-22 14:55:29,2014-12-07 21:01:06,closed,,0.15.2,3,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/9022,"b""Make Timestamp('now') equivalent to Timestamp.now()""","b""closes #9000 \r\nI opted to not change np_datetime_strings but instead short-circuit in Timestamp.  I don't have a strong preference one way or another."""
9019,51187034,behzadnouri,jreback,2014-12-06 15:51:22,2014-12-07 00:29:18,2014-12-07 00:18:23,closed,,0.15.2,3,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9019,b'reindex multi-index at level with reordered labels',"b""closes https://github.com/pydata/pandas/issues/4088\r\n\r\non branch:\r\n\r\n    >>> df\r\n                         vals\r\n    first second third       \r\n    mid   3rd    992     1.96\r\n                 562    12.06\r\n          1st    73     -6.46\r\n                 818   -15.75\r\n                 658     5.90\r\n    btm   2nd    915     9.75\r\n                 474    -1.47\r\n                 905    -6.03\r\n          1st    717     8.01\r\n                 909   -21.12\r\n          3rd    616    11.91\r\n                 675     1.06\r\n                 579    -4.01\r\n    top   1st    241     1.79\r\n                 363     1.71\r\n          3rd    677    13.38\r\n                 238   -16.77\r\n                 407    17.19\r\n          2nd    728   -21.55\r\n                 36      8.09\r\n    >>> df.reindex(['top', 'mid', 'btm'], level='first')\r\n                         vals\r\n    first second third       \r\n    top   1st    241     1.79\r\n                 363     1.71\r\n          3rd    677    13.38\r\n                 238   -16.77\r\n                 407    17.19\r\n          2nd    728   -21.55\r\n                 36      8.09\r\n    mid   3rd    992     1.96\r\n                 562    12.06\r\n          1st    73     -6.46\r\n                 818   -15.75\r\n                 658     5.90\r\n    btm   2nd    915     9.75\r\n                 474    -1.47\r\n                 905    -6.03\r\n          1st    717     8.01\r\n                 909   -21.12\r\n          3rd    616    11.91\r\n                 675     1.06\r\n                 579    -4.01\r\n    >>> df.reindex(['1st', '2nd', '3rd'], level='second')\r\n                         vals\r\n    first second third       \r\n    mid   1st    73     -6.46\r\n                 818   -15.75\r\n                 658     5.90\r\n          3rd    992     1.96\r\n                 562    12.06\r\n    btm   1st    717     8.01\r\n                 909   -21.12\r\n          2nd    915     9.75\r\n                 474    -1.47\r\n                 905    -6.03\r\n          3rd    616    11.91\r\n                 675     1.06\r\n                 579    -4.01\r\n    top   1st    241     1.79\r\n                 363     1.71\r\n          2nd    728   -21.55\r\n                 36      8.09\r\n          3rd    677    13.38\r\n                 238   -16.77\r\n                 407    17.19\r\n    >>> df.reindex(['top', 'btm'], level='first').reindex(['1st', '2nd'], level='second')\r\n                         vals\r\n    first second third       \r\n    top   1st    241     1.79\r\n                 363     1.71\r\n          2nd    728   -21.55\r\n                 36      8.09\r\n    btm   1st    717     8.01\r\n                 909   -21.12\r\n          2nd    915     9.75\r\n                 474    -1.47\r\n                 905    -6.03\r\n"""
9012,51102009,davaco,jreback,2014-12-05 14:05:11,2014-12-11 10:28:32,2014-12-11 01:34:25,closed,,0.15.2,9,Bug;Regression;Visualization,https://api.github.com/repos/pydata/pandas/issues/9012,b'Plotting problem in pandas 0.15.1 for DataFrame with datetime-index',"b'Hi have the following snippet:\r\n\r\n```\r\nfrom datetime import datetime\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nplot_df = pd.DataFrame(\r\n    np.random.rand(21, 2),\r\n    index=pd.bdate_range(datetime(2000, 1, 1), datetime(2000, 1, 31)),\r\n    columns=[\'a\', \'b\'])\r\n\r\nfig = plt.figure()\r\nax = fig.add_subplot(111)\r\nax = plot_df.plot(ax=ax)\r\nplt.axhline(y=0)\r\n```\r\n\r\nThis gives an exception in Pandas 0.15.2, **not** in Pandas 0.14.1.\r\nTrace:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test_p15.py"", line 14, in <module>\r\n    plt.axhline(y=0)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 2501, in axhline\r\n    ret = ax.axhline(y=y, xmin=xmin, xmax=xmax, **kwargs)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/axes/_axes.py"", line 729, in axhline\r\n    self.add_line(l)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/axes/_base.py"", line 1486, in add_line\r\n    self._update_line_limits(line)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/axes/_base.py"", line 1497, in _update_line_limits\r\n    path = line.get_path()\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/lines.py"", line 871, in get_path\r\n    self.recache()\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/lines.py"", line 568, in recache\r\n    xconv = self.convert_xunits(self._xorig)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/artist.py"", line 163, in convert_xunits\r\n    return ax.xaxis.convert_units(x)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/matplotlib/axis.py"", line 1448, in convert_units\r\n    ret = self.converter.convert(x, self.units, self)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/converter.py"", line 121, in convert\r\n    return PeriodIndex(values, freq=axis.freq).values\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/period.py"", line 641, in __new__\r\n    ordinal, freq = cls._from_arraylike(data, freq, tz)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/period.py"", line 691, in _from_arraylike\r\n    data = _get_ordinals(data, freq)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/period.py"", line 507, in _get_ordinals\r\n    return lib.map_infer(data, f)\r\n  File ""pandas/src/inference.pyx"", line 1020, in pandas.lib.map_infer (pandas/lib.c:56502)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/period.py"", line 503, in <lambda>\r\n    f = lambda x: Period(x, freq=freq).ordinal\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/period.py"", line 126, in __init__\r\n    dt, _, reso = parse_time_string(value, freq)\r\n  File ""/Users/dcoevord/anaconda/envs/p15/lib/python2.7/site-packages/pandas/tseries/tools.py"", line 460, in parse_time_string\r\n    raise DateParseError(e)\r\npandas.tseries.tools.DateParseError: day is out of range for month\r\n```\r\n\r\n\r\nVersion info:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.0.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.1\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 1.5\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\n'"
9010,51063586,PollyP,jreback,2014-12-05 04:59:56,2015-03-05 23:28:23,2015-03-05 23:28:23,closed,,0.16.0,3,Bug;Data Reader;Difficulty Novice,https://api.github.com/repos/pydata/pandas/issues/9010,"b""Yahoo options DataReader can't parse underlying prices with commas""","b'I ran into this problem retrieving option data for the SPX settlement index:\r\n\r\n```\r\n>>> options_object = web.Options(\'^spxpm\', \'yahoo\')\r\n>>> options_object.get_call_data( expiry=datetime.date(2014, 12, 20) )\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/pollyp/anaconda/lib/python2.7/site-packages/pandas/io/data.py"", line 785, in get_call_data\r\n    return self._get_data_in_date_range(expiry, call=True, put=False)\r\n  File ""/Users/pollyp/anaconda/lib/python2.7/site-packages/pandas/io/data.py"", line 1098, in _get_data_in_date_range\r\n    frame = self._get_option_data(expiry=expiry_date, name=name)\r\n  File ""/Users/pollyp/anaconda/lib/python2.7/site-packages/pandas/io/data.py"", line 717, in _get_option_data\r\n    frames = self._get_option_frames_from_yahoo(expiry)\r\n  File ""/Users/pollyp/anaconda/lib/python2.7/site-packages/pandas/io/data.py"", line 655, in _get_option_frames_from_yahoo\r\n    option_frames = self._option_frames_from_url(url)\r\n  File ""/Users/pollyp/anaconda/lib/python2.7/site-packages/pandas/io/data.py"", line 684, in _option_frames_from_url\r\n    self.underlying_price, self.quote_time = self._get_underlying_price(url)\r\n  File ""/Users/pollyp/anaconda/lib/python2.7/site-packages/pandas/io/data.py"", line 696, in _get_underlying_price\r\n    .getchildren()[0].text)\r\nValueError: invalid literal for float(): 2,071.92\r\n```\r\n\r\nHere\'s my show_versions output:\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.1\r\nnose: 1.3.4\r\nCython: 0.21\r\nnumpy: 1.9.0\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.2.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.7\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.0\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.7\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: 0.9999-dev\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None\r\n```'"
9008,51054644,jreback,jreback,2014-12-05 01:58:13,2014-12-05 02:34:01,2014-12-05 02:34:01,closed,,0.15.2,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/9008,"b'BUG: Bug in using a pd.Grouper(key=...) with no level/axis or level only (GH8795, GH8866)'",b'closes #8795\r\ncloses #8866'
9005,51042657,jorisvandenbossche,jreback,2014-12-04 23:13:52,2016-07-11 01:37:09,2016-07-11 01:37:08,closed,,0.19.0,9,Bug;Difficulty Intermediate;Dtypes;Effort Low;Timeseries,https://api.github.com/repos/pydata/pandas/issues/9005,"b""BUG: comparing 'NaT' in series with series returns True""","b""xref #12877 (test on Series & DataFrame)\r\n\r\nWhen having a 'NaT' in a datetime series and comparing this to a datetime series, the comparison with NaT returns True (and shouldn't comparisons with NA not always return False?) Comparing it with a scalar does return False:\r\n\r\n```\r\nIn [54]: s = pd.Series([pd.Timestamp('2012-01-01'), pd.Timestamp('NaT')])\r\nIn [55]: s2 = pd.Series([pd.Timestamp('2013-01-01'), pd.Timestamp('2014-01-01')])\r\n\r\nIn [57]: s\r\nOut[57]:\r\n0   2012-01-01\r\n1          NaT\r\ndtype: datetime64[ns]\r\n\r\nIn [58]: s2\r\nOut[58]:\r\n0   2013-01-01\r\n1   2014-01-01\r\ndtype: datetime64[ns]\r\n\r\nIn [59]: s < s2\r\nOut[59]:\r\n0    True\r\n1    True\r\ndtype: bool\r\n\r\nIn [60]: s < pd.Timestamp('2013-01-01')\r\nOut[60]:\r\n0     True\r\n1    False\r\ndtype: bool\r\n```\r\n\r\nOriginal from http://stackoverflow.com/questions/27305324/pandas-comparison-with-variable-number-of-columns/27305661#27305661"""
9002,51029381,pybokeh,jreback,2014-12-04 21:09:36,2016-02-29 09:46:03,2016-02-27 15:09:05,closed,,0.18.0,9,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/9002,b'StopIteration error when trying to import specific columns from Excel file using parse_cols',"b'```\r\ndf = pd.read_excel(r\'\\\\path_to_Excel_file\', \r\n                   \'Sheet1\', parse_cols=[\'MODEL_YEAR\'])\r\n\r\nreturns:\r\n\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n<ipython-input-29-336e4fd9eea1> in <module>()\r\n      4 \r\n      5 df = pd.read_excel(r\'\\\\path_to_Excel_file\', \r\n----> 6                    \'Sheet1\', parse_cols=[""MODEL_YEAR""])\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\excel.py in read_excel(io, sheetname, **kwds)\r\n    125     engine = kwds.pop(\'engine\', None)\r\n    126 \r\n--> 127     return ExcelFile(io, engine=engine).parse(sheetname=sheetname, **kwds)\r\n    128 \r\n    129 \r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\excel.py in parse(self, sheetname, header, skiprows, skip_footer, index_col, parse_cols, parse_dates, date_parser, na_values, thousands, chunksize, convert_float, has_index_names, **kwds)\r\n    236                                  skip_footer=skip_footer,\r\n    237                                  convert_float=convert_float,\r\n--> 238                                  **kwds)\r\n    239 \r\n    240     def _should_parse(self, i, parse_cols):\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\excel.py in _parse_excel(self, sheetname, header, skiprows, skip_footer, index_col, has_index_names, parse_cols, parse_dates, date_parser, na_values, thousands, chunksize, convert_float, **kwds)\r\n    355                             skip_footer=skip_footer,\r\n    356                             chunksize=chunksize,\r\n--> 357                             **kwds)\r\n    358 \r\n    359         return parser.read()\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in TextParser(*args, **kwds)\r\n   1285     """"""\r\n   1286     kwds[\'engine\'] = \'python\'\r\n-> 1287     return TextFileReader(*args, **kwds)\r\n   1288 \r\n   1289 \r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in __init__(self, f, engine, **kwds)\r\n    555             self.options[\'has_index_names\'] = kwds[\'has_index_names\']\r\n    556 \r\n--> 557         self._make_engine(self.engine)\r\n    558 \r\n    559     def _get_options_with_defaults(self, engine):\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in _make_engine(self, engine)\r\n    698             elif engine == \'python-fwf\':\r\n    699                 klass = FixedWidthFieldParser\r\n--> 700             self._engine = klass(self.f, **self.options)\r\n    701 \r\n    702     def _failover_to_python(self):\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in __init__(self, f, **kwds)\r\n   1392         # infer column indices from self.usecols if is is specified.\r\n   1393         self._col_indices = None\r\n-> 1394         self.columns, self.num_original_columns = self._infer_columns()\r\n   1395 \r\n   1396         # Now self.columns has the set of columns that we will process.\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in _infer_columns(self)\r\n   1607             columns = []\r\n   1608             for level, hr in enumerate(header):\r\n-> 1609                 line = self._buffered_line()\r\n   1610 \r\n   1611                 while self.line_pos <= hr:\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in _buffered_line(self)\r\n   1734             return self.buf[0]\r\n   1735         else:\r\n-> 1736             return self._next_line()\r\n   1737 \r\n   1738     def _empty(self, line):\r\n\r\nD:\\Miniconda3\\envs\\notebook\\lib\\site-packages\\pandas\\io\\parsers.py in _next_line(self)\r\n   1758                             break\r\n   1759                 except IndexError:\r\n-> 1760                     raise StopIteration\r\n   1761         else:\r\n   1762             while self.pos in self.skiprows:\r\n\r\nStopIteration: \r\n```\r\n\r\nDocumentation says the following:\r\nparse_cols : int or list, default None\r\n- If None then parse all columns\r\n- If int then indicates last column to be parsed\r\n- If list of ints then indicates list of column numbers to be parsed\r\n- If string then indicates comma separated list of column names and column ranges (e.g. \xa1\xb0A:E\xa1\xb1 or \xa1\xb0A,C,E:F\xa1\xb1)\r\n\r\nAlso did:\r\n```\r\ndf = pd.read_excel(r\'\\\\path_to_Excel_file\', \r\n                   \'Sheet1\', parse_cols=""MODEL_YEAR"")\r\n```\r\n\r\nBut still got the same error.\r\n\r\n```\r\nUsing Windows 7 with Miniconda3\r\nconda list output:\r\nbeautiful-soup            4.3.2                    py34_0\r\nbeautifulsoup4            4.3.2                     <pip>\r\nbrewer2mpl                1.4.1                     <pip>\r\ndateutil                  2.1                      py34_2\r\nggplot                    0.6.5                     <pip>\r\nipython                   2.3.1                    py34_0\r\njdcal                     1.0                      py34_0\r\njinja2                    2.7.3                    py34_1\r\nlxml                      3.4.0                    py34_0\r\nmarkupsafe                0.23                     py34_0\r\nmatplotlib                1.4.2                np19py34_0\r\nnumexpr                   2.3.1                np19py34_0\r\nnumpy                     1.9.1                    py34_0\r\nopenpyxl                  2.0.2                    py34_0\r\npandas                    0.15.1               np19py34_0\r\npatsy                     0.3.0                np19py34_0\r\npip                       1.5.6                    py34_0\r\npycrypto                  2.6.1                    py34_2\r\npygments                  2.0.1                    py34_0\r\npyodbc                    3.0.7                    py34_0\r\npyparsing                 2.0.1                    py34_0\r\npyqt                      4.10.4                   py34_0\r\npyreadline                2.0                      py34_0\r\npython                    3.4.2                         0\r\npython-dateutil           2.1                       <pip>\r\npytz                      2014.9                   py34_0\r\npywin32                   219                      py34_0\r\npyzmq                     14.4.1                   py34_0\r\nrequests                  2.4.3                    py34_0\r\nscipy                     0.14.0               np19py34_0\r\nseaborn                   0.5.0                     <pip>\r\nsetuptools                7.0                      py34_0\r\nsix                       1.8.0                    py34_0\r\nstatsmodels               0.6.0                np19py34_0\r\ntornado                   4.0.2                    py34_0\r\nxlrd                      0.9.3                    py34_0\r\n```'"
9001,51028074,EVaisman,jreback,2014-12-04 20:57:24,2014-12-04 23:22:45,2014-12-04 23:22:29,closed,,0.16.0,1,API Design;Bug;Duplicate;Reshaping,https://api.github.com/repos/pydata/pandas/issues/9001,b'concat sorts columns without preserving order',"b'Although it\'s _my_ desired behavior, I realize perhaps this is not _the_ desired behavior, but I think it\'s reasonable to bring up.\r\n\r\nIf I want to concat two data frames and their column names are the same, the order is preserved:\r\n\r\n```\r\n>>> df1 = pd.DataFrame(columns=[""A"",""1""])\r\n>>> pd.concat([df1,df1])\r\nEmpty DataFrame\r\nColumns: [A, 1]\r\nIndex: []\r\n```\r\n\r\nHowever, if they are not, it seems they are sorted:\r\n\r\n```\r\n>>> df2 = pd.DataFrame(columns=[""A"",""2""])\r\n>>> pd.concat([df1,df2])\r\nEmpty DataFrame\r\nColumns: [1, 2, A]\r\nIndex: []\r\n```\r\n\r\nWould it be reasonable to try to preserve the order? So the output would be:\r\n\r\n```\r\n>>> pd.concat([df1,df2])\r\nEmpty DataFrame\r\nColumns: [A, 1, 2]\r\nIndex: []\r\n```\r\n\r\nI realize that, in an ordered list, `""A""` doesn\'t come before `""1""` or `""2""`, but in both `df1` and `df2`,  `""A""` _does_ come before `""1""` and `""2""`.'"
9000,51007934,zhopan77,jreback,2014-12-04 18:07:54,2014-12-07 21:01:06,2014-12-07 21:01:06,closed,,0.15.2,6,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/9000,"b""Timestamp('now', tz=xxx) result wrong""","b""In earlier version (0.14 and earlier), Timestamp('now', tz=xxx) will return time now in xxx time zone.\r\n\r\nIn 0.15.1, it returns the time in UTC but tagged with xxx time zone.\r\n\r\nExample: \r\n\r\nIn version 0.12.0:\r\n\r\n>>> import pandas\r\n>>> pandas.version.version\r\n'0.12.0'\r\n>>> pandas.Timestamp('now', tz='US/Central')\r\nTimestamp('2014-12-04 12:06:03-0600', tz='US/Central')\r\n>>> pandas.Timestamp('now')\r\nTimestamp('2014-12-04 18:06:11', tz=None)\r\n\r\nIn version 0.15.1:\r\n\r\n>>> import pandas\r\n>>> pandas.version.version\r\n'0.15.1'\r\n>>> pandas.Timestamp('now', tz='US/Central')\r\nTimestamp('2014-12-04 18:07:29-0600', tz='US/Central')\r\n>>> pandas.Timestamp('now')\r\nTimestamp('2014-12-04 18:07:37')\r\n\r\n\r\n\r\n\r\n\r\n"""
8990,50918028,jreback,jreback,2014-12-04 01:36:00,2014-12-04 02:47:31,2014-12-04 02:47:31,closed,,0.15.2,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/8990,b'REGR: Regression in DatetimeIndex iteration with a Fixed/Local offset timezone (GH8890)',b'closes #8890'
8984,50872544,selasley,jreback,2014-12-03 18:18:24,2014-12-03 23:55:42,2014-12-03 21:56:45,closed,,0.15.2,6,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8984,"b'BUG in read_csv skipping rows after a row with trailing spaces, #8983'","b'Update tokenizer.c to fix a BUG in read_csv skipping rows after tokenizing a row with trailing spaces, Closes #8983'"
8983,50869068,selasley,jreback,2014-12-03 17:47:39,2014-12-03 21:56:45,2014-12-03 21:56:45,closed,,0.15.2,0,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8983,b'BUG in read_csv skipping a row after a row with trailing spaces',"b'as reported by @xdliao in #8752, skipping a row after a row with trailing spaces fails to create the expected dataframe\r\ndata = """"""A B C  \\nD E F  \\nH I J   \\n1 2 3  \\n4 5 6  \\n""""""\r\npd.read_csv(StringIO(data), skiprows=2, delim_whitespace=True) and\r\npd.read_csv(StringIO(data), skiprows=[0,1], delim_whitespace=True) work as expected, but the dataframes returned by\r\npd.read_csv(StringIO(data), skiprows=[0,2], delim_whitespace=True) and\r\npd.read_csv(StringIO(data), skiprows=[1,2], delim_whitespace=True) are incorrect'"
8981,50832251,jreback,jreback,2014-12-03 12:40:22,2014-12-04 02:48:20,2014-12-04 02:48:19,closed,,0.15.2,3,Bug;Dtypes;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8981,b'BUG: Bug in Timestamp-Timestamp not returning a Timedelta type (GH8865)',b'closes #8865 '
8980,50805837,femtotrader,jreback,2014-12-03 09:07:13,2014-12-07 00:07:23,2014-12-07 00:07:23,closed,,0.15.2,7,Bug;Data Reader;IO Google,https://api.github.com/repos/pydata/pandas/issues/8980,b'Google Finance DataReader returns columns with object type instead of float64',"b'Hello,\r\n\r\nGoogle Finance DataReader returns columns with object type instead of float64\r\n\r\n    In [112]: import pandas.io.data as web\r\n\r\n    In [113]: import datetime\r\n\r\n    In [114]: start = datetime.datetime(2010, 1, 1)\r\n\r\n    In [115]: end = datetime.datetime(2013, 1, 27)\r\n\r\n    In [116]: f=web.DataReader(""F"", \'google\', start, end)\r\n\r\n    In [117]: f.dtypes\r\n    Out[117]:\r\n    Open       object\r\n    High       object\r\n    Low        object\r\n    Close     float64\r\n    Volume      int64\r\n    dtype: object\r\n\r\n'"
8979,50792910,nickeubank,jreback,2014-12-03 07:11:34,2015-01-02 16:29:45,2015-01-02 16:29:45,closed,,0.16.0,4,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8979,"b'groupby().nth() missing docs, throws errors when multiple columns in group'","b""Hi All!\r\n\r\nI've been using .nth(), and found a couple issues (though it's extremely useful in general!). In particular, I couldn't find any help documentation (in python) for the function. \r\n\r\nAlso, I ran into issues with I had a two-column groupby. for example: \r\n\r\n```\r\ndf = pd.DataFrame([[1,2,3],[1,5,6], [7,8,6]], columns = ['col1','col2','col3'])\r\ndf.groupby('col1').nth([0,1]) \r\n```\r\n\r\nIs ok\r\n```\r\nIn [23]: df.groupby(['col1', 'col2'], as_index = False).nth([0,1])\r\nOut[23]: \r\n   col1  col2  col3\r\n0     1     2     3\r\n1     1     5     6\r\n2     7     8     6\r\n\r\nIn [24]: df.groupby(['col1', 'col2']).nth([0,1])\r\nValueError: Length of new names must be 1, got 2\r\n```\r\n"""
8977,50770719,bashtage,jreback,2014-12-03 03:07:24,2015-01-18 22:42:19,2014-12-03 14:24:58,closed,,0.15.2,8,Bug;IO Stata;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8977,b'BUG: StataWriter uses incorrect string length',b'Fixes bug where StataWriter always writes strings with\r\na size of 244.\r\n\r\ncloses #8969'
8975,50765616,jreback,jreback,2014-12-03 02:14:19,2014-12-03 02:53:18,2014-12-03 02:53:18,closed,,0.15.2,0,Bug;Categorical;Compat;Internals,https://api.github.com/repos/pydata/pandas/issues/8975,b'COMPAT: infer_dtype not handling categoricals (GH8974)',b'closes #8974 '
8969,50675794,dmsul,jreback,2014-12-02 16:26:14,2014-12-03 14:28:48,2014-12-03 14:24:58,closed,,0.15.2,2,Bug;IO Stata;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8969,b'to_stata always stores strings as str244',"b""Copied from #7858:\r\n\r\nI am still getting this bug (pandas 0.15.1, numpy 1.9.1, Stata 13.1 on Windows 7). The written DTA file still stores the strings as str244 even though the strings themselves have length 1. This also means that when they're read back into pandas the DataFrame looks the same. I think the only way to detect it from within Pandas is to look at the size of the DTA file itself.\r\n\r\n```python\r\ndf = pd.DataFrame(['a', 'b', 'c'], columns=['alpha'])\r\ndf.to_stata('test.dta')\r\ndf2 = pd.read_stata('test.dta')\r\nassert (df['alpha'] == df2['alpha']).min()\r\n```\r\nBut when loading in Stata:\r\n```stata\r\n. use test\r\n. describe\r\n\r\nContains data from D:\\data\\Pollution\\test.dta\r\n  obs:             3                          \r\n vars:             2                          02 Dec 2014 10:30\r\n size:           744                          \r\n--------------------------------------------------------------------------------------------\r\n              storage   display    value\r\nvariable name   type    format     label      variable label\r\n--------------------------------------------------------------------------------------------\r\nindex           long    %12.0g                \r\nalpha           str244  %1s                   \r\n--------------------------------------------------------------------------------------------\r\nSorted by:  \r\n\r\n. compress\r\n  index was long now byte\r\n  alpha was str244 now str1\r\n  (738 bytes saved)\r\n\r\n. describe\r\n\r\nContains data from D:\\data\\Pollution\\test.dta\r\n  obs:             3                          \r\n vars:             2                          02 Dec 2014 10:30\r\n size:             6                          \r\n--------------------------------------------------------------------------------------------\r\n              storage   display    value\r\nvariable name   type    format     label      variable label\r\n--------------------------------------------------------------------------------------------\r\nindex           byte    %12.0g                \r\nalpha           str1    %9s                   \r\n--------------------------------------------------------------------------------------------\r\nSorted by:  \r\n\r\n```\r\nI don't know how critical this is since there are workarounds. You can either use ``compress; save, replace`` in Stata after every use of pandas or, if the 244 problem makes the DTA exceed your memory limit (which causes quite the system error lightshow as I just experienced), you could pass it through a CSV first.\r\n\r\nIt's just a matter of convenience.\r\n"""
8967,50660573,femtotrader,jreback,2014-12-02 14:53:07,2015-03-07 23:17:36,2015-03-07 23:17:36,closed,,0.16.0,1,Bug;Data Reader;Unicode,https://api.github.com/repos/pydata/pandas/issues/8967,b'DataReader with google source returns DataFrame with incorrect index name',"b'Hello,\r\n\r\n    import pandas.io.data as web\r\n    import datetime\r\n\r\n    symbol = ""AAPL""\r\n    end = datetime.datetime.now()\r\n    start = end - datetime.timedelta(days=10)\r\n    df = web.DataReader(""AAPL"", \'google\', start, end)\r\n\r\n    df.index.name\r\n\r\nis `\'\\xef\\xbb\\xbfDate\'` it should be only `\'Date\'`\r\n\r\n\\xef\\xbb\\xbf characters are invisible !\r\n\r\nA quick and dirty solution is to change\r\n\r\n    def _retry_read_url(url, retry_count, pause, name):\r\n        for _ in range(retry_count):\r\n            time.sleep(pause)\r\n\r\n            # kludge to close the socket ASAP\r\n            try:\r\n                with urlopen(url) as resp:\r\n                    lines = resp.read()\r\n            except _network_error_classes:\r\n                pass\r\n            else:\r\n                rs = read_csv(StringIO(bytes_to_str(lines)), index_col=0,\r\n                              parse_dates=True)[::-1]\r\n                # Yahoo! Finance sometimes does this awesome thing where they\r\n                # return 2 rows for the most recent business day\r\n                if len(rs) > 2 and rs.index[-1] == rs.index[-2]:  # pragma: no cover\r\n                    rs = rs[:-1]\r\n                return rs\r\n\r\n        raise IOError(""after %d tries, %s did not ""\r\n                      ""return a 200 for url %r"" % (retry_count, name, url))\r\n\r\nto\r\n\r\n    def _retry_read_url(url, retry_count, pause, name):\r\n        for _ in range(retry_count):\r\n            time.sleep(pause)\r\n\r\n            # kludge to close the socket ASAP\r\n            try:\r\n                with urlopen(url) as resp:\r\n                    lines = resp.read()\r\n            except _network_error_classes:\r\n                pass\r\n            else:\r\n                rs = read_csv(StringIO(bytes_to_str(lines)), index_col=0,\r\n                              parse_dates=True)[::-1]\r\n                rs.index.name = \'Date\'\r\n                # Yahoo! Finance sometimes does this awesome thing where they\r\n                # return 2 rows for the most recent business day\r\n                if len(rs) > 2 and rs.index[-1] == rs.index[-2]:  # pragma: no cover\r\n                    rs = rs[:-1]\r\n                return rs\r\n\r\n        raise IOError(""after %d tries, %s did not ""\r\n                      ""return a 200 for url %r"" % (retry_count, name, url))\r\n\r\nso adding rs.index.name = \'Date\'\r\nfix the problem but it could have side effect for other datasource\r\n\r\nKind regards'"
8966,50647906,unutbu,jreback,2014-12-02 13:00:45,2014-12-03 14:27:45,2014-12-03 14:27:36,closed,,0.15.2,3,Bug;Missing-data;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8966,b'BUG: ValueError raised by cummin/cummax when datetime64 Series contains NaT.',b'closes #8965\r\n\r\n'
8965,50647016,unutbu,jreback,2014-12-02 12:50:12,2014-12-03 14:27:36,2014-12-03 14:27:36,closed,,0.15.2,0,Bug;Missing-data;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8965,b'Series.cummin raises ValueError when datetime64 Series contains NaT',"b""Ref: http://stackoverflow.com/q/27240365/190597\r\n\r\n```\r\ns = pd.Series(pd.date_range('2008-09-15', periods=10, freq='m'))\r\ns.loc[10] = pd.NaT\r\ns.cummin()\r\n\r\nValueError: Could not convert object to NumPy datetime\r\n```\r\n- validate for ``cummax`` & ``timedelta64[ns]``"""
8964,50641569,jamalsenouci,jreback,2014-12-02 11:45:28,2014-12-05 05:15:44,2014-12-05 01:59:06,closed,,0.16.0,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8964,b'BUG: fixes pd.Grouper for non-datetimelike groupings #8866',"b""closes #8866 \r\n\r\nThe bug was caused by a couple of things, firstly when specifying only a level, the axis default was None. This meant it failed to create the internal grouper breaking on line 254 of groupby.py: \r\n```python\r\nax = obj._get_axis(self.axis)\r\n```\r\nSecondly, the aggregate function is set up to take a basegrouper class whereas for datetime resampling we need to pass a datetime index. I split the ```_set_grouper``` method to return a datetime index when a resampling frequency is specified and a ```BaseGrouper``` class otherwise. I opted to split that out into two functions to reduce complexity with much of the ```_set_basegrouper``` function being borrowed from the ```_get_grouper``` function. Let me know if there's a better way to fix this.\r\n"""
8954,50574841,tiagoantao,jreback,2014-12-01 21:06:12,2015-04-28 12:00:07,2015-04-28 12:00:07,closed,,0.16.1,11,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8954,b'Applying max_colwidth to the DataFrame index (#7856)',b'closes #7856 \r\n\r\nreports a problem with the application of max_colwidth to groupby.\r\n\r\nThe real problem is that max_colwidth is not applied to the index. This patch solves that.\r\n\r\nI am not applying self.justify to the formatting of the index (as that would break a test). But maybe it makes more sense to change the test?'
8951,50521688,JackKelly,jorisvandenbossche,2014-12-01 13:06:23,2014-12-01 13:46:52,2014-12-01 13:45:39,closed,,,2,Bug;Duplicate,https://api.github.com/repos/pydata/pandas/issues/8951,b'`DataFrame.iterrows()` breaks timezone on index',"b'Duplicate of #8890. \r\n\r\nAs far as I can tell, the Timestamps for the index generated by `iterrows()` are 5 hours behind where they should be in this example:\r\n\r\n```python\r\nIn [33]: idx = pd.date_range(""2010-01-01 00:00:00-0500"", freq=\'D\', periods=3)\r\n\r\nIn [34]: df = pd.DataFrame([1,2,3], index=[idx])\r\n\r\nIn [35]: df # this looks correct\r\nOut[35]: \r\n                           0\r\n2010-01-01 00:00:00-05:00  1\r\n2010-01-02 00:00:00-05:00  2\r\n2010-01-03 00:00:00-05:00  3\r\n\r\nIn [36]: [index for index, row in df.iterrows()] # but this looks wrong:\r\nOut[36]: \r\n[Timestamp(\'2009-12-31 19:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\'),\r\n Timestamp(\'2010-01-01 19:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\'),\r\n Timestamp(\'2010-01-02 19:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\')]\r\n```\r\n\r\n I would have expected `iterrows()` to produce the same indices as this code:\r\n\r\n```python \r\nIn [38]: [df.index[i] for i in range(len(df))]\r\nOut[38]: \r\n[Timestamp(\'2010-01-01 00:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\'),\r\n Timestamp(\'2010-01-02 00:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\'),\r\n Timestamp(\'2010-01-03 00:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\')]\r\n```\r\n\r\nThe `row.name` is also incorrect:\r\n\r\n```python\r\nIn [37]: [row.name for index, row in df.iterrows()]\r\nOut[37]: \r\n[Timestamp(\'2009-12-31 19:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\'),\r\n Timestamp(\'2010-01-01 19:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\'),\r\n Timestamp(\'2010-01-02 19:00:00-0500\', tz=\'pytz.FixedOffset(-300)\', offset=\'D\')]\r\n```\r\n\r\nBut all is fine if we use a geographical timezone instead of a `pytz.FixedOffset`: \r\n\r\n```python\r\n\r\nIn [47]: idx = pd.date_range(""2010-01-01 00:00:00"", freq=\'D\', periods=3, tz=""America/New_York"")\r\n\r\nIn [48]: df = pd.DataFrame([1,2,3], index=[idx])\r\n\r\nIn [49]: [index for index, row in df.iterrows()]\r\nOut[49]: \r\n[Timestamp(\'2010-01-01 00:00:00-0456\', tz=\'America/New_York\', offset=\'D\'),\r\n Timestamp(\'2010-01-02 00:00:00-0456\', tz=\'America/New_York\', offset=\'D\'),\r\n Timestamp(\'2010-01-03 00:00:00-0456\', tz=\'America/New_York\', offset=\'D\')]\r\n\r\nIn [50]: df\r\nOut[50]: \r\n                           0\r\n2010-01-01 00:00:00-04:56  1\r\n2010-01-02 00:00:00-04:56  2\r\n2010-01-03 00:00:00-04:56  3\r\n```\r\n\r\nForgive me if I am using Pandas incorrectly!\r\n\r\nVersions:\r\n```\r\n\r\nIn [51]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-25-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.8.2\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.1\r\nsphinx: 1.2.3\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.10\r\nbottleneck: 0.6.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.2\r\nopenpyxl: 1.8.6\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.6\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: None\r\nrpy2: 2.3.8\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\n(it goes without saying that I\'m a huge fan of Pandas!)'"
8948,50473041,behzadnouri,jreback,2014-11-30 22:20:32,2014-12-01 12:54:02,2014-12-01 00:46:44,closed,,0.15.2,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8948,b'BUG: preserve left frame order in left merge',"b""closes https://github.com/pydata/pandas/issues/7331\r\n\r\non master:\r\n\r\n    >>> left\r\n          dates states\r\n    0  20140101     CA\r\n    1  20140102     NY\r\n    2  20140103     CA\r\n    >>> right\r\n       stateid states\r\n    0        1     CA\r\n    1        2     NY\r\n    >>> pd.merge(left, right, how='left', on='states', sort=False)\r\n          dates states  stateid\r\n    0  20140101     CA        1\r\n    1  20140103     CA        1\r\n    2  20140102     NY        2\r\n`DataFrame.join` already works fine:\r\n\r\n    >>> left.join(right.set_index('states'), on='states', how='left')\r\n          dates states  stateid\r\n    0  20140101     CA        1\r\n    1  20140102     NY        2\r\n    2  20140103     CA        1\r\n\r\non branch:\r\n\r\n    >>> pd.merge(left, right, how='left', on='states', sort=False)\r\n          dates states  stateid\r\n    0  20140101     CA        1\r\n    1  20140102     NY        2\r\n    2  20140103     CA        1\r\n\r\n    >>> pd.merge(left, right, how='left', on='states', sort=True)\r\n          dates states  stateid\r\n    0  20140101     CA        1\r\n    1  20140103     CA        1\r\n    2  20140102     NY        2\r\n"""
8946,50461695,jreback,jreback,2014-11-30 16:00:05,2014-12-05 15:46:03,2014-12-04 11:06:12,closed,,0.15.2,13,API Design;Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8946,b'API: Allow equality comparisons of Series with a categorical dtype and object type are allowed (GH8938)',b'closes #8938 '
8945,50461266,papaloizouc,jorisvandenbossche,2014-11-30 15:45:34,2015-05-01 16:09:31,2015-05-01 16:08:58,closed,,No action,14,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8945,b'BUG: Fixed plot label shows as None. #8905',b'Fixes https://github.com/pydata/pandas/issues/8905'
8941,50457958,hkleynhans,jorisvandenbossche,2014-11-30 13:33:57,2014-11-30 23:21:01,2014-11-30 23:21:01,closed,,0.15.2,6,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/8941,b'BUG: Resample across multiple days',b'Fixes an issue where resampling over multiple days causes a ValueError\r\nwhen a number of days between the normalized first and normalized last\r\ndays is not a multiple of the frequency.\r\n\r\nAdded test TestSeries.test_resample\r\n\r\nCloses #8683'
8929,50430351,aevri,jreback,2014-11-29 16:32:11,2014-12-03 22:38:46,2014-12-03 22:33:53,closed,,0.15.2,10,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8929,b'BUG: allow numpy.array as c values to scatterplot',"b""Ensure that we can pass an np.array as 'c' straight through to\r\nmatplotlib, this functionality was accidentally removed previously.\r\n\r\nAdd tests.\r\n\r\nCloses #8852"""
8927,50426515,papaloizouc,jreback,2014-11-29 14:03:47,2014-12-03 22:37:49,2014-12-03 22:37:35,closed,,0.15.2,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8927,b'BUG: fixed chunksize guessed to 0 (py3 only). #8621',b'Fixes https://github.com/pydata/pandas/issues/8621\r\nAs mentioned in the github issue its a py3 only bug. I copied the test @rmorgans made'
8916,50342468,shoyer,shoyer,2014-11-28 05:37:15,2014-12-09 23:27:02,2014-12-09 23:26:55,closed,,,9,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8916,b'BUG/ENH: cleanup for Timestamp arithmetic',"b""Fixes #8865 (Timestamp - Timestamp -> Timedelta)\r\n\r\nThis PR cleans up and extends `Timestamp` arithmetic similarly to my treatment for `Timedelta` in #8884.\r\n\r\nIt includes a new `to_datetime64()` method, and arithmetic now works between Timestamp and ndarrays. I also ensure comparison operations work properly between all of (Timestamp, Timedelta, NaT) and ndarrays.\r\n\r\nImplementation notes: wide use of the `NotImplemented` singleton let me cleanup some complex logic. I also strove to reduce the tight-coupling of `Timestamp`/`Timedelta` to pandas itself by removing use of the `_typ` property in tslib (I honestly don't quite understand why it needs to exist) and by not treating series/index any differently from any other ndarray-like objects.\r\n\r\nCC @jreback @jorisvandenbossche @immerrr"""
8907,50299198,behzadnouri,jreback,2014-11-27 14:54:46,2014-11-30 15:11:42,2014-11-29 22:03:08,closed,,0.15.2,12,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8907,b'BUG: DatetimeIndex with time object as key',"b""closes https://github.com/pydata/pandas/issues/8667\r\non master:\r\n\r\n    >>> from datetime import time\r\n    >>> from pandas.index import _SIZE_CUTOFF\r\n    >>> n = _SIZE_CUTOFF + 100\r\n    >>> idx = pd.date_range('2014-11-26', periods=n, freq='S')\r\n    >>> ts = pd.Series(np.random.randn(n), index=idx)\r\n    >>> key = time(15, 0)\r\n    >>> ts[key]\r\n    TypeError: 'datetime.time' object is not iterable\r\n    >>> ts.index.get_loc(key)\r\n    TypeError: unorderable types: int() > datetime.time()\r\n\r\nabove would work on master branch if `n` was smaller than `_SIZE_CUTOFF`. \r\n`_SIZE_CUTOFF` is set [here](https://github.com/pydata/pandas/blob/e5fe75e8d40e6b94eabf54882a17ab05341c9929/pandas/index.pyx#L68)"""
8905,50255404,micramm,jorisvandenbossche,2014-11-27 05:24:43,2015-05-01 16:05:38,2015-05-01 16:05:32,closed,,0.16.1,4,Bug;Difficulty Novice;Visualization,https://api.github.com/repos/pydata/pandas/issues/8905,b'Plot label None in line plot',"b""The plot label appears when plotting a dataframe as a scatter plot but not as a line plot. Is this expected?\r\n\r\n```python\r\ndf = pd.DataFrame(np.random.rand(2, 4), columns=['a', 'b', 'c', 'd'])\r\nax = df.plot(kind = 'scatter', x='a', y='b',color='DarkBlue', label='Label')\r\nax = df.plot(kind = 'line', x='a', y='b',color='DarkBlue', label='Label')\r\n```\r\n![image](https://cloud.githubusercontent.com/assets/885310/5212582/d59c7ec6-75b0-11e4-8f68-33a2b507e953.png)\r\n![image](https://cloud.githubusercontent.com/assets/885310/5212583/db1533b6-75b0-11e4-9c77-b8e34bac6abd.png)\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.0.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: 1.3.1\r\nCython: 0.21.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.1\r\nsphinx: 1.2.3\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.9\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```"""
8890,49945943,Oxtay,jreback,2014-11-24 21:39:18,2014-12-04 02:47:31,2014-12-04 02:47:31,closed,,0.15.2,7,Bug;Regression;Timezones,https://api.github.com/repos/pydata/pandas/issues/8890,b'BUG: iterator of DatetimeIndex broken with tzoffset timezone',"b'**Summary:**\r\n\r\nThe trigger is the `tzoffset` timezone. This bug can be reproduced as follows:\r\n\r\n```\r\nIn [86]: index = pd.date_range(""2012-01-01"", periods=3, freq=\'H\', tz=dateutil.tz.tzoffset(None, -28800))\r\n\r\nIn [87]: index\r\nOut[87]:\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2012-01-01 00:00:00-08:00, ..., 2012-01-01 02:00:00-08:00]\r\nLength: 3, Freq: H, Timezone: tzoffset(None, -28800)\r\n\r\nIn [88]: index[0]\r\nOut[88]: Timestamp(\'2012-01-01 00:00:00-0800\', tz=\'tzoffset(None, -28800)\', offset=\'H\')\r\n\r\nIn [90]: list(iter(index))[0]\r\nOut[90]: Timestamp(\'2011-12-31 16:00:00-0800\', tz=\'tzoffset(None, -28800)\', offset=\'H\')\r\n\r\nIn [91]: list(iter(index))[0] == index[0]\r\nOut[91]: False\r\n\r\n```\r\nIn 0.14 this last comparison gives True.\r\n\r\nThis appears in iterating over the index (`for time in index: ...`) or with using `DataFrame.iterrows()` (#8951).\r\n\r\n\r\n---\r\n**Original report**:\r\n\r\nI see there are a number of issues related to datetime and timeindex here in issues, and I suspect that mine has a lot in common with them. The cure for one of them will probably solve all of them. So here it goes.\r\n\r\nMy code was using Pandas 0.13.1 without an issue. I recently upgraded to 0.15.1\r\nThis is where my code acts unexpectedly:\r\n\r\n    time_points = df.index[df[\'candidate\'] == 1]\r\n            for time in time_points:\r\n                [...]\r\n\r\nThe index is in US/Pacific timezone. When the for loop returns `time`, it is still in US/Pacific timezone but with an added 8h to the time. So, while the actual time is `2014-11-23 23:25:02.916000-08:00`, `time` is set to `2014-11-23 15:49:12.972000-08:00`. \r\nI have made sure that the type of index is `pandas.Timestamp` and I can\'t find an elegant workaround that would ensure running it in both versions of Pandas.\r\n\r\nAny thoughts on this?'"
8884,49856928,shoyer,jreback,2014-11-24 05:08:33,2014-11-26 10:45:31,2014-11-26 02:30:02,closed,,0.15.2,8,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8884,b'BUG/ENH: cleanup for Timedelta arithmetic',"b""Fixes #8813 \r\nFixes #5963\r\nFixes #5436\r\n\r\nIf the other argument has a dtype attribute, I assume that it is ndarray-like\r\nand convert the `Timedelta` into a `np.timedelta64` object. Alternatively, we\r\ncould just return `NotImplemented` and let the other type handle it, but this\r\nhas the bonus of making `Timedelta` compatible with ndarrays.\r\n\r\nI also added a `Timedelta.to_timedelta64()` method to the public API. I\r\ncouldn't find a listing for `Timedelta` in the API docs -- we should probably\r\nadd that, right?\r\n\r\nNext up would be a similar treatment for `Timestamp`.\r\n\r\nCC @immerrr"""
8874,49740597,bashtage,jreback,2014-11-21 19:49:36,2015-06-12 21:50:02,2015-06-12 21:50:02,closed,,0.16.2,8,Bug;Compat;Error Reporting;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/8874,"b""HDF corrupts data when using complib='blosc:zlib'""","b""I'm not sure if this is supported or not -- it isn't in the doc string for HDFStore, but it seems to be allowed by the HDFStore (nothing is raised).\r\n\r\nUnfortunately so far I can only get it to show the bad behavior on a proprietary dataset, which is storing a `pd.Panel` which contains items of of mixed types.  \r\n\r\nSome of the float values are be changed from small (|x|<1.0) to very large (3.xe+308).\r\n\r\nIs the compressor just passed through to pytables? If so, this might be a pytables issue.\r\n\r\n```\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.23.3.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.1-24-g54e237b\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.7.0.dev-c8e980d\r\nIPython: 2.3.1\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 1.5\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n```\r\n"""
8866,49633830,jreback,jreback,2014-11-20 23:41:52,2014-12-05 02:34:01,2014-12-05 02:34:01,closed,,0.15.2,1,Bug;Difficulty Novice;Groupby,https://api.github.com/repos/pydata/pandas/issues/8866,b'BUG: pd.Grouper specification broken for non-datetimelike when level specified',"b""\r\n```\r\nIn [15]: s = Series(np.arange(8),index=pd.MultiIndex.from_product([list('ab'),range(2),pd.date_range('20130101',periods=2)],names=['one','two','three']))\r\n\r\nIn [16]: s\r\nOut[16]: \r\none  two  three     \r\na    0    2013-01-01    0\r\n          2013-01-02    1\r\n     1    2013-01-01    2\r\n          2013-01-02    3\r\nb    0    2013-01-01    4\r\n          2013-01-02    5\r\n     1    2013-01-01    6\r\n          2013-01-02    7\r\ndtype: int64\r\n\r\n# datetimelikes work\r\nIn [17]: s.groupby(pd.Grouper(level='three',freq='M')).sum()\r\nOut[17]: \r\nthree\r\n2013-01-31    28\r\nFreq: M, dtype: int64\r\n\r\n# just specifying a level breaks\r\nIn [18]: s.groupby(pd.Grouper(level='one')).sum()\r\nValueError: No axis named None for object type <class 'pandas.core.series.Series'>\r\n```"""
8865,49607297,shoyer,jreback,2014-11-20 19:26:54,2014-12-04 02:48:19,2014-12-04 02:48:19,closed,,0.15.2,1,Bug;Dtypes;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8865,"b'BUG: Timestamp - Timestamp should be pd.Timedelta, not datetime.timedelta'","b""Currently it returns a `datetime.timedelta` object:\r\n```\r\n>>> pd.Timestamp('2000-01-01') - pd.Timestamp('1999-01-01')\r\ndatetime.timedelta(365)\r\n```\r\n\r\nSomewhat related: #8813"""
8855,49340728,seth-p,jreback,2014-11-19 07:09:27,2014-12-02 20:13:13,2014-12-02 11:14:42,closed,,0.15.2,7,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8855,"b'BUG: DataFrame.stack(..., dropna=False) with partial MultiIndex.'","b'Closes #8844\r\n\r\nFixes ``DataFrame.stack(..., dropna=False)`` when the columns consist of a ""partial"" ``MultiIndex``, i.e. one in which the ``labels`` don\'t reference all the ``levels``.'"
8854,49334475,stahlous,jreback,2014-11-19 06:02:11,2015-05-09 16:06:55,2015-05-09 16:06:55,closed,,Next Major Release,2,Bug;Compat;Indexing,https://api.github.com/repos/pydata/pandas/issues/8854,b'BUG: Panel4D setitem by indexer to Series fails for mixed-dtype',b'This is another tweak to `_setitem_with_indexer` that allows it better to comprehend higher dimensional frames of mixed-dtype. With this tweak setting by indexer to a Series now works. Unfortunately setting to a DataFrame is still broken. I plan to open an issue for that.'
8853,49323281,behzadnouri,jreback,2014-11-19 03:07:16,2014-11-21 01:00:59,2014-11-20 22:55:08,closed,,0.15.2,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8853,b'BUG: type change breaks BlockManager integrity',"b'closes https://github.com/pydata/pandas/issues/8850\r\n\r\non master:\r\n\r\n    >>> cols = MultiIndex.from_tuples([(\'1st\', \'a\'), (\'2nd\', \'b\'), (\'3rd\', \'c\')])\r\n    >>> df = DataFrame([[1.0, 2, 3], [4.0, 5, 6]], columns=cols)\r\n    >>> df[\'2nd\'] = df[\'2nd\'] * 2.0  # type change in block manager\r\n    /usr/lib/python3.4/site-packages/numpy/lib/function_base.py:3612: FutureWarning: in the future negative indices will not be ignored by `numpy.delete`.\r\n      ""`numpy.delete`."", FutureWarning)\r\n    >>> df.values\r\n    ...\r\n      File ""/usr/lib/python3.4/site-packages/pandas-0.15.1_72_gf504885-py3.4-linux-x86_64.egg/pandas/core/internals.py"", line 2392, in _verify_integrity\r\n        tot_items))\r\n    AssertionError: Number of manager items must equal union of block items\r\n    # manager items: 3, # tot_items: 4\r\n    >>> df.blocks\r\n    ...\r\n      File ""/usr/lib/python3.4/site-packages/pandas-0.15.1_72_gf504885-py3.4-linux-x86_64.egg/pandas/core/internals.py"", line 2392, in _verify_integrity\r\n        tot_items))\r\n    AssertionError: Number of manager items must equal union of block items\r\n    # manager items: 3, # tot_items: 4\r\n`._data` is also broken:\r\n\r\n    >>> df._data\r\n    BlockManager\r\n    Items:       \r\n    1st  a\r\n    2nd  b\r\n    3rd  c\r\n    Axis 1: Int64Index([0, 1], dtype=\'int64\')\r\n    FloatBlock: slice(0, 1, 1), 1 x 2, dtype: float64\r\n    IntBlock: slice(1, 3, 1), 2 x 2, dtype: int64\r\n    FloatBlock: slice(1, 2, 1), 1 x 2, dtype: float64\r\ninteger block is bigger than what it should be and overlaps with one of the float blocks.'"
8852,49305593,TomAugspurger,jreback,2014-11-18 22:59:04,2014-12-03 22:33:53,2014-12-03 22:33:53,closed,,0.15.2,16,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8852,b'BUG: plot with kind=scatter fails when checking if an array is in the DataFrame',"b""```python\r\ndf = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\r\nc = np.array([1, 0])\r\ndf.plot(kind='scatter', x='A', y='B', c=c, cmap='spring')\r\n```\r\n\r\nfails with a TypeError.\r\n\r\nwe check `elif c in self.data.columns` which tries to hash the array.\r\n\r\n(I know I can just add the array as a column to the DataFrame)\r\n"""
8850,49239816,kuba59,jreback,2014-11-18 14:48:27,2014-11-20 22:55:08,2014-11-20 22:55:08,closed,,0.15.2,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8850,b'BUG: Accessing DataFrame multi-index column *seems* to modify its content',"b'In the example below, ```[13]``` gives a different result than ```[11]```, even though you would think that ```[12]``` has no effect on data.\r\n\r\n```\r\nPython 2.7.8 (default, Oct 19 2014, 16:02:00) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: d = {\'COLA\': {0: 0, 1: 0, 2: 0},\r\n   ...:      \'COLB\': {0: \'a\', 1: \'a\', 2: \'a\'},\r\n   ...:      \'COLC\': {0: 0, 1: 0, 2: 1}}\r\n\r\nIn [4]: df = pd.DataFrame(d)\r\n\r\nIn [5]: df\r\nOut[5]: \r\n   COLA COLB  COLC\r\n0     0    a     0\r\n1     0    a     0\r\n2     0    a     1\r\n\r\nIn [6]: df = df.groupby([\'COLA\',\'COLB\'])[\'COLC\']\\\r\n   ...:         .agg({\'Zeros\': lambda x: 0,\r\n   ...:               \'Averages\': lambda x: 100.*x.mean(),\r\n   ...:               \'Weird_stuff\': np.size})\\\r\n   ...:     .unstack()\r\n\r\nIn [7]: df\r\nOut[7]: \r\n       Averages Weird_stuff Zeros\r\nCOLB          a           a     a\r\nCOLA                             \r\n0     33.333333           3     0\r\n\r\nIn [8]: df.columns\r\nOut[8]: \r\nMultiIndex(levels=[[u\'Averages\', u\'Weird_stuff\', u\'Zeros\'], [u\'a\']],\r\n           labels=[[0, 1, 2], [0, 0, 0]],\r\n           names=[None, u\'COLB\'])\r\n\r\nIn [9]: df.index\r\nOut[9]: Int64Index([0], dtype=\'int64\')\r\n\r\nIn [10]: df[\'Weird_stuff\'] = df[\'Weird_stuff\'].apply(lambda x: 1000000., axis=1)\r\n/usr/local/lib/python2.7/site-packages/numpy/lib/function_base.py:3612: FutureWarning: in the future negative indices will not be ignored by `numpy.delete`.\r\n  ""`numpy.delete`."", FutureWarning)\r\n\r\nIn [11]: df\r\nOut[11]: \r\n       Averages Weird_stuff Zeros\r\nCOLB          a           a     a\r\nCOLA                             \r\n0     33.333333     1000000     0\r\n\r\nIn [12]: df[\'Zeros\']\r\nOut[12]: \r\nCOLB  a\r\nCOLA   \r\n0     0\r\n\r\nIn [13]: df # Look the value below (\'Weird_stuf\', \'a\') here and above.\r\nOut[13]: \r\n       Averages Weird_stuff Zeros\r\nCOLB          a           a     a\r\nCOLA                             \r\n0     33.333333           3     0\r\n\r\nIn [14]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.0.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: pl_PL.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: 1.3.0\r\nCython: 0.20.2\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\ndateutil: 2.2\r\npytz: 2014.7\r\nbottleneck: 0.8.0\r\ntables: 3.1.0\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.2\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.9\r\napiclient: 1.2\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None\r\n```'"
8847,49209012,jreback,jreback,2014-11-18 11:09:54,2014-11-18 11:56:11,2014-11-18 11:56:11,closed,,0.15.2,0,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/8847,b'BUG: Defined .size attribute across NDFrame objects to provide compat with numpy 1.9.1',b'closes #8846 '
8846,49196328,yemu,jreback,2014-11-18 08:56:24,2014-11-18 11:56:11,2014-11-18 11:56:11,closed,,0.15.2,2,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/8846,b'splitting pandas dataframe - np.array_split error',"b'I just noticed that after upgrading to numpy 1.9.0, when I\'m trying to split dataframe with pandas 0.15.1 with the code:\r\n\r\n    split_dfs = np.array_split(big_df,8)    \r\n\r\nI get the error:\r\n\r\n    Traceback (most recent call last):\r\n      File ""./test.py"", line 127, in <module>\r\n        split_dfs = np.array_split(big_df,8)\r\n      File ""/usr/lib/python2.7/site-packages/numpy/lib/shape_base.py"", line 426, in array_split\r\n        if sub_arys[-1].size == 0 and sub_arys[-1].ndim != 1:\r\n      File ""/usr/lib/python2.7/site-packages/pandas-0.15.1-py2.7-linux-x86_64.egg/pandas   /core/generic.py"", line 1936, in __getattr__\r\n    (type(self).__name__, name))\r\n    AttributeError: \'DataFrame\' object has no attribute \'size\'\r\n\r\nwith pandas 0.15.1 and numpy 1.8.1 it works fine.\r\nI\'m using pandas 0.15.1 on arch linux and python2.7'"
8844,49155022,seth-p,jreback,2014-11-17 22:34:21,2014-12-02 11:14:42,2014-12-02 11:14:42,closed,,0.15.2,12,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8844,b'BUG: .stack(dropna=False) looks through views incorrectly for dataframe views with multi-index columns',"b'xref #8850\r\n \r\nIn the example below, ``[11]`` is incorrectly reflecting columns in ``dfa`` that should not be visible to ``dfa1``. Note that this is not a problem when the columns are not a multi-index (``[5]`` and ``[6]``), or when ``dropna=True`` (the default; ``[10]`` and ``[12]``).\r\n```\r\nPython 3.4.2 (v3.4.2:ab2c023a9432, Oct  6 2014, 22:16:31) [MSC v.1600 64 bit (AMD64)]\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.3.1 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: df = pd.DataFrame(np.zeros((2,5)), columns=list(\'ABCDE\'))\r\n\r\nIn [4]: df1 = df[df.columns[:2]]\r\n\r\nIn [5]: df1.stack()\r\nOut[5]:\r\n0  A    0\r\n   B    0\r\n1  A    0\r\n   B    0\r\ndtype: float64\r\n\r\nIn [6]: df1.stack(dropna=False)\r\nOut[6]:\r\n0  A    0\r\n   B    0\r\n1  A    0\r\n   B    0\r\ndtype: float64\r\n\r\nIn [7]: dfa = pd.DataFrame(np.zeros((2,5)),\r\n                           columns=pd.MultiIndex.from_tuples([(1,\'A\'), (1,\'B\'), (1,\'C\'), (1,\'D\'), (1,\'E\')],\r\n                                                             names=[\'num\', \'let\']))\r\n\r\nIn [8]: dfa1 = dfa[dfa.columns[:2]]\r\n\r\nIn [10]: dfa1.stack()\r\nOut[10]:\r\nnum    1\r\n  let\r\n0 A    0\r\n  B    0\r\n1 A    0\r\n  B    0\r\n\r\nIn [11]: dfa1.stack(dropna=False)\r\nOut[11]:\r\nnum     1\r\n  let\r\n0 A     0\r\n  B     0\r\n  C   NaN\r\n  D   NaN\r\n  E   NaN\r\n1 A     0\r\n  B     0\r\n  C   NaN\r\n  D   NaN\r\n  E   NaN\r\n\r\nIn [12]: dfa1.stack(dropna=True)\r\nOut[12]:\r\nnum    1\r\n  let\r\n0 A    0\r\n  B    0\r\n1 A    0\r\n  B    0\r\n\r\nIn [13]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 8\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.1\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.6.0\r\nIPython: 2.3.1\r\nsphinx: None\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.9\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.6.3\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: None\r\n```'"
8841,49113833,jtratner,jreback,2014-11-17 16:40:39,2015-05-09 16:07:09,2015-05-09 16:07:09,closed,,Next Major Release,14,Bug;IO CSV;Unicode,https://api.github.com/repos/pydata/pandas/issues/8841,b'ENH: Allow unicode separators in to_csv & read_csv',"b""* single byte UTF8 separators will always work if utf8 encoding is specified (i.e. seps in range(256))\r\n* single byte delimiters with encodings will always work in Python 3, but will not work in Python 2 if they are multibyte when coerced to UTF8\r\n* multi byte delimiters with encodings will not work on reads with C engine and will not work with writes in Python 2. (but I believe they'll work in Python 3 if they're considered to be length 1).\r\n\r\nCloses #6035.\r\n\r\nThis was slightly complicated validation-wise, but I think it's in a good place\r\nwith tests and such. I'll squash together the commits and make sure I've\r\ncovered the relevant cases in tests.\r\n\r\nI also noticed some weirdness with passing `delimiter` as a keyword argument that\r\nmight need to be addressed in the future.\r\n\r\n\r\nCC @maxgrenderjones, @Midnighter if you want to take a look."""
8834,49012107,jreback,jreback,2014-11-16 19:23:30,2014-11-17 12:54:59,2014-11-17 12:54:59,closed,,0.15.2,4,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8834,b'BUG: Bug in csv parsing when passing dtype and names and the parsed data is a different data type (GH8833)',"b'closes #8833 \r\n\r\n```\r\nIn [5]: data = """"""1.0,a\r\n   ...: nan,b\r\n   ...: 3.0,c\r\n   ...: """"""\r\n\r\nIn [8]: read_csv(StringIO(data),sep=\',\',names=[\'A\',\'B\'])\r\nOut[8]: \r\n    A  B\r\n0   1  a\r\n1 NaN  b\r\n2   3  c\r\n\r\nIn [9]: read_csv(StringIO(data),sep=\',\',names=[\'A\',\'B\'],dtype={\'A\' : int})\r\nValueError: cannot safely convert passed user dtype of <i8 for float64 dtyped data in column 0\r\n```'"
8833,48990493,jreback,jreback,2014-11-16 14:34:48,2014-11-17 12:54:59,2014-11-17 12:54:59,closed,,0.15.2,1,Bug;Error Reporting;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8833,b'BUG: unhelpful parser exception when passing names and dtype',"b'```\r\nIn [1]: data = """"""1.0 1\r\n   ...: 2.0 2\r\n   ...: 3.0 3""""""\r\n\r\nIn [2]: read_csv(StringIO(data),sep=\'\\s+\')\r\nOut[2]: \r\n   1.0  1\r\n0    2  2\r\n1    3  3\r\n\r\nIn [3]: read_csv(StringIO(data),sep=\'\\s+\',header=None)\r\nOut[3]: \r\n   0  1\r\n0  1  1\r\n1  2  2\r\n2  3  3\r\n\r\nIn [4]: read_csv(StringIO(data),sep=\'\\s+\',header=None,names=[\'a\',\'b\'])\r\nOut[4]: \r\n   a  b\r\n0  1  1\r\n1  2  2\r\n2  3  3\r\n\r\nIn [6]: read_csv(StringIO(data),sep=\'\\s+\',header=None,names=[\'a\',\'b\'],dtype={\'a\' : int})\r\n\'NoneType\' object has no attribute \'dtype\'\r\n```'"
8828,48946608,onesandzeroes,jreback,2014-11-16 05:01:57,2015-12-09 15:18:25,2015-12-09 15:18:25,closed,,Next Major Release,15,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/8828,b'BUG: query modifies the frame when you compare with `=`',"b""Fix for #8664. The simplest way to fix this involved adding an `assignment_allowed=True` arg to the internal `eval()` function. All existing behaviour should be preserved. If adding the arg isn't okay I'm not quite sure how else to do it, as it's only once we reach the internal eval function that the expression is actually parsed and we know that it includes the assignment.\r\n\r\nIt also seems like a pretty bad side effect if query can silently overwrite values (obviously only if you accidentally include an assignment), so hopefully that's a good argument in favour of having the\r\n`assignment_allowed` arg."""
8822,48860995,artemyk,jreback,2014-11-15 01:33:18,2015-04-11 18:35:21,2015-04-11 18:35:12,closed,,0.16.1,19,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/8822,b'BUG: Error when creating sparse dataframe with nan column label',"b""Right now the following raises an exception:\r\n```\r\nfrom pandas import DataFrame, Series\r\nfrom numpy import nan\r\nnan_colname = DataFrame(Series(1.0,index=[0]),columns=[nan])\r\nnan_colname_sparse = nan_colname.to_sparse()\r\n```\r\n\r\nThis is because sparse dataframes use a dictionary to store information about columns, with the column label as the key.  `nan`'s do not equal themselves and create problems as dictionary keys. This avoids the issue by uses a dataframe to store this information."""
8818,48805437,8one6,jreback,2014-11-14 15:59:40,2014-11-14 16:02:16,2014-11-14 16:02:07,closed,,,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8818,b'BUG: retbins options not working with cut',"b'```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ns = pd.Series(np.random.randn(100))\r\npd.cut(s, 10, retbins=True)\r\n```\r\n\r\nThis throws \r\n```\r\nValueError: Wrong number of items passed 2, placement implies 100\r\n```\r\n\r\nAm I doing something obviously wrong?'"
8817,48799419,joshpurvis,jreback,2014-11-14 15:08:10,2014-11-14 15:13:42,2014-11-14 15:13:21,closed,,,2,Bug;Duplicate;Timezones,https://api.github.com/repos/pydata/pandas/issues/8817,b'Unexpected shift() and tshift() behavior with StaticTzInfo datetimes',"b""When shifting a DateTimeIndex with a StaticTzInfo (namely EST in my case), I'm seeing odd behavior in which it first seems to be shifting -5 (the offset for EST), and then shifting relative to that:\r\n\r\n```python\r\nimport pandas as pd\r\nimport pytz\r\nimport datetime\r\n\r\ndt = datetime.datetime(2014, 11, 14, 0)\r\ndt_est = pytz.timezone('EST').localize(dt)\r\ns = pd.Series(data=[1], index=[dt_est])\r\n\r\ns.shift(0, freq='h')  # 2014-11-14 00:00:00-05:00 (seems okay) \r\ns.shift(-1, freq='h')  # 2014-11-13 18:00:00-05:00 (expected 2014-11-13 23:00:00)\r\ns.shift(1, freq='h')  # 2014-11-13 20:00:00-05:00 (expected 2014-11-14 01:00:00)\r\n\r\ns.shift(-1, freq='s')  # 2014-11-13 18:59:59-05:00 (same with other freq)\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-39-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.0\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.0\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.0\r\nsphinx: None\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.0\r\nopenpyxl: 1.8.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```"""
8813,48729062,shoyer,jreback,2014-11-14 04:23:32,2014-11-26 02:30:02,2014-11-26 02:30:02,closed,,0.15.2,17,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8813,b'BUG: Timedelta * Series -> TypeError',"b""I encountered this today when rerunning a script that worked prior to the 0.15 arrival of Timedelta:\r\n```\r\nIn [2]: pd.to_timedelta(10, unit='D') * pd.Series(range(10))\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-a5e3bc3be480> in <module>()\r\n----> 1 pd.to_timedelta(10, unit='D') * pd.Series(range(10))\r\n\r\n/Users/shoyer/dev/pandas/pandas/tslib.so in pandas.tslib.Timedelta.__mul__ (pandas/tslib.c:31355)()\r\n\r\nTypeError: cannot multiply a Timedelta with <class 'pandas.core.series.Series'>\r\n```\r\n\r\n@jreback I think there might be another issue for this already?"""
8804,48666244,TomAugspurger,TomAugspurger,2014-11-13 17:32:54,2014-11-26 13:47:28,2014-11-20 15:20:53,closed,,,13,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8804,b'BUG: index union between Int64 and non orderable raise AttributeError',"b""```python\r\nx = pd.Index([1, 2, 3])\r\ny = pd.Index(['a', 'b', 'c'])\r\nx.union(y)\r\n```\r\n\r\nSorry no stack trace (on a remote server). Raises with an AttributeError and a message about no attribute is_monotonic_increasing. This is on 0.15.1\r\n\r\nI think I saw a thing in the release notes about adding that property in 0.15.1 and I can confirm that the example worked in 0.15.0."""
8795,48513243,jorisvandenbossche,jreback,2014-11-12 14:04:22,2014-12-05 02:34:01,2014-12-05 02:34:01,closed,,0.15.2,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8795,"b""BUG: Grouper(key='A') gives AttributeError when applying function""","b""I was playing a bit with `pd.Grouper`. Shouldn't the following work:\r\n\r\n```\r\nIn [109]: df = pd.DataFrame({'A':[0,0,1,1,2,2], 'B':[1,2,3,4,5,6]})\r\n\r\nIn [110]: df\r\nOut[110]:\r\n   A  B\r\n0  0  1\r\n1  0  2\r\n2  1  3\r\n3  1  4\r\n4  2  5\r\n5  2  6\r\n\r\nIn [111]: df.groupby(pd.Grouper(key='A')).sum()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-111-b9e051aabea3> in <module>()\r\n----> 1 df.groupby(pd.Grouper(key='A')).sum()\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\core\\groupby.pyc in f(self)\r\n    108             raise SpecificationError(str(e))\r\n    109         except Exception:\r\n--> 110             result = self.aggregate(lambda x: npfunc(x, axis=self.axis))\r\n\r\n    111             if _convert:\r\n    112                 result = result.convert_objects()\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\core\\groupby.pyc in aggregate(self, arg, *a\r\nrgs, **kwargs)\r\n   2634                 return getattr(self, cyfunc)()\r\n   2635\r\n-> 2636             if self.grouper.nkeys > 1:\r\n   2637                 return self._python_agg_general(arg, *args, **kwargs)\r\n   2638             else:\r\n\r\nAttributeError: 'Int64Index' object has no attribute 'nkeys'\r\n```\r\n\r\n\r\n\r\n"""
8792,48453331,davidastephens,jreback,2014-11-12 00:12:57,2014-11-14 12:49:41,2014-11-14 12:49:34,closed,,0.15.2,4,Bug;Data Reader;Dtypes;IO Google;Testing,https://api.github.com/repos/pydata/pandas/issues/8792,b'TST: Add test for get_data_google dtypes',b'closes #3995 '
8789,48432343,victorpoluceno,jreback,2014-11-11 20:42:15,2014-11-11 22:58:46,2014-11-11 22:31:14,closed,,,4,Bug;Duplicate;Resample,https://api.github.com/repos/pydata/pandas/issues/8789,b'Weird behavior on group by TimeGrouper followed by agg ',"b""I'm having problems with `groupby` with `TimeGrouper` followed by agg when `DataFrame` has two lines with unique values.\r\n\r\n```python\r\nfrom datetime import datetime\r\n \r\nimport pandas as pd\r\nimport numpy as np\r\n \r\nresult = np.empty(3, dtype=[('name', 'object'),\r\n                            ('value', 'int64'),\r\n                            ('date', 'datetime64[us]')])\r\n# doesn't work\r\nresult[0] = ('a', 1, datetime.now())\r\nresult[1] = ('b', 1, datetime.now())\r\n \r\n# this works\r\n#result[0] = ('a', 1, datetime.now())\r\n#result[1] = ('a', 1, datetime.now())\r\n\r\n# this also works\r\n#result[0] = ('a', 1, datetime.now())\r\n#result[1] = ('b', 1, datetime.now())\r\n#result[2] = ('c', 1, datetime.now())\r\n\r\ndf = pd.DataFrame.from_records(result, index='date')\r\ndf = df[df['name'].map(lambda x: not x is None)] # filter blank series\r\n \r\ngroup_list = ['name', pd.TimeGrouper(freq='%sS' % 60)]\r\ngrouped = df.groupby(group_list)\r\nresult = grouped.agg(['sum', 'count'])\r\nprint result.to_records()\r\n```\r\n\r\nThe main idea is to group by name and date by resample it into 60 seconds intervals. On this example the desired output would be something like this:\r\n\r\n```python\r\n[('a', Timestamp('2014-11-11 20:22:00'), 1, 1)\r\n ('b', Timestamp('2014-11-11 20:22:00'), 1, 1)]\r\n```\r\nBut it returns this:\r\n\r\n```python\r\n[(<pandas.tseries.resample.TimeGrouper object at 0x2fbea10>, 'b', 1, 1, 1)\r\n ('name', 'a', 1, 1, 1)]\r\n```\r\n\r\nSometimes `agg` ignores data if there is nothing to aggregate on, this doesn't seems right to me.\r\n\r\n>>> show_versions()\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.6.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.29.2.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: None\r\nCython: 0.21\r\nnumpy: 1.9.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```"""
8787,48412178,wholmgren,jreback,2014-11-11 17:46:19,2014-11-11 23:37:48,2014-11-11 23:37:43,closed,,0.15.2,2,Bug;Dtypes;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8787,"b'BUG: pd.Timedelta np.int, np.float. fixes #8757'",b'closes #8757 '
8781,48314827,SylvainCorlay,jreback,2014-11-10 21:14:40,2014-11-11 00:12:45,2014-11-11 00:11:42,closed,,0.15.2,8,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8781,b'Allowing empty slice in multi-indices',"b""This solves #8737 in the case where there is at least one boolean indexer. The patch of  #8739 does not allow the following:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nmulti_index = pd.MultiIndex.from_product((['foo', 'bar', 'baz'], ['alpha', 'beta']))\r\ndf = pd.DataFrame(np.random.randn(5, 6), index=range(5), columns=multi_index)\r\ndf = df.sortlevel(0, axis=1)\r\ndf.loc[:, (['foo'], [])]\r\n```"""
8773,48260534,benoitpointet,jreback,2014-11-10 12:43:23,2014-11-11 14:41:39,2014-11-11 14:08:22,closed,,,5,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/8773,b'read_ga bug',"b'Cannot get io.ga to work since update to 14.1 (using 0.15.0 right now):\r\n\r\nTypeError: list indices must be integers, not Index\r\n\r\n```python\r\nimport pandas.io.ga as ga\r\ndf = ga.read_ga(\r\n    property_id  = ""UA-XXX"",\r\n    profile_id   = ""XXX"",\r\n    metrics      = [\'users\'],\r\n    dimensions   = [\'dayOfWeek\',\'hour\'],\r\n    start_date   = ""2014-02-03"",\r\n    end_date     = ""2014-08-03""\r\n)\r\n```'"
8766,48218876,davidastephens,jorisvandenbossche,2014-11-09 23:54:51,2014-12-10 09:07:53,2014-12-10 09:07:40,closed,,0.15.2,10,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8766,b'BUG: fontsize parameter of plot only affects one axis.',"b'fixes #8765 \r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame(np.random.randn(10, 9), index=range(10))\r\ndf.plot(fontsize=7)\r\n```\r\n![fig_fonts_all_same_size](https://cloud.githubusercontent.com/assets/5957850/4969745/86e42902-686b-11e4-959c-e22b544e63e2.png)'"
8765,48217804,davidastephens,jorisvandenbossche,2014-11-09 23:20:25,2014-12-01 00:09:33,2014-12-01 00:09:33,closed,,0.15.2,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8765,b'BUG:  fontsize parameter of plot only affects x-axis.',"b'```python\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame(np.random.randn(10, 9), index=range(10))\r\ndf.plot()\r\n```\r\nResults in this plot (all the same font size):\r\n![figure_with_normal_font](https://cloud.githubusercontent.com/assets/5957850/4969656/a577b85c-6866-11e4-8f5a-9861650cb7ae.png)\r\n```python\r\ndf.plot(fontsize=4)\r\n```\r\nResults in this plot (only the x-axis font size is changed):\r\n![figure_with_small_font](https://cloud.githubusercontent.com/assets/5957850/4969658/c1d949de-6866-11e4-91cb-4495242e45b9.png)\r\n'"
8764,48216833,davidastephens,TomAugspurger,2014-11-09 22:48:58,2014-12-07 15:44:01,2014-12-07 15:42:01,closed,,0.15.2,14,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8764,b'BUG: Fix plots showing multiple sets of  axis labels if index is a timeseries.',b'Fixes #3964\r\n\r\n'
8758,48179429,davidastephens,jreback,2014-11-08 20:14:45,2014-11-10 01:18:32,2014-11-08 23:18:50,closed,,0.15.2,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8758,b'BUG: Fix groupby methods to include *args and **kwds if applicable.',b'Fixes #8733 (and any other bug that would result from trying to send an arbitrary keyword to a groupby method).\r\n\r\n '
8757,48175569,wholmgren,jreback,2014-11-08 18:57:44,2014-11-11 23:37:43,2014-11-11 23:37:43,closed,,0.15.2,8,Bug;Dtypes;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8757,"b'BUG: pd.Timedelta does not accept np.int32, etc.'","b'pd.Timedelta is a great addition, thanks! \r\n\r\nIs it the intended behavior of pd.Timedelta to only accept np.int64, python ints, and python floats? I ran into this when I tried to construct a list of pd.Timedeltas from the np.int32 array returned by pd.DatetimeIndex.dayofyear. \r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> pd.Timedelta(days=np.int64(1))\r\nTimedelta(\'1 days 00:00:00\')\r\n>>> pd.Timedelta(days=np.int32(1))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/tslib.pyx"", line 1638, in pandas.tslib.Timedelta.__new__ (pandas/tslib.c:27693)\r\nValueError: cannot construct a TimeDelta from the passed arguments, allowed keywords are [days, seconds, microseconds, milliseconds, minutes, hours, weeks]\r\n>>> pd.Timedelta(days=np.int16(1))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/tslib.pyx"", line 1638, in pandas.tslib.Timedelta.__new__ (pandas/tslib.c:27693)\r\nValueError: cannot construct a TimeDelta from the passed arguments, allowed keywords are [days, seconds, microseconds, milliseconds, minutes, hours, weeks]\r\n>>> pd.Timedelta(days=np.float64(1))\r\nTimedelta(\'1 days 00:00:00\')\r\n>>> pd.Timedelta(days=np.float32(1))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/tslib.pyx"", line 1638, in pandas.tslib.Timedelta.__new__ (pandas/tslib.c:27693)\r\nValueError: cannot construct a TimeDelta from the passed arguments, allowed keywords are [days, seconds, microseconds, milliseconds, minutes, hours, weeks]\r\n>>> pd.Timedelta(days=1)\r\nTimedelta(\'1 days 00:00:00\')\r\n>>> pd.Timedelta(days=1.0)\r\nTimedelta(\'1 days 00:00:00\')\r\n```\r\n\r\nHere\'s my version info:\r\n```python\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.17.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.1\r\nnose: 1.3.4\r\nCython: 0.21\r\nnumpy: 1.9.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.0\r\nsphinx: 1.2.3\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.9\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: 0.6.2.None\r\npsycopg2: None\r\n```'"
8754,48170392,jorisvandenbossche,jorisvandenbossche,2014-11-08 16:26:11,2014-11-18 10:14:22,2014-11-18 10:14:22,closed,,0.15.2,0,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/8754,b'BUG: step argument in the slice StringMethod not working',"b""The `step` argument in the `slice` StringMethods has no effect:\r\n\r\n```\r\nIn [18]: s = pd.Series(['abcde', 'fghij'])\r\n\r\nIn [19]: s.str.slice?\r\nDefinition:  s.str.slice(self, start=None, stop=None, step=1)\r\nSlice substrings from each element in array\r\n\r\nParameters\r\n----------\r\nstart : int or None\r\nstop : int or None\r\n\r\nReturns\r\n-------\r\nsliced : array\r\n\r\nIn [22]: s.str.slice(step=1)\r\nOut[22]: \r\n0    abcde\r\n1    fghij\r\ndtype: object\r\n\r\nIn [23]: s.str.slice(step=2)\r\nOut[23]: \r\n0    abcde\r\n1    fghij\r\ndtype: object\r\n\r\nIn [24]: s[0][::2]\r\nOut[24]: 'ace'\r\n```"""
8753,48158607,immerrr,jreback,2014-11-08 07:14:47,2014-11-19 22:15:58,2014-11-19 22:05:10,closed,,0.15.2,13,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8753,b'API: allow negative steps for label-based indexing',"b""This should fix #8716.\r\n\r\nSome of this refactoring may be useful for #8613, so I'd like someone to look through this.\r\n\r\ncc'ing @shoyer, @jreback and @jorisvandenbossche .\r\n\r\nTODO:\r\n- add tests for label slicing with negative step (ix, loc)\r\n   - [x] int64index\r\n   - [x] float64index\r\n   - [x] objectindex\r\n- [x] fix datetime/period indices\r\n-  add tests for label slicing with negative step (datetime, string, native pandas scalar) x (ix, loc)\r\n   - [x] datetimeindex\r\n   - [x] periodindex\r\n   - [x] timedeltaindex\r\n- [x] clean up old partial string code?\r\n- [x] benchmark"""
8752,48140705,selasley,jreback,2014-11-07 22:53:11,2015-02-01 03:19:50,2014-11-27 02:52:38,closed,,0.15.2,33,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8752,b'Update tokenizer to fix #8679 #8661',"b""Update tokenizer's handling of skipped lines.  \r\nFixes a problem with read_csv(delim_whitespace=True) and \r\nread_table(engine='c') when lines being skipped have trailing \r\nwhitespace. \r\n\r\ncloses #8679 \r\ncloses #8681 """
8744,47932953,akhilman,jreback,2014-11-06 07:58:37,2015-03-10 01:43:38,2014-11-06 12:42:30,closed,,0.16.0,5,Bug;Duplicate;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/8744,b'AmbiguousTimeError exception when resampling near DST change date',"b""```python\r\nindex = pd.to_datetime(pd.Series([\r\n'2014-10-26 07:35:49',\r\n'2014-10-26 07:45:08',\r\n'2014-10-26 08:04:58'\r\n]))\r\n\r\ndf = pd.DataFrame(np.arange(len(index)), index=index)\r\ndf = df.tz_localize('Asia/Krasnoyarsk', ambiguous='NaT')\r\ndf.resample('D')\r\n\r\nAmbiguousTimeError: Cannot infer dst time from Timestamp('2014-10-26 01:00:00'), try using the 'ambiguous' argument\r\n```\r\n\r\nUTC offset changed from +8 to +7 at 2:00 local time. \r\n"""
8743,47927209,davidastephens,jreback,2014-11-06 06:15:51,2014-11-10 01:18:27,2014-11-06 11:26:17,closed,,0.15.1,1,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/8743,b'BUG: DataReaders return missing data as NaN rather than warn.',b'Fixes #8433'
8742,47922929,davidastephens,jreback,2014-11-06 04:41:02,2014-11-10 01:18:28,2014-11-07 11:47:17,closed,,0.15.1,3,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/8742,b'BUG: Allow non-float values in get_yahoo_quotes.',b'Fixes issue #5229'
8739,47885077,jreback,jreback,2014-11-05 20:39:06,2014-11-05 21:21:15,2014-11-05 21:21:15,closed,,0.15.1,1,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8739,b'BUG: Bug in slicing a multi-index level with an empty-list (GH8737)',"b""closes #8737 \r\n\r\n```\r\nIn [5]: multi_index = pd.MultiIndex.from_product((['foo', 'bar', 'baz'], ['alpha', 'beta']))\r\n\r\nIn [6]: df = DataFrame(np.random.randn(5, 6), index=range(5), columns=multi_index).sortlevel(0,axis=1) \r\n\r\nIn [7]: df\r\nOut[7]: \r\n        bar                 baz                 foo          \r\n      alpha      beta     alpha      beta     alpha      beta\r\n0 -0.725086 -0.715498  1.141087 -0.586018  0.906581 -0.260218\r\n1 -1.111794 -0.393149  2.101567  0.648850  0.366892  0.554206\r\n2  1.258239  1.472659 -0.174970 -0.355639 -0.843127  1.101716\r\n3 -0.775790  0.419361 -0.521444  1.245519  0.759791  0.183877\r\n4 -0.001001 -1.526741 -1.721501  0.527531  0.407099 -0.415245\r\n\r\nIn [8]: df.loc[:, ([], slice(None))] \r\nOut[8]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [0, 1, 2, 3, 4]\r\n\r\nIn [9]: df.loc[:, ([], slice(None))].columns\r\nOut[9]: \r\nMultiIndex(levels=[[u'bar', u'baz', u'foo'], [u'alpha', u'beta']],\r\n           labels=[[], []])\r\n```"""
8737,47856693,SylvainCorlay,jreback,2014-11-05 16:34:18,2014-11-05 21:21:26,2014-11-05 21:21:26,closed,,0.15.1,8,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8737,b'Support for empty slice in multi indices',"b'It is currently possible to pass a list of labels for a given dataframe to `DataFrame.loc` in the case of a multi index, but the list cannot be empty unlike in the case of a non-hierarchical index.  '"
8735,47798365,onesandzeroes,jreback,2014-11-05 04:24:31,2015-12-13 18:33:06,2015-08-28 18:16:56,closed,,0.17.0,4,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/8735,b'Strange output from `DataFrame.apply` when applied func creates a dict',"b""Just had something odd come up while trying to come up with something for [this SO question](http://stackoverflow.com/questions/26747689/mapping-pandas-dataframe-rows-to-a-pandas-series).\r\n\r\nIf we use `DataFrame.apply()` to try and create dictionaries from the rows of a dataframe, it seems to return the `dict.values()` method rather than returning the dict itself.\r\n\r\n```python\r\ndf = pd.DataFrame({'k': ['a', 'b', 'c'], 'v': [1, 2, 3]})\r\ndf.apply(lambda row: {row['k']: row['v']}, axis=1)\r\nOut[52]: \r\n0    <built-in method values of dict object at 0x07...\r\n1    <built-in method values of dict object at 0x03...\r\n2    <built-in method values of dict object at 0x07...\r\ndtype: object\r\n```\r\n\r\nLooks like it's probably something to do with trying to grab the `values` attribute when the output of the applied function is a Series or something similar.\r\n\r\nLibrary versions:\r\n\r\n```\r\npd.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 30 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.0\r\nnose: 1.3.4\r\nCython: 0.21\r\nnumpy: 1.9.0\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.3.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 1.5\r\npytz: 2014.7\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.2\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.7\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\n"""
8734,47798278,vladfi1,jreback,2014-11-05 04:22:40,2014-11-05 06:33:22,2014-11-05 06:32:55,closed,,,1,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/8734,b'Inconsistent mean over booleans',"b""It appears that the mean aggregate function is inconsistent when operating on boolean values.\r\n\r\n    from pandas import DataFrame\r\n\r\n    df1 = DataFrame({'a': [1, 1], 'bools': [True, True]})\r\n    df2 = DataFrame({'a': [1, 1], 'bools': [True, False]})\r\n\r\n    print df1.mean().bools # 1.0\r\n    print df2.mean().bools # 0.5\r\n\r\n    print df1.groupby('a').mean() # DataFrame with 1: True\r\n    print df2.groupby('a').mean() # DataFrame with 1: 0.5\r\n\r\nFor some reason the mean of a set of True values is True instead of 1.0, but only for GroupBy objects and not for DataFrame objects."""
8733,47783796,vladfi1,jreback,2014-11-05 00:07:56,2014-11-08 23:18:50,2014-11-08 23:18:50,closed,,0.15.2,21,Bug;Difficulty Novice;Groupby;Visualization,https://api.github.com/repos/pydata/pandas/issues/8733,"b""pandas.core.groupby.SeriesGroupBy.hist doesn't take kwargs""","b""I'd like to pass certain kwargs to the hist function of a SeriesGroupBy object (alpha, legend, etc). The plot function provides more kwargs (e.g. legend, but not alpha) but no **kwargs to forward to matplotlib. The documentation (viewed with ? in ipython) suggests that there should be a kwds argument, but in reality this is not the case."""
8730,47763967,maxgrenderjones,jreback,2014-11-04 20:55:20,2015-03-31 22:39:59,2015-03-15 19:52:37,closed,,0.16.0,14,Bug;Indexing;Usage Question,https://api.github.com/repos/pydata/pandas/issues/8730,b'Improve clarity around when SettingWithCopyWarning can be ignored (if ever?)',"b""I [Edit: thought I got] a `SettingWarningCopyWarning ` when running the following code:\r\n```python\r\nframe[columnone][frame[columntwo]>x]=y\r\n```\r\nThe docs seem to imply that I can safely ignore this error.\r\n```python\r\n# passed via reference (will stay)\r\nIn [273]: dfb['c'][dfb.a.str.startswith('o')] = 42\r\n```\r\nIs it therefore generally true that I can always ignore errors when the command I'm running is of the form:\r\n```python\r\nframe[columnames][booleanconditiononframe]=x\r\n```\r\n\r\nI use this all the time, and I *think* it always works. If that's true, it would be great if it was specifically called out in a 'you can ignore this warning when...' section. If that's not true (e.g. it works for scalar x / numeric x but not for series x), it would be great if that were called out too.\r\n\r\nSorry if this seems trivial, but I'm trying to explain this to a colleague who's new to pandas and he's confused, and it looks like even old hands can be confused when confronted with this warning (see #6757).\r\n\r\n[Edit: looking at his code again, I suspect the SettingWithCopy warning was from a different line of code - (I wish warnings made it clear where they were from). All the same, it would still be great if the docs could be clear if there are circumstances where you always get a reference rather than a copy]"""
8728,47733781,aaront,jreback,2014-11-04 16:35:56,2015-03-06 22:59:19,2015-03-06 22:59:19,closed,,0.16.0,21,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/8728,"b""BUG: The 'jobComplete' key may be present but False in the BigQuery query results""","b""xref #9141 \r\n\r\nFixes an issue when looking for 'totalRows' on a large dataset that is not finished in between two subsequent checks."""
8719,47560420,thrasibule,jreback,2014-11-03 03:26:28,2014-11-05 21:18:37,2014-11-05 21:18:37,closed,,0.15.1,3,Bug;Compat;Docs;IO Excel,https://api.github.com/repos/pydata/pandas/issues/8719,b'ENH: read_csv dialect parameter can take a string (GH8703)',"b""Closes #8703\r\n\r\nI wasn't sure where to put the test, hopefully this is right.\r\n\r\nFollow-up of #8718"""
8716,47549910,immerrr,jreback,2014-11-02 21:43:14,2014-11-19 22:05:10,2014-11-19 22:05:10,closed,,0.15.2,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8716,"b""BUG: label-based slices don't support negative step""","b""Something feels wrong about this:\r\n```python\r\nIn [35]: s = pd.Series(np.arange(6), index=list('abcdef'))\r\n\r\nIn [36]: s\r\nOut[36]: \r\na    0\r\nb    1\r\nc    2\r\nd    3\r\ne    4\r\nf    5\r\ndtype: int64\r\n\r\nIn [37]: s.loc['e'::-1]\r\nOut[37]: Series([], dtype: int64)\r\n\r\nIn [38]: s.iloc[4::-1]\r\nOut[38]: \r\ne    4\r\nd    3\r\nc    2\r\nb    1\r\na    0\r\ndtype: int64\r\n\r\nIn [39]: s['e'::-1]\r\nOut[39]: Series([], dtype: int64)\r\n\r\n```"""
8715,47545946,jreback,jreback,2014-11-02 19:30:43,2014-11-02 21:28:33,2014-11-02 21:28:33,closed,,0.15.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8715,b'BUG: Bug in Panel indexing with a list-like (GH8710)',b'closes #8710 '
8714,47544573,jreback,jreback,2014-11-02 18:43:39,2014-11-09 22:10:39,2014-11-09 22:10:39,closed,,0.15.2,22,Bug;Categorical;Error Reporting,https://api.github.com/repos/pydata/pandas/issues/8714,b'BUG: concat of series of dtype category converting to object dtype (GH8641)',b'closes #8641 '
8710,47536470,femtotrader,jreback,2014-11-02 13:27:10,2014-11-02 21:28:33,2014-11-02 21:28:33,closed,,0.15.1,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8710,"b""Pandas Panel doesn't support getting DataFrame or (sub) Panel using a list of index""","b'Hello,\r\n\r\nwhen I try to get a DataFrame (or sub Panel) from a Panel using index as list it raises the following error:\r\n\r\n    TypeError: unhashable type: \'list\'\r\n\r\nSee the code\r\n\r\n    #!/usr/bin/env python\r\n    # -*- coding: utf-8 -*-\r\n\r\n    import pandas as pd\r\n    import pandas.io.data as web\r\n\r\n    start = ""2010-01-01""\r\n    end = ""2010-01-30""\r\n\r\n    df = web.DataReader(""AAPL"", \'yahoo\', start, end)\r\n\r\n    print(""DF 1"")\r\n    data = df[[\'Open\', \'Close\']] # returns pandas.core.frame.DataFrame\r\n    print(data)\r\n    print(type(data))\r\n\r\n    print(""DF 2"")\r\n    data = df[\'Open\'] # returns pandas.core.series.Series\r\n    print(data)\r\n    print(type(data))\r\n\r\n    print(""DF 3"")\r\n    data = df[[\'Open\']] # returns pandas.core.frame.DataFrame\r\n    print(data)\r\n    print(type(data))\r\n\r\n    panel = web.DataReader([""AAPL"", ""GOOGL""], \'yahoo\', start, end)\r\n    #Items axis: Open to Adj Close\r\n    #Major_axis axis: 2010-01-04 00:00:00 to 2010-01-29 00:00:00\r\n    #Minor_axis axis: AAPL to GOOGL\r\n    panel = panel.transpose(2, 1, 0) # symbol is panel index\r\n    #Items axis: AAPL to GOOGL\r\n    #Major_axis axis: 2010-01-04 00:00:00 to 2010-01-29 00:00:00\r\n    #Minor_axis axis: Open to Adj Close\r\n    print(panel)\r\n\r\n    print(""P 1"")\r\n    data = panel[""AAPL""] # returns pandas.core.frame.DataFrame\r\n    print(data)\r\n    print(type(data))\r\n\r\n    print(""P 2"")\r\n    #data = panel[[""AAPL""]] # Error: Should return a DataFrame but raises TypeError: unhashable type: \'list\'\r\n    #print(data)    \r\n    #print(type(data))\r\n\r\n    print(""P 3"")\r\n    #data = panel[[""AAPL"", ""GOOGL""]] # Error: Should return a ""sub"" Panel but raises TypeError: unhashable type: \'list\'\r\n\r\n    #print(data)\r\n    #print(type(data))\r\n\r\nBut that can be a misunderstanding on my side.\r\n\r\nThanks'"
8706,47530170,immerrr,jreback,2014-11-02 07:59:54,2014-11-02 21:37:50,2014-11-02 19:15:27,closed,,0.15.1,9,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8706,b'BUG: fix reverse comparison operations for Categorical',"b""This PR fixes #8658 and also adds `pd.lib.unbox_if_zerodim` that extracts values from zerodim arrays and adds detection of `pd.Period`, `datetime.date` and `datetime.time` in pd.lib.isscalar.\r\n\r\nI tried making `unbox_if_zerodim` usage more implicit but there's just too much to test when this is introduced (think every method that accepts a scalar must have another test to check it also accepts zerodim array).  Definitely too much to add for a single PR, but overall zerodim support could be added later on class-by-class basis."""
8705,47524531,jreback,jreback,2014-11-02 02:13:15,2014-11-02 21:51:43,2014-11-02 21:51:43,closed,,0.15.1,9,Bug;Categorical;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8705,b'ENH/BUG: support Categorical in to_panel reshaping (GH8704)',b'CLN: move block2d_to_blocknd support code to core/internal.py\r\nTST/BUG: support Categorical reshaping via .unstack\r\n\r\ncloses #8704 '
8704,47516952,davidrpugh,jreback,2014-11-01 21:01:31,2014-11-02 21:51:43,2014-11-02 21:51:43,closed,,0.15.1,7,Bug;Categorical;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8704,b'BUG: reshape of categorical via unstack/to_panel',"b'I have a hierarchical `pandas.DataFrame` that looks as follows...\r\n\r\n    In [12]: data.tail()\r\n    Out[12]: \r\n                             country    currency_unit         rgdpe  \\\r\n    countrycode year                                                  \r\n    ZWE         2007-01-01  Zimbabwe  Zimbabwe Dollar  45666.082031   \r\n                2008-01-01  Zimbabwe  Zimbabwe Dollar  35789.878906   \r\n                2009-01-01  Zimbabwe  Zimbabwe Dollar  49294.878906   \r\n                2010-01-01  Zimbabwe  Zimbabwe Dollar  51259.152344   \r\n                2011-01-01  Zimbabwe  Zimbabwe Dollar  55453.312500   \r\n  \r\nI would like to turn `data` into a `pandas.Panel` object. Previously I would do this using the `to_panel` method without issue. However, after upgrading to Pandas 0.15, this approach no longer works...\r\n\r\n    In [13]: data.to_panel()\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-13-dfa9fb7a4e83> in <module>()\r\n    ----> 1 data.to_panel()\r\n    \r\n    /Users/drpugh/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc in to_panel(self)\r\n       1061                 placement=block.mgr_locs, shape=shape,\r\n       1062                 labels=[major_labels, minor_labels],\r\n    -> 1063                 ref_items=selfsorted.columns)\r\n       1064             new_blocks.append(newb)\r\n       1065 \r\n\r\n    /Users/drpugh/anaconda/lib/python2.7/site-packages/pandas/core/reshape.pyc in     block2d_to_blocknd(values, placement, shape, labels, ref_items)\r\n       1206 \r\n       1207     if mask.all():\r\n    -> 1208         pvalues = np.empty(panel_shape, dtype=values.dtype)\r\n       1209     else:\r\n       1210         dtype, fill_value = _maybe_promote(values.dtype)\r\n\r\n    TypeError: data type not understood\r\n\r\nHas there been an API change? I could not find anything in the release notes to suggest that the above would no longer work. '"
8703,47514338,thrasibule,jreback,2014-11-01 19:20:29,2014-11-05 21:18:58,2014-11-05 21:18:58,closed,,0.15.1,2,Bug;Compat;Docs;IO Excel,https://api.github.com/repos/pydata/pandas/issues/8703,b'DOC/ENH: read_csv dialect parameters should accept string',"b'According to the doc of read_sql, dialect  can be string or a csv.Dialect instance.\r\nHowever this is not the case. If you look at the code, it can only really take a csv.Dialect instance. i.e. this doesn\'t work:\r\n```python\r\nfh = io.StringIO(u""pomme\\tpoire\\n1\\t\\2"")\r\npd.read_csv(fh, dialect=\'excel-tab\')\r\n```\r\nbut, this works:\r\n```python\r\nfh = io.StringIO(u""pomme\\tpoire\\n1\\t\\2"")\r\npd.read_csv(fh, dialect=csv.get_dialect(\'excel-tab\'))\r\n```\r\nI would expect it to be able to take any string returned by the csv.list_dialects() function, similar to the way csv.reader works. So we can either remove the mention of string from the doc, or make it work as intended by checking if the dialect string is part of csv.list_dialects(). Let me know if this makes sense and I\'ll submit a PR.'"
8702,47513259,stahlous,jreback,2014-11-01 18:38:12,2014-11-02 15:37:28,2014-11-02 14:09:38,closed,,0.15.1,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8702,b'BUG: setitem fails on mixed-type Panel4D',"b'Trying to set a value on a mixed-type Panel4D currently fails:\r\n\r\n    In [1]: from pandas.util import testing as tmd\r\n\r\n    In [2]: p4d = tmd.makePanel4D()\r\n\r\n    In [3]: p4d[\'foo\'] = \'bar\'\r\n\r\n    In [4]: p4d.iloc[0,0,0,0] = 0\r\n    ---------------------------------------------------------------------------\r\n    AttributeError                            Traceback (most recent call last)\r\n    <ipython-input-4-bb74b95f7244> in <module>()\r\n    ----> 1 p4d.iloc[0,0,0,0] = 0\r\n\r\n    /home/stahlous/pd_dev/pandas/pandas/core/indexing.py in __setitem__(self, key, value)\r\n        119                 indexer = self._convert_to_indexer(key, is_setter=True)\r\n        120\r\n    --> 121         self._setitem_with_indexer(indexer, value)\r\n        122\r\n        123     def _has_valid_type(self, k, axis):\r\n\r\n    /home/stahlous/pd_dev/pandas/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)\r\n        356             # indexer here\r\n        357             if (len(labels) == 1 and\r\n    --> 358                     isinstance(self.obj[labels[0]].index, MultiIndex)):\r\n        359                 item = labels[0]\r\n        360                 obj = self.obj[item]\r\n\r\n    /home/stahlous/pd_dev/pandas/pandas/core/generic.py in __getattr__(self, name)\r\n       1927         """"""\r\n       1928         if name in self._internal_names_set:\r\n    -> 1929             return object.__getattribute__(self, name)\r\n       1930         elif name in self._metadata:\r\n       1931             return object.__getattribute__(self, name)\r\n\r\n    AttributeError: \'Panel\' object has no attribute \'index\'\r\n\r\nFortunately, it looks like a simple fix. If this looks OK. I\'ll add a note to `v0.15.1.txt` (doesn\'t have a GH issue number yet).\r\n\r\nThis fix is needed for my latest attempt at the fillna bug #8395.'"
8700,47446437,jreback,jreback,2014-10-31 21:29:04,2014-11-01 13:11:51,2014-10-31 22:35:39,closed,,0.15.1,1,Bug;Categorical;Indexing,https://api.github.com/repos/pydata/pandas/issues/8700,b'BUG: various Categorical fixes (GH8623)',b'BUG: bug in selecting from a Categorical with iloc (GH8623)\r\nBUG: bug in groupby-transform with a Categorical (GH8623)\r\nBUG: bug in duplicated/drop_duplicates with a Categorical (GH8623)\r\n\r\ncloses #8623 '
8699,47436040,jreback,jreback,2014-10-31 19:34:33,2014-10-31 20:18:45,2014-10-31 20:18:44,closed,,0.15.1,0,Bug;Compat;Frequency,https://api.github.com/repos/pydata/pandas/issues/8699,b'BUG: incorrect serialization of a CustomBusinessDay in HDF5 (GH8591)',b'closes #8591 '
8697,47425133,hammer,jorisvandenbossche,2014-10-31 17:52:37,2014-11-17 08:31:24,2014-11-17 08:31:12,closed,,0.15.2,5,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/8697,b'BUG: SQL get_schema generates incorrect DDL for specific dialect',"b'I\'m importing data from an Excel spreadsheet and I\'d like to generate a CREATE TABLE statement for PostgreSQL that matches the data I\'ve imported from Excel.\r\n\r\nHere\'s what I\'ve got so far:\r\n```python\r\n>>> import pandas as pd\r\n>>> from sqlalchemy import create_engine\r\n>>> import psycopg2\r\n>>> basedata = pd.read_excel(\'mydata.xlsx\')\r\n>>> engine = create_engine(\'postgresql://myusername@localhost:5432/mydb\')\r\n>>> pd.io.sql.get_schema(basedata, \'basedata\', con=engine)\r\n```\r\n\r\nUnfortunately the CREATE TABLE statement generated uses a column type of ""DATETIME"" that does not exist in PostgreSQL. I suspect that the engine object is being ignored somehow and the DDL I\'m getting is for sqlite, not PostgreSQL. However, I can\'t get pandas to build on my Mac so that I can expand the tests to validate my assumption.'"
8695,47413392,jreback,jreback,2014-10-31 16:22:55,2014-10-31 20:11:29,2014-10-31 19:04:34,closed,,0.15.1,3,Bug;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8695,b'API/BUG: return np.nan rather than -1 for invalid datetime accessors values (GH8689)',b'BUG: millisecond Timestamp/DatetimeIndex accessor fixed\r\n\r\ncloses #8689 '
8689,47348592,danfrankj,jreback,2014-10-31 00:29:44,2014-10-31 19:04:34,2014-10-31 19:04:34,closed,,0.15.1,1,Bug;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8689,b'datetime operations on NaT produce dangerous results ',"b""    In [189]: ser = pd.Series([np.datetime64('nat')])\r\n\r\n    In [190]: ser\r\n    Out[190]: \r\n    0   NaT\r\n    dtype: datetime64[ns]\r\n\r\n    In [191]: ser.dt.hour\r\n    Out[191]: \r\n    0   -1\r\n    dtype: int64\r\n\r\nWhile -1 is not a valid hour it is a very valid integer and could (has) caused hard to track bugs. \r\n"""
8687,47322685,immerrr,jreback,2014-10-30 20:28:58,2014-11-02 07:44:47,2014-10-31 13:48:48,closed,,0.15.1,3,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8687,b'BUG: fix Categorical comparison to work with datetime',"b'`pd.Timestamp` is one of types that don\'t pass `np.isscalar` test previously\r\nused in _cat_compare_op, `pd.lib.isscalar` should be used instead.\r\n\r\n```python\r\nIn [1]: cat = pd.Categorical(pd.date_range(\'2014\', periods=5))\r\n\r\nIn [2]: cat > cat[0]\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-77883e4783fe> in <module>()\r\n----> 1 cat > cat[0]\r\n\r\n/home/dshpektorov/sources/pandas/pandas/core/categorical.py in f(self, other)\r\n     52             msg = ""Cannot compare a Categorical for op {op} with type {typ}. If you want to \\n"" \\\r\n     53                   ""compare values, use \'np.asarray(cat) <op> other\'.""\r\n---> 54             raise TypeError(msg.format(op=op,typ=type(other)))\r\n     55 \r\n     56     f.__name__ = op\r\n\r\nTypeError: Cannot compare a Categorical for op __gt__ with type <class \'pandas.tslib.Timestamp\'>. If you want to \r\ncompare values, use \'np.asarray(cat) <op> other\'.\r\n\r\n```'"
8683,47285559,amelio-vazquez-reina,jorisvandenbossche,2014-10-30 15:39:49,2014-11-30 23:21:01,2014-11-30 23:21:01,closed,,0.15.2,4,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/8683,b'ValueError exception with pd.resample',"b""When running `df.resample('2200L', how='sum', label='right')` with `df`:\r\n```\r\n2014-10-14 23:06:07.440000    6.44000\r\n2014-10-14 23:06:07.761000    5.09600\r\n2014-10-14 23:06:08.215000    6.44000\r\n2014-10-14 23:06:08.486000    6.44000\r\n2014-10-14 23:06:08.509000    5.20800\r\n2014-10-14 23:06:08.789000    4.02842\r\n2014-10-14 23:06:10.795000    5.65600\r\n2014-10-14 23:06:11.618000    6.21600\r\n2014-10-14 23:06:12.177000    6.21600\r\n2014-10-14 23:06:14.620000    5.10720\r\n2014-10-14 23:06:16.698000    5.95840\r\n2014-10-14 23:06:16.745000    6.44000\r\n2014-10-14 23:06:20.548000    6.21600\r\n2014-10-14 23:06:20.549000    6.44000\r\n2014-10-14 23:06:20.551000    5.95840\r\n2014-10-14 23:06:23.206000    6.44000\r\n2014-10-14 23:06:29.977000    6.44000\r\n2014-10-14 23:06:35.307000    5.20800\r\n2014-10-15 23:00:00               NaN\r\n2014-10-15 23:00:02.200000        NaN\r\nName: spend, dtype: float64\r\n```\r\n\r\nI got:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-20-71895ab1ef27> in <module>()\r\n----> 1 R_inst = loaded_sorted.tail(200).head(20).resample('2200L', how='sum', label='right')\r\n\r\n/Users/josh/anaconda3/envs/py34/lib/python3.4/site-packages/pandas/core/generic.py in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, \r\nloffset, limit, base)                                                                                                                                                  \r\n   2978                               fill_method=fill_method, convention=convention,\r\n   2979                               limit=limit, base=base)\r\n-> 2980         return sampler.resample(self).__finalize__(self)\r\n   2981 \r\n   2982     def first(self, offset):\r\n\r\n/Users/josh/anaconda3/envs/py34/lib/python3.4/site-packages/pandas/tseries/resample.py in resample(self, obj)\r\n     83 \r\n     84         if isinstance(ax, DatetimeIndex):\r\n---> 85             rs = self._resample_timestamps()\r\n     86         elif isinstance(ax, PeriodIndex):\r\n     87             offset = to_offset(self.freq)\r\n\r\n/Users/josh/anaconda3/envs/py34/lib/python3.4/site-packages/pandas/tseries/resample.py in _resample_timestamps(self, kind)\r\n    273         axlabels = self.ax\r\n    274 \r\n--> 275         self._get_binner_for_resample(kind=kind)\r\n    276         grouper = self.grouper\r\n    277         binner = self.binner\r\n\r\n/Users/josh/anaconda3/envs/py34/lib/python3.4/site-packages/pandas/tseries/resample.py in _get_binner_for_resample(self, kind)\r\n    121             kind = self.kind\r\n    122         if kind is None or kind == 'timestamp':\r\n--> 123             self.binner, bins, binlabels = self._get_time_bins(ax)\r\n    124         elif kind == 'timedelta':\r\n    125             self.binner, bins, binlabels = self._get_time_delta_bins(ax)\r\n\r\n/Users/josh/anaconda3/envs/py34/lib/python3.4/site-packages/pandas/tseries/resample.py in _get_time_bins(self, ax)\r\n    182 \r\n    183         # general version, knowing nothing about relative frequencies\r\n--> 184         bins = lib.generate_bins_dt64(ax_values, bin_edges, self.closed, hasnans=ax.hasnans)\r\n    185 \r\n    186         if self.closed == 'right':\r\n\r\n/Users/josh/anaconda3/envs/py34/lib/python3.4/site-packages/pandas/lib.so in pandas.lib.generate_bins_dt64 (pandas/lib.c:17928)()\r\n\r\nValueError: Values falls after last bin\r\n```\r\n\r\nwith `Python 3.4.1 :: Anaconda 2.1.0 (x86_64)` and:\r\n\r\n```\r\nCython==0.21\r\nDataShape==0.3.0\r\nFlask==0.10.1\r\nJinja2==2.7.3\r\nMarkupSafe==0.23\r\nPillow==2.5.1\r\nPyYAML==3.11\r\nPygments==1.6\r\nSQLAlchemy==0.9.7\r\nSphinx==1.2.3\r\nTheano==0.6.0\r\nWerkzeug==0.9.6\r\nXlsxWriter==0.5.7\r\nabstract-rendering==0.5.1\r\nappnope==0.1.0\r\nargcomplete==0.8.1\r\narrow==0.4.4\r\nastropy==0.4.2\r\nbeautifulsoup4==4.3.2\r\nbinstar==0.7.1\r\nbitarray==0.8.1\r\nblaze==0.6.3\r\nblz==0.6.2\r\nbokeh==0.6.1\r\nboto==2.34.0\r\ncffi==0.8.6\r\ncolorama==0.3.1\r\nconfigobj==5.0.6\r\ncryptography==0.5.4\r\ncytoolz==0.7.0\r\ndecorator==3.4.0\r\ndocutils==0.12\r\nfuture==0.13.1\r\ngreenlet==0.4.4\r\nh5py==2.3.1\r\nipython==2.2.0\r\nitsdangerous==0.24\r\njdcal==1.0\r\njedi==0.8.1-final0\r\n## FIXME: could not find svn URL in dependency_links for this package:\r\njoblib==0.8.3-r1\r\nllvmpy==0.12.7\r\nlpsolve55==5.5.2.0\r\nlxml==3.4.0\r\nmatplotlib==1.4.0\r\nmock==1.0.1\r\nmultipledispatch==0.4.7\r\nnetworkx==1.9.1\r\nnltk==3.0.0\r\nnose==1.3.4\r\nnumba==0.14.0\r\nnumexpr==2.3.1\r\nnumpy==1.9.0\r\nopenpyxl==1.8.5\r\npandas==0.15.0\r\nparse==1.6.4\r\npatsy==0.3.0\r\nply==3.4\r\npsutil==2.1.1\r\npsycopg2==2.5.4\r\npy==1.4.25\r\npyOpenSSL==0.14\r\npycosat==0.6.1\r\npycparser==2.10\r\npycrypto==2.6.1\r\npycurl==7.19.5\r\npyflakes==0.8.1\r\npymc==3.0\r\npyparsing==2.0.1\r\npytest==2.6.3\r\npython-dateutil==2.1\r\npytz==2014.7\r\npyzmq==14.3.1\r\nqds-sdk==1.2.2\r\nredis==2.9.1\r\nrequests==2.4.3\r\nrope-py3k==0.9.4-1\r\nrunipy==0.1.1\r\nscikit-image==0.10.1\r\nscikit-learn==0.15.2\r\nscipy==0.14.0\r\nseaborn==0.5.dev\r\nsix==1.8.0\r\nsockjs-tornado==1.0.1\r\nspyder==2.3.1\r\nstatsmodels==0.5.0\r\nsympy==0.7.5\r\ntables==3.1.1\r\ntoolz==0.7.0\r\ntornado==4.0.2\r\nujson==1.33\r\nxlrd==0.9.3\r\n```"""
8679,47220660,jiffyclub,jreback,2014-10-30 00:24:33,2014-11-27 02:52:38,2014-11-27 02:52:38,closed,,0.15.2,2,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8679,"b'Bug with read_table, skiprows, and C engine'","b""I'm reading the file available at ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt. The data start on line 73.\r\n\r\nIf I use the default C engine with `read_table` I have to specify `skiprows=85` to properly load the table:\r\n\r\n```python\r\npd.read_table(\r\n        'co2_mm_mlo.txt.', sep=r'\\s+', header=None, skiprows=85, engine='c',\r\n        names=['year', 'month', 'dec_year', 'average', 'interpolated', 'trend', 'days'])\r\n```\r\n\r\nBut if I use the Python engine then the expected `skiprows=72` works:\r\n\r\n```python\r\npd.read_table(\r\n        'co2_mm_mlo.txt.', sep=r'\\s+', header=None, skiprows=72, engine='python',\r\n        names=['year', 'month', 'dec_year', 'average', 'interpolated', 'trend', 'days'])\r\n```\r\n\r\nThe resulting DataFrame is expected to have 679 rows, but has 691 rows and data from the header if I use `skiprows=72` with the C engine.\r\n\r\nI've confirmed this behavior on Mac OS X Yosemite with Pandas 0.15.0 and a checkout of master@5cf3d85a7d4c448519fa08f918a114209cfbdf2b."""
8676,47214839,jreback,jreback,2014-10-29 23:01:54,2014-12-13 21:52:27,2014-10-30 11:19:17,closed,,0.15.1,10,Bug;MultiIndex;Performance,https://api.github.com/repos/pydata/pandas/issues/8676,b'PERF: set multiindex labels with a coerced dtype (GH8456)',b'closes #8456 '
8675,47213522,jreback,jreback,2014-10-29 22:45:27,2016-02-12 17:40:27,2014-10-29 22:56:36,closed,,0.15.1,0,Bug;Indexing;Windows,https://api.github.com/repos/pydata/pandas/issues/8675,b'TST: fix up for 32-bit indexers w.r.t. (GH8669)',b'xref #8669 '
8671,47100887,jreback,jreback,2014-10-29 01:10:58,2014-10-30 11:20:24,2014-10-29 01:31:56,closed,,0.15.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8671,b'BUG: Bug in setitem with empty indexer and unwanted coercion of dtypes (GH8669)',b'closes #8669 '
8669,47098537,ozhogin,jreback,2014-10-29 00:28:27,2014-10-30 11:19:44,2014-10-29 01:31:56,closed,,0.15.1,4,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/8669,b'Bug? Replacing NaN values based on a condition.',"b""Dataframe with 2 columns: A and B. If values in B are larger than values in A - replace those values with values of A. I used to do this by doing df.B[df.B > df.A] = df.A, however recent upgrade of pandas started giving a SettingWithCopyWarning when encountering this chained assignment. Official documentation recommends using .loc.\r\n\r\nOkay, I said, and did it through df.loc[df.B > df.A, 'B'] = df.A and it all works fine, unless column B has all values of NaN. Then something weird happens:\r\n```python\r\nIn [1]: df = pd.DataFrame({'A': [1, 2, 3],'B': [np.NaN, np.NaN, np.NaN]})\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A   B\r\n0  1 NaN\r\n1  2 NaN\r\n2  3 NaN\r\n\r\nIn [3]: df.loc[df.B > df.A, 'B'] = df.A\r\n\r\nIn [4]: df\r\nOut[4]: \r\n   A                    B\r\n0  1 -9223372036854775808\r\n1  2 -9223372036854775808\r\n2  3 -9223372036854775808\r\n```\r\nNow, if even one of B's elements satisfies the condition (larger than A), then it all works fine:\r\n```python\r\nIn [1]: df = pd.DataFrame({'A': [1, 2, 3],'B': [np.NaN, 4, np.NaN]})\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A   B\r\n0  1 NaN\r\n1  2   4\r\n2  3 NaN\r\n\r\nIn [3]: df.loc[df.B > df.A, 'B'] = df.A\r\n\r\nIn [4]: df\r\nOut[4]: \r\n   A   B\r\n0  1 NaN\r\n1  2   2\r\n2  3 NaN\r\n```\r\nBut if none of B's elements satisfy, then all NaNs get replaces with -9223372036854775808:\r\n```python\r\nIn [1]: df = pd.DataFrame({'A':[1,2,3],'B':[np.NaN,1,np.NaN]})\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A   B\r\n0  1 NaN\r\n1  2   1\r\n2  3 NaN\r\n\r\nIn [3]: df.loc[df.B > df.A, 'B'] = df.A\r\n\r\nIn [4]: df\r\nOut[4]: \r\n   A                    B\r\n0  1 -9223372036854775808\r\n1  2                    1\r\n2  3 -9223372036854775808\r\n```\r\n\r\nAm I doing something wrong, or this is a bug?\r\n\r\npandas: 0.15.0"""
8667,47089232,willpan,jreback,2014-10-28 22:22:59,2014-11-29 22:03:08,2014-11-29 22:03:08,closed,,0.15.2,3,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8667,b'BUG: Slicing timeseries with over 1000000 entries with time fails',"b""Slicing a timeseries with time results in a TypeError if the timeseries contains over 1,000,000 entries.  \r\n\r\n    import pandas as pd\r\n    from datetime import time\r\n\r\n    ser = pd.Series(index=pd.date_range('2000-1-1', periods=1000000, freq=pd.datetools.offsets.Second()))\r\n    ser.head(999999)[time(15,0)] # this is fine\r\n    ser[time(15,0)] # this fails"""
8664,47086114,TomAugspurger,jreback,2014-10-28 21:49:58,2016-01-04 02:17:59,2016-01-04 02:17:59,closed,,0.18.0,7,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/8664,b'BUG: query modifies the frame when you compare with `=`',"b""I messed up and used `=` instead of `==` in a `query`.\r\n\r\n```python\r\ndf = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\r\ndf.query('a=1')\r\n```\r\n\r\nThat raises a ValueError. But `df` was modified.\r\n\r\n```python\r\nIn [15]: df\r\nOut[15]:\r\n   a  b\r\n0  1  a\r\n1  1  b\r\n2  1  c\r\n```\r\n\r\nversions:\r\n\r\n```\r\npandas: 0.15.0-6-g403f38d\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.3.1\r\n```\r\n\r\nCan't look right now."""
8661,47048971,selasley,jreback,2014-10-28 16:35:15,2014-11-27 02:52:38,2014-11-27 02:52:38,closed,,0.15.2,5,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8661,"b'read_csv problem with delim_whitespace, skiprows and trailing spaces in skipped rows'","b""given this input file with linefeeds indicated by <lf>\r\n```\r\nskip1<lf>\r\nskip2<lf>\r\n0    1    2<lf>\r\n3    4    5<lf>\r\n```\r\nreading with read_csv() in pandas 0.15.0-42-g20be789 and python 3.4.2 works\r\n```\r\ndf = pd.read_csv('test.txt', skiprows=2, delim_whitespace=True, header=None)\r\ndf\r\n   0  1  2\r\n0  0  1  2\r\n1  3  4  5\r\n```\r\nIf I add a space after skip1 so the skipped lines are \r\n```\r\nskip1 <lf>\r\nskip2<lf>\r\n```\r\nthen read_csv() throws an error\r\n``CParserError: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3``\r\n\r\nAdding 1 to skiprows\r\n``df = pd.read_csv('test.txt', skiprows=3, delim_whitespace=True, header=None)``\r\ndoes not throw an exception and gives the expected DataFrame\r\n\r\nReading with skiprows=2 and without header=None does not throw an exception and produces a DataFrame with a multiindex\r\n```\r\n     skip2\r\n0 1      2\r\n3 4      5\r\n```\r\n\r\nIf there is a space after skip2 so the skipped lines are\r\n```\r\nskip1<lf>\r\nskip2 <lf>\r\n```\r\nthen\r\n``df = pd.read_csv('test.txt', skiprows=2, delim_whitespace=True, header=None)``\r\ndoes not throw an exception but it does not include the 0 1 2 row in the DataFrame\r\n\r\nIf there are spaces after skip1 and skip2 so the skipped lines are\r\n```\r\nskip1 <lf>\r\nskip2 <lf>\r\n```\r\nthen\r\n``df = pd.read_csv('test.txt', skiprows=2, delim_whitespace=True, header=None)``\r\nthrows the CParserError exception but\r\n``df = pd.read_csv('test.txt', skiprows=3, delim_whitespace=True, header=None)``\r\ndoes not and returns the expected DataFrame\r\n\r\nI would expect skiprows to skip the number of lines specified whether or not there are trailing spaces in those lines."""
8658,47024737,immerrr,jreback,2014-10-28 13:21:48,2014-11-02 19:15:27,2014-11-02 19:15:27,closed,,0.15.1,6,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8658,b'BUG: Categoricals cannot compare to ndim=0 array values',"b'Reproduction steps:\r\n\r\n```python\r\nIn [1]: cat = pd.Categorical([1,2,3])\r\n\r\nIn [2]: cat > cat[0]\r\nOut[2]: array([False,  True,  True], dtype=bool)\r\n\r\nIn [3]: cat[0] < cat\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-5c4bb0678680> in <module>()\r\n----> 1 cat[0] < cat\r\n\r\n/home/dshpektorov/sources/pandas/pandas/core/categorical.pyc in f(self, other)\r\n     52             msg = ""Cannot compare a Categorical for op {op} with type {typ}. If you want to \\n"" \\\r\n     53                   ""compare values, use \'np.asarray(cat) <op> other\'.""\r\n---> 54             raise TypeError(msg.format(op=op,typ=type(other)))\r\n     55 \r\n     56     f.__name__ = op\r\n\r\nTypeError: Cannot compare a Categorical for op __gt__ with type <type \'numpy.ndarray\'>. If you want to compare values, use \'np.asarray(cat) <op> other\'.\r\n```\r\n\r\nThe problem is that when `cat[0]` is the first operand (`type == np.int64`) it orchestrates the execution and the first step is casting it to a rank-0 array (it becomes `array(1)` rather than `np.int64(1)`) and at that point it ceases to pass `np.isscalar` check and corresponding branch of execution is skipped.\r\n\r\nThe fix is trivial, feel free to beat me to it.\r\n\r\nOn a broader scale, we might want to review all uses of `np.isscalar` and see if they can also benefit from accepting rank-0 arrays. If that\'s true, we might want to add `pd.isscalar` function and use it everywhere instead.'"
8653,46928851,lmancini,jreback,2014-10-27 16:44:21,2014-11-06 12:47:56,2014-11-06 12:44:42,closed,,0.15.2,5,Bug;Duplicate;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/8653,b'Exception when resampling ends on a DST change date',"b'Hi all, I\'m experiencing an exception when calling `resample(\'D\', how=\'mean\')` on a Series or Dataframe whose index (datetime-based) ends just on a DST change date.\r\n\r\nAs a workaround, I\'m currently try/except-ing the ValueError and eventually adding a np.nan datapoint at the end of the series (effectively ""walking over"" the DST change date).  At that point I call `resample` which completes correctly and filters away the np.nan datapoint anyway.\r\n\r\nSnippet [1] reproduces the bug, calling `resample` on a short series which ends on Oct 26th, which was the DST change date for Europe/Rome.  Raised exception is [2].  If relevant, [3] shows the installed versions info.\r\n\r\n[1]\r\n```python\r\nimport datetime\r\nimport pytz as tz\r\nimport pandas as pd\r\n\r\nrome = tz.timezone(\'Europe/Rome\')\r\n\r\ndr = []\r\nfor i in range(2):\r\n    dp = datetime.datetime(2014, 10, 25) + datetime.timedelta(days=i)\r\n    dr.append(rome.localize(dp))\r\n\r\nseries = {}\r\nfor i, ddr in enumerate(dr):\r\n    series[ddr] = i * 10\r\n\r\ns1 = pd.Series(series)\r\ns1 = s1.resample(\'D\', how=\'mean\')\r\n```\r\n\r\n[2]\r\n```\r\n\r\nFile ""resample_this.py"", line 18, in <module>\r\n    s1 = s1.resample(\'D\', how=\'mean\')\r\n  File ""pandas/core/generic.py"", line 2980, in resample\r\n    return sampler.resample(self).__finalize__(self)\r\n  File ""pandas/tseries/resample.py"", line 85, in resample\r\n    rs = self._resample_timestamps()\r\n  File ""pandas/tseries/resample.py"", line 314, in _resample_timestamps\r\n    result = grouped.aggregate(self._agg_method)\r\n  File ""pandas/core/groupby.py"", line 2265, in aggregate\r\n    return getattr(self, func_or_funcs)(*args, **kwargs)\r\n  File ""pandas/core/groupby.py"", line 693, in mean\r\n    return self._python_agg_general(f)\r\n  File ""pandas/core/groupby.py"", line 1125, in _python_agg_general\r\n    return self._wrap_aggregated_output(output)\r\n  File ""pandas/core/groupby.py"", line 2332, in _wrap_aggregated_output\r\n    return Series(output, index=index, name=name)\r\n  File ""pandas/core/series.py"", line 212, in __init__\r\n    data = SingleBlockManager(data, index, fastpath=True)\r\n  File ""pandas/core/internals.py"", line 3326, in __init__\r\n    ndim=1, fastpath=True)\r\n  File ""pandas/core/internals.py"", line 2060, in make_block\r\n    placement=placement)\r\n  File ""pandas/core/internals.py"", line 75, in __init__\r\n    len(self.values), len(self.mgr_locs)))\r\nValueError: Wrong number of items passed 2, placement implies 1\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.15.0\r\nnose: None\r\nCython: None\r\nnumpy: 1.9.0\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: 1.2.3\r\npatsy: None\r\ndateutil: 1.5\r\npytz: 2014.7\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: 2.1.1\r\nxlrd: None\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: 0.9\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```'"
8652,46903189,jreback,jreback,2014-10-27 13:14:02,2014-10-30 11:20:23,2014-10-28 00:03:02,closed,,0.15.1,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8652,b'BUG: various categorical fixes (GH8626)',b'BUG: coerce Categorical in record array creation (GH8626)\r\nBUG: Categorical not created properly with to_frame() from Series (GH8626)\r\nBUG: handle astype with passed pd.Categorical (GH8626)\r\n\r\ncloses #8626 '
8644,46861428,jreback,jreback,2014-10-27 00:57:48,2014-10-30 11:20:23,2014-10-27 12:13:31,closed,,0.15.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8644,"b'BUG: Bug in ix/loc block splitting on setitem (manifests with integer-like dtypes, eg. datetime64) (GH8607)'",b'closes #8607 '
8641,46855664,jreback,jreback,2014-10-26 21:37:29,2014-11-09 22:10:39,2014-11-09 22:10:39,closed,,0.15.2,2,Bug;Categorical;Error Reporting,https://api.github.com/repos/pydata/pandas/issues/8641,b'ERR: concat of 2 Series categories does not preserve category dtype',"b""xref #8640\r\n\r\nAs this is inconcistent with how DataFrame concat's work (its a different code-path), so needs updating\r\n```\r\ns = Series(list('abc'),dtype='category')\r\ns2 = Series(list('abd'),dtype='category')\r\n\r\n# this should raise\r\npd.concat([s,s2])\r\n\r\n# this should be a category\r\npd.concat([s,s])\r\n```"""
8626,46778842,fkaufer,jreback,2014-10-24 20:13:53,2014-11-11 12:58:20,2014-10-28 00:03:02,closed,,0.15.1,24,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8626,"b'BUG: DataFrame.to_records, DataFrame constructor broken for categoricals'","b""`to_records()` and the df constructor are broken for data containing categoricals when created with `dtype='category'` or `astype('category')`.\r\n\r\nBroken (`TypeError: data type not understood`):\r\n\r\n```python\r\npd.DataFrame(list('abc'),dtype='category')\r\n\r\ndf=pd.DataFrame(list('abc'), columns=['A'])\r\ndf['A'] = df['A'].astype('category')\r\ndf.to_records()\r\n```\r\n\r\nWorks:\r\n```python\r\npd.Series(list('abc'), dtype='category')\r\n\r\npd.DataFrame(list('abc'), dtype=pd.core.common.CategoricalDtype)\r\n\r\ndf=pd.DataFrame(list('abc'), columns=['A'])\r\ndf['A'] = df['A'].astype(pd.core.common.CategoricalDtype)\r\ndf.to_records()\r\n\r\npd.Series(list('abc'), dtype='category').to_frame().to_records()\r\n```\r\n\r\n`to_frame` seems to remove the category dtype though.\r\n\r\nPandas version: 0.15.0-20-g2737f5a"""
8624,46713608,kay1793,jorisvandenbossche,2014-10-24 08:10:06,2014-10-31 12:50:09,2014-10-31 12:50:09,closed,,0.15.1,4,Bug;Categorical;IO SQL,https://api.github.com/repos/pydata/pandas/issues/8624,b'pd.to/read_sql_table silently corrupts Categorical columns',"b""```python\r\nIn [29]: from sqlalchemy import create_engine\r\n    ...: engine = create_engine('sqlite://')\r\n    ...: df=pd.DataFrame([[1,'John P. Doe'],[2,'Jane Dove'],[1,'John P. Doe']],\r\n    ...: columns=['person_id','person_name'])\r\n    ...: df.to_sql('data1',engine)\r\n    ...: df['person_name']=pd.Categorical(df.person_name)\r\n    ...: df.to_sql('data2',engine)\r\n    ...: print pd.read_sql_table('data1',engine)\r\n    ...: print pd.read_sql_table('data2',engine)\r\n   index  person_id  person_name\r\n0      0          1  John P. Doe\r\n1      1          2    Jane Dove\r\n2      2          1  John P. Doe\r\n   index  person_id person_name\r\n0      0          1           J\r\n1      1          2           o\r\n2      2          1           h\r\n```\r\n\r\nusing relational db to store catagorical  columns in seperare tables would be very cool, and rebuilding the frame in pandas by JOIN from the multiple tables would save time on the wire. also memory if  categorical was build directly."""
8623,46710534,kay1793,jreback,2014-10-24 07:18:59,2014-10-31 22:35:39,2014-10-31 22:35:39,closed,,0.15.1,0,Bug;Categorical;Indexing,https://api.github.com/repos/pydata/pandas/issues/8623,"b'pd.Catagorical breaks transform, drop_duplicates, iloc'","b""```python\r\nx=pd.DataFrame([[1,'John P. Doe'],[2,'Jane Dove'],[1,'John P. Doe']],\r\ncolumns=['person_id','person_name'])\r\nx['person_name']=pd.Categorical(x.person_name) # doing this breaks transform\r\ng=x.groupby(['person_id'])\r\ng.transform(lambda x:x)\r\nAttributeError: 'ObjectBlock' object has no attribute '_holder'\r\n```\r\n\r\nusing `drop_duplicates` inside apply (I often need this):\r\n```python\r\ng.apply(lambda x: x.drop_duplicates('person_name'))\r\nSystemError: numpy/core/src/multiarray/iterators.c:370: bad argument to internal function\r\n```\r\n\r\nand iloc is not what I expected:\r\n```python\r\nx.iloc[0].person_name # ok \r\nx.person_name.iloc[0] # err\r\nTypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got Series)\r\n```\r\n\r\n\r\n"""
8622,46704468,rosnfeld,jreback,2014-10-24 04:56:35,2014-10-30 11:20:23,2014-10-24 18:36:38,closed,,0.15.1,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8622,"b'BUG: cut/qcut on Series with ""retbins"" (GH8589)'","b""Fixes #8589.\r\n\r\nI centralized a bit of logic into tile.py's ```_bins_to_cuts()``` so that this change could be made in one place, hope that's okay."""
8621,46702731,rmorgans,jreback,2014-10-24 04:05:07,2014-12-03 22:37:35,2014-12-03 22:37:35,closed,,0.15.2,12,Bug;Difficulty Novice;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8621,b'to_csv issue',"b'I have an issue using to_csv on a DataFrame object. It has a large number of columns d.shape = (3,454731).\r\n\r\n```python\r\nPython 3.4.2 (default, Oct  8 2014, 13:44:52)\r\n[GCC 4.9.1 20140903 (prerelease)] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas as pd\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.14.22-1-lts\r\nmachine: x86_64\r\nprocessor:\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_AU.UTF-8\r\n\r\npandas: 0.15.0-16-g7012d71\r\nnose: 1.3.4\r\nCython: 0.21.1\r\nnumpy: 1.9.0\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.3.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.2\r\npytz: 2014.7\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.2\r\nopenpyxl: 1.8.6\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.7\r\nlxml: 3.4.0\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.8\r\npymysql: None\r\npsycopg2: 2.5.4 (dt dec pq3 ext)\r\n>>> d=pd.read_msgpack(\'test.mpk\')\r\n>>> d.shape\r\n(3, 454731)\r\n>>> d.to_csv(\'test.csv\')\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/lib/python3.4/site-packages/pandas/util/decorators.py"", line 88, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/lib/python3.4/site-packages/pandas/core/frame.py"", line 1154, in to_csv\r\n    formatter.save()\r\n  File ""/usr/lib/python3.4/site-packages/pandas/core/format.py"", line 1400, in save\r\n    self._save()\r\n  File ""/usr/lib/python3.4/site-packages/pandas/core/format.py"", line 1492, in _save\r\n    chunks = int(nrows / chunksize) + 1\r\nZeroDivisionError: division by zero\r\n>>> d.T.to_csv(\'test.csv\')\r\n>>>\r\n\r\n```\r\n\r\nNot sure what\'s going on here - I\'ve written a nosetest here (any tips for improvements in my test?)\r\n\r\nhttps://github.com/rmorgans/pandas/commit/f3d0a9e629866fb0d4bcfdde13b631997efa7e5f\r\n\r\n'"
8613,46617626,jorisvandenbossche,jreback,2014-10-23 12:11:27,2015-03-04 20:46:25,2015-03-04 20:46:25,closed,,0.16.0,40,Bug;Docs;Indexing,https://api.github.com/repos/pydata/pandas/issues/8613,b'API: label-based slicing with not-included labels',"b""I didn't directly find an issue about it, or an explanation in the docs, but I stumbled today on the following, which did surprise me a bit:\r\n\r\nConsidering the following dataframe:\r\n\r\n```\r\nIn [18]: df = pd.DataFrame(np.random.randn(5,2), index=pd.date_range('2012-01-01', periods=5))\r\n\r\nIn [19]: df\r\nOut[19]:\r\n                   0         1\r\n2012-01-01  2.511337 -0.776326\r\n2012-01-02  0.133589  0.441911\r\n2012-01-03  0.348167  1.285188\r\n2012-01-04  1.075843  1.282131\r\n2012-01-05  0.683006  0.558459\r\n```\r\n\r\nSlicing with a label that is not included in the index works with `.ix`, but not with `.loc`:\r\n```\r\nIn [20]: df.ix['2012-01-03':'2012-01-31']\r\nOut[20]:\r\n                   0         1\r\n2012-01-03  0.348167  1.285188\r\n2012-01-04  1.075843  1.282131\r\n2012-01-05  0.683006  0.558459\r\n\r\nIn [21]: df.loc['2012-01-03':'2012-01-31']\r\n...\r\nKeyError: 'stop bound [2012-01-31] is not in the [index]'\r\n```\r\n\r\nContext: I was updating some older code, and I wanted to replace `.ix` with `.loc` (as this is what we recommend if it is purely label based to prevent confusion).\r\n\r\nSome things:\r\n\r\n- If this is intended, I don't find this stated somewhere in the docs. So the docs are at least lacking at this point.\r\n- the inconsistency between `[]`, `.ix[]` and `.loc[]` is a bit surprising here\r\n- it is also inconsistent with `iloc` -> that behaviour was changed in 0.14 to allow out of bound slicing (http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#whatsnew-0140-api)\r\n- Specifically for datetime-line indexing, it is also inconsistent with the feature of partial string indexing: `df.loc['2012-01-03':'2012-01']` will work and do the expected while `df.loc['2012-01-03':'2012-01-31']` fails\r\n\r\n"""
8607,46561671,dmarx,jreback,2014-10-22 21:15:03,2014-10-30 11:19:44,2014-10-27 12:13:31,closed,,0.15.1,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8607,b'column datatype conversion impacts whole dataframe when using df.ix indexing',"b'My dataframe has a timestamp column that is encoded as unix epoch. When I convert the column using named index selection it works fine, but when I use the \'.ix\' syntax it coerces the whole dataframe. Example:\r\n\r\n    import pandas as pd\r\n\r\n    df = pd.DataFrame(\r\n        {\'timestamp\':[1413840976, 1413842580, 1413760580], \r\n         \'delta\':[1174, 904, 161], \r\n         \'elapsed\':[7673, 9277, 1470]\r\n        })\r\n        \r\n    df2 = df.copy()\r\n\r\n    df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\'], unit=\'s\')\r\n    df2.ix[:,2] = pd.to_datetime(df[\'timestamp\'], unit=\'s\')\r\n\r\n    df\r\n\r\n        delta    elapsed    timestamp\r\n    0    1174    7673    2014-10-20 21:36:16\r\n    1    904    9277    2014-10-20 22:03:00\r\n    2    161    1470    2014-10-19 23:16:20\r\n\r\n    df2\r\n\r\n        delta    elapsed    timestamp\r\n    0    2014-10-20 21:36:16    1970-01-01 00:00:00.000007673    1970-01-01 00:00:01.413840976\r\n    1    2014-10-20 22:03:00    1970-01-01 00:00:00.000009277    1970-01-01 00:00:01.413842580\r\n    2    2014-10-19 23:16:20    1970-01-01 00:00:00.000001470    1970-01-01 00:00:01.413760580\r\n\r\nI strongly suspect the difference in behavior here is problematic and should be resolved. If this is actually ""how things should work,"" I\'d greatly appreciate it if someone could explain why the different indexing styles produce these different results\r\n'"
8601,46499065,gbakalian,jreback,2014-10-22 11:27:11,2014-11-06 12:43:58,2014-11-06 12:43:58,closed,,0.15.2,4,Bug;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/8601,b'Exception during resample w/ tz',"b""xref #8653\r\n \r\nHaving a similar issue\r\n\r\nidx = date_range('2014-10-08 00:00','2014-10-09 00:00', freq='D', tz='Europe/Berlin')\r\npd.Series(5, idx).resample('MS')\r\n\r\nValueError: Values falls after last bin\r\n\r\n\r\nWhile if I remove the tz :\r\n\r\nidx = date_range('2014-10-08 00:00','2014-10-09 00:00', freq='D')\r\npd.Series(5, idx).resample('MS')\r\n\r\nthat works perfectly\r\n\r\nversion : '0.15.0rc1'"""
8596,46428546,miketkelly,jreback,2014-10-21 18:38:30,2016-05-27 12:41:32,2016-05-27 12:41:32,closed,,Next Major Release,11,Bug;Difficulty Advanced;Dtypes;Effort Medium;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8596,b'BUG: DataFrame outer merge changes key columns from int64 to float64',"b""\r\n```\r\nimport pandas as pd\r\n\r\ndf1 = pd.DataFrame({'key': [1,2,3,4], 'val1': [1,2,3,4]})\r\ndf2 = pd.DataFrame({'key': [1,2,3,5], 'val2': [1,2,3,4]})\r\n\r\ndf = df1.merge(df2, how='outer')\r\n```\r\n\r\nWas expecting `key` to stay int64, since a merge can't introduce missing key values if none were present in the inputs.\r\n\r\n```\r\nprint df.dtypes\r\n\r\nkey     float64\r\nval1    float64\r\nval2    float64\r\ndtype: object\r\n```\r\n\r\nVersion 0.15.0-6-g403f38d"""
8591,46344467,bazeli,jreback,2014-10-21 01:12:19,2014-10-31 20:18:44,2014-10-31 20:18:44,closed,,0.15.1,2,Bug;Compat;Frequency,https://api.github.com/repos/pydata/pandas/issues/8591,"b""Pandas 15.0:  TypeError: can't pickle busdaycalendar objects""","b'Using the documentation\'s own CustomBusinessDay example, you can no longer save objects with a CustomBusinessDay frequency to an HDF store in Pandas 15.0 Final (but worked in 14.1):\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\nfrom datetime import datetime\r\nweekmask_egypt = \'Sun Mon Tue Wed Thu\'\r\nholidays = [\'2012-05-01\', datetime(2013, 5, 1), np.datetime64(\'2014-05-01\')]\r\nbday_egypt = CustomBusinessDay(holidays=holidays, weekmask=weekmask_egypt)\r\ndt = datetime(2013, 4, 30)\r\ndts = pd.date_range(dt, periods=5, freq=bday_egypt)\r\ns = (pd.Series(dts.weekday, dts).map(pd.Series(\'Mon Tue Wed Thu Fri Sat Sun\'.split())))\r\nstore = pd.HDFStore(\'test.hdf\')\r\nstore.put(\'test\',s)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py"", line 831, in put\r\n    self._write_to_group(key, value, append=append, **kwargs)\r\n  File ""/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py"", line 1280, in _write_to_group\r\n    s.write(obj=value, append=append, complib=complib, **kwargs)\r\n  File ""/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2557, in write\r\n    self.write_index(\'index\', obj.index)\r\n  File ""/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2316, in write_index\r\n    node._v_attrs.freq = index.freq\r\n  File ""/usr/local/lib/python2.7/site-packages/tables/attributeset.py"", line 455, in __setattr__\r\n    self._g__setattr(name, value)\r\n  File ""/usr/local/lib/python2.7/site-packages/tables/attributeset.py"", line 397, in _g__setattr\r\n    self._g_setattr(self._v_node, name, stvalue)\r\n  File ""hdf5extension.pyx"", line 700, in tables.hdf5extension.AttributeSet._g_setattr (tables/hdf5extension.c:6623)\r\n  File ""/usr/local/Cellar/python/2.7.8/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy_reg.py"", line 70, in _reduce_ex\r\n    raise TypeError, ""can\'t pickle %s objects"" % base.__name__\r\nTypeError: can\'t pickle busdaycalendar objects\r\n\r\n\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.15.0\r\nnose: 1.3.1\r\nCython: 0.20.2\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.2.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None'"
8589,46301982,rosnfeld,jreback,2014-10-20 17:20:06,2014-10-30 11:19:44,2014-10-24 18:36:38,closed,,0.15.1,2,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8589,"b'cut/qcut ""retbins"" broken in 0.15.0'","b""Works in 0.14.1:\r\n```python\r\nIn [11]: s = pd.Series(np.random.randn(100))\r\n\r\nIn [12]: pd.cut(s, 10, retbins=True)\r\nOut[12]: \r\n(   (0.993, 1.376]\r\n  (-0.924, -0.541]\r\n   (-0.158, 0.226]\r\n  (-1.695, -1.308]\r\n  (-0.924, -0.541]\r\n  (-0.541, -0.158]\r\n   (-0.158, 0.226]\r\n  (-0.541, -0.158]\r\n    (0.609, 0.993]\r\n    (0.226, 0.609]\r\n    (0.226, 0.609]\r\n    (0.609, 0.993]\r\n    (0.609, 0.993]\r\n ...\r\n    (0.993, 1.376]\r\n  (-0.541, -0.158]\r\n    (0.226, 0.609]\r\n    (0.226, 0.609]\r\n   (-0.158, 0.226]\r\n    (0.226, 0.609]\r\n   (-0.158, 0.226]\r\n  (-0.924, -0.541]\r\n  (-1.695, -1.308]\r\n    (0.993, 1.376]\r\n   (-0.158, 0.226]\r\n  (-0.924, -0.541]\r\n    (0.993, 1.376]\r\n Levels (10): Index(['(-1.695, -1.308]', '(-1.308, -0.924]',\r\n                     '(-0.924, -0.541]', '(-0.541, -0.158]',\r\n                     '(-0.158, 0.226]', '(0.226, 0.609]',\r\n                     '(0.609, 0.993]', '(0.993, 1.376]', '(1.376, 1.759]',\r\n                     '(1.759, 2.143]'], dtype=object)\r\n Length: 100,\r\n array([-1.69479925, -1.30760325, -0.92424086, -0.54087848, -0.1575161 ,\r\n         0.22584629,  0.60920867,  0.99257106,  1.37593344,  1.75929582,\r\n         2.14265821]))\r\n```\r\n\r\nbut does not work in 0.15.0/current master:\r\n\r\n```\r\nIn [13]: pd.cut(s, 10, retbins=True)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-958e04d69222> in <module>()\r\n----> 1 pd.cut(s, 10, retbins=True)\r\n\r\n/home/andrew/git/pandas-rosnfeld/pandas/tools/tile.pyc in cut(x, bins, right, labels, retbins, precision, include_lowest)\r\n    113                         include_lowest=include_lowest)\r\n    114     if isinstance(x, Series):\r\n--> 115         res = Series(res, index=x.index)\r\n    116     return res\r\n    117 \r\n\r\n/home/andrew/git/pandas-rosnfeld/pandas/core/series.pyc in __init__(self, data, index, dtype, name, copy, fastpath)\r\n    210                                        raise_cast_failure=True)\r\n    211 \r\n--> 212                 data = SingleBlockManager(data, index, fastpath=True)\r\n    213 \r\n    214         generic.NDFrame.__init__(self, data, fastpath=True)\r\n\r\n/home/andrew/git/pandas-rosnfeld/pandas/core/internals.pyc in __init__(self, block, axis, do_integrity_check, fastpath)\r\n   3324             block = make_block(block,\r\n   3325                                placement=slice(0, len(axis)),\r\n-> 3326                                ndim=1, fastpath=True)\r\n   3327 \r\n   3328         self.blocks = [block]\r\n\r\n/home/andrew/git/pandas-rosnfeld/pandas/core/internals.pyc in make_block(values, placement, klass, ndim, dtype, fastpath)\r\n   2058 \r\n   2059     return klass(values, ndim=ndim, fastpath=fastpath,\r\n-> 2060                  placement=placement)\r\n   2061 \r\n   2062 \r\n\r\n/home/andrew/git/pandas-rosnfeld/pandas/core/internals.pyc in __init__(self, values, ndim, fastpath, placement)\r\n   1372         super(ObjectBlock, self).__init__(values, ndim=ndim,\r\n   1373                                           fastpath=fastpath,\r\n-> 1374                                           placement=placement)\r\n   1375 \r\n   1376     @property\r\n\r\n/home/andrew/git/pandas-rosnfeld/pandas/core/internals.pyc in __init__(self, values, placement, ndim, fastpath)\r\n     73             raise ValueError('Wrong number of items passed %d,'\r\n     74                              ' placement implies %d' % (\r\n---> 75                                  len(self.values), len(self.mgr_locs)))\r\n     76 \r\n     77     @property\r\n\r\nValueError: Wrong number of items passed 2, placement implies 100\r\n```\r\n\r\nThe same goes for qcut. These work if retbins=False.\r\n\r\nI found this while testing out the new release, wanted to take a look at the Categorical type. I should have done this with the RC and caught it sooner..."""
8587,46223425,pallav-fdsi,jreback,2014-10-20 00:50:35,2014-10-30 11:20:23,2014-10-20 12:23:30,closed,,0.15.1,6,Bug;IO Google,https://api.github.com/repos/pydata/pandas/issues/8587,b'BUG: LooseVersion was used incorrectly',"b'closes #8581 \r\n\r\nVersion comparison had a formatting error resulting in ""TypeError: expected string or buffer"" and preventing Spyder from loading (and probably other issues). Fixed.'"
8585,46217313,behzadnouri,jreback,2014-10-19 21:00:29,2014-12-07 13:23:35,2014-10-28 00:04:25,closed,,0.15.1,11,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8585,b'BUG: column name conflict & as_index=False breaks groupby ops',b'closes https://github.com/pydata/pandas/issues/7115\r\ncloses https://github.com/pydata/pandas/issues/8112\r\ncloses https://github.com/pydata/pandas/issues/8582\r\n'
8582,46208111,behzadnouri,jreback,2014-10-19 15:18:41,2014-10-30 11:19:44,2014-10-28 00:04:25,closed,,0.15.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8582,"b'groupby with as_index=False, inconsistency between max and idxmax'","b""    >>> df = pd.DataFrame(np.random.randint(0, 100, (50, 2)), columns=['jim', 'joe'])\r\n    >>> ts = pd.Series(np.random.randint(0, 3, 50))\r\n    >>> df.groupby(ts, as_index=False).idxmax()\r\n       jim  joe\r\n    0   25   36\r\n    1   22    5\r\n    2    3    4\r\n    >>> df.groupby(ts, as_index=False).max()\r\n       NaN  jim  joe\r\n    0    0   93   94\r\n    1    1   94   89\r\n    2    2   85   93\r\n\r\n`max` adds the grouper to the result frame, whereas `idxmax` doesn't.\r\n\r\nThe API documentation [says](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby):\r\n> as_index=False is effectively \xa1\xb0SQL-style\xa1\xb1 grouped output\r\n\r\nso i would assume, it should not inject a new column with `NaN` name."""
8576,46156066,cpcloud,jreback,2014-10-18 00:01:47,2015-04-30 01:56:20,2014-10-18 12:55:27,closed,cpcloud,0.15.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/8576,b'BUG/FIX: Fix dtype inference issue in query',b'closes #8568 '
8573,46099492,ginzor,jreback,2014-10-17 13:33:26,2016-04-19 12:03:05,2016-04-19 12:03:05,closed,,0.18.1,4,Bug;Difficulty Novice;Effort Low;Groupby;Resample;Testing,https://api.github.com/repos/pydata/pandas/issues/8573,b'Using resample() with groupby on this DataFrame causes Segmentation Fault',"b'When trying to resample timestamps into 5 minute time slots grouping on an id column (tried both counting and summing aggregation in \'how\' parameter). In a DataFrame with TimeSeries data I get a memory crash, i.e. Segmentation Fault.\r\n\r\nI reduced the DataFrame as far as I could in reproducing the crash. Also noted that it will not cause a segfault if I sort the index (don\'t know if this is needed for resample() function could not find such documentation).\r\n\r\n```python\r\nimport datetime\r\nimport pandas as pd\r\n\r\nall_wins_and_wagers =\\\r\n[(1L, datetime.datetime(2013, 10, 1, 16, 20), 1L, 0L),\r\n (2L, datetime.datetime(2013, 10, 1, 16, 10), 1L, 0L),\r\n (2L, datetime.datetime(2013, 10, 1, 18, 15), 1L, 0L),\r\n (2L, datetime.datetime(2013, 10, 1, 16, 10, 31), 1L, 0L)]\r\n\r\ndf = pd.DataFrame.from_records(all_wins_and_wagers, columns=(""ID"", ""timestamp"", ""A"", ""B"")).set_index(""timestamp"")\r\ndf_resampled = df.groupby(""ID"").resample(""5min"", ""sum"")\r\n```\r\n\r\nTried on following setups of pandas.\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16-2-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.4\r\nCython: None\r\nnumpy: 1.9.0\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 2.3.0\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.7\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.0.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16-2-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.1\r\nCython: None\r\nnumpy: 1.8.0\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 2.3.0\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2012c\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.0.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nsqlalchemy: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None'"
8568,45985382,kay1793,jreback,2014-10-16 13:29:13,2014-10-18 12:56:00,2014-10-18 12:56:00,closed,,0.15.0,9,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8568,"b""df.query doesn't handle negation properly""","b""```python\r\n\r\nIn [13]: df=pd.DataFrame([[0,10],[1,20]],columns=['cat','count'])\r\n    ...: df\r\nOut[13]: \r\n   cat  count\r\n0    0     10\r\n1    1     20\r\n\r\nIn [14]: df.query('cat > 0')\r\nOut[14]: \r\n   cat  count\r\n1    1     20\r\n\r\nIn [15]: df.query('~(cat > 0)')\r\nOut[15]: \r\n   cat  count\r\n1    1     20\r\n0    0     10\r\n\r\nIn [16]: df.query(' not (cat > 0)')\r\nOut[16]: \r\n   cat  count\r\n1    1     20\r\n0    0     10\r\n```\r\nI expected the two last results to return only the first row."""
8542,45563739,jgbarah,jreback,2014-10-11 22:16:59,2016-04-29 13:40:30,2016-04-29 13:40:30,closed,,0.18.1,9,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/8542,"b'groupby, TimeGrouper error'","b'The following code seems to raise an error, since the result object does not make sense (well, at least to me):\r\n\r\n```\r\nIn [61]: import datetime\r\n\r\nIn [62]: import pandas as pd\r\n\r\nIn [63]: df = pd.DataFrame.from_records ( [[datetime.datetime(2014,9,10),1234,""start""],\r\n  [datetime.datetime(2013,10,10),1234,""start""]], columns = [""date"", ""change"", ""event""] )\r\n\r\nIn [64]: df\r\nOut[64]: \r\n        date  change  event\r\n0 2014-09-10    1234  start\r\n1 2013-10-10    1234  start\r\n\r\nIn [65]: ts = df.set_index(\'date\')\r\n\r\nIn [66]: ts\r\nOut[66]: \r\n            change  event\r\ndate                     \r\n2014-09-10    1234  start\r\n2013-10-10    1234  start\r\n\r\nIn [67]: byperiod = ts.groupby([pd.TimeGrouper(freq=""M""), ""event""], as_index=False)\r\n\r\nIn [68]: byperiod.groups\r\nOut[68]: \r\n{<pandas.tseries.resample.TimeGrouper at 0xab6bcaec>: [Timestamp(\'2014-09-10 00:00:00\')],\r\n \'event\': [Timestamp(\'2013-10-10 00:00:00\')]}\r\n```\r\n\r\nI would expect, for Out[68], two groups, one for each (date, event) pair.\r\n\r\nAm I wring, or this is a bug?'"
8541,45560509,TomAugspurger,TomAugspurger,2014-10-11 19:51:12,2014-10-30 11:20:22,2014-10-28 03:56:19,closed,,0.15.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8541,b'BUG: Plot with ``label`` would overwrite index name',"b'Closes https://github.com/pydata/pandas/issues/8494\r\n\r\nI took the easy solution and just copied the series.\r\n\r\nAlternatively we could attach a label property to the MPLObject, and check that wherever we check the index name.'"
8532,45490259,jorisvandenbossche,jreback,2014-10-10 15:05:59,2015-03-17 00:18:39,2015-03-17 00:18:39,closed,,0.16.0,5,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8532,b'BUG: Series not correctly formatted in truncated output',"b'Overview of the issues:\r\n\r\n- [ ] Number of rows is incorrect (#7508)\r\n- [ ] Setting `pd.options.display.max_rows` to larger values has no effect (https://github.com/pydata/pandas/issues/7508#issuecomment-58669898)\r\n- [ ] formatting seems to occur seperately for both parts (this issue #8532, see two exaples below)\r\n\r\n\r\n----\r\n\r\nWhen a Series gets truncated, it seems that both parts are formatted separately, with the consequence that eg:\r\n\r\n- the float formatting can be different in both parts\r\n- the alignment (depending on the width of the index) is different in both parts\r\n\r\nNote that this is not the case with a DataFrame\r\n\r\n```\r\nIn [44]: s = pd.Series([1,1,1,1,1,1,1,1,1,1,0.9999,1,1]*10)\r\n\r\nIn [45]: pd.options.display.max_rows = 10\r\n\r\nIn [46]: s\r\nOut[46]:\r\n0    1\r\n1    1\r\n2    1\r\n...\r\n127    0.9999\r\n128    1.0000\r\n129    1.0000\r\nLength: 130, dtype: float64\r\n\r\nIn [47]: s.to_frame()\r\nOut[47]:\r\n          0\r\n0    1.0000\r\n1    1.0000\r\n2    1.0000\r\n3    1.0000\r\n4    1.0000\r\n..      ...\r\n125  1.0000\r\n126  1.0000\r\n127  0.9999\r\n128  1.0000\r\n129  1.0000\r\n\r\n[130 rows x 1 columns]\r\n```\r\n'"
8526,45441435,behzadnouri,jreback,2014-10-10 02:56:48,2014-11-22 14:47:11,2014-11-21 23:20:24,closed,,0.15.2,21,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8526,b'index into multi-index past the lex-sort depth',"b""closes https://github.com/pydata/pandas/issues/7724\r\ncloses https://github.com/pydata/pandas/issues/2646\r\n\r\non current master:\r\n\r\n    >>> df\r\n                               jolia\r\n    jim joe jolie      joline       \r\n    1   z   2014-10-14 a          30\r\n        y   2014-10-13 b           3\r\n        x   2014-10-12 c          15\r\n    0   z   2014-10-11 d          35\r\n        y   2014-10-10 e          43\r\n        x   2014-10-09 f          36\r\n\r\nFalse negative if the key length exceeds lexsort depth:\r\n\r\n    >>> (0,) in df.index\r\n    False\r\n    >>> (0, 'z') in df.index\r\n    False\r\n    >>> (0, 'z', '2014-10-11') in df.index\r\n    False\r\n    >>> (0, 'z', Timestamp('2014-10-11')) in df.index\r\n    False\r\n    >>> (0, 'z', '2014-10-11', 'd') in df.index\r\n    False\r\n\r\nonly ones which work:\r\n\r\n    >>> 0 in df.index\r\n    True\r\n    >>> (0, 'z', Timestamp('2014-10-11'), 'd') in df.index\r\n    True\r\n\r\nwhich take a different code paths. The last one only works if the index is unique:\r\n\r\n    >>> (0, 'z', Timestamp('2014-10-11'), 'd') in pd.concat([df, df]).index\r\n    False\r\n\r\nfor all of the false negative cases above, obviously `df.loc[key]` fails:\r\n\r\n    >>> df.loc[(0, 'z', Timestamp('2014-10-11'))]\r\n    KeyError: 'Key length (3) was greater than MultiIndex lexsort depth (0)'\r\n\r\n*Some of these issues persist even if the index is lexically sorted:*\r\n\r\n    >>> df.sort_index(inplace=True)\r\n    >>> df  # lexically sorted\r\n                               jolia\r\n    jim joe jolie      joline       \r\n    0   x   2014-10-09 f          36\r\n        y   2014-10-10 e          43\r\n        z   2014-10-11 d          35\r\n    1   x   2014-10-12 c          15\r\n        y   2014-10-13 b           3\r\n        z   2014-10-14 a          30\r\n\r\ndate-time indexing with a full-key fails if index is unique:\r\n\r\n    >>> (0, 'x', '2014-10-09') in df.index  # partial key, works!\r\n    True\r\n    >>> (0, 'x', '2014-10-09', 'f') in df.index  # full key, unique index, breaks!\r\n    False\r\n\r\nalso, non-unique lexically sorted index always returns false positive with any full key:\r\n\r\n    >>> xdf = pd.concat([df, df]).sort_index()\r\n    >>> xdf\r\n                               jolia\r\n    jim joe jolie      joline       \r\n    0   x   2014-10-09 f          36\r\n                       f          36\r\n        y   2014-10-10 e          43\r\n                       e          43\r\n        z   2014-10-11 d          35\r\n                       d          35\r\n    1   x   2014-10-12 c          15\r\n                       c          15\r\n        y   2014-10-13 b           3\r\n                       b           3\r\n        z   2014-10-14 a          30\r\n                       a          30\r\n    >>> (0, '$', '2014-10-09') in xdf.index  # partial key works\r\n    False\r\n    >>> (0, '$', '2014-10-09', '#') in xdf.index  # full key always `True`\r\n    True\r\n    >>> xdf.loc[(0, '$', '2014-10-09', '#')]\r\n    KeyError: 'the label [$] is not in the [columns]'"""
8523,45392182,jreback,jreback,2014-10-09 17:11:37,2014-10-09 18:32:16,2014-10-09 18:32:16,closed,,0.15.0,0,Bug;Dtypes;Indexing;Regression,https://api.github.com/repos/pydata/pandas/issues/8523,b'BUG/REGR: bool-like Indexes not properly coercing to object (GH8522)',b'closes #8522 '
8520,45358277,jreback,jreback,2014-10-09 12:06:03,2014-10-09 13:36:59,2014-10-09 13:36:59,closed,,0.15.0,0,Bug;Indexing;Internals,https://api.github.com/repos/pydata/pandas/issues/8520,b'BUG: Bug in inplace operations with column sub-selections on the lhs (GH8511)',b'closes #8511\r\ncloses #5104 '
8511,45288711,dieterv77,jreback,2014-10-08 19:55:26,2014-10-09 13:36:59,2014-10-09 13:36:59,closed,,0.15.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8511,b'Indexing problem with in-place operator',"b'Hi, thanks to all of you who spend time on this project, it is invaluable to me.\r\nUnfortuatnely, i think I have run into an indexing-type issue, either that or my expectations are incorrect.  Consider the following script:\r\n\r\n```python\r\nimport random\r\nimport pandas\r\nimport numpy as np\r\n\r\ncolumns = list(\'abcdefg\')\r\nsubcols = columns[1:-1]\r\nX = pandas.DataFrame(0.0, columns=columns, index=range(100))\r\nZ = pandas.DataFrame(np.random.randn(100,len(subcols)), columns=subcols, index=range(100))\r\nblock1 = list(subcols)\r\nrandom.shuffle(block1)\r\n\r\nX[block1] -= Z\r\nprint X.corrwith(Z)\r\n\r\nX = pandas.DataFrame(0.0, columns=columns, index=range(100))\r\nX[block1] -= Z[block1]\r\nprint X.corrwith(Z)\r\n\r\nprint pandas.__version__\r\n```\r\nThe output i get is something like this (obviously depends on the random inputs)\r\na         NaN\r\nb   -0.127386\r\nc   -0.073521\r\nd   -0.073521\r\ne   -0.127386\r\nf   -1.000000\r\ng         NaN\r\ndtype: float64\r\na   NaN\r\nb    -1\r\nc    -1\r\nd    -1\r\ne    -1\r\nf    -1\r\ng   NaN\r\ndtype: float64\r\n0.15.0rc1-10-g215569a\r\n\r\nwhere the second result is what i would have expected.\r\nI was surprised to find that I have to ""reindex"" Z by block1 to get a correct result.  Is my expectation incorrect?\r\n\r\nMany thanks\r\n'"
8505,45209189,wikiped,sinhrks,2014-10-08 06:34:28,2015-08-08 19:46:01,2015-08-08 19:46:01,closed,,0.17.0,2,Bug;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8505,b'Behavior of pandas.DataFrame.duplicated()',"b""While trying to get handle on duplicated records I stumble upon [this](http://stackoverflow.com/questions/26244309/how-to-analyze-all-duplicate-entries-in-this-pandas-dataframe) which lead to conclusion that `.duplicated(take_last=True)` seem to be taking the first of duplicate rows and `.duplicate(take_last=False)` takes the last rows.\r\nHere is an illustration:\r\n\r\n    import pandas as pd\r\n    data = { 'key1':[1,2,3,1,2,3,2,2,2],\r\n          'key2':[2,2,1,2,2,2,2,2,2],\r\n          'dup':['d1_1','d2_1', 'n_d','d1_2','d2_2', 'n_d','d2_3','d2_4','d2_5']}\r\n    df = pd.DataFrame(data,columns=['key1','key2','dup'])\r\n    print df\r\n       key1  key2  dup\r\n    0     1     2  d1_1\r\n    1     2     2  d2_1\r\n    2     3     1   n_d\r\n    3     1     2  d1_2\r\n    4     2     2  d2_2\r\n    5     3     2   n_d\r\n    6     2     2  d2_3\r\n    7     2     2  d2_4\r\n    8     2     2  d2_5\r\n\r\nNow with `take_last=False` it would be fair to expect dn_1s to be in output, but this is not the case:\r\n\r\n    c1 = df.duplicated(['key1', 'key2'], take_last=False)\r\n    df[c1]\r\n       key1  key2   dup\r\n    3     1     2  d1_2\r\n    4     2     2  d2_2\r\n    6     2     2  d2_3\r\n    7     2     2  d2_4\r\n    8     2     2  d2_5\r\n\r\nAnd `take_last=True` outputs the first rows:\r\n\r\n    c2 = df.duplicated(['key1', 'key2'], take_last=True)\r\n    df[c2]\r\n       key1  key2   dup\r\n    0     1     2  d1_1\r\n    1     2     2  d2_1\r\n    4     2     2  d2_2\r\n    6     2     2  d2_3\r\n    7     2     2  d2_4\r\n\r\nUnless I am misunderstanding the doc:\r\n\r\n    take_last : boolean, default False\r\n        Take the last observed row in a row. Defaults to the first row\r\n\r\nit does feel that `.duplicated()` could be improved by fixing this behavior.\r\nAnd additionally it would have one more parameter:\r\n\r\n    take_all : boolean, default False\r\n        Take all observed rows. Overrides take_last\r\n\r\nor alternatively a keyword parameter:\r\n\r\n    take : 'last', 'first', 'all', default 'last'\r\n        Sets which observed duplicated rows to take. Default: take last observed rows.\r\n\r\nRight now trying to get all observed duplicates requires applying two above conditions `c1 | c2`.\r\nThis was done with pandas 0.14.1.\r\nThank you."""
8494,45034067,dpk011,TomAugspurger,2014-10-06 20:44:01,2014-10-30 11:19:43,2014-10-28 03:56:19,closed,,0.15.1,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8494,"b""Setting the label parameter in the call to DF.plot alters the DataFrame's index name""","b""Setting the label parameter in the call to df.plot alters the DataFrame's index name. For example,\r\n\r\n    df = pd.DataFrame(data=np.random.rand(5,3), index=np.arange(5), columns=['A', 'B', 'C'])\r\n    df.index.name='RowNumber'\r\n    df.plot(y='B', label='Test')\r\n    print df.index.name\r\n\r\n    Out[1]: Test\r\n\r\nSee a better example at [StackOverflow](http://stackoverflow.com/questions/26068598/specifying-y-array-and-plot-label-in-pandas-dataframe-plot-changes-the-dataframe). \r\n[An answer in above link](http://stackoverflow.com/questions/26068598/specifying-y-array-and-plot-label-in-pandas-dataframe-plot-changes-the-dataframe/26070511#26070511) points problem to [here](https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py#L2278). \r\npandas version 0.14.1. """
8491,45029213,jorisvandenbossche,TomAugspurger,2014-10-06 20:02:02,2014-10-11 19:27:25,2014-10-11 19:27:25,closed,,0.15.0,2,Bug;Docs;Visualization,https://api.github.com/repos/pydata/pandas/issues/8491,"b""VIS: legend ('None') also added when no label is given""","b""Compare stable http://pandas.pydata.org/pandas-docs/stable/visualization.html#basic-plotting-plot vs dev docs http://pandas-docs.github.io/pandas-docs-travis/visualization.html#basic-plotting-plot\r\n\r\nFor the simple example of Series plotting, a legens is added with entry None when there should be no legend:\r\n\r\n```\r\nts = Series(randn(1000), index=date_range('1/1/2000', periods=1000))\r\nts = ts.cumsum()\r\nts.plot()\r\n```"""
8484,44982195,jreback,jreback,2014-10-06 13:19:58,2014-10-06 13:54:52,2014-10-06 13:54:52,closed,,0.15.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8484,b'BUG: Bug in groupby .apply with a non-affecting mutation in the function (GH8467)',b'closes #8467 '
8480,44944844,behzadnouri,jreback,2014-10-06 03:31:45,2014-10-07 00:36:25,2014-10-06 13:03:02,closed,,0.15.0,1,API Design;Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8480,b'BUG: sub-frame assignment of a multi-index frame breaks alignment',"b""closes https://github.com/pydata/pandas/issues/7655\r\n\r\n    >>> df\r\n                 jim                                  joe                                jolie                               \r\n               first              last              first              last              first              last             \r\n                left center right left center right  left center right left center right  left center right left center right\r\n    2000-01-03     0      3     6    9     12    15    18     21    24   27     30    33    36     39    42   45     48    51\r\n    2000-01-04     1      4     7   10     13    16    19     22    25   28     31    34    37     40    43   46     49    52\r\n    2000-01-05     2      5     8   11     14    17    20     23    26   29     32    35    38     41    44   47     50    53\r\n\r\n    >>> i, j = df.index.values.copy(), it[-1][:]\r\n    >>> np.random.shuffle(i)\r\n    >>> df['jim'] = df['jolie'].loc[i, ::-1]\r\n    >>> df.loc[:, ['jim', 'jolie']]\r\n                 jim                                jolie                               \r\n               first              last              first              last             \r\n                left center right left center right  left center right left center right\r\n    2000-01-03    36     39    42   45     48    51    36     39    42   45     48    51\r\n    2000-01-04    37     40    43   46     49    52    37     40    43   46     49    52\r\n    2000-01-05    38     41    44   47     50    53    38     41    44   47     50    53\r\n\r\n    >>> np.random.shuffle(j)\r\n    >>> df[('joe', 'first')] = df[('jolie', 'last')].loc[i, j]\r\n    >>> np.random.shuffle(j)\r\n    >>> df[('joe', 'last')] = df[('jolie', 'first')].loc[i, j]\r\n    >>> df.loc[:, ['joe', 'jolie']]\r\n                 joe                                jolie                               \r\n               first              last              first              last             \r\n                left center right left center right  left center right left center right\r\n    2000-01-03    45     48    51   36     39    42    36     39    42   45     48    51\r\n    2000-01-04    46     49    52   37     40    43    37     40    43   46     49    52\r\n    2000-01-05    47     50    53   38     41    44    38     41    44   47     50    53\r\n"""
8476,44931103,jreback,jreback,2014-10-05 22:14:34,2014-10-05 23:52:47,2014-10-05 23:52:47,closed,,0.15.0,0,Bug;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8476,b'BUG: allow std to work with timedeltas (GH8471)',b'close #8471 '
8475,44928365,jreback,jreback,2014-10-05 20:42:22,2014-10-05 21:15:45,2014-10-05 21:15:45,closed,,0.15.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8475,b'BUG: .at indexing should allow enlargement scalars w/o regards to the type of index (GH8473)',b'closes #8473'
8471,44916527,jreback,jreback,2014-10-05 14:57:48,2014-10-05 23:52:47,2014-10-05 23:52:47,closed,,0.15.0,0,Bug;Difficulty Novice;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8471,b'BUG: allow .std() of timedelta types',"b""This currently raises ``TypeError`` as its disabled in ``core/nanops.py``, see here: https://github.com/pydata/pandas/commit/d29d4c6623a29db36a4eb8d062bdcfe603e2b490\r\n\r\nThe issue is that ``.var()`` can generate a value that is too big for the timedeltas to hold propery, and std CALLS var.\r\n\r\nsolns:\r\n\r\n- reduce the value of the timedeltas going into var to make them work (and inflate on exit), e.g. divide by 1e9 or something. lose some small amount of precision.\r\n- allow std to return a valid value by passing a flag thru to var (and don't wrap the results). a bit hacky\r\n\r\n```\r\nIn [7]: max_int = np.iinfo(np.int64).max\r\n\r\nIn [8]: max_int\r\nOut[8]: 9223372036854775807\r\n\r\nIn [9]: pd.Timedelta(max_int)\r\nOut[9]: Timedelta('106751 days 23:47:16.854775')\r\n\r\nIn [10]: big_float = 3e19\r\n\r\nIn [12]: pd.Timedelta(big_float)\r\n---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\nOverflowError: Python int too large to convert to C long\r\n\r\nIn [13]: pd.Timedelta(np.sqrt(big_float))\r\nOut[13]: Timedelta('0 days 00:00:05.477225')\r\n```"""
8462,44888729,immerrr,jreback,2014-10-04 17:18:39,2014-10-05 19:03:13,2014-10-05 14:05:18,closed,,0.15.0,8,Bug;Indexing;Internals,https://api.github.com/repos/pydata/pandas/issues/8462,b'BUG: fix Index.reindex to preserve type when target is empty list/ndarray',"b""This PR fixes #7774 and also includes:\r\n\r\nTST: check index/columns types when doing empty loc/ix tests\r\n\r\nCLN: don't _ensure_index in NDFrame._reindex_axes, it is done in Index.reindex"""
8461,44887167,jreback,jreback,2014-10-04 16:18:56,2014-10-04 16:20:49,2014-10-04 16:20:49,closed,,0.15.0,0,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8461,b'BUG: fix applymap to handle Timedelta',
8460,44884150,immerrr,jreback,2014-10-04 14:18:29,2014-11-02 21:33:35,2014-10-04 20:02:08,closed,,0.15.0,5,Bug;Indexing;Internals,https://api.github.com/repos/pydata/pandas/issues/8460,b'BUG: fix Index.reindex to preserve name when target is list/ndarray',"b""This PR fixes #6552 and also includes:\r\n\r\nCLN: drop copy_if_needed kwarg of Index.reindex, it's True everywhere\r\nTST: enable back empty-list loc/ix tests that failed before\r\nDOC: Bump Index.reindex docstring"""
8457,44854479,behzadnouri,jreback,2014-10-03 22:15:31,2014-10-04 17:41:00,2014-10-04 17:19:50,closed,,0.15.0,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8457,b'BUG: type diversity breaks alignment',"b""closes https://github.com/pydata/pandas/issues/8258, but more generally, on pandas master, assignment with indexer will align _both rows and columns_ if all columns have the same type:\r\n\r\n    >>> cols, idx = ['jim', 'joe', 'jolie'], ['first', 'last']\r\n    >>> vals = np.arange(1, 7).reshape(2, 3, order='F')\r\n    >>> df = DataFrame(vals, columns=cols, index=idx)\r\n    >>> left, rhs = df.copy(), - df.iloc[::-1, -2::-1]\r\n    >>> print(left, rhs, sep='\\n')\r\n           jim  joe  jolie\r\n    first    1    3      5\r\n    last     2    4      6\r\n           joe  jim\r\n    last    -4   -2\r\n    first   -3   -1\r\n    >>> \r\n    >>> left.loc[:, ['jim', 'joe']] = rhs\r\n    >>> left\r\n           jim  joe  jolie\r\n    first   -1   -3      5\r\n    last    -2   -4      6\r\n\r\nHowever, if I change the type of a column even _outside_ the section which gets assigned to, _only columns_ are aligned:\r\n\r\n    >>> df = DataFrame(vals, columns=cols, index=idx)\r\n    >>> df['jolie'] = df['jolie'].astype('float64')\r\n    >>> left, rhs = df.copy(), - df.iloc[::-1, -2::-1]\r\n    >>> print(left, rhs, sep='\\n')\r\n           jim  joe  jolie\r\n    first    1    3      5\r\n    last     2    4      6\r\n           joe  jim\r\n    last    -4   -2\r\n    first   -3   -1\r\n    >>> \r\n    >>> left.loc[:, ['jim', 'joe']] = rhs\r\n    >>> left\r\n           jim  joe  jolie\r\n    first   -2   -4      5\r\n    last    -1   -3      6\r\n\r\n\r\nCode-wise this is a bug, because [this line](https://github.com/pydata/pandas/blob/fccd7feabfe0bf15c30d712f3a4a4ff71e76ad4a/pandas/core/indexing.py#L446) is taking the _entire frame_'s index and re-indexes to that, regardless of the actual indexer which selects a section of the frame to assign to. That said, there are two places in tests where they rely on this behaviour, [here](https://github.com/pydata/pandas/blob/fccd7feabfe0bf15c30d712f3a4a4ff71e76ad4a/pandas/tests/test_frame.py#L1405) and [here](https://github.com/pydata/pandas/blob/fccd7feabfe0bf15c30d712f3a4a4ff71e76ad4a/pandas/tests/test_indexing.py#L1033)."""
8452,44816814,woztheproblem,jreback,2014-10-03 15:49:28,2014-12-07 22:59:39,2014-12-07 22:27:38,closed,,0.15.2,2,Bug;IO HTML;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8452,b'MultiIndex Column Offset Bug with to_html(index=False)',"b""I'm trying to use to_html() to print a DataFrame that has a MultiIndex column, and I want to hide the index.  When I do, the column labels are shifted right by one column.  I use: \r\n\r\n```python\r\nimport pandas as pd\r\nfrom IPython.display import HTML\r\ndf = pd.DataFrame({'a': range(10), 'b': range(10,20), 'c': range(10,20), 'd': range(10,20)})\r\ndf.columns = pd.MultiIndex.from_product([['a', 'b'], ['c', 'd']])\r\nHTML(df.to_html(index=False))\r\n```\r\n\r\nand the result is: \r\n![screen shot 2014-10-03 at 10 45 31 am](https://cloud.githubusercontent.com/assets/713139/4508278/6b5f4dca-4b14-11e4-818a-85f68bb67838.png)\r\n\r\nSeems to me to_html() isn't properly accounting for the index=False when there is a column Multindex.  With single-level column index ( df.columns=list('abcd') )  it prints fine.  With index=True, to_html() also works fine.  \r\n\r\nMy versions are: \r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.9.0\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.7\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None"""
8449,44796227,jreback,jreback,2014-10-03 12:05:53,2014-10-03 12:33:46,2014-10-03 12:33:46,closed,,0.15.0,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8449,b'BUG: bug in df.info() when embedded categorical (related GH7619)',b'xref #7619 '
8443,44689133,unutbu,jreback,2014-10-02 14:00:59,2014-10-02 16:18:39,2014-10-02 15:15:11,closed,,0.15.0,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8443,b'BUG: NDFrame.equals gives false negatives with dtype=object (GH8437)',
8441,44665463,lyakishev,jreback,2014-10-02 09:15:47,2014-10-02 11:35:24,2014-10-02 11:35:11,closed,,,1,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8441,b'ValueError when union multi-indexes with tuples as level values',"b'I can create multi-indexes with tuples as level values and union levels:\r\n\r\n```\r\n>> import pandas as pd\r\n>> pd.version.version\r\n\'0.14.1\'\r\n>> i1 = pd.MultiIndex.from_tuples([((1,2), 3), ((4, 5), 6)], names=[\'a\', \'b\'])\r\n>> i2 = pd.MultiIndex.from_tuples([((2,2), 3), ((5, 5), 6)], names=[\'a\', \'b\'])\r\n>> i1.levels[0]\r\nIndex([(1, 2), (4, 5)], dtype=\'object\')\r\n>> i2.levels[0]\r\nIndex([(2, 2), (5, 5)], dtype=\'object\')\r\n>> i1.levels[0].union(i2.levels[0])\r\nIndex([(1, 2), (2, 2), (4, 5), (5, 5)], dtype=\'object\')\r\n```\r\n\r\nBut can\'t union multi-indexes:\r\n```\r\n>> i1.union(i2)\r\n\r\n/home/lyakishev/apps/ui2_2/local/lib/python2.7/site-packages/pandas/core/index.pyc in union(self, other)\r\n   3753         uniq_tuples = lib.fast_unique_multiple([self.values, other.values])\r\n   3754         return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,\r\n-> 3755                                       names=result_names)\r\n   3756 \r\n   3757     def intersection(self, other):\r\n\r\n/home/lyakishev/apps/ui2_2/local/lib/python2.7/site-packages/pandas/core/index.pyc in from_arrays(cls, arrays, sortorder, names)\r\n   2795             return Index(arrays[0], name=name)\r\n   2796 \r\n-> 2797         cats = [Categorical.from_array(arr) for arr in arrays]\r\n   2798         levels = [c.levels for c in cats]\r\n   2799         labels = [c.labels for c in cats]\r\n\r\n/home/lyakishev/apps/ui2_2/local/lib/python2.7/site-packages/pandas/core/categorical.pyc in from_array(cls, data)\r\n    101             the unique values of `data`.\r\n    102         """"""\r\n--> 103         return Categorical(data)\r\n    104 \r\n    105     _levels = None\r\n\r\n/home/lyakishev/apps/ui2_2/local/lib/python2.7/site-packages/pandas/core/categorical.pyc in __init__(self, labels, levels, name)\r\n     82                 name = getattr(labels, \'name\', None)\r\n     83             try:\r\n---> 84                 labels, levels = factorize(labels, sort=True)\r\n     85             except TypeError:\r\n     86                 labels, levels = factorize(labels, sort=False)\r\n\r\n/home/lyakishev/apps/ui2_2/local/lib/python2.7/site-packages/pandas/core/algorithms.pyc in factorize(values, sort, order, na_sentinel)\r\n    128     table = hash_klass(len(vals))\r\n    129     uniques = vec_klass()\r\n--> 130     labels = table.get_labels(vals, uniques, 0, na_sentinel)\r\n    131 \r\n    132     labels = com._ensure_platform_int(labels)\r\n\r\n/home/lyakishev/apps/ui2_2/local/lib/python2.7/site-packages/pandas/hashtable.so in pandas.hashtable.Int64HashTable.get_labels (pandas/hashtable.c:7797)()\r\n\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n```\r\n'"
8437,44615970,ischwabacher,jreback,2014-10-01 20:38:52,2014-10-02 15:19:31,2014-10-02 15:19:31,closed,,0.15.0,9,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8437,b'DataFrame.equals false negatives with dtype=object',"b""I still don't have a good enough grasp on the internals to go diving for this one quickly, but here's the observed behavior:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: temp = [pd.Series([False, np.nan]), pd.Series([False, np.nan]), pd.Series(index=range(2)), pd.Series(index=range(2)), pd.Series(index=range(2)), pd.Series(index=range(2))]\r\n\r\nIn [4]: temp[2][:-1] = temp[3][:-1] = temp[4][0] = temp[5][0] = False\r\n\r\nIn [5]: [[x.equals(y) for y in temp] for x in temp]\r\nOut[5]: \r\n[[True, True, False, False, False, False],\r\n [True, True, False, False, False, False],\r\n [False, False, True, False, False, False],\r\n [False, False, False, True, False, False],\r\n [False, False, False, False, True, True],\r\n [False, False, False, False, True, True]]\r\n```\r\n\r\nHere the 4x4 square in the upper left should be `True`.  (`temp[4:6]` are different because inserting `False` into a `float64` array or vice-versa coerces `False` into `0.0`; this is inconvenient when attempting to use the array for boolean indexing.)"""
8434,44558082,jreback,jreback,2014-10-01 12:34:42,2014-10-01 13:03:57,2014-10-01 13:03:57,closed,,0.15.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8434,b'BUG: Groupby.transform related to BinGrouper and GH8046 (GH8430)',b'closes #8430 \r\nxref #8046 '
8433,44548691,gayodeji,jreback,2014-10-01 10:21:34,2014-11-06 11:26:17,2014-11-06 11:26:17,closed,,0.15.1,3,Bug;Data Reader;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8433,b'BUG/API: data readers should return missing data as NaN rather than warn',"b'I have pandas version:  pandas-0.13.1-py2.7-win32.egg\r\n\r\nRoutine pandas.io.data._dl_mult_symbols has an ""except IOError"" where the following is executed:\r\n                    stocks[sym] = np.nan\r\n\r\nThe next thing that happens is:\r\ntry:\r\n        return Panel(stocks).swapaxes(\'items\', \'minor\')\r\n\r\nHowever, if any of the values in stocks equals nan, then ""return Panel(stocks).swapaxes(\'items\', \'minor\')"" throws an error that is not handled and terminates the script.\r\n\r\nI discovered this by calling pandas.io.data.get_data_yahoo([\'EQQQ.F\'],\'20140930\',\'20140930\',3,0.001,False,False,5)'"
8430,44502841,dalejung,jreback,2014-09-30 21:35:39,2014-10-01 13:04:11,2014-10-01 13:03:57,closed,,0.15.0,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8430,b'BinGrouper breaks on DataFrame.transform',"b""```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom pandas.tseries.resample import TimeGrouper\r\nimport pandas.util.testing as tm\r\n\r\ndf = tm.makeTimeDataFrame()\r\ntg = TimeGrouper('M')\r\n\r\nfunc = lambda x: (x - 1)\r\ndf.groupby(tg).transform(func)\r\n\r\n```\r\n```\r\n/externals/pandas/pandas/core/groupby.py in _get_compressed_labels(self)\r\n   1360     def _get_compressed_labels(self):\r\n-> 1361         all_labels = [ping.labels for ping in self.groupings]\r\n   1362         if self._overflow_possible:\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nSeems related to #8049 as that check group_info which errors on BinGrouper since there are no `groupings`"""
8415,44296621,jreback,jreback,2014-09-29 14:03:46,2014-09-29 21:46:17,2014-09-29 21:43:23,closed,,0.15.0,1,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/8415,b'API/BUG: a UTC object inserted into a Series/DataFrame will preserve the UTC and be object dtype (GH8411)',b'closes #8411 '
8411,44243245,jreback,jreback,2014-09-28 22:04:22,2014-09-29 21:43:23,2014-09-29 21:43:23,closed,,0.15.0,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/8411,"b""BUG: using .dt.tz_localize('UTC') returns a tz-unaware object""","b""```\r\nIn [48]: s\r\nOut[48]: \r\n0    1411161507178\r\n1    1411138436009\r\n2    1411123732180\r\n3    1411167606146\r\n4    1411124780140\r\n5    1411159331327\r\n6    1411131745474\r\n7    1411151831454\r\n8    1411152487758\r\n9    1411137160544\r\nName: 1, dtype: int64\r\n\r\nIn [49]: pd.DatetimeIndex(pd.to_datetime(s,unit='ms')).tz_localize('UTC').tz_convert('US/Eastern')\r\nOut[49]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2014-09-19 17:18:27.178000-04:00, ..., 2014-09-19 10:32:40.544000-04:00]\r\nLength: 10, Freq: None, Timezone: US/Eastern\r\n```\r\n\r\nWhich should be equivalent to\r\n```\r\nIn [50]: pd.to_datetime(s,unit='ms').dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\r\nTypeError: Cannot convert tz-naive timestamps, use tz_localize to localize\r\n```"""
8399,44115939,jreback,jreback,2014-09-26 23:36:27,2014-09-26 23:36:40,2014-09-26 23:36:40,closed,,0.15.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8399,b'BUG: inconsisten panel indexing with aligning frame (GH7763)',b'closes #7763 '
8395,44003117,stahlous,jreback,2014-09-26 05:26:38,2015-10-26 22:40:03,2015-07-12 14:58:11,closed,,Next Major Release,18,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8395,b'BUG: fix Panel.fillna() ignoring axis parameter',"b""This fixes #8251. It may need some more comprehensive tests but before I go too much farther I want to make sure the axis numbering scheme makes sense . The default behavior of `fillna` is to fill along `axis=0`. In the case of a DataFrame this fills along the columns. However, in the case of a Panel, filling along `axis=0` means you're filling along items. I'm not sure if that's the behavior that most users would think is the default. You might note that I also removed the references to what the behavior of each axis value is in the docstring since it gets a bit arbitrary when you consider both DataFrames and Panels."""
8394,43995630,behzadnouri,jreback,2014-09-26 03:27:22,2014-10-01 00:32:59,2014-09-29 15:19:28,closed,,0.15.0,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8394,b'BUG: apply mask to aligned new values',b'closes https://github.com/pydata/pandas/issues/8387'
8387,43906154,smatting,jreback,2014-09-25 11:06:51,2014-09-29 15:19:49,2014-09-29 15:19:49,closed,,0.15.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8387,b'Pandas ignores index when assigning different dtypes.',"b'Consider the following example:\r\n\r\n```python\r\nimport pandas\r\n\r\nfoo = pandas.Series( [ 0, 1, 2, 0 ] )\r\n\r\nw   = foo > 0\r\nbar = foo[ w ].map( str )\r\nprint bar\r\n\r\nfoo[w] = bar\r\nprint foo\r\n\r\nfoo[w] = bar\r\nprint foo\r\n```\r\n\r\ngives the following output\r\n\r\n```\r\n1    1\r\n2    2\r\ndtype: object\r\n\r\n0      0\r\n1    NaN\r\n2      1\r\n3      0\r\ndtype: object\r\n\r\n0    0\r\n1    1\r\n2    2\r\n3    0\r\ndtype: object\r\n```\r\nIt looks as if in the first assignemend `foo[w] = bar` the matching index of both sides is ignored.\r\n\r\nIs this a bug or documented behavior?'"
8384,43830566,mcjcode,jreback,2014-09-25 00:31:10,2014-09-26 14:35:25,2014-09-26 14:35:18,closed,,0.15.0,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8384,b'non-existent columns in DataFrame.dropna subset argument now raises KeyError',"b""closes #8303. \r\n\r\n Now instead of interpreting a bad column name as the 'last column' of the `DataFrame`, `dropna` will raise a `KeyError` instead.  unit test and release note now included."""
8381,43770150,marctollin,jreback,2014-09-24 14:19:15,2015-09-30 11:37:42,2015-09-30 11:26:27,closed,,0.17.0,16,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8381,"b'groupby, as_index=False, with  pandas.Series.count() as an agg'","b""Why doesn't the pandas.Series.count() method work as a valid aggregation with groupby when as_index=False?\r\n\r\n```python\r\ndf=pd.DataFrame([['foo','foo','bar','bar','bar','oats'],[1.0,2.0,3.0,4.0,4.0,5.0],[2.0,3.0,4.0,5.0,1.0,5.0]]).T\r\ndf.columns=['mycat','var1','var2']\r\ndf.var1=df.var1.astype('int64')\r\ndf.var2=df.var2.astype('int64')\r\ndf\r\n```\r\n\r\n![df](https://cloud.githubusercontent.com/assets/7581592/4389284/d795b524-43f3-11e4-8f35-3fec5299a40c.jpg)\r\n\r\n\r\nNow, if I try to do a group by\r\n```python\r\ndf.groupby('mycat', as_index=False).var1.count() \r\n```\r\n\r\n\r\nHere is the error I get:\r\n\r\n```\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-383-27b244bccb8b> in <module>()\r\n----> 1 df.groupby('mycat', as_index=False).var1.count()\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in count(self, axis)\r\n    740 \r\n    741     def count(self, axis=0):\r\n--> 742         return self._count().astype('int64')\r\n    743 \r\n    744     def ohlc(self):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc in astype(self, dtype, copy, raise_on_error)\r\n   2096 \r\n   2097         mgr = self._data.astype(\r\n-> 2098             dtype=dtype, copy=copy, raise_on_error=raise_on_error)\r\n   2099         return self._constructor(mgr).__finalize__(self)\r\n   2100 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc in astype(self, dtype, **kwargs)\r\n   2235 \r\n   2236     def astype(self, dtype, **kwargs):\r\n-> 2237         return self.apply('astype', dtype=dtype, **kwargs)\r\n   2238 \r\n   2239     def convert(self, **kwargs):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc in apply(self, f, axes, filter, do_integrity_check, **kwargs)\r\n   2190                                                  copy=align_copy)\r\n   2191 \r\n-> 2192             applied = getattr(b, f)(**kwargs)\r\n   2193 \r\n   2194             if isinstance(applied, list):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc in astype(self, dtype, copy, raise_on_error, values)\r\n    319     def astype(self, dtype, copy=False, raise_on_error=True, values=None):\r\n    320         return self._astype(dtype, copy=copy, raise_on_error=raise_on_error,\r\n--> 321                             values=values)\r\n    322 \r\n    323     def _astype(self, dtype, copy=False, raise_on_error=True, values=None,\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc in _astype(self, dtype, copy, raise_on_error, values, klass)\r\n    337             if values is None:\r\n    338                 # _astype_nansafe works fine with 1-d only\r\n--> 339                 values = com._astype_nansafe(self.values.ravel(), dtype, copy=True)\r\n    340                 values = values.reshape(self.values.shape)\r\n    341             newb = make_block(values,\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/common.pyc in _astype_nansafe(arr, dtype, copy)\r\n   2410     elif arr.dtype == np.object_ and np.issubdtype(dtype.type, np.integer):\r\n   2411         # work around NumPy brokenness, #1987\r\n-> 2412         return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\r\n   2413     elif issubclass(dtype.type, compat.string_types):\r\n   2414         return lib.astype_str(arr.ravel()).reshape(arr.shape)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/lib.so in pandas.lib.astype_intsafe (pandas/lib.c:13456)()\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/lib.so in util.set_value_at (pandas/lib.c:55994)()\r\n\r\nValueError: invalid literal for long() with base 10: 'bar'\r\n\r\n```\r\n\r\nWhen i set as_index=True, I get\r\n\r\n```python\r\ndf.groupby('mycat', as_index=True).var1.count()\r\n```\r\n\r\n![df2](https://cloud.githubusercontent.com/assets/7581592/4389311/09ceabb8-43f4-11e4-91dd-115631d43f1e.jpg)\r\n\r\n\r\nWhen I change the agg function and set_index=False, I get a weird result tooL\r\n\r\n```python\r\ndf.groupby('mycat', as_index=False).var1.agg(np.count_nonzero)\r\n```\r\n\r\n![df3](https://cloud.githubusercontent.com/assets/7581592/4389480/7f755122-43f5-11e4-9878-3fcecf1ea704.jpg)\r\n\r\nUPDATE: Realized my last result was not counting correctly and am now thoroughly confused.\r\n\r\n"""
8374,43669400,tom-alcorn,jreback,2014-09-23 19:01:12,2014-09-29 14:28:06,2014-09-29 14:27:54,closed,,0.15.0,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8374,b'BUG: Intersection buggy for non-monotonic non-unique indices (GH8362)',b'Fixes intersection bug in the case of non-monotonic non-unique indexes. \r\ncloses #8362 '
8372,43662528,jreback,jreback,2014-09-23 18:02:55,2014-09-26 15:57:37,2014-09-26 15:57:37,closed,,0.15.0,0,Bug;Resample;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8372,"b""BUG: bug in non-evently divisible offsets when resampling (e.g. '7s') (GH8371)""","b'closes #8371\r\n\r\n```\r\nIn [1]:         data = """"""date,time,value\r\n   ...: 11-08-2014,00:00:01.093,1\r\n   ...: 11-08-2014,00:00:02.159,1\r\n   ...: 11-08-2014,00:00:02.667,1\r\n   ...: 11-08-2014,00:00:03.175,1\r\n   ...: 11-08-2014,00:00:07.058,1\r\n   ...: 11-08-2014,00:00:07.362,1\r\n   ...: 11-08-2014,00:00:08.324,1\r\n   ...: 11-08-2014,00:00:08.830,1\r\n   ...: 11-08-2014,00:00:08.982,1\r\n   ...: 11-08-2014,00:00:09.815,1\r\n   ...: 11-08-2014,00:00:10.540,1\r\n   ...: 11-08-2014,00:00:11.061,1\r\n   ...: 11-08-2014,00:00:11.617,1\r\n   ...: 11-08-2014,00:00:13.607,1\r\n   ...: 11-08-2014,00:00:14.535,1\r\n   ...: 11-08-2014,00:00:15.525,1\r\n   ...: 11-08-2014,00:00:17.960,1\r\n   ...: 11-08-2014,00:00:20.674,1\r\n   ...: 11-08-2014,00:00:21.191,1""""""\r\n\r\nIn [2]:         df = pd.read_csv(StringIO(data), parse_dates={\'timestamp\': [\'date\', \'time\']}, index_col=\'timestamp\')\r\n\r\nIn [3]: df\r\nOut[3]: \r\n                            value\r\ntimestamp                        \r\n2014-11-08 00:00:01.093000      1\r\n2014-11-08 00:00:02.159000      1\r\n2014-11-08 00:00:02.667000      1\r\n2014-11-08 00:00:03.175000      1\r\n2014-11-08 00:00:07.058000      1\r\n2014-11-08 00:00:07.362000      1\r\n2014-11-08 00:00:08.324000      1\r\n2014-11-08 00:00:08.830000      1\r\n2014-11-08 00:00:08.982000      1\r\n2014-11-08 00:00:09.815000      1\r\n2014-11-08 00:00:10.540000      1\r\n2014-11-08 00:00:11.061000      1\r\n2014-11-08 00:00:11.617000      1\r\n2014-11-08 00:00:13.607000      1\r\n2014-11-08 00:00:14.535000      1\r\n2014-11-08 00:00:15.525000      1\r\n2014-11-08 00:00:17.960000      1\r\n2014-11-08 00:00:20.674000      1\r\n2014-11-08 00:00:21.191000      1\r\n\r\nIn [4]: df.resample(\'6s\', how=\'sum\')\r\nOut[4]: \r\n                     value\r\ntimestamp                 \r\n2014-11-08 00:00:00      4\r\n2014-11-08 00:00:06      9\r\n2014-11-08 00:00:12      4\r\n2014-11-08 00:00:18      2\r\nIn [5]: df.resample(\'7s\', how=\'sum\')\r\nOut[5]: \r\n                     value\r\ntimestamp                 \r\n2014-11-08 00:00:00      4\r\n2014-11-08 00:00:07     10\r\n2014-11-08 00:00:14      4\r\n2014-11-08 00:00:21      1\r\n\r\nIn [6]: df.resample(\'11s\', how=\'sum\')\r\nOut[6]: \r\n                     value\r\ntimestamp                 \r\n2014-11-08 00:00:00     11\r\n2014-11-08 00:00:11      8\r\n\r\nIn [7]: df.resample(\'13s\', how=\'sum\')\r\nOut[7]: \r\n                     value\r\ntimestamp                 \r\n2014-11-08 00:00:00     13\r\n2014-11-08 00:00:13      6\r\n\r\nIn [8]: df.resample(\'17s\', how=\'sum\')\r\nOut[8]: \r\n                     value\r\ntimestamp                 \r\n2014-11-08 00:00:00     16\r\n2014-11-08 00:00:17      3\r\n```'"
8371,43650746,mapsa,jreback,2014-09-23 16:22:27,2014-09-26 15:57:37,2014-09-26 15:57:37,closed,,0.15.0,1,Bug;Resample;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8371,"b""Resample method doesn't round timestamps with prime numbers frequency""","b'Hi,\r\n\r\nI\'ve found this issue when I tried to resample at different frequencies. When the frequency is a prime number, the first timestamp is not rounded and it takes the first timestamp in the file. Do I need an extra parameter to round timestamps?\r\n\r\n```python\r\ncsv = r""""""Date,Time,Ask,Bid,AskVolume,BidVolume\r\n11-08-2014,00:00:01.093,1.34046,1.34042,1.25,1.69\r\n11-08-2014,00:00:02.159,1.34047,1.34043,4.69,1\r\n11-08-2014,00:00:02.667,1.34046,1.34042,1.32,2.44\r\n11-08-2014,00:00:03.175,1.34046,1.34043,1.32,1\r\n11-08-2014,00:00:07.058,1.34046,1.34043,3.75,1.69\r\n11-08-2014,00:00:07.362,1.34043,1.34041,2.25,1\r\n11-08-2014,00:00:08.324,1.34043,1.34042,1.5,1\r\n11-08-2014,00:00:08.830,1.34045,1.34043,2.44,1\r\n11-08-2014,00:00:08.982,1.34043,1.34041,3,1.69\r\n11-08-2014,00:00:09.815,1.34042,1.34041,1,1\r\n11-08-2014,00:00:10.540,1.34043,1.34041,2.63,1\r\n11-08-2014,00:00:11.061,1.34043,1.3404,7.99,2.25\r\n11-08-2014,00:00:11.617,1.34042,1.34041,1.13,1\r\n11-08-2014,00:00:13.607,1.34043,1.3404,7.5,3.19\r\n11-08-2014,00:00:14.535,1.34043,1.34041,5.25,1\r\n11-08-2014,00:00:15.525,1.34043,1.34041,3.38,1.25\r\n11-08-2014,00:00:17.960,1.34043,1.34041,4.5,1.25\r\n11-08-2014,00:00:20.674,1.34042,1.3404,1.13,2.07\r\n11-08-2014,00:00:21.191,1.34041,1.3404,1,1""""""\r\nfilename = \'sample.csv\'\r\nf = open(filename, \'w\')\r\nf.write(csv)\r\nf.close()\r\ndf = pd.read_csv(filename, parse_dates={\'Timestamp\': [\'Date\', \'Time\']}, index_col=\'Timestamp\')\r\nprint df.resample(\'6s\', fill_method=\'bfill\')\r\nprint df.resample(\'7s\', fill_method=\'bfill\')\r\nprint df.resample(\'11s\', fill_method=\'bfill\')\r\nprint df.resample(\'13s\', fill_method=\'bfill\')\r\nprint df.resample(\'17s\', fill_method=\'bfill\')\r\n```\r\nThanks,\r\nPaola'"
8370,43634526,jreback,jreback,2014-09-23 14:17:10,2014-09-23 14:58:45,2014-09-23 14:58:45,closed,,0.15.0,0,Bug;Compat;MultiIndex;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8370,b'BUG/COMPAT: set tz on DatetimeIndex on pickle reconstruction (GH8367)',b'closes #8367 '
8367,43598418,edrevo,jreback,2014-09-23 07:23:20,2014-09-23 15:12:58,2014-09-23 14:58:45,closed,,0.15.0,8,Bug;Compat;MultiIndex;Timezones,https://api.github.com/repos/pydata/pandas/issues/8367,b'Timezone info is lost when pickling datetime indices',"b'```python\r\n>>> c.head(3)\r\n                                                           end_time\r\nlocation_id sensor_id view_id start_time\r\n54          1000305   0       2014-07-21 07:00:00+00:00    2014-07-21 07:15:00+00:00\r\n                              2014-07-21 07:15:00+00:00    2014-07-21 07:30:00+00:00\r\n                              2014-07-21 07:30:00+00:00    2014-07-21 07:45:00+00:00\r\n\r\n>>> c.to_pickle(r""C:\\temp\\fff"")\r\n>>> pd.read_pickle(r""C:\\temp\\fff"").head(3)\r\n                                                   end_time\r\nlocation_id sensor_id view_id start_time\r\n54          1000305   0       2014-07-21 07:00:00  2014-07-21 07:15:00+00:00\r\n                              2014-07-21 07:15:00  2014-07-21 07:30:00+00:00\r\n                              2014-07-21 07:30:00  2014-07-21 07:45:00+00:00\r\n```\r\n\r\nPlease note that the timezone is lost from `start_time`.'"
8366,43580711,mcjcode,mcjcode,2014-09-23 01:44:39,2014-09-25 00:38:15,2014-09-25 00:05:35,closed,,0.15.0,3,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8366,b'DataFrame.dropna bug fix',b'Bug fix for #8303.  Now raise KeyError if any non-existent columns are passed in the subset argument.'
8365,43568197,sandbox,jreback,2014-09-22 23:15:24,2015-03-03 01:15:35,2015-03-03 01:15:35,closed,,0.16.0,2,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/8365,"b""Series/DataFrame.rank() doesn't handle certain floats properly""","b'There appears to be an issue with floats that are close together in series.rank(), pandas version 0.14. For reference this test worked in pandas 0.12.0.\r\n\r\n### Current Behavior\r\n```\r\n>>> series = pd.Series([1000.000669 , 1000.000041 , 1000.000059 , 1000.000063 , 1000.000121 , 1000.000104 , 1000.000040 , 1000.000062 , 1000.000095 , 1000.000091 , 1000.000050 , 1000.000074 , 1000.000063 , 1000.000076 , 1000.000083 , 1000.000061 , 1000.000030 , 1000.000069 , 1000.000090 , 1000.000116 , 1000.000058 , 1000.000074 , 1000.000035 , 1000.000084 , 1000.000067 , 1000.000072 , 1000.000105 , 1000.000091 , 1000.000077 , 1000.000040 , 1000.000108 , 1000.000117 , 1000.000114 , 1000.000117 , 1000.000099 , 1000.000039 , 1000.000046 , 1000.000105 , 1000.000057])\r\n>>> series.rank()\r\n0     39.0\r\n1     19.5\r\n2     19.5\r\n3     19.5\r\n4     19.5\r\n5     19.5\r\n6     19.5\r\n7     19.5\r\n8     19.5\r\n9     19.5\r\n10    19.5\r\n11    19.5\r\n12    19.5\r\n13    19.5\r\n14    19.5\r\n15    19.5\r\n16    19.5\r\n17    19.5\r\n18    19.5\r\n19    19.5\r\n20    19.5\r\n21    19.5\r\n22    19.5\r\n23    19.5\r\n24    19.5\r\n25    19.5\r\n26    19.5\r\n27    19.5\r\n28    19.5\r\n29    19.5\r\n30    19.5\r\n31    19.5\r\n32    19.5\r\n33    19.5\r\n34    19.5\r\n35    19.5\r\n36    19.5\r\n37    19.5\r\n38    19.5\r\ndtype: float64\r\n\r\n```\r\n\r\n### Expected Behavior\r\n```\r\n>>> from scipy import stats\r\n>>> stats.rankdata(series)\r\narray([ 39. ,   6. ,  11. ,  14.5,  38. ,  30. ,   4.5,  13. ,  28. ,\r\n        26.5,   8. ,  19.5,  14.5,  21. ,  23. ,  12. ,   1. ,  17. ,\r\n        25. ,  35. ,  10. ,  19.5,   2. ,  24. ,  16. ,  18. ,  31.5,\r\n        26.5,  22. ,   4.5,  33. ,  36.5,  34. ,  36.5,  29. ,   3. ,\r\n         7. ,  31.5,   9. ])\r\n```\r\n\r\n\r\n### System Information\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.4\r\nCython: 0.21\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: None\r\nsphinx: None\r\npatsy: 0.3.0\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.7\r\nbottleneck: 0.8.0\r\ntables: 3.0.0\r\nnumexpr: 2.4\r\nmatplotlib: None\r\nopenpyxl: 2.1.0\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.8.0\r\npymysql: None\r\npsycopg2: 2.5.4 (dt dec pq3 ext)\r\n\r\n```\r\n'"
8364,43561087,jreback,jreback,2014-09-22 22:07:36,2014-09-23 01:05:32,2014-09-23 01:05:32,closed,,0.15.0,0,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8364,b'BUG: Bug in alignment with TimeOps and non-unique indexes (GH8363)',b'closes #8363 '
8363,43559604,jreback,jreback,2014-09-22 21:56:11,2014-09-23 01:05:32,2014-09-23 01:05:32,closed,,0.15.0,1,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8363,b'BUG: datetime ops with non-unique indexes is buggy',"b""from [SO](http://stackoverflow.com/questions/25963622/column-operations-with-pandas-dataframe-objects-with-non-unique-indices/25964177?noredirect=1#comment40684868_25964177)\r\n\r\n```\r\nIn [3]:       df = DataFrame({'A' : np.arange(5), 'B' : np.arange(1,6)},index=[2,2,3,3,4])\r\n\r\nIn [4]:   df.B-df.A\r\nOut[4]: \r\n2    1\r\n2    1\r\n3    1\r\n3    1\r\n4    1\r\ndtype: int64\r\n\r\nIn [5]:        df = DataFrame({'A' : date_range('20130101',periods=5), 'B' : date_range('20130101 09:00:00', periods=5)},index=[2,2,3,3,4])\r\n\r\nIn [6]:   df.B-df.A\r\nOut[6]: \r\n2     0 days 09:00:00\r\n2   -1 days +09:00:00\r\n2     1 days 09:00:00\r\n2     0 days 09:00:00\r\n3     0 days 09:00:00\r\n3   -1 days +09:00:00\r\n3     1 days 09:00:00\r\n3     0 days 09:00:00\r\n4     0 days 09:00:00\r\ndtype: timedelta64[ns]\r\n```"""
8362,43557695,tom-alcorn,jreback,2014-09-22 21:40:52,2014-09-29 14:28:06,2014-09-29 14:28:06,closed,,0.15.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8362,b'Index.intersection bug?',"b""I ran into an odd behaviour when taking the intersection of two Indexes. Specifically,\r\n```python\r\nleft = pd.Index(['A','B','A','C'])\r\nright = pd.Index(['B','D'])\r\nleft.intersection(right)\r\n```\r\nreturns\r\n```python\r\nIndex(['B', 'C'], dtype='object')\r\n```\r\nHowever, I would expect this to return\r\n```python\r\nIndex(['B'], dtype='object')\r\n```\r\nIf ```Index(['B', 'C'], dtype='object') ``` is the intended behaviour, can someone explain the rationale behind it?"""
8358,43510528,seth-p,jreback,2014-09-22 16:16:24,2014-09-22 16:39:15,2014-09-22 16:19:20,closed,,,5,Bug;Duplicate;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/8358,b'API/BUG: Indexing a DatetimeIndex by Period',"b'Curiously one can index a DatetimeIndex by the string representation of a Period, but not by an actual Period object. Is this intentional?\r\n```\r\nPython 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:45:13) [MSC v.1600 64 bit (AMD64)]\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.2.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: from pandas import Series, date_range, Period\r\n\r\nIn [2]: s = Series(range(365), index=date_range(\'2001-01-01\', periods=365))\r\n\r\nIn [3]: s[str(Period(\'2001-03\', \'M\'))]\r\nOut[3]:\r\n2001-03-01    59\r\n2001-03-02    60\r\n2001-03-03    61\r\n2001-03-04    62\r\n2001-03-05    63\r\n2001-03-06    64\r\n2001-03-07    65\r\n2001-03-08    66\r\n2001-03-09    67\r\n2001-03-10    68\r\n2001-03-11    69\r\n2001-03-12    70\r\n2001-03-13    71\r\n2001-03-14    72\r\n2001-03-15    73\r\n2001-03-16    74\r\n2001-03-17    75\r\n2001-03-18    76\r\n2001-03-19    77\r\n2001-03-20    78\r\n2001-03-21    79\r\n2001-03-22    80\r\n2001-03-23    81\r\n2001-03-24    82\r\n2001-03-25    83\r\n2001-03-26    84\r\n2001-03-27    85\r\n2001-03-28    86\r\n2001-03-29    87\r\n2001-03-30    88\r\n2001-03-31    89\r\nFreq: D, dtype: int32\r\n\r\nIn [4]: s[Period(\'2001-03\', \'M\')]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-64d0668f17a9> in <module>()\r\n----> 1 s[Period(\'2001-03\', \'M\')]\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\series.py in __getitem__(self, key)\r\n    482     def __getitem__(self, key):\r\n    483         try:\r\n--> 484             result = self.index.get_value(self, key)\r\n    485\r\n    486             if not np.isscalar(result):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tseries\\index.py in get_value(self, series, key)\r\n   1256                 return self.get_value_maybe_box(series, key)\r\n   1257             except (TypeError, ValueError, KeyError):\r\n-> 1258                 raise KeyError(key)\r\n   1259\r\n   1260     def get_value_maybe_box(self, series, key):\r\n\r\nKeyError: Period(\'2001-03\', \'M\')\r\n```'"
8354,43500902,RenzoBertocchi,jreback,2014-09-22 15:19:37,2014-09-30 16:05:42,2014-09-30 16:05:15,closed,,0.15.0,9,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8354,b'Bug on pivot_table with margins and dict aggfunc',"b'closes #8349\r\n\r\nbug on pandas 0.14.1 pivot_table using dictionary aggfunc and margins.\r\n\r\nTest code:\r\n\r\n<pre>\r\n<code>\r\ndf=pandas.DataFrame([ \r\n{\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2013,\'MONTH\':12,\'DAYS\': 3,\'SALARY\': 17}, \r\n{\'JOB\':\'Employ\',\'NAME\':\'Mary\',\'YEAR\':2013,\'MONTH\':12,\'DAYS\': 5,\'SALARY\': 23}, \r\n{\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2014,\'MONTH\': 1,\'DAYS\':10,\'SALARY\':100}, \r\n{\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2014,\'MONTH\': 1,\'DAYS\':11,\'SALARY\':110}, \r\n{\'JOB\':\'Employ\',\'NAME\':\'Mary\',\'YEAR\':2014,\'MONTH\': 1,\'DAYS\':15,\'SALARY\':200}, \r\n{\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2014,\'MONTH\': 2,\'DAYS\': 8,\'SALARY\': 80}, \r\n{\'JOB\':\'Employ\',\'NAME\':\'Mary\',\'YEAR\':2014,\'MONTH\': 2,\'DAYS\': 5,\'SALARY\':190} ])\r\n\r\ndf=df.set_index([\'JOB\',\'NAME\',\'YEAR\',\'MONTH\'],drop=False,append=False)\r\n\r\ndf=df.pivot_table(index=[\'JOB\',\'NAME\'],\r\ncolumns=[\'YEAR\',\'MONTH\'],\r\nvalues=[\'DAYS\',\'SALARY\'],\r\naggfunc={\'DAYS\':\'mean\',\'SALARY\':\'sum\'})\r\n</code>\r\n</pre>\r\n\r\nAll works fine but raise error using margins:\r\n\r\n<pre>\r\n<code>\r\ndf=df.pivot_table(index=[\'JOB\',\'NAME\'],\r\ncolumns=[\'YEAR\',\'MONTH\'],\r\nvalues=[\'DAYS\',\'SALARY\'],\r\naggfunc={\'DAYS\':\'mean\',\'SALARY\':\'sum\'},\r\nmargins=True)\r\n</code>\r\n</pre>\r\n\r\ndf=df.pivot_table(index=[\'JOB\',\'NAME\'],columns=[\'YEAR\',\'MONTH\'],values=[\'DAYS\',\'SALARY\'],aggfunc={\'DAYS\':\'mean\',\'SALARY\':\'sum\'},margins=True)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/util/decorators.py"", line 60, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/util/decorators.py"", line 60, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/tools/pivot.py"", line 147, in pivot_table\r\ncols=columns, aggfunc=aggfunc)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/tools/pivot.py"", line 191, in _add_margins\r\nrow_margin[k] = grand_margin[k[0]]\r\n\r\nKeyError: \'SALARY\''"
8349,43421560,RenzoBertocchi,jreback,2014-09-22 07:01:38,2014-09-30 16:05:42,2014-09-30 16:05:42,closed,,0.15.0,8,Bug;Difficulty Novice;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8349,b'Bug on pivot_table with margins and dict aggfunc',"b'I think there is a bug on pandas 0.14.1 pivot_table using dictionary aggfunc and margins.\r\nTest code:\r\n<code>df=pandas.DataFrame([\r\n           {\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2013,\'MONTH\':12,\'DAYS\': 3,\'SALARY\': 17},\r\n           {\'JOB\':\'Employ\',\'NAME\':\'Mary\',\'YEAR\':2013,\'MONTH\':12,\'DAYS\': 5,\'SALARY\': 23},\r\n           {\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2014,\'MONTH\': 1,\'DAYS\':10,\'SALARY\':100},\r\n           {\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2014,\'MONTH\': 1,\'DAYS\':11,\'SALARY\':110},\r\n           {\'JOB\':\'Employ\',\'NAME\':\'Mary\',\'YEAR\':2014,\'MONTH\': 1,\'DAYS\':15,\'SALARY\':200},\r\n           {\'JOB\':\'Worker\',\'NAME\':\'Bob\' ,\'YEAR\':2014,\'MONTH\': 2,\'DAYS\': 8,\'SALARY\': 80},\r\n           {\'JOB\':\'Employ\',\'NAME\':\'Mary\',\'YEAR\':2014,\'MONTH\': 2,\'DAYS\': 5,\'SALARY\':190}\r\n            ])</code>\r\n\r\n<code>df=df.set_index([\'JOB\',\'NAME\',\'YEAR\',\'MONTH\'],drop=False,append=False)</code>\r\n\r\n<code>df=df.pivot_table(index=[\'JOB\',\'NAME\'],columns=[\'YEAR\',\'MONTH\'],values=[\'DAYS\',\'SALARY\'],aggfunc={\'DAYS\':\'mean\',\'SALARY\':\'sum\'})</code>\r\n\r\nAll works fine but raise error using margins:\r\n\r\n<code>df=df.pivot_table(index=[\'JOB\',\'NAME\'],columns=[\'YEAR\',\'MONTH\'],values=[\'DAYS\',\'SALARY\'],aggfunc={\'DAYS\':\'mean\',\'SALARY\':\'sum\'},margins=True)\r\n\r\ndf=df.pivot_table(index=[\'JOB\',\'NAME\'],columns=[\'YEAR\',\'MONTH\'],values=[\'DAYS\',\'SALARY\'],aggfunc={\'DAYS\':\'mean\',\'SALARY\':\'sum\'},margins=True)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/util/decorators.py"", line 60, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/util/decorators.py"", line 60, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/tools/pivot.py"", line 147, in pivot_table\r\ncols=columns, aggfunc=aggfunc)\r\nFile ""/usr/local/lib/python2.7/site-packages/pandas-0.14.1-py2.7-linux-x86_64.egg/pandas/tools/pivot.py"", line 191, in _add_margins\r\nrow_margin[k] = grand_margin[k[0]]\r\n\r\nKeyError: \'SALARY\'\r\n\r\n</code>'"
8348,43352856,neirbowj,jreback,2014-09-22 01:45:34,2014-10-15 17:55:49,2014-10-15 17:55:49,closed,,0.15.0,5,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/8348,b'BUG: _Openpyxl2Writer mishandles merged cell style',b'Thanks to @jtratner for catching this.'
8345,43347649,jreback,jreback,2014-09-21 22:28:34,2014-09-22 19:34:15,2014-09-22 19:29:24,closed,,0.15.0,0,Bug;Enhancement;Resample;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8345,b'ENH/BUG: allow timedelta resamples',"b""- [x] docs\r\n\r\n```\r\nIn [4]: pd.set_option('max_rows',10)\r\n\r\nIn [5]: df = DataFrame({'A' : np.arange(1480)},index=pd.to_timedelta(np.arange(1480),unit='T'))\r\n\r\nIn [6]: df\r\nOut[6]: \r\n                    A\r\n0 days 00:00:00     0\r\n0 days 00:01:00     1\r\n0 days 00:02:00     2\r\n0 days 00:03:00     3\r\n0 days 00:04:00     4\r\n...               ...\r\n1 days 00:35:00  1475\r\n1 days 00:36:00  1476\r\n1 days 00:37:00  1477\r\n1 days 00:38:00  1478\r\n1 days 00:39:00  1479\r\n\r\n[1480 rows x 1 columns]\r\n\r\nIn [7]: df.resample('30T',how='sum')\r\nOut[7]: \r\n                     A\r\n0 days 00:00:00    435\r\n0 days 00:30:00   1335\r\n0 days 01:00:00   2235\r\n0 days 01:30:00   3135\r\n0 days 02:00:00   4035\r\n...                ...\r\n0 days 22:30:00  40935\r\n0 days 23:00:00  41835\r\n0 days 23:30:00  42735\r\n1 days 00:00:00  43635\r\n1 days 00:30:00  14745\r\n\r\n[50 rows x 1 columns]\r\n```"""
8338,43336764,weatherfrog,jreback,2014-09-21 14:47:59,2014-09-21 16:10:43,2014-09-21 16:10:43,closed,,0.15.0,3,Bug;Duplicate;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8338,b'interpolate() crashes with limit argument',"b""Hi,\r\n\r\nPandas' `interpolate()` method crashes if the `limit` parameter is passed and the underlying data frame or series does not contain NaN values, i.e., if there is nothing to interpolate. I think this is a bug.\r\nThe following code was tested with Pandas 0.14.1 (NumPy 1.9.0):\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nx = pd.Series(np.arange(4))\r\nx[2] = np.nan\r\n# x looks like this:\r\n# 0    0\r\n# 1    1\r\n# 2    NaN\r\n# 3    3\r\n                  \r\nxx = x.interpolate(method='linear', limit=1)\r\n\r\n# calling interpolate() a second time...\r\nxx.interpolate(method='linear', limit=1)\r\n\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n[...]\r\n/.../python2.7/site-packages/pandas/core/common.pyc in _interp_limit(invalid, limit)\r\n   1437         violate = [invalid[x:x + limit + 1] for x in all_nans]\r\n   1438         violate = np.array([x.all() & (x.size > limit) for x in violate])\r\n-> 1439         return all_nans[violate] + limit\r\n   1440 \r\n   1441     xvalues = getattr(xvalues, 'values', xvalues)\r\n\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```\r\nIt seems that \xa8C since `all_nans` is empty \xa8C the dtype of `violate` is set to float (NumPy default). And float arrays cannot be used for indexing. It's probably safe to replace line `1438` with the following:\r\n```\r\nviolate = np.array([x.all() & (x.size > limit) for x in violate], dtype=np.int)\r\n```"""
8328,43289747,klonuo,jreback,2014-09-19 22:39:36,2014-09-23 14:40:59,2014-09-23 14:40:47,closed,,0.15.0,3,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8328,b'BUG: to_clipboard output formatting',"b'closes  https://github.com/pydata/pandas/issues/8305\r\n\r\n@jreback I made the simple patch as you suggested for fixing truncated columns, although problem seems to be https://github.com/pydata/pandas/blob/master/pandas/core/format.py#L2118 called from https://github.com/pydata/pandas/blob/master/pandas/core/format.py#L411\r\nPerhaps `conf_max` should respect if caller passes `minimum` argument so it wont truncate?\r\n\r\nSecond issue, where `index=False` was ignored (or just any other argument when `excel=False`) was because arguments were not passed to underlying function `obj.to_string()` and that was easy fix.\r\n\r\nI added test for checking if column width greater then `max_colwidth` passes clipboard test.\r\n'"
8322,43260238,jreback,jreback,2014-09-19 17:04:28,2014-09-19 20:05:55,2014-09-19 20:05:55,closed,,0.15.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8322,b'BUG: Bug in casting when setting a column in a same-dtype block (GH7704)',b'closes #7704 '
8320,43248088,jreback,jreback,2014-09-19 15:04:14,2014-09-19 15:40:59,2014-09-19 15:40:59,closed,,0.15.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8320,b'BUG: Bug in .at that would accept integer indexers on a non-integer index and do fallback (GH7814)',b'closes #7814 '
8305,43070858,klonuo,jreback,2014-09-17 22:41:20,2014-09-23 14:40:59,2014-09-23 14:40:59,closed,,0.15.0,9,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8305,b'to_clipboard(excel=False) truncates strings to 50 characters',"b""I hope this is not intended as I can't find the point in this behavior:\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame.from_dict({1: ''.join(['1234567890'] * 6)}, 'index')\r\ndf.to_clipboard(excel=False)\r\n```\r\nresult copied to clipboard:\r\n```\r\n                                                   0\r\n1  1234567890123456789012345678901234567890123456...\r\n```\r\n\r\nAnother issue - trying to omit index:\r\n\r\n```python\r\ndf.to_clipboard(excel=False, index=False)\r\n```\r\nresult copied to clipboard, just the same as previous (with index):\r\n```\r\n                                                   0\r\n1  1234567890123456789012345678901234567890123456...\r\n```\r\n\r\nThis doesn't happen if I leave `excel` parameter to default value."""
8303,43066099,arijun,jreback,2014-09-17 21:47:42,2014-09-26 14:35:18,2014-09-26 14:35:18,closed,,0.15.0,13,Bug;Difficulty Novice;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8303,b'subset keyword argument will include last column if incorrect keys are given',"b""If you give a non-existent key to the subset argument it will default to the last column. For example:\r\n```\r\nIn [3]: d = pd.DataFrame({'a':[1],'b':[2],'c':[np.nan]})\r\nIn [4]: len(d.dropna(subset=['x']))\r\nOut[4]: 0\r\n```\r\nIt seems the problem is in the line where dropna takes the subset:\r\n```\r\nagg_obj = self.take(ax.get_indexer_for(subset),axis=agg_axis)\r\n```\r\nHere, `ax.get_indexer_for(subset)` will return `-1` as a sentinel value for any keys that were not found in `subset`, and `self.take` interprets the `-1` as a request for the last column.\r\n\r\n"""
8298,43051444,jreback,jreback,2014-09-17 19:14:57,2014-09-17 20:46:01,2014-09-17 20:46:01,closed,,0.15.0,0,Bug;Compat;Indexing;Internals;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8298,b'BUG: Bug in iat return boxing for Timestamp/Timedelta (GH7729)',b'CLN/COMPAT: cleanup timedelta64[ns] series inferrence\r\n\r\ncloses #7729 '
8291,43003349,immerrr,jreback,2014-09-17 12:05:30,2014-09-17 14:48:32,2014-09-17 14:48:25,closed,,0.15.0,2,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/8291,b'BUG: fix setting dataframe column to a sparse array',b'This should fix #8131.'
8287,42957043,TomAugspurger,jreback,2014-09-17 02:39:48,2016-05-24 15:28:17,2016-05-24 15:28:17,closed,,Next Major Release,4,Bug;Docs;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/8287,b'BUG: HDFStore.select ignores start and stop parameters',"b""Or I'm doing this wrong\r\n\r\n```\r\nIn [1]: df = pd.DataFrame(np.random.randn(10, 4))\r\n\r\nIn [3]: df.to_hdf('test.h5', 'df')\r\n\r\nIn [4]: with pd.get_store('test.h5') as store:\r\n   ...:     df_ = store.select('df', start=0, stop=2)\r\n   ...:     \r\n\r\nIn [5]: df_\r\nOut[5]: \r\n          0         1         2         3\r\n0  1.504814  0.209394 -0.360703 -0.339684\r\n1  0.272718  0.768927 -1.061029  0.919742\r\n2  0.315669  0.337206 -1.358443  1.223715\r\n3 -0.414095  0.015512 -0.320634  0.703489\r\n4  0.889063 -0.194524  1.342604  1.078340\r\n5 -1.636940 -0.725176 -0.096803  0.346356\r\n6 -1.359400 -0.588007  0.651720 -0.127604\r\n7 -1.613468  1.131560 -0.074385 -0.830540\r\n8  0.176463 -1.298918 -0.215537  0.984911\r\n9 -0.372920  0.898528 -0.869616  1.178704\r\n```\r\n\r\nDocstring says:\r\n\r\n```\r\nstart : integer (defaults to None), row number to start selection\r\nstop  : integer (defaults to None), row number to stop selection\r\n```\r\n\r\nI can look tomorrow (catching up on pandas stuff all day tomorrow hopefully)"""
8282,42894180,jreback,jreback,2014-09-16 15:18:35,2014-10-29 14:47:16,2014-09-17 12:20:13,closed,,0.15.0,1,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8282,b'BUG: make sure that the multi-index is lex-sorted before passing to _lexsort_indexer (GH8017)',b'BUG: sparse repr of multi-index frame with a FloatIndex as a level was incorrect\r\n\r\ncloses #8017 '
8275,42803139,seth-p,jreback,2014-09-15 18:25:33,2014-09-17 13:24:32,2014-09-17 13:07:02,closed,,0.15.0,11,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/8275,"b'BUG: rolling_apply(..., center=True) should not append NaNs'",b'Closes #8269.'
8269,42724719,stahlous,jreback,2014-09-14 16:43:48,2014-09-17 13:07:02,2014-09-17 13:07:02,closed,,0.15.0,14,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/8269,b'Unexpected behavior for rolling moments when center=True and min_periods < window',"b'I noticed while using rolling moment functions with `center=True` and with `min_periods` < `window` that the values at the end of the series were showing up as `NaN` when they should have be finite floating point values. The reason for this behavior is that `_rolling_moment()` concats extra `NaN` values to the end of the input series when `center=True` to allow the `roll_generic()` function to continue on past the end of the original series and compute the values that will ""come within view"" when the result is centered. However, adding those `NaN` values into the window can bork the calculation and change a result that should be finite into a `NaN`.\r\n\r\nThis example illustrates the behavior:\r\n\r\n    n = 12\r\n    s = Series(np.random.randn(n))\r\n    s.plot(color=\'b\')\r\n    win=7\r\n    minp = 5\r\n    pd.rolling_mean(s, win, min_periods=minp, center=False).plot(color=\'g\')\r\n    pd.rolling_mean(s, win, min_periods=minp, center=True).plot(color=\'r\')\r\n    ticks = plt.xticks(np.arange(0, n, 1.0))\r\n\r\n![image](https://cloud.githubusercontent.com/assets/7545449/4264568/d93c7a9e-3c2c-11e4-843e-ac7914935ff7.png)\r\n\r\nI was able to mitigate this behavior, for one calculation at least, by modifying the cython function to essentially strip the trailing `NaN` values for the calculations made towards the end of the series. This requires passing in the `offset` calculated in `_rolling_moment()`. \r\n\r\nFixing this is going to touch a lot of functions that depend on `_rolling_moment()`. Before I go much further with this, I thought I\'d raise the issue here and see if anyone else has insight or better ideas about how to fix this.'"
8265,42710756,rockg,FrancescAlted,2014-09-14 02:27:16,2015-04-18 19:00:14,2015-04-18 19:00:14,closed,,,14,Bug;IO HDF5;Timezones,https://api.github.com/repos/pydata/pandas/issues/8265,b'HDF5 index corruption',"b""I generated a multindexed DataFrame and wrote it to hdf5 using to_hdf.  It uses zlib level 5 compression.  The file was written all at once.  The file is located here: https://www.dropbox.com/s/122q55g5ubcf4fl/indexIssue.h5?dl=0\r\n\r\nThe below methods should be identical but the former select with a where clause has 2892 records but getting all values and subselecting on the path returns 2972 (values are missing for path 6 between 3-5-2015 20:00 to 3-6-2015 9:00).  I tried using reindex on the able but that didn't fix anything.  I don't really know what's going on.\r\n\r\n```\r\nstore   =   HDFStore(path_to_file, mode='r')\r\n\r\np1      =   store.select('ts', where=Term('Path', '=', 6), auto_close=False)\r\nprint(len(p1))\r\np2      =   store.select('ts', auto_close=False)\r\np2s     =   p2[p2.index.get_level_values('Path') == 6]\r\nprint(len(p2s))\r\n```"""
8264,42708207,jreback,jreback,2014-09-13 23:18:33,2014-09-14 01:09:42,2014-09-14 01:09:42,closed,,0.15.0,0,Bug;MultiIndex;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8264,b'BUG: Bug in inference in a MultiIndex with datetime.date inputs (GH7888)',b'closes #7888 '
8258,42666469,johne13,jreback,2014-09-12 20:17:25,2014-10-04 17:19:50,2014-10-04 17:19:50,closed,,0.15.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/8258,b'BUG: alignment bug when setting a subset of rows and attempting to switch columns',"b""Originally posted at stackexchange:\r\nhttp://stackoverflow.com/questions/25811529/setting-values-on-a-subset-of-rows-indexing-boolean-setting/25812825#25812825\r\nJeff suggested posting here also.\r\n\r\n```python\r\ndf = DataFrame({'L':   ['left', 'right', 'left', 'right'],\r\n                'R':   ['right', 'left', 'right', 'left'],\r\n                'idx': [False, True, False, True],\r\n                'num': np.arange(4) })\r\n\r\ndf1 = df.copy()\r\ndf2 = df.copy()\r\ndf3 = df.copy()\r\ndf4 = df.copy()\r\n\r\n#1 nothing happens (expected result)\r\ndf1.loc[df1.idx,['L','R']] = df1.loc[df1.idx,['R','L']]\r\n\r\n#2 something weird happens\r\ndf2.loc[df2.idx,['L','R']] = df2[['R','L']]\r\n```\r\nResults below:\r\n```python\r\nIn [572]: df\r\nOut[572]: \r\n       L      R    idx  num\r\n0   left  right  False    0\r\n1  right   left   True    1\r\n2   left  right  False    2\r\n3  right   left   True    3\r\n\r\nIn [573]: df1\r\nOut[573]: \r\n       L      R    idx  num\r\n0   left  right  False    0\r\n1  right   left   True    1\r\n2   left  right  False    2\r\n3  right   left   True    3\r\n\r\nIn [574]: df2\r\nOut[574]: \r\n       L      R    idx  num\r\n0   left  right  False    0\r\n1   left  right   True    1\r\n2   left  right  False    2\r\n3  right   left   True    3\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.1.dev\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2014.3\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```"""
8256,42659984,CerebralMastication,jreback,2014-09-12 19:00:45,2014-09-15 14:52:26,2014-09-15 14:52:07,closed,,0.15.0,7,Bug;Duplicate;Visualization,https://api.github.com/repos/pydata/pandas/issues/8256,b'hist(by=) breaks when the output does not fill all rows and columns',"b'Example:\r\n\r\nimport pandas as pd\r\npd.Series(randn(1000)).hist(by=randint(0, 6, 1000), sharex=True, sharey=True)\r\n\r\n\r\npd.Series(randn(1000)).hist(by=randint(0, 5, 1000), sharex=True, sharey=True)\r\n\r\n![1](https://cloud.githubusercontent.com/assets/126879/4254821/07c113ea-3aaf-11e4-8905-490d6b5e9864.png)\r\n![2](https://cloud.githubusercontent.com/assets/126879/4254820/07bfeb5a-3aaf-11e4-803b-ebde9a0a948a.png)\r\n'"
8245,42540960,ifmihai,TomAugspurger,2014-09-11 16:02:17,2014-09-11 23:38:47,2014-09-11 23:38:47,closed,,0.15.0,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/8245,b'DataFrame.index.asof() bug',"b""An example is best:\r\n\r\n```python\r\n2 Inn: df = P.DataFrame(index=P.date_range('2014-9-6 1:30', '2014-9-9', freq='6H'))\r\n3 Inn: df\r\n3 Out: \r\n\r\n\r\n2014-09-06 01:30:00\r\n2014-09-06 07:30:00\r\n2014-09-06 13:30:00\r\n2014-09-06 19:30:00\r\n2014-09-07 01:30:00\r\n2014-09-07 07:30:00\r\n2014-09-07 13:30:00\r\n2014-09-07 19:30:00\r\n2014-09-08 01:30:00\r\n2014-09-08 07:30:00\r\n2014-09-08 13:30:00\r\n2014-09-08 19:30:00\r\n2014-09-09 01:30:00\r\n \r\n4 Inn: df.index.asof('2014-9-8')  # wrong\r\n4 Out: Timestamp('2014-09-08 00:00:00')\r\n\r\n5 Inn: df.index.asof(P.to_datetime('2014-9-8'))  # right\r\n5 Out: Timestamp('2014-09-07 19:30:00', offset='6H')\r\n\r\n7 Inn: P.version.version\r\n7 Out: '0.14.1'\r\n```\r\n\r\ndf.index.asof() appears to know how to handle datetime strings, but it cant"""
8241,42496620,jorisvandenbossche,jreback,2014-09-11 07:31:31,2014-09-11 11:37:35,2014-09-11 11:37:35,closed,,,1,Bug;Duplicate;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8241,b'BUG: reindex on level of multi-index does not respect order of new index ?',"b""When having a MultiIndex, and reindexing on a level of it, `reindex` does not respect the order of the new index. Is this a bug? Or a limitation of the multi-index reindexing (as in one level you don't have unique elements?) which should be documented?\r\n\r\n```\r\nIn [69]: s = pd.Series([0,1,2,3,4,5,6,7], index=pd.MultiIndex.from_product([[1,2,3,4],[7,8]]))\r\n\r\nIn [70]: s\r\nOut[70]:\r\n1  7    0\r\n   8    1\r\n2  7    2\r\n   8    3\r\n3  7    4\r\n   8    5\r\n4  7    6\r\n   8    7\r\ndtype: int64\r\n\r\nIn [71]: s.reindex([3,2], level=0)\r\nOut[71]:\r\n2  7    2\r\n   8    3\r\n3  7    4\r\n   8    5\r\ndtype: int64\r\n```\r\n\r\nAls with a dataframe multi-indexed columns:\r\n```\r\nIn [72]: df = pd.DataFrame(np.random.randn(5,6), columns=pd.MultiIndex.from_prod\r\nuct([list('abc'), list('ef')]))\r\n\r\nIn [73]: df\r\nOut[73]:\r\n          a                   b                   c\r\n          e         f         e         f         e         f\r\n0 -0.968421  0.343670  0.353754  0.211625  1.964620  0.696301\r\n1 -1.082284  0.532646  0.099657 -1.355934 -0.748171 -0.766453\r\n2  0.674002 -0.440444 -0.296365  0.225170  0.592352 -0.691878\r\n3  0.880808  0.470677  0.330382  0.937828 -0.225192 -1.059934\r\n4 -0.616588  0.501665 -0.232722  1.300403  0.235369  2.318810\r\n\r\nIn [74]: df.reindex(columns=['c', 'b'], level=0)\r\nOut[74]:\r\n          b                   c\r\n          e         f         e         f\r\n0  0.353754  0.211625  1.964620  0.696301\r\n1  0.099657 -1.355934 -0.748171 -0.766453\r\n2 -0.296365  0.225170  0.592352 -0.691878\r\n3  0.330382  0.937828 -0.225192 -1.059934\r\n4 -0.232722  1.300403  0.235369  2.318810\r\n```"""
8240,42483813,onesandzeroes,TomAugspurger,2014-09-11 02:50:00,2014-09-27 03:17:58,2014-09-17 19:32:26,closed,,0.15.0,5,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8240,b'BUG: boxplot fails when one column is all NaNs',"b""Fixes #8181. Currently the boxplot fails when trying to compute quantiles on an empty array, which numpy can't deal with. Works okay if we use an array with a single `np.nan` instead, the empty column is just left out of the plot as expected. \r\n\r\nI guess that might be a bit of a hack, relying on how numpy deals with a single `np.nan` in an array versus an empty array? If people think it's too risky to depend on that behaviour I'll try to rethink.\r\n\r\nNew output:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ndf = pd.DataFrame(np.random.randn(100, 4))\r\ndf.loc[:, 0] = np.nan\r\ndf.plot(kind='box')\r\nplt.show()\r\n```\r\n\r\n![missing_boxplot](https://cloud.githubusercontent.com/assets/1460294/4228586/39e763e4-395e-11e4-83d6-2f79fc12e4c3.png)\r\n"""
8237,42430853,jreback,jreback,2014-09-10 16:14:58,2014-09-10 18:09:28,2014-09-10 18:09:27,closed,,0.15.0,0,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/8237,b'BUG: Bug in putting a PeriodIndex into a Series would convert to int64',b'closes #7932'
8235,42424680,TomAugspurger,jreback,2014-09-10 15:21:35,2014-09-10 17:45:37,2014-09-10 17:45:32,closed,,0.15.0,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8235,b'BUG: interpolate with no nans and limit',"b""Closes https://github.com/pydata/pandas/issues/7173\r\n\r\nI was assuming that there would be at least one NaN when I was adjusting for the limits. Pretty simple fix. \r\nHere's the intended behavior:\r\n\r\n```python\r\nIn [1]: s = pd.Series([1., 2, 3])\r\n\r\nIn [2]: s.interpolate()\r\nOut[2]: \r\n0    1\r\n1    2\r\n2    3\r\ndtype: float64\r\n\r\nIn [3]: s.interpolate(limit=1)\r\nOut[3]: \r\n0    1\r\n1    2\r\n2    3\r\ndtype: float64\r\n```"""
8233,42394501,onesandzeroes,jreback,2014-09-10 09:37:14,2014-09-10 18:09:15,2014-09-10 18:09:05,closed,,0.15.0,2,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8233,b'BUG: DataFrame.to_string ignores col_space when header=False',"b""Fixes #8230. I think this is a pretty easy fix, `col_space` wasn't being used at all when `header` was `False` so just had to add the minimum width. \r\n\r\nResults:\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame([[0,1,2]], index=['Sum'], columns=['A','B','C'])\r\n\r\nprint(df.to_string(col_space=20))\r\n# Output:\r\n                       A                    B                    C\r\nSum                    0                    1                    2\r\n\r\n\r\nprint(df.to_string(col_space=20, header=False))\r\n# Output:\r\nSum                    0                    1                    2\r\n```"""
8230,42368235,0zeroth,jreback,2014-09-10 01:03:51,2014-09-10 18:09:15,2014-09-10 18:09:15,closed,,0.15.0,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/8230,b'DataFrame.to_string ignores col_space when header=False',"b""    import pandas as pd\r\n    df = pd.DataFrame([[0,1,2]], index=['Sum'], columns=['A','B','C'])\r\n    print(df.to_string(col_space=20))\r\n    print(df.to_string(col_space=20, header=False))\r\n\r\nOutput:\r\n\r\n                           A                    B                    C \r\n    Sum                    0                    1                    2\r\n    Sum  0  1  2\r\n\r\n(a minor irritation...)"""
8220,42285390,onesandzeroes,jreback,2014-09-09 09:08:06,2014-09-09 11:29:36,2014-09-09 11:29:21,closed,,0.15.0,3,API Design;Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8220,b'API: Series.to_csv(path=None) should return string to match DataFrame.to_csv()',"b'Addresses #8215. Currently, `Series.to_csv(path=None)` returns `None`. The `DataFrame` method returns a string, and already had this behaviour mentioned in the docstring, so it should be OK to make the `Series` method match this.'"
8218,42253909,jreback,jreback,2014-09-08 23:08:34,2014-09-08 23:40:44,2014-09-08 23:40:44,closed,,0.15.0,0,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8218,b'BUG: Bug in read_csv where squeeze=True would return a view (GH8217)',b'closes #8217 '
8217,42236202,patricktokeeffe,jreback,2014-09-08 19:52:55,2014-09-08 23:40:54,2014-09-08 23:40:44,closed,,0.15.0,1,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8217,b'View returned when using squeeze=True with read_csv?',"b'````python\r\nIn [1]: import pandas as pd, StringIO\r\n\r\nIn [2]: csv = StringIO.StringIO(\'time,data\\n0,10\\n1,11\\n2,12\\n4,14\\n5,15\\n3,13\')\r\n\r\nIn [3]: df = pd.read_csv(csv, index_col=\'time\')\r\n\r\nIn [4]: df\r\nOut[4]: \r\n      data\r\ntime      \r\n0       10\r\n1       11\r\n2       12\r\n4       14\r\n5       15\r\n3       13\r\n\r\nIn [5]: df.sort(inplace=True)\r\n\r\nIn [6]: df\r\nOut[6]: \r\n      data\r\ntime      \r\n0       10\r\n1       11\r\n2       12\r\n3       13\r\n4       14\r\n5       15\r\n````\r\n\r\nSpecifying `squeeze=True` returns a view instead of first-class object? Is this by design and if so, why?\r\n\r\nAs of 0.14.1, `inplace=True` is the default (related #5190) for series sorts so the following error will occur by default if `squeeze=True` was specified.\r\n\r\n````python\r\nIn [1]: import pandas as pd, StringIO\r\n\r\nIn [2]: csv = StringIO.StringIO(\'time,data\\n0,10\\n1,11\\n2,12\\n4,14\\n5,15\\n3,13\')\r\n\r\nIn [3]: df = pd.read_csv(csv, index_col=\'time\', squeeze=True)\r\n\r\nIn [4]: df\r\nOut[4]: \r\n      data\r\ntime      \r\n0       10\r\n1       11\r\n2       12\r\n4       14\r\n5       15\r\n3       13\r\n\r\nIn [5]: df.sort(inplace=True)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-850f333de498> in <module>()\r\n----> 1 df.sort(inplace=True)\r\n\r\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\series.pyc in sort(self, axis, ascending, kind, na_position, inplace)\r\n   1651                           kind=kind,\r\n   1652                           na_position=na_position,\r\n-> 1653                           inplace=inplace)\r\n   1654 \r\n   1655     def order(self, na_last=None, ascending=True, kind=\'quicksort\', na_position=\'last\', inplace=False):\r\n\r\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\series.pyc in order(self, na_last, ascending, kind, na_position, inplace)\r\n   1684         # GH 5856/5853\r\n   1685         if inplace and self._is_cached:\r\n-> 1686             raise ValueError(""This Series is a view of some other array, to ""\r\n   1687                              ""sort in-place you must create a copy"")\r\n   1688 \r\n\r\nValueError: This Series is a view of some other array, to sort in-place you must create a copy\r\n````\r\n'"
8209,42157364,flaviovs,jreback,2014-09-08 00:56:58,2014-09-13 22:59:30,2014-09-13 22:59:30,closed,,0.15.0,11,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/8209,b'AttributeError on datetime.timedelta assignment after reindex',"b'In the code below, I expected that the assignment didn\'t fail, since everything seems ""right"" . However, AttributeError is being raised.\r\n\r\nA bug? Am I doing something wrong?\r\n\r\n```python\r\nIn [106]: import pandas as pd\r\n\r\nIn [107]: import datetime as dt\r\n\r\nIn [108]: s = pd.Series([])\r\n\r\nIn [109]: s\r\nOut[109]: Series([], dtype: float64)\r\n\r\nIn [110]: s.loc[\'B\'] = dt.timedelta(1)\r\n\r\nIn [111]: s\r\nOut[111]: \r\nB   1 days\r\ndtype: timedelta64[ns]\r\n\r\nIn [112]: s = s.reindex(s.index.insert(0, \'A\'))\r\n\r\nIn [113]: s\r\nOut[113]: \r\nA      NaT\r\nB   1 days\r\ndtype: timedelta64[ns]\r\n\r\nIn [114]: s.loc[\'A\'] = dt.timedelta(1)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-114-f662acce72ae> in <module>()\r\n----> 1 s.loc[\'A\'] = dt.timedelta(1)\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in setitem(self, **kwargs)\r\n\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in apply(self, f, axes, filter, do_integrity_check, **kwargs)\r\n\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in setitem(self, indexer, value)\r\n\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in _try_coerce_args(self, values, other)\r\n\r\n/home/flaviovs/.virtualenv/hm/local/lib/python2.7/site-packages/pandas-0.14.1_354_g12248ff-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in masker(v)\r\n\r\nAttributeError: \'datetime.timedelta\' object has no attribute \'view\'\r\n```\r\n\r\nTried with 0.14.1 and latest on GH (v0.15pre) , to no avail.\r\n\r\nI\'m so sorry I\'m not able to provide a patch right now (still wrapping my head around pandas internals -- any directions about how to fix this is issue will be appreciated, though).\r\n\r\nVersions:\r\n\r\n```python\r\nIn [125]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.2.60\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1-354-g12248ff\r\nnose: None\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 2.1.0\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.1.1rc2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```'"
8189,42068431,TomAugspurger,jreback,2014-09-05 17:11:38,2014-09-06 14:59:08,2014-09-06 14:59:08,closed,,0.16.0,2,Bug;Missing-data;Timezones,https://api.github.com/repos/pydata/pandas/issues/8189,b'BUG/API: fillna converts column of Timestamps to datetime64',"b""Not sure if this is intentional / known.\r\n\r\n```\r\nIn [58]: tz = pd.DatetimeIndex(start='2010-01-01', periods=10, freq='d', tz='US/Central')\r\n\r\nIn [59]: df = pd.DataFrame(np.random.randn(10, 2), columns=['A', 'B'])\r\n\r\nIn [60]: df['time'] = tz\r\n\r\nIn [62]: df.dtypes\r\nOut[62]: \r\nA       float64\r\nB       float64\r\ntime     object\r\ndtype: object\r\n\r\nIn [63]: df.fillna(0).dtypes\r\nOut[63]: \r\nA              float64\r\nB              float64\r\ntime    datetime64[ns]\r\ndtype: object\r\n```\r\nNotice that the dtype of the `time` column changes from `object` (column of `Timestamps`) to `datetime64[ns]`. \r\nI think what's happening is the timestamps are being downcast:\r\n\r\n```\r\nIn [64]: df.fillna(0, downcast=False).dtypes\r\nOut[64]: \r\nA       float64\r\nB       float64\r\ntime     object\r\ndtype: object\r\n```\r\n\r\nThe reason I worry about this as the default is because the timestamp information is lost."""
8181,42048694,TomAugspurger,TomAugspurger,2014-09-05 13:56:08,2014-09-17 19:33:59,2014-09-17 19:33:59,closed,,0.15.0,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8181,b'BUG: boxplot fails when one column is all NaNs',"b'Very much an edge case:\r\n\r\n```python\r\nIn [17]: df = pd.DataFrame(np.random.randn(100, 4))\r\nIn [18]: df.loc[:, 0] = np.nan\r\nIn [19]: df.plot(kind=\'box\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-19-06443674b77d> in <module>()\r\n----> 1 df.plot(kind=\'box\')\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/pandas/pandas/tools/plotting.py in plot_frame(frame, x, y, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, style, title, xlim, ylim, logx, logy, xticks, yticks, kind, sort_columns, fontsize, secondary_y, layout, **kwds)\r\n   2362                              secondary_y=secondary_y, layout=layout, **kwds)\r\n   2363 \r\n-> 2364     plot_obj.generate()\r\n   2365     plot_obj.draw()\r\n   2366     return plot_obj.result\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/pandas/pandas/tools/plotting.py in generate(self)\r\n    913         self._compute_plot_data()\r\n    914         self._setup_subplots()\r\n--> 915         self._make_plot()\r\n    916         self._add_table()\r\n    917         self._make_legend()\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/pandas/pandas/tools/plotting.py in _make_plot(self)\r\n   2140             kwds = self.kwds.copy()\r\n   2141 \r\n-> 2142             ret, bp = plotf(ax, y, column_num=0, **kwds)\r\n   2143             self.maybe_color_bp(bp)\r\n   2144             self._return_obj = ret\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/pandas/pandas/tools/plotting.py in plotf(ax, y, column_num, **kwds)\r\n   2059             else:\r\n   2060                 y = remove_na(y)\r\n-> 2061             bp = ax.boxplot(y, **kwds)\r\n   2062 \r\n   2063             if self.return_type == \'dict\':\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/matplotlib-1.5.x-py3.4-macosx-10.9-x86_64.egg/matplotlib/axes/_axes.py in boxplot(self, x, notch, sym, vert, whis, positions, widths, patch_artist, bootstrap, usermedians, conf_intervals, meanline, showmeans, showcaps, showbox, showfliers, boxprops, labels, flierprops, medianprops, meanprops, capprops, whiskerprops, manage_xticks)\r\n   3043         """"""\r\n   3044         bxpstats = cbook.boxplot_stats(x, whis=whis, bootstrap=bootstrap,\r\n-> 3045                                        labels=labels)\r\n   3046         if flierprops is None:\r\n   3047             flierprops = dict(sym=sym)\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/matplotlib-1.5.x-py3.4-macosx-10.9-x86_64.egg/matplotlib/cbook.py in boxplot_stats(X, whis, bootstrap, labels)\r\n   1963 \r\n   1964         # medians and quartiles\r\n-> 1965         q1, med, q3 = np.percentile(x, [25, 50, 75])\r\n   1966 \r\n   1967         # interquartile range\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input)\r\n   2818         axis = 0\r\n   2819 \r\n-> 2820     return _compute_qth_percentile(sorted, q, axis, out)\r\n   2821 \r\n   2822 # handle sequence of q\'s without calling sort multiple times\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/numpy/lib/function_base.py in _compute_qth_percentile(sorted, q, axis, out)\r\n   2824     if not isscalar(q):\r\n   2825         p = [_compute_qth_percentile(sorted, qi, axis, None)\r\n-> 2826              for qi in q]\r\n   2827 \r\n   2828         if out is not None:\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/numpy/lib/function_base.py in <listcomp>(.0)\r\n   2824     if not isscalar(q):\r\n   2825         p = [_compute_qth_percentile(sorted, qi, axis, None)\r\n-> 2826              for qi in q]\r\n   2827 \r\n   2828         if out is not None:\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/numpy/lib/function_base.py in _compute_qth_percentile(sorted, q, axis, out)\r\n   2854     # Use add.reduce in both cases to coerce data type as well as\r\n   2855     #   check and use out array.\r\n-> 2856     return add.reduce(sorted[indexer]*weights, axis=axis, out=out)/sumval\r\n   2857 \r\n   2858 def trapz(y, x=None, dx=1.0, axis=-1):\r\n\r\nValueError: operands could not be broadcast together with shapes (0,) (2,) \r\n```\r\nSame result for `df.boxplot()`.'"
8177,41969880,TomAugspurger,TomAugspurger,2014-09-04 18:36:23,2014-09-07 00:30:08,2014-09-07 00:30:07,closed,,0.15.0,4,Bug;Missing-data;Visualization,https://api.github.com/repos/pydata/pandas/issues/8177,b'BUG: barplot with NaNs',"b""Closes https://github.com/pydata/pandas/issues/8175\r\n\r\nThis makes barplot more consistent with how NaN's are handled in AreaPlots and PiePlots.\r\n"""
8176,41967212,jreback,jreback,2014-09-04 18:11:29,2014-09-04 18:58:18,2014-09-04 18:58:17,closed,,0.15.0,1,API Design;Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/8176,b'BUG/API: Previously an enlargement with a mixed-dtype frame would act unlike append (related GH2578)',b'related #2578\r\n\r\nfrom [SO](http://stackoverflow.com/questions/25670760/copying-a-row-screws-up-pandas-column-data-types-how-to-avoid)'
8175,41965216,tom-alcorn,TomAugspurger,2014-09-04 17:51:45,2014-09-07 00:30:07,2014-09-07 00:30:07,closed,,0.15.0,2,Bug;Missing-data;Visualization,https://api.github.com/repos/pydata/pandas/issues/8175,b'Stacked bar plot negative values do not work correctly if dataframe contains NaN values',"b""While trying to produce a stacked bar plot which includes negative values, I found that if the dataframe contains NaN values the bar plot does not display correctly.\r\n\r\nSpecifically, this code:\r\n```python\r\ndf = pd.DataFrame([[10,20,5,40],[-5,5,20,30],[np.nan,-10,-10,20],[10,20,20,-40]], columns = ['A','B','C','D'])\r\ndf.plot(kind = 'bar', stacked=True); plt.show();\r\n```\r\nincorrectly produces this plot\r\n\r\n![screen shot 2014-09-04 at 1 38 47 pm](https://cloud.githubusercontent.com/assets/8483583/4153575/71c69b9e-345a-11e4-9d2d-cc0b00f4ee8b.png)\r\nNotice that at '2' on the x-axis, there should be a bar of size -10 for each of the 'B' and 'C' categories.\r\n\r\nHowever, when I replace the NaN values with 0s by doing\r\n```python\r\ndf = pd.DataFrame([[10,20,5,40],[-5,5,20,30],[np.nan,-10,-10,20],[10,20,20,-40]], columns = ['A','B','C','D'])\r\ndf = df.fillna(0)\r\ndf.plot(kind = 'bar', stacked=True); plt.show();\r\n```\r\nthen the plot displays correctly\r\n\r\n![screen shot 2014-09-04 at 1 41 41 pm](https://cloud.githubusercontent.com/assets/8483583/4153668/1e31ba26-345b-11e4-88a9-e7a0a764365b.png)\r\n\r\nThis is clearly not a good behaviour. I suspect that this happens because the bars corresponding to the negative values are trying to use np.nan as their 'bottom' argument and thus not displaying at all, but I haven't investigated further. \r\n\r\nIt would be nice if area-style plots like this would either automatically replace NaN values with 0 or throw an error about NaN values present in the dataframe causing problems for the plotting functions."""
8171,41881237,behzadnouri,jreback,2014-09-03 23:54:36,2014-09-06 19:50:01,2014-09-06 19:13:41,closed,,0.15.0,4,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/8171,b'BUG: GroupBy.count() with float32 data type does not exclude nan',b'closes https://github.com/pydata/pandas/issues/8169'
8169,41848175,jreback,jreback,2014-09-03 18:17:32,2014-09-06 19:13:41,2014-09-06 19:13:41,closed,,0.15.0,0,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/8169,b'BUG: groupby.count() on different dtypes seems buggy',"b""from [SO](http://stackoverflow.com/questions/25648923/groupby-count-returns-different-values-for-pandas-dataframe-count-vs-describ)\r\n\r\nsomething odd going on here:\r\n\r\n```\r\nvals = np.hstack((np.random.randint(0,5,(100,2)), np.random.randint(0,2,(100,2))))\r\ndf = pd.DataFrame(vals, columns=['a', 'b', 'c', 'd'])\r\ndf[df==2] = np.nan\r\ndf2 = df.copy()\r\ndf2['a'] = df2['a'].astype('float32')\r\ndf2['b'] = df2['b'].astype('float32')\r\n```\r\n\r\n```\r\ndf.groupby(['c', 'd']).count()\r\ndf2.groupby(['c','d']).count()\r\n```"""
8165,41803280,colbrac,jreback,2014-09-03 10:47:12,2015-10-22 13:26:30,2015-10-22 13:26:30,closed,,Next Major Release,10,Bug;IO HDF5;Timezones,https://api.github.com/repos/pydata/pandas/issues/8165,b'BUG: round-trip of tz in an index using fixed-format for HDF5',"b'xref #9270 \r\n\r\nWith Python 2.7.6, pandas 0.13.1 and numpy 1.8.1, pytables 3.1.1, both 32 and 64bit (Python x,y and WinPython), I can load my hdf5 file. \r\nWith Python 3.4.1 64bit, pandas 0.14.1, numpy 1.8.2, pytables 3.1.1  (Anaconda3 2.0.1) I get the following error:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-2-fe436a8e76a5>"", line 1, in <module>\r\n    test = pd.read_hdf(\'datafile.h5\', \'data\')\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 330, in read_hdf\r\n    return f(store, True)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 322, in <lambda>\r\n    key, auto_close=auto_close, **kwargs)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 669, in select\r\n    auto_close=auto_close).get_values()\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 1335, in get_values\r\n    results = self.func(self.start, self.stop)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 658, in func\r\n    columns=columns, **kwargs)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 2658, in read\r\n    ax = self.read_index(\'axis%d\' % i)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 2257, in read_index\r\n    _, index = self.read_index_node(getattr(self.group, key))\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py"", line 2385, in read_index_node\r\n    _unconvert_index(data, kind, encoding=self.encoding), **kwargs)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\index.py"", line 125, in __new__\r\n    result = DatetimeIndex(data, copy=copy, name=name, **kwargs)\r\n\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\pandas\\tseries\\index.py"", line 301, in __new__\r\n    infer_dst=infer_dst)\r\n\r\n  File ""tslib.pyx"", line 2165, in pandas.tslib.tz_localize_to_utc (pandas\\tslib.c:33574)\r\n\r\n  File ""tslib.pyx"", line 2082, in pandas.tslib._get_deltas (pandas\\tslib.c:32187)\r\n\r\n  File ""tslib.pyx"", line 872, in pandas.tslib._get_utcoffset (pandas\\tslib.c:16036)\r\n\r\nAttributeError: \'numpy.bytes_\' object has no attribute \'utcoffset\'\r\n\r\n\r\nThe index in question is:\r\nclass \'pandas.tseries.index.DatetimeIndex\'\r\n[2013-04-03 00:00:00+02:00, ..., 2013-04-04 00:00:00+02:00]\r\nLength: 8641, Freq: 10S, Timezone: Europe/Amsterdam'"
8157,41665467,htkm,jreback,2014-09-01 23:12:42,2014-09-04 00:32:04,2014-09-04 00:29:43,closed,,0.15.0,3,Bug;Compat,https://api.github.com/repos/pydata/pandas/issues/8157,"b""BUG: add 'import warnings' statement in panel.py (GH8152)""",b'closes #8152.'
8152,41605254,aidoom,jreback,2014-09-01 03:55:23,2014-09-04 00:29:43,2014-09-04 00:29:43,closed,,0.15.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/8152,b'BUG: warnings import missing for panel',"b'It appears the `warnings` import is missing in `panel.py`\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: pd.__version__\r\nOut[3]: \'0.14.1\'\r\n\r\nIn [4]: p = pd.Panel(np.random.rand(3, 3, 3))\r\n\r\nIn [5]: p.major_xs(1, copy=False)\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-5-0b8025289379> in <module>()\r\n----> 1 p.major_xs(1, copy=False)\r\n\r\n/home/aido/.env/local/lib/python2.7/site-packages/pandas/core/panel.pyc in major_xs(self, key, copy)\r\n    715         """"""\r\n    716         if copy is not None:\r\n--> 717             warnings.warn(""copy keyword is deprecated, ""\r\n    718                           ""default is to return a copy or a view if possible"")\r\n    719 \r\n\r\nNameError: global name \'warnings\' is not defined\r\n```'"
8151,41597909,behzadnouri,behzadnouri,2014-08-31 22:39:18,2015-05-10 19:52:12,2015-05-10 19:37:37,closed,,Next Major Release,8,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8151,b'BUG: consistency between logical ops & type casts',"b""Closes [#6528](https://github.com/pydata/pandas/issues/6528).\r\nCurrently series logical operations are bound to return a series with `dtype='bool'`. That being a good decision or not, it is good to have some internal consistency:\r\n\r\n    >>> a = pd.Series([nan, nan, nan])\r\n    >>> b = pd.Series([False, True, nan])\r\n    >>> a | b == b | a  # not symmetric\r\n    0     True\r\n    1    False\r\n    2     True\r\n    dtype: bool\r\n    >>> a & b == a.astype(bool) & b.astype(bool)  # not consistent w/ type casts\r\n    0     True\r\n    1    False\r\n    2    False\r\n    dtype: bool\r\n\r\nWith this patch, [all these tests pass.](https://github.com/pydata/pandas/pull/8151/files#diff-933180870dd152b775952fc54a99dba7R2371)"""
8146,41533875,jreback,jreback,2014-08-29 19:49:11,2014-08-29 20:44:25,2014-08-29 20:44:25,closed,,0.15.0,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8146,b'BUG: comparison of category vs np dtypes buggy (GH8143)',b'closes #8143 '
8143,41531521,TomAugspurger,jreback,2014-08-29 19:17:29,2014-08-29 20:44:25,2014-08-29 20:44:25,closed,,0.15.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8143,"b""BUG/API: DataFrame.dtypes == 'categorical'""","b'Should this work?\r\n\r\n```python\r\nIn [70]: cat = pd.Categorical([\'a\', \'b\', \'c\'])\r\n\r\nIn [71]: obj = pd.Series([\'a\', \'b\', \'c\'])\r\n\r\nIn [72]: num = pd.Series([1, 2, 3])\r\n\r\nIn [73]: df = pd.concat([pd.Series(cat), obj, num], axis=1, keys=[\'cat\', \'obj\', \'num\'])\r\n\r\nIn [74]: df\r\nOut[74]: \r\n  cat obj  num\r\n0   a   a    1\r\n1   b   b    2\r\n2   c   c    3\r\n\r\nIn [75]: df.dtypes\r\nOut[75]: \r\ncat    category\r\nobj      object\r\nnum       int64\r\ndtype: object\r\n\r\nIn [76]: df.dtypes == \'category\'\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-76-48fb3d0c49cf> in <module>()\r\n----> 1 df.dtypes == \'category\'\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/pandas-0.14.1_291_g7a8a030-py3.4-macosx-10.9-x86_64.egg/pandas/core/ops.py in wrapper(self, other)\r\n    579 \r\n    580             # scalars\r\n--> 581             res = na_op(values, other)\r\n    582             if np.isscalar(res):\r\n    583                 raise TypeError(\'Could not compare %s type with Series\'\r\n\r\n/Users/tom/Envs/py3/lib/python3.4/site-packages/pandas-0.14.1_291_g7a8a030-py3.4-macosx-10.9-x86_64.egg/pandas/core/ops.py in na_op(x, y)\r\n    526             msg = ""Cannot compare a Categorical for op {op} with type {typ}. If you want to \\n"" \\\r\n    527                   ""compare values, use \'series <op> np.asarray(cat)\'.""\r\n--> 528             raise TypeError(msg.format(op=op,typ=type(y)))\r\n    529         if x.dtype == np.object_:\r\n    530             if isinstance(y, list):\r\n\r\nTypeError: Cannot compare a Categorical for op <built-in function eq> with type <class \'str\'>. If you want to \r\ncompare values, use \'series <op> np.asarray(cat)\'.\r\n```\r\n\r\nFor comparison\r\n\r\n```python\r\nIn [77]: df.dtypes == \'object\'\r\nOut[77]: \r\ncat    False\r\nobj     True\r\nnum    False\r\ndtype: bool\r\n\r\nIn [78]: df.dtypes == \'int64\'\r\nOut[78]: \r\ncat    False\r\nobj    False\r\nnum     True\r\ndtype: bool\r\n```\r\n\r\nThis doesn\'t work either: `In [87]: df.dtypes == df.cat.dtype` (raise the same TypeError)'"
8134,41395116,jreback,jreback,2014-08-28 14:21:03,2014-08-28 15:04:07,2014-08-28 15:04:07,closed,,0.15.0,3,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8134,b'BUG: Bug in multi-index slicing with various edge cases (GH8132)',"b'closes #8132 \r\n```\r\n# A1 - Works - Get all values under ""A0"" and ""A1""\r\ndf1.loc[(slice(\'A1\')),:]\r\n\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B0 2013-06-11      22\r\n      2013-07-02      35\r\n   B1 2013-07-09      14\r\n      2013-07-30       9\r\n   B2 2013-08-06       4\r\nA1 B0 2013-06-11      40\r\n      2013-07-02      18\r\n   B1 2013-07-09       4\r\n      2013-07-30       2\r\n   B2 2013-08-06       5\r\n```\r\n\r\n\r\n```\r\n# A2 - Works - Get all values from the start to ""A2""\r\ndf1.loc[(slice(\'A2\')),:]\r\n\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B0 2013-06-11      22\r\n      2013-07-02      35\r\n   B1 2013-07-09      14\r\n      2013-07-30       9\r\n   B2 2013-08-06       4\r\nA1 B0 2013-06-11      40\r\n      2013-07-02      18\r\n   B1 2013-07-09       4\r\n      2013-07-30       2\r\n   B2 2013-08-06       5\r\nA2 B0 2013-09-03       1\r\n      2013-10-01       2\r\n   B1 2013-07-09       3\r\n      2013-08-06       4\r\n   B2 2013-09-03       2\r\n```\r\n\r\n```\r\n# A3 - Works - Get all values under ""B1"" or ""B2""\r\ndf1.loc[(slice(None),slice(\'B1\',\'B2\')),:]\r\n\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B1 2013-07-09      14\r\n      2013-07-30       9\r\n   B2 2013-08-06       4\r\nA1 B1 2013-07-09       4\r\n      2013-07-30       2\r\n   B2 2013-08-06       5\r\nA2 B1 2013-07-09       3\r\n      2013-08-06       4\r\n   B2 2013-09-03       2\r\n```\r\n\r\n```\r\n# A4 - Works - Get all values between 2013-07-02 and 2013-07-09\r\ndf1.loc[(slice(None),slice(None),slice(\'20130702\',\'20130709\')),:]\r\n\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B0 2013-07-02      35\r\n   B1 2013-07-09      14\r\nA1 B0 2013-07-02      18\r\n   B1 2013-07-09       4\r\nA2 B1 2013-07-09       3\r\n```\r\n\r\n```\r\n# B1 -  Get all values in B0 that are also under A0, A1 and A2\r\ndf1.loc[(slice(\'A2\'),slice(\'B0\')),:]\r\n\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B0 2013-06-11      22\r\n      2013-07-02      35\r\nA1 B0 2013-06-11      40\r\n      2013-07-02      18\r\nA2 B0 2013-09-03       1\r\n      2013-10-01       2\r\n```\r\n\r\n```\r\n# B2 - Get all values in B0, B1 and B2 (similar to what #2 is doing for the As)\r\ndf1.loc[(slice(None),slice(\'B2\')),:]\r\n\r\n                 VALUES\r\nA  B  DATE              \r\nA0 B0 2013-06-11      22\r\n      2013-07-02      35\r\n   B1 2013-07-09      14\r\n      2013-07-30       9\r\n   B2 2013-08-06       4\r\nA1 B0 2013-06-11      40\r\n      2013-07-02      18\r\n   B1 2013-07-09       4\r\n      2013-07-30       2\r\n   B2 2013-08-06       5\r\nA2 B0 2013-09-03       1\r\n      2013-10-01       2\r\n   B1 2013-07-09       3\r\n      2013-08-06       4\r\n   B2 2013-09-03       2\r\n ```\r\n\r\n```\r\n# B3 - Get all values from B1 to B2 and up to 2013-08-06\r\ndf1.loc[(slice(None),slice(\'B1\',\'B2\'),slice(\'2013-08-06\')),:]\r\n\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B1 2013-07-09      14\r\n      2013-07-30       9\r\n   B2 2013-08-06       4\r\nA1 B1 2013-07-09       4\r\n      2013-07-30       2\r\n   B2 2013-08-06       5\r\nA2 B1 2013-07-09       3\r\n      2013-08-06       4\r\n```\r\n\r\n```\r\n# B4 - Same as A4 but the start of the date slice is not a key.\r\ndf1.loc[(slice(None),slice(None),slice(\'20130701\',\'20130709\')),:]\r\n                  VALUES\r\nA  B  DATE              \r\nA0 B0 2013-07-02      35\r\n   B1 2013-07-09      14\r\nA1 B0 2013-07-02      18\r\n   B1 2013-07-09       4\r\nA2 B1 2013-07-09       3\r\n```'"
8132,41386491,jreback,jreback,2014-08-28 12:52:15,2014-08-28 15:04:07,2014-08-28 15:04:07,closed,,0.15.0,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8132,b'BUG: multiindex slicers edge cases',"b'from [SO](http://stackoverflow.com/questions/25530234/in-the-weeds-on-pandas-multi-indexing-using-slicers)\r\n\r\n```\r\ndf = pd.DataFrame({\'A\': [\'A0\'] * 5 + [\'A1\']*5 + [\'A2\']*5,\r\n            \'B\': [\'B0\',\'B0\',\'B1\',\'B1\',\'B2\'] * 3,\r\n            \'DATE\': [""2013-06-11"",\r\n                    ""2013-07-02"",\r\n                    ""2013-07-09"",\r\n                    ""2013-07-30"",\r\n                    ""2013-08-06"",\r\n                    ""2013-06-11"",\r\n                    ""2013-07-02"",\r\n                    ""2013-07-09"",\r\n                    ""2013-07-30"",\r\n                    ""2013-08-06"",\r\n                    ""2013-09-03"",\r\n                    ""2013-10-01"",\r\n                    ""2013-07-09"",\r\n                    ""2013-08-06"",\r\n                    ""2013-09-03""],\r\n             \'VALUES\': [22, 35, 14,  9,  4, 40, 18, 4, 2, 5, 1, 2, 3,4, 2]})\r\n\r\ndf.DATE = df[\'DATE\'].apply(lambda x: pd.to_datetime(x))\r\n\r\ndf1 = df.set_index([\'A\', \'B\', \'DATE\'])\r\ndf1 = df1.sortlevel()\r\n\r\ndf2 = df.set_index(\'DATE\')\r\n\r\n# A1 - Works - Get all values under ""A0"" and ""A1""\r\ndf1.loc[(slice(\'A1\')),:]\r\n\r\n# A2 - Works - Get all values from the start to ""A2""\r\ndf1.loc[(slice(\'A2\')),:]\r\n\r\n# A3 - Works - Get all values under ""B1"" or ""B2""\r\ndf1.loc[(slice(None),slice(\'B1\',\'B2\')),:]\r\n\r\n# A4 - Works - Get all values between 2013-07-02 and 2013-07-09\r\ndf1.loc[(slice(None),slice(None),slice(\'20130702\',\'20130709\')),:]\r\n\r\n##############################################\r\n# These do not work and I\'m wondering why... #\r\n##############################################\r\n\r\n# B1 - Does not work - Get all values in B0 that are also under A0, A1 and A2\r\ndf1.loc[(slice(\'A2\'),slice(\'B0\')),:]\r\n\r\n# B2 - Does not work - Get all values in B0, B1 and B2 (similar to what #2 is doing for the As)\r\ndf1.loc[(slice(None),slice(\'B2\')),:]\r\n\r\n# B3 - Does not work - Get all values from B1 to B2 and up to 2013-08-06\r\ndf1.loc[(slice(None),slice(\'B1\',\'B2\'),slice(\'2013-08-06\')),:]\r\n\r\n# B4 - Does not work - Same as A4 but the start of the date slice is not a key.\r\n#                      Would have thought the behavior would be similar to something like df2[\'20130701\':]\r\n#                      In other words, date indexing allowed starting on non-key points\r\ndf1.loc[(slice(None),slice(None),slice(\'20130701\',\'20130709\')),:]\r\n```'"
8131,41386063,jreback,jreback,2014-08-28 12:47:37,2014-09-17 14:48:25,2014-09-17 14:48:25,closed,,0.15.0,4,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/8131,b'BUG: setting a sparse column in a frame buggy',"b""from [SO](http://stackoverflow.com/questions/25517342/adding-a-sparse-series-to-a-dense-dataframe-in-pandas)\r\n\r\nthought this was well tested.....\r\n\r\n```\r\ndf = pd.DataFrame({'c_1':['a', 'b', 'c'], 'n_1': [1., 2., 3.]})\r\ndf['new_column'] = pd.Series([0, 0, 1]).to_sparse(fill_value=0)\r\n# AssertionError: Shape of new values must be compatible with manager shape\r\n```"""
8129,41365948,goyodiaz,jreback,2014-08-28 08:25:11,2014-09-17 15:30:14,2014-09-17 15:30:14,closed,,0.15.0,3,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8129,b'TypeError casting int64 to int32 when shifting time series',"b'I think I hit another 32bits issue. This happens in Win7 32 bits:\r\n\r\n```\r\nPython 2.7.6 (default, Nov 10 2013, 19:24:18) [MSC v.1500 32 bit (Intel)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas as pd\r\n>>> index = pd.date_range(\'2000-01-01\', periods=5, freq=\'D\')\r\n>>> s1 = pd.Series(range(5), index=index)\r\n>>> p = s1.iloc[1]\r\n>>> s1.shift(periods=p)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\generic.py"", line 3290, in shift\r\n    new_data = self._data.shift(periods=periods, axis=block_axis)\r\n  File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 2228, in shift\r\n    return self.apply(\'shift\', **kwargs)\r\n  File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 2192, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 791, in shift\r\n    new_values = np.roll(new_values, periods, axis=axis)\r\n  File ""D:\\Python27\\lib\\site-packages\\numpy\\core\\numeric.py"", line 1294, in roll\r\n    res = a.take(indexes, axis)\r\nTypeError: Cannot cast array data from dtype(\'int64\') to dtype(\'int32\') according to the rule \'safe\'\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: x86\r\nprocessor: x86 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: None\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.2.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.4.0\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```\r\n'"
8126,41337124,onesandzeroes,TomAugspurger,2014-08-27 22:19:58,2014-09-14 07:24:58,2014-08-30 14:10:48,closed,,0.15.0,8,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8126,"b""BUG: Fix bad axis limits when boxplotting with 'by' (#7528)""","b""closes #5517\r\n \r\nThe original bug involved the y-limits for boxplots being wrong when using `'by': turns out it has to do with the dummy axis that gets created when the plots don't fill the axis layout, e.g. when we're plotting 3 columns in a 2 by 2 layout.\r\n\r\nThe fix for this basically involves not setting the `sharey`/`sharex` argument for blank/dummy axes.\r\n\r\nI've tried to test this in a couple of different ways, but some feedback on better ways to test would be good. In particular, I'm not sure if it's okay to check the `_sharey` attribute of the axes, given that it's private."""
8122,41292885,mdmueller,jreback,2014-08-27 15:01:32,2014-08-28 17:02:07,2014-08-28 17:01:12,closed,,0.15.0,4,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8122,b'Made line comments work with delim_whitespace and custom line terminator',b'This extends the functionality in #7582 to whitespace-delimited and custom line terminated parsing in the C reader (fixes #8115).'
8115,41175547,jmkuhn,jreback,2014-08-26 14:05:11,2014-08-28 17:01:12,2014-08-28 17:01:12,closed,,0.15.0,9,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/8115,b'BUG: read_table with full line comment and delim_whitespace=True',"b'I expect the following to produce a DataFrame with 3 rows, 5 columns and no NaNs.  Instead it produces 5 rows with the comment lines filled with NaNs.  Pandas version 0.14.1.\r\n```python\r\nIn [2]: !cat test.txt\r\n# comment\r\n 0  1  2  3  4\r\n 5  6  7  8  9\r\n# comment\r\n10 11 12 13 15\r\n\r\nIn [3]: df = pd.read_table(""test.txt"", skipinitialspace=True,\r\n   ...: names=[""A"", ""B"", ""C"", ""D"", ""E""], delim_whitespace=True, comment=""#"")\r\n\r\nIn [4]: df\r\nOut[4]: \r\n    A   B   C   D   E\r\n0 NaN NaN NaN NaN NaN\r\n1   0   1   2   3   4\r\n2   5   6   7   8   9\r\n3 NaN NaN NaN NaN NaN\r\n4  10  11  12  13  15\r\n```\r\n'"
8112,41074058,behzadnouri,jreback,2014-08-25 15:41:52,2014-10-30 11:19:43,2014-10-28 00:04:25,closed,,0.15.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8112,b'BUG: groupby drops columns if the column name is the same as the grouper',"b""    >>> df = pd.DataFrame(np.random.randint(1, 9, (2, 3)), columns=['jim', 'joe', 'jolie'])\r\n    >>> df\r\n       jim  joe  jolie\r\n    0    4    7      8\r\n    1    7    8      3\r\n    >>> ts = df['joe'] * 0\r\n    >>> ts.name\r\n    'joe'\r\n    >>> gr = df.groupby(ts)\r\n    >>> gr.nth(0)  # this invokes _set_selection_from_grouper internally\r\n         jim  jolie\r\n    joe\r\n    7      4      8\r\n    >>> gr.apply(sum)  # joe column is gone\r\n         jim  jolie\r\n    joe\r\n    0     11     11\r\nwhereas:\r\n\r\n    >>> df.groupby(ts).apply(sum)\r\n         jim  joe  jolie\r\n    joe\r\n    0     11   15     11\r\n\r\nWhat happens is that [this line](https://github.com/pydata/pandas/blob/35a95274bcc90e1a44a881aac2e0183663272fc2/pandas/core/groupby.py#L474) removes the column from selection if it has the same name as the grouper."""
8103,40974479,sinhrks,jreback,2014-08-23 12:22:07,2014-08-30 01:19:10,2014-08-29 18:58:35,closed,,0.15.0,0,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8103,b'BUG: pivot_table raises KeyError with nameless index and columns',"b""``pivot_table`` raises ``KeyError`` when args passed to ``index`` and ``columns`` don't have ``name`` attribute. The fix looks especially useful when pivotting by ``dt`` accessor.\r\n\r\n```\r\ndf = pd.DataFrame({'dt1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 2, 1),\r\n                           datetime.datetime(2011, 3, 1), datetime.datetime(2011, 4, 1),\r\n                           datetime.datetime(2011, 5, 1), datetime.datetime(2012, 1, 1),\r\n                           datetime.datetime(2012, 2, 1), datetime.datetime(2012, 3, 1),\r\n                           datetime.datetime(2012, 4, 1)],\r\n                   'col1': 'A A A B B B C C C'.split(),\r\n                   'val1': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\r\n\r\npd.pivot_table(df, index=df['dt1'].dt.month, columns=df['dt1'].dt.year, values='val1'))\r\n# KeyError: 'Level None not found'\r\n```\r\n\r\n\r\n### After fix\r\n```\r\npd.pivot_table(df, index=df['dt1'].dt.month, columns=df['dt1'].dt.year, values='val1'))\r\n#    2011  2012\r\n# 1     1     6\r\n# 2     2     7\r\n# 3     3     8\r\n# 4     4     9\r\n# 5     5   NaN\r\n```\r\n"""
8095,40912321,numenic,numenic,2014-08-22 14:15:53,2014-08-26 07:14:24,2014-08-26 07:14:24,closed,,,2,Bug;Data IO;IO Excel,https://api.github.com/repos/pydata/pandas/issues/8095,"b""pandas 0.14.1 Excel Writer with 'excelwriter' engine cannot create a 2nd sheet""","b'When saving dataframe to Excel, and using ""xlsxwriter"", it cannot create a second sheet. Althought the second sheet appears in ""writer.sheets"", the excel file created do not show it. \r\n\r\nPython 2.7, Windows 7\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> print(pd.__version__)\r\n0.14.1\r\n>>> df=pd.DataFrame({\'a\': [1,2,3], \'b\':[4,5,6]})\r\n>>> writer = pd.ExcelWriter(\'try_xlsxwriter.xlsx\')\r\n>>> df.to_excel(writer, \'sheet1\')\r\n>>> writer.save() # ok, perfect\r\n>>> df.to_excel(writer, \'sheet2\')\r\n>>> writer.save()  # nothing appears in the Excel file!\r\n>>> print \'engine: %s\' % writer.engine\r\nengine: xlsxwriter\r\n>>> print \'sheets: %s\'% writer.sheets.keys()  # strangely, sheets look to be there though\r\nsheets: [\'sheet1\', \'sheet2\']\r\n```\r\n\r\nUsing the ""openpyxl"" engine works as expected.\r\n\r\n```python\r\n>>> writer = pd.ExcelWriter(\'try_openpyxl.xlsx\', engine=\'openpyxl\')\r\n>>> df.to_excel(writer, \'sheet1\')\r\n>>> writer.save()   # This works\r\n>>> df.to_excel(writer, \'sheet2\')\r\n>>> writer.save()   # This works too!\r\n>>> print \'engine: %s\' % writer.engine\r\nengine: openpyxl\r\n>>> print \'sheets: %s\'% writer.sheets.keys()\r\nsheets: [\'sheet1\', \'sheet2\']\r\n```\r\n\r\n'"
8094,40911162,cpcloud,cpcloud,2014-08-22 14:02:19,2014-08-22 15:04:27,2014-08-22 15:04:25,closed,cpcloud,0.15.0,2,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/8094,b'BUG: fix iat and at for Float64Index',b'closes #8092'
8092,40904573,IanSudbery,cpcloud,2014-08-22 12:39:14,2014-08-22 15:04:25,2014-08-22 15:04:25,closed,cpcloud,0.15.0,2,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/8092,b'.at and .iat indexing with Float64Index',"b'Using .at and .iat indexing with a Float64Index produces an error:\r\n\r\n```python\r\n\r\nIn [0]: import pandas as pd\r\nIn [1]: s = pd.Series([1,2,3], index = [0.1,0.2,0.3])\r\nIn [2]: s.at[0.1]\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-a5888722fbca> in <module>()\r\n----> 1 s.at[0.1]\r\n\r\n/ifs/apps/apps/python-2.7.1/lib/python2.7/site-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n   1264 \r\n   1265         key = self._convert_key(key)\r\n-> 1266         return self.obj.get_value(*key)\r\n   1267 \r\n   1268     def __setitem__(self, key, value):\r\n\r\n/ifs/apps/apps/python-2.7.1/lib/python2.7/site-packages/pandas/core/series.pyc in get_value(self, label)\r\n    778         value : scalar value\r\n    779         """"""\r\n--> 780         return self.index.get_value(self.values, label)\r\n    781 \r\n    782     def set_value(self, label, value):\r\n\r\n/ifs/apps/apps/python-2.7.1/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)\r\n   1821         k = _values_from_object(key)\r\n   1822         loc = self.get_loc(k)\r\n-> 1823         new_values = series.values[loc]\r\n   1824         if np.isscalar(new_values):\r\n   1825             return new_values\r\n\r\nAttributeError: \'numpy.ndarray\' object has no attribute \'values\'\r\n\r\nIn [3]: s.iat[1]\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-db1ebf8cc8e6> in <module>()\r\n----> 1 s.iat[1]\r\n\r\n/ifs/apps/apps/python-2.7.1/lib/python2.7/site-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n   1264 \r\n   1265         key = self._convert_key(key)\r\n-> 1266         return self.obj.get_value(*key)\r\n   1267 \r\n   1268     def __setitem__(self, key, value):\r\n\r\n/ifs/apps/apps/python-2.7.1/lib/python2.7/site-packages/pandas/core/series.pyc in get_value(self, label)\r\n    778         value : scalar value\r\n    779         """"""\r\n--> 780         return self.index.get_value(self.values, label)\r\n    781 \r\n    782     def set_value(self, label, value):\r\n\r\n/ifs/apps/apps/python-2.7.1/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)\r\n   1821         k = _values_from_object(key)\r\n   1822         loc = self.get_loc(k)\r\n-> 1823         new_values = series.values[loc]\r\n   1824         if np.isscalar(new_values):\r\n   1825             return new_values\r\n\r\nAttributeError: \'numpy.ndarray\' object has no attribute \'values\'\r\n\r\n```\r\n\r\nI\'m not sure if this is intended behavoir. The docs only talk about [], loc, iloc and ix indexing with Float64Index, but if so, perhaps a more useful error message would be in order. I wasn\'t even aware my index was Float64. Its gotten changed at some point without me being aware of it. \r\n'"
8076,40670549,shoyer,jreback,2014-08-20 07:35:05,2014-08-21 21:33:10,2014-08-21 21:33:10,closed,,0.15.0,2,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/8076,b'BUG: Putting a Categorical series in a DataFrame with a different index raises IndexError',"b'```\r\n>>> import pandas as pd\r\n>>> pd.DataFrame({\'x\': pd.Series([\'a\', \'b\', \'c\'])}, index=pd.date_range(\'20000101\', periods=3))\r\n              x\r\n2000-01-01  NaN\r\n2000-01-02  NaN\r\n2000-01-03  NaN\r\n>>> df = pd.DataFrame({\'x\': pd.Series(pd.Categorical([\'a\', \'b\', \'c\']))}, index=pd.date_range(\'20000101\', periods=3))\r\n>>> df\r\n<repr(<pandas.core.frame.DataFrame at 0x107f09f50>) failed: IndexError: Out of bounds on buffer access (axis 0)>\r\n>>> df.values\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-13-e8bb9a30bd4f> in <module>()\r\n----> 1 df.values\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/generic.py in values(self)\r\n   2071         int32.\r\n   2072         """"""\r\n-> 2073         return self.as_matrix()\r\n   2074 \r\n   2075     @property\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/generic.py in as_matrix(self, columns)\r\n   2053         self._consolidate_inplace()\r\n   2054         if self._AXIS_REVERSED:\r\n-> 2055             return self._data.as_matrix(columns).T\r\n   2056         return self._data.as_matrix(columns)\r\n   2057 \r\n\r\n/Users/shoyer/dev/pandas/pandas/core/internals.py in as_matrix(self, items)\r\n   2676 \r\n   2677         if self._is_single_block or not self.is_mixed_type:\r\n-> 2678             return mgr.blocks[0].get_values()\r\n   2679         else:\r\n   2680             return mgr._interleave()\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/internals.py in get_values(self, dtype)\r\n   1079     def get_values(self, dtype=None):\r\n   1080         """""" need to to_dense myself (and always return a ndim sized object) """"""\r\n-> 1081         values = self.values.to_dense()\r\n   1082         if values.ndim == self.ndim - 1:\r\n   1083             values = values.reshape((1,) + values.shape)\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/categorical.py in to_dense(self)\r\n    683     def to_dense(self):\r\n    684         """""" Return my \'dense\' repr """"""\r\n--> 685         return np.asarray(self)\r\n    686 \r\n    687     def fillna(self, fill_value=None, method=None, limit=None, **kwargs):\r\n\r\n/Users/shoyer/miniconda/envs/pandas-dev/lib/python2.7/site-packages/numpy/core/numeric.pyc in asarray(a, dtype, order)\r\n    458 \r\n    459     """"""\r\n--> 460     return array(a, dtype, copy=False, order=order)\r\n    461 \r\n    462 def asanyarray(a, dtype=None, order=None):\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/categorical.py in __array__(self, dtype)\r\n    484             dtype as categorical.levels.dtype\r\n    485         """"""\r\n--> 486         ret = com.take_1d(self.levels.values, self._codes)\r\n    487         if dtype and dtype != self.levels.dtype:\r\n    488             return np.asarray(ret, dtype)\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/common.py in take_nd(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\r\n    805                                  axis=axis, mask_info=mask_info)\r\n    806 \r\n--> 807     func(arr, indexer, out, fill_value)\r\n    808 \r\n    809     if flip_order:\r\n\r\n/Users/shoyer/dev/pandas/pandas/algos.so in pandas.algos.take_1d_object_object (pandas/algos.c:78943)()\r\n\r\nIndexError: Out of bounds on buffer access (axis 0)\r\n```\r\n\r\nThis is on master.'"
8070,40595743,jreback,jreback,2014-08-19 14:16:29,2014-08-19 15:12:53,2014-08-19 15:12:53,closed,,0.15.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8070,b'BUG: Bug in Timestamp comparisons with == and dtype of int64 (GH8058)',b'closes #8058 '
8059,40509453,seth-p,jreback,2014-08-18 17:03:26,2014-09-10 00:11:40,2014-08-28 17:34:49,closed,,0.15.0,8,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/8059,b'BUG: rolling_count() and expanding_*() with zero-length args; rolling/expanding_apply with min_periods=0',b'Closes https://github.com/pydata/pandas/issues/8056\r\nCloses https://github.com/pydata/pandas/issues/8080'
8058,40509260,l736x,jreback,2014-08-18 17:01:11,2014-08-19 15:12:53,2014-08-19 15:12:53,closed,,0.15.0,6,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8058,b'Timestamp cannot be compared to np.int64: impact on series binary operations',"b""Binary operations between two series fail if the .name attribute of the first is a Timestamp and the .name of the second is a numpy.int64.\r\nIt seems a corner case, but it's pretty common when the series are slices of dataframes.\r\nIt happens independently if, say, + or .add() is used.\r\nIt doesn't happen if the order is reversed.\r\nI guess it boils down to Timestamps not having a sufficiently wide __equal__ method.\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-2d9155319574> in <module>()\r\n----> 1 pd.__version\r\n\r\nAttributeError: 'module' object has no attribute '__version'\r\n\r\nIn [3]: df = pd.DataFrame(randn(5,2))\r\n\r\nIn [4]: a = df[0]\r\n\r\nIn [5]: b = pd.Series(randn(5))\r\n\r\nIn [6]: b.name = pd.Timestamp('2000-01-01')\r\n\r\nIn [7]: a / b\r\nOut[7]:\r\n0    -1.019117\r\n1    -1.455596\r\n2     0.716716\r\n3    -0.052173\r\n4   -10.725603\r\ndtype: float64\r\n\r\nIn [8]: b / a\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-fc1551a96997> in <module>()\r\n----> 1 b / a\r\n\r\n/home/ldeleo/venv/test/lib/python2.7/site-packages/pandas/core/ops.pyc in wrapper(left, right, name)\r\n    488         if isinstance(rvalues, pd.Series):\r\n    489             rindex = getattr(rvalues,'index',rvalues)\r\n--> 490             name = _maybe_match_name(left, rvalues)\r\n    491             lvalues = getattr(lvalues, 'values', lvalues)\r\n    492             rvalues = getattr(rvalues, 'values', rvalues)\r\n\r\n/home/ldeleo/venv/test/lib/python2.7/site-packages/pandas/core/common.pyc in _maybe_match_name(a, b)\r\n   2927     a_name = getattr(a, 'name', None)\r\n   2928     b_name = getattr(b, 'name', None)\r\n-> 2929     if a_name == b_name:\r\n   2930         return a_name\r\n   2931     return None\r\n\r\n/home/ldeleo/venv/test/lib/python2.7/site-packages/pandas/tslib.so in pandas.tslib._Timestamp.__richcmp__ (pandas/tslib.c:12147)()\r\n\r\nTypeError: Cannot compare type 'Timestamp' with type 'int64'\r\n```"""
8057,40504532,JnyJny,jreback,2014-08-18 16:10:32,2014-08-19 20:33:43,2014-08-19 20:33:43,closed,,0.15.0,1,Bug;Community;Docs,https://api.github.com/repos/pydata/pandas/issues/8057,b'broken 0.14.1 download link',"b'On pandas.pydata.org, the download link for 0.14.1 in the Official Release box on the right hand side of the page sends users to the download page for 0.14.0.  Manually changing the URL to 0.14.1 will access the correct page, so the download link is just broke.'"
8056,40503476,seth-p,jreback,2014-08-18 15:59:30,2014-08-28 17:34:49,2014-08-28 17:34:49,closed,,0.15.0,0,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/8056,b'BUG: rolling_count() and expanding_*() return error for empty Series',"b'This probably should have been caught in https://github.com/pydata/pandas/pull/7766, but wasn\'t.\r\n\r\n```\r\nIn [1]: from pandas import Series, rolling_count, expanding_count, expanding_mean\r\n\r\nIn [2]: expanding_count(Series([]))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-16cb9f6736f0> in <module>()\r\n----> 1 expanding_count(Series([]))\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in expanding_count(arg, freq, center)\r\n    872     of :meth:`~pandas.Series.resample` (i.e. using the `mean`).\r\n    873     """"""\r\n--> 874     return rolling_count(arg, len(arg), freq=freq, center=center)\r\n    875\r\n    876\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in rolling_count(arg, window, freq, center, how)\r\n    188     converted = np.isfinite(values).astype(float)\r\n    189     result = rolling_sum(converted, window, min_periods=1,\r\n--> 190                          center=center)  # already converted\r\n    191\r\n    192     # putmask here?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in f(arg, window, min_periods, freq, center, how, **kwargs)\r\n    593             return func(arg, window, minp, **kwds)\r\n    594         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n--> 595                                center=center, how=how, **kwargs)\r\n    596\r\n    597     return f\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _rolling_moment(arg, window, func, minp, axis, freq, center, how, args, kwa\r\nrgs, **kwds)\r\n    345         result = np.apply_along_axis(calc, axis, values)\r\n    346     else:\r\n--> 347         result = calc(values)\r\n    348\r\n    349     rs = return_hook(result)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in <lambda>(x)\r\n    339     arg = _conv_timerule(arg, freq, how)\r\n    340     calc = lambda x: func(x, window, minp=minp, args=args, kwargs=kwargs,\r\n--> 341                           **kwds)\r\n    342     return_hook, values = _process_data_structure(arg)\r\n    343     # actually calculate the moment. Faster way to do this?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in call_cython(arg, window, minp, args, kwargs, **kwds)\r\n    591         def call_cython(arg, window, minp, args=(), kwargs={}, **kwds):\r\n    592             minp = check_minp(minp, window)\r\n--> 593             return func(arg, window, minp, **kwds)\r\n    594         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n    595                                center=center, how=how, **kwargs)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos.roll_sum (pandas\\algos.c:25271)()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos._check_minp (pandas\\algos.c:16394)()\r\n\r\nValueError: min_periods (1) must be <= window (0)\r\n\r\nIn [3]: rolling_count(Series([]), window=3)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-040797e90b09> in <module>()\r\n----> 1 rolling_count(Series([]), window=3)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in rolling_count(arg, window, freq, center, how)\r\n    188     converted = np.isfinite(values).astype(float)\r\n    189     result = rolling_sum(converted, window, min_periods=1,\r\n--> 190                          center=center)  # already converted\r\n    191\r\n    192     # putmask here?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in f(arg, window, min_periods, freq, center, how, **kwargs)\r\n    593             return func(arg, window, minp, **kwds)\r\n    594         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n--> 595                                center=center, how=how, **kwargs)\r\n    596\r\n    597     return f\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _rolling_moment(arg, window, func, minp, axis, freq, center, how, args, kwa\r\nrgs, **kwds)\r\n    345         result = np.apply_along_axis(calc, axis, values)\r\n    346     else:\r\n--> 347         result = calc(values)\r\n    348\r\n    349     rs = return_hook(result)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in <lambda>(x)\r\n    339     arg = _conv_timerule(arg, freq, how)\r\n    340     calc = lambda x: func(x, window, minp=minp, args=args, kwargs=kwargs,\r\n--> 341                           **kwds)\r\n    342     return_hook, values = _process_data_structure(arg)\r\n    343     # actually calculate the moment. Faster way to do this?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in call_cython(arg, window, minp, args, kwargs, **kwds)\r\n    591         def call_cython(arg, window, minp, args=(), kwargs={}, **kwds):\r\n    592             minp = check_minp(minp, window)\r\n--> 593             return func(arg, window, minp, **kwds)\r\n    594         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n    595                                center=center, how=how, **kwargs)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos.roll_sum (pandas\\algos.c:25271)()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos._check_minp (pandas\\algos.c:16394)()\r\n\r\nValueError: min_periods (1) must be <= window (0)\r\n\r\nIn [5]: expanding_mean(Series([]))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-b131e71cc033> in <module>()\r\n----> 1 expanding_mean(Series([]))\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in f(arg, min_periods, freq, center, **kwargs)\r\n    825             return func(arg, window, minp, **kwds)\r\n    826         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n--> 827                                center=center, **kwargs)\r\n    828\r\n    829     return f\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _rolling_moment(arg, window, func, minp, axis, freq, center, how, args, kwa\r\nrgs, **kwds)\r\n    345         result = np.apply_along_axis(calc, axis, values)\r\n    346     else:\r\n--> 347         result = calc(values)\r\n    348\r\n    349     rs = return_hook(result)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in <lambda>(x)\r\n    339     arg = _conv_timerule(arg, freq, how)\r\n    340     calc = lambda x: func(x, window, minp=minp, args=args, kwargs=kwargs,\r\n--> 341                           **kwds)\r\n    342     return_hook, values = _process_data_structure(arg)\r\n    343     # actually calculate the moment. Faster way to do this?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in call_cython(arg, window, minp, args, kwargs, **kwds)\r\n    823         def call_cython(arg, window, minp, args=(), kwargs={}, **kwds):\r\n    824             minp = check_minp(minp, window)\r\n--> 825             return func(arg, window, minp, **kwds)\r\n    826         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n    827                                center=center, **kwargs)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos.roll_mean (pandas\\algos.c:25815)()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos._check_minp (pandas\\algos.c:16394)()\r\n\r\nValueError: min_periods (1) must be <= window (0)\r\n```'"
8049,40444716,jreback,jreback,2014-08-17 22:03:29,2014-09-30 21:35:39,2014-08-18 15:26:26,closed,,0.15.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8049,b'BUG: Bug in DataFrameGroupby.transform when transforming with a passed non-sorted key (GH8046)',b'closes #8046 '
8046,40425369,wesm,jreback,2014-08-17 02:39:32,2014-10-01 12:34:42,2014-08-18 15:26:26,closed,,0.15.0,4,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/8046,b'GroupBy.transform bug/API change',"b""hi folks,\r\n\r\nA *Python for Data Analysis* reader noted the following issue with recent versions of pandas (as of 1 year ago):\r\n\r\n```\r\nimport pandas as pd\r\nfrom pandas import DataFrame\r\nimport numpy as np\r\n\r\ndef demean(arr):\r\n    return arr - arr.mean()\r\n\r\npeople = DataFrame(np.random.randn(5, 5),\r\ncolumns=['a', 'b', 'c', 'd', 'e'],\r\nindex=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\r\nkey = ['one', 'two', 'one', 'two', 'one']\r\n```\r\n\r\non pandas 0.14.1:\r\n\r\n```\r\nIn [14]: people.groupby(key).transform(demean).groupby(key).mean()\r\nOut[14]: \r\n            a         b         c         d         e\r\none -0.228006  0.246737  0.201117  0.250544  0.273858\r\ntwo  0.342009 -0.370106 -0.301676 -0.375816 -0.410788\r\n```\r\n\r\non the other hand:\r\n\r\n```\r\nIn [15]: people.groupby(key).apply(demean).groupby(key).mean()\r\nOut[15]: \r\n                a             b             c             d             e\r\none -3.700743e-17  7.401487e-17 -7.401487e-17  7.401487e-17  0.000000e+00\r\ntwo  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  5.551115e-17\r\n```\r\n\r\nLooks like `transform` has undergone some work in recent times; any ideas? I need to look at the book text and see if I can triage by replacing `transform` with `apply`. At this point `transform` feels a little bit anachronistic. """
8043,40390582,TomAugspurger,TomAugspurger,2014-08-15 22:17:05,2014-08-18 13:14:00,2014-08-18 13:14:00,closed,,0.15.0,1,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8043,b'BUG: stack with datetimes',"b'closes https://github.com/pydata/pandas/issues/8039\r\n\r\nThe only change I made was to select from the MultiIndex itself, rather than the underlying `.values` attribute. The test set passed but I want to look at this a bit more closely to make sure everything is ok.'"
8041,40366198,Wilfred,jreback,2014-08-15 17:16:33,2014-08-18 18:20:52,2014-08-18 15:38:09,closed,,0.15.0,8,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8041,"b""BUG: Don't call np.roll on empty arrays.""","b'On some versions of numpy (such as 1.7, but not 1.8), this throws a\r\nZeroDivisionError.\r\n\r\nFixes #8019.'"
8039,40361024,ldkge,TomAugspurger,2014-08-15 16:12:36,2014-08-19 13:32:17,2014-08-18 13:14:00,closed,,0.15.0,14,Bug;MultiIndex;Usage Question,https://api.github.com/repos/pydata/pandas/issues/8039,b'Stacking MultiIndex DataFrame columns with Timestamps levels fails',"b""You can see the bug in the following code:\r\n\r\n    import pandas as pd\r\n    import datetime as dt\r\n\r\n    key = pd.MultiIndex.from_tuples([(\r\n                                dt.datetime(2014,8,1,0,0,0),\r\n                                'SomeColumnName',\r\n                                'AnotherOne')])\r\n\r\n    data = {\r\n        '1' : 34204,\r\n        '2' : 43580,\r\n        '3' : 84329,\r\n        '5' : 23485\r\n    }\r\n\r\n\r\n    ts = pd.Series(data=data)\r\n    df = pd.DataFrame(data=ts, columns=key)\r\n\r\n    stacked = df.stack()\r\n\r\n    print stacked\r\n\r\n\r\nWe would expect the data to be unchanged, however the returned DataFrame is empty. \r\n\r\nThe Pandas version used was 0.11.0"""
8038,40354001,jorisvandenbossche,jreback,2014-08-15 14:53:27,2015-11-18 20:12:31,2015-11-18 20:12:31,closed,,0.17.1,4,API Design;Bug;Difficulty Novice;Effort Low;Error Reporting,https://api.github.com/repos/pydata/pandas/issues/8038,b'ERR: Index.shift() gives confusing error message when no DatetimeIndex',"b""```\r\nIn [1]: idx = pd.Index(range(5))\r\n\r\nIn [2]: idx\r\nOut[2]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\r\n\r\nIn [3]: idx.shift(1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-d8362983dbf7> in <module>()\r\n----> 1 idx.shift(1)\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\index.pyc in shift(self, period\r\ns, freq)\r\n   1096             return self\r\n   1097\r\n-> 1098         offset = periods * freq\r\n   1099         return Index([idx + offset for idx in self], name=self.name)\r\n   1100\r\n\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\r\n```\r\n\r\nShould `shift` be allowed at all when it is no `DatetimeIndex`?"""
8030,40279796,cpcloud,cpcloud,2014-08-14 17:46:50,2014-08-14 19:41:24,2014-08-14 19:41:22,closed,cpcloud,0.15.0,2,Bug;Data IO;IO HTML;Testing,https://api.github.com/repos/pydata/pandas/issues/8030,b'BUG: fix py3 read_html bytes input',b'closes #7927'
8029,40259809,jreback,jreback,2014-08-14 14:33:58,2014-08-16 00:39:45,2014-08-16 00:39:45,closed,,0.15.0,12,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/8029,b'BUG: fix HDFStore iterator to handle a where properly (GH8014)',b'closes #8014'
8027,40246868,sinhrks,jreback,2014-08-14 11:54:01,2014-10-02 12:18:35,2014-08-15 12:50:09,closed,,0.15.0,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/8027,b'BUG: Area plot legend has incorrect color',"b""Derived from #7636. Area plot sets incorrect ``alpha`` values to legend when ``stacked=True``.\r\n\r\n#### NG: Each areas are drawn with ``alpha=1.0``, but legend has ``alpha=0.5``\r\n![figure_ng](https://cloud.githubusercontent.com/assets/1696302/3919509/643924e0-23a9-11e4-983d-23a16e9c01bc.png)\r\n\r\n#### After fix\r\n```\r\ndf = pd.DataFrame(np.random.rand(20, 5), columns=['A', 'B', 'C', 'D', 'E'])\r\ndf.plot(kind='area')\r\n```\r\n![figure_1](https://cloud.githubusercontent.com/assets/1696302/3919515/8b4ecf4e-23a9-11e4-926b-85101760882a.png)\r\n```\r\n# When alpha is specified\r\ndf.plot(kind='area', alpha=0.2)\r\n```\r\n![figure_2](https://cloud.githubusercontent.com/assets/1696302/3919516/8fc2c166-23a9-11e4-8ea6-b14479d6a72b.png)\r\n\r\n \r\n\r\n\r\n\r\n"""
8026,40246684,jreback,jreback,2014-08-14 11:50:31,2014-08-14 13:08:19,2014-08-14 13:08:19,closed,,0.15.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8026,"b'BUG: related to GH5080, get_indexer choking on boolean type promotion (GH8024)'",b'closes #8024'
8024,40241998,janschulz,jreback,2014-08-14 10:34:40,2014-08-14 19:43:06,2014-08-14 13:08:19,closed,,0.15.0,8,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/8024,b'Regression in series.map?',"b'```\r\nimport pandas\r\nfrom statsmodels import datasets\r\n\r\n\r\n# load the data and clean it a bit\r\naffairs = datasets.fair.load_pandas()\r\ndatas = affairs.exog\r\n# any time greater than 0 is cheating\r\ndatas[\'cheated\'] = affairs.endog > 0\r\n# sort by the marriage quality and give meaningful name\r\n# [rate_marriage, age, yrs_married, children,\r\n# religious, educ, occupation, occupation_husb]\r\ndatas = datas.sort([\'rate_marriage\', \'religious\'])\r\nnum_to_desc = {1: \'awful\', 2: \'bad\', 3: \'intermediate\',\r\n                  4: \'good\', 5: \'wonderful\'}\r\ndatas[\'rate_marriage\'] = datas[\'rate_marriage\'].map(num_to_desc)\r\nnum_to_faith = {1: \'non religious\', 2: \'poorly religious\', 3: \'religious\',\r\n                  4: \'very religious\'}\r\ndatas[\'religious\'] = datas[\'religious\'].map(num_to_faith)\r\nnum_to_cheat = {False: \'faithful\', True: \'cheated\'}\r\ndatas[\'cheated\'] = datas[\'cheated\'].map(num_to_cheat)\r\n```\r\n\r\npart of the following test that fails on pythonxy Ubuntu testing\r\n\r\n======================================================================\r\nERROR: statsmodels.graphics.tests.test_mosaicplot.test_mosaic\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/decorators.py"",\r\nline 146, in skipper_func\r\n    return f(*args, **kwargs)\r\n  File ""/build/buildd/statsmodels-0.6.0~ppa18~revno/debian/python-statsmodels/usr/lib/python2.7/dist-packages/statsmodels/graphics/tests/test_mosaicplot.py"",\r\nline 124, in test_mosaic\r\n    datas[\'cheated\'] = datas[\'cheated\'].map(num_to_cheat)\r\n  File ""/usr/lib/pymodules/python2.7/pandas/core/series.py"", line 1960, in map\r\n    indexer = arg.index.get_indexer(values)\r\n  File ""/usr/lib/pymodules/python2.7/pandas/core/index.py"", line 1460,\r\nin get_indexer\r\n    if not self.is_unique:\r\n  File ""properties.pyx"", line 34, in pandas.lib.cache_readonly.__get__\r\n(pandas/lib.c:38722)\r\n  File ""/usr/lib/pymodules/python2.7/pandas/core/index.py"", line 571,\r\nin is_unique\r\n    return self._engine.is_unique\r\n  File ""index.pyx"", line 205, in\r\npandas.index.IndexEngine.is_unique.__get__ (pandas/index.c:4338)\r\n  File ""index.pyx"", line 234, in\r\npandas.index.IndexEngine._do_unique_check (pandas/index.c:4790)\r\n  File ""index.pyx"", line 247, in\r\npandas.index.IndexEngine._ensure_mapping_populated\r\n(pandas/index.c:4995)\r\n  File ""index.pyx"", line 253, in pandas.index.IndexEngine.initialize\r\n(pandas/index.c:5092)\r\n  File ""hashtable.pyx"", line 731, in\r\npandas.hashtable.PyObjectHashTable.map_locations\r\n(pandas/hashtable.c:12440)\r\nValueError: Does not understand character buffer dtype format string (\'?\')\r\n```\r\n\r\nThis works on \'0.13.1\' but not on \'0.14.1-202-g7d702e9\''"
8019,40170576,Wilfred,jreback,2014-08-13 16:08:29,2014-08-19 13:32:17,2014-08-18 15:38:09,closed,,0.15.0,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/8019,"b""Can't shift an empty dataframe with pandas 0.14.1 and numpy 1.7.1""","b""We have some code which filters a dataframe, then shifts the result. However, this dataframe may end up being empty. In pandas 0.13, this works without problem:\r\n\r\n```\r\nIn [61]: df = DataFrame(columns=['foo'])\r\n\r\nIn [62]: df.shift(-1)\r\nOut[62]: \r\nEmpty DataFrame\r\nColumns: [foo]\r\nIndex: []\r\n\r\n[0 rows x 1 columns]\r\n```\r\n\r\nHowever, pandas 0.14 throws a division by zero error:\r\n\r\n```\r\nIn [1]: from pandas import *\r\n\r\nIn [2]: df = DataFrame(columns=['foo'])\r\n\r\nIn [3]: df.shift(-1)\r\n---------------------------------------------------------------------------\r\nZeroDivisionError                         Traceback (most recent call last)\r\n<ipython-input-3-6aa009807b04> in <module>()\r\n----> 1 df.shift(-1)\r\n\r\n/users/is/whughes/pyenvs/da497516f84bbd5b/lib/python2.7/site-packages/pandas/core/generic.pyc in shift(self, periods, freq, axis, **kwds)\r\n   3288         block_axis = self._get_block_manager_axis(axis)\r\n   3289         if freq is None and not len(kwds):\r\n-> 3290             new_data = self._data.shift(periods=periods, axis=block_axis)\r\n   3291         else:\r\n   3292             return self.tshift(periods, freq, **kwds)\r\n\r\n/users/is/whughes/pyenvs/da497516f84bbd5b/lib/python2.7/site-packages/pandas/core/internals.pyc in shift(self, **kwargs)\r\n   2226 \r\n   2227     def shift(self, **kwargs):\r\n-> 2228         return self.apply('shift', **kwargs)\r\n   2229 \r\n   2230     def fillna(self, **kwargs):\r\n\r\n/users/is/whughes/pyenvs/da497516f84bbd5b/lib/python2.7/site-packages/pandas/core/internals.pyc in apply(self, f, axes, filter, do_integrity_check, **kwargs)\r\n   2190                                                  copy=align_copy)\r\n   2191 \r\n-> 2192             applied = getattr(b, f)(**kwargs)\r\n   2193 \r\n   2194             if isinstance(applied, list):\r\n\r\n/users/is/whughes/pyenvs/da497516f84bbd5b/lib/python2.7/site-packages/pandas/core/internals.pyc in shift(self, periods, axis)\r\n    789             new_values = new_values.T\r\n    790             axis = new_values.ndim - axis - 1\r\n--> 791         new_values = np.roll(new_values, periods, axis=axis)\r\n    792         axis_indexer = [ slice(None) ] * self.ndim\r\n    793         if periods > 0:\r\n\r\n/users/is/whughes/pyenvs/da497516f84bbd5b/lib/python2.7/site-packages/numpy/core/numeric.pyc in roll(a, shift, axis)\r\n   1145         n = a.shape[axis]\r\n   1146         reshape = False\r\n-> 1147     shift %= n\r\n   1148     indexes = concatenate((arange(n-shift,n),arange(n-shift)))\r\n   1149     res = a.take(indexes, axis)\r\n\r\nZeroDivisionError: integer division or modulo by zero\r\n```\r\n\r\nThe behaviour is correcct with numpy 1.8 however."""
8017,40155400,8one6,jreback,2014-08-13 13:44:09,2014-10-29 15:40:22,2014-09-17 12:20:13,closed,,0.15.0,18,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8017,b'BUG: sort_index/sortlevel fails  MultiIndex after columns are added.',"b""I have a `DataFrame` with a `MultiIndex` on the columns.  The first level of the MultiIndex contains `str`ings.  The second, `float`s (though the problem persists if the second level is `int`s).  I add a column to the `DataFrame` (which should *not* come last if the columns are sorted).  I try to sort the `DataFrame`.  The result does not seem to be sorted.  The behavior is fine if the columns are simply an `Index` (even after adding columns).  And the sort works fine in the `MultiIndex` case as long as no columns have been added since the `DataFrame` was created.\r\n\r\nMWE:\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nnp.random.seed(0)\r\ndata = np.random.randn(3,4)\r\n\r\ndf_multi_float = pd.DataFrame(data, index=list('def'), columns=pd.MultiIndex.from_tuples([('red', i) for i in [1., 3., 2., 5.]]))\r\n\r\nprint df_multi_float\r\n\r\n#OUTPUT\r\n        red                              \r\n          1         3         2         5\r\nd  1.764052  0.400157  0.978738  2.240893\r\ne  1.867558 -0.977278  0.950088 -0.151357\r\nf -0.103219  0.410599  0.144044  1.454274\r\n```\r\n\r\nThis sorts just fine as it isnow:\r\n```\r\nprint df_multi_float.sort_index(axis=1)\r\n\r\n#OUTPUT\r\n        red                              \r\n          1         2         3         5\r\nd  1.764052  0.978738  0.400157  2.240893\r\ne  1.867558  0.950088 -0.977278 -0.151357\r\nf -0.103219  0.144044  0.410599  1.454274\r\n```\r\n\r\nBut if I add columns to both this `DataFrame and then show it sorted, I get what looks to be a wrong result (the new column remains last, rather than being placed second-to-last as it should be):\r\n```\r\ndf_multi_float[('red', 4.0)] = 'world'\r\n\r\nprint df_multi_float.sort_index(axis=1)\r\n\r\n#OUTPUT\r\n        red                                  red\r\n          1         2         3         5      4\r\nd  1.764052  0.978738  0.400157  2.240893  world\r\ne  1.867558  0.950088 -0.977278 -0.151357  world\r\nf -0.103219  0.144044  0.410599  1.454274  world\r\n```\r\n\r\nI'm able to produce this behavior on two systems.  The first runs Pandas 0.14.0 and Numpy 1.8.1 and the second runs Pandas 0.14.1 and Numpy 1.8.2.  This issue is described here: http://stackoverflow.com/questions/25287130/pandas-sort-index-fails-with-multiindex-containing-floats-as-one-level-when-col?noredirect=1#comment39408150_25287130"""
8014,40121564,bboerner,jreback,2014-08-13 04:04:40,2014-08-16 00:39:45,2014-08-16 00:39:45,closed,,0.15.0,12,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/8014,b'Iterating through TableIterator with where clause can incorrectly ignore data',"b'Expected behaviour: Using appendable table stored using HDFStore summed length of DataFrames returned using an iterator with a where clause should equal the length of the DataFrame when returned using the same where clause but with iterator=False e.g. TableIterator.get_values().\r\n\r\nThe attached code generates appendable tables of size 100064, 200064, ..., 400064. It uses a where clause which is a superset of all possible values to get DataFrames with iterator=False, with and without the where clause, and with iterator=True, also with and without the where clause. In all cases except for iterator=True with the where clause the length of the returned DataFrames is correct.\r\n\r\nFor the failure cases in closer inspection in iPython it is the last 64 rows which are not being returned.\r\n\r\nNote: in create_file() the appending of DataFrames with lengths of 58689 and 41375 was chosen specifically to reproduce the problem. I originally encountered the problem with a dataset with length 174000064 and the last append was size 41375. I attempted to reproduce the problem by creating various length tables in chunks of 100000 with a final append of 64 and wasn\'t able to do so.\r\n\r\nCreating the table with the last chunk = 41375 with total length exceeding 300000 does in my tests reproduce the problem.\r\n\r\nOutput:\r\n```\r\niteration: 0 PASSED\r\nexpected: 100064, df len: 100064, it (no where clause) len: 100064, it len: 100064\r\niteration: 1 PASSED\r\nexpected: 200064, df len: 200064, it (no where clause) len: 200064, it len: 200064\r\niteration: 2 FAILED\r\nexpected: 300064, df len: 300064, it (no where clause) len: 300064, it len: 300000\r\niteration: 3 FAILED\r\nexpected: 400064, df len: 400064, it (no where clause) len: 400064, it len: 400000\r\n```\r\n\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-32-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.1\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 1.2.1\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2012c\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nsqlalchemy: None\r\nlxml: 3.3.3\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None\r\n\r\n```python\r\n\r\nimport os\r\nfrom dateutil.relativedelta import relativedelta\r\n\r\nimport numpy as np\r\nrandn = np.random.randn\r\nfrom pandas import DataFrame, HDFStore, date_range\r\n\r\ndef create_df(beg_dt, periods=1e5):\r\n    """""" Create a DataFrame containing values v. """"""\r\n\r\n    dr = date_range(beg_dt, periods=periods, freq=\'S\')\r\n    df = DataFrame(index=dr, data=np.random.randn(periods, 4), columns=[\'bid_price\',\'bid_vol\', \'ask_price\',\'ask_vol\'])\r\n    return(df)\r\n\r\ndef create_file(iterations=1):\r\n    beg_dt = \'2014-08-12 13:30:00.000000\'\r\n    periods = 1e5\r\n    for i in xrange(iterations):\r\n        df = create_df(beg_dt, periods)\r\n        store_append(store, df, key=""df"")\r\n        beg_dt = df.index[-1:][0] + relativedelta(seconds=1)\r\n\r\n    df = create_df(beg_dt, 58689)\r\n    store_append(store, df, key=""df"")\r\n    beg_dt = df.index[-1:][0] + relativedelta(seconds=1)\r\n    df = create_df(beg_dt, 41375)\r\n    store_append(store, df, key=""df"")\r\n    beg_dt = df.index[-1:][0] + relativedelta(seconds=1)\r\n\r\n    return(df)\r\n\r\ndef store_open(fname):\r\n    return(HDFStore(fname))\r\n\r\ndef store_get(store, key=""df"", where=None, start=None, stop=None, iterator=False, chunksize=None):\r\n    df = None\r\n    try:\r\n        df = store.select(key, where=where, start=start, stop=stop, iterator=iterator, chunksize=chunksize)\r\n    except (KeyError, TypeError, ):\r\n        pass\r\n\r\n    return(df)\r\n\r\ndef store_append(store, df, key=""df"", where=""""):\r\n    store.append(key, df, format=\'table\')\r\n\r\npath = \'.\'\r\nfname = \'/\'.join([path, \'delme_test.h5\'])\r\n\r\nstore = None\r\nfor n in xrange(0, 4):\r\n    pass\r\n\r\n    if store:\r\n        try: store.close()\r\n        except: pass\r\n    try: os.unlink(fname)\r\n    except: pass\r\n    store = store_open(fname)\r\n\r\n    create_file(n)\r\n    store.close()\r\n\r\n    store = store_open(fname)\r\n    where = None\r\n    df = store_get(store, \'df\', where=where, iterator=False)\r\n\r\n    expected_ln = len(df)\r\n\r\n    beg_dt = \'2014-08-12 13:30:00.000000\'\r\n    end_dt = \'2032-12-31 13:30:00.000000\'\r\n    where = ""index >= \'%s\' & index <= \'%s\'"" % (beg_dt, end_dt)\r\n\r\n    # where clause, iterator=False\r\n    df = store_get(store, \'df\', where=where, iterator=False)\r\n    ln_df = len(df)\r\n\r\n    # no where clause\r\n    it = store_get(store, \'df\', where=None, iterator=True)\r\n    dfs = [df for df in it if not df.empty]\r\n    ln_it_no_where_clause = sum([len(df) for df in dfs])\r\n\r\n    # where clause, iterator=True\r\n    it = store_get(store, \'df\', where=where, iterator=True)\r\n    dfs = [df for df in it if not df.empty]\r\n    ln_it = sum([len(df) for df in dfs])\r\n\r\n    if expected_ln == ln_df and expected_ln == ln_it:\r\n        print(""iteration: %d PASSED"" % n)\r\n    else:\r\n        print(""iteration: %d FAILED"" % n)\r\n    print(""expected: %d, df len: %d, it (no where clause) len: %d, it len: %d"" %\r\n        (expected_ln, ln_df, ln_it_no_where_clause, ln_it))\r\n\r\nstore.close()\r\n```\r\n'"
8013,40117239,grahamjeffries,jreback,2014-08-13 02:04:24,2016-04-04 23:57:50,2015-07-28 21:56:26,closed,,Next Major Release,9,Bug;Difficulty Intermediate;Effort Low;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8013,b'fix issue #8000 - interpolation extrapolates over trailing missing values',b'This pull request is in response to [issue #8000](https://github.com/pydata/pandas/issues/8000).\r\n\r\nChanges to core/common.py add np.nan as the default value for missing values to the left and right non-missing values during interpolation. This prevents DataFrame.interpolate() from extrapolating the last non-missing value over all trailing missing values (the default).\r\n\r\nChanges to tests/test_generic.py add test coverage to the above change. A passing test is where an interpolated series with a trailing missing value maintains that trailing missing value after interpolation.'
8007,40082909,janschulz,jreback,2014-08-12 18:14:31,2014-08-29 18:41:49,2014-08-19 13:58:05,closed,,0.15.0,52,API Design;Bug;Categorical;Missing-data,https://api.github.com/repos/pydata/pandas/issues/8007,b'CLN/DOC/TST: Categorical fixups (GH7768)',"b""replaces #7768, #8006 \r\ncloses #3678 \r\n\r\nCategorical: preserve ints when NaN are present\r\n\r\n  `Categorical([1, np.nan])` would end up with a single `1.` float level.\r\n  This commit ensures that if `values` is a list of ints and contains np.nan,\r\n  the float conversation does not take place.\r\n\r\nCategorical: fix describe with np.nan\r\n\r\nCategorical: ensure that one can assign np.nan\r\n\r\nCategorical: fix assigning NaN if NaN in levels\r\n\r\nAPI: change default Categorical.from_codes() to ordered=False\r\n\r\n  In the normal constructor `ordered=True` is only assumed if the levels\r\n  are given or the values are sortable (which is most of the cases), but\r\n  in `from_codes(...)` we can't asssume this so the default should be\r\n  `False`.\r\n\r\nCategorical: add some links to Categorical in the other docs\r\n\r\nCategorical: use s.values when calling private methods\r\n\r\n  s.values is the underlying Categorical object, s.cat will be changed\r\n  to only expose the API methods/properties.\r\n\r\nCategorical: Change series.cat to only expose the API\r\n\r\nCategorical: Fix order and na_position\r\n\r\nCategorical: Fix comparison of Categoricals and Series|Categorical|np.array\r\n\r\n  Categorical can only be comapred to another Categorical with the same levels\r\n  and the same ordering or to a scalar value.\r\n\r\n  If the Categorical has no order defined (cat.ordered == False), only equal\r\n  (and not equal) are defined.\r\n\r\nCategorical: Tab completition tests\r\n\r\nCategorical: Fix for NA handling/float converting in levels\r\n    \r\nCategorical: make sure fillna gets both missing values and NA-levels\r\n\r\nCategorical: add back labels as deprecated property\r\n    \r\nCategorical: Fix assigment to a Series(Categorical)) and remove Series.cat.codes\r\n    \r\nCategorical: declare most methods in Categorical NON-API"""
8003,40066202,jreback,jreback,2014-08-12 15:22:40,2014-08-14 17:13:13,2014-08-14 17:13:13,closed,,0.15.0,9,API Design;Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/8003,b'API: consistency in .loc indexing when no values are found in a list-like indexer GH7999)',b'closes #7999'
7999,40055468,ruidc,jreback,2014-08-12 13:39:24,2014-08-14 17:13:13,2014-08-14 17:13:13,closed,,0.15.0,9,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7999,b'Documentation/behaviour error for selecting by label',"b'```\r\ndf = pandas.DataFrame([[\'a\'], [\'b\']], index=[1,2], columns=[\'x\'])\r\ndf.loc[[3]] # does not raise KeyError as documented\r\n```\r\n""ALL of the labels for which you ask, must be in the index or a KeyError will be raised!""\r\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label\r\nand\r\n"".loc is strictly label based, will raise KeyError when the items are not found"":\r\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing-loc-iloc-and-ix\r\n\r\nalso, possibly separate bug: \r\n```\r\ndf.loc[pandas.Index([\'a\', \'k\', \'b\']), : ]\r\n```\r\nDOES fail, so we have an inconsistency between .loc[values,:] and .loc[values]'"
7988,39987498,MichaelWS,jreback,2014-08-11 18:43:05,2014-08-19 19:33:11,2014-08-19 17:07:30,closed,,0.15.0,5,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7988,b'bug fix for 7987 and add day of week functionality to Holiday',b'This should fix issue 7987 where there is no offset or observance.\r\n\r\ncloses https://github.com/pydata/pandas/issues/7987'
7987,39986582,MichaelWS,jreback,2014-08-11 18:33:13,2014-08-19 17:07:30,2014-08-19 17:07:30,closed,,0.15.0,17,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7987,b'Annual Holiday with no offset or observance',"b"" if observance is None and offset is None, Holiday breaks when Timestamp is added to None.  This was a bug in the 0.14.1 before a recent PR.  I will issue a pr for this shortly.  \r\n\r\nExample:\r\n\r\n        july_3rd = Holiday('July 4th Eve', month=7,  day=3)\r\nJuly 3rd is a nyse exchange early close that is observed only  on July 3rd."""
7982,39916125,shoyer,jreback,2014-08-10 20:38:49,2014-09-09 05:46:21,2014-08-19 17:51:26,closed,,0.15.0,5,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7982,b'Fix DataFrame.to_latex() midrule positioning with MultiIndex columns',"b""Currently, the positioning of \\midrule is determined by the number of index\r\nlevels, not columns levels:\r\n\r\n\t>>> print pd.DataFrame({('x', 'y'): ['a']}).to_latex()\r\n\t\\begin{tabular}{ll}\r\n\t\\toprule\r\n\t{} &  x \\\\\r\n\t\\midrule\r\n\t{} &  y \\\\\r\n\t0 &  a \\\\\r\n\t\\bottomrule\r\n\t\\end{tabular}\r\n\r\nThe fix is simple: use the number of column levels instead of the number\r\nof index levels."""
7974,39898416,dsm054,jreback,2014-08-10 01:13:29,2014-08-10 03:47:49,2014-08-10 03:47:08,closed,,0.15.0,3,API Design;Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7974,b'BUG: Allow __name__less callables as groupby hows (GH7929)',"b'Allow `functools.partial` objects (and similar callables without `__name__` attributes) to be used as `groupby` functions, which also adds support along the tree (e.g. `resample`, which is specifically tested.)\r\n\r\nCloses #7929.\r\n\r\nThis does not address the potential enhancements allowing automatic naming of duplicate-named functions (relatively minor) and the use of a Series to specify names (more useful).'"
7973,39897509,cpcloud,cpcloud,2014-08-10 00:03:55,2014-08-10 00:27:51,2014-08-10 00:27:49,closed,,0.15.0,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7973,b'BUG: fix transform with integers',b'closes #7972'
7972,39896990,dsm054,cpcloud,2014-08-09 23:23:22,2014-08-10 00:28:04,2014-08-10 00:27:49,closed,,0.15.0,5,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7972,b'BUG: odd transform behaviour with integers',"b'After grouping on an integer column, we seem to forget that we have groups:\r\n\r\n```\r\n>>> pd.__version__\r\n\'0.14.1-172-gab64d58\'\r\n>>> x = np.arange(6, dtype=np.int64)\r\n>>> df = pd.DataFrame({""a"": x//2, ""b"": 2.0*x, ""c"": 3.0*x})\r\n>>> df\r\n   a   b   c\r\n0  0   0   0\r\n1  0   2   3\r\n2  1   4   6\r\n3  1   6   9\r\n4  2   8  12\r\n5  2  10  15\r\n>>> df.groupby(""a"").transform(""mean"")\r\n    b     c\r\n0   1   1.5\r\n1   5   7.5\r\n2   9  13.5\r\n3 NaN   NaN\r\n4 NaN   NaN\r\n5 NaN   NaN\r\n>>> df[""a""] = df[""a""]*1.0\r\n>>> df.groupby(""a"").transform(""mean"")\r\n   b     c\r\n0  1   1.5\r\n1  1   1.5\r\n2  5   7.5\r\n3  5   7.5\r\n4  9  13.5\r\n5  9  13.5\r\n```\r\nTo make it even more obvious:\r\n```\r\n>>> df.index = range(20, 26)\r\n>>> df.groupby(""a"").transform(""mean"")\r\n     b     c\r\n0    1   1.5\r\n1    5   7.5\r\n2    9  13.5\r\n20 NaN   NaN\r\n21 NaN   NaN\r\n22 NaN   NaN\r\n23 NaN   NaN\r\n24 NaN   NaN\r\n25 NaN   NaN\r\n```\r\n\r\nSwitching to a float index seems to avoid the issues as well.'"
7966,39842818,sinhrks,jreback,2014-08-08 16:58:44,2014-08-10 15:20:26,2014-08-10 14:53:57,closed,,0.15.0,1,Bug;Period;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7966,b'ENH/BUG: Period and PeriodIndex ops supports timedelta-like',"b""Must be revisited after #7954 to remove ``_skip_if_not_numpy17_friendly``\r\n\r\nCloses #7740.\r\n\r\nAllow ``Period`` and ``PeriodIndex`` ``add`` and ``sub`` to support ``timedelta``-like. If period freq is ``offsets.Tick``, offsets can be added if the result can have same freq. Otherwise, ``ValueError`` will be raised.\r\n```\r\nidx = pd.period_range('2014-07-01 09:00', periods=5, freq='H')\r\nidx + pd.offsets.Hour(2)\r\n# <class 'pandas.tseries.period.PeriodIndex'>\r\n# [2014-07-01 11:00, ..., 2014-07-01 15:00]\r\n# Length: 5, Freq: H\r\n\r\nidx + datetime.timedelta(minutes=120)\r\n# <class 'pandas.tseries.period.PeriodIndex'>\r\n# [2014-07-01 11:00, ..., 2014-07-01 15:00]\r\n# Length: 5, Freq: H\r\n\r\nidx + np.timedelta64(7200, 's')\r\n# <class 'pandas.tseries.period.PeriodIndex'>\r\n# [2014-07-01 11:00, ..., 2014-07-01 15:00]\r\n# Length: 5, Freq: H\r\n\r\nidx + pd.offsets.Minute(5)\r\n# ValueError: Input has different freq from PeriodIndex(freq=H)\r\n```\r\n\r\n If period freq isn't ``Tick``, only the same offset can be added. Otherwise, ``ValueError`` will be raised.\r\n\r\n```\r\nidx = pd.period_range('2014-07', periods=5, freq='M')\r\nidx + pd.offsets.MonthEnd(3)\r\n# <class 'pandas.tseries.period.PeriodIndex'>\r\n# [2014-10, ..., 2015-02]\r\n# Length: 5, Freq: M\r\nidx + pd.offsets.MonthBegin(3)\r\n# ValueError: Input has different freq from PeriodIndex(freq=M)\r\n```"""
7965,39822085,ojdo,jreback,2014-08-08 13:33:25,2014-09-17 19:28:01,2014-09-17 19:28:01,closed,,0.15.0,13,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7965,b'BUG: at with non-unique multi-index buggy',"b""I fear that I somehow created a DataFrame with a numeric MultiIndex that triggers (un?)intended behaviour in the `.loc` function. I failed to create a reproducible example with anything but a pickled dump of a DataFrame.\r\n\r\n**Question** Is this a bug, or a user error? I'm confused...\r\n\r\n### Steps to reproduce\r\n\r\n1. Get `http://ojdo.de/tmp/df.pickle` (17 kB)\r\n2. Execute \r\n```python\r\nimport pandas as pd\r\ndf = pd.read_pickle('df.pickle')\r\ndf.loc[(1,199), 'Elec']\r\n```\r\n\r\n### Resulting output \r\n\r\n```python\r\nVertex1  Vertex2\r\n1        199        7.602552\r\nName: Elec, dtype: float64\r\n```\r\n### Expected output\r\n\r\nThe value on its own:\r\n``` 7.602552 ```\r\n\r\n### It get's weirder\r\n\r\nThere must be something between row 80 and 90 in this DataFrame, because the following snippet yields a single value, while returing a Series if executed with `.head(90)`.\r\n\r\n```python\r\nIn [20]: df2 = df.head(80)\r\n\r\nIn [21]: df2.loc[(1,199), 'Elec']\r\nOut[21]: 7.6025524199999994\r\n```\r\n\r\nInstalled versions\r\n=============\r\n\r\ncommit: None\r\npython: 2.7.0.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: None\r\nnumpy: 1.8.1\r\nscipy: 0.12.0\r\nstatsmodels: None\r\nIPython: 0.13.2\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 1.5-mpl\r\npytz: 2012d\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.2.1\r\nopenpyxl: 2.0.2\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: 1.0b3\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None"""
7955,39751718,cpcloud,cpcloud,2014-08-07 18:03:20,2014-08-07 19:06:59,2014-08-07 19:00:31,closed,cpcloud,0.15.0,6,Bug;Compat;Data IO,https://api.github.com/repos/pydata/pandas/issues/7955,b'Index cannot be written with scipy.io.savemat',"b""because i live dangerously and run master pretty much all the time i found this in some code of mine that saves indices to matlab files, the solution is to call `.values`, but it would be nice if we could not break this.\r\n\r\nHere's a simple test:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom scipy.io import savemat\r\n\r\nidx = pd.Index(np.arange(10))\r\n\r\nsavemat('test.mat', {'idx': idx}, oned_as='column')\r\n```\r\ni'll look into fixing this"""
7949,39643435,maxchang,jreback,2014-08-06 17:10:27,2014-09-04 22:03:08,2014-09-04 22:02:54,closed,,0.15.0,6,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/7949,b'BUG: Fix for to_excel +/- infinity',"b'BUG: Previously, a negative sign was being prepended for positive infinity, not for negative infinity. (GH6812)\r\n\r\nxref #6812 '"
7940,39515382,jreback,jreback,2014-08-05 12:51:27,2014-08-06 15:44:30,2014-08-05 13:18:00,closed,,0.15.0,3,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/7940,b'BUG/API: Consistency in .where() when setting with None for both inplace in a Series (GH7939)',b'closes #7939'
7932,39470901,jreback,jreback,2014-08-04 23:18:26,2014-09-10 18:09:27,2014-09-10 18:09:27,closed,,0.15.0,0,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/7932,b'BUG: periodindex converted to Series incorrectly getting coerce to int64',"b""```\r\nIn [47]: span\r\nOut[47]: \r\n<class 'pandas.tseries.period.PeriodIndex'>\r\n[2012-12-31, ..., 9999-12-31]\r\nLength: 3, Freq: D\r\n\r\nIn [48]: Series(span)\r\nOut[48]: \r\n0      15705\r\n1      16404\r\n2    2932896\r\ndtype: int64\r\n\r\nIn [49]: Series(span.asobject)\r\nOut[49]: \r\n0    2012-12-31\r\n1    2014-11-30\r\n2    9999-12-31\r\ndtype: object\r\n```\r\n\r\nI believe this works correctly if its a setitem in a frame, e.g.\r\n\r\n``df['span'] = span`` but not in the ``Series`` constructor, needs this: https://github.com/pydata/pandas/blob/master/pandas/core/series.py#L2436 called in the ``Series.__init__``"""
7931,39464905,jreback,jreback,2014-08-04 21:50:55,2014-08-04 22:42:19,2014-08-04 22:42:19,closed,,0.15.0,0,Bug;Data IO;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7931,"b""BUG: Bug in to_datetime when format='%Y%m%d and coerce=True are specified (GH7930)""",b'closes #7930 '
7930,39462433,CarstVaartjes,jreback,2014-08-04 21:21:37,2014-11-07 17:40:14,2014-08-04 22:42:19,closed,,0.15.0,5,Bug;Data IO;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7930,b'to_datetime %Y%m%d does not coerce correctly',"b'Hi,\r\n\r\nsee this example:\r\n\r\n    # imagine a dataframe with a from/to \r\n    x_df = DataFrame([[20120101, 20121231], [20130101, 20131231], [20140101, 20141231], [20150101, 99991231]])\r\n    x_df.columns = [\'date_from\', \'date_to\']\r\n    date_def = \'%Y%m%d\'\r\n    # so everything is ok & peachy for the from dates\r\n    x_df[\'date_from_2\'] = pd.to_datetime(x_df[\'date_from\'], format=date_def, coerce=True)\r\n    x_df[\'date_from_2\'].dtype\r\n    list(x_df[\'date_from_2\'])\r\n    # but with out of bound dates it goes horribly wrong\r\n    x_df[\'date_to_2\'] = pd.to_datetime(x_df[\'date_to\'], format=date_def, coerce=True)\r\n    x_df[\'date_to_2\'].dtype \r\n    list(x_df[\'date_to_2\']) # note the lack of NATs and conversion to datetime.datetime instead of np.datetime64\r\n    # now we can do \r\n    x_df[\'date_to_3\'] = [np.datetime64(date_val, unit=\'s\') for date_val in x_df[\'date_to_2\']] # works great but unfortunately pandas chose to aim for nanoseconds as a standard for date detail...\r\n    x_df[\'date_to_3\'] = [pd.Timestamp(date_val) for date_val in x_df[\'date_to_2\']] # which breaks on the 99991231 example\r\n\r\nThis is a bug, but also related to the discussion in #7307. Probably has to do with:\r\n""Note Specifying a format argument will potentially speed up the conversion considerably and on versions later then 0.13.0 explicitly specifying a format string of \xa1\xae%Y%m%d\xa1\xaf takes a faster path still.""'"
7929,39456357,dsm054,jreback,2014-08-04 20:13:40,2014-08-12 19:24:44,2014-08-10 03:47:08,closed,,0.15.0,11,API Design;Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7929,b'BUG: resample unexpectedly branching on type of `how` callable',"b'I was about to answer an SO question giving the standard ""use `lambda` or `functools.partial`"" to create a new function with bound arguments when to my surprise my example didn\'t work with the OP\'s code.  After some experimentation, it turns out to be because we don\'t always get the same return type.  For example:\r\n\r\n```\r\ndf = pd.DataFrame(np.arange(5, dtype=np.int64), \r\n                  index=pd.DatetimeIndex(start=\'2014/01/01\', periods=5, freq=\'d\'))\r\n\r\ndef f(x,a=1):\r\n    print(type(x))\r\n    return int(isinstance(x, pd.DataFrame)) + 1000*a\r\n```\r\n\r\nAfter which:\r\n\r\n```\r\n>>> df.resample(""M"", how=f)\r\n<class \'pandas.core.series.Series\'>\r\n               0\r\n2014-01-31  1000\r\n>>> df.resample(""M"", how=lambda x: f(x,5))\r\n<class \'pandas.core.series.Series\'>\r\n               0\r\n2014-01-31  5000\r\n>>> df.resample(""M"", how=partial(f, a=9))\r\n<class \'pandas.core.frame.DataFrame\'>\r\n               0\r\n2014-01-31  9001\r\n```\r\n\r\n`how` shouldn\'t care about how the function was constructed, only whether it\'s callable, and we should be getting the same result (whether Series or DataFrame) fed into `how` in every case.\r\n'"
7927,39443401,jreback,cpcloud,2014-08-04 17:44:53,2014-08-14 19:41:22,2014-08-14 19:41:22,closed,cpcloud,0.15.0,27,Bug;Data IO;IO HTML;Testing,https://api.github.com/repos/pydata/pandas/issues/7927,b'TST: test_encode in test_html',"b'@cpcloud not sure if this is something I did (or didn\'t do)\r\n\r\nI was testing the index sub-class on 3.4 (may have appeared on travis too)\r\n\r\n\r\n``/mnt/home/jreback/venv/py3.4/index/pandas/io/tests/data/html_encoding/chinese_utf-16.html\'``\r\n\r\n```\r\n======================================================================\r\nERROR: test_encode (pandas.io.tests.test_html.TestReadHtmlEncoding)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/mnt/home/jreback/venv/py3.4/index/pandas/io/tests/test_html.py"", line 627, in test_encode\r\n    from_string = self.read_string(f, encoding).pop()\r\n  File ""/mnt/home/jreback/venv/py3.4/index/pandas/io/tests/test_html.py"", line 622, in read_string\r\n    return self.read_html(fobj.read(), encoding=encoding, index_col=0)\r\n  File ""/mnt/home/jreback/venv/py3.4/index/pandas/io/tests/test_html.py"", line 610, in read_html\r\n    return read_html(*args, **kwargs)\r\n  File ""/mnt/home/jreback/venv/py3.4/index/pandas/io/html.py"", line 843, in read_html\r\n    parse_dates, tupleize_cols, thousands, attrs, encoding)\r\n  File ""/mnt/home/jreback/venv/py3.4/index/pandas/io/html.py"", line 709, in _parse\r\n    raise_with_traceback(retained)\r\n  File ""/mnt/home/jreback/venv/py3.4/index/pandas/compat/__init__.py"", line 705, in raise_with_traceback\r\n    raise exc.with_traceback(traceback)\r\nTypeError: Cannot read object of type \'bytes\'\r\n\r\n----------------------------------------------------------------------\r\nRan 64 tests in 61.014s\r\n\r\nFAILED (errors=1)\r\n(py3.4)jreback@sheep:~/venv/py3.4/index$ nosetests pandas//io/tests/test_html.py  --pdb --pdb-failure^C\r\n(py3.4)jreback@sheep:~/venv/py3.4/index$ python ci/print_versions.py \r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: d1c4fbb0d170cfaf920a27907c014e8cc45752d1\r\npython: 3.4.0.beta.3\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: None\r\nnose: 1.3.3\r\nCython: 0.20.2\r\nnumpy: 1.8.0\r\nscipy: 0.13.3\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: 0.3.0\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: 0.8.0\r\ntables: 3.1.0\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.0.4\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.6\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.6\r\npymysql: 0.6.1.None\r\npsycopg2: 2.5.2 (dt dec pq3 ext)\r\n```'"
7924,39428821,jreback,jreback,2014-08-04 15:01:57,2014-08-04 20:29:34,2014-08-04 20:29:34,closed,,0.15.0,3,Bug;Categorical;Docs;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/7924,b'BUG/DOC: Categorical fixes (GH7918)',b'Categoricals now raise NotImplementedError when writing to HDFStore with a Fixed type store\r\nSlicing bug with a single-dtyped category and a possible view\r\n\r\ncloses #7918\r\n \r\n'
7923,39428661,jmorris0x0,jreback,2014-08-04 15:00:30,2014-09-08 04:26:23,2014-09-07 14:30:53,closed,,0.15.0,13,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7923,b'BUG: Fix Grouper with multi-level index and frequency (GH7885)',b'close #7885 '
7921,39421237,jreback,jreback,2014-08-04 13:40:12,2014-08-04 15:17:33,2014-08-04 15:17:33,closed,,0.15.0,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7921,b'REGR: Regression in multi-index indexing with a non-scalar type object (GH7914)',b'closes #7914'
7918,39398284,jorisvandenbossche,jreback,2014-08-04 08:04:39,2014-08-04 20:54:19,2014-08-04 20:29:34,closed,,0.15.0,5,Bug;Categorical;Docs,https://api.github.com/repos/pydata/pandas/issues/7918,b'DOC/BUG: errors in the categorical docs',"b'After #7917 still some errors in the categorical docs:\r\n\r\n* Docs say ""Writing data to a HDF store works"", but `df.to_hdf(hdf_file, ""frame"")` gives a ```TypeError: objects of type ``Categorical`` are not supported in this context, sorry``` (http://pandas-docs.github.io/pandas-docs-travis/categorical.html#getting-data-in-out). Also `pd.read_hdf(hdf_file, ""frame"")` gives an error.\r\n\r\n* In the section about indexing, `df.ix[""h"":""j"",0:1]` gives a `AttributeError: \'DataFrame\' object has no attribute \'_is_view\'` (http://pandas-docs.github.io/pandas-docs-travis/categorical.html#getting)'"
7914,39390513,briangerke,jreback,2014-08-04 04:51:34,2014-08-04 18:59:52,2014-08-04 15:17:33,closed,,0.15.0,3,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7914,b'DataFrame with MultiIndex columns implicitly assumes that stored data has an ndim attribute',"b""\r\nI submit that this should be a valid pandas DataFrame, odd though it may appear\r\n\r\n```\r\ndf = DataFrame([[np.mean, np.median],['mean','median']],\r\n               columns=MultiIndex.from_tuples([('functs','mean'),\r\n                                               ('functs','median')]), \r\n               index=['function', 'name'])\r\n```\r\n\r\nIt looks like this:\r\n```\r\nIn [45]: print df\r\n                                functs                                \r\n                                  mean                          median\r\nfunction  <function mean at 0x33fb8c0>  <function median at 0x34fac80>\r\nname                              mean                          median\r\n\r\n```\r\nHowever, it can't always be indexed using the `.ix` or `.loc` indexers.\r\n\r\nThis works as expected:\r\n```\r\nprint df[('functs','mean')]['function']\r\n```\r\n\r\nAs does this:\r\n```\r\nprint df.loc['name'][('functs','mean')]\r\n```\r\n\r\nBut this raises an AttributeError, because `np.mean` does not have an `ndim` attribute, but the indexing code assumes that it does:\r\n```\r\nprint df.loc['function'][('functs','mean')]\r\n```\r\n\r\nThe traceback is as follows:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-43-95d403db0b5a> in <module>()\r\n----> 1 df.loc['function',('functs','mean')]\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n   1140     def __getitem__(self, key):\r\n   1141         if type(key) is tuple:\r\n-> 1142             return self._getitem_tuple(key)\r\n   1143         else:\r\n   1144             return self._getitem_axis(key, axis=0)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    653     def _getitem_tuple(self, tup):\r\n    654         try:\r\n--> 655             return self._getitem_lowerdim(tup)\r\n    656         except IndexingError:\r\n    657             pass\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_lowerdim(self, tup)\r\n    761         # we may have a nested tuples indexer here\r\n    762         if self._is_nested_tuple_indexer(tup):\r\n--> 763             return self._getitem_nested_tuple(tup)\r\n    764 \r\n    765         # we maybe be using a tuple to represent multiple dimensions here\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_nested_tuple(self, tup)\r\n    842             # has the dim of the obj changed?\r\n    843             # GH 7199\r\n--> 844             if obj.ndim < current_ndim:\r\n    845 \r\n    846                 # GH 7516\r\n\r\nAttributeError: 'function' object has no attribute 'ndim'\r\n\r\n```\r\n\r\nThis worked for me until I upgraded to 0.14.1. Either it is a bug or I am wrong that DataFrames are type-agnostic containers. (In the latter case, it seems that some kind of error-handling should be in place to check for disallowed datatypes.)"""
7909,39377327,rockg,jreback,2014-08-03 19:40:39,2014-08-05 17:14:05,2014-08-05 17:14:01,closed,,0.15.0,6,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7909,"b'Remove from start/end dates if tz is not None (#7901, #7835)'","b'Below fixes date_range when input dates are localized.  In that case inferred_freq is not None and so the dates do not have their tzinfo removed.  This causes a fixed offset to be applied when the range is created.  If tzinfo is removed, they will be correctly localized.  \r\n\r\nFixes #7901 \r\nFixes #7835.'"
7907,39369546,sinhrks,jreback,2014-08-03 12:45:12,2014-08-12 22:11:41,2014-08-11 12:17:58,closed,,0.15.0,7,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/7907,b'BUG: Timestamp cannot parse nanosecond from string',"b""Fixes 2 problems related to ``Timestamp`` string parsing:\r\n- ``Timestamp`` parsing results incorrect if input contains offset string (Closes #7833).\r\n\r\n     NOTE: Also modified ``Timestamp.__repr__`` to display fixed timezone info, because this can be either ``pytz`` or ``dateutil``.\r\n\r\n```\r\n# Result after the fix\r\n\r\n# If string contains offset, it will be parsed using fixed timezone offset. Following results in 2013-11-01 05:00:00 in UTC (There is some existing tests checking this behaviour).\r\nrepr(pd.Timestamp('2013-11-01 00:00:00-0500'))\r\n# Timestamp('2013-11-01 00:00:00-0500', tz='pytz.FixedOffset(-300)')\r\n\r\n# If tz is specified simultaneously, it should convert the timezone. \r\nrepr(pd.Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago'))\r\n# Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago')\r\n\r\nrepr(pd.Timestamp('2013-11-01 00:00:00-0500', tz='Asia/Tokyo'))\r\n# Timestamp('2013-11-01 14:00:00+0900', tz='Asia/Tokyo')\r\n```\r\n\r\n- ``Timestamp`` loses ``nanosecond`` info when parsing from ``str``. (Closes #7878)\r\n```\r\n# Result after the fix\r\n\r\nrepr(pd.Timestamp('2001-01-01 00:00:00.000000005'))\r\n# Timestamp('2001-01-01 00:00:00.000000005')\r\n\r\nrepr(pd.Timestamp('2001-01-01 00:00:00.000000005', tz='US/Eastern'))\r\n# Timestamp('2001-01-01 00:00:00.000000005-0500', tz='US/Eastern')\r\n\r\nrepr(pd.Timestamp('2001-01-01 00:00:00.000001234+09:00'))\r\n# Timestamp('2001-01-01 00:00:00.000001234+0900', tz='pytz.FixedOffset(540)')\r\n\r\nrepr(pd.Timestamp('2001-01-01 00:00:00.000001234+09:00', tz='Asia/Tokyo'))\r\n# Timestamp('2001-01-01 00:00:00.000001234+0900', tz='Asia/Tokyo')\r\n\r\n# Because non ISO 8601 format is parsed by dateutil, nanosecond will lost (no change)\r\nrepr(pd.Timestamp('01-01-01 00:00:00.000000001'))\r\n# Timestamp('2001-01-01 00:00:00')\r\n\r\nrepr(pd.Timestamp('01-01-01 00:00:00.000000001+9:00'))\r\n# Timestamp('2001-01-01 00:00:00+0900', tz='tzoffset(None, 32400)')\r\n```\r\nCC @cyber42 @ischwabacher @rockg @adamgreenhall"""
7902,39344208,toobaz,jreback,2014-08-02 07:21:11,2014-08-02 13:06:56,2014-08-02 12:53:36,closed,,0.15.0,3,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7902,b'BUG: do not assume 0 is in index of potential Series',b'closes #7857... this time it checks also for a Series (and it passes all tests).'
7901,39338980,rockg,jreback,2014-08-02 01:10:16,2014-08-05 17:14:00,2014-08-05 17:14:00,closed,,0.15.0,19,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7901,b'date_range does not capture right timezone from input dates',"b""Example is below.  I would expect that if dates have a timezone on them, date_range would then use that timezone to fill in the rest of the period.  However, something goes awry (notice the 01:00 below).  If I have dates with a timezone and pass a timezone (case 2), it still doesn't work.  Only when I remove the timezone from the dates does it work (case 3).  I would expect all these to work the same.\r\n\r\n```\r\nimport pytz\r\ntz = pytz.timezone('US/Eastern')\r\nfrom datetime import datetime\r\nsd = tz.localize(datetime(2014, 3, 6))\r\ned = tz.localize(datetime(2014, 3, 12))\r\nlist(pd.date_range(sd, ed, freq='D'))\r\nOut[41]: \r\n[Timestamp('2014-03-06 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-07 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-08 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-09 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-10 01:00:00-0400', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-11 01:00:00-0400', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-12 01:00:00-0400', tz='US/Eastern', offset='D')]\r\n\r\nlist(pd.date_range(sd, ed, freq='D', tz='US/Eastern'))\r\nOut[42]: \r\n[Timestamp('2014-03-06 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-07 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-08 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-09 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-10 01:00:00-0400', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-11 01:00:00-0400', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-12 01:00:00-0400', tz='US/Eastern', offset='D')]\r\n\r\nlist(pd.date_range(sd.replace(tzinfo=None), ed.replace(tzinfo=None), freq='D', tz='US/Eastern'))\r\nOut[43]: \r\n[Timestamp('2014-03-06 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-07 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-08 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-09 00:00:00-0500', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-10 00:00:00-0400', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-11 00:00:00-0400', tz='US/Eastern', offset='D'),\r\n Timestamp('2014-03-12 00:00:00-0400', tz='US/Eastern', offset='D')]\r\n```\r\n\r\n"""
7900,39333057,seth-p,jreback,2014-08-01 22:29:59,2014-09-15 06:23:09,2014-09-10 00:02:26,closed,,0.15.0,15,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7900,b'BUG: Inconsistencies in calculating second moments of a single value',"b'I noticed (in https://github.com/pydata/pandas/issues/7884) that ``ewmvar``, ``ewmstd``, ``ewmvol``, ``ewmcov``, ``rolling_var``, and ``rolling_std`` return ``0.0`` for a single value (assuming ``min_periods=0``); whereas ``Series.std``, ``Series.var``, ``ewmcorr``, ``expanding_cov``, ``expanding_corr``, ``rolling_cov``, and ``rolling_corr`` all return ``NaN`` for a single value. ``expanding_std`` and ``expanding_var`` produce ``Value Error: min_periods (2) must be <= window (1)``.\r\n\r\nI think all of these should all return ``NaN`` for a single value. At any rate, I would expect greater consistency one way or the other.\r\n\r\nMildly related, when calculating the correlation of a constant series with itself, ``Series.corr()``, ``expanding_corr``, and ``rolling_corr`` return ``NaN``, while ``ewmcorr`` sometimes returns ``NaN``, sometimes ``1`` and sometimes ``-1``, due to numerical accuracy issues. Actually, as shown in a separate comment below, ``rolling_corr`` also produces inconsistent results for a constant subseries following different prior values.\r\n\r\nInconsistencies in calculating second moments of a single point:\r\n```\r\nPython 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:45:13) [MSC v.1600 64 bit (AMD64)]\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: from pandas import Series, ewmvar, ewmstd, ewmvol, ewmcov, rolling_var, rolling_std, ewmcorr, expanding_cov, expanding_corr,\r\n expanding_std, expanding_var, rolling_cov, rolling_corr\r\nIn [2]: s = Series([1.])\r\n\r\nIn [3]: ewmvar(s, halflife=2., min_periods=0)\r\nOut[3]:\r\n0    0\r\ndtype: float64\r\n\r\nIn [4]: ewmstd(s, halflife=2., min_periods=0)\r\nOut[4]:\r\n0    0\r\ndtype: float64\r\n\r\nIn [5]: ewmvol(s, halflife=2., min_periods=0)\r\nOut[5]:\r\n0    0\r\ndtype: float64\r\n\r\nIn [6]: ewmcov(s, s, halflife=2., min_periods=0)\r\nOut[6]:\r\n0    0\r\ndtype: float64\r\n\r\nIn [7]: rolling_var(s, window=2, min_periods=0)\r\nOut[7]:\r\n0    0\r\ndtype: float64\r\n\r\nIn [8]: rolling_std(s, window=2, min_periods=0)\r\nOut[8]:\r\n0    0\r\ndtype: float64\r\n\r\nIn [9]: s.std()\r\nOut[9]: nan\r\n\r\nIn [10]: s.var()\r\nOut[10]: nan\r\n\r\nIn [11]: ewmcorr(s, s, halflife=2., min_periods=0)\r\nOut[11]:\r\n0   NaN\r\ndtype: float64\r\n\r\nIn [12]: expanding_cov(s, s, min_periods=0)\r\nOut[12]:\r\n0   NaN\r\ndtype: float64\r\n\r\nIn [13]: expanding_corr(s, s, min_periods=0)\r\nOut[13]:\r\n0   NaN\r\ndtype: float64\r\n\r\nIn [16]: rolling_cov(s, s, window=3, min_periods=0)\r\nOut[16]:\r\n0   NaN\r\ndtype: float64\r\n\r\nIn [17]: rolling_corr(s, s, window=3, min_periods=0)\r\nOut[17]:\r\n0   NaN\r\ndtype: float64\r\n\r\n\r\nIn [14]: expanding_std(s, min_periods=0)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-14-7320ee579e6a> in <module>()\r\n----> 1 expanding_std(s, min_periods=0)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in f(arg, min_periods, freq, center, **kwargs)\r\n    825             return func(arg, window, minp, **kwds)\r\n    826         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n--> 827                                center=center, **kwargs)\r\n    828\r\n    829     return f\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _rolling_moment(arg, window, func, minp, axis, freq, center, how, args, kwa\r\nrgs, **kwds)\r\n    345         result = np.apply_along_axis(calc, axis, values)\r\n    346     else:\r\n--> 347         result = calc(values)\r\n    348\r\n    349     rs = return_hook(result)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in <lambda>(x)\r\n    339     arg = _conv_timerule(arg, freq, how)\r\n    340     calc = lambda x: func(x, window, minp=minp, args=args, kwargs=kwargs,\r\n--> 341                           **kwds)\r\n    342     return_hook, values = _process_data_structure(arg)\r\n    343     # actually calculate the moment. Faster way to do this?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in call_cython(arg, window, minp, args, kwargs, **kwds)\r\n    823         def call_cython(arg, window, minp, args=(), kwargs={}, **kwds):\r\n    824             minp = check_minp(minp, window)\r\n--> 825             return func(arg, window, minp, **kwds)\r\n    826         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n    827                                center=center, **kwargs)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in <lambda>(*a, **kw)\r\n    604                                how=\'median\')\r\n    605\r\n--> 606 _ts_std = lambda *a, **kw: _zsqrt(algos.roll_var(*a, **kw))\r\n    607 rolling_std = _rolling_func(_ts_std, \'Unbiased moving standard deviation.\',\r\n    608                             check_minp=_require_min_periods(1))\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos.roll_var (pandas\\algos.c:28990)()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos._check_minp (pandas\\algos.c:16394)()\r\n\r\nValueError: min_periods (2) must be <= window (1)\r\n\r\nIn [15]: expanding_var(s, min_periods=0)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-28e2018aee68> in <module>()\r\n----> 1 expanding_var(s, min_periods=0)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in f(arg, min_periods, freq, center, **kwargs)\r\n    825             return func(arg, window, minp, **kwds)\r\n    826         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n--> 827                                center=center, **kwargs)\r\n    828\r\n    829     return f\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _rolling_moment(arg, window, func, minp, axis, freq, center, how, args, kwa\r\nrgs, **kwds)\r\n    345         result = np.apply_along_axis(calc, axis, values)\r\n    346     else:\r\n--> 347         result = calc(values)\r\n    348\r\n    349     rs = return_hook(result)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in <lambda>(x)\r\n    339     arg = _conv_timerule(arg, freq, how)\r\n    340     calc = lambda x: func(x, window, minp=minp, args=args, kwargs=kwargs,\r\n--> 341                           **kwds)\r\n    342     return_hook, values = _process_data_structure(arg)\r\n    343     # actually calculate the moment. Faster way to do this?\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in call_cython(arg, window, minp, args, kwargs, **kwds)\r\n    823         def call_cython(arg, window, minp, args=(), kwargs={}, **kwds):\r\n    824             minp = check_minp(minp, window)\r\n--> 825             return func(arg, window, minp, **kwds)\r\n    826         return _rolling_moment(arg, window, call_cython, min_periods, freq=freq,\r\n    827                                center=center, **kwargs)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos.roll_var (pandas\\algos.c:28990)()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\algos.pyd in pandas.algos._check_minp (pandas\\algos.c:16394)()\r\n\r\nValueError: min_periods (2) must be <= window (1)\r\n\r\n\r\nIn [20]: show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: 0.20.2\r\nnumpy: 1.9.0b1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.3.0\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\nInstability in ``ewmcorr`` of a constant series with itself:\r\n```\r\nIn [1]: from pandas import Series, ewmcorr, expanding_corr, rolling_corr\r\n\r\nIn [2]: s = Series([5., 5., 5.])\r\n\r\nIn [3]: s.corr(s)\r\nOut[3]: nan\r\n\r\nIn [4]: expanding_corr(s, s)\r\nOut[4]:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\ndtype: float64\r\n\r\nIn [5]: rolling_corr(s, s, window=3)\r\nOut[5]:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\ndtype: float64\r\n\r\nIn [6]: ewmcorr(s, s, halflife=3.)\r\nOut[6]:\r\n0   -1\r\n1   -1\r\n2   -1\r\ndtype: float64\r\n\r\nIn [9]: ewmcorr(Series([3., 3., 3.]), halflife=3.)\r\nOut[9]:\r\n0   NaN\r\n1     1\r\n2     1\r\ndtype: float64\r\n```'"
7888,39191690,jreback,jreback,2014-07-31 12:58:58,2014-09-14 01:09:42,2014-09-14 01:09:42,closed,,0.15.0,0,Bug;MultiIndex;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7888,b'BUG: creating multi-index with datetimes in the levels',"b""http://stackoverflow.com/questions/25059140/pandas-multiindex-not-working-with-read-csv-and-datetime-objects/25059472#25059472\r\n\r\n```\r\nIn [3]: date1 = datetime.date.today()\r\n\r\nIn [4]: date2 = datetime.date.today().replace(month=1)\r\n\r\nIn [5]: date_cols=['date1', 'date2']\r\n\r\nIn [6]: index = pd.MultiIndex.from_product([[date1],[date2]])\r\n```\r\n\r\nThe first level of this multi-index is a ``Index`` of object dtyped ``datetimes``, rather than a ``DatetimeIndex``.\r\n\r\nmight be a bug in the creation process"""
7885,39157965,jreback,jreback,2014-07-31 02:50:57,2014-09-07 14:30:53,2014-09-07 14:30:53,closed,,0.15.0,0,Algos;Bug;Difficulty Novice;Groupby,https://api.github.com/repos/pydata/pandas/issues/7885,b'BUG: using pd.Grouper with a multi-level index and specifying level (and freq) buggy',"b""http://stackoverflow.com/questions/25050015/resampling-over-dates-in-both-levels-of-a-multiindex-pandas-dataframe/25050074#25050074\r\n\r\n```\r\nfrom datetime import date\r\nd0 = date.today() - timedelta(days=14)\r\ndates = pd.date_range(d0, date.today())\r\ndate_index = pd.MultiIndex.from_product([dates, dates], names=['cohort_date', 'event_date'])\r\ndf = pd.DataFrame(np.random.randint(0, 100, 225), index=date_index)\r\n```\r\n\r\nWorks\r\n```\r\ndf.reset_index().groupby([pd.Grouper(key='cohort_date',freq='W'),pd.Grouper(key='event_date',freq='W')]).sum()\r\n```\r\n\r\nThis should be equivalent, but raises\r\n```\r\ndf.groupby([pd.Grouper(level='cohort_date',freq='W'),pd.Grouper(level='event_date',freq='W')]).sum()\r\n```"""
7880,39110192,alorenzo175,jreback,2014-07-30 16:24:32,2014-08-03 01:41:58,2014-08-03 01:41:58,closed,,0.15.0,3,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7880,b'Intersection of non-overlapping DatetimeIndexes with tz set fails',"b""When taking the intersection of two `DatetimeIndex`s that have timezone information and don't overlap, I get an`IndexError`. I don't have any problems if they don't have any timezone information or they do overlap.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\na = pd.date_range('2014-07-07 12', freq='3min', periods=20,)\r\nb = pd.date_range('2014-07-07 13', freq='3min', periods=20,)\r\nprint a.intersection(b)\r\n\r\nc = a.tz_localize('MST')\r\nd = b.tz_localize('MST')\r\nprint c.intersection(d)\r\n```\r\n```python\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\nLength: 0, Freq: None, Timezone: None\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-17-b073ce8aa1f2> in <module>()\r\n      7 c = a.tz_localize('MST')\r\n      8 d = b.tz_localize('MST')\r\n----> 9 print c.intersection(d)\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/pandas/tseries/index.pyc in intersection(self, other)\r\n   1131             if isinstance(result, DatetimeIndex):\r\n   1132                 if result.freq is None:\r\n-> 1133                     result.offset = to_offset(result.inferred_freq)\r\n   1134             return result\r\n   1135 \r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/pandas/lib.so in pandas.lib.cache_readonly.__get__ (pandas/lib.c:36380)()\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/pandas/tseries/index.pyc in inferred_freq(self)\r\n   1415     def inferred_freq(self):\r\n   1416         try:\r\n-> 1417             return infer_freq(self)\r\n   1418         except ValueError:\r\n   1419             return None\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/pandas/tseries/frequencies.pyc in infer_freq(index, warn)\r\n    657 \r\n    658     index = pd.DatetimeIndex(index)\r\n--> 659     inferer = _FrequencyInferer(index, warn=warn)\r\n    660     return inferer.get_freq()\r\n    661 \r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/pandas/tseries/frequencies.pyc in __init__(self, index, warn)\r\n    697 \r\n    698         if index.tz is not None:\r\n--> 699             self.values = _tz_convert_with_transitions(self.values,'UTC',index.tz)\r\n    700 \r\n    701         self.warn = warn\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/pandas/tseries/frequencies.pyc in _tz_convert_with_transitions(values, to_tz, from_tz)\r\n    685             return tslib.tz_convert(values,to_tz,from_tz)\r\n    686 \r\n--> 687     return np.vectorize(f)(values)\r\n    688 \r\n    689 class _FrequencyInferer(object):\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/numpy/lib/function_base.pyc in __call__(self, *args, **kwargs)\r\n   1571             vargs.extend([kwargs[_n] for _n in names])\r\n   1572 \r\n-> 1573         return self._vectorize_call(func=func, args=vargs)\r\n   1574 \r\n   1575     def _get_ufunc_and_otypes(self, func, args):\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/numpy/lib/function_base.pyc in _vectorize_call(self, func, args)\r\n   1631             _res = func()\r\n   1632         else:\r\n-> 1633             ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)\r\n   1634 \r\n   1635             # Convert args to object arrays first\r\n\r\n/home/supas/forecasting_python/python_env_051214/lib/python2.7/site-packages/numpy/lib/function_base.pyc in _get_ufunc_and_otypes(self, func, args)\r\n   1594             # Assumes that ufunc first evaluates the 0th elements in the input\r\n   1595             # arrays (the input values are not checked to ensure this)\r\n-> 1596             inputs = [asarray(_a).flat[0] for _a in args]\r\n   1597             outputs = func(*inputs)\r\n   1598 \r\n\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\n\r\nMy system info is:\r\n```\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.17.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.0.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.2\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```"""
7878,39086908,cyber42,jreback,2014-07-30 12:30:58,2014-08-11 12:17:58,2014-08-11 12:17:58,closed,,0.15.0,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7878,b'Timestamp losing nanosecond accuracy when reading from string',"b""Hi,\r\n\r\nI'm on pandas 0.14.1. Timestamp seems to lose nanoseconds when converting to string and back. Wonder if this is bug.\r\n\r\n```python\r\n\r\nIn [18]: pd.Timestamp('20010101 00:00:00')+pd.tseries.offsets.Nano()\r\nOut[18]: Timestamp('2001-01-01 00:00:00.000000001')\r\n\r\nIn [19]: pd.Timestamp('2001-01-01 00:00:00.000000001')\r\nOut[19]: Timestamp('2001-01-01 00:00:00')\r\n```\r\n\r\nThanks,\r\nMark\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.11.0-26-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.0\r\nCython: 0.19.2\r\nnumpy: 1.8.1\r\nscipy: 0.13.2\r\nstatsmodels: 0.5.0\r\nIPython: 0.13.2\r\nsphinx: None\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.2\r\nbottleneck: 0.8.0\r\ntables: 3.0.0\r\nnumexpr: 2.3\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.8\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n\r\n"""
7871,39014211,cpcloud,cpcloud,2014-07-29 17:43:20,2014-07-30 23:15:11,2014-07-30 23:15:10,closed,cpcloud,0.15.0,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7871,b'BUG/FIX: groupby should raise on multi-valued filter',b'closes #7870'
7870,39005576,phobson,cpcloud,2014-07-29 16:14:26,2014-07-30 23:15:10,2014-07-30 23:15:10,closed,cpcloud,0.15.0,13,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7870,b'groupby filtering is missing some groups',"b""I brought this up on the mailing list. @cpcloud modified my example into very concise sample showing expected and resulting output:\r\n\r\n```\r\nIn [20]: df  = pd.DataFrame([\r\n    ['best', 'a', 'x'],\r\n    ['worst', 'b', 'y'],\r\n    ['best', 'c', 'x'],\r\n    ['best','d', 'y'],\r\n    ['worst','d', 'y'],\r\n    ['worst','d', 'y'],\r\n    ['best','d', 'z'],\r\n], columns=['a', 'b', 'c'])\r\n\r\nIn [21]: pd.concat(v[v.a == 'best'] for _, v in df.groupby('c'))\r\nOut[21]:\r\n      a  b  c\r\n0  best  a  x\r\n2  best  c  x\r\n3  best  d  y # <--- missing from the next statement\r\n6  best  d  z\r\n\r\nIn [22]: df.groupby('c').filter(lambda g: g.a == 'best')\r\nOut[22]:\r\n      a  b  c\r\n0  best  a  x\r\n2  best  c  x\r\n6  best  d  z\r\n```"""
7869,39003517,ischwabacher,jreback,2014-07-29 15:54:38,2014-09-13 22:59:30,2014-09-13 22:59:30,closed,,0.16.0,12,Bug;Dtypes;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7869,b'BUG: nanops on empty Series have wrong type',"b""```python\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.Series(dtype='m8[ns]').sum()\r\nOut[2]: 0\r\n\r\nIn [3]: pd.Series(dtype='m8[ns]').mean()\r\nOut[3]: \r\n0   NaT\r\ndtype: timedelta64[ns]\r\n\r\nIn [4]: pd.Series(dtype='m8[ns]').median()\r\nOut[4]: nan\r\n\r\nIn [5]: pd.Series(dtype=float).sum()\r\nOut[5]: 0\r\n\r\nIn [6]: pd.Series([0], dtype=float).sum()\r\nOut[6]: 0.0\r\n```\r\n\r\nIt looks like `_wrap_results` is doing double duty, both converting an output to the right time dtype, *and* wrapping that result in a series (I don't know why it does this), and in some cases results aren't getting wrapped.  But the wrapper ignores other dtypes, which doesn't make sense to me; aside from the unfortunate fact that numpy provides no `_NA_integer` equivalent, I as a user expect the dtype of a result to depend only on the dtype of the inputs, not on their shapes or values, so it would make sense to have a more general wrapper somewhere.\r\n\r\nI'm not sure how I'd implement such a wrapper, because ideally it would be able to handle situations like that the standard deviation of a `Series` of `datetime64`s is a `timedelta64` or such.  Maybe that's a bit ambitious, though, since it's inching towards a general notion of symbolic units; in such a case the user can unwrap, operate and wrap manually.\r\n\r\nIt looks like there are several points in pandas/core/nanops.py where a result is returned without wrapping, and I could submit a narrow PR for just this issue but I want to wait until I have time to really go over `nanops.py` and get all of these."""
7867,38981460,jreback,jreback,2014-07-29 11:55:44,2014-07-31 11:25:56,2014-07-30 23:48:47,closed,,0.15.0,9,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7867,b'BUG: Bug in multi-index slicing with missing indexers (GH7866)',b'closes #7866\r\n\r\n'
7866,38980484,jreback,jreback,2014-07-29 11:39:45,2014-07-31 15:49:45,2014-07-30 23:48:47,closed,,0.15.0,0,API Design;Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7866,b'BUG/API: bug in multi-index slicing with missing indexers',"b""I think the following *could* work and not raise ``KeyError`` because of a missing indexer. This would follow the API of loc which will be effectively a reindex as long as you have at least 1 found value.\r\n\r\n```\r\ns = pd.Series(np.arange(9),index=pd.MultiIndex.from_product([['A','B','C'],['foo','bar','baz']],names=['one','two'])).sortlevel()\r\n\r\ns.loc[['A','D']]\r\n\r\nidx = pd.IndexSlice\r\ns.loc[idx[:,['foo','bah']]]\r\n```"""
7864,38973804,has2k1,jreback,2014-07-29 09:53:55,2014-07-29 22:44:36,2014-07-29 22:44:36,closed,,0.15.0,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/7864,b'pd.concat reordering categorical levels lexically',"b'Look at `dfx`\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame({""id"":[1,2,3,4,5,6], ""raw_grade"":[\'a\', \'b\', \'b\', \'a\', \'a\', \'e\']})\r\n   ...: df[""grade""] = pd.Categorical(df[""raw_grade""])\r\n   ...: df[\'grade\'].cat.reorder_levels([\'e\', \'a\', \'b\'])\r\n   ...: \r\n\r\nIn [3]: df1 = df[0:3]\r\n   ...: df2 = df[3:]\r\n   ...: \r\n\r\nIn [4]: df[\'grade\'].cat.levels\r\nOut[4]: Index([u\'e\', u\'a\', u\'b\'], dtype=\'object\')\r\n\r\nIn [5]: df1[\'grade\'].cat.levels\r\nOut[5]: Index([u\'e\', u\'a\', u\'b\'], dtype=\'object\')\r\n\r\nIn [6]: df2[\'grade\'].cat.levels\r\nOut[6]: Index([u\'e\', u\'a\', u\'b\'], dtype=\'object\')\r\n\r\nIn [7]: dfx = pd.concat([df1, df2])\r\n\r\nIn [8]: dfx[\'grade\'].cat.levels\r\nOut[8]: Index([u\'a\', u\'b\', u\'e\'], dtype=\'object\')\r\n```\r\n\r\nversion: `pandas: 0.14.1-78-g24b309f`\r\n\r\nThis is still the case after either of PR #7768 and PR #7850.\r\n'"
7862,38961101,bashtage,jreback,2014-07-29 06:19:15,2014-08-20 15:32:49,2014-08-01 13:30:09,closed,,0.15.0,17,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/7862,b'BUG: Fixed incorrect string length calculation when writing strings in Stata',"b'\r\nStrings were incorrectly written using 244 character irrespective of the actual\r\nlength of the underlying due to changes in pandas where the underlying NumPy\r\ndatatype of strings is always np.object_, and never np.string_. Closes #7858'"
7859,38900089,toobaz,toobaz,2014-07-28 15:14:58,2014-08-02 13:04:33,2014-08-02 07:23:53,closed,,0.15.0,3,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7859,"b'BUG: Check the first element of ""others.values"" rather than ""others"".'",b'Closes #7857'
7857,38896247,toobaz,jreback,2014-07-28 14:38:00,2014-08-02 12:53:36,2014-08-02 12:53:36,closed,,0.15.0,5,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7857,b'KeyError when using str.cat and index was changed',"b""    df = DataFrame(index=MultiIndex.from_product([[2011, 2012], [1,2,3]],\r\n                                                 names=['year', 'month']))\r\n    \r\n    df = df.reset_index()\r\n    \r\n    str_year = df.year.astype('str')\r\n    str_month = df.month.astype('str')\r\n    str_both = str_year.str.cat(str_month, sep=' ')\r\n\r\n... so far, everything is fine. Now filter the index and retry:\r\n\r\n    df = df[df.month > 1]\r\n    \r\n    str_year = df.year.astype('str')\r\n    str_month = df.month.astype('str')\r\n    str_both = str_year.str.cat(str_month, sep=' ')\r\n\r\n... you will get a KeyError (tested against git, commit 90fa87e7a51141ac318382b979f60a8c043b8505 ):\r\n\r\n    KeyError                                  Traceback (most recent call last)\r\n    <ipython-input-12-9d3f1fbb70fc> in <module>()\r\n         11 str_year = df.year.astype('str')\r\n         12 str_month = df.month.astype('str')\r\n    ---> 13 str_both = str_year.str.cat(str_month, sep=' ')\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/core/strings.py in cat(self, others, sep, na_rep)\r\n        933     @copy(str_cat)\r\n        934     def cat(self, others=None, sep=None, na_rep=None):\r\n    --> 935         result = str_cat(self.series, others=others, sep=sep, na_rep=na_rep)\r\n        936         return self._wrap_result(result)\r\n        937 \r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/core/strings.py in str_cat(arr, others, sep, na_rep)\r\n         41 \r\n         42     if others is not None:\r\n    ---> 43         arrays = _get_array_list(arr, others)\r\n         44 \r\n         45         n = _length_check(arrays)\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/core/strings.py in _get_array_list(arr, others)\r\n         13 \r\n         14 def _get_array_list(arr, others):\r\n    ---> 15     if len(others) and isinstance(others[0], (list, np.ndarray)):\r\n         16         arrays = [arr] + list(others)\r\n         17     else:\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/core/series.py in __getitem__(self, key)\r\n        491     def __getitem__(self, key):\r\n        492         try:\r\n    --> 493             result = self.index.get_value(self, key)\r\n        494 \r\n        495             if not np.isscalar(result):\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/core/index.py in get_value(self, series, key)\r\n       1194 \r\n       1195         try:\r\n    -> 1196             return self._engine.get_value(s, k)\r\n       1197         except KeyError as e1:\r\n       1198             if len(self) > 0 and self.inferred_type in ['integer','boolean']:\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/index.so in pandas.index.IndexEngine.get_value (pandas/index.c:2991)()\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/index.so in pandas.index.IndexEngine.get_value (pandas/index.c:2806)()\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/index.so in pandas.index.IndexEngine.get_loc (pandas/index.c:3532)()\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/hashtable.so in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7033)()\r\n\r\n    /home/pietro/nobackup/repo/pandas/pandas/hashtable.so in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6974)()\r\n\r\n    KeyError: 0"""
7856,38851887,brifordwylie,jreback,2014-07-27 22:09:31,2015-04-28 12:00:23,2015-04-28 12:00:23,closed,,0.16.1,5,Bug;Difficulty Novice;Effort Low;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7856,"b""set_option('max_colwidth', N) not working on groupby output""","b""Hi All,\r\n\r\nMight be pilot error (or the incorrect expectations), but I was expecting the max_colwidth option to work on groupby terminal output. Replication here:\r\n``` \r\n$ python\r\n> import pandas as pd\r\n> df = pd.DataFrame([{'a':'foo','b':'bar','c':'uncomfortably long line with lots of stuff','d':1},  \\\r\n                                   {'a':'foo','b':'bar','c':'stuff', 'd':1}])\r\n> df\r\n     a    b                                           c  d\r\n0  foo  bar  uncomfortably long line with lots of stuff  1\r\n1  foo  bar                                       stuff  1\r\n\r\n> pd.set_option('max_colwidth', 20)\r\n> df\r\n     a    b                    c  d\r\n0  foo  bar  uncomfortably lo...  1\r\n1  foo  bar                stuff  1\r\n\r\n> group_df = df.groupby(['a','b','c']).sum()\r\n> group_df\r\n                                                    d\r\na   b   c                                            \r\nfoo bar stuff                                       1\r\n        uncomfortably long line with lots of stuff  1\r\n>>> \r\n```\r\n\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: None\r\nCython: 0.20.2\r\nnumpy: 1.8.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n>>> \r\n```"""
7853,38828988,behzadnouri,jreback,2014-07-27 00:05:31,2014-09-04 00:24:19,2014-08-21 22:10:45,closed,,0.15.0,20,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7853,b'BUG: left join on index with multiple matches now works (GH5391)',"b""closes #5391\r\n\r\n    >>> ### left join with multiple matches - single index case\r\n\r\n    >>> left = DataFrame([\r\n    ...     ['X', 'Y', 'C', 'a'],\r\n    ...     ['W', 'Y', 'C', 'e'],\r\n    ...     ['V', 'Q', 'A', 'h'],\r\n    ...     ['V', 'R', 'D', 'i'],\r\n    ...     ['X', 'Y', 'D', 'b'],\r\n    ...     ['X', 'Y', 'A', 'c'],\r\n    ...     ['W', 'Q', 'B', 'f'],\r\n    ...     ['W', 'R', 'C', 'g'],\r\n    ...     ['V', 'Y', 'C', 'j'],\r\n    ...     ['X', 'Y', 'B', 'd']],\r\n    ...     columns=['cola', 'colb', 'colc', 'tag'],\r\n    ...     index=[3, 2, 0, 1, 7, 6, 4, 5, 9, 8])\r\n    >>>                                                                    \r\n    ... right = DataFrame([\r\n    ...     ['W', 'R', 'C',  0],\r\n    ...     ['W', 'Q', 'B',  3],\r\n    ...     ['W', 'Q', 'B',  8],\r\n    ...     ['X', 'Y', 'A',  1],\r\n    ...     ['X', 'Y', 'A',  4],\r\n    ...     ['X', 'Y', 'B',  5],\r\n    ...     ['X', 'Y', 'C',  6],\r\n    ...     ['X', 'Y', 'C',  9],\r\n    ...     ['X', 'Q', 'C', -6],\r\n    ...     ['X', 'R', 'C', -9],\r\n    ...     ['V', 'Y', 'C',  7],\r\n    ...     ['V', 'R', 'D',  2],\r\n    ...     ['V', 'R', 'D', -1],\r\n    ...     ['V', 'Q', 'A', -3]],\r\n    ...     columns=['col1', 'col2', 'col3', 'val'])\r\n    >>>                                                                    \r\n    ... right.set_index(['col1', 'col2', 'col3'], inplace=True)\r\n    >>> result = left.join(right, on=['cola', 'colb', 'colc'], how='left')\r\n    >>>                                                                    \r\n    ... expected = DataFrame([\r\n    ...     ['X', 'Y', 'C', 'a',   6],\r\n    ...     ['X', 'Y', 'C', 'a',   9],\r\n    ...     ['W', 'Y', 'C', 'e', nan],\r\n    ...     ['V', 'Q', 'A', 'h',  -3],\r\n    ...     ['V', 'R', 'D', 'i',   2],\r\n    ...     ['V', 'R', 'D', 'i',  -1],\r\n    ...     ['X', 'Y', 'D', 'b', nan],\r\n    ...     ['X', 'Y', 'A', 'c',   1],\r\n    ...     ['X', 'Y', 'A', 'c',   4],\r\n    ...     ['W', 'Q', 'B', 'f',   3],\r\n    ...     ['W', 'Q', 'B', 'f',   8],\r\n    ...     ['W', 'R', 'C', 'g',   0],\r\n    ...     ['V', 'Y', 'C', 'j',   7],\r\n    ...     ['X', 'Y', 'B', 'd',   5]],\r\n    ...     columns=['cola', 'colb', 'colc', 'tag', 'val'],\r\n    ...     index=[3, 3, 2, 0, 1, 1, 7, 6, 6, 4, 4, 5, 9, 8])\r\n    >>>                                                                    \r\n    ... tm.assert_frame_equal(result, expected)\r\n    >>> print(left, right, result, sep='\\n')\r\n\r\n      cola colb colc tag\r\n    3    X    Y    C   a\r\n    2    W    Y    C   e\r\n    0    V    Q    A   h\r\n    1    V    R    D   i\r\n    7    X    Y    D   b\r\n    6    X    Y    A   c\r\n    4    W    Q    B   f\r\n    5    W    R    C   g\r\n    9    V    Y    C   j\r\n    8    X    Y    B   d\r\n\r\n                    val\r\n    col1 col2 col3     \r\n    W    R    C       0\r\n         Q    B       3\r\n              B       8\r\n    X    Y    A       1\r\n              A       4\r\n              B       5\r\n              C       6\r\n              C       9\r\n         Q    C      -6\r\n         R    C      -9\r\n    V    Y    C       7\r\n         R    D       2\r\n              D      -1\r\n         Q    A      -3\r\n\r\n      cola colb colc tag  val\r\n    3    X    Y    C   a    6\r\n    3    X    Y    C   a    9\r\n    2    W    Y    C   e  NaN\r\n    0    V    Q    A   h   -3\r\n    1    V    R    D   i    2\r\n    1    V    R    D   i   -1\r\n    7    X    Y    D   b  NaN\r\n    6    X    Y    A   c    1\r\n    6    X    Y    A   c    4\r\n    4    W    Q    B   f    3\r\n    4    W    Q    B   f    8\r\n    5    W    R    C   g    0\r\n    9    V    Y    C   j    7\r\n    8    X    Y    B   d    5\r\n    \r\n    >>> ### left join with multiple matches - multi index case\r\n\r\n    >>> left = DataFrame([\r\n    ...     ['c', 0],\r\n    ...     ['b', 1],\r\n    ...     ['a', 2],\r\n    ...     ['b', 3]],\r\n    ...     columns=['tag', 'val'],\r\n    ...     index=[2, 0, 1, 3])\r\n    >>>                                                 \r\n    ... right = DataFrame([\r\n    ...     ['a', 'v'],\r\n    ...     ['c', 'w'],\r\n    ...     ['c', 'x'],\r\n    ...     ['d', 'y'],\r\n    ...     ['a', 'z'],\r\n    ...     ['c', 'r'],\r\n    ...     ['e', 'q'],\r\n    ...     ['c', 's']],\r\n    ...     columns=['tag', 'char'])\r\n    >>>                                                 \r\n    ... right.set_index('tag', inplace=True)\r\n    >>> result = left.join(right, on='tag', how='left')\r\n    >>>                                                 \r\n    ... expected = DataFrame([\r\n    ...     ['c', 0, 'w'],\r\n    ...     ['c', 0, 'x'],\r\n    ...     ['c', 0, 'r'],\r\n    ...     ['c', 0, 's'],\r\n    ...     ['b', 1, nan],\r\n    ...     ['a', 2, 'v'],\r\n    ...     ['a', 2, 'z'],\r\n    ...     ['b', 3, nan]],\r\n    ...     columns=['tag', 'val', 'char'],\r\n    ...     index=[2, 2, 2, 2, 0, 1, 1, 3])\r\n    >>>                                                 \r\n    ... tm.assert_frame_equal(result, expected)\r\n    >>> print(left, right, result, sep='\\n')\r\n\r\n      tag  val\r\n    2   c    0\r\n    0   b    1\r\n    1   a    2\r\n    3   b    3\r\n\r\n        char\r\n    tag     \r\n    a      v\r\n    c      w\r\n    c      x\r\n    d      y\r\n    a      z\r\n    c      r\r\n    e      q\r\n    c      s\r\n\r\n      tag  val char\r\n    2   c    0    w\r\n    2   c    0    x\r\n    2   c    0    r\r\n    2   c    0    s\r\n    0   b    1  NaN\r\n    1   a    2    v\r\n    1   a    2    z\r\n    3   b    3  NaN\r\n\r\nThis closes on: https://github.com/pydata/pandas/issues/5391\r\nBy providing all the matches when doing left join on index, both in the case of single index and multi-index. It also preserves the index order of the calling (left) DataFrame (as it used to), though when there are multiple matches the indices repeat and the index loses integrity.\r\n\r\nThe added test cases should be self-explanatory.\r\n\r\nThank you,"""
7851,38818758,cpcloud,cpcloud,2014-07-26 14:47:20,2014-07-28 13:24:39,2014-07-28 13:24:39,closed,cpcloud,0.15.0,9,API Design;Bug;Deprecate;IO HTML,https://api.github.com/repos/pydata/pandas/issues/7851,b'BUG: fix greedy date parsing in read_html',"b'Also, `infer_types` now has no effect *for real* :)\n\ncloses #7762\ncloses #7032'"
7850,38817830,jreback,jreback,2014-07-26 13:53:03,2014-07-31 21:27:35,2014-07-29 22:44:36,closed,,0.15.0,14,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/7850,b'BUG: fix multi-column sort that includes Categoricals / concat (GH7848/GH7864)',b'CLN: refactor _lexsort_indexer to use Categoricals\r\n\r\ncloses #7848 \r\ncloses #7864 '
7848,38814676,has2k1,jreback,2014-07-26 10:03:14,2014-07-29 22:44:36,2014-07-29 22:44:36,closed,,0.15.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/7848,b'Categorical in dataframe is sorted lexically.',"b'code \r\n```python\r\ndf = pd.DataFrame({""id"":[6,5,4,3,2,1], ""raw_grade"":[\'a\', \'b\', \'b\', \'a\', \'a\', \'e\']})\r\ndf[""grade""] = pd.Categorical(df[""raw_grade""])\r\ndf[\'grade\'].cat.reorder_levels([\'b\', \'e\', \'a\'])\r\n\r\n# sorts \'grade\' according to the order of the levels\r\ndf.sort(columns=[\'grade\'])  \r\n```\r\ncorrect output\r\n```\r\n  id   raw_grade  grade\r\n4 5    b          b\r\n3 4    b          b\r\n0 1    e          e\r\n2 6    a          a\r\n1 3    a          a\r\n5 2    a          a\r\n```\r\n---\r\ncode\r\n```python\r\n# sorts \'grade\' lexically\r\ndf.sort(columns=[\'grade\', \'id\'])\r\n```\r\nwrong output\r\n```\r\n  id   raw_grade  grade\r\n4 2    a          a\r\n3 3    a          a\r\n0 6    a          a\r\n2 4    b          b\r\n1 5    b          b\r\n5 1    e          e\r\n```\r\n\r\nWhen there is more than one element in the columns list, the Categoricals columns are sorted lexically.\r\n\r\n`pandas: 0.14.1-78-g24b309f`'"
7844,38779512,sinhrks,jreback,2014-07-25 23:37:18,2014-08-30 21:40:48,2014-07-28 16:00:10,closed,,0.15.0,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7844,b'BUG/VIS: rot and fontsize are not applied to timeseries plots',"b""In some plots, ``rot`` and ``fontsize`` arguments are not applied properly.\r\n\r\n- timeseries line / area plot: ``rot`` is not applied to minor ticklabels, and ``fontsize`` is completely ignored. (Fixed to apply to xticklabels)\r\n- kde plot: ``rot`` and ``fontsize`` are completely ignored. (Fixed to apply to xticklabels)\r\n- scatter and hexbin plots: ``rot`` and ``fontsize`` are completely ignored. (Under confirmation)\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame(np.random.randn(10, 4), index=pd.date_range(start='2014-07-01', freq='M', periods=10))\r\ndf.plot(rot=80, fontsize=15)\r\n```\r\n\r\n### Current Result\r\n![figure_ng](https://cloud.githubusercontent.com/assets/1696302/3709379/7841977e-1454-11e4-8396-fc42e943d54a.png)\r\n\r\n### After Fix\r\n![figure_ok](https://cloud.githubusercontent.com/assets/1696302/3709380/7cc1037a-1454-11e4-9456-a9335907473b.png)\r\n\r\n"""
7842,38776656,jreback,jreback,2014-07-25 22:41:05,2014-07-25 23:30:04,2014-07-25 23:30:04,closed,,0.15.0,0,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/7842,b'BUG: Series.__iter__ not dealing with category type well (GH7839)',b'closes #7839'
7839,38716619,has2k1,jreback,2014-07-25 10:22:30,2014-07-25 23:30:04,2014-07-25 23:30:04,closed,,0.15.0,1,Bug;Categorical,https://api.github.com/repos/pydata/pandas/issues/7839,b'itertuples does not work with categorical column in dataframe',"b'Code Snippet\r\n\r\n```python\r\ndf = pd.DataFrame({""id"":[1,2,3,4,5,6], ""raw_grade"":[\'a\', \'b\', \'b\', \'a\', \'a\', \'e\']})\r\ndf[\'grade\'] = pd.Categorical(df[\'raw_grade\'])\r\nfor t in df.itertuples(index=False):\r\n    print(t)\r\n```\r\n\r\nError\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-45-182b29164b1a> in <module>()\r\n      1 df = pd.DataFrame({""id"":[1,2,3,4,5,6], ""raw_grade"":[\'a\', \'b\', \'b\', \'a\', \'a\', \'e\']})\r\n      2 df[\'grade\'] = pd.Categorical(df[\'raw_grade\'])\r\n----> 3 for t in df.itertuples(index=False):\r\n      4     print(t)\r\n\r\n/home/has2k1/scm/pandas/pandas/core/frame.pyc in itertuples(self, index)\r\n    549         # use integer indexing because of possible duplicate column names\r\n    550         arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\r\n--> 551         return zip(*arrays)\r\n    552 \r\n    553     if compat.PY3:  # pragma: no cover\r\n\r\nTypeError: izip argument #3 must support iteration\r\n```\r\n\r\nThis is on master at commit 24b309f.\r\n\r\nEdit:\r\nVersion string - pandas: 0.14.1-78-g24b309f'"
7836,38665287,cpcloud,cpcloud,2014-07-24 19:25:07,2014-07-24 21:25:42,2014-07-24 21:25:40,closed,cpcloud,0.15.0,0,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/7836,b'TST/BUG: html tests not skipping properly if lxml is not installed',
7835,38665146,ischwabacher,jreback,2014-07-24 19:23:15,2014-09-10 17:46:09,2014-08-05 17:14:01,closed,,0.15.0,0,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7835,"b'BUG: date_range(str, tz=str) and date_range(Timestamp) handle tz discontinuity differently'","b""xref #7825\r\nxref (addtl test cases) #7901 \r\n\r\nI believe `Out[2]` is correct and `Out[3]` is a bug, and that others will agree with me.\r\nI believe `Out[4]` is a bug, `Out[7]` is correct and `Out[10]` is a bug, but that I will have to do some work to convince others to agree with me.\r\n\r\n```python\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: list(pd.date_range('2013-11-1', tz='America/Chicago', periods=7, freq='D'))\r\nOut[2]: \r\n[Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-02 00:00:00-0500', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-03 00:00:00-0500', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-04 00:00:00-0600', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-05 00:00:00-0600', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-06 00:00:00-0600', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-07 00:00:00-0600', tz='America/Chicago', offset='D')]\r\n\r\nIn [3]: list(pd.date_range(pd.Timestamp('2013-11-1', tz='America/Chicago'), periods=7, freq='D'))\r\nOut[3]: \r\n[Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-02 00:00:00-0500', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-03 00:00:00-0500', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-03 23:00:00-0600', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-04 23:00:00-0600', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-05 23:00:00-0600', tz='America/Chicago', offset='D'),\r\n Timestamp('2013-11-06 23:00:00-0600', tz='America/Chicago', offset='D')]\r\n\r\nIn [4]: list(pd.date_range('2013-11-1', tz='America/Chicago', periods=7, freq='24H'))\r\nOut[4]: \r\n[Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago', offset='24H'),\r\n Timestamp('2013-11-02 00:00:00-0500', tz='America/Chicago', offset='24H'),\r\n Timestamp('2013-11-03 00:00:00-0500', tz='America/Chicago', offset='24H'),\r\n Timestamp('2013-11-04 00:00:00-0600', tz='America/Chicago', offset='24H'),\r\n Timestamp('2013-11-05 00:00:00-0600', tz='America/Chicago', offset='24H'),\r\n Timestamp('2013-11-06 00:00:00-0600', tz='America/Chicago', offset='24H'),\r\n Timestamp('2013-11-07 00:00:00-0600', tz='America/Chicago', offset='24H')]\r\n\r\nIn [5]: out = [pd.Timestamp('2013-11-1', tz='America/Chicago')]\r\n\r\nIn [6]: for i in range(6):\r\n   ...:     out.append(out[-1] + pd.offsets.Hour(24))\r\n   ...:      \r\n\r\nIn [7]: out\r\nOut[7]: \r\n[Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago'),\r\n Timestamp('2013-11-02 00:00:00-0500', tz='America/Chicago'),\r\n Timestamp('2013-11-03 00:00:00-0500', tz='America/Chicago'),\r\n Timestamp('2013-11-03 23:00:00-0600', tz='America/Chicago'),\r\n Timestamp('2013-11-04 23:00:00-0600', tz='America/Chicago'),\r\n Timestamp('2013-11-05 23:00:00-0600', tz='America/Chicago'),\r\n Timestamp('2013-11-06 23:00:00-0600', tz='America/Chicago')]\r\n\r\nIn [8]: out = [pd.Timestamp('2013-11-1', tz='America/Chicago')]\r\n\r\nIn [9]: for i in range(6):\r\n   ...:     out.append(out[-1] + pd.offsets.Day(1))\r\n   ...:      \r\n\r\nIn [10]: out\r\nOut[10]: \r\n[Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago'),\r\n Timestamp('2013-11-02 00:00:00-0500', tz='America/Chicago'),\r\n Timestamp('2013-11-03 00:00:00-0500', tz='America/Chicago'),\r\n Timestamp('2013-11-03 23:00:00-0600', tz='America/Chicago'),\r\n Timestamp('2013-11-04 23:00:00-0600', tz='America/Chicago'),\r\n Timestamp('2013-11-05 23:00:00-0600', tz='America/Chicago'),\r\n Timestamp('2013-11-06 23:00:00-0600', tz='America/Chicago')]\r\n```"""
7833,38645935,ischwabacher,jreback,2014-07-24 15:53:19,2014-09-10 17:46:09,2014-08-11 12:17:58,closed,,0.15.0,8,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7833,b'BUG: Constructing Timestamp from local time + offset + time zone yields wrong offset',"b""xref #7825\r\n \r\n```python\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago')\r\nOut[2]: Timestamp('2013-10-31 23:09:00-0551', tz='America/Chicago')\r\n```\r\n\r\nThis bug is inherited from `datetime`, which is using local mean time (the first entry for `America/Chicago` in tzinfo)."""
7826,38574724,maxgrenderjones,jorisvandenbossche,2014-07-23 21:34:03,2014-08-22 08:50:47,2014-08-22 08:50:47,closed,,0.15.0,15,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/7826,b'read_sql chokes on mysql when using labels with queries due to unnecessary quoting',"b'Not sure if this is a pandas bug or an upstream one, but here\'s an example of the bug (pandas-0.14.1, mariadb 10, sqlalchemy-0.9.4)\r\n\r\n```python\r\nengine=create_engine(\'mysql://{username}:{password}@{host}/{database}?charset=utf8\'.format(**db))\r\npandas.io.sql.read_sql(\'SELECT onlinetransactions.id FROM onlinetransactions LIMIT 1\', engine)   #Does what you\'d expect\r\npandas.io.sql.read_sql(\'SELECT onlinetransactions.id as firstid FROM onlinetransactions LIMIT 1\', engine)  #Fails\r\n```\r\n\r\nThe error you get back is:\r\n```python\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.pyc in read_sql(sql, con, index_col, coerce_float, params, parse_dates, columns)\r\n    421             coerce_float=coerce_float, parse_dates=parse_dates)\r\n    422 \r\n--> 423     if pandas_sql.has_table(sql):\r\n    424         pandas_sql.meta.reflect(only=[sql])\r\n    425         return pandas_sql.read_table(\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.pyc in has_table(self, name)\r\n    847 \r\n    848     def has_table(self, name):\r\n--> 849         return self.engine.has_table(name)\r\n    850 \r\n    851     def get_table(self, table_name):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in has_table(self, table_name, schema)\r\n   1757 \r\n   1758         """"""\r\n-> 1759         return self.run_callable(self.dialect.has_table, table_name, schema)\r\n   1760 \r\n   1761     def raw_connection(self):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in run_callable(self, callable_, *args, **kwargs)\r\n   1661         """"""\r\n   1662         with self.contextual_connect() as conn:\r\n-> 1663             return conn.run_callable(callable_, *args, **kwargs)\r\n   1664 \r\n   1665     def execute(self, statement, *multiparams, **params):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in run_callable(self, callable_, *args, **kwargs)\r\n   1188 \r\n   1189         """"""\r\n-> 1190         return callable_(self, *args, **kwargs)\r\n   1191 \r\n   1192     def _run_visitor(self, visitorcallable, element, **kwargs):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\base.pyc in has_table(self, connection, table_name, schema)\r\n   2274         try:\r\n   2275             try:\r\n-> 2276                 rs = connection.execute(st)\r\n   2277                 have = rs.fetchone() is not None\r\n   2278                 rs.close()\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in execute(self, object, *multiparams, **params)\r\n    710         """"""\r\n    711         if isinstance(object, util.string_types[0]):\r\n--> 712             return self._execute_text(object, multiparams, params)\r\n    713         try:\r\n    714             meth = object._execute_on_connection\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in _execute_text(self, statement, multiparams, params)\r\n    859             statement,\r\n    860             parameters,\r\n--> 861             statement, parameters\r\n    862         )\r\n    863         if self._has_events or self.engine._has_events:\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in _execute_context(self, dialect, constructor, statement, parameters, *args)\r\n    945                                 parameters,\r\n    946                                 cursor,\r\n--> 947                                 context)\r\n    948 \r\n    949         if self._has_events or self.engine._has_events:\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)\r\n   1106                                         self.dialect.dbapi.Error,\r\n   1107                                         connection_invalidated=self._is_disconnect),\r\n-> 1108                                     exc_info\r\n   1109                                 )\r\n   1110 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\util\\compat.pyc in raise_from_cause(exception, exc_info)\r\n    183             exc_info = sys.exc_info()\r\n    184         exc_type, exc_value, exc_tb = exc_info\r\n--> 185         reraise(type(exception), exception, tb=exc_tb)\r\n    186 \r\n    187 if py3k:\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\base.pyc in _execute_context(self, dialect, constructor, statement, parameters, *args)\r\n    938                                      statement,\r\n    939                                      parameters,\r\n--> 940                                      context)\r\n    941         except Exception as e:\r\n    942             self._handle_dbapi_exception(\r\n\r\nC:\\Anaconda\\lib\\site-packages\\sqlalchemy\\engine\\default.pyc in do_execute(self, cursor, statement, parameters, context)\r\n    433 \r\n    434     def do_execute(self, cursor, statement, parameters, context=None):\r\n--> 435         cursor.execute(statement, parameters)\r\n    436 \r\n    437     def do_execute_no_params(self, cursor, statement, context=None):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\MySQLdb\\cursors.pyc in execute(self, query, args)\r\n    203             del tb\r\n    204             self.messages.append((exc, value))\r\n--> 205             self.errorhandler(self, exc, value)\r\n    206         self._executed = query\r\n    207         if not self._defer_warnings: self._warning_check()\r\n\r\nC:\\Anaconda\\lib\\site-packages\\MySQLdb\\connections.pyc in defaulterrorhandler(***failed resolving arguments***)\r\n     34     del cursor\r\n     35     del connection\r\n---> 36     raise errorclass, errorvalue\r\n     37 \r\n     38 re_numeric_part = re.compile(r""^(\\d+)"")\r\n\r\nProgrammingError: (ProgrammingError) (1103, ""Incorrect table name \'SELECT onlinetransactions.id as firstid FROM onlinetransactions LIMIT 1\'"") \'DESCRIBE `SELECT onlinetransactions.id as firstid FROM onlinetransactions LIMIT 1`\' ()\r\n```\r\n\r\nSo it never gets as far as running the actual query, because it\'s tried to run a `DESCRIBE` query with ``` quotes which fails. i.e.\r\n\r\n```\r\nMariaDB [transactions]> DESCRIBE `SELECT onlinetransactions.id as firstid FROM onlinetransactions LIMIT 1`;\r\nERROR 1103 (42000): Incorrect table name \'SELECT onlinetransactions.id as firstid FROM onlinetransactions LIMIT 1\'\r\n```\r\nBut\r\n```\r\nMariaDB [transactions]> DESCRIBE SELECT onlinetransactions.id as firstid FROM onlinetransactions LIMIT 1;\r\n+------+-------------+--------------------+-------+---------------+--------------------------------+---------+------+----------+-------------+\r\n| id   | select_type | table              | type  | possible_keys | key                            | key_len | ref  | rows     | Extra       |\r\n+------+-------------+--------------------+-------+---------------+--------------------------------+---------+------+----------+-------------+\r\n|    1 | SIMPLE      | onlinetransactions | index | NULL          | ix_OnlineTransactions_Rpt_Year | 5       | NULL | 11485535 | Using index |\r\n+------+-------------+--------------------+-------+---------------+--------------------------------+---------+------+----------+-------------+\r\n1 row in set (0.00 sec)\r\n```\r\n\r\nOk. So looking at the stacktrace, I reckon this *is* a pandas bug as it seems to be calling `has_table` on my sql query, which doesn\'t seem to make any sense?'"
7823,38542186,jreback,jreback,2014-07-23 15:45:55,2014-07-23 17:01:23,2014-07-23 17:01:23,closed,,0.15.0,0,Bug;Dtypes;Timezones,https://api.github.com/repos/pydata/pandas/issues/7823,b'BUG: Bug in passing a DatetimeIndex with a timezone that was not being retained in Frame construction (GH7822)',b'closes #7822 '
7822,38539653,jreback,jreback,2014-07-23 15:21:19,2014-07-23 17:01:23,2014-07-23 17:01:23,closed,,0.15.0,1,Bug;Dtypes;Timezones,https://api.github.com/repos/pydata/pandas/issues/7822,b'API: preserver tz on created series from Index when possible',"b""I think this could be done w/o breaking anything else.\r\n\r\nxref: #3950 \r\n\r\n```\r\nIn [16]: ts = pd.date_range('1/1/2011', periods=5, freq='10s', tz = 'US/Eastern')\r\n\r\nIn [17]: ts\r\nOut[17]:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2011-01-01 00:00:00-05:00, ..., 2011-01-01 00:00:40-05:00]\r\nLength: 5, Freq: 10S, Timezone: US/Eastern\r\n\r\nIn [18]: pd.DataFrame( {'a' : ts } )\r\n```"""
7818,38422925,bashtage,jreback,2014-07-22 17:24:49,2014-08-20 15:32:51,2014-07-23 13:40:44,closed,,0.15.0,10,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/7818,b'BUG: Fixed failure in StataReader when reading variable labels in 117 ',"b""\r\nStata's implementation does not match the online dta file format description.\r\nThe solution used here is to directly compute the offset rather than reading\r\nit from the dta file.  If Stata fixes their implementation, the original code\r\ncan be restored.\r\ncloses #7816"""
7816,38364949,shafiquejamal,jreback,2014-07-22 02:59:39,2014-07-23 13:40:44,2014-07-23 13:40:44,closed,,0.15.0,14,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/7816,"b""StataReader.variable_labels() does not read variable label correctly for stata datasets saved under Stata 13 using 'save' (but it can read datasets saved using 'saveold')""","b'If I use SataReader to read a Stata dataset saved in Stata 13 using the `save` command, I can get the data but not the variable labels. \r\n\r\nIf, however, I use the `saveold` command in Stata 13, I am able to get the variable labels in Python3 using `StataReader.variable_labels()`.\r\n\r\nCan anyone suggest how to accommodate Stata 13? Thanks,'"
7814,38313328,jreback,jreback,2014-07-21 15:22:57,2014-09-19 15:40:58,2014-09-19 15:40:58,closed,,0.15.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7814,"b'BUG: at allowing fallback type indexing (which is not correct, its a restricted version of loc)'","b""xref #7651\r\n```\r\nIn [19]: s = pd.Series([1,2,3], index=list('abc'))\r\n\r\nIn [20]: s.at['a']\r\nOut[20]: 1\r\n\r\nIn [23]: s.at[0]\r\nOut[23]: 1\r\n```"""
7803,38231202,sinhrks,jreback,2014-07-19 10:50:15,2014-07-19 15:21:58,2014-07-19 14:11:35,closed,,0.15.0,3,Bug;Period;Visualization,https://api.github.com/repos/pydata/pandas/issues/7803,b'BUG: is_superperiod and is_subperiod cannot handle higher freq than S',"b'Closes #7772, Closes #7760.'"
7802,38229801,sinhrks,jreback,2014-07-19 09:10:42,2014-07-19 15:24:03,2014-07-19 13:39:59,closed,,0.15.0,1,Bug;MultiIndex;Period;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7802,b'BUG: reset_index with MultiIndex contains PeriodIndex raises ValueError',"b""Closes #7746, Closes #7793.\r\n\r\nSorry, caused by #7533. The ``ValueError`` is being raised when  ``PeriodIndex`` or ``DatetimeIndex`` with tz 's unique values are less than ``MultiIndex`` length.\r\n"""
7801,38229614,sinhrks,jreback,2014-07-19 08:57:16,2014-07-23 11:08:36,2014-07-22 11:38:59,closed,,0.15.0,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7801,b'BUG: timeseries subplots may display unnecessary minor ticklabels',"b""Related to #7457. The fix was incomplete because it only hides major ticklabels, not minor ticklabels. This causes incorrect result in time-series plot.\r\n\r\n(And another problem is ``rot`` default is not applied to minor ticks. I'll check this separatelly)\r\n\r\n```\r\ndf = pd.DataFrame(np.random.randn(10, 4), index=pd.date_range(start='2014-07-01', freq='M', periods=10))\r\ndf.plot(subplots=True)\r\n```\r\n### Current result:\r\nMinor ticklabels of top 3 axes are not hidden.\r\n![figure_ng](https://cloud.githubusercontent.com/assets/1696302/3634314/36ebc68c-0f22-11e4-9f9c-da803dd27d5e.png)\r\n\r\n### Result after fix:\r\n![figure_ok](https://cloud.githubusercontent.com/assets/1696302/3634865/57a28f1a-0f4e-11e4-847f-fff225740439.png)\r\n\r\n\r\n"""
7793,38195001,jreback,jreback,2014-07-18 17:16:05,2014-07-19 09:10:42,2014-07-18 19:06:27,closed,,0.15.0,1,Bug;Duplicate;MultiIndex;Timezones,https://api.github.com/repos/pydata/pandas/issues/7793,b'BUG: multi-index with one level with a tz buggy',"b""related: #7791, #7792 (and I think this causes both)\r\nthis is a dupe of #7746 \r\n\r\n```\r\nIn [8]: df = DataFrame(np.arange(6).reshape(6,1),index=MultiIndex.from_product([['a','b'],date_range('20130101',periods=3,tz='Europe/Berlin')]))\r\n\r\nIn [9]: df\r\nOut[9]: \r\n                             0\r\na 2013-01-01 00:00:00+01:00  0\r\n  2013-01-02 00:00:00+01:00  1\r\n  2013-01-03 00:00:00+01:00  2\r\nb 2013-01-01 00:00:00+01:00  3\r\n  2013-01-02 00:00:00+01:00  4\r\n  2013-01-03 00:00:00+01:00  5\r\n\r\nIn [10]: df.reset_index()\r\n\r\nValueError: Length of values does not match length of index\r\n```"""
7791,38192414,Poquaruse,jreback,2014-07-18 16:42:38,2015-11-11 03:20:28,2015-11-11 03:20:28,closed,,0.17.1,7,Bug;Difficulty Intermediate;Effort Low;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7791,b'MultiIndex DataFrame to_csv() ignores date_format',"b""Hi all,\r\n\r\nI'm running pandas 0.14.1:\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 8\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: DE\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.1\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n\r\n\r\nI'm experiencing a bug which might be related to https://github.com/pydata/pandas/issues/7622\r\n\r\nHow to reproduce the bug:\r\nAt first, a new DataFrame is created\r\n```\r\ndt_rng = pd.date_range(start='2014-01-01 00:00', periods = 1000, freq='1s')\r\ndf = pd.DataFrame({'a':np.random.randn(1000), 'b': np.random.randn(1000)},index = dt_rng)\r\ndf['b'] = df['b'].round()\r\n```\r\n\r\nThen, this DataFrame is exported as csv with a custom date_format:\r\n```\r\ndf.to_csv(date_format='%Y demo')\r\n```\r\nThis does work: ',a,b\\n2014 demo,-0.5582228932333034,1.0\\n2014 demo,[...]'\r\n\r\nNow, the other scenario. Same DataFrame but a groupby('b') and a resample:\r\n```\r\ndf.groupby(df['b']).resample('1min').to_csv(date_format='%y demo')\r\n```\r\n\r\nHere, the custom date_format is not used: \r\n'b,,a\\n-4.0,2014-01-01 00:12:00,1.571130069273494[...]'\r\n\r\nThanks and best regards"""
7790,38189120,alorenzo175,jreback,2014-07-18 16:00:34,2014-07-22 15:26:00,2014-07-22 15:24:22,closed,,0.15.0,13,Bug;IO HDF5;Timezones,https://api.github.com/repos/pydata/pandas/issues/7790,b'BUG: read_column did not preserve UTC tzinfo',"b'BUG: Fixes #7777, HDFStore.read_column did not preserve timezone information\r\nwhen fetching a DatetimeIndex column with tz=UTC'"
7788,38184738,filmor,jreback,2014-07-18 15:11:18,2014-08-18 18:40:23,2014-07-21 11:39:29,closed,,0.15.0,6,Bug;Error Reporting;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/7788,b'Raise exception on non-unique column index in to_hdf for fixed format.',b'Fixes #7761.'
7786,38178631,jreback,jreback,2014-07-18 14:01:14,2014-07-18 14:32:29,2014-07-18 14:32:28,closed,,0.15.0,0,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7786,b'BUG: Bug in Series 0-division with a float and integer operand dtypes  (GH7785)',b'closes #7785'
7785,38175874,xdliao,jreback,2014-07-18 13:28:24,2014-07-18 14:32:38,2014-07-18 14:32:28,closed,,0.15.0,3,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7785,"b""BUG: Series.float divided by Series.interger (with 0's)""","b'# problem when denominator is interger type and has 0 element\r\n# no such problem with pandas <= 0.13.1\r\nprint pd.__version__\r\nprint np.__version__\r\n\r\ntestdata=pd.DataFrame({\'TL\': (1,0), \'PI\': (-0.01,-0.02)})\r\nprint ""--- direct pd.Series division (wrong) : \\n"", (testdata.PI)/testdata.TL\r\nprint ""--- np.where filtered (still wrong!!): "",np.where(testdata.TL>0.001,testdata.PI/testdata.TL,0 )\r\nprint ""--- direct pd.Series division (OK for float type) : \\n"", testdata.PI/(testdata.TL.astype(float))\r\nprint ""--- values division (correct)"", testdata.PI.values/testdata.TL.values\r\n\r\n-------------- Output ---------\r\n0.14.0\r\n1.7.1\r\n--- direct pd.Series division (wrong) : \r\n0   -inf\r\n1   -inf\r\ndtype: float64\r\n--- np.where filtered (still wrong!!):  [-inf   0.]\r\n--- direct pd.Series division (OK for float type) : \r\n0   -0.010000\r\n1        -inf\r\ndtype: float64\r\n--- values division (correct) [-0.01  -inf]\r\n'"
7782,38167340,sinhrks,jreback,2014-07-18 11:11:57,2014-08-03 21:26:08,2014-08-03 21:26:08,closed,,0.15.0,8,Bug;Internals;Timezones,https://api.github.com/repos/pydata/pandas/issues/7782,b'BUG: tslib.tz_convert_single converts DST incorrectly',"b""Apply ``tz_convert_single`` to the same time representation with different tz should be all ``True``?\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport datetime\r\nimport pytz\r\n\r\nfor i in range(9):\r\n    utc_dt = datetime.datetime(2014, 3, 9, i, 0, tzinfo=pytz.timezone('UTC'))\r\n    tz_dt = datetime.datetime(2014, 3, 9, i, 0, tzinfo=pytz.timezone('US/Eastern'))\r\n\r\n    utc_ts = pd.Timestamp(utc_dt)\r\n    tz_ts = pd.Timestamp(tz_dt)\r\n\r\n    print(utc_ts, tz_ts)\r\n\r\n    print(utc_ts.value == pd.tslib.tz_convert_single(tz_ts.value, pytz.timezone('UTC'),\r\n          pytz.timezone('US/Eastern')))\r\n\r\n    print(tz_ts.value == pd.tslib.tz_convert_single(utc_ts.value, pytz.timezone('US/Eastern'),\r\n          pytz.timezone('UTC')))\r\n```\r\n\r\n## Current Result\r\n```\r\n(Timestamp('2014-03-09 00:00:00+0000', tz='UTC'), Timestamp('2014-03-09 00:00:00-0500', tz='US/Eastern'))\r\nTrue\r\nTrue\r\n(Timestamp('2014-03-09 01:00:00+0000', tz='UTC'), Timestamp('2014-03-09 01:00:00-0500', tz='US/Eastern'))\r\nTrue\r\nTrue\r\n(Timestamp('2014-03-09 02:00:00+0000', tz='UTC'), Timestamp('2014-03-09 02:00:00-0500', tz='US/Eastern'))\r\nTrue\r\nTrue\r\n(Timestamp('2014-03-09 03:00:00+0000', tz='UTC'), Timestamp('2014-03-09 03:00:00-0500', tz='US/Eastern'))\r\nFalse\r\nTrue\r\n(Timestamp('2014-03-09 04:00:00+0000', tz='UTC'), Timestamp('2014-03-09 04:00:00-0500', tz='US/Eastern'))\r\nFalse\r\nTrue\r\n(Timestamp('2014-03-09 05:00:00+0000', tz='UTC'), Timestamp('2014-03-09 05:00:00-0500', tz='US/Eastern'))\r\nFalse\r\nTrue\r\n(Timestamp('2014-03-09 06:00:00+0000', tz='UTC'), Timestamp('2014-03-09 06:00:00-0500', tz='US/Eastern'))\r\nFalse\r\nTrue\r\n(Timestamp('2014-03-09 07:00:00+0000', tz='UTC'), Timestamp('2014-03-09 07:00:00-0500', tz='US/Eastern'))\r\nFalse\r\nTrue\r\n(Timestamp('2014-03-09 08:00:00+0000', tz='UTC'), Timestamp('2014-03-09 08:00:00-0500', tz='US/Eastern'))\r\nFalse\r\nFalse\r\n```"""
7779,38136597,jreback,jreback,2014-07-17 23:34:17,2014-07-18 01:00:30,2014-07-18 01:00:30,closed,,0.15.0,6,Bug;Dtypes;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7779,b'BUG: unwanted conversions of timedelta dtypes when in a mixed datetimelike frame (GH7778)',b'closes #7778\r\n\r\nTST: tests for internals/as_matrix() for all dtypes (including categoricals)'
7778,38131343,stharrold,jreback,2014-07-17 22:05:20,2014-07-18 01:00:41,2014-07-18 01:00:30,closed,,0.15.0,7,Bug;Dtypes;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7778,"b'BUG: df.apply handles np.timedelta64 as timestamp, should be timedelta'","b'I think there may be a bug with the row-wise handling of ```numpy.timedelta64``` data types when using ```DataFrame.apply```. As a check, the problem does not appear when using ```DataFrame.applymap```. The problem may be related to #4532, but I\'m unsure. I\'ve included an example below.\r\n\r\nThis is only a minor problem for my use-case, which is cross-checking timestamps from a counter/timer card. I can easily work around the issue with ```DataFrame.itertuples``` etc.\r\n\r\nThank you for your time and for making such a useful package!\r\n\r\n#### Example\r\n\r\n##### Version\r\n\r\nImport and check versions.\r\n\r\n```\r\n$ date\r\nThu Jul 17 16:28:38 CDT 2014\r\n$ conda update pandas\r\nFetching package metadata: ..\r\n# All requested packages already installed.\r\n# packages in environment at /Users/harrold/anaconda:\r\n#\r\npandas                    0.14.1               np18py27_0  \r\n$ ipython\r\nPython 2.7.8 |Anaconda 2.0.1 (x86_64)| (default, Jul  2 2014, 15:36:00) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.1.0 -- An enhanced Interactive Python.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://binstar.org\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: from __future__ import print_function\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: import pandas as pd\r\n\r\nIn [4]: pd.util.print_versions.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.8.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 11.4.2\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.8\r\napiclient: 1.2\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\n##### Create test data\r\n\r\nUsing subset of original raw data as example.\r\n\r\n```\r\nIn [5]: datetime_start = np.datetime64(u\'2014-05-31T01:23:19.9600345Z\')\r\n\r\nIn [6]: timedeltas_elapsed = [30053400, 40053249, 50053098]\r\n```\r\n\r\nCompute datetimes from elapsed timedeltas, then create differential timedeltas from datetimes. All elements are either type ```numpy.datetime64``` or ```numpy.timedelta64```.\r\n\r\n```\r\nIn [7]: df = pd.DataFrame(dict(datetimes = timedeltas_elapsed))\r\n\r\nIn [8]: df = df.applymap(lambda elt: np.timedelta64(elt, \'us\'))\r\n\r\nIn [9]: df = df.applymap(lambda elt: np.datetime64(datetime_start + elt))\r\n\r\nIn [10]: df[\'differential_timedeltas\'] = df[\'datetimes\'] - df[\'datetimes\'].shift()\r\n\r\nIn [11]: print(df)\r\n                      datetimes  differential_timedeltas\r\n0 2014-05-31 01:23:50.013434500                      NaT\r\n1 2014-05-31 01:24:00.013283500          00:00:09.999849\r\n2 2014-05-31 01:24:10.013132500          00:00:09.999849\r\n```\r\n\r\n##### Expected behavior\r\n\r\nWith element-wise handling using ```DataFrame.applymap```, all elements are correctly identified as datetimes (timestamps) or timedeltas.\r\n\r\n```\r\nIn [12]: print(df.applymap(lambda elt: type(elt)))\r\n                          datetimes     differential_timedeltas\r\n0  <class \'pandas.tslib.Timestamp\'>  <type \'numpy.timedelta64\'>\r\n1  <class \'pandas.tslib.Timestamp\'>  <type \'numpy.timedelta64\'>\r\n2  <class \'pandas.tslib.Timestamp\'>  <type \'numpy.timedelta64\'>\r\n```\r\n\r\n##### Bug\r\n\r\nWith row-wise handling using ```DataFrame.apply```, all elements are type ```pandas.tslib.Timestamp```. I expected \'differential_timedeltas\' to be type ```numpy.timedelta64``` or another type of timedelta, not a type of datetime (timestamp).\r\n\r\n```\r\nIn [13]: # For \'datetimes\':\r\n\r\nIn [14]: print(df.apply(lambda row: type(row[\'datetimes\']), axis=1))\r\n0    <class \'pandas.tslib.Timestamp\'>\r\n1    <class \'pandas.tslib.Timestamp\'>\r\n2    <class \'pandas.tslib.Timestamp\'>\r\ndtype: object\r\n\r\nIn [15]: # For \'differential_timedeltas\':\r\n\r\nIn [16]: print(df.apply(lambda row: type(row[\'differential_timedeltas\']), axis=1))\r\n0      <class \'pandas.tslib.NaTType\'>\r\n1    <class \'pandas.tslib.Timestamp\'>\r\n2    <class \'pandas.tslib.Timestamp\'>\r\ndtype: object\r\n```'"
7777,38131341,alorenzo175,jreback,2014-07-17 22:05:17,2014-09-03 13:57:26,2014-07-22 15:24:22,closed,,0.15.0,7,Bug;IO HDF5;Timezones,https://api.github.com/repos/pydata/pandas/issues/7777,b'BUG: select_column not preserving a UTC timezone',"b'I was having issues with lost tz-info when retrieving a DatetimeIndex from an HDF store using `store.select_column(\'data\', \'index\')`.  I was able to track down the issue to `tseries/index.py` in the `Index._to_embed` method. The issue is \r\n ```python \r\n    def _to_embed(self, keep_tz=False):\r\n        """""" return an array repr of this object, potentially casting to object """"""\r\n        if keep_tz and self.tz is not None and str(self.tz) != \'UTC\':                                                                                                                                                                       \r\n             return self.asobject.values\r\n        return self.values\r\n```\r\nIt looks like it explicitly rejects UTC timezones.  Is there a good reason for this?\r\n\r\nThe below code reproduces the problem for me.\r\n```python\r\nimport pandas as pd\r\n\r\ndrange = pd.date_range(\'2014-07-07 00:00:00\', \'2014-07-07 03:00:00\', freq=\'1h\')\r\ndrange_utc = drange.tz_localize(\'UTC\')\r\ndrange_mst = drange.tz_localize(\'MST\')\r\n\r\nprint drange._to_embed(keep_tz=True)\r\nprint drange_utc._to_embed(keep_tz=True)\r\nprint drange_mst._to_embed(keep_tz=True)\r\n```\r\n\r\n\r\nI\'m using python 2.7.6 with the following packages:\r\n```\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.17.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\n\r\npandas: 0.14.1\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.0.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.2\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```'"
7773,38064459,Nodd,jreback,2014-07-17 09:34:07,2016-04-18 15:51:26,2016-04-18 15:51:26,closed,,0.18.1,7,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/7773,b'BUG: sniffing a csv raises with only a header',"b'I\'m trying to read a bunch of csv files, but some of them have a header line without any data. I expect to get empty data but Pandas raises an exception on these files:\r\n```\r\nTraceback (most recent call last):\r\n  File ""/usr/bin/positionneur_rng"", line 9, in <module>\r\n    load_entry_point(\'pymela==0.1.0\', \'console_scripts\', \'positionneur_rng\')()\r\n  File ""/data/libs/python/pymela_scripts/positionneur_rng.py"", line 203, in main\r\n    = importer_csv_pandas(fichier)\r\n  File ""/data/libs/python/pymela_scripts/positionneur_rng.py"", line 179, in importer_csv_pandas\r\n    engine=\'python\')\r\n  File ""/usr/lib/python3.4/site-packages/pandas/io/parsers.py"", line 443, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File ""/usr/lib/python3.4/site-packages/pandas/io/parsers.py"", line 235, in _read\r\n    return parser.read()\r\n  File ""/usr/lib/python3.4/site-packages/pandas/io/parsers.py"", line 686, in read\r\n    ret = self._engine.read(nrows)\r\n  File ""/usr/lib/python3.4/site-packages/pandas/io/parsers.py"", line 1506, in read\r\n    self.index_names)\r\n  File ""/usr/lib/python3.4/site-packages/pandas/io/parsers.py"", line 2129, in _get_empty_meta\r\n    index = MultiIndex.from_arrays([[]] * len(index_col),\r\nTypeError: object of type \'bool\' has no len()\r\n```\r\nI used this command: ```pd.read_csv(fichier_data, sep=None, index_col=False, engine=\'python\')```\r\nThe file content is a single line:\r\n ```\r\nTEMPS;MODE;STATUS;LATITUDE;LONGITUDE;ALTITUDE;VNORD;VEST;VHAUT;ROLL;PITCH;YAW;DELTA YAW;RATEX;RATEY;RATEZ;ACCX;ACCY;ACCZ;GISEMENT;SITE;CONAZ;CONEL;POSAZ;POSEL;LATCIBLE;LONGICIBLE;ALTICIBLE;DISTANCE;\r\n```\r\n\r\n```\r\n>>> pandas.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.43-1-MANJARO\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\n\r\npandas: 0.14.0\r\nnose: 1.3.3\r\nCython: 0.20.2\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.6\r\npymysql: None\r\npsycopg2: None\r\n```'"
7772,38057574,hawkerpl,jreback,2014-07-17 08:23:20,2014-07-19 17:27:14,2014-07-19 14:11:35,closed,,0.15.0,2,Bug;Period;Visualization,https://api.github.com/repos/pydata/pandas/issues/7772,"b'Problem with plotting frequencies ""L"" and ""S""'","b'IO am opening new issue as i solved what is wrong with https://github.com/pydata/pandas/issues/7760 .\r\nSo, i have two sets of data one with frequency 1 second:\r\nhttp://pastebin.com/HenYJxdV\r\nAnd other with frequency 0.1 second (100 miliseconds):\r\nhttp://pastebin.com/HenYJxdV\r\n\r\nI wanted to plot them on same axe so i did:\r\n```\r\nseriesA=dfA[""more""]\r\nseriesB=dfB[""less""]\r\n\r\nplt.figure()\r\nseriesA.plot()\r\nseriesB.plot()\r\n```\r\n\r\nwhich yielded error seen in linked issue.\r\nBut having simmilar data, from the same source, of the same type it often was plotted corectly.\r\nWhat caused this, and what was the difference since data was from the same source ?\r\nTwo things:\r\n1.First: \r\nline 1515 in tools\\plotting.py\r\n```\r\nreturn Period(x[0], freq).to_timestamp(tz=x.tz) == x[0]\r\n```\r\nThis line caused different behaviour of plotting function for different set of data, but only for datas with milisecond frequency (seriesB.plot())\r\nHow different ? \r\nWhen the x[0] is\r\n```\r\n Timestamp: 2001-01-01 09:12:13.200000\r\n```\r\nthen Period(x[0], freq).to_timestamp(tz=x.tz) is:\r\n```\r\nTimestamp: 2001-01-01 09:12:13.200000002\r\n````\r\nwhich causes the comparison to return False value, but having x[0]:\r\n```\r\n Timestamp: 2001-01-01 09:03:11.500000\r\n```\r\nwe have expression Period(x[0], freq).to_timestamp(tz=x.tz) being set to:\r\n```\r\n Timestamp: 2001-01-01 09:03:11.500000\r\n````\r\n\r\nWhich returns True.\r\nSuprisingly it plotted correctly in the first case, when that line returns false (ex. Timestamp: 2001-01-01 09:12:13.200000002)\r\nBecause call stack goes way back to line 1539 same file:\r\n```\r\nif self._is_ts_plot():\r\n```\r\nand caused self._is_ts_plot() to be False, which cause pandas not to try detect frequency and try to convert it but simply plot it.\r\n\r\n2.Second:\r\nline 92 of tseries\\plotting.py\r\n```\r\ndef _maybe_resample(series, ax, freq, plotf, kwargs):\r\n    ax_freq = _get_ax_freq(ax)\r\n    if ax_freq is not None and freq != ax_freq:\r\n        if frequencies.is_superperiod(freq, ax_freq):  # upsample input\r\n            series = series.copy()\r\n            series.index = series.index.asfreq(ax_freq, how=\'s\')\r\n            freq = ax_freq\r\n        elif _is_sup(freq, ax_freq):  # one is weekly\r\n            how = kwargs.pop(\'how\', \'last\')\r\n            series = series.resample(\'D\', how=how).dropna()\r\n            series = series.resample(ax_freq, how=how).dropna()\r\n            freq = ax_freq\r\n        elif frequencies.is_subperiod(freq, ax_freq) or _is_sub(freq, ax_freq):\r\n            _upsample_others(ax, freq, plotf, kwargs)\r\n            ax_freq = freq\r\n        else:  # pragma: no cover\r\n            raise ValueError(\'Incompatible frequency conversion\')\r\n    return freq, ax_freq, series\r\n```\r\n\r\nthis function for the set of data which does not plot (the one with  Timestamp: 2001-01-01 09:03:11.500000) goes through all the conditions and goes to the last ""else""\r\nwith \r\n```\r\nax_freq = ""S""\r\nfreq = ""L""\r\n```\r\nNone of the functions handles the frequency ""L"". \r\nI presume that it should be handled in\r\n```\r\nfrequencies.is_subperiod(freq, ax_freq)\r\n```\r\nbut as you can see here https://github.com/pydata/pandas/blob/790d6464130fe9448739e48678f466d5452992ca/pandas/tseries/frequencies.py#L904\r\nit is not done.\r\n\r\nAnyway, as i presume fixing problem with sampling may be tricky, i\'d like to suggest an option (if it does not exist) which could allow pandas not to look for periods, or treat it as timeseries, but simply plot it.\r\nSomething like \r\n```\r\nseriesB.plot(please_oh_please_make_this_thing_plot=True)\r\n```\r\nwhich would cause to not treat it as timeseries and forced \r\n```self._is_ts_plot():``` to be false.'"
7770,38036539,onesandzeroes,jreback,2014-07-16 23:51:25,2014-07-21 11:43:11,2014-07-21 11:42:51,closed,,0.15.0,11,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7770,b'BUG: Fix for passing multiple ints as levels in DataFrame.stack() (#7660)',"b""closes #7660\r\nThe specific case that came up in #7660 (and originally in #7653) seems easy enough to fix, so I've covered that in the PR. I feel like this raises a few other potential problems though, e.g.:\r\n\r\n* If the list of level numbers is not in order, is there any sensible way to deal with them? I've sorted the level list in my fix, as that seems like the most straightforward way of making sure that when you do each stack, it's only the level numbers higher than the current that are affected. This might produce undesired results though, so maybe we should just raise a `ValueError` if the level numbers aren't sorted?\r\n* I'm not sure how to extend this to deal with negative level numbers."""
7766,37941967,seth-p,jreback,2014-07-16 01:29:45,2014-09-10 00:12:30,2014-07-23 12:17:01,closed,,0.15.0,22,Bug;Error Reporting;Numeric,https://api.github.com/repos/pydata/pandas/issues/7766,b'BUG: rolling_* functions should not shrink window',b'Closes https://github.com/pydata/pandas/issues/7764.'
7765,37928830,fulmicoton,jreback,2014-07-15 21:42:06,2014-07-16 21:49:37,2014-07-16 11:57:48,closed,,0.15.0,4,Bug;Dtypes;Enhancement;Unicode,https://api.github.com/repos/pydata/pandas/issues/7765,b'Closes #7758 - astype(unicode) returning unicode.',"b""I didn't have to rely on `infer_dtype` as suggested by @jreback in #7758\r\nas I delegated to numpy to do all the dirty job :\r\nJust calls numpy.unicode on all the values.\r\n\r\nPlease have a critical look at this pull request before merging :\r\nas I am unfamiliar with both pandas and cython; and may have \r\nmisunderstood the way pandas is working.\r\n\r\nUnit tests are attached and seem to work alright on `python 2.7.2` and `python 3.3.0`\r\n\r\ncloses #7758 \r\n\r\n"""
7764,37915202,creeson,jreback,2014-07-15 19:04:18,2014-07-23 12:17:17,2014-07-23 12:17:17,closed,,0.15.0,11,Bug;Error Reporting;Numeric,https://api.github.com/repos/pydata/pandas/issues/7764,b'rolling_cov and rolling_corr error out when len(series) < min_periods',"b""I am updating a codebase from an OLD version of pandas (0.7.3) to current version (0.14.1) in python 2.7.\r\n```\r\npandas: 0.14.1\r\nnumpy: 1.8.0\r\n```\r\n\r\nNot sure if this is properly classified as a bug, or if it is an expected code change. However, it seems rolling_cov / rolling_corr are specifying the window as min(window given, max(len(arg1), len(arg2)), and then erring out if arg1 AND arg2 are shorter than min_periods. Before, we would just get NaNs back.\r\n\r\nHere is some test code:\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(0)\r\n\r\ns1 = pd.Series(np.random.randn(4))\r\ns2 = pd.Series(np.random.randn(4))\r\n\r\nprint '\\nrolling_corr:'\r\ntry:\r\n    print pd.rolling_corr(s1, s2, window=10, min_periods=5)\r\nexcept Exception as inst:\r\n    print type(inst)\r\n    print inst\r\nprint '\\nrolling_cov:'\r\ntry:\r\n    print pd.rolling_cov(s1, s2, window=10, min_periods=5)\r\nexcept Exception as inst:\r\n    print type(inst)\r\n    print inst\r\n```\r\n\r\nIn **0.14.1**:\r\n```\r\nrolling_corr:\r\n<type 'exceptions.ValueError'>\r\nmin_periods (5) must be <= window (4)\r\nrolling_cov:\r\n<type 'exceptions.ValueError'>\r\nmin_periods (5) must be <= window (4)\r\n```\r\n\r\nIn **0.7.3**:\r\n```\r\nrolling_corr:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\n\r\nrolling_cov:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\n```\r\n\r\nNote that changing either s1 OR s2 to be 6, the code doesn't error out, and instead outputs:\r\n```\r\nrolling_corr:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\n4   NaN\r\n5   NaN\r\ndtype: float64\r\n\r\nrolling_cov:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\n4   NaN\r\n5   NaN\r\ndtype: float64\r\n```\r\n\r\nMeanwhile, in 0.14.1, other rolling functions continue to behave as before:\r\n```python\r\ns1 = pd.Series(np.random.randn(4))\r\n\r\nprint '\\rolling_std:'\r\nprint pd.rolling_std(s1, window=10, min_periods=5)\r\nprint '\\nrolling_max:'\r\nprint pd.rolling_max(s1, window=10, min_periods=5)\r\n```\r\nOutputs:\r\n```\r\nrolling_std:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\ndtype: float64\r\n\r\nrolling_max:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\ndtype: float64\r\n```\r\n"""
7763,37906528,creeson,jreback,2014-07-15 17:29:04,2014-09-26 23:36:40,2014-09-26 23:36:40,closed,,0.15.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7763,b'BUG: Assigning Panel item using .ix/.loc no longer works',"b""I am updating a codebase from an OLD version of pandas (0.7.3) to current version (0.14.1).\r\n```\r\npandas: 0.14.1\r\nnumpy: 1.8.0\r\n```\r\n\r\nThe following code has stopped working correctly (using .ix[] or .loc[]):\r\n ```python\r\nimport numpy as np\r\nimport pandas as ps\r\n\r\nnp.random.seed(0)\r\n\r\nindex=range(3)\r\ncolumns = list('abc')\r\n\r\npanel = ps.Panel({'A' : ps.DataFrame(np.random.randn(3, 3), index=index, columns=columns),\r\n                  'B' : ps.DataFrame(np.random.randn(3, 3), index=index, columns=columns),\r\n                  'C' : ps.DataFrame(np.random.randn(3, 3), index=index, columns=columns)\r\n                  })\r\n\r\nreplace = ps.DataFrame(np.eye(3,3), index=range(3), columns=columns)\r\n\r\nprint panel['A']\r\n\r\nfor idx in list('ABC'):\r\n    panel.loc[idx,:,:] = replace\r\n    \r\nprint panel['A']\r\n ```\r\nOutput:\r\n```\r\n          a         b         c\r\n0  1.764052  0.400157  0.978738\r\n1  2.240893  1.867558 -0.977278\r\n2  0.950088 -0.151357 -0.103219\r\n    a   b   c\r\n0 NaN NaN NaN\r\n1 NaN NaN NaN\r\n2 NaN NaN NaN\r\n```\r\nHowever, if I simply do this:\r\n```python\r\nfor idx in list('ABC'):\r\n    panel[idx] = replace\r\n\r\nprint panel['A']\r\n```\r\nIt behaves as expected:\r\n```\r\n   a  b  c\r\n0  1  0  0\r\n1  0  1  0\r\n2  0  0  1\r\n```\r\n\r\nAlso, if I assign more than just the item, it works fine:\r\n```python\r\npanel.loc['A', 0, 'a'] = 1.0\r\nprint panel['A']\r\n```\r\n\r\n```\r\n          a         b         c\r\n0  1.000000  0.400157  0.978738\r\n1  2.240893  1.867558 -0.977278\r\n2  0.950088 -0.151357 -0.103219\r\n```\r\n\r\nThanks."""
7762,37904832,haraldschilly,cpcloud,2014-07-15 17:08:10,2014-07-28 13:24:39,2014-07-28 13:24:39,closed,cpcloud,0.15.0,4,Bug;IO HTML,https://api.github.com/repos/pydata/pandas/issues/7762,"b""read_html doesn't work for wikipedia tables""","b'I assumed reading wikipedia html tables should work for `read_html`, but it returned a lot of garbage :frowning: \r\n\r\nExample:\r\n```\r\npd.io.html.read_html(""https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_area"", ""Arizona"")\r\n```\r\n\r\nI\'ve seen several related issues, maybe this is an useful test case.\r\n\r\nVersions:\r\n```\r\npandas: 0.14.0\r\nCython: 0.19.2\r\nIPython: 2.1.0\r\nbs4: 4.3.1\r\nhtml5lib: 0.95-dev\r\nlxml: 3.3.5\r\ndateutil: 2.2\r\n```'"
7761,37892664,filmor,jreback,2014-07-15 15:05:57,2014-07-21 11:39:29,2014-07-21 11:39:29,closed,,0.15.0,5,Bug;Error Reporting;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/7761,b'Non-unique column names in fixed format HDF5',"b'This leads to an error on read, writing is possible however. Since the format has a fundamental problem with this case if there are columns with the same name but different data types this is not fixable.\r\n\r\nHowever, `df.to_hdf` should just not write the respective dataframe if the column index is not unique.'"
7760,37880455,hawkerpl,jreback,2014-07-15 13:06:55,2014-07-19 14:11:35,2014-07-19 14:11:35,closed,,0.15.0,1,Bug;Period;Visualization,https://api.github.com/repos/pydata/pandas/issues/7760,"b'Plotting series with different frequency yields ""ValueError: Incompatible frequency conversion""'","b'So i have two dataframes with simmilar data but, indexed by datetime64\r\ndfA : http://pastebin.com/HenYJxdV\r\ndfB : http://pastebin.com/qAsEqSxf\r\n\r\nso i have following problem when i try to plot them this way:\r\n```\r\nIn [107]:\r\n\r\nseriesA=dfA[""more""]\r\nseriesB=dfB[""less""]\r\nIn [108]:\r\n\r\nplt.figure()\r\nplt.plot(seriesA.index.tolist(),seriesA.tolist())\r\nplt.plot(seriesB.index.tolist(),seriesB.tolist())\r\n```\r\n\r\nit\'s ok and both series have been plotted on same figure\r\n\r\nbut when i\'ll try this:\r\n```\r\nplt.figure()\r\nseriesA.plot()\r\nseriesB.plot()\r\n```\r\n\r\nit wrecks and i get following output\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-109-5054484e1e31> in <module>()\r\n      1 plt.figure()\r\n      2 seriesA.plot()\r\n----> 3 seriesB.plot()\r\n      4 \r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tools\\plotting.pyc in plot_series(series, label, kind, use_index, rot, xticks, yticks, xlim, ylim, ax, style, grid, legend, logx, logy, secondary_y, **kwds)\r\n   2254                      secondary_y=secondary_y, **kwds)\r\n   2255 \r\n-> 2256     plot_obj.generate()\r\n   2257     plot_obj.draw()\r\n   2258 \r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tools\\plotting.pyc in generate(self)\r\n    900         self._compute_plot_data()\r\n    901         self._setup_subplots()\r\n--> 902         self._make_plot()\r\n    903         self._add_table()\r\n    904         self._make_legend()\r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tools\\plotting.pyc in _make_plot(self)\r\n   1549         if self._is_ts_plot():\r\n   1550             data = self._maybe_convert_index(self.data)\r\n-> 1551             self._make_ts_plot(data)\r\n   1552         else:\r\n   1553             from pandas.core.frame import DataFrame\r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tools\\plotting.pyc in _make_ts_plot(self, data, **kwargs)\r\n   1646             y_values = self._get_stacked_values(y, label)\r\n   1647 \r\n-> 1648             newlines = plotf(y_values, ax, label, style, **kwds)\r\n   1649             self._add_legend_handle(newlines[0], label, index=i)\r\n   1650 \r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tools\\plotting.pyc in _plot(data, ax, label, style, **kwds)\r\n   1623             else:\r\n   1624                 lines = tsplot(data, plotf, ax=ax, label=label,\r\n-> 1625                                style=style, **kwds)\r\n   1626                 return lines\r\n   1627         return _plot\r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tseries\\plotting.pyc in tsplot(series, plotf, **kwargs)\r\n     57             series = series.to_period(freq=freq)\r\n     58         freq, ax_freq, series = _maybe_resample(series, ax, freq, plotf,\r\n---> 59                                                 kwargs)\r\n     60 \r\n     61     # Set ax with freq info\r\n\r\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\tseries\\plotting.pyc in _maybe_resample(series, ax, freq, plotf, kwargs)\r\n    107             ax_freq = freq\r\n    108         else:  # pragma: no cover\r\n--> 109             raise ValueError(\'Incompatible frequency conversion\')\r\n    110     return freq, ax_freq, series\r\n    111 \r\n\r\nValueError: Incompatible frequency conversion\r\n```\r\n\r\nWhich is extremely weird because before, i plotted similar but yet slightly different data and it plotted correctly.\r\n\r\nP.S. pd.show_versions():\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 37 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.0\r\nnose: 1.3.1\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.13.3\r\nstatsmodels: 0.5.0\r\nIPython: 2.0.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2013.9\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n```'"
7758,37876887,fulmicoton,jreback,2014-07-15 12:21:22,2014-07-16 11:57:13,2014-07-16 11:57:13,closed,,0.15.0,11,Bug;Dtypes;Enhancement;Unicode,https://api.github.com/repos/pydata/pandas/issues/7758,b'astype(unicode) does not work as expected',"b'astype unicode seems to call str, so that the following code throws\r\n\r\n    import pandas\r\n    df = pandas.DataFrame({""somecol"": [u""\xdfm\xb5\xb1""]})\r\n    df[""somecol""].astype(""unicode"")\r\n\r\nraises :\r\n\r\n    UnicodeEncodeError: \'ascii\' codec can\'t encode ch\r\n    aracters in position 0-1: ordinal not in range(12\r\n    8)'"
7746,37740426,jreback,jreback,2014-07-13 12:36:14,2014-07-19 13:45:50,2014-07-19 13:39:59,closed,,0.15.0,0,Bug;MultiIndex;Period;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7746,b'BUG: reset_index with a multi-index PeriodIndex',"b""http://stackoverflow.com/questions/24718730/is-there-any-way-to-keep-a-periodindex-as-a-series-of-periods-with-a-reset-index\r\n\r\nrelated: #7791, #7792 (and I think this causes both)\r\nduped by #7793 \r\n```\r\nIn [10]: df = DataFrame(np.arange(9).reshape(-1,1),index=pd.MultiIndex.from_product([period_range('20130101',periods=3,freq='M'),['a','b','c']],names=['month','feature']),columns=['value'])\r\n\r\nIn [11]: df\r\nOut[11]: \r\n                 value\r\nmonth   feature       \r\n2013-01 a            0\r\n        b            1\r\n        c            2\r\n2013-02 a            3\r\n        b            4\r\n        c            5\r\n2013-03 a            6\r\n        b            7\r\n        c            8\r\n\r\nIn [12]: df.reset_index()\r\nValueError: Length of values does not match length of index\r\n```"""
7740,37730672,sinhrks,jreback,2014-07-12 23:17:49,2014-08-10 14:53:57,2014-08-10 14:53:57,closed,,0.15.0,1,Bug;Period;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7740,b'BUG: Period ops with np.timedelta64 cause unexpected result',"b""``Period`` + `` np.timedelta64`` doesn't raise ``TypeError(NotImplemented)`` (like ``offsets`` does), and results in erratic internal representation.\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\np = pd.Period('2011-01-01', freq='D')\r\np = p + np.timedelta64(2, 'h')\r\n# This raises no error\r\n\r\nprint(p)\r\n# TypeError: don't know how to convert scalar number to int\r\n\r\np.ordinal\r\n# 14977 hours\r\n```\r\n\r\n``PeriodIndex`` + `` np.timedelta64`` also doesn't raise ``TypeError(NotImplemented)``, and are added ignoring ``PeriodIndex.freq``.\r\n```\r\npidx = pd.period_range('2011-01-01', freq='D', periods=5)\r\npidx = pidx + np.timedelta64(2, 'h')\r\npidx\r\n# <class 'pandas.tseries.period.PeriodIndex'>\r\n# [2011-01-03, ..., 2011-01-07]\r\n# Length: 5, Freq: D\r\n```"""
7738,37726161,seth-p,jreback,2014-07-12 18:57:33,2014-09-10 00:12:39,2014-07-25 14:34:07,closed,,0.15.0,14,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7738,"b""BUG: _flex_binary_moment() doesn't preserve column order or handle multiple columns with the same label""",b'Closes https://github.com/pydata/pandas/issues/7542.'
7736,37718938,sinhrks,TomAugspurger,2014-07-12 12:51:02,2014-09-10 12:10:57,2014-08-19 17:09:15,closed,,0.15.0,25,API Design;Bug;Enhancement;Visualization,https://api.github.com/repos/pydata/pandas/issues/7736,b'ENH: plot functions accept multiple axes and layout kw',"b""- Added ``layout`` keyword to ``plot_frame`` (Closes #6667)\r\n- Allow to pass multiple axes to ``plot_frame``, ``hist`` and ``boxplot`` (Closes #5353, Closes #6970, Closes #7069)\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas.util.testing as tm\r\n\r\nn = 20\r\ngender = tm.choice(['Male', 'Female'], size=n)\r\nclassroom = tm.choice(['A', 'B', 'C'], size=n)\r\ndf = pd.DataFrame({'gender': gender,\r\n                          'classroom': classroom,\r\n                          'height': np.random.normal(66, 4, size=n),\r\n                          'weight': np.random.normal(161, 32, size=n),\r\n                          'category': np.random.randint(4, size=n)})\r\n\r\nfig, axes = plt.subplots(6, 3, figsize=(6, 7))\r\ndf.boxplot(by='category', column=['height', 'weight', 'category'], ax=axes[0])\r\ndf.groupby('classroom').boxplot(column=['height', 'weight', 'category'], ax=axes[1])\r\ndf.hist(column=['height', 'weight', 'category'], ax=axes[2])\r\ndf.hist(by='classroom', ax=axes[3])\r\ndf.plot(subplots=True, ax=axes[4], legend=False)\r\ndf.plot(kind='pie', subplots=True, ax=axes[5], legend=False)\r\nplt.subplots_adjust(hspace=1, bottom=0.05)\r\n```\r\n### Result\r\n![multiax](https://cloud.githubusercontent.com/assets/1696302/3561467/f1ce4c82-09c2-11e4-8623-08b1bd519ebc.png)\r\n"""
7735,37718180,sinhrks,jreback,2014-07-12 11:54:36,2014-07-26 13:27:25,2014-07-25 21:04:51,closed,,0.15.0,9,Bug;Numeric;Timezones,https://api.github.com/repos/pydata/pandas/issues/7735,b'BUG: DTI.value_counts doesnt preserve tz',"b""Found 2 problems related to ``value_counts``.\r\n\r\n- ``DatetimeIndex.value_counts`` loses tz.\r\n```\r\ndidx = pd.date_range('2011-01-01 09:00', freq='H', periods=3, tz='Asia/Tokyo')\r\nprint(didx.value_counts())\r\n# 2011-01-01 00:00:00    1\r\n# 2011-01-01 01:00:00    1\r\n# 2011-01-01 02:00:00    1\r\n# dtype: int64\r\n```\r\n- ``PeriodIndex.value_counts`` results in ``Int64Index``, and unable to drop ``NaT``.\r\n```\r\npidx = pd.PeriodIndex(['2011-01-01 09:00', '2011-01-01 10:00', pd.NaT], freq='H')\r\nprint(pidx.value_counts())\r\n#  359410                 1\r\n#  359409                 1\r\n# -9223372036854775808    1\r\n# dtype: int64\r\n```"""
7733,37707879,sinhrks,jreback,2014-07-12 00:14:34,2014-07-25 20:44:05,2014-07-24 12:59:45,closed,,0.15.0,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7733,b'BUG: Repeated timeseries plot may result in incorrect kind',"b""Must be revisited after #7717.\r\n\r\nRepeated line and area plot may result incorrect if it requires resampling.\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfig, axes = plt.subplots(2, 2, figsize=(7, 5))\r\nnp.random.seed(1)\r\n\r\ndf1 = pd.DataFrame(np.random.rand(5, 2), pd.date_range('2011-01-01', periods=5, freq='D'))\r\ndf2 = pd.DataFrame(np.random.rand(2, 2), pd.date_range('2011-01-01', periods=2, freq='M'))\r\n\r\ndf1.plot(kind='line', ax=axes[0][0], legend=False)\r\ndf2.plot(kind='area', ax=axes[0][0], legend=False)\r\n\r\ndf1.plot(kind='area', ax=axes[1][0], legend=False)\r\ndf2.plot(kind='line', ax=axes[1][0], legend=False)\r\n\r\ndf2.plot(kind='line', ax=axes[0][1], legend=False)\r\ndf1.plot(kind='area', ax=axes[0][1], legend=False)\r\n# ValueError: Argument dimensions are incompatible\r\n\r\ndf2.plot(kind='area', ax=axes[1][1], legend=False)\r\ndf1.plot(kind='line', ax=axes[1][1], legend=False)\r\n```\r\n### Result using current master\r\n- line with low  freq -> area with high freq results in ``ValueError`` (top-right axes)\r\n- area with low  freq -> line with high freq results in all lines, not area (bottom-right axes)\r\n![figure_ng](https://cloud.githubusercontent.com/assets/1696302/3560065/dd93e976-0958-11e4-83e1-acd1dbffa1df.png)\r\n\r\n### Result after fix\r\n![figure_ok](https://cloud.githubusercontent.com/assets/1696302/3560079/622f2eb6-0959-11e4-90ed-5e7eee5dea52.png)\r\n\r\n\r\n"""
7729,37667460,yonil7,jreback,2014-07-11 14:56:31,2014-09-17 20:46:01,2014-09-17 20:46:01,closed,,0.15.0,2,Bug;Dtypes;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7729,b'BUG: wrong type is returned by iat[] for date time series',"b""These return object of type class ``pandas.tslib.Timestamp``:\r\n```\r\n    print type(pd.Series(['2014-01-01', '2014-02-02'], dtype='datetime64[ns]')[0])\r\n    print type(pd.Series(['2014-01-01', '2014-02-02'], dtype='datetime64[ns]').min())\r\n```\r\nbut using ``iat[]`` returns object of type type ``numpy.datetime64``:\r\n```\r\n    print type(pd.Series(['2014-01-01', '2014-02-02'], dtype='datetime64[ns]').iat[0])\r\n```"""
7728,37620271,dsm054,jreback,2014-07-11 01:17:23,2014-09-08 14:13:33,2014-09-08 14:13:22,closed,,0.15.0,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7728,"b'BUG: allow get default value upon IndexError, GH #7725'",b'Fixes #7725 by adding IndexError to the tuple of caught exceptions.'
7725,37605320,dsm054,jreback,2014-07-10 21:09:41,2014-09-08 14:13:22,2014-09-08 14:13:22,closed,,0.15.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7725,"b""BUG(?): get default value doesn't work for Series""","b'`Series` have a `get` method, but using the default value doesn\'t work for certain type combinations:\r\n\r\n```\r\n>>> s = pd.Series([1,2,3], index=[""a"",""b"",""c""])\r\n>>> s\r\na    1\r\nb    2\r\nc    3\r\ndtype: int64\r\n>>> s.get(""d"", 0)\r\n0\r\n>>> s.get(10, 0)\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-18-26d73ac73179>"", line 1, in <module>\r\n    s.get(10, 0)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_421_g20dfc6b-py2.7-linux-x86_64.egg/pandas/core/generic.py"", line 1040, in get\r\n    return self[key]\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_421_g20dfc6b-py2.7-linux-x86_64.egg/pandas/core/series.py"", line 484, in __getitem__\r\n    result = self.index.get_value(self, key)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_421_g20dfc6b-py2.7-linux-x86_64.egg/pandas/core/index.py"", line 1202, in get_value\r\n    return tslib.get_value_box(s, key)\r\n  File ""tslib.pyx"", line 540, in pandas.tslib.get_value_box (pandas/tslib.c:11831)\r\n  File ""tslib.pyx"", line 555, in pandas.tslib.get_value_box (pandas/tslib.c:11678)\r\nIndexError: index out of bounds\r\n```\r\n\r\nI\'m not sure whether it makes the most sense just to teach `.get` to catch IndexErrors as well as KeyErrors and ValueErrors (which is what it does now), or whether a deeper change is warranted.\r\n'"
7724,37599625,tipanverella,jreback,2014-07-10 20:05:04,2014-11-21 23:20:24,2014-11-21 23:20:24,closed,,0.15.2,17,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7724,b'Bug behavior when checking element inclusion in non-unique MultiIndex ',"b""I am using `pandas.__version__==0.14.0`.\r\nThis is my first bug report in github so excuse my possibly poorly formatted submission.\r\n\r\nThis bug report might be related to, or duplicate:\r\n1. https://github.com/pydata/pandas/issues/2646\r\n2. https://github.com/pydata/pandas/issues/6501\r\n\r\nIt was explained to me by [http://stackoverflow.com/users/1427416/brenbarn] on the following StackOverflow question, http://stackoverflow.com/questions/24683023/having-issue-with-hierarchical-index-set-behavior/24684844#24684844\r\n\r\nEssentially,  the following behavior is not desirable:\r\n\r\n    print pd.__version__\r\n    WeirdIdx = pd.MultiIndex(levels=[[0], [1]],labels=[[0, 0], [0,0]],names=[u'X', u'Y'])\r\n    print WeirdIdx\r\n    print (0, 0) in WeirdIdx\r\n    print (1, 0) in WeirdIdx\r\n    print (100, 0) in WeirdIdx\r\n    print (100, 100) in WeirdIdx\r\n\r\nsince it prints:\r\n    0.14.0\r\n    X  Y\r\n    0  1\r\n       1\r\n    True\r\n    True\r\n    True\r\n    True\r\n\r\ndespite the fact that `(100,0)` and `(100,100)` are unambiguously not part of the index."""
7719,37567251,jorisvandenbossche,jreback,2014-07-10 14:24:58,2014-07-10 22:35:31,2014-07-10 22:34:56,closed,,0.14.1,15,Bug;Dtypes;IO SQL,https://api.github.com/repos/pydata/pandas/issues/7719,b'SQL: suppress warning for BIGINT with sqlite and sqlalchemy<0.8.2 (GH7433)',"b""From discussion here: https://github.com/pydata/pandas/pull/7634#issuecomment-48111148 \r\nDue to switching from Integer to BigInteger (to support int64 on some database systems), reading a table from sqlite with integers leads to a warning when you have an slqalchemy version of below 0.8.2.\r\n\r\nI know it is very very late and goes against all reservations of putting in new stuff just before a release, but after some more consideration, I think we should include this (or at least something that fixes it, and I think this does). @jreback, you said not to worry about it (it is just a warning), but the sqlalchemy release that fixes it is only just a year old and this is something most users will try as the first thing I think when using the sqlalchemy functions (writing/reading simple dataframe with some numbers with sqlite), so should not get a warning they don't understand.\r\n\r\nI tested it locally with sqlalchemy 0.7.8 (on Windows), and on travis it is tested with 0.7.1 (the py2.6 build) and there also the warnings disappeared.\r\n\r\nWhat do you think?"""
7712,37506839,cpcloud,cpcloud,2014-07-09 20:48:17,2014-07-10 15:44:34,2014-07-10 15:15:02,closed,cpcloud,0.14.1,15,Bug;Dtypes;Period,https://api.github.com/repos/pydata/pandas/issues/7712,"b""BUG: PeriodIndex constructor doesn't work with Series objects""",b'closes #7701 '
7705,37468091,sinhrks,jreback,2014-07-09 14:10:24,2014-07-10 11:01:35,2014-07-09 15:48:51,closed,,0.14.1,5,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7705,b'BUG: offset normalize option may not work in addition/subtraction',b'Closes problem found in #7375.\r\n@jreback Is this for 0.15 (needs release note)?'
7704,37458756,jreback,jreback,2014-07-09 12:19:05,2014-09-19 20:05:55,2014-09-19 20:05:55,closed,,0.15.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/7704,b'BUG: invalid dtype conversion when setting string column',"b""http://stackoverflow.com/questions/24650117/how-can-i-select-all-dataframe-rows-that-are-within-a-certain-distance-of-a-give/24653406#24653406\r\n\r\n```\r\nIn [32]: df = pd.DataFrame(np.random.rand(30, 3), columns=tuple('ABC'))\r\n\r\nIn [33]: df['event'] = np.nan\r\n\r\nIn [34]: df.dtypes\r\nOut[34]: \r\nA        float64\r\nB        float64\r\nC        float64\r\nevent    float64\r\ndtype: object\r\n\r\nIn [35]: df.loc[10,'event'] = 'foo'\r\n\r\nIn [37]: df.dtypes\r\nOut[37]: \r\nA        object\r\nB        object\r\nC        object\r\nevent    object\r\ndtype: object\r\n```"""
7697,37384875,sinhrks,jreback,2014-07-08 15:55:25,2014-07-25 20:42:15,2014-07-25 15:10:47,closed,,0.15.0,15,Bug;Frequency;Performance,https://api.github.com/repos/pydata/pandas/issues/7697,b'BUG/PERF: offsets.apply doesnt preserve nanosecond',"b""Main Fix is to preserve nanosecond info which can lost during ``offset.apply``, but it also includes:\r\n- Support dateutil timezone\r\n- Little performance improvement. Even though v0.14.1 should take longer than v0.14.0 because perf test in v0.14 doesn't perform timestamp conversion which was fixed in #7502.\r\nNOTE: This caches ``Tick.delta`` because it was calculated 3 times repeatedly, but does it cause any side effect?\r\n\r\n### Before\r\n```\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\ntimeseries_year_incr                         |   0.0164 |   0.0103 |   1.5846 |\r\ntimeseries_year_apply                        |   0.0153 |   0.0094 |   1.6356 |\r\ntimeseries_day_incr                          |   0.0187 |   0.0053 |   3.5075 |\r\ntimeseries_day_apply                         |   0.0164 |   0.0033 |   4.9048 |\r\n\r\nTarget [d0076db] : PERF: Improve index.min and max perf\r\nBase   [da0f7ae] : RLS: 0.14.0 final\r\n```\r\n\r\n### After the fix\r\n```\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\ntimeseries_year_incr                         |   0.0150 |   0.0087 |   1.7339 |\r\ntimeseries_year_apply                        |   0.0126 |   0.0073 |   1.7283 |\r\ntimeseries_day_incr                          |   0.0130 |   0.0053 |   2.4478 |\r\ntimeseries_day_apply                         |   0.0107 |   0.0033 |   3.2143 |\r\n\r\nTarget [64dd021] : BUG: offsets.apply doesnt preserve nanosecond\r\nBase   [da0f7ae] : RLS: 0.14.0 final\r\n```"""
7691,37345237,bjonen,jreback,2014-07-08 08:48:52,2014-09-18 15:36:47,2014-09-18 13:55:33,closed,,0.15.0,34,Bug;Enhancement;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7691,b'Fix 7180 autodetect',b'This PR closes #7180\r\n\r\nIn the terminal: \r\nIf maxcols == 0 then auto detect columns\r\nif maxrows == 0 auto detect rows \r\n'
7679,37263838,ianp,jreback,2014-07-07 12:25:44,2014-09-10 15:25:29,2014-07-07 13:30:46,closed,,0.15.0,8,Bug;Duplicate;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7679,b'Interpolate fails when passed limit argument.',"b""When I try to interpolate time series data it fails with an index error: arrays used as indices must be of integer (or boolean) type.\r\n\r\n```python\r\nfrom pandas import read_csv\r\nfrom StringIO import StringIO\r\n\r\ndata = '''2013-09-28,4018.02\r\n2013-10-05,5806.78\r\n2013-10-26,6334.4\r\n2013-11-02,6199.6\r\n2013-11-09,6213.07\r\n2013-11-16,7099.39\r\n2013-11-23,7332.98\r\n2013-11-30,6157.37\r\n2013-12-07,7427.65\r\n2014-01-04,5856.38\r\n2014-01-11,6208.99\r\n2014-01-18,7440.29\r\n2014-01-25,7295.97\r\n2014-02-08,7654.42\r\n'''\r\n\r\ndf = read_csv(StringIO(data))\r\ndf.interpolate(limit=1)\r\n```\r\n\r\nRemoving the `limit=1` argument works as expected.\r\n\r\nPython 2.7.6, pandas 0.14.0, numpy 1.8.1, on OS X.\r\n\r\nI'll try it with a Python 3 environment when I get chance later this week."""
7678,37253821,yelite,jorisvandenbossche,2014-07-07 09:47:10,2014-07-07 20:07:52,2014-07-07 17:46:41,closed,,0.14.1,12,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/7678,b'FIX: to_sql takes the boolean column as text column',"b'In the original code, `com.is_bool(arr_or_dtype)` checks whether `arr_or_dtype` is a boolean value instead of a boolean dtype.\r\n\r\nA new function `is_bool_dtype` is added to `pandas.core.common` to fix this bug.'"
7677,37250973,acorbe,jreback,2014-07-07 09:08:09,2015-10-19 20:52:48,2015-10-19 20:52:40,closed,,0.17.0,16,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/7677,b'BUG: sqrt not implemented in df.eval ',"b""In the following scenario:\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n\r\n    a = np.random.rand(1000)\r\n    df = pd.DataFrame({'a' : a })\r\n\r\nThis call doesn't work (while it is supposed to, I guess)\r\n\r\n    df.eval('sqrt(a)')\r\n\r\n         NotImplementedError: 'Call' nodes are not implemented\r\n\r\nwhile this one does\r\n\r\n    df.eval('(a)**(.5)')\r\n\r\nI guess this is a bug."""
7667,37176413,jreback,jreback,2014-07-04 15:41:25,2014-07-04 16:10:17,2014-07-04 16:10:17,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7667,"b'BUG: Bug in multi-index slice setting, related GH3738'",
7666,37145177,veor,jreback,2014-07-04 06:42:48,2014-07-04 14:57:00,2014-07-04 14:57:00,closed,,0.16.0,1,Bug;Dtypes;Duplicate;Groupby,https://api.github.com/repos/pydata/pandas/issues/7666,b'BUG: aggregate(sum) returns wrong result for certain boolean input',"b""I have a DataFrame that looks like the following format: \r\n```python\r\ndf = pd.DataFrame({'foo': [1, 2, 2], 'bar': [True, False, False]})\r\n```\r\nI want group this by foo and count the number of True values in the bar column. Counting the True values can be achieved with the sum command.\r\n```python\r\nIn [7]: bar = [True, False, True, False, False]\r\n\r\nIn [8]: sum(bar)\r\nOut[8]: 2\r\n\r\nIn [9]: sum(df['bar'])\r\nOut[9]: 1\r\n```\r\nTo group and count this:\r\n\r\n\r\n```python\r\nIn [16]: df.groupby('foo').aggregate(sum)\r\nOut[16]:\r\n       bar\r\nfoo\r\n1     True\r\n2    False\r\n```\r\nThis output is erroneous. Expected output is:\r\n```python\r\n       bar\r\nfoo\r\n1      1\r\n2      0\r\n```\r\n\r\nIt works in the following case (changed so that not all cases for foo:2 are false). \r\n```python\r\nIn [18]: df = pd.DataFrame({'foo': [1, 2, 2, 2, 2], 'bar': [True, True, True, False, False]})\r\nIn [18]: df.groupby('foo').aggregate(sum)\r\nOut[18]:\r\n     bar\r\nfoo\r\n1      1\r\n2      2\r\n```\r\n\r\nHere are my installed versions:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.0\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2014.3\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n  ```"""
7661,37126259,ischwabacher,jreback,2014-07-03 21:36:46,2014-09-14 14:20:10,2014-09-14 14:20:10,closed,,0.15.0,10,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7661,b'FIX: Scalar timedelta NaT results raise',"b""Currently, coercion of scalar results from `float` to `timedelta64[ns]` passes through `int`, which raises when attempting to coerce `NaN` to `NaT`.\r\n\r\nTo reproduce:\r\n```python\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\npd.Series([np.timedelta64('NaT')]).sum()\r\n# TypeError: reduction operation 'sum' not allowed for this dtype\r\n```"""
7660,37116660,jreback,jreback,2014-07-03 19:39:08,2014-07-21 11:42:51,2014-07-21 11:42:51,closed,,0.15.0,0,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7660,b'BUG: stacking multiple levels',"b'from #7653\r\ncc @bluefir\r\n\r\n```\r\ndf = DataFrame(np.random.randn(1000, 27), \r\n               columns=MultiIndex.from_tuples(list(\r\n                                   itertools.product(xrange(3), repeat=3))))\r\ndf.stack(level=[1, 2])\r\n```\r\n\r\nshould be equiv to:\r\n\r\n``df.stack(level=1).stack(level=1)``\r\n\r\nbut is raising'"
7655,37079161,jreback,jreback,2014-07-03 12:59:47,2014-10-06 13:03:02,2014-10-06 13:03:02,closed,,0.15.0,0,API Design;Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7655,b'BUG/API: assignment of frame in a multi-indexed column frame with a matching level',"b""http://stackoverflow.com/questions/24553921/pandas-matching-on-level-of-hierarchical-index/24554350#24554350\r\n\r\nThis should align on the columns axis. Not clear how to specify the alignment level though (e.g. broadcasting would be tricky here).\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\narrays = [np.hstack([ ['one']*3, ['two']*3]), ['Dog', 'Bird', 'Cat']*2]\r\ncolumns = pd.MultiIndex.from_arrays(arrays, names=['foo', 'bar'])\r\n\r\ndf = pd.DataFrame(np.zeros((3,6)),columns=columns,\r\n                  index=pd.date_range('20000103',periods=3))\r\n\r\ndf['one'] = pd.DataFrame({'Bird' : np.ones(3)*2,\r\n                          'Dog' : np.ones(3),\r\n                          'Cat' : np.ones(3)*3},\r\n                          index= pd.date_range('20000103',periods=3))\r\ndf['two'] = pd.DataFrame({'Dog' : np.ones(3)*4,\r\n                          'Bird' : np.ones(3)*5,\r\n                          'Cat' : np.ones(3)*6,},\r\n                          index= pd.date_range('20000103',periods=3))\r\n```"""
7647,36976741,socheon,jreback,2014-07-02 12:26:08,2014-09-04 00:21:50,2014-09-04 00:21:50,closed,,0.16.0,2,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/7647,b'Different behaviour when assigning None using loc[] and []',"b""```\r\nimport pandas as pd\r\nimport numpy as np \r\n\r\nser = pd.Series(['a', 'b', np.nan])\r\nprint ser\r\n\r\nser.loc[ser.isnull()] = None\r\nprint ser\r\n\r\nser[ser.isnull()] = None\r\nprint ser \r\n```\r\n\r\nOutput\r\n```\r\n0      a\r\n1      b\r\n2    NaN\r\ndtype: object\r\n0       a\r\n1       b\r\n2    None\r\ndtype: object\r\n0      a\r\n1      b\r\n2    NaN\r\ndtype: object\r\n```\r\n\r\nI am not sure whether this behaviour is intentional. Using .loc[], None was correctly assigned but when using [], NaN was assigned instead. Thanks\r\n\r\nI am using version 0.14.0-169-g7684b6e\r\n"""
7638,36904710,jreback,jreback,2014-07-01 15:39:11,2014-07-01 16:02:12,2014-07-01 16:02:12,closed,,0.14.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7638,b'BUG: Bug in Series.get with a boolean accessor (GH7407)',b'closes #7407 '
7631,36886310,jreback,jreback,2014-07-01 12:10:14,2014-07-01 12:50:25,2014-07-01 12:50:25,closed,,0.14.1,0,Bug;Docs;Groupby,https://api.github.com/repos/pydata/pandas/issues/7631,b'BUG: doc example in groupby.rst (GH7559 / GH7628)',b'closes #7628 '
7628,36867599,jorisvandenbossche,jreback,2014-07-01 07:12:20,2014-07-01 12:50:25,2014-07-01 12:50:25,closed,,0.14.1,1,Bug;Docs;Groupby,https://api.github.com/repos/pydata/pandas/issues/7628,b'BUG: nth on series groupby with dropna=True gives TypeError',"b'From the docs: http://pandas-docs.github.io/pandas-docs-travis/groupby.html#taking-the-nth-row-of-each-group\r\n\r\n```\r\nIn [1]: df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=[\'A\', \'B\'])\r\n\r\nIn [2]: g = df.groupby(\'A\')\r\n\r\nIn [3]: g.B.nth(0, dropna=True)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-bcb9dcbefdf5> in <module>()\r\n----> 1 g.B.nth(0, dropna=True)\r\n\r\nC:\\Users\\vdbosscj\\Scipy\\pandas-joris\\pandas\\core\\groupby.py in nth(self, n, drop\r\nna)\r\n    831         # get a new grouper for our dropped obj\r\n    832         grouper, exclusions, obj = _get_grouper(dropped, key=self.keys,\r\naxis=self.axis,\r\n--> 833                                                 level=self.level, sort=s\r\nelf.sort)\r\n    834\r\n    835         sizes = obj.groupby(grouper).size()\r\n\r\nC:\\Users\\vdbosscj\\Scipy\\pandas-joris\\pandas\\core\\groupby.py in _get_grouper(obj,\r\n key, axis, level, sort)\r\n   2035             raise AssertionError(errmsg)\r\n   2036\r\n-> 2037         ping = Grouping(group_axis, gpr, obj=obj, name=name, level=level\r\n, sort=sort)\r\n   2038         groupings.append(ping)\r\n   2039\r\n\r\nC:\\Users\\vdbosscj\\Scipy\\pandas-joris\\pandas\\core\\groupby.py in __init__(self, in\r\ndex, grouper, obj, name, level, sort)\r\n   1874             # no level passed\r\n   1875             if not isinstance(self.grouper, (Series, np.ndarray)):\r\n-> 1876                 self.grouper = self.index.map(self.grouper)\r\n   1877                 if not (hasattr(self.grouper, ""__len__"") and\r\n   1878                         len(self.grouper) == len(self.index)):\r\n\r\nC:\\Users\\vdbosscj\\Scipy\\pandas-joris\\pandas\\core\\index.py in map(self, mapper)\r\n   1332\r\n   1333     def map(self, mapper):\r\n-> 1334         return self._arrmap(self.values, mapper)\r\n   1335\r\n   1336     def isin(self, values):\r\n\r\nC:\\Users\\vdbosscj\\Scipy\\pandas-joris\\pandas\\algos.pyd in pandas.algos.arrmap_int\r\n64 (pandas\\algos.c:72351)()\r\n\r\nTypeError: \'NoneType\' object is not callable\r\n```'"
7622,36842139,Poquaruse,jreback,2014-06-30 21:48:08,2014-07-18 16:42:38,2014-06-30 22:16:09,closed,,,4,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7622,"b'DataFrame groupby partially drops timezone info (to_csv, in notebook)'","b""Hi all,\r\n\r\nI've encountered a problem with DataFrames, groupby and timezones.\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndt_rng = pd.date_range(start='2014-01-01 00:00', periods = 1000, freq='1s', tz='Europe/Berlin')\r\ndf = pd.DataFrame({'a':np.random.randn(1000), 'b': np.random.randn(1000)},index = dt_rng)\r\ndf['b'] = df['b'].round()\r\ndf.to_csv()\r\n\r\n--> Timezones are shown in the csv output, for example 2014-01-01 00:00:00+01:00\r\n\r\nNow with resampling:\r\n\r\ndt_rng = pd.date_range(start='2014-01-01 00:00', periods = 1000, freq='1s', tz='Europe/Berlin')\r\ndf = pd.DataFrame({'a':np.random.randn(1000), 'b': np.random.randn(1000)},index = dt_rng)\r\ndf['b'] = df['b'].round()\r\ndf.groupby(df['b']).resample('1min').to_csv()\r\n\r\n--> 2013-12-31 23:01:00 no timezone info, not even UTC.\r\n\r\nHowever:\r\n\r\ndt_rng = pd.date_range(start='2014-01-01 00:00', periods = 1000, freq='1s', tz='Europe/Berlin')\r\ndf = pd.DataFrame({'a':np.random.randn(1000), 'b': np.random.randn(1000)},index = dt_rng)\r\ndf['b'] = df['b'].round()\r\ndf.groupby(df['b']).resample('1min').index.levels[1]\r\n\r\nshows: Timezone: Europe/Berlin\r\n\r\nSo the info seems to be there, but is not exported - even if it was exported before without resampling...\r\n\r\nAny ideas?\r\n\r\nThanks and best regards\r\n"""
7618,36806229,AllenDowney,jreback,2014-06-30 15:08:40,2014-09-23 13:04:18,2014-09-23 13:04:18,closed,,0.15.0,12,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7618,"b""BUG: rolling_window yields unexpected results with win_type='triang'""","b""Here's the example in the documentation, modified to have non-zero mean:\r\n\r\n    n = 100\r\n    ser = pandas.Series(randn(n)+10, index=pandas.date_range('1/1/2000', periods=n))\r\n    pandas.rolling_window(ser, 5, 'triang').plot()\r\n    pandas.rolling_window(ser, 5, 'boxcar').plot()\r\n\r\nThe rolling boxcar window is centered around 10, as expected.\r\n\r\nThe triang window is centered around 6.  That suggests that the weights in the window don't add up to 1.\r\n\r\nEither that or my expectation of how it should work is wrong?"""
7616,36797416,jreback,jreback,2014-06-30 13:32:38,2014-09-05 03:18:03,2014-06-30 14:35:18,closed,,0.14.1,0,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7616,"b'BUG: Bug in to_timedelta that accepted invalid units and misinterpreted m/h (GH7611, GH6423)'",b'closes #7611\r\ncloses #6423 '
7611,36747251,vfilimonov,jreback,2014-06-29 14:38:05,2014-09-05 19:17:55,2014-09-05 10:24:18,closed,,0.14.1,12,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7611,"b""to_timedelta does not recognize 'h' and 'm' units""","b""'h' and 'm' units are not correctly recognized in to_timedelta:\r\n```\r\nprint pd.to_timedelta(1,unit='D')\r\nprint pd.to_timedelta(1,unit='h')\r\nprint pd.to_timedelta(1,unit='m')\r\nprint pd.to_timedelta(1,unit='s')\r\nprint pd.to_timedelta(1,unit='ms')\r\nprint pd.to_timedelta(1,unit='us')\r\nprint pd.to_timedelta(1,unit='ns')\r\n```\r\nresults in\r\n```\r\n86400000000000 nanoseconds\r\n1 nanoseconds\r\n1 nanoseconds\r\n1000000000 nanoseconds\r\n1000000 nanoseconds\r\n1000 nanoseconds\r\n1 nanoseconds\r\n```\r\nversion: 0.14.0\r\n\r\nMost likely related to Issue #6423"""
7610,36746799,sinhrks,jreback,2014-06-29 14:14:44,2014-07-02 16:45:22,2014-06-30 13:29:09,closed,,0.14.1,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7610,b'BUG: Timestamp.__new__ doesnt preserve nanosecond',"b""When ``Timestamp.__new__`` accepts a timestamp with nanosecond, nanosecond is not preserved properly.\r\n```\r\n# create Timestamp with ns (OK)\r\nt = pd.Timestamp('2011-01-01') + pd.offsets.Nano(5)\r\nt, t.value\r\n# (Timestamp('2011-01-01 00:00:00.000000005'), 1293840000000000005)\r\n\r\n# If it is passed to Timestamp.__init__, ns is not displayed even though internal value includes it. (NG)\r\nt = pd.Timestamp(t)\r\nt, t.value\r\n# (Timestamp('2011-01-01 00:00:00'), 1293840000000000005)\r\n\r\n# If offset is added to above result, ns is displayed properly (OK)\r\nt = t + pd.offsets.Nano(5)\r\nt, t.value\r\n# (Timestamp('2011-01-01 00:00:00.000000010'), 1293840000000000010)\r\n```\r\n\r\nNOTE: Unrelated to this issue, test_tslib had ``test_timedelta_ns_arithmetic`` method duplicatelly. Thus renamed."""
7608,36743168,sinhrks,jreback,2014-06-29 10:16:42,2014-07-05 04:48:06,2014-07-03 21:31:22,closed,,0.14.1,3,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7608,b'BUG: DatetimeIndex.delete with tz raises ValueError',"b""Found and fixed ``DatetimeIndex.delete`` (#7302) results in ``ValueError`` when it has ``tz`` and ``freq`` is less than daily..\r\n```\r\nidx = pd.date_range(start='2011-01-01 09:00', periods=40, freq='H', tz='Asia/Tokyo')\r\nidx.delete(0)\r\n# ValueError: Inferred frequency None from passed dates does notconform to passed frequency H\r\n```\r\n\r\n``insert`` (#7299) and ``take`` don't have problems, but changed test case to use hourly frequencies to detect this kind of problem."""
7606,36740475,sinhrks,jreback,2014-06-29 06:37:38,2014-06-30 10:58:09,2014-06-29 11:12:57,closed,,0.14.1,1,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7606,b'BUG: DTI.freqstr raises AttributeError when freq is None',"b'``DatetimeIndex.freqstr`` raises ``AttributeError`` if ``freq/offset`` is ``None``, even though docstring says ""return the frequency object as a string if its set, otherwise None""\r\n```\r\nidx = pd.DatetimeIndex([\'2011-01-01\', \'2011-01-02\', \'2011-01-03\', \'2011-01-04\'])\r\nidx.freqstr\r\n# AttributeError: \'NoneType\' object has no attribute \'freqstr\'\r\n```\r\nAlso,  ``DataFrame/Series.to_period`` has a logic to use ``inferred_freq`` when the freq is not passed, but it doesn\'t work actually because of the bug. Moved the logic to ``DatetimeIndex.to_period`` for consistency and made it works.\r\n\r\nThe fix will simplify #7602 a little.'"
7604,36736394,seth-p,jreback,2014-06-29 00:20:04,2014-09-10 00:12:47,2014-07-01 10:14:18,closed,,0.14.1,3,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7604,"b""BUG: {expanding,rolling}_{cov,corr} don't handle arguments with different index sets properly""",b'Closes https://github.com/pydata/pandas/issues/7512.'
7600,36723909,sinhrks,jreback,2014-06-28 11:56:31,2014-07-05 04:47:11,2014-06-30 12:33:49,closed,,0.14.1,6,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7600,b'BUG: GroupBy.size created by TimeGrouper raises AttributeError',b'Related to #7453 2nd issue:\r\n- ``GroupBy.size`` created by ``TimeGrouper`` raises ``AttributeError``'
7594,36704696,rockg,jreback,2014-06-27 21:18:31,2014-06-28 02:09:17,2014-06-28 01:38:16,closed,,0.14.1,5,Bug;Dtypes;Timezones,https://api.github.com/repos/pydata/pandas/issues/7594,b'Timezone-aware times are inconsistent using object or datetime64',"b""I just upgraded to 0.14 and am seeing an inconsistent behavior when converting datetimes to objects.  This used to be consistent in 0.12.1 and now it loses timezone information when converting to datetime64.  Is this expected?\r\n\r\n```\r\nimport pytz\r\nimport datetime\r\nfrom pandas import DataFrame\r\ntz = pytz.timezone('US/Eastern')\r\ndt = tz.localize(datetime.datetime(2012, 6, 1))\r\ndt\r\nOut[31]: datetime.datetime(2012, 6, 1, 0, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)\r\ndf = DataFrame([{'End Date': dt}])\r\ndf\r\nOut[26]: \r\n                    End Date\r\n0  2012-06-01 00:00:00-04:00\r\ndf.dtypes\r\nOut[27]: \r\nEnd Date    object\r\ndtype: object\r\ndf_c = df.convert_objects(convert_dates=True)\r\ndf_c\r\nOut[29]: \r\n             End Date\r\n0 2012-06-01 04:00:00\r\ndf_c.dtypes\r\nOut[30]: \r\nEnd Date    datetime64[ns]\r\ndtype: object\r\n```"""
7593,36698460,jreback,jreback,2014-06-27 19:52:29,2014-06-27 20:44:03,2014-06-27 20:44:03,closed,,0.14.1,0,Bug;Missing-data;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7593,b'BUG: Bug in timedelta inference when assigning an incomplete Series (GH7592)',b'closes #7592 '
7592,36692185,ischwabacher,jreback,2014-06-27 18:31:42,2014-06-27 20:44:54,2014-06-27 20:44:03,closed,,0.14.1,4,Bug;Indexing;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7592,b'Assigning timedelta64 to new column casts to float instead of filling missing values with NaT',"b""When assigning a `timedelta64` array to a subset of a new column of a `DataFrame`, missing data is not filled with `NaT` as expected; rather, the new column is cast to `float64` and `NaN` is used instead.  This cast does not usually occur when all values are present, *except* when there are already `float64` columns but no `timedelta64` columns in the `DataFrame` *and* indexing is done through `.ix` or `.loc`.\r\n\r\nIt's possible these should be two separate issues.\r\n\r\nThere are a lot of issues involving `NaT` in the issue tracker; I'm not 100% sure that this isn't a duplicate.  (Nor am I 100% sure this isn't intended behavior, but if it is I'd expect it to be documented more prominently.)\r\n\r\n```python\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\none_hour = 60*60*10**9\r\n\r\ntemp = pd.DataFrame({}, index=pd.date_range('2014-1-1', periods=4))\r\ntemp['A'] = np.array([1*one_hour]*4, dtype='m8[ns]')\r\ntemp.loc[:,'B'] = np.array([2*one_hour]*4, dtype='m8[ns]')\r\ntemp.loc[:3,'C'] = np.array([3*one_hour]*3, dtype='m8[ns]')\r\ntemp.ix[:,'D'] = np.array([4*one_hour]*4, dtype='m8[ns]')\r\ntemp.ix[:3,'E'] = np.array([5*one_hour]*3, dtype='m8[ns]')\r\ntemp['F'] = np.timedelta64('NaT')\r\ntemp.ix[:-1,'F'] = np.array([6*one_hour]*3, dtype='m8[ns]')\r\n\r\ntemp\r\n#                   A        B             C        D             E        F\r\n# 2014-01-01 01:00:00 02:00:00  1.080000e+13 04:00:00  1.800000e+13 06:00:00\r\n# 2014-01-02 01:00:00 02:00:00  1.080000e+13 04:00:00  1.800000e+13 06:00:00\r\n# 2014-01-03 01:00:00 02:00:00  1.080000e+13 04:00:00  1.800000e+13 06:00:00\r\n# 2014-01-04 01:00:00 02:00:00           NaN 04:00:00           NaN      NaT\r\n# \r\n# [4 rows x 6 columns]\r\n\r\ntemp = pd.DataFrame({}, index=pd.date_range('2014-1-1', periods=4))\r\n# Partial assignment converts\r\ntemp.ix[:-1,'A'] = np.array([1*one_hour]*3, dtype='m8[ns]')\r\n# DataFrame is all floats; converts\r\ntemp.ix[:,'B'] = np.array([2*one_hour]*4, dtype='m8[ns]')\r\n# .ix and .loc behave the same\r\ntemp.loc[:,'C'] = np.array([3*one_hour]*4, dtype='m8[ns]')\r\n# straight column assignment doesn't convert\r\ntemp['D'] = np.array([4*one_hour]*4, dtype='m8[ns]')\r\n# Now there are timedeltas; doesn't convert\r\ntemp.ix[:,'E'] = np.array([5*one_hour]*4, dtype='m8[ns]')\r\n# .ix and .loc still behave the same\r\ntemp.loc[:,'F'] = np.array([6*one_hour]*4, dtype='m8[ns]')\r\n\r\ntemp\r\n#                        A             B             C        D        E  \\\r\n# 2014-01-01  3.600000e+12  7.200000e+12  1.080000e+13 04:00:00 05:00:00   \r\n# 2014-01-02  3.600000e+12  7.200000e+12  1.080000e+13 04:00:00 05:00:00   \r\n# 2014-01-03  3.600000e+12  7.200000e+12  1.080000e+13 04:00:00 05:00:00   \r\n# 2014-01-04           NaN  7.200000e+12  1.080000e+13 04:00:00 05:00:00   \r\n# \r\n#                   F  \r\n# 2014-01-01 06:00:00  \r\n# 2014-01-02 06:00:00  \r\n# 2014-01-03 06:00:00  \r\n# 2014-01-04 06:00:00  \r\n# \r\n# [4 rows x 6 columns]\r\n\r\ntemp = pd.DataFrame({}, index=pd.date_range('2014-1-1', periods=4))\r\n# No columns yet, no conversion\r\ntemp.ix[:,'A'] = np.array([2*one_hour]*4, dtype='m8[ns]')\r\n#                   A\r\n# 2014-01-01 02:00:00\r\n# 2014-01-02 02:00:00\r\n# 2014-01-03 02:00:00\r\n# 2014-01-04 02:00:00\r\n# \r\n# [4 rows x 1 columns]\r\n\r\n```"""
7591,36691650,mcwitt,jreback,2014-06-27 18:25:31,2014-06-30 21:05:10,2014-06-30 19:26:49,closed,,0.14.1,1,Bug;IO CSV;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7591,"b'ENH: read_{csv,table} look for index columns in row after header with C engine'","b'Closes #6893.\r\n\r\nCurrently the Python parser can read data with the index columns specified on the first line after the header, e.g.\r\n\r\n```python\r\nIn [3]: pd.__version__\r\nOut[3]: \'0.14.0-271-gf8b101c\'\r\n\r\nIn [4]: text = """"""                      A       B       C       D        E\r\none two three   four\r\na   b   10.0032 5    -0.5109 -2.3358 -0.4645  0.05076  0.3640\r\na   q   20      4     0.4473  1.4152  0.2834  1.00661  0.1744\r\nx   q   30      3    -0.6662 -0.5243 -0.3580  0.89145  2.5838""""""\r\n\r\nIn [5]: pd.read_table(StringIO(text), sep=\'\\s+\', engine=\'python\')\r\nOut[5]: \r\n                           A       B       C        D       E\r\none two three   four                                         \r\na   b   10.0032 5    -0.5109 -2.3358 -0.4645  0.05076  0.3640\r\n    q   20.0000 4     0.4473  1.4152  0.2834  1.00661  0.1744\r\nx   q   30.0000 3    -0.6662 -0.5243 -0.3580  0.89145  2.5838\r\n```\r\n\r\nbut the C parser fails:\r\n\r\n```python\r\nIn [6]: pd.read_table(StringIO(text), sep=\'\\s+\', engine=\'c\')\r\n---------------------------------------------------------------------------\r\nCParserError                              Traceback (most recent call last)\r\n. . .\r\nCParserError: Error tokenizing data. C error: Expected 5 fields in line 3, saw 9\r\n```\r\n\r\nThis PR patches the C parser to enable this feature.\r\n'"
7587,36663391,cpcloud,cpcloud,2014-06-27 13:30:21,2014-06-27 19:35:10,2014-06-27 19:35:09,closed,cpcloud,0.14.1,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7587,b'BUG: bug in float64index assignment with a non scalar indexer',b'closes #7586'
7586,36657071,jreback,cpcloud,2014-06-27 11:49:38,2014-06-29 11:09:31,2014-06-27 19:35:09,closed,cpcloud,0.14.1,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7586,b'BUG: assignment in Float64Index',"b'http://stackoverflow.com/questions/24446500/pandas-valueerror-when-assigning-dataframe-entries-using-index-due-to-a-change\r\n\r\n```\r\nIn [2]: A = pd.DataFrame(np.random.rand(10,4), index=np.random.rand(10))\r\n\r\nIn [3]: A.loc[A.index] = A.loc[A.index]\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n```'"
7580,36607174,jreback,jreback,2014-06-26 19:37:22,2014-06-30 11:05:59,2014-06-30 11:05:58,closed,,0.14.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7580,b'BUG/TST: fix tests for groupby nth on Series (GH7559)',"b""closes #7559 \r\nrelated #7287\r\n\r\nAnd some improvements when using any/all for nth\r\n(still the nth are 10x slower than the cythonized first/last, but that's another issue)\r\n```\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\ngroupby_nth_object_any                       | 503.6680 | 17163.4649 |   0.0293 |\r\ngroupby_nth_datetimes_any                    | 553.3033 | 13455.8423 |   0.0411 |\r\ngroupby_series_nth_any                       |   2.7003 |  26.1161 |   0.1034 |\r\ngroupby_frame_nth_any                        |   5.4483 |  15.4017 |   0.3537 |\r\ngroupby_multi_different_functions            |  11.3816 |  12.2033 |   0.9327 |\r\ngroupby_multi_different_numpy_functions      |  10.4934 |  11.1040 |   0.9450 |\r\ngroupby_object_nth                           | 397.3277 | 416.7441 |   0.9534 |\r\ngroupby_pivot_table                          |  16.2361 |  16.9867 |   0.9558 |\r\ngroupby_frame_apply_overhead                 |   8.8013 |   9.1234 |   0.9647 |\r\ngroupby_multi_python                         | 129.7946 | 133.2393 |   0.9741 |\r\ngroupby_apply_dict_return                    |  37.9413 |  38.7277 |   0.9797 |\r\ngroupby_frame_apply                          |  41.9403 |  42.6593 |   0.9831 |\r\ngroupby_object_first                         |  15.0506 |  15.2097 |   0.9895 |\r\ngroupby_frame_cython_many_columns            |   3.3430 |   3.3733 |   0.9910 |\r\ngroupby_nth_float64                          |  51.4960 |  51.9506 |   0.9912 |\r\ngroupby_frame_nth                            |   2.7520 |   2.7746 |   0.9918 |\r\ngroupby_multi_size                           |  21.6714 |  21.8496 |   0.9918 |\r\ngroupby_transform2                           | 159.0919 | 160.3904 |   0.9919 |\r\ngroupby_indices                              |   6.2350 |   6.2837 |   0.9923 |\r\ngroupby_multi_cython                         |  13.9600 |  14.0460 |   0.9939 |\r\ngroupby_frame_median                         |   6.1007 |   6.1367 |   0.9941 |\r\ngroupby_sum_booleans                         |   1.2400 |   1.2473 |   0.9941 |\r\ngroupby_datetimes_last                       |  10.7520 |  10.8067 |   0.9949 |\r\ngroupby_series_simple_cython                 | 178.8233 | 179.7036 |   0.9951 |\r\ngroupby_nth_float32                          |  52.6213 |  52.8786 |   0.9951 |\r\ngroupby_multi_series_op                      |  12.5150 |  12.5673 |   0.9958 |\r\ngroupby_transform                            | 166.5533 | 167.0943 |   0.9968 |\r\ngroupby_frame_singlekey_integer              |   2.3321 |   2.3394 |   0.9969 |\r\ngroupby_multi_count                          |   8.7133 |   8.7360 |   0.9974 |\r\ngroupby_last_float32                         |   3.5094 |   3.5140 |   0.9987 |\r\ngroupby_transform_ufunc                      |   6.1913 |   6.1987 |   0.9988 |\r\ngroupby_last                                 |   3.5419 |   3.5423 |   0.9999 |\r\ngroupby_first                                |   3.3600 |   3.3563 |   1.0011 |\r\ngroupby_object_last                          |  14.8129 |  14.7574 |   1.0038 |\r\ngroupby_int_count                            |   4.4247 |   4.4080 |   1.0038 |\r\ngroupby_first_float32                        |   3.3464 |   3.3300 |   1.0049 |\r\ngroupby_mixed_first                          |  10.9203 |  10.8653 |   1.0051 |\r\ngroupby_datetimes_nth                        | 445.6376 | 423.4857 |   1.0523 |\r\ngroupby_simple_compress_timing               |  33.8430 |  27.2744 |   1.2408 |\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\n\r\nRatio < 1.0 means the target commit is faster then the baseline.\r\nSeed used: 1234\r\n\r\nTarget [def0155] : BUG/TST: fix tests for groupby nth on Series (GH7559)\r\nBase   [4082c1a] : Merge pull request #7593 from jreback/timedelta_nat\r\n\r\nBUG: Bug in timedelta inference when assigning an incomplete Series (GH7592)\r\n```"""
7574,36575326,chrisaycock,jreback,2014-06-26 13:28:07,2014-07-02 16:04:34,2014-07-02 16:04:34,closed,,0.16.0,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7574,b'BUG: Aggregating over an integer on an empty DataFrame removes the index name',"b""Suppose I have this simple DataFrame:\r\n```python\r\nIn [91]: df = pd.DataFrame({'ref':[1, 2, 3], 'symbol':['GOOGL', 'IBM', 'MSFT'], 'price':[585.93, 180.72, 42.03]}, columns=['ref', 'symbol', 'price'])\r\n\r\nIn [92]: df\r\nOut[92]:\r\n   ref symbol   price\r\n0    1  GOOGL  585.93\r\n1    2    IBM  180.72\r\n2    3   MSFT   42.03\r\n\r\nIn [93]: df.dtypes\r\nOut[93]:\r\nref         int64\r\nsymbol     object\r\nprice     float64\r\ndtype: object\r\n```\r\nIf I aggregate over an object and then reset the index, everything comes back as expected:\r\n``` python\r\nIn [95]: df.groupby('symbol').first().reset_index()\r\nOut[95]:\r\n  symbol  ref   price\r\n0  GOOGL    1  585.93\r\n1    IBM    2  180.72\r\n2   MSFT    3   42.03\r\n```\r\nSimilarly with aggregating over an integer:\r\n``` python\r\nIn [96]: df.groupby('ref').first().reset_index()\r\nOut[96]:\r\n   ref symbol   price\r\n0    1  GOOGL  585.93\r\n1    2    IBM  180.72\r\n2    3   MSFT   42.03\r\n```\r\nNow let's say I have an empty DataFrame. Aggregating over an object produces what I expect:\r\n``` python\r\nIn [97]: df.query('price > 1000').groupby('symbol').first().reset_index()\r\nOut[97]:\r\nEmpty DataFrame\r\nColumns: [symbol, ref, price]\r\nIndex: []\r\n```\r\nBut aggregating over an integer gives me `index` as the column name instead of the expected `ref`!\r\n```python\r\nIn [98]: df.query('price > 1000').groupby('ref').first().reset_index()\r\nOut[98]:\r\nEmpty DataFrame\r\nColumns: [index, symbol, price]\r\nIndex: []\r\n          ^^^^^\r\n```\r\nInterestingly, setting the index in a non-aggregating function does the right thing:\r\n``` python\r\nIn [100]: df.query('price > 1000').set_index('ref').reset_index()\r\nOut[100]:\r\nEmpty DataFrame\r\nColumns: [ref, symbol, price]\r\nIndex: []\r\n```\r\nSo aggregating over an integer in an empty DataFrame removes the index name. This was discovered in pandas 0.14.0.\r\n"""
7572,36544184,jaimefrio,jreback,2014-06-26 04:10:41,2014-07-02 14:53:03,2014-07-02 11:46:29,closed,,0.14.1,8,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7572,"b'BUG: Error in rolling_var if window is larger than array, fixes #7297'","b'fixes #7297\r\n\r\nOther rolling window functions rely on the logic in `_check_minp`, and split the iteration into the ranges `[0, minp-1)` and `(minp-1, N)`. Because of the way `rolling_var` handles things, splitting the iteration into `[0, win)` and `(win, N)` makes more sense. Added also some comments as to what is going on elsewhere in the code.\r\n\r\nNo tests have been added, as @kdiether seems to have that very advanced.'"
7568,36476012,jreback,jreback,2014-06-25 12:52:08,2014-06-25 13:48:23,2014-06-25 13:48:23,closed,,0.14.1,7,Bug;Dtypes;Groupby;Performance,https://api.github.com/repos/pydata/pandas/issues/7568,b'BUG/PERF: perf issues in object groupby aggregations (GH7555)',"b""related #7555\r\n\r\n```\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\ngroupby_object_last                          |  15.2340 | 4175.6846 |   0.0036 |\r\ngroupby_object_first                         |  15.6957 | 4166.6920 |   0.0038 |\r\ngroupby_transform_ufunc                      |   5.4990 |   5.7740 |   0.9524 |\r\ngroupby_indices                              |   6.7443 |   7.0153 |   0.9614 |\r\ngroupby_multi_different_functions            |  12.6866 |  12.9040 |   0.9832 |\r\ngroupby_multi_series_op                      |  13.2560 |  13.4466 |   0.9858 |\r\ngroupby_multi_cython                         |  15.0924 |  15.2720 |   0.9882 |\r\ngroupby_multi_different_numpy_functions      |  11.1883 |  11.2376 |   0.9956 |\r\ngroupby_first_float32                        |   3.4907 |   3.4963 |   0.9984 |\r\ngroupby_last_float32                         |   3.6677 |   3.6586 |   1.0025 |\r\ngroupby_frame_apply_overhead                 |   9.2754 |   9.2440 |   1.0034 |\r\ngroupby_first                                |   3.5706 |   3.5577 |   1.0036 |\r\ngroupby_pivot_table                          |  17.2007 |  17.1343 |   1.0039 |\r\ngroupby_transform                            | 174.6823 | 173.8520 |   1.0048 |\r\ngroupby_frame_singlekey_integer              |   2.4533 |   2.4397 |   1.0056 |\r\ngroupby_frame_nth                            |   2.7863 |   2.7630 |   1.0085 |\r\ngroupby_simple_compress_timing               |  35.6836 |  35.3360 |   1.0098 |\r\ngroupby_sum_booleans                         |   1.2880 |   1.2727 |   1.0121 |\r\ngroupby_transform2                           | 162.5650 | 160.2323 |   1.0146 |\r\ngroupby_apply_dict_return                    |  39.7460 |  39.1597 |   1.0150 |\r\ngroupby_multi_size                           |  23.2656 |  22.9087 |   1.0156 |\r\ngroupby_datetimes_last                       |  12.0660 |  11.8797 |   1.0157 |\r\ngroupby_series_simple_cython                 | 192.2614 | 188.9360 |   1.0176 |\r\ngroupby_last                                 |   3.8017 |   3.7254 |   1.0205 |\r\ngroupby_int_count                            |   4.6777 |   4.5680 |   1.0240 |\r\ngroupby_mixed_first                          |  12.3363 |  12.0014 |   1.0279 |\r\ngroupby_frame_median                         |   7.6403 |   7.4034 |   1.0320 |\r\ngroupby_frame_apply                          |  45.0197 |  43.6123 |   1.0323 |\r\ngroupby_object_nth                           | 430.5693 | 416.2673 |   1.0344 |\r\ngroupby_datetimes_nth                        | 428.6557 | 414.3833 |   1.0344 |\r\ngroupby_frame_cython_many_columns            |   4.3843 |   4.2194 |   1.0391 |\r\ngroupby_multi_python                         | 142.0707 | 135.8123 |   1.0461 |\r\ngroupby_nth_float32                          |  63.5343 |  59.9463 |   1.0599 |\r\ngroupby_nth_float64                          |  64.0316 |  59.8179 |   1.0704 |\r\ngroupby_multi_count                          |   9.0540 |   8.2114 |   1.1026 |\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\n\r\nRatio < 1.0 means the target commit is faster then the baseline.\r\nSeed used: 1234\r\n\r\nTarget [e28ec0d] : BUG/PERF: perf issues in object groupby aggregations (GH7555)\r\nBase   [69bb0e8] : Merge pull request #7556 from onesandzeroes/expand-grid\r\n\r\nDOC: Cookbook recipe for emulating R's expand.grid() (#7426)\r\n\r\n```\r\n"""
7562,36390897,mjnicky,jreback,2014-06-24 14:46:40,2014-06-24 14:50:53,2014-06-24 14:50:40,closed,,0.14.1,1,Bug;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/7562,b'Timezone information lost after pandas.concat',"b""I found that when using pandas.concat on dataframes with timezone information, the timezone might be lost when using 'innder' join:\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: from pandas.util.print_versions import show_versions\r\n\r\nIn [4]: show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.2.0-0.bpo.4-amd64\r\nmachine: x86_64\r\nprocessor:\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US\r\n\r\npandas: 0.14.0\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2014.3\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\n\r\nIn [5]: dr1 = pd.date_range('20110101 0000','20110101 2359',freq='T',tz='Asia/Tokyo')\r\n\r\nIn [6]: dr2 = pd.date_range('20110101 0000','20110102 2359',freq='T',tz='Asia/Tokyo')\r\n\r\nIn [7]: df1 = pd.DataFrame(np.random.randn(5,1440),columns=dr1)\r\n\r\nIn [8]: df2 = pd.DataFrame(np.random.randn(5,2880),columns=dr2)\r\n\r\nIn [9]: outer = pd.concat([df1,df2],join='outer')\r\n\r\nIn [10]: inner = pd.concat([df1,df2],join='inner')\r\n\r\nIn [11]: print outer.axes[1]\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2011-01-01 00:00:00+09:00, ..., 2011-01-02 23:59:00+09:00]\r\nLength: 2880, Freq: T, Timezone: Asia/Tokyo\r\n\r\nIn [12]: print inner.axes[1]\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2010-12-31 15:00:00, ..., 2011-01-01 14:59:00]\r\nLength: 1440, Freq: T, Timezone: None\r\n```\r\nLooking at the output [11], the timezone information is kept after concat(), however, if we use 'inner' join, the timezone information is lost!"""
7559,36379492,jreback,jreback,2014-06-24 12:37:37,2014-06-30 11:05:58,2014-06-30 11:05:58,closed,,0.14.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7559,b'BUG: groupby-nth failure',"b'This is a vbench/groupby.py\r\n\r\n```\r\nIn [1]: df = DataFrame(np.random.randint(1, 100, (10000, 2)))\r\n\r\nIn [2]: df[1].groupby(df[0]).nth(0)\r\nIndexError: index 9999 is out of bounds for size 9999\r\n```'"
7558,36378515,janschulz,jreback,2014-06-24 12:23:01,2014-06-24 12:30:54,2014-06-24 12:30:34,closed,cpcloud,0.14.1,2,Bug;Duplicate;Missing-data;Numeric,https://api.github.com/repos/pydata/pandas/issues/7558,b'Series.min(skipna=True) includes nan/inf?',"b'\r\n```\r\nIn[31]: s2 = pd.Series([""a"",""z"",np.nan])\r\nIn[32]: s2.max()\r\nOut[32]: \'z\'\r\nIn[33]: s2.min()\r\nOut[33]: inf\r\nIn[34]: s2.min(skipna=True)\r\nOut[34]: inf\r\n```\r\n\r\nReading the docs I get the impression that it should return ""a"" in both cases (as skipna defaults to True).\r\n\r\nIf so, #7217 also needs to change.'"
7552,36308051,jreback,jreback,2014-06-23 16:37:05,2014-06-23 17:18:29,2014-06-23 17:18:29,closed,,0.14.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7552,b'BUG: Bug in setitem with list-of-lists and single vs mixed types (GH7551)',b'closes #7551 '
7551,36306798,jreback,jreback,2014-06-23 16:22:13,2014-06-23 17:18:29,2014-06-23 17:18:29,closed,,0.14.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7551,b'BUG: iloc setitem indexing with list of lists',"b""different results when mixed vs single dtypes\r\n\r\n```\r\nIn [1]: df = DataFrame(dict(A = np.arange(5), B = np.arange(5,10)))\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A  B\r\n0  0  5\r\n1  1  6\r\n2  2  7\r\n3  3  8\r\n4  4  9\r\n\r\nIn [4]: df.iloc[2:4] = [[10,11],[12,13]]\r\n\r\nIn [5]: df\r\nOut[5]: \r\n    A   B\r\n0   0   5\r\n1   1   6\r\n2  10  11\r\n3  12  13\r\n4   4   9\r\n```\r\n\r\n```\r\nIn [6]: df = DataFrame(dict(A = list('abcde'), B = np.arange(5,10)))\r\n\r\nIn [7]:  df.iloc[2:4] = [['x',11],['y',13]]\r\n\r\nIn [8]: df\r\nOut[8]: \r\n    A   B\r\n0   a   5\r\n1   b   6\r\n2   x   y\r\n3  11  13\r\n4   e   9\r\n```"""
7549,36252567,cpcloud,cpcloud,2014-06-22 21:13:48,2014-06-22 21:49:00,2014-06-22 21:48:58,closed,cpcloud,0.14.1,1,Bug;Network IO;Testing,https://api.github.com/repos/pydata/pandas/issues/7549,b'TST: some yahoo options tests missing network decorator',
7542,36230536,seth-p,jreback,2014-06-21 19:39:51,2014-07-25 14:34:07,2014-07-25 14:34:07,closed,,0.15.0,0,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7542,"b""BUG: _flex_binary_moment() doesn't preserve column order or handle multiple columns with the same label""","b""_flex_binary_moment() sorts the columns alphabetically. I think it should preserve the order.\r\n```\r\nIn [23]: df = DataFrame([[2,4],[1,2],[5,2],[8,1]], columns=['B','A'])\r\n\r\nIn [24]: expanding_corr(df, df, pairwise=True)[3]\r\nOut[24]:\r\n          A         B\r\nA  1.000000 -0.670166\r\nB -0.670166  1.000000\r\n```\r\n\r\n(I originally mentioned this in https://github.com/pydata/pandas/issues/7514, but then broke it out as a separate issue.)\r\n\r\nAlso, _flex_binary_moment() doesn't work if multiple columns have the same name. While it's not obvious to me what the outcome should be for two distinct DataFrames with pairwise=False, there's no reason things shouldn't work for all other cases (i.e. a DataFrame with itself with pairwise=True or False; two distinct DataFrames with pairwise=True; and a DataFrame with a Series).\r\n```\r\nIn [13]: df = DataFrame([[2,4],[1,2],[5,2],[8,1]], columns=['C','C'])\r\n\r\nIn [14]: expanding_corr(df, df, pairwise=True)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-14-f20d94c2d8dc> in <module>()\r\n----> 1 expanding_corr(df, df, pairwise=True)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in expanding_corr(arg1, arg2, min_periods, freq, center, pairwise)\r\n    940     return rolling_corr(arg1, arg2, window,\r\n    941                         min_periods=min_periods,\r\n--> 942                         freq=freq, center=center, pairwise=pairwise)\r\n    943\r\n    944\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in rolling_corr(arg1, arg2, window, min_periods, freq, center, pairwise, how)\r\n    246         return num / den\r\n    247\r\n--> 248     return _flex_binary_moment(arg1, arg2, _get_corr, pairwise=bool(pairwise))\r\n    249\r\n    250\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _flex_binary_moment(arg1, arg2, f, pairwise)\r\n    278                             results[k1][k2] = results[k2][k1]\r\n    279                         else:\r\n--> 280                             results[k1][k2] = f(*_prep_binary(arg1[k1], arg2[k2]))\r\n    281                 return Panel.from_dict(results).swapaxes('items', 'major')\r\n    282             else:\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _get_corr(a, b)\r\n    239         adj_window = min(window, len(a), len(b))\r\n    240         num = rolling_cov(a, b, adj_window, min_periods, freq=freq,\r\n--> 241                           center=center)\r\n    242         den = (rolling_std(a, adj_window, min_periods, freq=freq,\r\n    243                            center=center) *\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in rolling_cov(arg1, arg2, window, min_periods, freq, center, pairwise, how)\r\n    217         bias_adj = count / (count - 1)\r\n    218         return (mean(X * Y) - mean(X) * mean(Y)) * bias_adj\r\n--> 219     rs = _flex_binary_moment(arg1, arg2, _get_cov, pairwise=bool(pairwise))\r\n    220     return rs\r\n    221\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\stats\\moments.py in _flex_binary_moment(arg1, arg2, f, pairwise)\r\n    290                 results[col] = f(X[col], Y)\r\n    291\r\n--> 292         return DataFrame(results, index=X.index, columns=res_columns)\r\n    293     else:\r\n    294         return _flex_binary_moment(arg2, arg1, f)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self, data, index, columns, dtype, copy)\r\n    201                                  dtype=dtype, copy=copy)\r\n    202         elif isinstance(data, dict):\r\n--> 203             mgr = self._init_dict(data, index, columns, dtype=dtype)\r\n    204         elif isinstance(data, ma.MaskedArray):\r\n    205             import numpy.ma.mrecords as mrecords\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\frame.py in _init_dict(self, data, index, columns, dtype)\r\n    325\r\n    326         return _arrays_to_mgr(arrays, data_names, index, columns,\r\n--> 327                               dtype=dtype)\r\n    328\r\n    329     def _init_ndarray(self, values, index, columns, dtype=None,\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\frame.py in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\r\n   4623\r\n   4624     # don't force copy because getting jammed in an ndarray anyway\r\n-> 4625     arrays = _homogenize(arrays, index, dtype)\r\n   4626\r\n   4627     # from BlockManager perspective\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\frame.py in _homogenize(data, index, dtype)\r\n   4932\r\n   4933             v = _sanitize_array(v, index, dtype=dtype, copy=False,\r\n-> 4934                                 raise_cast_failure=False)\r\n   4935\r\n   4936         homogenized.append(v)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\series.py in _sanitize_array(data, index, dtype, copy, raise_cast_failure)\r\n   2507             raise Exception('Data must be 1-dimensional')\r\n   2508         else:\r\n-> 2509             subarr = _asarray_tuplesafe(data, dtype=dtype)\r\n   2510\r\n   2511     # This is to prevent mixed-type Series getting all casted to\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\common.py in _asarray_tuplesafe(values, dtype)\r\n   2191             except ValueError:\r\n   2192                 # we have a list-of-list\r\n-> 2193                 result[:] = [tuple(x) for x in values]\r\n   2194\r\n   2195     return result\r\n\r\nValueError: cannot copy sequence with size 2 to array axis with dimension 4\r\n```"""
7541,36230320,seth-p,seth-p,2014-06-21 19:27:09,2014-06-28 13:54:28,2014-06-28 13:54:28,closed,,,4,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7541,"b'BUG: expanding_{cov,corr} for objects of different lengths'",b'Closes https://github.com/pydata/pandas/issues/7512 and https://github.com/pydata/pandas/issues/7514.'
7534,36219095,sinhrks,jreback,2014-06-21 08:01:17,2014-06-21 14:47:49,2014-06-21 13:41:19,closed,,0.14.1,3,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/7534,b'BUG: Timestamp.tz_localize resets nanosecond',"b""``Timestamp.tz_localize`` resets ``nanosecond``.\r\n\r\n```\r\nt = pd.Timestamp('2011-01-01') + pd.offsets.Nano(1)\r\nt.tz_localize('US/Eastern')\r\n# Warning: discarding nonzero nanoseconds\r\n# 2011-01-01 00:00:00-05:00\r\n```\r\nEven though ``DatetimeIndex.tz_convert`` can preserve it.\r\n```\r\nidx = pd.date_range('3/11/2012 04:00', periods=10, freq='N')\r\nidx.tz_localize('US/Eastern')\r\n# <class 'pandas.tseries.index.DatetimeIndex'>\r\n# [2012-03-11 04:00:00-04:00, ..., 2012-03-11 04:00:00.000000009-04:00]\r\n# Length: 10, Freq: N, Timezone: US/Eastern\r\n```"""
7533,36219000,sinhrks,jreback,2014-06-21 07:51:17,2014-06-22 00:56:59,2014-06-21 20:10:21,closed,,0.14.1,4,Bug;MultiIndex;Timezones,https://api.github.com/repos/pydata/pandas/issues/7533,b'BUG: df.reset_index loses tz',b'Closes #3950.'
7532,36218395,sinhrks,jreback,2014-06-21 07:04:50,2014-07-02 16:46:04,2014-07-01 15:29:53,closed,,0.14.1,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7532,b'BUG: single column bar plot is misaligned',b'Closes #7498.'
7529,36199958,sinhrks,jreback,2014-06-20 20:28:40,2014-06-21 02:39:07,2014-06-20 21:38:59,closed,,0.14.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7529,b'BUG: DatetimeIndex comparison handles NaT incorrectly',"b""Derived from #7485. Comparison with ``DatetimeIndex`` result incorrectly if ``NaT`` is included.\r\n\r\n```\r\nidx1 = pd.DatetimeIndex(['2011-01-01', pd.NaT], freq='M')\r\nidx2 = pd.DatetimeIndex([pd.NaT, '2011-01-01'], freq='M')\r\nprint(idx1 > idx2)\r\n# [True, False]\r\n# it must be [False, False]\r\n```\r\n"""
7528,36198480,gdtroszak,TomAugspurger,2014-06-20 20:06:33,2014-09-14 07:24:58,2014-08-30 14:11:57,closed,,0.15.0,7,API Design;Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7528,"b""BUG: Bad axis limits when using DataFrame.boxplot() with odd number of columns (excluding 1) and 'by' parameter.""","b""The head of my Pandas dataframe , `df`, is shown below:\r\n```\r\n      count1  count2  totalcount  season\r\n    0       3      13          16       1\r\n    1       8      32          40       1\r\n    2       5      27          32       1\r\n    3       3      10          13       1\r\n    4       0       1           1       1\r\n```\r\nI'd like to make boxplots of `count1`, `count2`, and `totalcount`, grouped by `season` (there are 4 seasons) and have each set of box plots show up on their own subplot in a single figure.\r\n\r\nWhen I do this with only two of the columns, say `count1` and `count2`, everything looks great.\r\n\r\n    df.boxplot(['count1', 'count2'], by='season')\r\n![enter image description here][1]\r\n\r\nBut when I add `totalcount` to the mix, the axis limits go haywire.\r\n\r\n    df.boxplot(['count1', 'count2', 'totalcount'], by='season')\r\n![enter image description here][2]\r\n\r\nThis happens irregardless of the order of the columns. I realize there are several ways around this problem, but it would be much more convenient if this worked properly. I believe this worked as expected in Pandas 0.13.1.\r\n\r\nOutput from `pd.show_versions()`\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 13.2.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.0\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 2.1.0\r\nsphinx: None\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.4\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: 2.5.3 (dt dec pq3 ext)\r\n```\r\n\r\n  [1]: http://i.stack.imgur.com/1Jbav.png\r\n  [2]: http://i.stack.imgur.com/Yv74U.png"""
7519,36132225,jreback,jreback,2014-06-20 00:51:47,2014-06-20 12:04:29,2014-06-20 12:04:29,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7519,b'BUG: Bug in Panel indexing with a multi-index axis (GH 7516)',b'closes #7516 '
7516,36115960,seth-p,jreback,2014-06-19 20:44:59,2014-06-20 12:04:29,2014-06-20 12:04:29,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7516,b'BUG: Panel.iloc[] bug with MultiIndex axis (another one)',"b'I\'d expect the [7] to produce the same result as [6], but instead it gives an error.\r\n\r\n```\r\nPython 3.4.0 (v3.4.0:04f714765c13, Mar 16 2014, 19:25:23) [MSC v.1600 64 bit (AMD64)]\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: from numpy import zeros\r\n\r\nIn [2]: from pandas import Panel, MultiIndex\r\n\r\nIn [3]: p1 = Panel(zeros((3,3,3)), items=[\'a\',\'b\',\'c\'], major_axis=[\'x\',\'y\',\'z\'], minor_axis=[\'u\',\'v\',\'w\'])\r\n\r\nIn [4]: mi = MultiIndex.from_tuples([(0,\'x\'), (1,\'y\'), (1,\'z\')])\r\n\r\nIn [5]: p2 = Panel(zeros((3,3,3)), items=[\'a\',\'b\',\'c\'], major_axis=mi, minor_axis=[\'u\',\'v\',\'w\'])\r\n\r\nIn [6]: p1.iloc[:, 1, 0]\r\nOut[6]:\r\na    0\r\nb    0\r\nc    0\r\nName: u, dtype: float64\r\n\r\nIn [7]: p2.iloc[:, 1, 0]\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-f7c04712ad2b> in <module>()\r\n----> 1 p2.iloc[:, 1, 0]\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key)\r\n   1125     def __getitem__(self, key):\r\n   1126         if type(key) is tuple:\r\n-> 1127             return self._getitem_tuple(key)\r\n   1128         else:\r\n   1129             return self._getitem_axis(key, axis=0)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_tuple(self, tup)\r\n   1332                 continue\r\n   1333\r\n-> 1334             retval = getattr(retval, self.name)._getitem_axis(key, axis=axis)\r\n   1335\r\n   1336             # if the dim was reduced, then pass a lower-dim the next time\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis, validate_iterable)\r\n   1386                 self._is_valid_integer(key, axis)\r\n   1387\r\n-> 1388             return self._get_loc(key, axis=axis)\r\n   1389\r\n   1390     def _convert_to_indexer(self, obj, axis=0, is_setter=False):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _get_loc(self, key, axis)\r\n     87\r\n     88     def _get_loc(self, key, axis=0):\r\n---> 89         return self.obj._ixs(key, axis=axis)\r\n     90\r\n     91     def _slice(self, obj, axis=0, typ=None):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\panel.py in _ixs(self, i, axis)\r\n    803         if _is_list_like(key):\r\n    804             indexer = {self._get_axis_name(axis): key}\r\n--> 805             return self.reindex(**indexer)\r\n    806\r\n    807         # a reduction\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\panel.py in reindex(self, items, major_axis, minor_axis, **kwargs)\r\n   1109                       else kwargs.pop(\'minor\', None))\r\n   1110         return super(Panel, self).reindex(items=items, major_axis=major_axis,\r\n-> 1111                                           minor_axis=minor_axis, **kwargs)\r\n   1112\r\n   1113     @Appender(_shared_docs[\'rename\'] % _shared_doc_kwargs)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\generic.py in reindex(self, *args, **kwargs)\r\n   1622         # perform the reindex on the axes\r\n   1623         return self._reindex_axes(axes, level, limit,\r\n-> 1624                                   method, fill_value, copy).__finalize__(self)\r\n   1625\r\n   1626     def _reindex_axes(self, axes, level, limit, method, fill_value, copy):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\generic.py in _reindex_axes(self, axes, level, limit, method, fill_value, copy)\r\n   1639             ax = self._get_axis(a)\r\n   1640             new_index, indexer = ax.reindex(\r\n-> 1641                 labels, level=level, limit=limit, method=method)\r\n   1642\r\n   1643             obj = obj._reindex_with_indexers(\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\index.py in reindex(self, target, method, level, limit, copy_if_needed)\r\n   3256             else:\r\n   3257                 # hopefully?\r\n-> 3258                 target = MultiIndex.from_tuples(target)\r\n   3259\r\n   3260         return target, indexer\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\index.py in from_tuples(cls, tuples, sortorder, names)\r\n   2809                 tuples = tuples.values\r\n   2810\r\n-> 2811             arrays = list(lib.tuples_to_object_array(tuples).T)\r\n   2812         elif isinstance(tuples, list):\r\n   2813             arrays = list(lib.to_object_array_tuples(tuples).T)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.tuples_to_object_array (pandas\\lib.c:52859)()\r\n\r\nTypeError: object of type \'numpy.int64\' has no len()\r\n```'"
7515,36110874,sinhrks,jreback,2014-06-19 19:41:30,2014-07-05 04:46:54,2014-07-05 01:31:30,closed,,0.14.1,3,Bug;Timezones;Visualization,https://api.github.com/repos/pydata/pandas/issues/7515,b'BUG: area plot raises ValueError with tz-aware data',b'Closes #7471.\r\n\r\nMust be revisit after #7322.'
7514,36110467,seth-p,seth-p,2014-06-19 19:36:24,2014-06-28 17:29:42,2014-06-28 17:29:42,closed,,0.16.0,8,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7514,b'BUG: _flex_binary_moment() problems with different index sets',"b""There are several issues with _flex_binary_moment().\r\n\r\nA. When pairwise=True, it doesn't properly handle two DataFrames with different index sets. In the following example, I believe [6], [7], and [8] should all produce the result in [9].\r\n```\r\nIn [1]: from pandas import DataFrame, expanding_corr\r\n\r\nIn [2]: df1 = DataFrame([[1,2], [3, 2], [3,4]], columns=['A','B'])\r\n\r\nIn [3]: df1a = DataFrame([[1,2], [3,4]], columns=['A','B'], index=[0,2])\r\n\r\nIn [4]: df2 = DataFrame([[5,6], [None,None], [2,1]], columns=['X','Y'])\r\n\r\nIn [5]: df2a = DataFrame([[5,6], [2,1]], columns=['X','Y'], index=[0,2])\r\n\r\nIn [6]: expanding_corr(df1, df2, pairwise=True)[2]\r\nOut[6]:\r\n          X         Y\r\nA -1.224745 -1.224745\r\nB -1.224745 -1.224745\r\n\r\nIn [7]: expanding_corr(df1, df2a, pairwise=True)[2]\r\nOut[7]:\r\n    X   Y\r\nA NaN NaN\r\nB NaN NaN\r\n\r\nIn [8]: expanding_corr(df1a, df2, pairwise=True)[2]\r\nOut[8]:\r\n    X   Y\r\nA NaN NaN\r\nB NaN NaN\r\n\r\nIn [9]: expanding_corr(df1a, df2a, pairwise=True)[2]\r\nOut[9]:\r\n   X  Y\r\nA -1 -1\r\nB -1 -1\r\n```\r\n"""
7512,36098746,seth-p,jreback,2014-06-19 17:16:44,2014-07-01 10:14:18,2014-07-01 10:14:18,closed,,0.14.1,4,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7512,"b'BUG: {expanding,rolling}_{cov,corr} functions between objects with different index sets'","b""related #7514\r\n\r\nThere appears to be a bug in the expanding_{cov,corr} functions when dealing with two objects with different indexes.\r\n\r\nFirst, there is a problem with series. See example below, where I would expect expanding_corr(s1, s2) to produce the result produced by expanding_corr(s1, s2a).\r\n\r\nThe problem is due to the fact that expanding_corr is implemented in terms of rolling_corr with window = max(len(arg1), len(arg2)), but then rolling_corr resets window to window = min(window, len(arg1), len(arg2)). The end result is that window = min(len(arg1), len(arg2)) -- and these are the raw, unaligned arg1 and arg2. Thus in the expanding_corr(s1, s2) example below, window=2, and so when calculating the third row (index=2) it tries to calculate the correlation between [2, 3] and [NaN, 3], producing NaN -- rather than calculating the correlation between [1, 2, 3] and [1, Nan, 3] and producing 1.\r\n\r\nThe solution would appear to be simply deleting the window = min(window, len(arg1), len(arg2)) line from rolling_cov and rolling_corr, as I believe the rolling_* functions run fine with a window larger than the data, or at least replacing it with window = min(window, max(len(arg1), len(arg2))).\r\n```\r\nIn [1]: from pandas import Series, expanding_corr\r\n\r\nIn [2]: s1 = Series([1, 2, 3], index=[0, 1, 2])\r\n\r\nIn [3]: s2 = Series([1, 3], index=[0, 2])\r\n\r\nIn [4]: expanding_corr(s1, s2)\r\nOut[4]:\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\ndtype: float64\r\n\r\nIn [5]: s2a = Series([1, None, 3], index=[0, 1, 2])\r\n\r\nIn [6]: expanding_corr(s1, s2a)\r\nOut[6]:\r\n0   NaN\r\n1   NaN\r\n2     1\r\ndtype: float64\r\n```\r\n\r\n\r\nNext, there is a problem with data frames. [This was originally reported separately in https://github.com/pydata/pandas/issues/7512, but I've merged it into this issue.]\r\n\r\nThe problem is with with _flex_binary_moment(). When pairwise=True, it doesn't properly handle two DataFrames with different index sets. In the following example, I believe [6], [7], and [8] should all produce the result in [9].\r\n```\r\nIn [1]: from pandas import DataFrame, expanding_corr\r\n\r\nIn [2]: df1 = DataFrame([[1,2], [3, 2], [3,4]], columns=['A','B'])\r\n\r\nIn [3]: df1a = DataFrame([[1,2], [3,4]], columns=['A','B'], index=[0,2])\r\n\r\nIn [4]: df2 = DataFrame([[5,6], [None,None], [2,1]], columns=['X','Y'])\r\n\r\nIn [5]: df2a = DataFrame([[5,6], [2,1]], columns=['X','Y'], index=[0,2])\r\n\r\nIn [6]: expanding_corr(df1, df2, pairwise=True)[2]\r\nOut[6]:\r\n          X         Y\r\nA -1.224745 -1.224745\r\nB -1.224745 -1.224745\r\n\r\nIn [7]: expanding_corr(df1, df2a, pairwise=True)[2]\r\nOut[7]:\r\n    X   Y\r\nA NaN NaN\r\nB NaN NaN\r\n\r\nIn [8]: expanding_corr(df1a, df2, pairwise=True)[2]\r\nOut[8]:\r\n    X   Y\r\nA NaN NaN\r\nB NaN NaN\r\n\r\nIn [9]: expanding_corr(df1a, df2a, pairwise=True)[2]\r\nOut[9]:\r\n   X  Y\r\nA -1 -1\r\nB -1 -1\r\n```\r\n\r\nAnd there are similar problems with rolling_cov and rolling_corr. For example, continuing with the previous example, [77], [78], and [79] should give the same result as [80].\r\n```\r\nIn [77]: rolling_corr(df1, df2, window=3, pairwise=True, min_periods=2)[2]\r\nOut[77]:\r\n          X         Y\r\nA -1.224745 -1.224745\r\nB -1.224745 -1.224745\r\n\r\nIn [78]: rolling_corr(df1, df2a, window=3, pairwise=True, min_periods=2)[2]\r\nOut[78]:\r\n    X   Y\r\nA NaN NaN\r\nB NaN NaN\r\n\r\nIn [79]: rolling_corr(df1a, df2, window=3, pairwise=True, min_periods=2)[2]\r\nOut[79]:\r\n    X   Y\r\nA NaN NaN\r\nB NaN NaN\r\n\r\nIn [80]: rolling_corr(df1a, df2a, window=3, pairwise=True, min_periods=2)[2]\r\nOut[80]:\r\n   X  Y\r\nA -1 -1\r\nB -1 -1\r\n```\r\n"""
7508,36079905,jorisvandenbossche,jreback,2014-06-19 13:51:00,2015-03-17 00:18:39,2015-03-17 00:18:39,closed,,0.16.0,11,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7508,b'Incorrect truncating of Series according to max_rows',"b'Series is not correctly truncated according to the max_rows (number of rows is halved).\r\n```\r\nIn [146]: s = pd.Series(np.random.randn(20))\r\n\r\nIn [147]: pd.options.display.max_rows = 10\r\n\r\nIn [148]: s\r\nOut[148]:\r\n0    1.545202\r\n1   -1.427565\r\n2   -0.961094\r\n...\r\n17    0.125228\r\n18    2.153724\r\n19   -0.384024\r\nLength: 20, dtype: float64\r\n\r\nIn [149]: s.to_frame()\r\nOut[149]:\r\n           0\r\n0   1.545202\r\n1  -1.427565\r\n2  -0.961094\r\n3   0.387392\r\n4   1.036472\r\n..       ...\r\n15  0.918433\r\n16 -0.874253\r\n17  0.125228\r\n18  2.153724\r\n19 -0.384024\r\n\r\n[20 rows x 1 columns]\r\n```\r\n\r\n '"
7506,36040109,jreback,jreback,2014-06-19 00:23:26,2014-06-19 09:53:31,2014-06-19 09:53:31,closed,,0.14.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7506,b'BUG: Bug in DataFrame.where with a symmetric shaped frame and a passed other of a DataFrame',b'from SO: http://stackoverflow.com/questions/24296480/pandas-dataframe-where-misbehaving'
7505,36034733,benjwadams,jreback,2014-06-18 22:40:48,2014-06-30 19:38:12,2014-06-30 19:38:04,closed,,0.14.1,5,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7505,"b'BUG, TST: Fix pandas.core.strings.str_contains when handling regex=False and case=False'","b'`pandas.core.strings.str_contains` does not match in a case insensitive fashion *at all* when given `regex=False` and `case=False`.  This PR should fix the situation.  Additionally, there is test coverage for `pandas.core.strings.str_contains` case insensitive matching both with and without regular expressions enabled.'"
7503,36030176,jreback,jreback,2014-06-18 21:37:53,2014-06-19 00:01:34,2014-06-19 00:01:34,closed,,0.14.1,0,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7503,b'BUG: Bug in timeops with non-aligned Series (GH7500)',b'closes #7500 '
7502,36026249,sinhrks,jreback,2014-06-18 20:50:11,2014-06-21 22:30:16,2014-06-21 20:13:29,closed,,0.14.1,1,API Design;Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7502,b'BUG: offsets.apply may return datetime',"b""Currently, ``offsets.appy``, ``rollforward`` and ``rollback`` returns `Timestamp` if argument is `Timestamp` or ``np.datetime64``. \r\n\r\nIf input is ``datetime``, these functions return ``datetime`` or ``Timestamp`` inconsistently depending on internal process. It may better to always return ``Timestanp``? \r\n\r\n### Affected Offsets\r\n- 'pandas.tseries.offsets.Day'\r\n- 'pandas.tseries.offsets.MonthBegin'\r\n- 'pandas.tseries.offsets.FY5253Quarter'\r\n- 'pandas.tseries.offsets.FY5253'\r\n- 'pandas.tseries.offsets.Week'\r\n- 'pandas.tseries.offsets.WeekOfMonth'\r\n- 'pandas.tseries.offsets.Easter'\r\n- 'pandas.tseries.offsets.Hour'\r\n- 'pandas.tseries.offsets.Minute'\r\n- 'pandas.tseries.offsets.Second'\r\n- 'pandas.tseries.offsets.Milli'\r\n- 'pandas.tseries.offsets.Micro'"""
7500,36017989,aullrich2013,jreback,2014-06-18 19:15:25,2014-06-19 00:01:48,2014-06-19 00:01:34,closed,,0.14.1,3,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7500,"b""Possible Bug - math on like-indexed datetime series doesn't work as expected""","b""![firstordernoteval](https://cloud.githubusercontent.com/assets/4734283/3319074/b707a340-f71c-11e3-942f-c06129ab246f.jpg)\r\n![firstevalorder](https://cloud.githubusercontent.com/assets/4734283/3319075/bb0908d0-f71c-11e3-828f-2dd601125732.jpg)\r\n\r\nI have two series that are like-indexed datetimes. I'm trying to do simple math operations on them and noticed the results don't match what I'd expect. Specifically, subtracting one datetime from the other doesn't always result in subtraction across the aligned indices. Transforming the series to a dataframe with a dummy column gets us closer but the type manipulation isn't correct. \r\n\r\n``` python\r\nprint firstOrderNotEval.loc[site]\r\nprint firstEvalOrder.loc[site]\r\nprint type(firstOrderNotEval.loc[site])\r\nprint type(firstEvalOrder.loc[site])\r\n\r\n### output:\r\n# 2008-08-21 00:00:00\r\n# 2013-09-10 00:00:00\r\n# <class 'pandas.tslib.Timestamp'>\r\n# <class 'pandas.tslib.Timestamp'>\r\n\r\ntimeToFirstNonEvalPurchase_doesntWork = ((firstOrderNotEval - firstEvalOrder)/np.timedelta64(1,'D'))\r\ntimeToFirstNonEvalPurchase = ((firstOrderNotEval.to_frame('a') - firstEvalOrder.to_frame('a'))/np.timedelta64(1,'D'))['a']\r\n\r\nprint timeToFirstNonEvalPurchase_doesntWork.loc[2898717]\r\nprint timeToFirstNonEvalPurchase.loc[2898717]\r\n\r\n### output:\r\n# nan\r\n# -1846 nanoseconds # note should be 1846 days\r\n\r\n```\r\n\r\nSubtracting individual elements gives the correct result but as a datetime.timedelta type. subtracting the series directly gives NaT: \r\n``` python\r\nsite = 2898717   \r\nprint (firstOrderNotEval.loc[site] - firstEvalOrder.loc[site])\r\nprint type(firstOrderNotEval.loc[site] - firstEvalOrder.loc[site])\r\nprint (firstOrderNotEval - firstEvalOrder).loc[site]\r\n### output:\r\n# -1846 days, 0:00:00\r\n# <type 'datetime.timedelta'>\r\n# NaT\r\n```\r\n\r\nPerhaps this has to do with the timestamp type itself given the following example:\r\n``` python\r\nprint (firstOrderNotEval.to_frame('a') - firstEvalOrder.to_frame('a')).loc[site]/np.timedelta64(1,'D')\r\nprint ((firstOrderNotEval.to_frame('a') - firstEvalOrder.to_frame('a'))/np.timedelta64(1,'D')).loc[site]\r\n\r\n### output:\r\n# a   -1846\r\n# Name: 2898717.0, dtype: float64\r\n# a   -00:00:00.000002\r\n# Name: 2898717.0, dtype: timedelta64[ns]\r\n```\r\n\r\nNote that the following have different results based on how the divide by timedelta64 is performed:\r\n``` python\r\ntmp = ((firstOrderNotEval.to_frame('a') - firstEvalOrder.to_frame('a')))\r\nprint (tmp/np.timedelta64(1,'D')).loc[site]\r\nprint tmp.apply(lambda x: x/np.timedelta64(1,'D')).loc[site]\r\n### output:\r\n# a   -00:00:00.000002\r\n# Name: 2898717.0, dtype: timedelta64[ns]\r\n### what we'd expect: \r\n# a   -1846\r\n# Name: 2898717.0, dtype: float64\r\n```\r\nThe attached pickle files (as .jpg) include the series used in this example\r\n``` python\r\nfirstOrderNotEval.to_pickle('./firstOrderNotEval.jpg')\r\nfirstEvalOrder.to_pickle('./firstEvalOrder.jpg')\r\n```"""
7498,35999914,fonnesbeck,jreback,2014-06-18 15:55:24,2014-07-01 15:29:53,2014-07-01 15:29:53,closed,,0.14.1,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7498,b'Bar plots are misaligned by default',"b""There has been a recent change in bar plot default behavior that is causing misalignment of the bars on the y-axis. I'm not sure when this change occurred, but I will estimate 3-4 months ago. \r\n\r\nPreviously, bar plots looked like this:\r\n\r\n![good bar plot](http://d.pr/i/c2mO+)\r\n\r\nHowever, the same code under '0.14.0-205-gbbde837' yields the following (ignore the theme differences):\r\n\r\n![bad bar plot](http://d.pr/i/uoOf+)\r\n\r\nSo, the bars are bizarrely pushed to the right. Is matplotlib to blame for this (running 1.4.x)? \r\n\r\nRunning Python 2.7.6 (homebrew) on OS X 10.9.3."""
7497,35997249,jreback,jreback,2014-06-18 15:29:24,2014-06-19 09:53:10,2014-06-19 09:53:10,closed,,0.14.1,3,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7497,b'BUG: Bug in .loc performing fallback integer indexing with object dtype indices (GH7496)',"b""closes #7496\r\n\r\n```\r\n\r\n\r\nIn [4]: s = Series()\r\n\r\nIn [5]: s.loc[1] = 1\r\n\r\nIn [6]: s.loc['a'] = 2\r\n\r\nIn [7]: s.loc[-1]\r\nKeyError: 'the label [-1] is not in the [index]'\r\n\r\nIn [8]: s.loc[[-1, -2]]\r\nOut[8]: \r\n-1   NaN\r\n-2   NaN\r\ndtype: float64\r\n\r\nIn [9]: s.loc[['4']]\r\nOut[9]: \r\n4   NaN\r\ndtype: float64\r\n\r\nIn [10]: s.loc[-1] = 3\r\n\r\nIn [11]: s.loc[[-1,-2]]\r\nOut[11]: \r\n-1     3\r\n-2   NaN\r\ndtype: float64\r\n\r\nIn [12]: s['a'] = 2\r\n\r\nIn [13]: s.loc[[-2]]\r\nOut[13]: \r\n-2   NaN\r\ndtype: float64\r\n\r\nIn [14]: del s['a']\r\n\r\nIn [15]: s.loc[[-2]] = 0\r\nKeyError: '[-2] not in index'\r\n```\r\n"""
7496,35994959,podshumok,jreback,2014-06-18 15:08:06,2014-06-19 09:53:10,2014-06-19 09:53:10,closed,,0.14.1,3,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7496,b'BUG: strange behavior of .loc indexer: falling back to integer-based indexing with list ',"b'    Python 2.7.6 (default, Mar 22 2014, 22:59:56) \r\n    [GCC 4.8.2] on linux2\r\n    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n    >>> import pandas\r\n    >>> a = pandas.Series()\r\n    >>> a.loc[1] = 1\r\n    >>> a.loc[\'a\'] = 2\r\n    >>> a.loc[[-1, -2]]\r\n    1    1\r\n    a    2\r\n    dtype: int64\r\n\r\nThis is ok:\r\n\r\n    >>> a.loc[-1]\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py"", line 1129, in __getitem__\r\n        return self._getitem_axis(key, axis=0)\r\n      File ""/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py"", line 1261, in _getitem_axis\r\n        self._has_valid_type(key, axis)\r\n      File ""/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py"", line 1234, in _has_valid_type\r\n        error()\r\n      File ""/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py"", line 1221, in error\r\n        (key, self.obj._get_axis_name(axis)))\r\n    KeyError: \'the label [-1] is not in the [index]\'\r\n\r\nThis is ok too:\r\n\r\n    >>> a.loc[[\'W\']]\r\n    W   NaN\r\n    dtype: float64\r\n\r\nBut this is not:\r\n\r\n    >>> a.loc[-1] = 3\r\n    >>> a.loc[[-1, -2]]\r\n    -1    3\r\n    1    1\r\n    dtype: int64\r\n\r\nAnd this is not good at all:\r\n\r\n    >>> a\r\n    1    1\r\n    -1    3\r\n    dtype: int64\r\n    >>> a[\'a\'] = 2\r\n    >>> a\r\n    1     1\r\n    -1    3\r\n    a     2\r\n    dtype: int64\r\n    >>> a.loc[[-2]] = 0\r\n    >>> a\r\n    1     0\r\n    -1    3\r\n    a     2\r\n    dtype: int64\r\n\r\nWithout `\'a\'` string in the index it raises while I expect new item (`{-2: 0}`) to be added\r\n\r\n    >>> del a[\'a\']\r\n    >>> a.loc[[-2]] = 0\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py"", line 117, in __setitem__\r\n        indexer = self._convert_to_indexer(key, is_setter=True)\r\n      File ""/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py"", line 1068, in _convert_to_indexer\r\n        raise KeyError(\'%s not in index\' % objarr[mask])\r\n    KeyError: \'[-2] not in index\'\r\n'"
7485,35904823,sinhrks,jreback,2014-06-17 16:04:40,2014-06-20 14:45:55,2014-06-19 19:27:10,closed,,0.14.1,3,Bug;Missing-data;Period,https://api.github.com/repos/pydata/pandas/issues/7485,b'ENH/BUG: Period/PeriodIndex supports NaT',b'Closes #7228. Closes #4731.\r\n\r\n`Period` and `PeriodIndex` now can contain `NaT` using `iNaT` as its internal value.\r\n\r\n'
7481,35852902,cpcloud,jreback,2014-06-17 02:13:49,2015-01-25 23:30:05,2015-01-25 23:30:05,closed,cpcloud,0.16.0,6,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7481,b'BUG/WIP: fix pivot with nan indexes',"b""closes #7466\n\n- [ ] vbench\n- [ ] make sure there isn't a better way to do this"""
7479,35851231,dsm054,jreback,2014-06-17 01:32:57,2014-06-17 11:23:03,2014-06-17 11:22:59,closed,,0.14.1,4,Bug;Missing-data;Testing,https://api.github.com/repos/pydata/pandas/issues/7479,b'BUG: Make copies of certain interpolate arguments (GH7295)',"b""closes #7295\r\nOn the alt_methods branch in interpolate_1d, make copies if x, y, and new_x don't have their `writeable` attribute set.  It seems a little weird that we need to do this, but it'll work around the issue so that the test can pass, anyway."""
7476,35817860,jreback,jreback,2014-06-16 17:29:55,2014-08-05 21:50:04,2014-08-05 15:43:12,closed,,0.16.0,2,Bug;Enhancement;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7476,b'BUG: Bug Multiindex assignment with an aligned rhs (but at different levels) (GH7475)',b'closes #7475'
7474,35795689,jreback,jreback,2014-06-16 13:26:53,2014-06-16 14:20:06,2014-06-16 14:20:06,closed,,0.14.1,0,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7474,b'BUG: Bug in Panel.apply with a multi-index as an axis (GH7469)',b'closes #7469'
7471,35770444,balzer82,jreback,2014-06-16 06:20:12,2014-07-08 14:16:27,2014-07-05 01:31:30,closed,,0.14.1,6,Bug;Timezones;Visualization,https://api.github.com/repos/pydata/pandas/issues/7471,"b""kind='area' plot not showing""","b""Maybe it is a bug, but following problem: In the new Pandas 0.14 you guys implemented an awesome kind='area' option for plot. If I plot a dataframe normally, like\r\n\r\n`df.plot()`\r\n\r\n![](http://i.stack.imgur.com/4voj8.png)\r\n\r\nbut when I take the option\r\n\r\n`df.plot(kind='area')`, I just get\r\n\r\n```Out[69]:\r\n<matplotlib.axes.AxesSubplot at 0x122794550>\r\n<matplotlib.figure.Figure at 0x121b69e10>\r\n```\r\n\r\nin my IPython notebook. I used the `%pylab inline` function, so I don't know, why it is not showing. Any suggestions?"""
7469,35761816,colindickson,jreback,2014-06-16 00:33:42,2014-06-16 14:45:19,2014-06-16 14:20:06,closed,,0.14.1,4,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7469,b'Panel.apply fails when one of the axes is a Multi-Indexed DataFrame',b'Wrote up an example notebook:\r\n\r\nhttp://nbviewer.ipython.org/urls/gist.githubusercontent.com/colindickson/850d3b5172f0320f204a/raw/62e4dfd450964326b8e3c2635c5c3f4c098a7352/panel.ipynb\r\n'
7466,35738423,cpcloud,jreback,2014-06-14 22:55:00,2014-12-22 13:08:53,2014-12-22 13:08:53,closed,,0.16.0,4,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7466,b'BUG: pivot with nans gives a seemingly wrong result',"b'related to #3588\r\n\r\nThis test\r\n\r\n```python\r\ndef test_pivot_index_with_nan(self):\r\n    # GH 3588\r\n    nan = np.nan\r\n    df = DataFrame({""a"":[\'R1\', \'R2\', nan, \'R4\'], \'b\':[""C1"", ""C2"", ""C3"" , ""C4""], ""c"":[10, 15, nan , 20]})\r\n    result = df.pivot(\'a\',\'b\',\'c\')\r\n    expected = DataFrame([[nan,nan,nan,nan],[nan,10,nan,nan],\r\n                            [nan,nan,nan,nan],[nan,nan,15,20]],\r\n                            index = Index([\'R1\',\'R2\',nan,\'R4\'],name=\'a\'),\r\n                            columns = Index([\'C1\',\'C2\',\'C3\',\'C4\'],name=\'b\'))\r\n    tm.assert_frame_equal(result, expected)\r\n```\r\nseems very odd to me, even though the expected result is constructed by hand.\r\n\r\nHere\'s `df` and `result`:\r\n\r\n```\r\nIn [2]: df\r\nOut[2]:\r\n     a   b   c\r\n0   R1  C1  10\r\n1   R2  C2  15\r\n2  NaN  C3 NaN\r\n3   R4  C4  20\r\n\r\nIn [3]: result\r\nOut[3]:\r\nb    C1  C2  C3  C4\r\na\r\nR1  NaN NaN NaN NaN\r\nR2  NaN  10 NaN NaN\r\nNaN NaN NaN NaN NaN\r\nR4  NaN NaN  15  20\r\n```\r\n\r\nThe way I understand `pivot` here is that it makes a `DataFrame` with `a` as the `index`, `b` as the columns and then uses the third argument, in this case `c` as values, where `a` and `b` form a kind of coordinate system for `c`. Thus, instead of the current output, I would expect the result to be \r\n\r\n```\r\nIn [14]: e\r\nOut[14]:\r\nb    C1  C2  C3  C4\r\na\r\nR1   10 NaN NaN NaN\r\nR2  NaN  15 NaN NaN\r\nNaN NaN NaN NaN NaN\r\nR4  NaN NaN NaN  20\r\n```\r\n\r\nIf, on the other hand, you *don\'t* have any `nan`s in your frame, the result is what I would expect:\r\n\r\n```\r\nIn [16]: df.loc[2, \'a\'] = \'R3\'\r\n\r\nIn [17]: df.loc[2, \'c\'] = 17\r\n\r\nIn [18]: df\r\nOut[18]:\r\n    a   b   c\r\n0  R1  C1  10\r\n1  R2  C2  15\r\n2  R3  C3  17\r\n3  R4  C4  20\r\n\r\nIn [19]: df.pivot(\'a\',\'b\',\'c\')\r\nOut[19]:\r\nb   C1  C2  C3  C4\r\na\r\nR1  10 NaN NaN NaN\r\nR2 NaN  15 NaN NaN\r\nR3 NaN NaN  17 NaN\r\nR4 NaN NaN NaN  20\r\n```\r\n\r\nI\'ll have a look at #3588 to see if this is a regression, or if I\'m just misunderstanding how this is supposed to work.\r\n\r\nI have a suspicion that this is related to #7403 '"
7465,35737387,sinhrks,jreback,2014-06-14 21:42:43,2014-06-17 14:38:59,2014-06-17 11:57:36,closed,,0.14.1,1,Bug;Frequency;Timezones,https://api.github.com/repos/pydata/pandas/issues/7465,b'BUG: Some offsets.apply cannot handle tz properly',"b""There are some offsets which cannot handle input with `tz` properly\r\n```\r\npd.offsets.Day().apply(pd.Timestamp('2010-01-01 9:00', tz='US/Eastern'))\r\n# 2010-01-02 09:00:00-05:00 (Expected)\r\n\r\npd.offsets.CustomBusinessDay().apply(pd.Timestamp('2010-01-01 9:00', tz='US/Eastern'))\r\n# 2010-01-04 09:00:00 (tzinfo lost)\r\n\r\npd.offsets.CustomBusinessMonthEnd().apply(pd.Timestamp('2010-01-01 9:00', tz='US/Eastern'))\r\n# ValueError: Cannot compare tz-naive and tz-aware timestamps\r\n```\r\n\r\n### Affected Offsets\r\n- pandas.tseries.offsets.CustomBusinessDay'\r\n- 'pandas.tseries.offsets.CustomBusinessMonthEnd'\r\n- 'pandas.tseries.offsets.CustomBusinessMonthBegin'\r\n- 'pandas.tseries.offsets.BusinessMonthBegin'\r\n- 'pandas.tseries.offsets.YearBegin'\r\n- 'pandas.tseries.offsets.BYearBegin'\r\n- 'pandas.tseries.offsets.YearEnd'\r\n- 'pandas.tseries.offsets.BYearEnd'\r\n- 'pandas.tseries.offsets.BQuarterBegin'\r\n- 'pandas.tseries.offsets.LastWeekOfMonth'\r\n- 'pandas.tseries.offsets.FY5253Quarter'\r\n- 'pandas.tseries.offsets.FY5253'\r\n- 'pandas.tseries.offsets.Week'\r\n- 'pandas.tseries.offsets.WeekOfMonth'\r\n- 'pandas.tseries.offsets.Easter'"""
7464,35735808,cpcloud,cpcloud,2014-06-14 20:15:22,2014-06-15 01:33:36,2014-06-15 01:33:35,closed,cpcloud,0.14.1,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/7464,b'BUG: astype(float) in Index does the wrong thing',
7462,35734643,cpcloud,jreback,2014-06-14 19:09:58,2015-04-08 14:49:18,2015-04-08 14:49:18,closed,cpcloud,Next Major Release,14,Bug;Dtypes;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7462,b'BUG: unstack fails for mixed dtype subset',b'closes #7405'
7459,35727343,sinhrks,jreback,2014-06-14 12:30:28,2014-07-09 12:39:10,2014-07-07 15:25:12,closed,,0.14.1,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7459,b'BUG/CLN: LinePlot uses incorrect xlim when secondary_y=True',"b""`xlim` is not set properly when `secondary_y=True`. This is different issue from #7322.\r\n\r\nAlso, refactored `secondary_y` to be handled only by `MPLPlot`, because it will be required in future fix to pass multiple axes to `df.plot`, like #7069 (I'm willing to work soon..).\r\n\r\n```\r\ndf = pd.DataFrame(np.random.randn(5, 5), columns=['A', 'B', 'C', 'D', 'E'])\r\ndf.plot(secondary_y=True)\r\n```\r\n![figure_1](https://cloud.githubusercontent.com/assets/1696302/3278423/15d4ba78-f3be-11e3-9d8e-172c9d0b754b.png)\r\n\r\n```\r\ndf = pd.DataFrame(np.random.randn(5, 5), columns=['A', 'B', 'C', 'D', 'E'])\r\ndf.plot(secondary_y=['A', 'B'], subplots=True)\r\n```\r\n![figure_2](https://cloud.githubusercontent.com/assets/1696302/3278424/1b2dae44-f3be-11e3-8b8c-b19f5ae7fef3.png)"""
7458,35726976,sinhrks,jreback,2014-06-14 12:02:01,2014-06-14 15:34:31,2014-06-14 14:30:36,closed,,0.14.1,0,Bug;Frequency;Timezones,https://api.github.com/repos/pydata/pandas/issues/7458,b'BUG: DTI.intersection doesnt preserve tz',"b""Closes #4690.\r\n\r\nAlso, found and fixed a bug which non-monotonic `Index.union` incorrectly preserves `name` when `Index` have different names.\r\n```\r\n# monotonic\r\nidx1 = pd.Index([1, 2, 3, 4, 5], name='idx1')\r\nidx2 = pd.Index([4, 5, 6, 7, 8], name='other')\r\nidx1.intersection(idx2).name\r\n# None (Expected)\r\n\r\n# non-monotonic\r\nidx1 = pd.Index([5, 4, 3, 2, 1], name='idx1')\r\nidx2 = pd.Index([4, 5, 6, 7, 8], name='other')\r\nidx1.intersection(idx2).name\r\n# idx1\r\n```"""
7457,35726376,sinhrks,jreback,2014-06-14 11:21:40,2014-09-18 20:28:41,2014-07-01 15:28:58,closed,,0.14.1,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7457,b'BUG: Better axis label handling for partial layout',"b""Closes #5897.\r\n\r\nHandle `ticklabels` and `labels` based on below rules. Currently `labels` are always displayed even if `ticklabels` are hidden and causes confusion.\r\n\r\n- If `sharex` is `True`, display only most bottom `xticklabels` on each columns. (Because #7035 hides the bottom-right axes). Hide `xlabel` as the same manner as `xticklabels`\r\n- If `sharey` is True, display most left `yticklabels` (no change). Hide `ylabel` as the same manner as `yticklabels` (changed)\r\n\r\n```\r\nimport pandas as pd\r\nfrom numpy.random import randn\r\nimport matplotlib.pyplot as plt\r\n\r\nd = pd.DataFrame({'one':randn(5), 'two':randn(5), 'three':randn(5), 'label':['label'] * 5},\r\n        columns = ['one','two','three', 'label'])\r\nbp= d.boxplot(by='label', rot=45)\r\n```\r\n\r\n![figure_1](https://cloud.githubusercontent.com/assets/1696302/3074667/70d267ea-e34c-11e3-8996-24db3e31ed67.png)"""
7453,35718698,sinhrks,jreback,2014-06-14 01:37:22,2016-05-19 19:57:17,2016-04-10 14:04:45,closed,,0.18.1,6,Bug;Difficulty Novice;Effort Low;Groupby;Missing-data;Testing,https://api.github.com/repos/pydata/pandas/issues/7453,b'BUG: Unable to aggregate TimeGrouper ',"b""Derived from #7373. There seems to be 3 issues related to `TimeGrouper` aggregation.\r\n\r\n##### 1. var, std, mean\r\nvar/std/mean raises `ValueError` when group key contains `NaT`. \r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndata = np.random.randn(20, 4)\r\ndf = pd.DataFrame(data, columns=['A', 'B', 'C', 'D'])\r\ndf['dt'] = [datetime.datetime(2013, 1, 1), datetime.datetime(2013, 1, 2),\r\n            datetime.datetime(2013, 1, 3), datetime.datetime(2013, 1, 4),\r\n            datetime.datetime(2013, 1, 5)] * 4\r\ndf['dt_nat'] = [datetime.datetime(2013, 1, 1), datetime.datetime(2013, 1, 2),\r\n                pd.NaT, datetime.datetime(2013, 1, 4),\r\n                datetime.datetime(2013, 1, 5)] * 4\r\n\r\ndf.groupby(pd.TimeGrouper(key='dt', freq='D')).mean()\r\n# OK\r\ndf.groupby(pd.TimeGrouper(key='dt_nat', freq='D')).mean()\r\n# ValueError: month must be in 1..12\r\n```\r\n\r\n##### 2. size (#7600)\r\n`size` raises `AttributeError` regardless of `NaT` existence.\r\n```\r\ndf.groupby(pd.TimeGrouper(key='dt', freq='D')).size()\r\n# AttributeError: 'BinGrouper' object has no attribute 'groupings'\r\n```\r\n\r\n##### 3. first, last, nth\r\nIt looks work, but `TimeGrouper` outputs different result from normal `groupby`. \r\n```\r\ndf.groupby('dt').first()\r\n#                    A         B         C         D  key     dt_nat\r\n# dt                                                                \r\n# 2013-01-01 -1.868691 -0.554116 -0.094949  0.009740    1 2013-01-01\r\n# 2013-01-02  0.272139 -0.106543  1.319331 -0.532377    2 2013-01-02\r\n# 2013-01-03 -1.637544  2.699557 -0.164414 -1.451295    3        NaT\r\n# 2013-01-04  1.642609 -0.313832  0.494468 -0.698104    4 2013-01-04\r\n# 2013-01-05 -1.554106  1.230299 -1.408515 -0.000722    5 2013-01-05\r\n\r\n\r\ndf.groupby(pd.TimeGrouper(key='dt', freq='D')).first()\r\n#                    A         B         C         D  key     dt_nat\r\n# dt                                                                \r\n# 2013-01-01 -1.868691 -0.554116 -0.094949  0.009740    1 2013-01-01\r\n# 2013-01-02  0.272139 -0.106543  1.319331 -0.532377    2 2013-01-02\r\n# 2013-01-03 -1.637544  2.699557 -0.164414 -1.451295    3        NaT\r\n# 2013-01-04  1.642609 -0.313832  0.494468 -0.698104    4 2013-01-04\r\n# 2013-01-05 -0.024332  1.668172 -0.328200  1.731480    5 2013-01-05\r\n\r\n# Compare 5th row\r\n```\r\n\r\n\r\nI assume the difference derived from `BinGrouper` sorts rows differently from normal groupby. Thus, result of normal groupby and `TimeGrouper` can differ. \r\n```\r\ndf.groupby('dt').get_group(datetime.datetime(2013, 1, 5))\r\n#            A         B         C         D         dt     dt_nat\r\n# 4   0.632937  0.224670 -0.201186 -0.340428 2013-01-05 2013-01-05\r\n# 9  -1.238944 -0.031075 -1.173326 -0.314716 2013-01-05 2013-01-05\r\n# 14  2.108985  0.993430  1.300605  1.452049 2013-01-05 2013-01-05\r\n# 19  0.315452 -0.817634 -0.526728  0.201415 2013-01-05 2013-01-05\r\n\r\ndf.groupby(pd.TimeGrouper(key='dt', freq='D')).get_group(datetime.datetime(2013, 1, 5))\r\n#            A         B         C         D         dt     dt_nat\r\n# 9  -1.238944 -0.031075 -1.173326 -0.314716 2013-01-05 2013-01-05\r\n# 4   0.632937  0.224670 -0.201186 -0.340428 2013-01-05 2013-01-05\r\n# 14  2.108985  0.993430  1.300605  1.452049 2013-01-05 2013-01-05\r\n# 19  0.315452 -0.817634 -0.526728  0.201415 2013-01-05 2013-01-05\r\n```"""
7450,35703954,bashtage,jreback,2014-06-13 20:23:23,2014-06-16 12:52:01,2014-06-16 12:51:56,closed,,0.14.1,2,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/7450,b'FIX: Enable fixed width strings to be read from Stata 13 (117) files',"b'Fixes a bug which prevented files containing fixed width string data from being read.  Stata 13 files also allow variable length strings, which are not supported in the current version, and an explicit exception regarding this type is now given.  Added tests which cover these cases, and Stata 13 format files. \r\n\r\nfixes #7360'"
7448,35654419,toddrjen,jreback,2014-06-13 08:49:59,2014-06-22 15:29:59,2014-06-13 16:42:59,closed,,0.14.1,18,Bug;Dtypes;Internals,https://api.github.com/repos/pydata/pandas/issues/7448,"b""Fix bug where ``nanops._has_infs`` doesn't work with many dtypes (issue #7357)""","b""Fixes issue #7357, where where ``nanops._has_infs`` doesn't work with many dtypes"""
7440,35582648,toddrjen,jreback,2014-06-12 13:18:52,2014-06-13 13:23:36,2014-06-12 22:41:29,closed,,0.14.1,10,Bug;Internals;Numeric,https://api.github.com/repos/pydata/pandas/issues/7440,b'support axis=None for nanmedian ( issue #7352 )',"b'This fixes #7352, where ``nanmedian`` does not work when ``axis==None``.'"
7437,35574570,toddrjen,jreback,2014-06-12 11:09:11,2014-06-12 13:12:55,2014-06-12 12:59:15,closed,,0.14.1,5,Bug;Internals;Numeric,https://api.github.com/repos/pydata/pandas/issues/7437,b'make nanops work when ndim==1 and axis==0 ( issue #7354 )',"b'This fixes issue #7354, where some ``nanops`` functions fail for 1-dimensional arrays for the argument ``axis=0``.'"
7435,35514350,jreback,jreback,2014-06-11 18:51:00,2014-06-13 15:26:37,2014-06-13 15:22:05,closed,,0.14.1,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/7435,b'API: Improved inference of datetime/timedelta with mixed null objects. (GH7431)',b'- regression from 0.13.1 in interpretation of an object Index\r\n- additional tests to validate datetimelike inferences\r\n\r\ncloses #7431'
7431,35494089,dbew,jreback,2014-06-11 15:16:39,2014-06-13 15:22:05,2014-06-13 15:22:05,closed,,0.14.1,3,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/7431,b'series created with DatetimeIndex when index is [None]',"b""This looks like a change from 0.13.1 to HEAD. If you create a series like this:\r\n\r\n``` python\r\npd.Series(index=np.array([None]))\r\n```\r\n\r\nThen in 0.13.1 the result is:\r\n\r\n``` python\r\nOut[16]: \r\nNaN   NaN\r\ndtype: float64\r\n\r\nIn [19]: x.index.dtype\r\nOut[19]: dtype('O')\r\n\r\nIn [20]: type(x.index)\r\nOut[20]: pandas.core.index.Index\r\n```\r\nbut in HEAD the result is \r\n\r\n``` python\r\nOut[191]: \r\nNaT   NaN\r\ndtype: float64\r\n\r\nIn[192]: x.index.dtype\r\nOut[192]: dtype('<M8[ns]')\r\n\r\nIn[193]: type(x.index)\r\nOut[193]: pandas.tseries.index.DatetimeIndex\r\n```\r\n\r\nSo the index has changed from Index/object to DatetimeIndex/datetime64."""
7430,35486137,jreback,jreback,2014-06-11 14:02:04,2014-06-12 07:02:34,2014-06-11 14:43:24,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7430,b'BUG: Bug in multi-index slicing with datetimelike ranges (strings and Timestamps) (GH7429)',b'closes #7429'
7429,35482245,jreback,jreback,2014-06-11 13:22:41,2014-06-11 14:48:35,2014-06-11 14:43:24,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7429,b'BUG: multi-index slicing buggy with datetime selectors',"b""http://stackoverflow.com/questions/24152509/slicing-a-pandas-multiindex-using-datetime-datatype\r\n\r\n```\r\ndates = pd.DatetimeIndex([datetime.datetime(2012,1,1,12,12,12)+datetime.timedelta(days = i) for i in range(6)])\r\nfreq = [1,2]\r\niterables = [dates, freq]\r\n\r\nindex = pd.MultiIndex.from_product(iterables, names=['date','frequency'])\r\ndf = pd.DataFrame(np.random.randn(6*2,4),index=index,columns=list('ABCD'))\r\n```\r\n\r\n- This should work in a single step\r\n- show this using ``pd.IndexSlice`` notation\r\n- accept strings / partial strings as indexers\r\n```\r\ndf_temp = df.loc[(slice(pd.Timestamp('2012-01-01 12:12:12'),pd.Timestamp('2012-01-03 12:12:12'))), slice('A','B')]\r\ndf_temp.loc[(slice(None),slice(1,1)),:]\r\n```\r\n\r\nAfter #7430, the following works (just using ``IndexSlice`` as a conven)\r\n\r\n```\r\nidx = pd.IndexSlice\r\ndf.loc[(idx[pd.Timestamp('2012-01-01 12:12:12'),pd.Timestamp('2012-01-03 12:12:12'),idx[1:1]], idx['A','B']]\r\n```\r\n\r\nas well as partial string slicing\r\n```\r\ndf.loc[idx['2012-01-01 12:12:12':'2012-01-03 12:12:12',1],idx['A':'B']]\r\n```"""
7428,35481845,toddrjen,jreback,2014-06-11 13:17:42,2014-06-12 11:03:13,2014-06-12 10:46:48,closed,,0.14.1,3,Bug;Internals;Numeric,https://api.github.com/repos/pydata/pandas/issues/7428,b'make nanops._maybe_null_out work with complex numbers ( issue #7353 )',"b""This fixes #7353 where ``nanops._maybe_null_out``, and thus other functions that call on it, don't work with complex numbers."""
7424,35438408,hayd,jreback,2014-06-11 00:00:33,2014-06-17 15:21:34,2014-06-17 12:01:37,closed,,0.14.1,9,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7424,b'FIX value_counts should skip NaT',b'fixes #7423  \r\nfixes #5569.'
7423,35438278,hayd,jreback,2014-06-10 23:58:08,2014-06-17 12:01:37,2014-06-17 12:01:37,closed,,0.14.1,0,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7423,"b""value counts NaT doesn't count as NaN""","b""fix on the way.\r\n```\r\ndt = pd.to_datetime(['NaT', '2014-01-01'])\r\nvalue_counts(dt)  # should have one entry\r\n```\r\n\r\n(Actually there is an issue with timedeltas too (you can't create an timedelta index.)"""
7419,35419028,cpcloud,cpcloud,2014-06-10 19:59:18,2014-06-18 20:05:35,2014-06-10 20:08:29,closed,cpcloud,0.14.1,6,Bug;CI;Testing,https://api.github.com/repos/pydata/pandas/issues/7419,b'openpyxl version issue for things that depend on pandas',b'ML conversation: https://groups.google.com/forum/#!topic/pydata/gontRY254Xo'
7416,35390057,cpcloud,cpcloud,2014-06-10 14:54:38,2014-06-13 05:13:26,2014-06-10 21:52:01,closed,cpcloud,0.14.1,16,Bug;Dtypes;Enhancement;Numeric,https://api.github.com/repos/pydata/pandas/issues/7416,b'BUG/DTYPES: preserve bools in convert_objects',b'closes #7126'
7415,35373206,toddrjen,toddrjen,2014-06-10 11:36:08,2014-06-22 15:29:59,2014-06-22 15:29:36,closed,,0.16.0,2,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/7415,"b""BUG:  nanops._ensure_numeric can't handle object dtypes containing complex objects""","b""```nanops._ensure_numeric``` is used to make sure ndarrays are a numeric dtype.  However, it can't handle the case where the dtype is ``object`` and one or more of the axes is ``complex``:\r\n\r\n```Python\r\n>>> from pandas.core.nanops import nanmean\r\n>>> import numpy as np\r\n>>> \r\n>>> value = np.vstack([np.array(np.nan).astype('O'), np.array(1+1j).astype('O')])\r\n>>> nanmean(value, axis=0)\r\nTypeError: can't convert complex to float\r\n```"""
7410,35292765,cpcloud,cpcloud,2014-06-09 15:11:54,2014-06-13 05:13:24,2014-06-09 22:34:38,closed,cpcloud,0.14.1,3,Bug;Indexing;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7410,b'BUG: fix repring of nat multiindex and fix neg indexing in datetimeindex',"b'closes #7406, #7408, #7409'"
7409,35292654,cpcloud,cpcloud,2014-06-09 15:10:34,2014-06-09 22:35:41,2014-06-09 22:35:41,closed,cpcloud,0.14.1,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7409,b'multiindex repr of datetime nat is incorrect',"b'related to #7406, #7401'"
7408,35292271,cpcloud,cpcloud,2014-06-09 15:06:18,2014-06-09 22:35:29,2014-06-09 22:35:29,closed,cpcloud,0.14.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7408,b'incorrect slicing for negative indexers in datetimeindex',b'got a fix coming in a minute or so  just wanted to track the issue\r\n\r\nrelated #7406 '
7407,35292004,jreback,jreback,2014-06-09 15:03:32,2014-07-01 16:02:12,2014-07-01 16:02:12,closed,,0.14.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7407,b'BUG: Series.get w boolean accessor',"b""from ML: https://groups.google.com/forum/#!topic/pydata/CDnF9cNR2ho\r\n\r\n```\r\nIn [10]: df=pd.DataFrame({'i':[0]*10, 'b':[False]*10})\r\n\r\nIn [11]: vc_i=df.i.value_counts()\r\n\r\nIn [12]: vc_i.get(99,default='Missing')\r\nOut[12]: 'Missing'\r\n\r\nIn [13]: vc_b=df.b.value_counts()\r\n\r\nIn [14]: vc_b\r\nOut[14]: \r\nFalse    10\r\ndtype: int64\r\n\r\nIn [15]: vc_b.get(False,default='Missing')\r\nOut[15]: 10\r\n\r\nIn [16]: vc_b.get(True,default='Missing')\r\n\r\nIndexError: index out of bounds\r\n```"""
7406,35288458,cpcloud,cpcloud,2014-06-09 14:23:31,2014-06-09 22:34:38,2014-06-09 22:34:38,closed,cpcloud,0.14.1,0,Bug;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7406,b'repr of MultiIndexed frame with NaT as the first element of level 0 of the index raises during repr',"b""world's longest PR title, for a very strange bug\r\n\r\n```\r\nIn [145]: idx = pd.to_datetime([pd.NaT] + pd.date_range('20130101', periods=2).tolist())\r\n\r\nIn [146]: idx\r\nOut[146]:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[NaT, ..., 2013-01-02]\r\nLength: 3, Freq: None, Timezone: None\r\n\r\nIn [149]: df = DataFrame({'X': range(len(idx))}, index=[idx, tm.choice(list('ab'), size=len(idx))])\r\n\r\nIn [150]: df\r\nOut[150]: <repr(<pandas.core.frame.DataFrame at 0x7ff147824950>) failed: ValueError: boolean index array has too many values>\r\n```\r\n\r\nbut strangely enough if I construct it with slightly different dates it works fine:\r\n\r\n```\r\nIn [184]: idx = pd.to_datetime([pd.NaT, pd.Timestamp('2013-01-01'), pd.Timestamp('2013-01-02')])\r\n\r\nIn [185]: df = DataFrame({'X': range(len(idx))}, index=[idx, tm.choice(list('ab'), size=len(idx))])\r\n\r\nIn [186]: df\r\nOut[186]: <repr(<pandas.core.frame.DataFrame at 0x7ff14792d1d0>) failed: ValueError: boolean index array has too many values>\r\n\r\nIn [187]: idx = pd.to_datetime([pd.NaT, pd.Timestamp('2013-01-03'), pd.Timestamp('2013-01-02')])\r\n\r\nIn [188]: df = DataFrame({'X': range(len(idx))}, index=[idx, tm.choice(list('ab'), size=len(idx))])\r\n\r\nIn [189]: df\r\nOut[189]:\r\n              X\r\nNaN        a  0\r\n2013-01-03 b  1\r\n2013-01-02 a  2\r\n```\r\n\r\n"""
7405,35287154,jorisvandenbossche,jreback,2014-06-09 14:07:12,2015-01-26 01:29:07,2015-01-26 01:29:07,closed,cpcloud,0.16.0,1,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7405,"b'BUG: unstacking with partial selection and mixed dtype gives ""ValueError: shape of passed values ...""'","b'And yet another unstacking bug (but now not related to NaNs as in #7403). Getting a `ValueError: Shape of passed values is (2, 3), indices imply (2, 5)` (where 3 is the correct one, 5 is the original number of rows) in some specific conditions with mixed dtype and selection of the rows:\r\n\r\n```\r\nIn [23]: df = pd.DataFrame({\'A\': [\'a\']*5,\r\n   ....:                    \'B\':pd.date_range(\'2012-01-01\', periods=5),\r\n   ....:                    \'C\':np.zeros(5),\r\n   ....:                    \'D\':np.zeros(5)})\r\n\r\nIn [25]: df = df.set_index([\'A\', \'B\'])\r\n\r\nIn [26]: df\r\nOut[26]:\r\n              C  D\r\nA B\r\na 2012-01-01  0  0\r\n  2012-01-02  0  0\r\n  2012-01-03  0  0\r\n  2012-01-04  0  0\r\n  2012-01-05  0  0\r\n```\r\nUnstacking this or a selection of it, works as expected\r\n```\r\nIn [27]: df.unstack(0)\r\nOut[27]:\r\n            C  D\r\nA           a  a\r\nB\r\n2012-01-01  0  0\r\n2012-01-02  0  0\r\n2012-01-03  0  0\r\n2012-01-04  0  0\r\n2012-01-05  0  0\r\n\r\nIn [28]: df.iloc[:3].unstack(0)\r\nOut[28]:\r\n            C  D\r\nA           a  a\r\nB\r\n2012-01-01  0  0\r\n2012-01-02  0  0\r\n2012-01-03  0  0\r\n```\r\nBut when the dataframe has mixed dtypes:\r\n```\r\nIn [29]: df[\'D\'] = df[\'D\'].astype(\'int64\')\r\n\r\nIn [31]: df.dtypes\r\nOut[31]:\r\nC    float64\r\nD      int64\r\ndtype: object\r\n```\r\nunstacking still does work, but not anymore on the selection:\r\n```\r\nIn [32]: df.unstack(0)\r\nOut[32]:\r\n            C  D\r\nA           a  a\r\nB\r\n2012-01-01  0  0\r\n2012-01-02  0  0\r\n2012-01-03  0  0\r\n2012-01-04  0  0\r\n2012-01-05  0  0\r\n\r\nIn [33]: df.iloc[:3].unstack(0)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n...\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\internals.pyc in _verify_integr\r\nity(self)\r\n   2090         for block in self.blocks:\r\n   2091             if not block.is_sparse and block.shape[1:] != mgr_shape[1:]:\r\n\r\n-> 2092                 construction_error(tot_items, block.shape[1:], self.axes\r\n)\r\n   2093         if len(self.items) != tot_items:\r\n   2094             raise AssertionError(\'Number of manager items must equal uni\r\non of \'\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\internals.pyc in construction_e\r\nrror(tot_items, block_shape, axes, e)\r\n   3162         raise e\r\n   3163     raise ValueError(""Shape of passed values is {0}, indices imply {1}"".\r\nformat(\r\n-> 3164         passed,implied))\r\n   3165\r\n   3166\r\n\r\nValueError: Shape of passed values is (2, 3), indices imply (2, 5)\r\n```\r\nIf the index is resetted and setted again (so the levels and labels are recalculated based on the selection), it does work again:\r\n```\r\nIn [34]: df.iloc[:3].reset_index().set_index([\'A\', \'B\']).unstack(0)\r\nOut[34]:\r\n            C  D\r\nA           a  a\r\nB\r\n2012-01-01  0  0\r\n2012-01-02  0  0\r\n2012-01-03  0  0\r\n```\r\n\r\nI am not sure about the exact circumstances this happens, because I can\'t reproduce it with a small example with different values in the `A` index level (now only `a`), but in the large real dataframe where I experienced it, there were multiple levels.\r\n'"
7404,35283558,jreback,jreback,2014-06-09 13:20:02,2014-06-15 23:53:41,2014-06-09 15:47:18,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7404,"b'BUG: mi indexing bugs (GH7399,GH7400)'",b'closes #7399\r\ncloses #7400'
7403,35283431,jorisvandenbossche,jreback,2014-06-09 13:17:35,2015-01-26 01:29:07,2015-01-26 01:29:07,closed,cpcloud,0.16.0,2,Bug;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7403,b'BUG: incorrect unstacking with NaNs in the index',"b""Related to #7401, but another issue I think. Some more strange behaviour with NaNs in the index when unstacking (but now not specifically to datetime).\r\n\r\nFirst case:\r\n```\r\nIn [9]: df = pd.DataFrame({'A': list('aaaabbbb'),\r\n   ...:                    'B':range(8),\r\n   ...:                    'C':range(8)})\r\n\r\nIn [10]: df.set_index(['A', 'B']).unstack(0)\r\nOut[10]:\r\n    C\r\nA   a   b\r\nB\r\n0   0 NaN\r\n1   1 NaN\r\n2   2 NaN\r\n3   3 NaN\r\n4 NaN   4\r\n5 NaN   5\r\n6 NaN   6\r\n7 NaN   7\r\n\r\nIn [11]: df.iloc[3,1] = np.NaN\r\n\r\nIn [12]: df.set_index(['A', 'B']).unstack(0)\r\nOut[12]:\r\n      C\r\nA     a   b\r\nB\r\n 0    3 NaN\r\n 1    0 NaN\r\n 2    1 NaN\r\nNaN NaN NaN\r\n 4  NaN   2\r\n 5  NaN   4\r\n 6  NaN   5\r\n 7    6   7\r\n```\r\nThe values in the first column are totally mixed up.\r\n\r\nSecond case (with repeating values in the second level):\r\n\r\n```\r\nIn [13]: df = pd.DataFrame({'A': list('aaaabbbb'),\r\n   ....:                    'B':range(4)*2,\r\n   ....:                    'C':range(8)})\r\nIn [14]: df\r\nOut[14]:\r\n   A  B  C\r\n0  a  0  0\r\n1  a  1  1\r\n2  a  2  2\r\n3  a  3  3\r\n4  b  0  4\r\n5  b  1  5\r\n6  b  2  6\r\n7  b  3  7\r\n\r\nIn [15]: df.set_index(['A', 'B']).unstack(0)\r\nOut[15]:\r\n   C\r\nA  a  b\r\nB\r\n0  0  4\r\n1  1  5\r\n2  2  6\r\n3  3  7\r\n\r\nIn [16]: df.iloc[2,1] = np.NaN\r\n\r\nIn [17]: df.set_index(['A', 'B']).unstack(0)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-17-2f4735e48b98> in <module>()\r\n----> 1 df.set_index(['A', 'B']).unstack(0)\r\n\r\n...\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\reshape.pyc in _make_selectors(\r\nself)\r\n    139\r\n    140         if mask.sum() < len(self.index):\r\n--> 141             raise ValueError('Index contains duplicate entries, '\r\n    142                              'cannot reshape')\r\n    143\r\n\r\nValueError: Index contains duplicate entries, cannot reshape\r\n```\r\n\r\nand another error message with the NaN on the last place (of the sublevel):\r\n\r\n```\r\nIn [20]: df = pd.DataFrame({'A': list('aaaabbbb'),\r\n   ....:                    'B':range(4)*2,\r\n   ....:                    'C':range(8)})\r\nIn [21]: df.iloc[3,1] = np.NaN\r\n\r\nIn [22]: df.set_index(['A', 'B']).unstack(0)\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n\r\n...\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\reshape.pyc in get_result(self)\r\n\r\n    173                 values_indexer = com._ensure_int64(l[~mask])\r\n    174                 for i, j in enumerate(values_indexer):\r\n--> 175                     values[j] = orig_values[i]\r\n    176             else:\r\n    177                 index = index.take(self.unique_groups)\r\n\r\nIndexError: index 4 is out of bounds for axis 0 with size 4\r\n```\r\n\r\nI know NaNs in the index is not really recommended, but just exploring this (as I was caught by such an issue, you don't always think of looking if you have NaNs if you get such errors)\r\n"""
7402,35281981,jreback,jreback,2014-06-09 12:54:30,2014-06-15 23:54:11,2014-06-09 13:16:42,closed,,0.14.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7402,b'BUG: ix should return a Series for duplicate indices (GH7150)',b'closes #7150 '
7401,35279505,jorisvandenbossche,jreback,2014-06-09 12:06:31,2015-01-26 01:29:07,2015-01-26 01:29:07,closed,cpcloud,0.16.0,9,Bug;Dtypes;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7401,"b'unstack with DatetimeIndex with NaN gives ""ValueError: cannot convert float NaN to integer""'","b""With following code:\r\n```\r\ndf = pd.DataFrame({'A': list('aaaaabbbbb'),\r\n                   'B':pd.date_range('2012-01-01', periods=5).tolist()*2,\r\n                   'C':np.zeros(10)})\r\ndf.iloc[3,1] = np.NaN\r\ndf = df.set_index(['A', 'B'])\r\n```\r\nunstacking gives:\r\n```\r\nIn [6]: df.unstack()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-9a91d46cdd8d> in <module>()\r\n----> 1 df.unstack()\r\n...\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\tseries\\index.pyc in _simple_new(cls\r\n, values, name, freq, tz)\r\n    461     def _simple_new(cls, values, name, freq=None, tz=None):\r\n    462         if values.dtype != _NS_DTYPE:\r\n--> 463             values = com._ensure_int64(values).view(_NS_DTYPE)\r\n    464\r\n    465         result = values.view(cls)\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\algos.pyd in pandas.algos.ensure_int\r\n64 (pandas\\algos.c:47444)()\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\algos.pyd in pandas.algos.ensure_int\r\n64 (pandas\\algos.c:47349)()\r\n\r\nValueError: cannot convert float NaN to integer\r\n```\r\n\r\nI don't know if this should work (unstacking with NaN in the index), but at least this is not a very clear error message to know what could be going wrong."""
7400,35261402,shoyer,jreback,2014-06-09 05:40:24,2014-06-09 15:47:17,2014-06-09 15:47:17,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7400,b'BUG: step with .loc on MultiIndex with list of indexers skips wrong elements',"b""Setup (shared with #7399):\r\n```python\r\nimport pandas as pd\r\nmidx = pd.MultiIndex.from_product([range(5), ['a', 'b', 'c']])\r\ns = pd.Series(range(15), midx)\r\nprint(s.loc[2:4:2, 'a':'c'])\r\n```\r\nWhat I get:\r\n```\r\n2  a     6\r\n   c     8\r\n3  b    10\r\n4  a    12\r\n   c    14\r\ndtype: int64\r\n```\r\nWhat I expected (the step should only apply to the first level):\r\n```\r\n2  a     6\r\n   b     7\r\n   c     8\r\n4  a    12\r\n   b    13\r\n   c    14\r\ndtype: int64\r\n```"""
7399,35261299,shoyer,jreback,2014-06-09 05:36:59,2014-06-09 15:47:17,2014-06-09 15:47:17,closed,,0.14.1,1,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7399,b'BUG: .loc on MultiIndex with list of indexers including an incomplete slice raises KeyError',"b'Tested on latest version \r\n```python\r\nimport pandas as pd\r\nmidx = pd.MultiIndex.from_product([range(5), [\'a\', \'b\', \'c\']])\r\ns = pd.Series(range(15), midx)\r\ns.loc[:, \'a\':\'c\'] # works\r\ns.loc[0:4, \'a\':\'c\'] # works\r\ns.loc[:4, \'a\':\'c\'] # raises KeyError\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-4865919a19c8> in <module>()\r\n----> 1 s.loc[:4, \'a\':\'c\'] # raises KeyError\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/indexing.pyc in __getitem__(self, key)\r\n   1125     def __getitem__(self, key):\r\n   1126         if type(key) is tuple:\r\n-> 1127             return self._getitem_tuple(key)\r\n   1128         else:\r\n   1129             return self._getitem_axis(key, axis=0)\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    643     def _getitem_tuple(self, tup):\r\n    644         try:\r\n--> 645             return self._getitem_lowerdim(tup)\r\n    646         except IndexingError:\r\n    647             pass\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/indexing.pyc in _getitem_lowerdim(self, tup)\r\n    751         # we may have a nested tuples indexer here\r\n    752         if self._is_nested_tuple_indexer(tup):\r\n--> 753             return self._getitem_nested_tuple(tup)\r\n    754\r\n    755         # we maybe be using a tuple to represent multiple dimensions here\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/indexing.pyc in _getitem_nested_tuple(self, tup)\r\n    810\r\n    811             # this is a series with a multi-index specified a tuple of selectors\r\n--> 812             return self._getitem_axis(tup, axis=0, validate_iterable=True)\r\n    813\r\n    814         # handle the multi-axis by taking sections and reducing\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/indexing.pyc in _getitem_axis(self, key, axis, validate_iterable)\r\n   1267             # nested tuple slicing\r\n   1268             if _is_nested_tuple(key, labels):\r\n-> 1269                 locs = labels.get_locs(key)\r\n   1270                 indexer = [ slice(None) ] * self.ndim\r\n   1271                 indexer[axis] = locs\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/index.pyc in get_locs(self, tup)\r\n   3602             elif isinstance(k,slice):\r\n   3603                 # a slice, include BOTH of the labels\r\n-> 3604                 ranges.append(self._get_level_indexer(k,level=i))\r\n   3605             else:\r\n   3606                 # a single label\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/index.pyc in _get_level_indexer(self, key, level)\r\n   3527             # otherwise a boolean indexer\r\n   3528\r\n-> 3529             start = level_index.get_loc(key.start)\r\n   3530             stop  = level_index.get_loc(key.stop)\r\n   3531             step = key.step\r\n\r\n/Users/shoyer/dev/pandas/pandas/core/index.pyc in get_loc(self, key)\r\n   1167         loc : int if unique index, possibly slice or mask if not\r\n   1168         """"""\r\n-> 1169         return self._engine.get_loc(_values_from_object(key))\r\n   1170\r\n   1171     def get_value(self, series, key):\r\n\r\n/Users/shoyer/dev/pandas/pandas/index.so in pandas.index.IndexEngine.get_loc (pandas/index.c:3651)()\r\n\r\n/Users/shoyer/dev/pandas/pandas/index.so in pandas.index.IndexEngine.get_loc (pandas/index.c:3578)()\r\n\r\nKeyError: None\r\n```'"
7394,35227885,sinhrks,jreback,2014-06-08 04:26:27,2014-07-02 16:46:36,2014-07-01 15:29:22,closed,,0.14.1,5,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7394,b'BUG: grouped hist and scatter use old figsize default',b'Grouped ``hist`` and ``scatter`` plots use ``figsize`` default which is different from pandas current and mpl.'
7391,35219434,sinhrks,jorisvandenbossche,2014-06-07 21:45:50,2014-07-09 12:39:17,2014-07-06 14:09:51,closed,,0.14.1,18,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7391,b'BUG: inconsistent subplot ax handling',"b'There is an inconsistency `DataFrame.plot`, `hist` and `boxplot` when subplot enabled (`subplots=True` or use `by` kw).\r\n\r\n### Current behaviour\r\n\r\n- `plot` and `hist`:\r\nWhen `ax` kw is passed, plot will be drawn on the figure which the passed ax belongs. The figure will be once cleared even if the required number of subplot is 1. Thus, any artists contained in the passed `ax` will be flushed.\r\n\r\n- `box`:\r\nWhen `ax` kw is passed and a required number of subplots is 1, use the passed `ax` without clearing the existing plot. If more than 1 axes is required for the subplots, raises `ValueError`.\r\n\r\n### Fix\r\n\r\n- When `ax` kw is passed and a required number of subplots is 1, use the passed `ax` without clearing the existing plot.\r\n- If more than 1 axes is required for the subplots, subplots will be output on the figure which the passed ax belongs. The figure will be once cleared if the number of subplots is more than 1.'"
7390,35219049,cpcloud,jreback,2014-06-07 21:29:45,2014-07-09 18:20:25,2014-06-09 12:54:56,closed,cpcloud,0.14.1,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7390,b'BUG: ix should return a Series for duplicate indices',"b""closes #7150\r\n\r\nit seems like there's a better way to do this .... """
7388,35215479,cpcloud,cpcloud,2014-06-07 19:00:57,2014-07-12 06:03:54,2014-06-07 19:37:22,closed,cpcloud,0.14.1,0,Bug;CI;Testing,https://api.github.com/repos/pydata/pandas/issues/7388,b'TST/BUG: use BytesIO for Python 3.4 TestEncoding',
7382,35185221,dtanders,jreback,2014-06-06 21:38:57,2014-06-09 17:06:16,2014-06-09 16:46:30,closed,,0.14.1,16,Bug;Windows,https://api.github.com/repos/pydata/pandas/issues/7382,b'Sorting a Larg-ish Series Appears to be Broken',"b'I\'m doing this in an iPython Notebook using 0.13.1 installed by Anaconda on Windows:\r\n```python\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom csv import excel\r\n%pylab inline\r\n\r\nomob = pd.read_csv(""LoadTimes.csv"", names=[""Timing""], dialect=excel).Timing\r\n\r\nsliced = omob[(omob > 0) & (omob < 120000)]\r\nwtf = pd.Series(sorted(sliced))\r\nidk = pd.Series(sorted(omob.clip(0, 120000)))\r\npd.stats.moments.rolling_mean(wtf, 100).plot(logy=True,color=\'g\')\r\npd.stats.moments.rolling_mean(idk, 100).plot(logy=True,color=\'b\')\r\n\r\nsrt = pd.Series(sliced)\r\nodr = sliced.order()\r\nsrt.sort()\r\npd.stats.moments.rolling_mean(srt, 100).plot(logy=True,color=\'r\')\r\npd.stats.moments.rolling_mean(odr, 100).plot(logy=True,color=\'black\')\r\n```\r\nhttps://www.dropbox.com/s/ozko9yj6rgzjske/LoadTimes.csv for the data - it\'s only about half a MB.\r\n\r\nOnly one of those plots is actually sorted and it\'s the first one.  The second clearly got close and stopped for some reason.  The last two look like seismograph readings:\r\n![Imgur](http://i.imgur.com/eIgwaSM.png)\r\n\r\n'"
7379,35175997,cpcloud,cpcloud,2014-06-06 19:40:15,2014-06-25 19:42:16,2014-06-06 23:28:58,closed,cpcloud,0.14.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/7379,b'BUG: correct Period comparisons',b'closes #7376'
7378,35175908,jreback,jreback,2014-06-06 19:39:16,2014-06-11 12:48:06,2014-06-11 12:48:06,closed,,0.14.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/7378,b'BUG: Datetime.index issues from 7299',b'https://github.com/pydata/pandas/pull/7299\r\n'
7377,35174248,jreback,jreback,2014-06-06 19:18:26,2014-06-06 19:27:11,2014-06-06 19:26:05,closed,,0.14.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/7377,b'BUG: infer_freq broken by GH7371',b'#7371'
7376,35168662,jseabold,cpcloud,2014-06-06 18:07:38,2014-06-06 23:28:58,2014-06-06 23:28:58,closed,cpcloud,0.14.1,8,Bug,https://api.github.com/repos/pydata/pandas/issues/7376,b'bug in replace?',"b""I think this is distinct from #5541 though I agree that an API change here might be helpful. AFAICT, I am using replace as 'expected', but all of the keys aren't being matched. Any idea what's going on?\r\n\r\n    d = {u'fname' :\r\n        {'out_augmented_AUG_2011.json' : pd.Period(year=2011, month=8, freq='M'), \r\n        'out_augmented_JAN_2011.json' : pd.Period(year=2011, month=1, freq='M'),\r\n        'out_augmented_MAY_2012.json' : pd.Period(year=2012, month=5, freq='M'),\r\n        'out_augmented_SUBSIDY_WEEK.json' : pd.Period(year=2011, month=4, freq='M'),\r\n        'out_augmented_AUG_2012.json' : pd.Period(year=2012, month=8, freq='M'),\r\n        'out_augmented_MAY_2011.json' : pd.Period(year=2011, month=5, freq='M'),\r\n        'out_augmented_SEP_2013.json' : pd.Period(year=2013, month=9, freq='M'),\r\n        }}\r\n\r\n    df = pd.DataFrame(['out_augmented_AUG_2012.json',\r\n                    'out_augmented_SEP_2013.json',\r\n                    'out_augmented_SUBSIDY_WEEK.json',\r\n                    'out_augmented_MAY_2012.json',\r\n                    'out_augmented_MAY_2011.json',\r\n                    'out_augmented_AUG_2011.json',\r\n                    'out_augmented_JAN_2011.json'], columns=['fname'])\r\n\r\n    df.replace(d)\r\n\r\nOn\r\n\r\n    [31]: pd.version.version\r\n    [31]: '0.13.1-753-g4614ac8'\r\n"""
7375,35159182,sinhrks,jreback,2014-06-06 16:08:58,2014-07-09 15:34:02,2014-06-11 14:08:31,closed,,0.14.1,23,Bug;Frequency;Testing,https://api.github.com/repos/pydata/pandas/issues/7375,b'ENH/BUG: Offset.apply dont preserve time',b'Closes #7156.'
7374,35157151,yarikoptic,yarikoptic,2014-06-06 15:45:41,2014-06-06 17:18:40,2014-06-06 17:18:40,closed,,0.14.1,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/7374,b'RFC: what could attribute to failed tests in TestClipboard?',"b'Originally reported https://buildd.debian.org/status/fetch.php?pkg=pandas&arch=kfreebsd-amd64&ver=0.14.0%2Bgit17-g3849d5d-1%2Bb1&stamp=1401974000\r\n\r\nI have tried to reproduce on (an interactive) kfreebsd-amd64 porter box, but there tests completed fine.\r\n\r\n```\r\n======================================================================\r\nERROR: test_read_clipboard_infer_excel (pandas.io.tests.test_clipboard.TestClipboard)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 81, in test_read_clipboard_infer_excel\r\n    df = pd.read_clipboard()\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/clipboard.py"", line 51, in read_clipboard\r\n    return read_table(StringIO(text), **kwargs)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 443, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 228, in _read\r\n    parser = TextFileReader(filepath_or_buffer, **kwds)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 533, in __init__\r\n    self._make_engine(self.engine)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 670, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/parsers.py"", line 1032, in __init__\r\n    self._reader = _parser.TextReader(src, **kwds)\r\n  File ""parser.pyx"", line 483, in pandas.parser.TextReader.__cinit__ (pandas/parser.c:4626)\r\n  File ""parser.pyx"", line 605, in pandas.parser.TextReader._get_header (pandas/parser.c:6089)\r\nCParserError: Passed header=0 but only 0 lines in file\r\n\r\n======================================================================\r\nFAIL: test_round_trip_frame (pandas.io.tests.test_clipboard.TestClipboard)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 69, in test_round_trip_frame\r\n    self.check_round_trip_frame(dt)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 57, in check_round_trip_frame\r\n    tm.assert_frame_equal(data, result, check_dtype=False)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/util/testing.py"", line 563, in assert_frame_equal\r\n    assert_index_equal(left.columns, right.columns)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/util/testing.py"", line 501, in assert_index_equal\r\n    right.dtype))\r\nAssertionError: [index] left [object Index([u\'LpXeYtA8n5\', u\'5Y0fhC0WyE\', u\'tTHQGDhUd3\'], dtype=\'object\')], right [Index([u\'xKjebdbwV9\', u\'QQD0rTKXvy\', u\'MdjTlE5CWY\'], dtype=\'object\') object]\r\n\r\n======================================================================\r\nFAIL: test_round_trip_frame_sep (pandas.io.tests.test_clipboard.TestClipboard)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 61, in test_round_trip_frame_sep\r\n    self.check_round_trip_frame(dt,sep=\',\')\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 57, in check_round_trip_frame\r\n    tm.assert_frame_equal(data, result, check_dtype=False)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/util/testing.py"", line 563, in assert_frame_equal\r\n    assert_index_equal(left.columns, right.columns)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/util/testing.py"", line 501, in assert_index_equal\r\n    right.dtype))\r\nAssertionError: [index] left [object Index([u\'9da8bYrdhD\', u\'h4CraEsPjW\', u\'N0bmM7oPk7\'], dtype=\'object\')], right [Index([u\'a\', u\'b\', u\'c\'], dtype=\'object\') object]\r\n\r\n======================================================================\r\nFAIL: test_round_trip_frame_string (pandas.io.tests.test_clipboard.TestClipboard)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 65, in test_round_trip_frame_string\r\n    self.check_round_trip_frame(dt,excel=False)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_clipboard.py"", line 57, in check_round_trip_frame\r\n    tm.assert_frame_equal(data, result, check_dtype=False)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/util/testing.py"", line 563, in assert_frame_equal\r\n    assert_index_equal(left.columns, right.columns)\r\n  File ""/BUILDDIR/pandas-0.14.0+git17-g3849d5d/debian/tmp/usr/lib/python2.7/dist-packages/pandas/util/testing.py"", line 501, in assert_index_equal\r\n    right.dtype))\r\nAssertionError: [index] left [object Index([u\'9da8bYrdhD\', u\'h4CraEsPjW\', u\'N0bmM7oPk7\'], dtype=\'object\')], right [Index([u\'a\', u\'b\', u\'c\'], dtype=\'object\') object]\r\n\r\n----------------------------------------------------------------------\r\nRan 7157 tests in 763.157s\r\n\r\nFAILED (SKIP=247, errors=1, failures=3)\r\n```\r\n'"
7373,35154181,sinhrks,jreback,2014-06-06 15:12:56,2014-06-14 15:34:24,2014-06-14 14:00:51,closed,,0.14.1,5,Bug;Groupby;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7373,b'BUG: resample raises ValueError when NaT is included',"b""Closes #7227. `resample(how=count)` should work now.\r\n\r\nThere are some aggregations which doesn't work with `TimeGrouper` yet, I'll open separate issue. """
7371,35139423,sinhrks,jreback,2014-06-06 11:53:48,2014-07-05 04:49:40,2014-06-06 13:00:33,closed,,0.14.1,6,Bug;Frequency;Timezones,https://api.github.com/repos/pydata/pandas/issues/7371,b'BUG: infer_freq results in None for hourly freq with timezone',"b""Found fix #7318 was incorrect. It works for lower frequencies than dates, but not for higher freq than hours.\r\n I couldn't detect as test cases are also incorrect...\r\n\r\nThis should work for all the freqs with timezones.\r\n\r\nNOTE: I couldn't use normal `tslib.tz_convert` which returns incorrect results maybe caused by DST. Applying `tslib.tz_convert_single` can work though."""
7369,35118500,bquistorff,jreback,2014-06-06 04:38:38,2014-06-13 20:33:41,2014-06-13 20:32:59,closed,,0.14.1,11,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/7369,b'add fix for opening zero observation dta files',"b'Opening a Stata dta file with no observations (but having variables) resulted in an error. Example file: https://dl.dropboxusercontent.com/u/6705315/no_obs_v115.dta.\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.read_stata(""no_obs_v115.dta"")\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\stata.py"", line 49, in read_stata\r\n    return reader.data(convert_dates, convert_categoricals, index)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\stata.py"", line 855, in data\r\n    data = DataFrame(data, columns=self.varlist, index=index)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 255, in __init__\r\n    copy=copy)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 367, in _init_ndarray\r\n    return create_block_manager_from_blocks([values.T], [columns, index])\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 3185, in create_block_manager_from_blocks\r\n    construction_error(tot_items, blocks[0].shape[1:], axes, e)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 3166, in construction_error\r\n    passed,implied))\r\nValueError: Shape of passed values is (0, 0), indices imply (1, 0)\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: AMD64 Family 16 Model 6 Stepping 3, AuthenticAMD\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.0\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: 0.8.0\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: 1.8.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\nThe PR fixes this issue in stata.py, though maybe the issue should be fixed in DataFrame.'"
7368,35117042,cpcloud,cpcloud,2014-06-06 03:46:56,2014-06-15 12:57:35,2014-06-06 04:32:03,closed,cpcloud,0.14.1,1,Bug;Indexing;Regression,https://api.github.com/repos/pydata/pandas/issues/7368,b'BUG/REG: fix float64index -> mixed float assignment',b'closes #7366'
7367,35115622,cpcloud,cpcloud,2014-06-06 03:02:28,2014-07-12 06:13:01,2014-06-06 03:45:40,closed,cpcloud,0.14.1,8,Bug;Indexing;Regression,https://api.github.com/repos/pydata/pandas/issues/7367,b'BUG/REG: fix float64index -> mixed float assignment',b'closes #7366'
7366,35114741,fonnesbeck,cpcloud,2014-06-06 02:36:11,2014-06-19 16:27:43,2014-06-06 04:32:03,closed,cpcloud,0.14.1,19,Bug;Indexing;Regression,https://api.github.com/repos/pydata/pandas/issues/7366,b'Adding column of floats to DataFrame yields TypeError',"b""I have a function that builds a DataFrame of summary statistics that I have used routinely for several months, but now breaks due to a change in Pandas over the past few weeks. Specifically, I have the following list of floats:\r\n\r\n    (Pdb) [r[0] for r in ratios]\r\n    [1.1200000000000001, 5.0, 0.73999999999999999, 0.35999999999999999,  \r\n    1.1100000000000001, 1.1699999999999999, 0.92000000000000004,  0.94999999999999996, 1.0600000000000001, 0.77000000000000002,  \r\n    0.59999999999999998, 2.0099999999999998, 3.2999999999999998, 0.37,  \r\n    1.6100000000000001, 1.02]\r\n\r\nWhich I use to create a column in the following table:\r\n\r\n    (Pdb) table\r\n    oxygen                0     1\r\n    male               0.57  0.59\r\n    under 2 months     0.06  0.23\r\n    2-11 months        0.66  0.59\r\n    12-23 months       0.23  0.10\r\n    Jordanian          0.90  0.91\r\n    Palestinian        0.05  0.06\r\n    vitamin D < 20     0.55  0.53\r\n    vitamin D < 11     0.40  0.38\r\n    prev_cond          0.11  0.11\r\n    heart_hx           0.05  0.04\r\n    breastfed          0.68  0.56\r\n    premature          0.13  0.23\r\n    adm_pneumo         0.09  0.25\r\n    adm_bronchopneumo  0.52  0.28\r\n    adm_sepsis         0.11  0.16\r\n    adm_bronchiolitis  0.21  0.21\r\n\r\nHowever, this now causes the following:\r\n\r\n    (Pdb) table['foo'] = [r[0] for r in ratios]\r\n    *** TypeError: Not implemented for this type\r\n\r\nHere is a more verbose output:\r\n\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-49-0723b2a631c0> in <module>()\r\n    ----> 1 make_table(groupby_o2, table_vars=table_vars, replace_dict={0.0: 'No Oxygen', 1.0: 'Oxygen'})\r\n\r\n    <ipython-input-47-6f3ebc37c721> in make_table(groupby, table_vars, replace_dict)\r\n          3     ratios = [calc_or(groupby, v) for v in table.index]\r\n          4     import pdb; pdb.set_trace()\r\n    ----> 5     table['OR'] = [r[0] for r in ratios]\r\n          6     table['Interval'] = [r[1] for r in ratios]\r\n          7     table['N'] = [r[2] for r in ratios]\r\n\r\n    /usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc in __setitem__(self, key, value)\r\n       1899         else:\r\n       1900             # set column\r\n    -> 1901             self._set_item(key, value)\r\n       1902 \r\n       1903     def _setitem_slice(self, key, value):\r\n\r\n    /usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc in _set_item(self, key, value)\r\n       1982         self._ensure_valid_index(value)\r\n       1983         value = self._sanitize_column(key, value)\r\n    -> 1984         NDFrame._set_item(self, key, value)\r\n       1985 \r\n       1986         # check if we are modifying a copy\r\n\r\n    /usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc in _set_item(self, key, value)\r\n       1137 \r\n       1138     def _set_item(self, key, value):\r\n    -> 1139         self._data.set(key, value)\r\n       1140         self._clear_item_cache()\r\n       1141 \r\n\r\n    /usr/local/lib/python2.7/site-packages/pandas/core/internals.pyc in set(self, item, value, check)\r\n       2637 \r\n       2638         try:\r\n    -> 2639             loc = self.items.get_loc(item)\r\n       2640         except KeyError:\r\n       2641             # This item wasn't present, just insert at end\r\n\r\n    /usr/local/lib/python2.7/site-packages/pandas/core/index.pyc in get_loc(self, key)\r\n       2055 \r\n       2056     def get_loc(self, key):\r\n    -> 2057         if np.isnan(key):\r\n       2058             try:\r\n       2059                 return self._nan_idxs.item()\r\n\r\n    TypeError: Not implemented for this type\r\n\r\nNot exactly sure which change caused it, but this code was working on the same data 3-4 weeks ago.\r\n\r\nCurrently running 0.13.1-936-g592a537 on OS X 10.9.3, Python 2.7.6 from Homebrew."""
7360,35086406,refp16,jreback,2014-06-05 18:57:52,2014-06-16 12:51:56,2014-06-16 12:51:56,closed,,0.14.1,13,Bug;IO Stata,https://api.github.com/repos/pydata/pandas/issues/7360,b'Unable to import Stata 13 database files with read_stata()',"b'pandas v0.14.0 (May 31 , 2014) seems uncapable of importing Stata 13 datasets although according to this http://pandas.pydata.org/pandas-docs/stable/whatsnew.html, it should. Stata 12 files can be imported without problems.\r\n\r\nThe output of running this\r\n\r\n```python\r\nimport pandas\r\npandas.show_versions()\r\ndta = pandas.io.stata.read_stata(\'D:\\\\Datos\\\\rferrer\\\\Desktop\\\\myauto.dta\')\r\n```\r\n\r\nfollows:\r\n\r\n``` python\r\n%run D:/Datos/RFERRER/Desktop/import_stata13.py\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 45 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.0\r\nnose: 1.3.0\r\nCython: 0.19.2\r\nnumpy: 1.8.0\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: 0.2.0\r\nscikits.timeseries: 0.91.3\r\ndateutil: 2.2\r\npytz: 2013.8\r\nbottleneck: None\r\ntables: 2.4.0\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.2.3\r\nbs4: None\r\nhtml5lib: 0.95-dev\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.8.3\r\npymysql: None\r\npsycopg2: None\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\openpyxl\\__init__.py:31: UserWarning: The installed version of lxml is too old to be used with openpyxl\r\n  warnings.warn(""The installed version of lxml is too old to be used with openpyxl"")\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.4.0.1938.win-x86_64\\lib\\site-packages\\IPython\\utils\\py3compat.pyc in execfile(fname, glob, loc)\r\n    195             else:\r\n    196                 filename = fname\r\n--> 197             exec compile(scripttext, filename, \'exec\') in glob, loc\r\n    198     else:\r\n    199         def execfile(fname, *where):\r\n\r\nD:\\Datos\\RFERRER\\Desktop\\import_stata13.py in <module>()\r\n      3 pandas.show_versions()\r\n      4 \r\n----> 5 dta = pandas.io.stata.read_stata(\'D:\\\\Datos\\\\rferrer\\\\Desktop\\\\myauto.dta\')\r\n\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\stata.pyc in read_stata(filepath_or_buffer, convert_dates, convert_categoricals, encoding, index)\r\n     45         identifier of column that should be used as index of the DataFrame\r\n     46     """"""\r\n---> 47     reader = StataReader(filepath_or_buffer, encoding)\r\n     48 \r\n     49     return reader.data(convert_dates, convert_categoricals, index)\r\n\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\stata.pyc in __init__(self, path_or_buf, encoding)\r\n    455             self.path_or_buf = path_or_buf\r\n    456 \r\n--> 457         self._read_header()\r\n    458 \r\n    459     def _read_header(self):\r\n\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\stata.pyc in _read_header(self)\r\n    657 \r\n    658         """"""Calculate size of a data record.""""""\r\n--> 659         self.col_sizes = lmap(lambda x: self._calcsize(x), self.typlist)\r\n    660 \r\n    661     def _calcsize(self, fmt):\r\n\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\stata.pyc in <lambda>(x)\r\n    657 \r\n    658         """"""Calculate size of a data record.""""""\r\n--> 659         self.col_sizes = lmap(lambda x: self._calcsize(x), self.typlist)\r\n    660 \r\n    661     def _calcsize(self, fmt):\r\n\r\nC:\\Users\\rferrer\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\stata.pyc in _calcsize(self, fmt)\r\n    661     def _calcsize(self, fmt):\r\n    662         return (type(fmt) is int and fmt\r\n--> 663                 or struct.calcsize(self.byteorder + fmt))\r\n    664 \r\n    665     def _col_size(self, k=None):\r\n\r\nTypeError: cannot concatenate \'str\' and \'NoneType\' objects\r\n```\r\nThe dataset `myauto.dta` is just the `auto` dataset made available running `sysuse auto` within Stata.\r\n\r\nThe problem is originally documented here: http://stackoverflow.com/questions/24053652/pandas-and-stata-13-files.\r\n\r\nMy Python is set up with [Enthough Canopy 1.4.0 (64 bit)](https://www.enthought.com/products/canopy/).\r\n'"
7358,35077222,toddrjen,jreback,2014-06-05 17:08:03,2014-06-22 15:28:29,2014-06-11 14:33:38,closed,,0.14.1,9,Bug;Dtypes;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/7358,b'Add test for core.nanops and fix several bugs in nanops',b'This pull request accomplishes two main things:\r\n\r\n1. It adds unit tests for ```nanops```\r\n2. It fixes several bugs identified by the unit tests\r\n\r\nrelated #7352 \r\nrelated #7353 \r\nrelated #7354 \r\nrelated #7357 '
7357,35075952,toddrjen,jreback,2014-06-05 16:52:34,2014-06-13 16:43:25,2014-06-13 16:43:25,closed,,0.14.1,4,Bug;Dtypes;Internals;Numeric,https://api.github.com/repos/pydata/pandas/issues/7357,"b""BUG: nanops._has_infs doesn't work with many dtypes""","b""```nanops._has_infs```, which is used in determining whether to employ bottleneck optimizations, fails on dtypes which should otherwise be supported, including 'f2', complex, and multidimensional 'f4' and 'f8'.\r\n\r\n```Python\r\n>>> from pandas.core import nanops\r\n>>> import numpy as np\r\n>>> \r\n>>> val0 = np.ones(10)*np.inf\r\n>>> val1 = val0.astype('f2')\r\n>>> val2 = val0+val0*1j\r\n>>> val3 = np.tile(val0, (1, 10))\r\n>>> \r\n>>> nanops._has_infs(val0)\r\nTrue\r\n>>> nanops._has_infs(val1)\r\nFalse\r\n>>> nanops._has_infs(val2)\r\nFalse\r\n>>> nanops._has_infs(val3)\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n```"""
7354,35074039,toddrjen,jreback,2014-06-05 16:30:49,2014-06-12 12:59:31,2014-06-12 12:59:31,closed,,0.14.1,1,Bug;Internals;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/7354,b'BUG: several nanops fail when axis==0 for 1-dimensional nan arrays',"b""Several of the functions in ```nanops```, including ```nanmean```, ```nanmedian```, and anything using ```_get_counts``` will crash when using a 1-dimensional all-nan array when the ```axis``` argument is set to 0.\r\n\r\n```Python\r\n>>> from pandas.core import nanops\r\n>>> import bottleneck\r\n>>> import numpy as np\r\n>>> nanops._USE_BOTTLENECK = False\r\n>>> \r\n>>> val = np.tile(np.nan, (5,))\r\n>>> \r\n>>> bottleneck.nanmean(val, axis=0)\r\nnan\r\n>>> nanops.nanmean(val, axis=0)\r\nTypeError: 'numpy.float64' object does not support item assignment\r\n```"""
7353,35073367,toddrjen,jreback,2014-06-05 16:23:37,2014-06-12 13:47:33,2014-06-12 10:46:48,closed,,0.14.1,3,Bug;Internals;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/7353,"b""BUG: nanops._maybe_null_out doesn't work with complex numbers""","b""A number of nanops functions rely on the private internal function ```nanops._maybe_null_out```.  These include ```nansum```, ```nanmin```, ```nanmax```, and ```nanprod```.  All of these, in principle, should work with arrays with a complex dtype.  However, due to a bug in ```_maybe_null_out```, they convert the array to a float under certain situations, resulting in erroneous results.  The easiest way to trigger this is to have a complex array where all the imaginary values are nan, and make ```axis``` argument something other than ```None```.\r\n\r\n```Python\r\n>>> from pandas.core import nanops\r\n>>> import bottleneck\r\n>>> import numpy as np\r\n>>> nanops._USE_BOTTLENECK = False\r\n>>> \r\n>>> val = np.tile(1, (11, 7)) + np.tile(np.nan*1j, (11, 7))\r\n>>> \r\n>>> bottleneck.nansum(val, axis=0)\r\narray([ nan+0.j,  nan+0.j,  nan+0.j,  nan+0.j,  nan+0.j,  nan+0.j,  nan+0.j])\r\n>>> nanops.nansum(val, axis=0)\r\narray([ nan,  nan,  nan,  nan,  nan,  nan,  nan])\r\n>>> bottleneck.nansum(val, axis=0).dtype\r\ndtype('complex128')\r\n>>> nanops.nansum(val, axis=0).dtype\r\ndtype('float64')\r\n```"""
7352,35071910,toddrjen,jreback,2014-06-05 16:07:55,2014-06-12 22:41:29,2014-06-12 22:41:29,closed,,0.14.1,4,Bug;Internals;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/7352,"b""BUG: nanops.nanmedian doesn't work when axis=None""","b""The default value for the ```axis``` argument for ```nanops.nanmedia``` is ```None```.  However, the function raises an exception when using this value:\r\n\r\n```Python\r\n>>> from pandas.core import nanops\r\n>>> import bottleneck\r\n>>> import numpy as np\r\n>>> nanops._USE_BOTTLENECK = False\r\n>>> \r\n>>> val = np.random.randn(13, 11, 7)\r\n>>> \r\n>>> bottleneck.nanmedian(val, axis=None).ndim\r\n0\r\n>>> nanops.nanmedian(val, axis=None)\r\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'int'\r\n>>> nanops.nanmedian(val)\r\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'int'\r\n```"""
7350,35052675,jreback,jreback,2014-06-05 12:34:39,2014-06-27 12:37:06,2014-06-05 13:30:46,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7350,b'BUG: Bug in .loc with a list of indexers on a single-multi index level (that is not nested) GH7349',b'closes #7349'
7349,35048510,jreback,jreback,2014-06-05 11:34:07,2014-06-05 13:30:46,2014-06-05 13:30:46,closed,,0.14.1,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7349,b'BUG: loc with a multi-index and a list incorrectly selecting by position',"b'\r\nfrom [SO](http://stackoverflow.com/questions/24056277/pandas-use-array-index-all-values)\r\n\r\nThis is a bug; the first should work like the 2nd (it is incorrectly selecting by position); essentially this is ``.ix`` fallback type behavior\r\n\r\n```\r\nIn [10]: df = DataFrame(np.arange(12).reshape(-1,1),index=pd.MultiIndex.from_product([[1,2,3,4],[1,2,3]]))\r\n\r\nIn [11]: df.loc[[1,2]]\r\nOut[11]: \r\n     0\r\n1 2  1\r\n  3  2\r\n\r\nIn [12]: df.loc[([1,2],),:]\r\nOut[12]: \r\n     0\r\n1 1  0\r\n  2  1\r\n  3  2\r\n2 1  3\r\n  2  4\r\n  3  5\r\n```'"
7336,34957934,onesandzeroes,jreback,2014-06-04 13:22:59,2014-06-16 14:59:08,2014-06-05 17:57:28,closed,,0.14.1,8,Bug,https://api.github.com/repos/pydata/pandas/issues/7336,b'BUG: Series.map fails when keys are tuples of different lengths (#7333)',"b""closes #7333 seems to be due to the new behaviour of `Series`, which will automatically create a MultiIndex out of the dict's keys when creating a series to map the values. The MultiIndex doesn't match the values for the shorter tuples, so they fail to map.\r\n\r\nThe fix is fairly simple, just pass the dict keys explicitly as the index.\r\n\r\nI've added a test case for this specific issue, but could add some more if others can see related issues that would arise from this. I'll try to finalize the PR (squashing into a single commit, rebasing if necessary) tomorrow if it looks good."""
7333,34936804,onesandzeroes,jreback,2014-06-04 07:49:00,2014-06-05 17:57:28,2014-06-05 17:57:28,closed,,0.14.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/7333,"b""BUG: Series.map(dict) doesn't work when dict keys are tuples of length 1?""","b""I ran across an odd issue today where some analysis code that was working pre-pandas 0.14.0 started failing. The issue seems to be specific to trying to use `Series.map(some_dict)` when the dict keys (and values in the series) are single item tuples. Code to demonstrate the issue:\r\n\r\n```python\r\nimport pandas as pd\r\npd.version.version\r\nOut[10]: '0.14.0'\r\n\r\ndf = pd.DataFrame({'a': [(1,), (2,), (3, 4), (5, 6)]})\r\nlabel_mappings = {\r\n    (1,): 'A',\r\n    (2,): 'B',\r\n    (3, 4): 'A',\r\n    (5, 6): 'B'\r\n}\r\ndf['labels'] = df['a'].map(label_mappings)\r\ndf\r\nOut[7]: \r\n        a labels\r\n0    (1,)    NaN\r\n1    (2,)    NaN\r\n2  (3, 4)      A\r\n3  (5, 6)      B\r\n```\r\n\r\nThe expected output here is:\r\n\r\n```python\r\ndf\r\nOut[7]: \r\n        a labels\r\n0    (1,)      A\r\n1    (2,)      B\r\n2  (3, 4)      A\r\n3  (5, 6)      B\r\n```"""
7332,34931590,dsm054,jreback,2014-06-04 05:47:55,2014-06-04 20:30:39,2014-06-04 20:30:39,closed,,0.14.1,12,Bug;Performance,https://api.github.com/repos/pydata/pandas/issues/7332,b'strange dtype behaviour as function of series length',"b""Found when tracking down what was going on with [this question](http://stackoverflow.com/questions/24028281/pandas-series-operations-very-slow-after-upgrade#comment37041169_24028281) about performance.\r\n\r\nFirst the case that makes sense:\r\n\r\n```\r\n>>> s = pd.Series(range(10**3), dtype=np.int32)\r\n>>> s.dtype\r\ndtype('int32')\r\n>>> s.dtype.type\r\n<type 'numpy.int32'>\r\n>>> s.dtype.type in pd.lib._TYPE_MAP\r\nTrue\r\n>>> \r\n>>> orig_sum_type = (s+s).dtype.type\r\n>>> orig_sum_type\r\n<type 'numpy.int32'>\r\n>>> orig_sum_type in pd.lib._TYPE_MAP\r\nTrue\r\n```\r\n\r\nNow let's increase the length of the series.\r\n\r\n``` \r\n>>> s = pd.Series(range(10**5), dtype=np.int32)\r\n>>> s.dtype\r\ndtype('int32')\r\n>>> s.dtype.type\r\n<type 'numpy.int32'>\r\n>>> s.dtype.type in pd.lib._TYPE_MAP\r\nTrue\r\n>>> \r\n>>> new_sum_type = (s+s).dtype.type\r\n>>> new_sum_type\r\n<type 'numpy.int32'>\r\n>>> new_sum_type in pd.lib._TYPE_MAP\r\nFalse\r\n```\r\n\r\n.. wait, what?\r\n\r\n```\r\n>>> orig_sum_type, new_sum_type\r\n(<type 'numpy.int32'>, <type 'numpy.int32'>)\r\n>>> orig_sum_type == new_sum_type\r\nFalse\r\n>>> orig_sum_type is new_sum_type\r\nFalse\r\n>>> np.int32 is orig_sum_type\r\nTrue\r\n>>> np.int32 is new_sum_type\r\nFalse\r\n```\r\n\r\nWe've now got a new `numpy.int32` type floating around, not equal to the one in `numpy`.  The crossover seems to be at 10k:\r\n\r\n```\r\n>>> def find_first():\r\n...         for i in range(1, 10**5):\r\n...                 s = pd.Series(range(i), dtype=np.int32)\r\n...                 if (s+s).dtype.type not in pd.lib._TYPE_MAP:\r\n...                         return i\r\n...         \r\n>>> find_first()\r\n10001\r\n```\r\n\r\nISTM that this lack of recognition of the dtype as in `_TYPE_MAP` prevents the early exit from being taken in `infer_dtype` upon recognition that it's an integer dtype, and that slows things down considerably."""
7331,34924282,xdliao,jreback,2014-06-04 02:56:13,2014-12-01 00:46:44,2014-12-01 00:46:44,closed,,0.15.2,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/7331,b'merge does not preserve order of data frame when left key has duplicate values ',"b'```\r\nimport pandas as pd\r\ndates = [  20140101, 20140102, 20140103]\r\nstates = [  ""CA"",  ""NY"", ""CA""]\r\nx = pd.DataFrame({ \'dates\' : dates, \'states\' : states })\r\n\r\n#y = pandas.DataFrame({ \'state\' : [ \'CA\', \'OR\' ], \'value\' : [ 1, 2]})\r\ny = pd.DataFrame({ \'states\' : [ ""CA"", ""NY"" ], \'stateid\' : [ 1, 2]})\r\nz = pd.merge(x, y, how=\'left\', on=\'states\',)\r\n\r\n\r\nx=\r\n \tdates \tstates\r\n0 \t20140101 \tCA\r\n1 \t20140102 \tNY\r\n2 \t20140103 \tCA\r\n\r\ny=\r\n    \tstateid \tstates\r\n0 \t1 \tCA\r\n1 \t2 \tNY\r\n\r\nz= \t dates \tstates \tstateid\r\n0 \t20140101 \tCA \t1\r\n1 \t20140103 \tCA \t1\r\n2 \t20140102 \tNY \t2\r\n```\r\n------------------\r\nNote z is always sorted by ""states"" column whether argument sort=True or False.\r\nThis only happens when the x\'s states column is not unique. If x.states is unique(such as NY, CA, CT), sort=True and False behaves as expected.\r\n\r\nThis causes inconvenience when x is a time series and merge does not\r\npreserve the time sequence.\r\n\r\n'"
7328,34916350,jreback,jreback,2014-06-03 23:39:45,2014-06-16 02:31:33,2014-06-04 00:03:36,closed,,0.14.1,0,Bug;Missing-data;Numeric,https://api.github.com/repos/pydata/pandas/issues/7328,"b'BUG: Bug in broadcasting with .div, integer dtypes and divide-by-zero (GH7325)'",b'closes #7325'
7327,34912882,cpcloud,cpcloud,2014-06-03 22:58:25,2014-07-07 05:52:08,2014-06-04 20:53:41,closed,cpcloud,0.14.1,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7327,b'BUG: already mixed indexes should not sort',b'cc @jreback'
7325,34908127,dsm054,jreback,2014-06-03 21:52:22,2014-06-04 00:03:36,2014-06-04 00:03:36,closed,,0.14.1,2,Bug;Missing-data;Numeric,https://api.github.com/repos/pydata/pandas/issues/7325,b'BUG: division misbehaving',"b'Attempting to reduce [this question](http://stackoverflow.com/questions/24024928/division-in-pandas-multiple-columns-by-another-column-of-the-same-dataframe) to a minimal test case produced:\r\n\r\n```\r\n>>> df = pd.DataFrame(np.arange(3*2).reshape((3,2)))\r\n>>> df\r\n   0  1\r\n0  0  1\r\n1  2  3\r\n2  4  5\r\n>>> df.add(df[0], axis=\'index\')\r\n   0  1\r\n0  0  1\r\n1  4  5\r\n2  8  9\r\n>>> df.sub(df[0], axis=\'index\')\r\n   0  1\r\n0  0  1\r\n1  0  1\r\n2  0  1\r\n>>> df.mul(df[0], axis=\'index\')\r\n    0   1\r\n0   0   0\r\n1   4   6\r\n2  16  20\r\n>>> df.div(df[0], axis=\'index\')\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-47-5b698f939cc6>"", line 1, in <module>\r\n    df.div(df[0], axis=\'index\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_5_g3b82634-py2.7-linux-x86_64.egg/pandas/core/ops.py"", line 763, in f\r\n    return self._combine_series(other, na_op, fill_value, axis, level)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_5_g3b82634-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 2830, in _combine_series\r\n    return self._combine_match_index(other, func, level=level, fill_value=fill_value)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_5_g3b82634-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 2860, in _combine_match_index\r\n    return self._constructor(func(left.values.T, right.values).T,\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_5_g3b82634-py2.7-linux-x86_64.egg/pandas/core/ops.py"", line 754, in na_op\r\n    result = com._fill_zeros(result, x, y, name, fill_zeros)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_5_g3b82634-py2.7-linux-x86_64.egg/pandas/core/common.py"", line 1254, in _fill_zeros\r\n    np.putmask(result, mask & ~nans, fill)\r\nValueError: operands could not be broadcast together with shapes (3,) (6,) \r\n```\r\n\r\nFor comparison, if we change the types:\r\n\r\n```\r\n>>> (df*1.0).div((df*1.0)[0], axis=\'index\')\r\n    0         1\r\n0 NaN       inf\r\n1   1  1.500000\r\n2   1  1.250000\r\n```\r\n\r\nVersion info:\r\n\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-27-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.14.0-5-g3b82634\r\nnose: 1.3.3\r\nCython: 0.20.1post0\r\nnumpy: 1.8.1\r\nscipy: 0.14.0\r\nstatsmodels: 0.5.0\r\nIPython: 1.2.1\r\nsphinx: None\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.0.3\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.3\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\n\r\n'"
7322,34874565,rosnfeld,jreback,2014-06-03 15:28:12,2014-07-03 20:56:46,2014-07-01 23:59:35,closed,,0.14.1,15,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7322,"b'BUG: xlim on plots with shared axes (GH2960, GH3490)'","b""Fixes #2960\r\nFixes #3490. \r\n\r\nSummary of changes:\r\n- for irregular timeseries plots, considers all lines already plotted when calculating xlim\r\n- for plots with secondary_y axis, considers lines on both left and right axes when calculating xlim. There are tests for non-timeseries plots (always worked, but now a test to confirm this), irregular timeseries plots, and regular timeseries plots.\r\n\r\nThis does not fix #6608, which I now think is a fairly specific/rare case that could always be manually avoided by ```x_compat=True```. I will still try and fix that, but it looks a bit uglier and I didn't want to hold back these more important changes.\r\n"""
7320,34863730,sinhrks,jreback,2014-06-03 13:38:41,2014-07-12 06:20:06,2014-06-05 16:01:00,closed,,0.14.1,8,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7320,b'BUG: Revisit Index.delete preserves freq',"b'Fix the issue that freq is preserved inappropriately, caused by #7302. Fix preserves freq if edge element is being deleted (same as #7299). \r\nCC: @rosnfeld'"
7318,34854363,sinhrks,jreback,2014-06-03 11:25:36,2014-07-01 15:08:30,2014-06-03 13:16:57,closed,,0.14.1,2,Bug;Frequency;Timezones,https://api.github.com/repos/pydata/pandas/issues/7318,b'BUG: inferred_freq results in None with eastern hemisphere timezones',b'Closes #7310'
7315,34795382,cpcloud,cpcloud,2014-06-02 17:47:44,2014-06-24 04:07:16,2014-06-03 00:44:37,closed,cpcloud,0.14.1,3,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7315,"b""BUG: isnull doesn't properly check for inf when requested""",b'closes #7314'
7314,34786492,amelio-vazquez-reina,cpcloud,2014-06-02 15:58:40,2014-06-03 00:44:37,2014-06-03 00:44:37,closed,cpcloud,0.14.1,19,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7314,b'Dropping non-finite entries',"b'I have been looking for a solution for this for a long time. I tried the ideas in the following threads (with the latest Pandas):\r\n\r\n1. [Keep finite entries only in Pandas](http://stackoverflow.com/questions/22799208/keep-finite-entries-only-in-pandas)\r\n2. [Row filtering so that we only keep finite entries](http://stackoverflow.com/questions/23921766/row-filtering-so-that-we-only-have-finite-entries-in-a-specific-column?lq=1)\r\n\r\nbut none of them work. See thread 2 above, and the comments in its only answer to see why.\r\n\r\nWhat is a good way to drop indices (either rows or columns) that meet a specific criteria such as: `""they contain entries that are not finite""`.?'"
7313,34782354,cpcloud,cpcloud,2014-06-02 15:17:23,2014-06-13 04:48:46,2014-06-02 20:51:23,closed,cpcloud,0.14.1,4,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7313,b'BUG: single group series should preserve group name',"b""Extract currently doesn't preserve group names when a series is returned. This\nPR fixes that."""
7312,34780111,TomAugspurger,jreback,2014-06-02 14:54:00,2014-06-13 20:29:03,2014-06-11 15:10:21,closed,,0.14.1,5,API Design;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7312,b'BUG: quantile ignores axis kwarg',b'Closes https://github.com/pydata/pandas/issues/7306\r\n\r\nWe may want to wait on this till later today while I figure out what will happen in https://github.com/pydata/pandas/issues/7308'
7311,34779938,cpcloud,cpcloud,2014-06-02 14:52:11,2014-06-14 21:27:05,2014-06-02 16:51:23,closed,cpcloud,0.14.1,4,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/7311,b'BUG/TST: should skip openpyxl before testing version compat',
7310,34779770,sinhrks,jreback,2014-06-02 14:50:36,2014-06-06 19:34:55,2014-06-06 19:34:55,closed,,0.14.1,3,Bug;Frequency;Timezones,https://api.github.com/repos/pydata/pandas/issues/7310,b'BUG: inferred_freq results in None for eastern hemispheres timezones',"b""Found during #7299.\r\n```\r\ndates = ['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-30']\r\npd.DatetimeIndex(dates).inferred_freq\r\n# M\r\n\r\npd.DatetimeIndex(dates, tz='US/Eastern').inferred_freq\r\n# M\r\n\r\npd.DatetimeIndex(dates, tz='Mexico/General').inferred_freq\r\n# M\r\n\r\npd.DatetimeIndex(dates, tz='Asia/Tokyo').inferred_freq\r\n# None\r\n\r\npd.DatetimeIndex(dates, tz='Europe/Paris').inferred_freq\r\n# None\r\n\r\npd.DatetimeIndex(dates, tz='Australia/Sydney').inferred_freq\r\n# None\r\n```"""
7306,34756431,CRP,jreback,2014-06-02 09:13:36,2014-06-11 15:10:21,2014-06-11 15:10:21,closed,,0.14.1,3,API Design;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7306,b'.quantile ignores axis parameter',"b'```\r\nIn [10]: a=TS.DataFrame(np.random.random((10,3)))\r\n\r\nIn [11]: a\r\nOut[11]: \r\n         0        1        2\r\n0 0.293227 0.489837 0.389012\r\n1 0.070556 0.230420 0.259219\r\n2 0.105651 0.026292 0.737140\r\n3 0.859360 0.092215 0.306204\r\n4 0.377702 0.452027 0.409900\r\n5 0.801594 0.123576 0.138083\r\n6 0.810958 0.827973 0.363331\r\n7 0.325093 0.527616 0.226507\r\n8 0.903319 0.611720 0.636183\r\n9 0.951649 0.663440 0.251934\r\n\r\nIn [12]: a.quantile(0.5,axis=0)\r\nOut[12]: \r\n0   0.589648\r\n1   0.470932\r\n2   0.334767\r\ndtype: float64\r\n\r\nIn [13]: a.quantile(0.5,axis=1)\r\nOut[13]: \r\n0   0.589648\r\n1   0.470932\r\n2   0.334767\r\ndtype: float64\r\n```'"
7304,34736805,cpcloud,cpcloud,2014-06-01 21:05:19,2014-06-17 04:56:54,2014-06-02 14:43:50,closed,cpcloud,0.14.1,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/7304,b'BUG: replace() alters unrelated values',b'closes #7140'
7303,34728361,cpcloud,cpcloud,2014-06-01 13:25:57,2014-06-16 19:22:42,2014-06-01 21:14:22,closed,cpcloud,0.14.1,2,Bug;Regression,https://api.github.com/repos/pydata/pandas/issues/7303,b'BUG: do not remove temporaries from eval/query scope',b'closes #7300'
7302,34717566,sinhrks,jreback,2014-05-31 23:12:44,2014-06-16 10:54:28,2014-06-01 16:37:17,closed,,0.14.1,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7302,b'BUG: Index.delete doesnt preserve name and other attrs',"b'Made `Index.delete` to preserve `name`, `tz` and `freq` attributes.'"
7300,34716234,jorisvandenbossche,cpcloud,2014-05-31 22:07:57,2014-06-01 21:14:22,2014-06-01 21:14:22,closed,cpcloud,0.14.1,1,Bug;Regression,https://api.github.com/repos/pydata/pandas/issues/7300,b'BUG: query with local strings gives KeyError',"b'From SO question (http://stackoverflow.com/questions/23974664/unable-to-query-a-local-variable-in-pandas-0-14-0), referencing a local variable gives an error when this is a string (workaround seems to be to wrap it in a list):\r\n\r\n```\r\nIn [39]: fills = pd.DataFrame({\'Symbol\':[\'BUD US\', \'BUD US\', \'IBM US\', \'IBM US\'], \'Price\':[109.70, 109.72, 183.30, 183.35]})\r\n\r\nIn [40]: fills\r\nOut[40]: \r\n    Price  Symbol\r\n0  109.70  BUD US\r\n1  109.72  BUD US\r\n2  183.30  IBM US\r\n3  183.35  IBM US\r\n\r\nIn [41]: my_symbol = [\'BUD US\']\r\n\r\nIn [42]: fills.query(\'Symbol==@my_symbol\')\r\nOut[42]: \r\n    Price  Symbol\r\n0  109.70  BUD US\r\n1  109.72  BUD US\r\n\r\nIn [43]: my_symbol = \'BUD US\'\r\n\r\nIn [44]: fills.query(\'Symbol==@my_symbol\')\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-44-c16846018a13> in <module>()\r\n----> 1 fills.query(\'Symbol==@my_symbol\')\r\n\r\n...\r\n\r\n/home/joris/scipy/pandas/pandas/computation/expr.pyc in _rewrite_membership_op(self, node, left, right)\r\n    341             # of one string, kind of a hack\r\n    342             if right_str:\r\n--> 343                 self.env.remove_tmp(right.name)\r\n    344                 name = self.env.add_tmp([right.value])\r\n    345                 right = self.term_type(name, self.env)\r\n\r\n/home/joris/scipy/pandas/pandas/computation/scope.pyc in remove_tmp(self, name)\r\n    287             The name of a temporary to be removed\r\n    288         """"""\r\n--> 289         del self.temps[name]\r\n    290 \r\n    291     @property\r\n\r\nKeyError: \'__pd_eval_local_my_symbol\'\r\n> /home/joris/scipy/pandas/pandas/computation/scope.py(289)remove_tmp()\r\n    288         """"""\r\n--> 289         del self.temps[name]\r\n    290 \r\n```\r\n@cpcloud '"
7299,34716194,sinhrks,jreback,2014-05-31 22:06:03,2014-06-12 06:09:49,2014-06-06 12:58:23,closed,,0.14.1,21,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7299,b'BUG: DatetimeIndex.insert doesnt preserve name and tz',"b""`DatetimeIndex.insert` doesn't preserve `name` and `tz` attributes. Also modified `DatetimeIndex.asobject` to return an object `Index` which has the same name as original to cover the case when the result is being object `Index`. \r\n\r\n```\r\n# normal Index preserves its name after insertion\r\nidx = pd.Index([1, 2, 3], name='normal')\r\ninserted = idx.insert(0, 1)\r\ninserted.name\r\n# normal\r\n\r\n# But DatetimeIndex doesn't\r\ndtidx = pd.DatetimeIndex([datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4)], freq='M', name='dtidx')\r\ninserted = dtidx.insert(0, datetime.datetime(2011, 1, 5))\r\ninserted.name\r\n# None\r\n```"""
7297,34711612,kdiether,jreback,2014-05-31 17:55:39,2014-07-02 11:46:29,2014-07-02 11:46:29,closed,,0.14.1,17,Bug;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/7297,b'Change in behavior for rolling_var when win > len(arr) for 0.14: now raises error',"b'In 0.13 I could pass a window length greater than the length of the `Series` passed to `rolling_var` (or, of course, `rolling_std`). In 0.14 that raises an error. Behavior is unchanged from 0.13 for other rolling functions:\r\n\r\n```python\r\ndata = """"""\r\nx\r\n0.1\r\n0.5\r\n0.3\r\n0.2\r\n0.7\r\n""""""\r\n\r\ndf = pd.read_csv(StringIO(data),header=True)\r\n\r\n>>> pd.rolling_mean(df[\'x\'],window=6,min_periods=2)\r\n\r\n0      NaN\r\n1    0.300\r\n2    0.300\r\n3    0.275\r\n4    0.360\r\ndtype: float64\r\n\r\n>>> pd.rolling_skew(df[\'x\'],window=6,min_periods=2)\r\n\r\n0             NaN\r\n1             NaN\r\n2    3.903128e-15\r\n3    7.528372e-01\r\n4    6.013638e-01\r\ndtype: float64\r\n\r\n>>> pd.rolling_skew(df[\'x\'],window=6,min_periods=6)\r\n\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\n4   NaN\r\ndtype: float64\r\n```\r\n\r\nThose work, but not `rolling_var`:\r\n\r\n```python\r\n>>> pd.rolling_var(df[\'x\'],window=6,min_periods=2)\r\n\r\nTraceback (most recent call last):\r\n  File ""./foo.py"", line 187, in <module>\r\n    print pd.rolling_var(df[\'x\'],window=6,min_periods=2)\r\n  File ""/usr/lib64/python2.7/site-packages/pandas/stats/moments.py"", line 594, in f\r\n    center=center, how=how, **kwargs)\r\n  File ""/usr/lib64/python2.7/site-packages/pandas/stats/moments.py"", line 346, in _rolling_moment\r\n    result = calc(values)\r\n  File ""/usr/lib64/python2.7/site-packages/pandas/stats/moments.py"", line 340, in <lambda>\r\n    **kwds)\r\n  File ""/usr/lib64/python2.7/site-packages/pandas/stats/moments.py"", line 592, in call_cython\r\n    return func(arg, window, minp, **kwds)\r\n  File ""algos.pyx"", line 1177, in pandas.algos.roll_var (pandas/algos.c:28449)\r\nIndexError: Out of bounds on buffer access (axis 0)\r\n```\r\n\r\nIf this is the new desired default behavior for the rolling functions, I can work around it. I do like the behavior of `rolling_skew` and `rolling_mean` better. It was nice default behavior for me when I was doing rolling standard deviations for reasonably large financial data panels.\r\n\r\nIt looks to me like the issue is caused by the fact that the 0.14 algo for rolling variance is implemented such that the initial loop (`roll_var` (algos.pyx))  is the following:\r\n\r\n```python\r\nfor i from 0 <= i < win:\r\n```\r\n\r\nSo it loops to `win` even when `win > N`. \r\n\r\nIt looks like to me that the other rolling functions try to implement their algos in such a way that the first loop counts over the following:\r\n\r\n```python\r\nfor i from 0 <= i < minp - 1:\r\n```\r\n\r\n```python\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.10-200.fc20.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.14.0\r\nnose: 1.3.1\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.13.3\r\nstatsmodels: 0.6.0.dev-b52bc09\r\nIPython: 2.0.0\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: 0.8.0\r\ntables: None\r\nnumexpr: 2.4\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.3\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.4\r\npymysql: None\r\npsycopg2: None\r\nNon\r\n```\r\nKarl D.'"
7295,34709006,dsm054,jreback,2014-05-31 15:50:18,2014-06-17 11:22:59,2014-06-17 11:22:59,closed,,0.14.1,5,Bug;Missing-data;Testing,https://api.github.com/repos/pydata/pandas/issues/7295,b'BUG: test_interp_regression failure',"b'```\r\ntest_interp_regression (__main__.TestSeries) ... > /home/dsm/sys/pandas/pandas/tests/stringsource(327)View.MemoryView.memoryview.__cinit__ (scipy/interpolate/_ppoly.c:19922)()\r\n(Pdb) cont\r\nERROR\r\n\r\n======================================================================\r\nERROR: test_interp_regression (__main__.TestSeries)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test_generic.py"", line 501, in test_interp_regression\r\n    interp_s = ser.reindex(new_index).interpolate(method=\'pchip\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/generic.py"", line 2582, in interpolate\r\n    **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2197, in interpolate\r\n    return self.apply(\'interpolate\', **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2164, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 667, in interpolate\r\n    **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 733, in _interpolate\r\n    interp_values = np.apply_along_axis(func, axis, data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/lib/shape_base.py"", line 81, in apply_along_axis\r\n    res = func1d(arr[tuple(i.tolist())],*args)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 730, in func\r\n    bounds_error=False, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/common.py"", line 1489, in interpolate_1d\r\n    bounds_error=bounds_error, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.14.0_12_g150cb3b-py2.7-linux-i686.egg/pandas/core/common.py"", line 1541, in _interpolate_scipy_wrapper\r\n    new_y = method(x, y, new_x)\r\n  File ""/usr/local/lib/python2.7/dist-packages/scipy/interpolate/_monotone.py"", line 221, in pchip_interpolate\r\n    return P(x)\r\n  File ""/usr/local/lib/python2.7/dist-packages/scipy/interpolate/_monotone.py"", line 98, in __call__\r\n    out = self._bpoly(x, der, extrapolate)\r\n  File ""/usr/local/lib/python2.7/dist-packages/scipy/interpolate/interpolate.py"", line 689, in __call__\r\n    self._evaluate(x, nu, extrapolate, out)\r\n  File ""/usr/local/lib/python2.7/dist-packages/scipy/interpolate/interpolate.py"", line 1087, in _evaluate\r\n    self.x, x, nu, bool(extrapolate), out, self.c.dtype)\r\n  File ""_ppoly.pyx"", line 846, in scipy.interpolate._ppoly.evaluate_bernstein (scipy/interpolate/_ppoly.c:15014)\r\n  File ""stringsource"", line 622, in View.MemoryView.memoryview_cwrapper (scipy/interpolate/_ppoly.c:23370)\r\n  File ""stringsource"", line 327, in View.MemoryView.memoryview.__cinit__ (scipy/interpolate/_ppoly.c:19922)\r\nValueError: buffer source array is read-only\r\n\r\n----------------------------------------------------------------------\r\nRan 63 tests in 3.981s\r\n\r\nFAILED (SKIP=1, errors=1)\r\n```\r\n\r\nVersion info:\r\n\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 32\r\nOS: Linux\r\nOS-release: 3.11.0-20-generic\r\nmachine: i686\r\nprocessor: i686\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.14.0-12-g150cb3b\r\nnose: 1.3.1\r\nCython: 0.20.1\r\nnumpy: 1.9.0.dev-ef7901d\r\nscipy: 0.15.0.dev-93656a8\r\nstatsmodels: 0.6.0.dev-985037f\r\nIPython: 2.0.0-dev\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.1\r\nbottleneck: 0.8.0\r\ntables: None\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.x\r\nopenpyxl: 2.0.2\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.3\r\nbs4: 4.3.2\r\nhtml5lib: 1.0b3\r\nbq: None\r\napiclient: 1.2\r\nrpy2: None\r\nsqlalchemy: 0.9.3\r\npymysql: None\r\npsycopg2: None\r\n```\r\n'"
7286,34677155,bquistorff,jreback,2014-05-30 20:12:56,2014-06-16 12:52:02,2014-06-16 12:52:02,closed,,0.14.1,4,Bug;IO Stata;Unicode,https://api.github.com/repos/pydata/pandas/issues/7286,b'Error when writing non-ascii allowed characters to Stata dta',"b'When trying to write a Stata dataset with strings containing upper latin-1 characters (which are allowed by the Stata format), I get an encoding error.\r\n```python\r\nimport pandas.io.stata as sta\r\nsr = sta.StataReader(\'pandas/pandas/io/tests/data/stata1_encoding.dta\')\r\ndf = sr.data()\r\nsw = sta.StataWriter(\'stata1_encoding_dup.dta\', df)\r\nsw.write_file()\r\n```\r\nI get the following output:\r\n```python\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\stata.py"", line 1242, in write_file\r\n    self._write_data_nodates()\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\stata.py"", line 1326, in _write_data_nodates\r\n    self._write(var)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\stata.py"", line 1104, in _write\r\n    self._file.write(to_write)\r\nUnicodeEncodeError: \'ascii\' codec can\'t encode character u\'\\xfc\' in position 1:ordinal not in range(128)\r\n```\r\nMachine info:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: AMD64 Family 16 Model 6 Stepping 3, AuthenticAMD\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.14.0\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: 0.8.0\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: 1.8.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```'"
7285,34657522,cpcloud,cpcloud,2014-05-30 15:54:36,2014-06-12 12:04:55,2014-06-02 11:29:30,closed,cpcloud,0.14.1,22,Bug;Compat;Error Reporting;IO Excel,https://api.github.com/repos/pydata/pandas/issues/7285,b'BUG/CI/TST: fix format strings',b'Also test against an unsupported version of openpyxl (2.0.3)\n\ncloses #7284'
7284,34655786,jd,cpcloud,2014-05-30 15:35:31,2014-07-02 00:35:36,2014-06-02 11:29:30,closed,,0.14.1,25,Bug;Compat;IO Excel,https://api.github.com/repos/pydata/pandas/issues/7284,b'Pandas 0.14 broke Python 2.6 support',"b'Since today I\'ve the following issue when importing Pandas on Python 2.6\r\n\r\n```\r\n2014-05-30 15:14:29.999 |   File ""gnocchi/carbonara.py"", line 22, in <module>\r\n2014-05-30 15:14:29.999 |     import pandas\r\n2014-05-30 15:14:29.999 |   File ""/home/jenkins/workspace/gate-gnocchi-python26/.tox/py26/lib/python2.6/site-packages/pandas/__init__.py"", line 45, in <module>\r\n2014-05-30 15:14:30.000 |     from pandas.io.api import *\r\n2014-05-30 15:14:30.000 |   File ""/home/jenkins/workspace/gate-gnocchi-python26/.tox/py26/lib/python2.6/site-packages/pandas/io/api.py"", line 7, in <module>\r\n2014-05-30 15:14:30.000 |     from pandas.io.excel import ExcelFile, ExcelWriter, read_excel\r\n2014-05-30 15:14:30.000 |   File ""/home/jenkins/workspace/gate-gnocchi-python26/.tox/py26/lib/python2.6/site-packages/pandas/io/excel.py"", line 626, in <module>\r\n2014-05-30 15:14:30.000 |     .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))\r\n2014-05-30 15:14:30.000 | ValueError: zero length field name in format\r\n```'"
7280,34596224,hayd,jreback,2014-05-29 21:40:05,2014-09-09 23:30:37,2014-09-09 23:25:57,closed,hayd,0.15.0,11,Bug;Dtypes;Reshaping;Timedelta,https://api.github.com/repos/pydata/pandas/issues/7280,b'FIX DataFrame diff with timedelta ',b'fixes #4533\r\n\r\n*need to fix test in numpy 1.6...*'
7279,34587897,sinhrks,jreback,2014-05-29 20:05:50,2014-06-29 10:25:42,2014-05-30 14:24:07,closed,,0.14.1,1,Bug;Dtypes;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7279,b'BUG: Index.min and max doesnt handle nan and NaT properly',b'Closes #7261.'
7277,34556234,sinhrks,TomAugspurger,2014-05-29 13:54:32,2014-06-13 16:34:47,2014-05-30 19:02:13,closed,,0.14.1,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7277,b'BUG: hist raises TypeError when df contains non numeric column',"b""`DataFrame.hist` without `by` keyword raises `TypeError` when it contains non-numeric column.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas.util.testing as tm\r\n\r\nn=100\r\ngender = tm.choice(['Male', 'Female'], size=n)\r\nclassroom = tm.choice(['A', 'B', 'C'], size=n)\r\nsingle = tm.choice(['S'], size=n)\r\n\r\ndf = pd.DataFrame({'gender': gender,\r\n                   'classroom': classroom,\r\n                   'height': np.random.normal(66, 4, size=n),\r\n                   'weight': np.random.normal(161, 32, size=n),\r\n                   'category': np.random.randint(4, size=n)})\r\n\r\ndf.hist(by='gender')\r\n# This works, even though 'classroom' (str) column in still contained\r\n\r\ndf.hist()\r\n# TypeError: cannot concatenate 'str' and 'float' objects\r\n```"""
7276,34553192,sinhrks,jreback,2014-05-29 13:10:18,2014-06-13 19:38:11,2014-05-30 15:35:32,closed,,0.14.1,1,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/7276,b'BUG: TimeGrouper doesnt exclude the column specified by key',b'This solves latter exclude column issue described in #7227.'
7261,34467040,sinhrks,jreback,2014-05-28 13:42:55,2014-05-30 14:24:07,2014-05-30 14:24:07,closed,,0.14.1,1,Bug;Dtypes;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7261,b'API: Should Index.min and max use nanmin and nanmax?',"b""Index and Series `min` and `max` handles `nan` and `NaT` differently. Even though `min` and `max` are defined in `IndexOpsMixin`, `Series` doesn't use them and use `NDFrame` definitions.\r\n\r\n```\r\npd.Index([np.nan, 1.0]).min()\r\n# nan\r\n\r\npd.Index([np.nan, 1.0]).max()\r\n# nan\r\n\r\npd.DatetimeIndex([pd.NaT, '2011-01-01']).min()\r\n# NaT\r\n\r\npd.DatetimeIndex([pd.NaT, '2011-01-01']).max()\r\n# 2011-01-01 00:00:00\r\n\r\n# Series excludes nan and NaT\r\npd.Series([np.nan, 1.0]).min()\r\n# 1.0\r\n\r\npd.Series([np.nan, 1.0]).max()\r\n# 1.0\r\n\r\npd.Series([pd.NaT, pd.Timestamp('2011-01-01')]).min()\r\n# 2011-01-01 00:00:00\r\n\r\npd.Series([pd.NaT, pd.Timestamp('2011-01-01')]).max()\r\n# 2011-01-01 00:00:00\r\n```\r\n"""
7256,34441467,benjaminmgross,jreback,2014-05-28 07:17:22,2014-05-30 23:32:33,2014-05-30 23:32:33,closed,,0.14.1,7,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7256,b'DatetimeIndex.insert works with only positive values',"b'Consider allowing negative values for `loc` parameter of `DatetimeIndex.insert(loc, item)`.  Currently, negative parameters result in an ""off by 1"" return value.  For example:\r\n\r\n    In  [ 1]: index = pandas.DatetimeIndex(start = \'01/01/2000\', freq = \'B\', periods = 5)\r\n    In  [ 2]: dt = pandas.Timestamp(\'01/01/2013\')\r\n    In  [ 3]: new_ind = index.insert(-1, dt)\r\n    In  [ 4]: new_ind[-1]\r\n    Out[ 4]: Timestamp(\'2000-01-07 00:00:00\', tz=None)\r\n\r\nBut:\r\n\r\n    In  [ 5]: new_ind[-2]\r\n    Out[ 5]: Timestamp(\'2013-01-01 00:00:00\', tz=None)\r\n\r\nI can put together a quick fork if it\'s a change you\'d like.  If not, I think it makes sense to change the docstring to say that ""negative values are not allowed.""\r\n\r\n'"
7242,34348516,wabu,hayd,2014-05-27 07:42:41,2014-06-16 22:44:39,2014-06-04 03:33:43,closed,,0.14.1,12,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7242,b'BUG: string methods on empty series (GH7241)',b'closes #7241'
7241,34345499,wabu,jreback,2014-05-27 06:39:35,2014-06-05 17:30:05,2014-06-05 17:30:05,closed,,0.14.1,1,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/7241,b'StringMethods.extract fails on empty series',"b""When using extract with more then one group, it fails on empty series:\r\n```\r\n>>> pd.Series([], dtype=str).str.extract('()')\r\nSeries([], dtype: float64)\r\n\r\n>>> pd.Series([], dtype=str).str.extract('()()')\r\nValueError: Shape of passed values is (0, 0), indices imply (2, 0)\r\n```\r\nI expect it to return an empty DataFrame with the correct columns."""
7238,34327404,cpcloud,cpcloud,2014-05-26 20:15:08,2014-05-26 20:36:18,2014-05-26 20:28:19,closed,,0.14.1,4,Bug;Msgpack,https://api.github.com/repos/pydata/pandas/issues/7238,"b""read_msgpack doesn't work for StringIO""","b""```\r\nIn [1]: sio = StringIO()\r\n\r\nIn [2]: pd.to_msgpack(sio, 'a', 'b')\r\n\r\nIn [3]: pd.read_msgpack(sio)\r\nOut[3]: []\r\n```"""
7234,34243771,sinhrks,jreback,2014-05-24 22:12:03,2014-07-09 12:37:50,2014-07-07 19:27:56,closed,,0.14.1,5,Bug;Groupby;Visualization,https://api.github.com/repos/pydata/pandas/issues/7234,b'BUG: grouped hist raises AttributeError with single group',"b""Includes 3 minor fixes for `hist`.\r\n\r\n+ `DataFrame.hist` raises `AttributeError` when the target column specifiedwith `by` kw only contains a single value.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas.util.testing as tm\r\n\r\nn=100\r\ngender = tm.choice(['Male', 'Female'], size=n)\r\nclassroom = tm.choice(['A', 'B', 'C'], size=n)\r\nsingle = tm.choice(['S'], size=n)\r\n\r\ndf = pd.DataFrame({'gender': gender,\r\n                   'classroom': classroom,\r\n                   'single': single, \r\n                   'height': np.random.normal(66, 4, size=n),\r\n                   'weight': np.random.normal(161, 32, size=n),\r\n                   'category': np.random.randint(4, size=n)})\r\n\r\ndf.hist(by='single')\r\n# AttributeError: 'AxesSubplot' object has no attribute 'ravel'\r\n```\r\n\r\n\r\n+ `hist` can accept `rot` kw only when `by` is specified, but `rot` actually does nothing. I understand `rot` value should be used for `xticklabels` rotation if `xrot` is not specified?\r\n```\r\ndf.hist(rot=45)\r\n# AttributeError: Unknown property rot\r\n# -> This is OK because we can use xrot kw\r\n\r\ndf.hist(by='classroom', rot=45)\r\n# It raises no error, but ticks are NOT rotated.\r\n```\r\n\r\n+ `hist` always displays `xticklabels` on all axes even if specifying `sharex=True`. Other plots only draws `xticklabels` on bottom axes if `sharex` is True."""
7232,34241846,bjonen,jreback,2014-05-24 20:17:51,2014-06-27 11:31:39,2014-05-25 16:06:45,closed,,0.14.0,16,Bug;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7232,b'BUG: multi-index output formatting is buggy (GH7174)',b'Closes #7174\r\n\r\nAdjusts the truncation in the notebook for multiindex dfs. Both rows and columns are affected. \r\n\r\nhttp://nbviewer.ipython.org/gist/bjonen/492fea9559fd73edf579'
7229,34235905,jreback,jreback,2014-05-24 15:14:25,2014-06-21 20:15:09,2014-06-21 20:15:09,closed,cpcloud,0.14.1,5,Bug;IO HTML,https://api.github.com/repos/pydata/pandas/issues/7229,b'BUG: 3.4 with current versions of bs4/lxml/html5lib minor parsing errors',"b'```\r\n======================================================================\r\nERROR: test_thousands_macau_index_col (pandas.io.tests.test_html.TestReadHtml)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\tests\\test_html.py"", line 410, in test_thousands_macau_index_col\r\n    dfs = self.read_html(macau_data, index_col=0, header=0)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\tests\\test_html.py"", line 96, in read_html\r\n    return read_html(*args, **kwargs)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\html.py"", line 840, in read_html\r\n    parse_dates, tupleize_cols, thousands, attrs)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\html.py"", line 709, in _parse\r\n    raise_with_traceback(retained)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\compat\\__init__.py"", line 705, in raise_with_traceback\r\n    raise exc.with_traceback(traceback)\r\nUnicodeDecodeError: \'charmap\' codec can\'t decode byte 0x81 in position 6275: character maps to <undefined>\r\n\r\n======================================================================\r\nERROR: test_thousands_macau_stats (pandas.io.tests.test_html.TestReadHtml)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\tests\\test_html.py"", line 401, in test_thousands_macau_stats\r\n    attrs={\'class\': \'style1\'})\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\tests\\test_html.py"", line 96, in read_html\r\n    return read_html(*args, **kwargs)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\html.py"", line 840, in read_html\r\n    parse_dates, tupleize_cols, thousands, attrs)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\io\\html.py"", line 709, in _parse\r\n    raise_with_traceback(retained)\r\n  File ""c:\\Users\\Jeff Reback\\Documents\\GitHub\\pandas\\build\\lib.win-amd64-3.4\\pandas\\compat\\__init__.py"", line 705, in raise_with_traceback\r\n    raise exc.with_traceback(traceback)\r\nUnicodeDecodeError: \'charmap\' codec can\'t decode byte 0x81 in position 6275: character maps to <undefined>\r\n\r\n----------------------------------------------------------------------\r\nRan 7205 tests in 459.919s\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.0.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: None\r\nnose: 1.3.0\r\nCython: 0.20.1\r\nnumpy: 1.8.0\r\nscipy: 0.13.3\r\nstatsmodels: 0.5.0\r\nIPython: None\r\nsphinx: None\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: 0.8.0\r\ntables: 3.1.0\r\nnumexpr: 2.3\r\nmatplotlib: 1.3.1\r\nopenpyxl: 2.0.3\r\nxlrd: 0.9.3\r\nxlwt: None\r\nxlsxwriter: 0.5.5\r\nlxml: 3.3.5\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.2\r\npymysql: None\r\npsycopg2: None\r\n```'"
7228,34235803,jreback,jreback,2014-05-24 15:09:20,2014-06-19 19:27:10,2014-06-19 19:27:10,closed,,0.14.1,0,Bug;Period;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7228,b'BUG: round-trip datetime-period-datetime not preserving NaT',"b""```\r\nIn [76]: data\r\nOut[76]: \r\n0   2014-10-01\r\n1   2014-10-01\r\n2   2014-10-31\r\n3   2014-11-15\r\n4   2014-11-30\r\n5          NaT\r\n6   2014-12-01\r\ndtype: datetime64[ns]\r\n```\r\n\r\n```\r\nIn [78]: DatetimeIndex(data).to_period(freq='M')\r\nOut[78]: \r\n<class 'pandas.tseries.period.PeriodIndex'>\r\nfreq: M\r\n[2014-10, ..., 2014-12]\r\nlength: 7\r\n\r\nIn [79]: DatetimeIndex(data).to_period(freq='M').values\r\nOut[79]: array([ 537,  537,  537,  538,  538, 3507,  539])\r\n\r\nIn [80]: DatetimeIndex(data).to_period(freq='M').to_timestamp()\r\nOut[80]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2014-10-01, ..., 2014-12-01]\r\nLength: 7, Freq: None, Timezone: None\r\n\r\nIn [81]: DatetimeIndex(data).to_period(freq='M').to_timestamp().values\r\nOut[81]: \r\narray(['2014-09-30T20:00:00.000000000-0400',\r\n       '2014-09-30T20:00:00.000000000-0400',\r\n       '2014-09-30T20:00:00.000000000-0400',\r\n       '2014-10-31T20:00:00.000000000-0400',\r\n       '2014-10-31T20:00:00.000000000-0400',\r\n       '2262-03-31T20:00:00.000000000-0400',\r\n       '2014-11-30T19:00:00.000000000-0500'], dtype='datetime64[ns]')\r\n```"""
7227,34235745,jreback,jreback,2014-05-24 15:07:11,2014-06-14 14:00:51,2014-06-14 14:00:51,closed,,0.14.1,2,Bug;Groupby;Missing-data;Resample,https://api.github.com/repos/pydata/pandas/issues/7227,b'BUG: resample/groupby with NaT in the grouper',"b""related to this: https://github.com/pydata/pandas/issues/6992\r\n\r\nThese are broken\r\n```\r\nIn [70]: df.set_index('A').resample('M',how='count')\r\nValueError: month must be in 1..12\r\n\r\nIn [73]: df.groupby(pd.Grouper(freq='MS',key='A')).count()\r\nValueError: month must be in 1..12\r\n```\r\n\r\nSeparately, this should not also return the A column\r\n```\r\nIn [74]: df.dropna(how='any',subset=['A']).groupby(pd.Grouper(key='A',freq='M')).count()\r\nOut[74]: \r\n            A  B\r\nA               \r\n2014-10-31  3  3\r\n2014-11-30  2  2\r\n2014-12-31  1  1\r\n```\r\n\r\n```\r\nIn [57]: data\r\nOut[57]: \r\n0   2014-10-01\r\n1   2014-10-01\r\n2   2014-10-31\r\n3   2014-11-15\r\n4   2014-11-30\r\n5          NaT\r\n6   2014-12-01\r\ndtype: datetime64[ns]\r\n\r\nIn [58]: df = DataFrame(dict(A = data, B = np.arange(len(data))))\r\n```\r\n\r\nThis is a work-around\r\n```\r\nIn [59]: df.dropna(how='any',subset=['A']).set_index('A').resample('M',how='count')\r\nOut[59]: \r\n            B\r\nA            \r\n2014-10-31  3\r\n2014-11-30  2\r\n2014-12-31  1\r\n```\r\n\r\n"""
7225,34234050,sinhrks,jreback,2014-05-24 13:45:55,2014-06-13 14:48:24,2014-05-27 17:50:52,closed,,0.14.0,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7225,b'BUG: boxplot returns incorrect dict',"b""#7096 returns incorrect `dict` when number of columns and subplots are different.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas.util.testing as tm\r\n\r\n\r\nn=100\r\ngender = tm.choice(['Male', 'Female'], size=n)\r\nclassroom = tm.choice(['A', 'B', 'C'], size=n)\r\n\r\ndf = pd.DataFrame({'gender': gender,\r\n                          'classroom': classroom,\r\n                          'height': np.random.normal(66, 4, size=n),\r\n                          'weight': np.random.normal(161, 32, size=n),\r\n                          'category': np.random.randint(4, size=n)})\r\n\r\ndf.boxplot(by='classroom', return_type='axes')\r\n# {'category': array([<matplotlib.axes.AxesSubplot object at 0x104700e90>,\r\n#        <matplotlib.axes.AxesSubplot object at 0x10671ca90>], dtype=object),\r\n#  'height': array([<matplotlib.axes.AxesSubplot object at 0x106744dd0>,\r\n#        <matplotlib.axes.AxesSubplot object at 0x106766b50>], dtype=object)}\r\n\r\n# This must be a dict with 3 keys ('category', 'height' and 'weight') which value is AxesSubplot (not numpy.array).\r\n```\r\n\r\nAlso, `boxplot` sometimes return `OrderedDict` and sometimes `dict` inconsistently.\r\n```\r\ndf.boxplot(by='classroom', return_type='dict')\r\n# OrderedDict([('category', {'medians': [<matplotlib.lines.Line2D object at 0x106fe8c10>,\r\n# ...\r\n```\r\nThe fix makes `boxplot` to always return `OrderedDict` which has correct mapping of keys and values"""
7222,34191590,seth-p,jreback,2014-05-23 16:40:37,2016-02-12 14:30:24,2016-02-12 14:30:24,closed,,0.18.0,8,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7222,"b""BUG: Series.plot() doesn't work with CustomBusinessDay frequency""","b'I posted this on https://groups.google.com/forum/#!topic/pydata/FnHBkkdBIFY.\r\n\r\nIn IPython, trying to .plot() a Series with a CustomBusinessDay frquency produces ""ValueError: Unknown freqstr: C"". See below. I imagine DataFrame.plot() would have a similar issue.\r\n\r\nAm running http://pandas.pydata.org/pandas-build/dev/pandas-0.14.0rc1-51-gccd593f.win-amd64-py3.4.exe and matplotlib 1.2.0.\r\n\r\n```\r\nimport pandas as pd\r\n\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\n\r\n \r\n\r\ns = pd.Series(range(100, 121),\r\n              index=pd.bdate_range(start=\'2014-05-01\', end=\'2014-06-01\',\r\n                                   freq=CustomBusinessDay(holidays=[\'2014-05-26\'])))\r\n\r\ns.plot()\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-8a6b3fabc67e> in <module>()\r\n      3 \r\n      4 s = pd.Series(range(100,121), index=pd.bdate_range(start=\'2014-05-01\', end=\'2014-06-01\', freq=CustomBusinessDay(holidays=[\'2014-05-26\'])))\r\n----> 5 s.plot()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in plot_series(series, label, kind, use_index, rot, xticks, yticks, xlim, ylim, ax, style, grid, legend, logx, logy, secondary_y, **kwds)\r\n   2254                      secondary_y=secondary_y, **kwds)\r\n   2255 \r\n-> 2256     plot_obj.generate()\r\n   2257     plot_obj.draw()\r\n   2258 \r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in generate(self)\r\n    900         self._compute_plot_data()\r\n    901         self._setup_subplots()\r\n--> 902         self._make_plot()\r\n    903         self._add_table()\r\n    904         self._make_legend()\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in _make_plot(self)\r\n   1547         self._neg_prior = np.zeros(len(self.data))\r\n   1548 \r\n-> 1549         if self._is_ts_plot():\r\n   1550             data = self._maybe_convert_index(self.data)\r\n   1551             self._make_ts_plot(data)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in _is_ts_plot(self)\r\n   1541     def _is_ts_plot(self):\r\n   1542         # this is slightly deceptive\r\n-> 1543         return not self.x_compat and self.use_index and self._use_dynamic_x()\r\n   1544 \r\n   1545     def _make_plot(self):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in _use_dynamic_x(self)\r\n   1537                 return False\r\n   1538 \r\n-> 1539         return (freq is not None) and self._is_dynamic_freq(freq)\r\n   1540 \r\n   1541     def _is_ts_plot(self):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in _is_dynamic_freq(self, freq)\r\n   1510             freq = get_base_alias(freq)\r\n   1511         freq = get_period_alias(freq)\r\n-> 1512         return freq is not None and self._no_base(freq)\r\n   1513 \r\n   1514     def _no_base(self, freq):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tools\\plotting.py in _no_base(self, freq)\r\n   1518             and isinstance(self.data.index, DatetimeIndex)):\r\n   1519             import pandas.tseries.frequencies as freqmod\r\n-> 1520             base = freqmod.get_freq(freq)\r\n   1521             x = self.data.index\r\n   1522             if (base <= freqmod.FreqGroup.FR_DAY):\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tseries\\frequencies.py in get_freq(freq)\r\n     67 def get_freq(freq):\r\n     68     if isinstance(freq, compat.string_types):\r\n---> 69         base, mult = get_freq_code(freq)\r\n     70         freq = base\r\n     71     return freq\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tseries\\frequencies.py in get_freq_code(freqstr)\r\n    105 \r\n    106     base, stride = _base_and_stride(freqstr)\r\n--> 107     code = _period_str_to_code(base)\r\n    108 \r\n    109     return code, stride\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\tseries\\frequencies.py in _period_str_to_code(freqstr)\r\n    612             alias = _period_alias_dict[freqstr]\r\n    613         except KeyError:\r\n--> 614             raise ValueError(""Unknown freqstr: %s"" % freqstr)\r\n    615 \r\n    616         return _period_code_map[alias]\r\n\r\nValueError: Unknown freqstr: C\r\n```'"
7219,34176378,jreback,jreback,2014-05-23 13:52:11,2014-06-25 08:58:13,2014-05-23 14:18:45,closed,,0.14.0,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7219,b'BUG: bug in setitem with multi-index and a 0-dim ndarray (GH7218)',b'closes #7218 '
7218,34174368,jreback,jreback,2014-05-23 13:26:55,2014-05-23 14:18:45,2014-05-23 14:18:45,closed,,0.14.0,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7218,b'BUG: assignment with 0-dim arrays and a multi-index is buggy',"b""This works in the non-multi-index case, so is a bug\r\nhttp://stackoverflow.com/questions/23830248/potential-bug-pandas-multiindex-and-0-d-sized-values\r\n```\r\nnp.random.seed(1234)\r\nT = 100 # time\r\nN = 80 # firms\r\nTIndex = np.arange(0, T)\r\nFIndex = np.arange(0, N)\r\n\r\nindex = pd.MultiIndex.from_product([TIndex, FIndex], names=['time', 'firm'])\r\ndf = pd.DataFrame(-999, columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)\r\nt, n = 0, 2\r\ndf.loc[(t,n), 'X'] = np.array(0)\r\n```"""
7216,34171437,jreback,jreback,2014-05-23 12:46:10,2014-06-26 19:53:59,2014-05-23 13:52:50,closed,,0.14.0,0,Bug;Docs,https://api.github.com/repos/pydata/pandas/issues/7216,b'DOC/CLN for GH7213/GH7206',"b""DOC: doc fixups for (GH7213)\r\nCLN: cleanup related to (GH7206) don't create the disabled properties at all for Series\r\n\r\nrelated #7206\r\nrelated #7213\r\n"""
7212,34125320,eldad-a,shoyer,2014-05-22 21:49:12,2015-05-26 04:06:27,2015-05-26 04:06:27,closed,,Next Major Release,7,Bug;Difficulty Novice;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/7212,b'read_hdf / store.select modifies the passed columns parameters when multi-indexed',"b""code to reproduce:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n## generate data\r\ndf = pd.DataFrame(np.random.rand(4,5), index=list('abcd'), columns=list('ABCDE'))\r\ndf.index.name = 'letters'\r\ndf = df.set_index(keys='E' , append=True)\r\n\r\n## save to hdf5\r\nh5name = 'tst.h5'\r\nkey = 'tst_key'\r\ndf.to_hdf(h5name, key,\r\n          mode='a', append=True,\r\n          data_columns = df.index.names+df.columns.tolist(),\r\n          index=False, \r\n          complevel=5, complib='blosc', \r\n          #expectedrows = expectedrows ,\r\n          )\r\n\r\n## load part of df\r\ncols2load = list('BCD')\r\nprint 'before loading: \\n\\t cols2load = {}'.format(cols2load)\r\ndf_ = pd.read_hdf(h5name, key, columns= cols2load)\r\nprint 'after loading: \\n\\t cols2load = {}'.format(cols2load)\r\n```\r\n\r\nThe printed output:\r\n>before loading: \r\n>\t cols2load = ['B', 'C', 'D']\r\n>after loading: \r\n>\t cols2load = ['E', 'letters', 'B', 'C', 'D']\r\n\r\npd.__version__ = '0.13.1'"""
7202,34037768,jreback,jreback,2014-05-21 23:28:39,2014-06-26 19:53:50,2014-05-22 00:06:23,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7202,b'BUG: Bug in multi-axis indexing with > 2 ndim and a multi-index (GH7199)',b'closes #7199'
7201,34032338,jreback,jreback,2014-05-21 22:36:41,2014-06-17 04:57:09,2014-05-21 23:26:20,closed,,0.14.0,9,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7201,"b'BUG: Bug in expressions evaluation with reversed ops, showing in series-dataframe ops (GH7198)'",b'closes #7198\r\ncloses #7192'
7199,34027908,seth-p,jreback,2014-05-21 21:51:43,2014-05-22 04:27:46,2014-05-22 00:06:23,closed,,0.14.0,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7199,b'BUG: Panel.iloc[] bug with MultiIndex axis',"b'I just posted this at https://groups.google.com/forum/#!topic/pydata/wmUjOttpcpI.\r\n\r\nAll of [6], [7], [8], and [9] below should have the same dimensions, but [8] is incorrect. Similarly, [12] should work and look like [14]. The only difference between wd1 and wd2 is that the former has minor_axis=multi_index, whereas the latter has minor_axis=simple_index.\r\n\r\n```\r\nPython 3.4.0 (v3.4.0:04f714765c13, Mar 16 2014, 19:25:23) [MSC v.1600 64 bit (AMD64)]\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 2.0.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: multi_index = pd.MultiIndex.from_tuples([(\'ONE\', \'one\'),\r\n                                                 (\'TWO\', \'two\'),\r\n                                                 (\'THREE\', \'three\')],\r\n                                                names=[\'UPPER\', \'lower\'])\r\n\r\nIn [3]: simple_index = [x[0] for x in multi_index]\r\n\r\nIn [4]: wd1 = pd.Panel(items=[\'First\', \'Second\'],\r\n                       major_axis=[\'a\', \'b\', \'c\', \'d\'],\r\n                       minor_axis=multi_index)\r\n\r\nIn [5]: wd2 = pd.Panel(items=[\'First\', \'Second\'],\r\n                       major_axis=[\'a\', \'b\', \'c\', \'d\'],\r\n                       minor_axis=simple_index)\r\n\r\nIn [6]: wd1[\'First\'].iloc[[True, True, True, False], [0, 2]]\r\nOut[6]:\r\nUPPER  ONE  THREE\r\nlower  one  three\r\na      NaN    NaN\r\nb      NaN    NaN\r\nc      NaN    NaN\r\n\r\nIn [7]: wd2[\'First\'].iloc[[True, True, True, False], [0, 2]]\r\nOut[7]:\r\n   ONE  THREE\r\na  NaN    NaN\r\nb  NaN    NaN\r\nc  NaN    NaN\r\n\r\nIn [8]: wd1.iloc[0, [True, True, True, False], [0, 2]]  # WRONG\r\nOut[8]:\r\nUPPER  ONE  TWO  THREE\r\nlower  one  two  three\r\na      NaN  NaN    NaN\r\nc      NaN  NaN    NaN\r\n\r\nIn [9]: wd2.iloc[0, [True, True, True, False], [0, 2]]\r\nOut[9]:\r\n   ONE  THREE\r\na  NaN    NaN\r\nb  NaN    NaN\r\nc  NaN    NaN\r\n\r\nIn [10]: pd.__version__\r\nOut[10]: \'0.14.0rc1-39-g9d01fe1\'\r\n\r\nIn [11]: wd1.iloc[0,0,[0,1,2]]\r\nOut[11]:\r\nUPPER  lower\r\nONE    one     NaN\r\nTWO    two     NaN\r\nTHREE  three   NaN\r\nName: a, dtype: float64\r\n\r\nIn [12]: wd1.iloc[0,[0],[0,1,2]] # should be similar to [14] below\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-12-b14471499110> in <module>()\r\n----> 1 wd1.iloc[0,[0],[0,1,2]]\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key)\r\n   1122     def __getitem__(self, key):\r\n   1123         if type(key) is tuple:\r\n-> 1124             return self._getitem_tuple(key)\r\n   1125         else:\r\n   1126             return self._getitem_axis(key, axis=0)\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_tuple(self, tup)\r\n   1329                 continue\r\n   1330\r\n-> 1331             retval = getattr(retval, self.name)._getitem_axis(key, axis=axis)\r\n   1332\r\n   1333             # if the dim was reduced, then pass a lower-dim the next time\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis, validate_iterable)\r\n   1368\r\n   1369                 # validate list bounds\r\n-> 1370                 self._is_valid_list_like(key, axis)\r\n   1371\r\n   1372                 # force an actual list\r\n\r\nC:\\Python34\\lib\\site-packages\\pandas\\core\\indexing.py in _is_valid_list_like(self, key, axis)\r\n   1307         l = len(ax)\r\n   1308         if len(arr) and (arr.max() >= l or arr.min() <= -l):\r\n-> 1309             raise IndexError(""positional indexers are out-of-bounds"")\r\n   1310\r\n   1311         return True\r\n\r\nIndexError: positional indexers are out-of-bounds\r\n\r\nIn [13]: wd2.iloc[0,0,[0,1,2]]\r\nOut[13]:\r\nONE     NaN\r\nTWO     NaN\r\nTHREE   NaN\r\nName: a, dtype: float64\r\n\r\nIn [14]: wd2.iloc[0,[0],[0,1,2]]\r\nOut[14]:\r\n   ONE  TWO  THREE\r\na  NaN  NaN    NaN\r\n```'"
7198,34027774,jreback,jreback,2014-05-21 21:49:50,2014-05-21 23:26:20,2014-05-21 23:26:20,closed,,0.14.0,2,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/7198,b'BUG: odd indexing issue w.r.t series/frame div op (may involve numexpr)',"b""worked in 0.13.1\r\n\r\nworks for n = 4, but not 4000\r\n\r\n\r\n```\r\n        for n in [ 4, 4000 ]:\r\n\r\n            df = DataFrame(1,index=range(n),columns=list('abcd'))\r\n            df.iloc[0] = 2\r\n\r\n            expected = DataFrame(np.tile(df.mean().values,n).reshape(n,-1),\r\n                                 columns=list('abcd'))/df\r\n            result = df.mean()/df\r\n            assert_frame_equal(result,expected)\r\n```"""
7196,34024720,sinhrks,jreback,2014-05-21 21:10:50,2014-06-27 07:41:02,2014-06-04 00:06:46,closed,,0.14.1,2,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7196,b'BUG: CustomBusinessDay apply raises NameError when np.datetime64 is pass...',"b""`CustomBusinessDay` raises `NameError` when `np.datetime64` is passed. It works if `normalize=True` is specified.\r\n```\r\nd = pd.offsets.CustomBusinessDay().apply(np.datetime64('2011-01-01 09:00:00'))\r\n# NameError: global name 'np_day_incr' is not defined\r\n\r\nd = pd.offsets.CustomBusinessDay(normalize=True).apply(np.datetime64('2011-01-01 09:00:00'))\r\n# 2011-01-03 00:00:00\r\n```\r\n"""
7195,34021043,sinhrks,jreback,2014-05-21 20:29:34,2014-06-13 17:19:15,2014-06-03 23:42:06,closed,,0.14.1,3,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/7195,b'BUG: Easter works incorrectly in negative offsets',"b""When an offset is negative, Easter skips the date just before and returns one before.\r\n```\r\n(- pd.offsets.Easter()).apply(pd.Timestamp('2011-01-01 09:00:00'))\r\n# 2009-04-12 00:00:00\r\n\r\n(- pd.offsets.Easter()).apply(pd.Timestamp('2012-01-01 09:00:00'))\r\n# returns 2010-04-04 00:00:00\r\n\r\n(- pd.offsets.Easter()).apply(pd.Timestamp('2010-04-05 09:00:00'))\r\n# 2009-04-12 00:00:00\r\n```"""
7192,33991478,jreback,jreback,2014-05-21 15:08:28,2014-05-21 23:26:20,2014-05-21 23:26:20,closed,,0.14.0,5,Bug,https://api.github.com/repos/pydata/pandas/issues/7192,b'BUG: numexpr / ops with boolean and conversion',"b'these cases should give the same results. Somehow the ops order is being reversed or something with numexpr.\r\n\r\nnumpy 1.8.1, numexpr 2.4, pandas 0.14rc1\r\nCan be worked around like:\r\n\r\n```\r\nIn [39]: df = DataFrame(dict(A=np.random.randn(25000)))\r\n\r\nIn [40]: df.iloc[0:5] = np.nan\r\n\r\nIn [41]: (1-np.isnan(df)).iloc[0:25]\r\nOut[41]: \r\n       A\r\n0      0\r\n1      0\r\n2      0\r\n3      0\r\n4      0\r\n5     -1\r\n6     -1\r\n7     -1\r\n8     -1\r\n9     -1\r\n10    -1\r\n11    -1\r\n12    -1\r\n13    -1\r\n14    -1\r\n15    -1\r\n16    -1\r\n17    -1\r\n18    -1\r\n19    -1\r\n20    -1\r\n21    -1\r\n22    -1\r\n23    -1\r\n24    -1\r\n25    -1\r\n```\r\n\r\n```\r\nIn [43]: df = DataFrame(dict(A=np.random.randn(25)))\r\n\r\nIn [44]: df.iloc[0:5] = np.nan\r\n\r\nIn [45]: (1-np.isnan(df)).iloc[0:25] \r\nOut[45]: \r\n    A\r\n0   0\r\n1   0\r\n2   0\r\n3   0\r\n4   0\r\n5   1\r\n6   1\r\n7   1\r\n8   1\r\n9   1\r\n10  1\r\n11  1\r\n12  1\r\n13  1\r\n14  1\r\n15  1\r\n16  1\r\n17  1\r\n18  1\r\n19  1\r\n20  1\r\n21  1\r\n22  1\r\n23  1\r\n24  1\r\n```'"
7191,33978355,jreback,jreback,2014-05-21 12:42:16,2014-06-15 01:48:02,2014-05-21 13:03:59,closed,,0.14.0,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7191,"b'BUG: Bug in setitem with a single item, and a multi-index and integer indices (GH7190)'",b'closes #7190'
7190,33978167,jreback,jreback,2014-05-21 12:39:38,2014-05-21 13:03:59,2014-05-21 13:03:59,closed,,0.14.0,0,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7190,b'BUG: setitem with a multi-index and integer indices',b'http://stackoverflow.com/questions/23764306/multi-indexed-dataframe-setting-values/23764822?noredirect=1#comment36573548_23764822'
7189,33939777,jreback,jreback,2014-05-20 23:34:19,2014-06-13 22:52:24,2014-05-21 00:22:35,closed,,0.14.0,0,Bug;Error Reporting;Indexing,https://api.github.com/repos/pydata/pandas/issues/7189,b'BUG: bug in recognizing out-of-bounds on iloc indexers on tuple indexers (GH )',"b""reported on ML: https://groups.google.com/forum/#!topic/pydata/OscVmQlNdTc\r\n\r\n```\r\nIn [1]: p = Panel(np.random.rand(4,3,2), items=['A','B','C','D'], major_axis=['U','V','W'], minor_axis=['X','Y'])\r\n\r\nIn [2]: p\r\nOut[2]: \r\n<class 'pandas.core.panel.Panel'>\r\nDimensions: 4 (items) x 3 (major_axis) x 2 (minor_axis)\r\nItems axis: A to D\r\nMajor_axis axis: U to W\r\nMinor_axis axis: X to Y\r\n\r\nIn [3]: p['A']\r\nOut[3]: \r\n          X         Y\r\nU  0.541463  0.546778\r\nV  0.079211  0.229914\r\nW  0.621594  0.973252\r\n\r\nIn [4]: p.iloc[0,:,:]\r\nOut[4]: \r\n          X         Y\r\nU  0.541463  0.546778\r\nV  0.079211  0.229914\r\nW  0.621594  0.973252\r\n\r\nIn [5]: p.iloc[0,[True,True,True],:]\r\nOut[5]: \r\n          X         Y\r\nU  0.541463  0.546778\r\nV  0.079211  0.229914\r\nW  0.621594  0.973252\r\n\r\nIn [6]: p.iloc[0,[True,True,True],[0,1]]\r\nOut[6]: \r\n          X         Y\r\nU  0.541463  0.546778\r\nV  0.079211  0.229914\r\nW  0.621594  0.973252\r\n\r\nIn [7]: p.iloc[0,[True,True,True],[0,1,2]]\r\nIndexError: positional indexers are out-of-bounds\r\n\r\nIn [8]: p.iloc[0,[True,True,True],[2]]\r\nIndexError: positional indexers are out-of-bounds\r\n```"""
7181,33836895,cpcloud,cpcloud,2014-05-19 20:59:13,2014-06-26 10:59:06,2014-05-20 21:10:17,closed,cpcloud,0.14.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/7181,b'BUG: correctly lookup global constants in query/eval',b'closes #7178'
7180,33835593,bjonen,jreback,2014-05-19 20:43:23,2014-09-18 13:55:48,2014-09-18 13:55:48,closed,,0.15.0,20,Bug;Enhancement;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7180,b'max_columns == 0 incorrectly wraps around for wide dfs',"b'![screenshot from 2014-05-19 22 34 47](https://cloud.githubusercontent.com/assets/832380/3019649/110d4f60-df95-11e3-9bb2-c7c0a281d70e.png)\r\n\r\nAccording to the description of `max_columns`, should switch to ""smaller"" (at the beginning to info, nowadays to truncate) view when set to zero instead of wrapping around. \r\n\r\n```\r\ndisplay.max_columns: [default: 20] [currently: 0]\r\n: int\r\n        max_rows and max_columns are used in __repr__() methods to decide if\r\n        to_string() or info() is used to render an object to a string.  In case\r\n        python/IPython is running in a terminal this can be set to 0 and pandas\r\n        will correctly auto-detect the width the terminal and swap to a smaller\r\n        format in case all columns would not fit vertically. The IPython notebook,\r\n        IPython qtconsole, or IDLE do not run in a terminal and hence it is not\r\n        possible to do correct auto-detection.\r\n        \'None\' value means unlimited.\r\n```\r\n\r\nOriginally introduced here https://github.com/pydata/pandas/pull/453#issuecomment-3047570\r\n\r\nFixed after broken here: https://github.com/pydata/pandas/pull/2881\r\n\r\nBehavior change (broken again) here https://github.com/pydata/pandas/pull/5550'"
7178,33828541,amelio-vazquez-reina,cpcloud,2014-05-19 19:20:57,2014-05-20 21:10:17,2014-05-20 21:10:17,closed,,0.14.0,5,Bug,https://api.github.com/repos/pydata/pandas/issues/7178,b'Inf in query / eval',"b""In `df.eval()` or `df.query()`, is there any way to run comparisons against `np.Inf`?\r\n\r\nWhen I do:\r\n\r\n`temp_df.eval('my_col < Inf')` or\r\n`temp_df.eval('my_col < np.Inf')` or\r\n\r\nI get `UndefinedVariableError`\r\n"""
7176,33812222,shoyer,jreback,2014-05-19 15:53:35,2014-06-17 13:25:53,2014-05-20 00:55:35,closed,,0.14.0,6,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7176,b'BUG: fix isnull for 0d object arrays',"b'If this looks good, it would be nice to get it in 0.14, too.'"
7174,33810603,cpcloud,jreback,2014-05-19 15:35:12,2014-05-25 16:06:59,2014-05-25 16:06:44,closed,,0.14.0,17,Bug;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7174,b'MultiIndex notebook repr is incorrect when number of columns is > max_columns',b'See this comment: https://github.com/pydata/pandas/issues/7146#issuecomment-43518940\r\n\r\nnotebook http://goo.gl/qLUUx8'
7173,33799746,JohnSmizz,jreback,2014-05-19 13:38:06,2014-09-21 14:51:19,2014-09-10 17:45:32,closed,,0.15.0,5,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7173,b'DataFrame interpolate method error when using keyword limit ',"b'Hi-\r\n\r\ninterpolate works fine on most of my datasets but I\'ve just encountered for the first time a new error when attempting to run this method.\r\n```\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```\r\nI\'m not sure how/if its sensible to paste the dataset I used somehow on here (along with the datetimeindex) so you can attempt to replicate this error -- happy to do so if you tell me how.\r\nThis error only occurs when I place a restriction via the limit keyword. Eg.\r\n```\r\ntemp1df.interpolate(method=\'time\') works fine.\r\ntemp1df.interpolate(method=\'time\', limit=4) yields above error.\r\n```\r\ntemp1df is a dataframe with a datetimeindex with a 208 count, and a datetimeindex of 576 length.\r\nI am running 0.13.1 on 3.3.3 on a windows machine.\r\n\r\nFull traceback below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-88-394bcc7f44e2>"", line 1, in <module>\r\n    temp1df.between_time(\'08:00\', \'18:00\').interpolate(limit=4)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\generic.py"", line 2532, in interpolate\r\n    **kwargs)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\internals.py"", line 2404, in interpolate\r\n    return self.apply(\'interpolate\', *args, **kwargs)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\internals.py"", line 2375, in apply\r\n    applied = getattr(blk, f)(*args, **kwargs)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\internals.py"", line 821, in interpolate\r\n    **kwargs)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\internals.py"", line 882, in _interpolate\r\n    interp_values = np.apply_along_axis(func, axis, data)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\numpy\\lib\\shape_base.py"", line 79, in apply_along_axis\r\n    res = func1d(arr[tuple(i.tolist())],*args)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\internals.py"", line 879, in func\r\n    bounds_error=False, **kwargs)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\common.py"", line 1380, in interpolate_1d\r\n    violate_limit = _interp_limit(invalid, limit)\r\n\r\n  File ""C:\\WinPython3.3.3.2\\python-3.3.3.amd64\\lib\\site-packages\\pandas\\core\\common.py"", line 1374, in _interp_limit\r\n    return all_nans[violate] + limit\r\n\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```'"
7168,33769857,allhailwesttexas,jreback,2014-05-19 04:06:39,2014-07-01 15:50:58,2014-07-01 15:50:52,closed,,0.14.1,3,Bug;Indexing;Won't Fix,https://api.github.com/repos/pydata/pandas/issues/7168,"b""rolling_sum gives ValueError for column name of '64' / DateTimeIndex in year 1964""","b'pandas version: 0.13.1, Python 2.7 32-bit. Windows.\r\n\r\nWhen attempting to calculate rolling sums for a dataframe with a DateTimeIndex in 1964, and naming the column \'64\' (as a string), I get a ValueError: could not broadcast input array from shape (x) into shape (x, 1). If I change one of the 64s, the error does not occur.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nincr = range(150)\r\ndatetimes = pd.date_range(\'1964/01/01 00:00:00\', periods=150)\r\n\r\ndf = pd.DataFrame(data=incr, index=datetimes)\r\ns = pd.Series(data=incr, index=datetimes)\r\ndf.columns = [\'increments\']\r\ndf[\'64\'] = pd.rolling_sum(df[\'increments\'], 5)\r\n```\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""C:\\Jobs\\29075\\2014\\Software\\stormy\\pdbug.py"", line 9, in <module>\r\n    df[\'64\'] = pd.rolling_sum(df[\'increments\'], 5)\r\n  File ""C:\\Python27_w32\\lib\\site-packages\\pandas\\core\\frame.py"", line 1879, in __setitem__\r\n    return self._setitem_slice(indexer, value)\r\n  File ""C:\\Python27_w32\\lib\\site-packages\\pandas\\core\\frame.py"", line 1891, in _setitem_slice\r\n    self.ix._setitem_with_indexer(key, value)\r\n  File ""C:\\Python27_w32\\lib\\site-packages\\pandas\\core\\indexing.py"", line 422, in _setitem_with_indexer\r\n    self.obj._data = self.obj._data.setitem(indexer, value)\r\n  File ""C:\\Python27_w32\\lib\\site-packages\\pandas\\core\\internals.py"", line 2396, in setitem\r\n    return self.apply(\'setitem\', *args, **kwargs)\r\n  File ""C:\\Python27_w32\\lib\\site-packages\\pandas\\core\\internals.py"", line 2376, in apply\r\n    applied = getattr(blk, f)(*args, **kwargs)\r\n  File ""C:\\Python27_w32\\lib\\site-packages\\pandas\\core\\internals.py"", line 636, in setitem\r\n    values[indexer] = value\r\nValueError: could not broadcast input array from shape (150) into shape (150,1)\r\n```'"
7161,33751779,sinhrks,jreback,2014-05-18 13:34:54,2014-07-16 09:06:28,2014-05-18 16:56:27,closed,,0.14.0,3,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/7161,b'BUG: PeriodIndex slices with out of bounds results incorrect',b'Closes #5407.'
7160,33744974,rafaljozefowicz,jreback,2014-05-18 04:06:33,2016-05-23 21:43:04,2016-05-23 21:43:04,closed,,0.19.0,10,Bug;Difficulty Intermediate;Effort Medium;IO CSV,https://api.github.com/repos/pydata/pandas/issues/7160,b'Bug in read_csv with duplicated column names',"b'Tested on 0.13.0, 0.13.1 and 0.14.0rc1:\r\n```python\r\nfrom StringIO import StringIO\r\nimport pandas as pd\r\n\r\n# this is correct\r\nprint(pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[""a"", ""b"", ""a""]))\r\n# and this is fine as well\r\n# (although, changing the column names to a,b,a.1)\r\nprint(pd.read_csv(StringIO(""a,b,a\\n0,1,2\\n3,4,5"")))\r\n# but this is not correct\r\nprint(pd.read_csv(StringIO(""0,1,2\\n3,4,5""), names=[""a"", ""b"", ""a""]))\r\n```\r\n\r\nThe last one returns:\r\n```\r\nOut[5]: \r\n   a  b  a\r\n0  2  1  2\r\n1  5  4  5\r\n```\r\n\r\nI would expect all 3 methods to return the same DataFrame. I noticed this when I wanted to read csv file that had a separate file with a header (and a duplicated column in it). BTW is there a better way to do it than to read the header file first and pass the output into \'names\' parameter of read_csv?'"
7157,33732616,miketkelly,jreback,2014-05-17 15:23:33,2014-06-16 02:52:03,2014-05-18 19:57:35,closed,,0.14.0,6,Algos;Bug,https://api.github.com/repos/pydata/pandas/issues/7157,b'BUG: hashtable memory error causes test_factorize_nan crash',"b""ObjectVector class resizes its array without reseting its capacity count, so subsequent appends are invalid.\r\n\r\nMac OS 10.9, Python 2.7.6, numpy 1.9.0.dev-ee49411. \r\n\r\n```\r\n==57654== Invalid write of size 8\r\n==57654==    at 0x137F856: __pyx_f_6pandas_9hashtable_12ObjectVector_append (in /Users/mtk/Projects/pandas/pandas/hashtable.so)\r\n==57654==    by 0x139B16F: __pyx_pw_6pandas_9hashtable_17PyObjectHashTable_25get_labels (in /Users/mtk/Projects/pandas/pandas/hashtable.so)\r\n==57654==    by 0x138CA9E: __pyx_pw_6pandas_9hashtable_10Factorizer_5factorize (in /Users/mtk/Projects/pandas/pandas/hashtable.so)\r\n==57654==    by 0xD227E: PyEval_EvalFrameEx (in /usr/local/anaconda/lib/libpython2.7.dylib)\r\n\r\n==57654==  Address 0x10095efd0 is 16 bytes inside a block of size 256 free'd\r\n==57654==    at 0x7858: realloc (in /usr/local/Cellar/valgrind/3.9.0/lib/valgrind/vgpreload_memcheck-amd64-darwin.so)\r\n==57654==    by 0x13C0F55: PyDataMem_RENEW (in /usr/local/anaconda/lib/python2.7/site-packages/numpy/core/multiarray.so)\r\n==57654==    by 0x1488DE7: PyArray_Resize (in /usr/local/anaconda/lib/python2.7/site-packages/numpy/core/multiarray.so)\r\n==57654==    by 0x14647A0: array_resize (in /usr/local/anaconda/lib/python2.7/site-packages/numpy/core/multiarray.so)\r\n==57654==    by 0x1396A12: __pyx_pw_6pandas_9hashtable_12ObjectVector_5to_array (in /Users/mtk/Projects/pandas/pandas/hashtable.so)\r\n==57654==    by 0x138CFEE: __pyx_pw_6pandas_9hashtable_10Factorizer_5factorize (in /Users/mtk/Projects/pandas/pandas/hashtable.so)\r\n==57654==    by 0xD227E: PyEval_EvalFrameEx (in /usr/local/anaconda/lib/libpython2.7.dylib)\r\n```"""
7156,33732276,sinhrks,jreback,2014-05-17 15:03:36,2014-06-11 14:08:31,2014-06-11 14:08:31,closed,,0.14.1,3,Bug;Frequency;Testing,https://api.github.com/repos/pydata/pandas/issues/7156,b'BUG: Some date-like Offset.apply resets time ',"b""There seems to be inconsistencies regarding time (hours, minutes, etc) handling of `Offset.apply`. Maybe all these should preserve hours/minutes like `DayOffset`? Or any other rules?\r\n\r\nFollowings are results offset(n=1) applied to `datetime(2011, 1, 1, 9, 0)`.\r\n\r\n```\r\n# Offsets which preserves time\r\n(<Day>, datetime.datetime(2011, 1, 2, 9, 0))\r\n(<BusinessDay>, Timestamp('2011-01-03 09:00:00'))\r\n(<CustomBusinessDay>, Timestamp('2011-01-03 09:00:00'))\r\n(<MonthBegin>, Timestamp('2011-02-01 09:00:00'))\r\n(<CustomBusinessMonthBegin>, Timestamp('2011-01-03 09:00:00'))\r\n(<YearBegin: month=1>, Timestamp('2012-01-01 09:00:00'))\r\n(<YearEnd: month=12>, Timestamp('2011-12-31 09:00:00'))\r\n(<BusinessYearEnd: month=12>, Timestamp('2011-12-30 09:00:00'))\r\n(<QuarterBegin: startingMonth=3>, Timestamp('2011-03-01 09:00:00'))\r\n(<BusinessQuarterBegin: startingMonth=3>, Timestamp('2011-03-01 09:00:00'))\r\n(<QuarterEnd: startingMonth=3>, Timestamp('2011-03-31 09:00:00'))\r\n(<BusinessQuarterEnd: startingMonth=3>, Timestamp('2011-03-31 09:00:00'))\r\n(<Week: weekday=None>, Timestamp('2011-01-08 09:00:00'))\r\n\r\n# Offsets which don't preserves time\r\n(<BusinessMonthBegin>, Timestamp('2011-01-03 00:00:00'))\r\n(<MonthEnd>, Timestamp('2011-01-31 00:00:00'))\r\n(<BusinessMonthEnd>, Timestamp('2011-01-31 00:00:00'))\r\n(<CustomBusinessMonthEnd>, Timestamp('2011-01-31 00:00:00'))\r\n(<BusinessYearBegin: month=1>, Timestamp('2011-01-03 00:00:00'))\r\n(<LastWeekOfMonth: weekday=5>, Timestamp('2011-01-29 00:00:00'))\r\n(<FY5253Quarter: kwds={'startingMonth': 1, 'weekday': 1, 'variation': 'last'}, qtr_with_extra_week=1>, Timestamp('2011-01-25 00:00:00'))\r\n(<FY5253: kwds={'qtr_with_extra_week': 1}, startingMonth=1, variation='last', weekday=1>, Timestamp('2011-01-25 00:00:00'))\r\n(<LastWeekOfMonth: weekday=5>, Timestamp('2011-01-29 00:00:00'))\r\n(<Easter>, datetime.datetime(2011, 4, 24, 0, 0))\r\n```"""
7149,33722942,cpcloud,cpcloud,2014-05-17 04:10:48,2014-06-17 05:57:46,2014-06-01 22:08:10,closed,cpcloud,0.14.1,3,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/7149,b'BUG: allow dup indexing with Float64Index',b'closes #7143'
7144,33674557,jreback,jreback,2014-05-16 13:30:01,2014-07-16 09:06:18,2014-05-16 14:03:21,closed,,0.14.0,0,Bug;Compat;Timezones,https://api.github.com/repos/pydata/pandas/issues/7144,"b""COMPAT/BUG: compat with pytz 2014.3, exposing several test that don't localize (GH7137)""",b'closes #7137 '
7143,33673399,jorisvandenbossche,cpcloud,2014-05-16 13:13:37,2014-06-01 22:08:10,2014-06-01 22:08:10,closed,cpcloud,0.14.1,6,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/7143,"b""Duplicate index with FloatIndex giving 'ValueError: Length mismatch'""","b""Not certain if this is a bug or defined behaviour (but then the error message is not clear in any case).\r\n\r\nIn 0.13.1:\r\n```\r\nIn [28]: df = pd.DataFrame(np.random.randn(9).reshape(3,3), index=[0.1,0.2,0.2],\r\n columns=['a','b','c'])\r\nIn [29]: df\r\nOut[29]:\r\n            a         b         c\r\n0.1  1.711117  1.218853 -1.322363\r\n0.2  0.956266  0.230374 -1.005935\r\n0.2 -0.137729 -0.993931 -0.902793\r\n\r\nIn [30]: df.ix[0.2,'a']\r\nOut[30]: array([ 0.95626607, -0.13772877])\r\n\r\nIn [31]: df.ix[0.2]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n...\r\nValueError: Length mismatch: Expected axis has 0 elements, new values have 2 ele\r\nments\r\n```\r\nIn master, both (`df.loc[0.2]` and `df.loc[0.2, 'a']`) give this error message. Wile for integer index, this works."""
7140,33662408,toobaz,cpcloud,2014-05-16 10:09:13,2014-06-02 14:43:50,2014-06-02 14:43:50,closed,cpcloud,0.14.1,8,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/7140,b'replace() alters unrelated values',"b'I don\'t know if it is the same bug than #7126 (looks similar, but I didn\'t check the implementation), but\r\n\r\n    In [1]: from pandas import DataFrame\r\n    \r\n    In [2]: import numpy as np\r\n    \r\n    In [3]: df = DataFrame(index=range(2))\r\n    \r\n    In [4]: df[\'a\'] = True\r\n    \r\n    In [5]: df.a\r\n    \r\n    Out[5]: \r\n    0    True\r\n    1    True\r\n    Name: a, dtype: bool\r\n    \r\n    In [6]: df.replace([np.inf, -np.inf], np.nan).a\r\n    \r\n    Out[6]: \r\n    0   NaN\r\n    1   NaN\r\n    Name: a, dtype: float64\r\n\r\nNotice the operation of doing such a replace on boolean variables may seem stupid... but it is less so if you consider that ""replace"" (differently from i.e. dropna) does not support a ""subset"" argument.'"
7137,33604903,jreback,jreback,2014-05-15 17:03:14,2014-05-16 14:03:21,2014-05-16 14:03:21,closed,,0.14.0,0,Bug;Compat;Testing;Timezones,https://api.github.com/repos/pydata/pandas/issues/7137,b'TST: failures with latest pytz (2014.3)',"b'only shows on the numpy 1.9-dev build as this is the only one to pull a current version of pytz\r\nBug report for pytz\r\n\r\nhttps://bugs.launchpad.net/pytz/+bug/1319939\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 592a5379dea49f6e88e736a651bf82a00ea4fcc8\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: None\r\nnose: 1.3.3\r\nCython: 0.20.1\r\nnumpy: 1.9.0.dev-13b32e9\r\nscipy: None\r\nstatsmodels: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.3\r\nbottleneck: 0.9.0dev\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\n```\r\n```\r\n======================================================================\r\nFAIL: test_getitem_setitem_datetime_tz (pandas.tests.test_series.TestSeries)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/tests/test_series.py"", line 4594, in test_getitem_setitem_datetime_tz\r\n    assert_series_equal(result, ts)\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/util/testing.py"", line 530, in assert_series_equal\r\n    assert_almost_equal(left.values, right.values, check_less_precise)\r\n  File ""testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2536)\r\n  File ""testing.pyx"", line 82, in pandas._testing.assert_almost_equal (pandas/src/testing.c:1710)\r\nAssertionError: Length of two iterators not the same: 51 != 50\r\n\r\n----------------------------------------------------------------------\r\nRan 311 tests in 2.951s\r\n\r\nFAILED (SKIP=6, failures=1)\r\n(py2.7_bottleneck)jreback@sheep:~/venv/py2.7_bottleneck/pandas$ nosetests pandas//tseries/tests/test_timezones.py \r\n.................E.....F.......................F..................\r\n======================================================================\r\nERROR: test_getitem_pydatetime_tz (pandas.tseries.tests.test_timezones.TestTimeZoneSupport)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/tseries/tests/test_timezones.py"", line 648, in test_getitem_pydatetime_tz\r\n    self.assertEqual(ts[time_pandas], ts[time_datetime])\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/core/series.py"", line 475, in __getitem__\r\n    result = self.index.get_value(self, key)\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/tseries/index.py"", line 1263, in get_value\r\n    return self.get_value_maybe_box(series, key)\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/tseries/index.py"", line 1289, in get_value_maybe_box\r\n    values = self._engine.get_value(_values_from_object(series), key)\r\n  File ""index.pyx"", line 97, in pandas.index.IndexEngine.get_value (pandas/index.c:2987)\r\n  File ""index.pyx"", line 105, in pandas.index.IndexEngine.get_value (pandas/index.c:2802)\r\n  File ""index.pyx"", line 515, in pandas.index.DatetimeEngine.get_loc (pandas/index.c:9256)\r\nKeyError: Timestamp(\'2012-12-24 17:07:00+0100\', tz=\'Europe/Berlin\')\r\n\r\n======================================================================\r\nFAIL: test_infer_tz (pandas.tseries.tests.test_timezones.TestTimeZoneSupport)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/tseries/tests/test_timezones.py"", line 396, in test_infer_tz\r\n    assert(tools._infer_tzinfo(start, end) is eastern)\r\nAssertionError\r\n\r\n======================================================================\r\nFAIL: test_with_tz (pandas.tseries.tests.test_timezones.TestTimeZoneSupport)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/tseries/tests/test_timezones.py"", line 321, in test_with_tz\r\n    self.assertIs(central[0].tz, tz)\r\n  File ""/mnt/home/jreback/venv/py2.7_bottleneck/pandas/pandas/util/testing.py"", line 94, in assertIs\r\n    assert a is b, ""%s: %r is not %r"" % (msg.format(a,b), a, b)\r\nAssertionError: : <DstTzInfo \'US/Central\' CST-1 day, 18:00:00 STD> is not <DstTzInfo \'US/Central\' LMT-1 day, 18:09:00 STD>\r\n\r\n----------------------------------------------------------------------\r\nRan 66 tests in 0.488s\r\n\r\nFAILED (errors=1, failures=2)\r\n(py2.7_bottleneck)jreback@sheep:~/venv/py2.7_bottleneck/pandas$ \r\n```'"
7126,33500357,cpcloud,cpcloud,2014-05-14 15:00:40,2014-06-10 21:52:01,2014-06-10 21:52:01,closed,cpcloud,0.14.1,2,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/7126,b'convert_objects incorrectly converting bools to nans',"b""SO xref: http://stackoverflow.com/q/23658092/564538\r\n\r\n```\r\nIn [45]: paste\r\n>>> s1 = Series([1, True, 3, 5], index=['a', 'b', 'c', 'e'])\r\n\r\n## -- End pasted text --\r\n\r\nIn [46]: s1\r\nOut[46]:\r\na       1\r\nb    True\r\nc       3\r\ne       5\r\ndtype: object\r\n\r\nIn [47]: s1.convert_objects(convert_numeric=True)\r\nOut[47]:\r\na     1\r\nb   NaN\r\nc     3\r\ne     5\r\ndtype: float64\r\n```"""
7116,33419041,jreback,jreback,2014-05-13 17:12:33,2014-05-15 14:31:34,2014-05-13 22:20:39,closed,,0.14.0,4,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/7116,b'BUG: period partial string slicing failng again numpy master',"b'xref: #7043\r\n\r\nThis fails on numpy master for some reason, can you investigate: https://travis-ci.org/pydata/pandas/jobs/25073475'"
7115,33412181,jreback,jreback,2014-05-13 15:56:29,2014-10-30 11:19:43,2014-10-28 00:04:25,closed,,0.15.1,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7115,b'BUG: groupby with reducer of max and as_index=False raises',"b""This shouldn't raise\r\n\r\nfrom here originally (not the bug, the data/question):\r\nhttp://stackoverflow.com/questions/23635647/how-to-force-pandas-dataframe-apply-on-a-grouped-dataframe\r\n```\r\nIn [30]: df = pd.DataFrame({'a':range(5), 'b': range(5, 10)})\r\n\r\nIn [31]: df\r\nOut[31]: \r\n   a  b\r\n0  0  5\r\n1  1  6\r\n2  2  7\r\n3  3  8\r\n4  4  9\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [32]: df.groupby(df.a<2).max()\r\nOut[32]: \r\n       a  b\r\na          \r\nFalse  4  9\r\nTrue   1  6\r\n\r\n[2 rows x 2 columns]\r\n\r\nIn [33]: df.groupby(df.a<2,as_index=False).max()\r\nValueError: cannot insert a, already exists\r\n```"""
7114,33411132,jreback,jreback,2014-05-13 15:46:02,2014-06-30 14:19:44,2014-05-13 16:27:19,closed,,0.14.0,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7114,b'BUG: Regression in the display of a MultiIndexed Series with display.max_rows (GH7101)',b'closes #7101'
7112,33402970,sinhrks,jreback,2014-05-13 14:25:54,2014-06-14 22:31:57,2014-06-04 00:12:40,closed,,0.14.1,2,Bug;MultiIndex;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/7112,b'BUG: tzinfo lost when concatenating multiindex arrays',"b""Closes #6606. \r\n\r\nThe problem is caused by `MultiIndex.append`. Based on current master, `MultiIndex.append` works as below. The fix covers case1 and case2 which the result will be `MultiIndex`. \r\n\r\nThe fix is applied to `concat`, and also `pivot_table` work as expected.\r\n\r\n```\r\nimport pandas as pd\r\n\r\nidx1 = pd.Index([1.1, 1.2, 1.3])\r\nidx2 = pd.date_range('2011-01-01', freq='D', periods=3, tz='Asia/Tokyo')\r\nidx3 = pd.Index(['A', 'B', 'C'])\r\n\r\nmidx_lv2 = pd.MultiIndex.from_arrays([idx1, idx2])\r\nmidx_lv3 = pd.MultiIndex.from_arrays([idx1, idx2, idx3])\r\n\r\n#1 results in MultiIndex, which nlevels is 2\r\nmidx_lv2.append(midx_lv2)\r\n\r\n#2 results in MultiIndex, which nlevels is 2, not 3. 3rd line will be disappeared.\r\nmidx_lv2.append(midx_lv3)\r\n\r\n#3 results in tupled Index.\r\nmidx_lv2.append(idx1)\r\n\r\n#4 results in tupled Index.\r\nresult = midx_lv3.append(midx_lv2)\r\n```"""
7107,33392682,jreback,jreback,2014-05-13 12:20:40,2014-06-30 19:48:12,2014-05-13 12:54:42,closed,,0.14.0,0,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7107,b'BUG: remove unique requirement rom MultiIndex.get_locs (GH7106)',"b'enables slicing of non-unique multi-indexes with slicers, closes #7106'"
7106,33391220,jreback,jreback,2014-05-13 11:57:08,2014-05-13 17:47:22,2014-05-13 12:54:42,closed,,0.14.0,0,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7106,b'BUG: multiindex slicers requires a unique index',"b""This is actually not required, and so should work I think\r\n(requires lexsorting only)\r\n```\r\nIn [70]: df = DataFrame(dict(A = ['foo','foo','foo','foo'], B = ['a','a','a','a'], C = [1,2,1,3], D = [1,2,3,4])).set_index(['A','B','C']).sortlevel()\r\n\r\nIn [71]: df\r\nOut[71]: \r\n         D\r\nA   B C   \r\nfoo a 1  1\r\n      1  3\r\n      2  2\r\n      3  4\r\n\r\n[4 rows x 1 columns]\r\n\r\nIn [72]: df.xs(1,level=2)\r\nOut[72]: \r\n       D\r\nA   B   \r\nfoo a  1\r\n    a  3\r\n\r\n[2 rows x 1 columns]\r\n\r\nIn [73]: df.loc[(slice(None),slice(None),1),:]\r\nValueError: MultiIndex Slicing requires a unique index\r\n```"""
7101,33271242,bjonen,jreback,2014-05-11 21:31:01,2014-05-13 16:27:19,2014-05-13 16:27:19,closed,,0.14.0,6,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7101,b'Truncated view for MultiIndex pd.Series broken',b'Taking the simple example of MultiIndex Series from the docs http://pandas.pydata.org/pandas-docs/stable/indexing.html#hierarchical-indexing-multiindexc\r\n\r\n![screenshot from 2014-05-11 23 24 02](https://cloud.githubusercontent.com/assets/832380/2939644/b62d4a02-d952-11e3-97c6-447df3f63589.png)\r\n\r\nConstraining max_rows results in an unexpected output. \r\n\r\n![screenshot from 2014-05-11 23 24 26](https://cloud.githubusercontent.com/assets/832380/2939645/b62d8058-d952-11e3-98ac-5ce48d57bfae.png)\r\n\r\nThe behavior of pd.DataFrame is as expected in 0.13.1: \r\n\r\n![screenshot from 2014-05-11 23 27 59](https://cloud.githubusercontent.com/assets/832380/2939653/2544a50c-d953-11e3-8d18-39e8e435f1d1.png)\r\n'
7099,33263436,sinhrks,jreback,2014-05-11 15:03:14,2014-06-13 19:00:49,2014-05-13 12:13:26,closed,,0.14.0,2,Bug;Groupby;Timezones,https://api.github.com/repos/pydata/pandas/issues/7099,"b""BUG: GroupBy doesn't preserve timezone""",b'Closes #3950. This is a fix for `groupby` issue.\r\n(#7092 is for original report)'
7095,33249430,jreback,jreback,2014-05-10 21:28:22,2015-09-20 18:52:13,2015-09-20 18:52:13,closed,,0.17.0,1,Bug;Internals;Missing-data,https://api.github.com/repos/pydata/pandas/issues/7095,b'BUG: fillna with invalid value for a block type',"b'from SO: https://groups.google.com/forum/#!topic/pydata/C8VoV4do7io\r\n\r\nneed to catch errors on the putmask and skip that block (returning it unchanged)\r\n\r\n```\r\nIn [9]: df = DataFrame(dict(A = np.random.randn(3),B=date_range(\'20130101\',periods=3),C=[\'foo\',\'bar\',\'bah\'],D=[\'foo2\',\'bar2\',\'bah2\']),index=date_range(\'20130110\',periods=3))\r\n\r\nIn [10]: df\r\nOut[10]: \r\n                   A          B    C     D\r\n2013-01-10 -2.002643 2013-01-01  foo  foo2\r\n2013-01-11  1.897382 2013-01-02  bar  bar2\r\n2013-01-12  0.038903 2013-01-03  bah  bah2\r\n\r\n[3 rows x 4 columns]\r\n\r\nIn [11]: df.iloc[2,[0,2,3]] = np.nan\r\n\r\nIn [12]: df\r\nOut[12]: \r\n                   A          B    C     D\r\n2013-01-10 -2.002643 2013-01-01  foo  foo2\r\n2013-01-11  1.897382 2013-01-02  bar  bar2\r\n2013-01-12       NaN 2013-01-03  NaN   NaN\r\n\r\n[3 rows x 4 columns]\r\n\r\nIn [13]: df.fillna(-1)\r\nOut[13]: \r\n                   A          B    C     D\r\n2013-01-10 -2.002643 2013-01-01  foo  foo2\r\n2013-01-11  1.897382 2013-01-02  bar  bar2\r\n2013-01-12 -1.000000 2013-01-03   -1    -1\r\n\r\n[3 rows x 4 columns]\r\n\r\nIn [14]: df.fillna(\'?\')\r\nValueError: Error parsing datetime string ""?"" at position 0\r\n```\r\n\r\n'"
7094,33246610,jreback,jreback,2014-05-10 19:31:34,2014-07-16 09:05:33,2014-05-10 19:49:51,closed,,0.14.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7094,b'REGR: Regression in groupby.nth() for out-of-bounds indexers (GH6621)',b'closes #6621 '
7093,33243147,TomAugspurger,TomAugspurger,2014-05-10 17:20:15,2014-07-16 09:05:32,2014-05-10 21:51:19,closed,,0.14.0,4,Bug;Dtypes;Numeric;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7093,b'BUG: Let DataFrame.quantile() handle datetime',b'Closes https://github.com/pydata/pandas/issues/6965\r\n\r\n'
7092,33240271,sinhrks,jreback,2014-05-10 15:28:00,2014-06-13 19:01:07,2014-05-12 13:34:58,closed,,0.14.0,2,Bug;Dtypes;Period,https://api.github.com/repos/pydata/pandas/issues/7092,b'BUG: tz info lost by set_index and reindex',"b""Closes #6631. Closes #5878. Regarding #3950, original problem can be fixed by this, but `groupby` problem isn't.\r\n\r\nAlso, this includes the fix for `MultiIndex.get_level_values` doesn't retain `tz` and `freq`, as the method is used in `set_index`.\r\n\r\n```\r\n# current master\r\n>>> import pandas as pd\r\n>>> didx = pd.DatetimeIndex(start='2013/01/01', freq='H', periods=4, tz='Asia/Tokyo')\r\n>>> pidx = pd.PeriodIndex(start='2013/01/01', freq='H', periods=4)\r\n\r\n>>> midx = pd.MultiIndex.from_arrays([didx, pidx])\r\n>>> midx.get_level_values(0)\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2012-12-31 15:00:00, ..., 2012-12-31 18:00:00]\r\nLength: 4, Freq: None, Timezone: None\r\n\r\n>>> midx.get_level_values(1)\r\nInt64Index([376944, 376945, 376946, 376947], dtype='int64')\r\n```\r\n\r\n```\r\n# after fix\r\n>>> midx.get_level_values(0)\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2013-01-01 00:00:00+09:00, ..., 2013-01-01 03:00:00+09:00]\r\nLength: 4, Freq: H, Timezone: Asia/Tokyo\r\n\r\n>>> midx.get_level_values(1)\r\nPeriodIndex([u'2013-01-01 00:00', u'2013-01-01 01:00', u'2013-01-01 02:00', u'2013-01-01 03:00'], freq='H')\r\n"""
7089,33199719,cpcloud,jreback,2014-05-09 19:08:36,2014-07-16 09:05:23,2014-05-10 15:23:31,closed,cpcloud,0.14.0,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7089,b'BUG: use size attribute (not method call)',b'xref: #7055 '
7087,33171626,jreback,jreback,2014-05-09 13:25:30,2014-06-26 19:40:48,2014-05-09 18:21:49,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7087,b'BUG: cache coherence issue with chain indexing and setitem (GH7084)',b'closes #7084'
7084,33137805,Jenders74,jreback,2014-05-09 00:50:09,2014-05-09 18:21:49,2014-05-09 18:21:49,closed,,0.14.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7084,b'Updating a DataFrame by iteratively indexing into a columns',"b""I am initializing a DataFrame with 0 and then update it by iteratively indexing into indvidual columns. The behavior of my code has changed with pandas 0.13.0 such that resulting DataFrame out[['A']] remains 0 but series out['A'] has the correct values:\r\n\r\n```\r\n>>> print out[['A']]\r\n            A\r\n2014-05-07  0\r\n2014-05-08  0\r\n2014-05-09  0\r\n\r\n >>> print out['A']\r\n2014-05-07    600\r\n2014-05-08    600\r\n2014-05-09    600\r\n```\r\n\r\nIs this a bug?\r\n\r\n```python\r\nimport pandas\r\n#initialize a DataFrame with 0 values\r\nout = pandas.DataFrame({'A': [0, 0, 0]})\r\nout.index = pandas.date_range('5/7/2014', '5/9/2014')\r\n\r\n#DataFrame to update out with\r\ndf = pandas.DataFrame({'C': ['A', 'A', 'A'], 'D': [100, 200, 300]})\r\n\r\n#loop through df to update out\r\nfor ix, row in df.iterrows():\r\n    six = pandas.Timestamp('5/7/2014')\r\n    eix = pandas.Timestamp('5/9/2014')\r\n    out[row['C']][six:eix] = out[row['C']][six:eix] + row['D']\r\n\r\nprint out\r\nprint out[['A']]\r\nprint out['A']\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 2.7.6.final.0\r\nOS: Windows\r\nRelease: 7\r\nProcessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US\r\n\r\npandas: 0.13.0\r\nCython: 0.20\r\nNumpy: 1.7.1\r\nScipy: 0.12.0\r\nstatsmodels: 0.5.0\r\n    patsy: 0.2.1\r\nscikits.timeseries: Not installed\r\ndateutil: 1.5\r\npytz: 2013.9\r\nbottleneck: 0.7.0\r\nPyTables: 3.1.0rc2\r\n    numexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.2\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: Not installed\r\nsqlalchemy: Not installed\r\nlxml: Not installed\r\nbs4: 4.3.2\r\nhtml5lib: Not installed\r\nbigquery: Not installed\r\napiclient: Not installed\r\n```"""
7081,33124133,TomAugspurger,TomAugspurger,2014-05-08 21:08:46,2014-07-16 09:05:02,2014-05-09 00:30:13,closed,,0.14.0,3,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/7081,b'BUG: read_fwf colspec should treat None like slice',b'Closes https://github.com/pydata/pandas/issues/7079'
7079,33113636,AllenDowney,TomAugspurger,2014-05-08 19:08:43,2014-05-09 00:30:13,2014-05-09 00:30:13,closed,,0.14.0,10,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/7079,b'BUG: None should be a valid colspec?',"b'In https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py, the __init__ method for FixedWidthReaders checks whether the colspecs are valid.  Since None is a value slice index, should it be allowed as a colspec?\r\n\r\nThis would help me because I don\'t know the width of the last column until I start reading the file.\r\n\r\n    class FixedWidthReader(object):\r\n    """"""\r\n    A reader of fixed-width lines.\r\n    """"""\r\n    def __init__(self, f, colspecs, delimiter, comment):\r\n        self.f = f\r\n        self.buffer = None\r\n        self.delimiter = \'\\r\\n\' + delimiter if delimiter else \'\\n\\r\\t \'\r\n        self.comment = comment\r\n        if colspecs == \'infer\':\r\n            self.colspecs = self.detect_colspecs()\r\n        else:\r\n            self.colspecs = colspecs\r\n\r\n        if not isinstance(self.colspecs, (tuple, list)):\r\n            raise TypeError(""column specifications must be a list or tuple, ""\r\n                            ""input was a %r"" % type(colspecs).__name__)\r\n\r\n        for colspec in self.colspecs:\r\n            if not (isinstance(colspec, (tuple, list)) and\r\n                    len(colspec) == 2 and\r\n                    isinstance(colspec[0], (int, np.integer)) and\r\n                    isinstance(colspec[1], (int, np.integer))):\r\n                raise TypeError(\'Each column specification must be \'\r\n                                \'2 element tuple or list of integers\')\r\n'"
7075,33065144,filmor,jreback,2014-05-08 08:57:46,2015-01-06 14:44:42,2014-05-08 13:38:37,closed,,0.14.0,6,Bug;IO Excel;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/7075,b'Fix Xl(sx|wt)Writer always using date_format even if a datetime is supplied.',"b'In the `XlsxWriter` and `XlwtWriter` implementations there is a minor bug resulting in the `datetime_format` being discarded (i.e. https://github.com/pydata/pandas/blob/master/pandas/io/excel.py#L662):\r\n\r\n    if isinstance(cell.val, datetime.datetime):\r\n        num_format_str = self.datetime_format\r\n    if isinstance(cell.val, datetime.date):\r\n        num_format_str = self.date_format\r\n\r\nSince `datetime.datetime` derives from `datetime.date` the second `if`-clause in the  will always trigger, leading to the `num_format_str` being overwritten by the `date_format`.'"
7068,33029770,cpcloud,cpcloud,2014-05-07 22:09:43,2014-07-16 09:04:53,2014-05-08 12:39:13,closed,cpcloud,0.14.0,9,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7068,b'BUG: PEBKAC bug in Float64Index',b'closes #7066'
7066,33024171,goyodiaz,cpcloud,2014-05-07 21:00:13,2014-05-08 12:39:13,2014-05-08 12:39:13,closed,cpcloud,0.14.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/7066,b'Index.isin() always True for nans',"b'This looks like a regression to me, the following code used to print `[ True]`, which I think is the correct behaviour, but prints `[False]` in recent development versions (0.13.1.dev).\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ni = pd.Index([np.nan])\r\nprint i.isin({0})  # prints [ True], should be [False]\r\n```\r\n\r\nI do not think the following is relevant but just in case:\r\n\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-24-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: es_ES.UTF-8\r\n\r\npandas: 0.13.1.dev\r\nnose: 1.3.1\r\nCython: None\r\nnumpy: 1.8.1\r\nscipy: 0.13.3\r\nstatsmodels: 0.6.0.dev-Unknown\r\nIPython: 3.0.0-dev\r\nsphinx: None\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2012c\r\nbottleneck: 0.8.0\r\ntables: 3.1.1\r\nnumexpr: 2.2.2\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.7.0\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.2\r\nlxml: 3.3.3\r\nbs4: 4.2.1\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.4.5 (dt dec mx pq3 ext)\r\n```'"
7060,32935447,sboehler,sboehler,2014-05-06 21:34:21,2014-08-22 12:01:02,2014-08-22 12:00:52,closed,,,3,Bug;IO Excel;Timezones,https://api.github.com/repos/pydata/pandas/issues/7060,b'Remove tzinfo from datetime before writing cell with xlsxwriter',b'fixes #7056'
7052,32875614,Marigold,jreback,2014-05-06 08:59:35,2014-05-06 10:14:52,2014-05-06 10:14:52,closed,,0.16.0,1,Bug;Duplicate;Missing-data;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7052,b'Hierarchical reindexing with NaN in index',"b""related #6322\r\ndupe of #7031 \r\n\r\nI'm on 0.13.1, don't know if this is already fixed on master, but haven't found any issues about this. If I you have `NaN` value in MultiIndex, reindexing will fill the first value from reindexed series to all `NaN` indices. Perhaps an example will clarify it\r\n\r\nCode:\r\n``` python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nix = pd.MultiIndex.from_tuples([('a',np.nan)], names=['col1', 'col2'])\r\npd.Series([1], index=[1]).reindex(ix, level=1)\r\n```\r\n\r\nOutput:\r\n```\r\ncol1  col2\r\na     NaN     1\r\n```\r\n\r\nDesired output:\r\n```\r\ncol1  col2\r\na     NaN     NaN\r\n```"""
7044,32805988,jreback,jreback,2014-05-05 12:43:30,2014-07-16 09:04:40,2014-05-12 12:41:17,closed,,0.14.0,7,API Design;Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/7044,b'API: update nth to use the _set_selection_from_grouper makes first==nth(0) and last==nth(-1)',"b""closes  #6732\r\n\r\n``nth`` sets the index appropriately and the same as first/last.\r\n\r\nthis becomes less like head/tail in that ``as_index`` determines when you have an index\r\n\r\nhere's the revised behavior:\r\n\r\n```\r\nIn [1]: df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\r\n\r\nIn [3]: df\r\nOut[3]: \r\n   A   B\r\n0  1 NaN\r\n1  1   4\r\n2  5   6\r\n\r\n[3 rows x 2 columns]\r\n\r\nIn [2]: g = df.groupby('A')\r\n\r\nIn [9]: gni = df.groupby('A',as_index=False)\r\n```\r\n\r\n```\r\nIn [5]: g.first()\r\nOut[5]: \r\n   B\r\nA   \r\n1  4\r\n5  6\r\n\r\n[2 rows x 1 columns]\r\n\r\n# this was a regression from 0.13.1 (in that before this PR, this was returning like ``as_index=False``)\r\nIn [6]: g.nth(0)\r\nOut[6]: \r\n    B\r\nA    \r\n1 NaN\r\n5   6\r\n\r\n[2 rows x 1 columns]\r\n\r\nIn [7]: g.nth(0,dropna='all')\r\nOut[7]: \r\n   B\r\nA   \r\n1  4\r\n5  6\r\n\r\n[2 rows x 1 columns]\r\n```\r\n\r\n```\r\nIn [10]: gni.nth(0)\r\nOut[10]: \r\n   A   B\r\n0  1 NaN\r\n2  5   6\r\n\r\n[2 rows x 2 columns]\r\n\r\nIn [11]: gni.nth(0,dropna='all')\r\nOut[11]: \r\n   A  B\r\n0  1  4\r\n1  5  6\r\n\r\n[2 rows x 2 columns]\r\n```"""
7043,32805800,sinhrks,jreback,2014-05-05 12:39:57,2014-06-17 01:47:00,2014-05-13 14:29:35,closed,,0.14.0,5,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/7043,b'ENH/BUG: partial string indexing with PeriodIndex',b'Closes #6716.'
7038,32786180,sinhrks,jreback,2014-05-05 04:12:11,2014-07-16 09:04:30,2014-05-10 11:32:16,closed,,0.14.0,8,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/7038,b'BUG: DatetimeIndex cannot parse string ndarray with dayfirst',b'Closes #5917.'
7035,32780042,sinhrks,TomAugspurger,2014-05-04 23:16:27,2014-06-14 11:21:40,2014-05-10 16:16:08,closed,,0.14.1,15,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/7035,b'ENH/BUG: boxplot now supports layout',"b""Closes #6769. Added `layout` kw to `boxplot`. I think tests can be improved if boxplot can return ndarray with the same shape as layout, as the same manner as hist (maybe after #4472).\r\n\r\n## Bug fix\r\nIt includes the fix to hide unnecessary axes than required in boxplot and hist. For example, `layout=(2, 2)` is specified for 3 subplots. I've listed affected cases as below.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({'3g': 'A A A B B B C C C'.split(),\r\n                   '2g': [1, 2, 1, 2, 1, 2, 1, 2, 2],\r\n                   'values': np.random.rand(9), \r\n                   'values2': np.random.rand(9),\r\n                   'values3': np.random.rand(9)})\r\n# BoxPlot\r\n\r\n# specify 3 columns with by keyword -> should be 3 axes in (2, 2) layout\r\ndf.boxplot(column=['values', 'values2', 'values3'], by='2g')\r\n\r\n# groupby results in 3 groups -> should be 3 axes \r\ndf.groupby('3g').boxplot()\r\n\r\n# groupby results in 3 groups, with column kw -> should be 3 axes in (2, 2) layout\r\ndf.groupby('3g').boxplot(column=['values', 'values2', 'values3'])\r\n\r\n# Histogram\r\n\r\n# specify 3 columns without by kw -> should be 3 axes in (2, 2) layout\r\ndf.hist(column=['values', 'values2', 'values3'])\r\n\r\n# groupby results 3 groups results of by kw -> should be 3 axes in (2, 2) layout\r\ndf.hist(column=['values'], by='3g')\r\n# This results KeyError: '3g' in current master, and the error has been fixed.\r\n\r\n# groupby results in 3 groups -> should be 3 axes in (2, 2) layout\r\ndf.hist(by='3g')\r\n\r\n# layout contains more size than groups -> should be 2 axes in specified layout\r\ndf.hist(by='2g', layout=(2, 2))\r\n```"""
7033,32777682,TomAugspurger,jreback,2014-05-04 21:46:30,2015-05-09 16:08:38,2015-05-09 16:08:38,closed,,Next Major Release,11,API Design;Bug;Enhancement;Groupby,https://api.github.com/repos/pydata/pandas/issues/7033,"b""API: Allow groupby's by to take column and index names [WIP]""","b'closes https://github.com/pydata/pandas/issues/5677\r\n\r\nAs a reminder, with a  data frame like:\r\n\r\n```python\r\nfrom itertools import cycle, islice \r\n\r\nnp.random.seed(0)\r\ndf = pd.DataFrame(np.random.randn(20, 2))\r\ndf.columns = [""foo"",""bar""]\r\ndf[\'g0\'] = list(islice(cycle(\'ab\'), 20))\r\ndf[\'g1\'] = [\'a\'] * 10 + [\'b\'] * 10\r\ndf[\'g2\'] = [\'c\'] * 5 + [\'d\'] * 5 + [\'c\'] * 5 + [\'d\'] * 5\r\n\r\ndf = df.set_index([\'g1\', \'g2\'])\r\n```\r\n\r\nBefore if you wanted to groupby a a column and index level, you\'d have to\r\n\r\n```python\r\ng = df.reset_index().groupby([\'g0\', \'g1\'])\r\n```\r\n\r\nNow you can just do `df.groupby([\'g0\', \'g1\'])`. In ambiguous cases where a there\'s a key in `by` that\'s in both the index names and columns we warn and proceed with grouping by the columns (I haven\'t tested this part yet).'"
7029,32753996,mcwitt,jreback,2014-05-03 20:04:50,2014-06-23 23:28:38,2014-05-06 18:02:22,closed,,0.14.0,7,Bug;IO CSV;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/7029,b'BUG: fix reading multi-index data in python parser',"b'partial fix for #6893\r\n\r\nThe python parser has a problem reading data with a multi-index specified in the row following the header, for example\r\n\r\n```python\r\nIn [3]: text = """"""                      A       B       C       D        E\r\none two three   four\r\na   b   10.0032 5    -0.5109 -2.3358 -0.4645  0.05076  0.3640\r\na   q   20      4     0.4473  1.4152  0.2834  1.00661  0.1744\r\nx   q   30      3    -0.6662 -0.5243 -0.3580  0.89145  2.5838""""""\r\n\r\nIn [4]: pd.read_table(StringIO(text), sep=\'\\s+\', engine=\'python\')\r\nOut[4]: \r\n                           E\r\none two three   four        \r\na   b   10.0032 5     0.3640\r\n    q   20.0000 4     0.1744\r\nx   q   30.0000 3     2.5838\r\n\r\n[3 rows x 1 columns]\r\n```\r\n\r\n(the C parser doesn\'t make it this far, see #6893)\r\n\r\nThis PR fixes the bug in the python parser:\r\n\r\n```python\r\nIn [4]: pd.read_table(StringIO(text), sep=\'\\s+\', engine=\'python\')\r\nOut[4]: \r\n                           A       B       C        D       E\r\none two three   four                                         \r\na   b   10.0032 5    -0.5109 -2.3358 -0.4645  0.05076  0.3640\r\n    q   20.0000 4     0.4473  1.4152  0.2834  1.00661  0.1744\r\nx   q   30.0000 3    -0.6662 -0.5243 -0.3580  0.89145  2.5838\r\n\r\n[3 rows x 5 columns]\r\n```\r\n'"
7026,32688907,jreback,jreback,2014-05-02 12:44:11,2014-07-05 02:58:59,2014-05-02 21:37:21,closed,,0.14.0,0,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/7026,b'BUG: error in Float64Index with contains and non-float (GH7025)',b'closes #7025'
7025,32688171,grundprinzip,jreback,2014-05-02 12:31:04,2014-05-02 21:37:21,2014-05-02 21:37:21,closed,,0.14.0,3,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/7025,b'GroupBy with Float Index not Plotting',"b""I have the following problem: I have a dataset with a float index that I want to group and plot. If I'm using integer indices it works, with floats it doesn't. Here is the minimal example that once works and once not. I'm testing with the current master. Am I doing something wrong?\r\n\r\nThis code snippet fails:\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    test = pd.DataFrame({'def': [1,1,1,2,2,2,3,3,3], 'val': np.random.randn(9)}, index=[1.0,2.0,3.0,1.0,2.0,3.0,1.0,2.0,3.0])\r\n    test.groupby('def')['val'].plot()\r\n\r\nWhile this works:\r\n\r\n    # This Works\r\n    test = pd.DataFrame({'def': [1,1,1,2,2,2,3,3,3], 'val': np.random.randn(9)}, index= [1,2,3,1,2,3,1,2,3])\r\n    test.groupby('def')['val'].plot()\r\n\r\n\r\nThe error I get is:\r\n\r\n    ---------------------------------------------------------------------------\r\n    NotImplementedError                       Traceback (most recent call last)\r\n    <ipython-input-83-e50f1a9b07c6> in <module>()\r\n    ----> 1 test.groupby('def')['val'].plot()\r\n    \r\n    /Library/Python/2.7/site-packages/pandas-0.13.1_758_g4a67608-py2.7-macosx-10.9-intel.egg/pandas/core/groupby.pyc in __getattr__(self, attr)\r\n        452         if attr in self._internal_names_set:\r\n        453             return object.__getattribute__(self, attr)\r\n    --> 454         if attr in self.obj:\r\n        455             return self[attr]\r\n        456 \r\n    \r\n    /Library/Python/2.7/site-packages/pandas-0.13.1_758_g4a67608-py2.7-macosx-10.9-intel.egg/pandas/core/series.pyc in __contains__(self, key)\r\n        379 \r\n        380     def __contains__(self, key):\r\n    --> 381         return key in self.index\r\n        382 \r\n        383     # complex\r\n    \r\n    /Library/Python/2.7/site-packages/pandas-0.13.1_758_g4a67608-py2.7-macosx-10.9-intel.egg/pandas/core/index.pyc in __contains__(self, other)\r\n       2042         try:\r\n       2043             # if other is a sequence this throws a ValueError\r\n    -> 2044             return np.isnan(other) and self._hasnans\r\n       2045         except ValueError:\r\n       2046             try:\r\n    \r\n    NotImplementedError: Not implemented for this type"""
7019,32618386,jreback,jreback,2014-05-01 14:32:42,2014-07-16 09:04:12,2014-05-01 15:13:57,closed,,0.14.0,0,Bug;Groupby;Testing,https://api.github.com/repos/pydata/pandas/issues/7019,"b'TST: tests for groupby not using grouper column, solved in GH7000, (GH5614)'",b'closes #5614'
7007,32505826,rosnfeld,TomAugspurger,2014-04-30 04:27:11,2014-06-12 21:07:21,2014-05-01 17:50:35,closed,,0.14.0,7,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/7007,"b'BUG: fixing tseries plot cursor display, resolves #5453'","b""Fixes #5453\r\n\r\nI am not sure how to test a mouse-over, and the original commit 922c6102eebb7347cea587fffc4795a3ca3e73b2 introducing this functionality did not have tests either, which is sadly probably how this got broken. @willfurnass do you happen to have any ideas for how to test this functionality, maybe from when you first looked at this?\r\n\r\nThe new code doesn't display timezone information like the old code once did (before it was broken) as Periods don't seem to have timezone info, and all ```tseries/plotting.py``` plots are Period-based. One could perhaps do some minor acrobatics to convert the tz-naive start time of the Period into a tz-aware Timestamp, I can research that if people feel strongly about it.\r\n"""
7006,32498032,jreback,jreback,2014-04-30 00:43:53,2014-07-16 09:03:54,2014-04-30 01:05:17,closed,,0.14.0,0,Bug;Indexing;Testing,https://api.github.com/repos/pydata/pandas/issues/7006,b'BUG: duplicate indexing with setitem with iloc (GH6766)',b'closes #6766 '
7000,32456085,jreback,jreback,2014-04-29 15:30:07,2014-06-15 01:48:09,2014-04-29 20:12:46,closed,,0.14.0,22,Bug;Enhancement;Groupby,https://api.github.com/repos/pydata/pandas/issues/7000,b'ENH/BUG: add count to grouper / ensure that grouper keys are not included in the returned',b'closes #5610\r\n'
6997,32410538,hayd,jreback,2014-04-29 00:19:34,2014-07-17 17:02:27,2014-04-29 15:32:00,closed,,0.14.0,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6997,b'make grouping column an agg',b'fixes #5610'
6991,32356272,onesandzeroes,TomAugspurger,2014-04-28 12:52:51,2014-06-25 17:29:12,2014-05-08 13:21:04,closed,,0.14.0,11,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6991,b'BUG: df.boxplot fails to use existing axis/subplot (#3578)',"b'Alright, I think I have a fix for issue #3578. In `plotting._grouped_plot_by_column`, there was no check being done for whether `ax` was `None`, which was the main source of the issue. When the boxplot is of multiple columns, I don\'t think there\'s anything sensible that can be done with the `ax` argument, so that\r\nnow raises a `ValueError`.\r\n\r\nI\'ve written some tests, but I\'m not sure if they really get to the heart of the problem, so any insight on how to improve them would be appreciated.\r\n\r\nTesting code adapted from the original bug report to demonstrate the new behaviour:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport pandas\r\nfrom pandas import DataFrame, Series\r\n\r\ndata = {\'day\': Series([1, 1, 1, 2, 2, 2, 3, 3, 3]),\r\n        \'group2\': Series([2, 2, 2, 2, 2, 1, 1, 1, 1]),\r\n        \'val\': Series([3, 4, 5, 6, 7, 8, 9, 10, 11]),\r\n        \'val2\': Series([8, 9, 10, 11, 12, 13, 14, 15])}\r\ndf = pandas.DataFrame(data)\r\n\r\n# Single-column using existing axis: should create a single plot\r\nplt.figure()\r\nplt.subplot(2, 2, 1)\r\nplt.plot([1, 2, 3])\r\nplt.subplot(2, 2, 4)\r\nax = plt.gca()\r\nfig = ax.get_figure()\r\naxes = df.boxplot(\'val\', \'day\', ax=ax)\r\nprint(""Testing identity of returned axes: (should be True)"")\r\nprint(id(axes) == id(ax.get_axes()))\r\nplt.show()\r\n\r\n# Multiple column, not using existing axis: should create two plots\r\nplt.figure()\r\nplt.subplot(2, 2, 1)\r\nplt.plot([1, 2, 3])\r\nplt.subplot(2, 2, 4)\r\nax = plt.gca()\r\naxes = df.boxplot([\'val\', \'val2\'], \'day\')\r\nprint(""Testing identity of returned axes: (should be False)"")\r\nprint(id(axes) == id(ax.get_axes()))\r\nplt.show()\r\n\r\n\r\n# Multiple column using existing axis: should raise an exception,\r\n# since it\'s hard to know what to do: we need an axis for each\r\n# column and ax is just a single axis object\r\nplt.figure()\r\nplt.subplot(2, 2, 1)\r\nplt.plot([1, 2, 3])\r\nplt.subplot(2, 2, 4)\r\nax = plt.gca()\r\ntry:\r\n    df.boxplot([\'val\', \'val2\'], \'day\', ax=ax)\r\nexcept ValueError:\r\n    print(""Raising exception as expected"")\r\nplt.show()\r\n```'"
6981,32314514,jreback,jreback,2014-04-27 16:09:21,2014-07-22 18:40:54,2014-04-27 16:09:32,closed,,,0,Bug;Dtypes;Windows,https://api.github.com/repos/pydata/pandas/issues/6981,b'COMPAT: windows dtype fix in for Panel.count',
6975,32300283,maxgrenderjones,jreback,2014-04-26 22:55:12,2014-06-20 23:25:14,2014-04-30 12:40:10,closed,,0.14.0,8,Bug;Strings;Unicode,https://api.github.com/repos/pydata/pandas/issues/6975,b'Fix for GH 6885 - get_dummies chokes on unicode values',"b""closes #6885\r\n\r\nPlease be gentle - this is the first time I've tried to contribute either to someone else's python project or another project on github, so if my python or git foo is lacking, I apologise in advance!\r\n"""
6974,32296708,dalejung,jreback,2014-04-26 19:50:48,2014-06-18 12:23:39,2014-04-28 23:21:23,closed,,0.14.0,4,Bug;Dtypes;Performance,https://api.github.com/repos/pydata/pandas/issues/6974,b'Panel shift revert',b'Reverts #6605 closes #6959 #6826\r\n\r\n## vs 13.1\r\n\r\n```\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\npanel_shift                                  |   0.0773 |   0.0990 |   0.7809 |\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\n\r\nRatio < 1.0 means the target commit is faster then the baseline.\r\nSeed used: 1234\r\n\r\nTarget [3d0099a] : BUG: Change Panel.shift to use slice_shift #6605 #6826\r\nBase   [d10a658] : RLS: set released to True. v0.13.1\r\n```\r\n\r\n## vs master\r\n\r\nNote that the `pct_change is slower due to the deferred alignment. \r\n\r\n```\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\npanel_shift                                  |   0.1650 | 522.0180 |   0.0003 |\r\npanel_pct_change_major                       | 7615.9450 | 6213.1660 |   1.2258 |\r\npanel_pct_change_minor                       | 7864.9763 | 5877.1353 |   1.3382 |\r\npanel_pct_change_items                       | 8646.0753 | 5884.1163 |   1.4694 |\r\n-------------------------------------------------------------------------------\r\nTest name                                    | head[ms] | base[ms] |  ratio   |\r\n-------------------------------------------------------------------------------\r\n\r\nRatio < 1.0 means the target commit is faster then the baseline.\r\nSeed used: 1234\r\n\r\nTarget [3ec426f] : BUG: Change Panel.shift to use slice_shift #6605 #6826\r\nBase   [0d2966f] : COMPAT:  32-bit platform compat for Panel.count\r\n```'
6970,32260194,Amyunimus,TomAugspurger,2014-04-25 19:32:29,2014-08-19 17:09:14,2014-08-19 17:09:14,closed,,0.15.0,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6970,b'Cannot subset',"b""from SO: http://stackoverflow.com/q/23301072/564538\r\n\r\nI'm trying to print three pandas boxplots next to each other in iPython Notebook.\r\n\r\nThe dataframes each look basically like this:\r\n\r\n```python\r\nsub    cond   accuracy\r\ns1     A      0.814868\r\ns2     A      0.504574\r\ns3     A      0.438314\r\ns4     A      0.956235\r\ns5     A      0.370771\r\ns1     B      0.228724\r\ns2     B      0.691374\r\ns3     B      0.237314\r\ns4     B      0.32633\r\ns5     B      0.859961 \r\n```\r\nWhen I plot accuracy without subsetting by condition, this works great:\r\n\r\n```python\r\nfig, axes = plt.subplots(nrows=1, ncols=3)\r\nfor i, df in enumerate([df1, df2, df3]):\r\n    df.boxplot('accuracy',ax=axes[i])\r\n```\r\nHowever, when I try to plot accuracy by condition for each dataframe using instead:\r\n\r\n```python\r\ndf.boxplot('accuracy',by='cond',ax=axes[i])\r\n```\r\n\r\nonly the last plot displays (and not where it should be according to its axes assignment). \r\n\r\nThis seems to be an issue ing the ```by=X``` argument."""
6969,32259973,cpcloud,jreback,2014-04-25 19:29:00,2014-04-25 19:36:00,2014-04-25 19:36:00,closed,cpcloud,0.16.0,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6969,b'boxplot with by parameter draws over multiple axes',b'http://stackoverflow.com/q/23301072/564538'
6966,32235469,immerrr,jreback,2014-04-25 14:15:57,2015-04-28 11:56:04,2015-04-28 11:56:04,closed,,0.16.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/6966,"b""BUG: clip_lower/-upper don't accept a sequence starting from 0.12.0""","b'Here\'s what I mean:\r\n\r\n```python\r\nIn [4]: pd.Series(np.arange(7))\r\nOut[4]: \r\n0    0\r\n1    1\r\n2    2\r\n3    3\r\n4    4\r\n5    5\r\n6    6\r\ndtype: int64\r\n\r\n# Let\'s put some limit on weekends\r\nIn [5]: _4.clip_upper([3,3,3,3,3, 4,4])\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-3268c7bee292> in <module>()\r\n----> 1 _4.clip_upper([3,3,3,3,3, 4,4])\r\n/home/dshpektorov/sources/pandas/pandas/core/generic.pyc in clip_upper(self, threshold)\r\n   2636         clipped : same type as input\r\n   2637         """"""\r\n-> 2638         if isnull(threshold):\r\n   2639             raise ValueError(""Cannot use an NA value as a clip threshold"")\r\n   2640 \r\n\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\n```'"
6965,32225705,TomAugspurger,TomAugspurger,2014-04-25 11:57:01,2014-05-10 21:51:19,2014-05-10 21:51:19,closed,,0.14.0,2,Bug;Dtypes;Numeric;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6965,b'BUG: DataFrame.quantile fails on datetime values',"b""```python\r\nIn [2]: df = DataFrame({'a': pd.to_datetime(['2010', '2011']), 'b': [0, 5]})\r\n\r\nIn [3]: df.quantile(.5, numeric_only=False)\r\nOut[3]: \r\na    1.278072e+18\r\nb    1.278072e+18\r\ndtype: float64\r\n```\r\n\r\nSeries handles things fine.\r\n\r\nWe need to do a `view('i8')` on the date time array somewhere, but I had trouble just sticking it inside of the `f` that gets passed to `data.apply(f, ...)`"""
6959,32198029,dalejung,jreback,2014-04-25 00:28:46,2014-04-28 23:21:23,2014-04-28 23:21:23,closed,,0.14.0,0,Bug;Dtypes;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6959,b'Panel.shift does not respect dtypes 13.1',"b""Because the 13.1 shift uses `.values`, it will upcast mixed dtypes to object. \r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pandas.util.testing as tm\r\n\r\n\r\ndata = [('item '+ch, tm.makeMixedDataFrame()) for ch in list('abcde')]\r\ndata = dict(data)\r\nmixed_panel = pd.Panel.from_dict(data, orient='minor')\r\nshifted = mixed_panel.shift(1)\r\ntm.assert_series_equal(mixed_panel.dtypes, shifted.dtypes) #fails\r\n```\r\n\r\nHave PR for this."""
6954,32155153,jreback,jreback,2014-04-24 14:49:48,2014-07-16 09:03:00,2014-04-24 22:43:18,closed,,0.14.0,0,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/6954,b'BUG: Bug in sum/mean on 32-bit platforms on overflows (GH6915)',b'closes #6915\r\n\r\noverflow when doing sum (and mean) using bottleneck/numpy on 32-bit platforms with 32-bit dtypes'
6949,32138215,immerrr,sinhrks,2014-04-24 10:50:41,2016-04-05 21:50:43,2016-04-05 21:50:43,closed,,Next Major Release,1,Bug;Sparse,https://api.github.com/repos/pydata/pandas/issues/6949,"b""BUG: filling doesn't work well for sparse blocks""","b""```python\r\n\r\nIn [1]: pd.__version__\r\nOut[1]: '0.13.1'\r\n\r\nIn [2]: pd.DataFrame({'a': pd.SparseArray([1,2,3, np.nan, np.nan]), 'b': [1,2,3,np.nan,np.nan]})\r\nOut[2]: \r\n    a   b\r\n0   1   1\r\n1   2   2\r\n2   3   3\r\n3 NaN NaN\r\n4 NaN NaN\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [3]: _2.reindex(_2.index[1:], fill_value=10.)\r\nOut[3]: \r\n    a   b\r\n1   2   2\r\n2   3   3\r\n3  10 NaN\r\n4  10 NaN\r\n\r\n[4 rows x 2 columns]\r\n\r\n```\r\n\r\nAlso, this looks weird, I'd expect sparse array to preserve nans:\r\n\r\n```python\r\nIn [3]: pd.SparseArray([np.nan, np.nan], fill_value=0.0).to_dense()\r\nOut[3]: array([ 0.,  0.])\r\n```"""
6941,32074485,jreback,jreback,2014-04-23 16:00:53,2014-06-17 14:27:30,2014-04-23 16:26:27,closed,,0.14.0,0,Bug;Error Reporting;IO CSV,https://api.github.com/repos/pydata/pandas/issues/6941,b'REGR/API: accept TextFileReader in concat (GH6583)',"b'closes #6583\r\n\r\n- API: change AssertionError to TypeError for invalid types passed to concat \r\n- REGR: TextFileReader in concat, which was affecting a common user idiom '"
6934,32023083,jmcnamara,jreback,2014-04-23 00:34:18,2014-06-18 06:59:55,2014-04-24 01:24:01,closed,,0.14.0,9,Bug;IO Excel;Timedelta,https://api.github.com/repos/pydata/pandas/issues/6934,b'BUG: Fix to read decimal seconds from Excel.',"b'Fix to allow decimal seconds to be read from Excel dates and times\r\ninto datetime objects. #5945.\r\n\r\nThis required a fix to the `xlrd` module to return milliseconds from Excel dates and times. That fix was recently released to PyPI in xlrd version 0.9.3.\r\n\r\nTests, version updates and release note included.'"
6931,32002190,jreback,jreback,2014-04-22 19:21:40,2014-07-16 09:02:47,2014-04-22 19:44:47,closed,,0.14.0,0,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6931,b'BUG/INT: Internal tests for patching __finalize__ / bug in concat not finalizing (GH6927)',b'closes #6927'
6927,31989036,d10genes,jreback,2014-04-22 16:35:45,2014-04-27 13:34:55,2014-04-22 19:44:47,closed,,0.14.0,2,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6927,b'DataFrame.__finalize__ not called in pd.concat',"b""When I assign metadata to a df\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    np.random.seed(10)\r\n\r\n    pd.DataFrame._metadata = ['filename']\r\n    df1 = pd.DataFrame(np.random.randint(0, 4, (3, 2)), columns=list('ab'))\r\n    df1.filenames = {'a': 'f1', 'b': 'f2'}\r\n    df1\r\n           a  b\r\n        0  1  1\r\n        1  0  3\r\n        2  0  1\r\n\r\nand define a `__finalize__` that prints when it's called\r\n\r\n    def finalize_df(self, other, method=None, **kwargs):\r\n        print 'finalize called'\r\n        for name in self._metadata:\r\n            object.__setattr__(self, name, getattr(other, name, None))\r\n        return self\r\n\r\n    pd.DataFrame.__finalize__ = finalize_df\r\n\r\nnothing is preserved when `pd.concat` is called:\r\n\r\n    stacked = pd.concat([df1, df1])  # Nothing printed\r\n    stacked\r\n           a  b\r\n        0  1  1\r\n        1  0  3\r\n        2  0  1\r\n        0  1  1\r\n        1  0  3\r\n        2  0  1\r\n    stacked.finalize  # => AttributeError \r\n\r\nFor this specific case it seems reasonable that `__finalize__` should be used since all of the elements are from the same dataframe, though I'm not sure about the general use since `concat` can also take types other than a DataFrame. But should we/do we have some method to stack dataframes that preserves metadata?\r\n\r\nSimilar to #6923.\r\n"""
6924,31974973,jreback,jreback,2014-04-22 14:01:12,2014-07-09 18:05:32,2014-04-22 14:24:08,closed,,0.14.0,0,Bug;Reshaping;Testing,https://api.github.com/repos/pydata/pandas/issues/6924,b'BUG/INT: Internal tests for patching __finalize__ / bug in merge not finalizing (GH6923)',b'closes #6923'
6923,31973148,d10genes,d10genes,2014-04-22 13:39:38,2014-04-27 13:34:16,2014-04-22 14:21:46,closed,,0.14.0,5,Bug;Reshaping;Testing,https://api.github.com/repos/pydata/pandas/issues/6923,b'series __finalized__ not correctly called in merge?',"b""I got some help from Jeff on [stackoverflow](http://stackoverflow.com/questions/23200524/propagate-pandas-series-metadata-through-joins), but either I'm misunderstanding the way `__finalized__` works, or there's a bug in how it's called. My intent was to preserve series metadata after 2 dataframes being merged, and I believe `__finalize__` should be able to handle this.\r\n\r\nI define a couple dataframes, and assign metadata values to all the series:\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    np.random.seed(10)\r\n    df1 = pd.DataFrame(np.random.randint(0, 4, (3, 2)), columns=['a', 'b'])\r\n    df2 = pd.DataFrame(np.random.randint(0, 4, (3, 2)), columns=['c', 'd'])\r\n    df1\r\n         a  b\r\n      0  1  1\r\n      1  0  3\r\n      2  0  1\r\n    df2\r\n         c  d\r\n      0  3  0\r\n      1  1  1\r\n      2  0  1\r\n\r\nThen I assign metadata field `filename` to series\r\n\r\n    pd.Series._metadata = ['name', 'filename']\r\n    \r\n    for c1 in df1:\r\n        df1[c1].filename = 'fname1.csv'\r\n    for c2 in df2:\r\n        df2[c2].filename = 'fname2.csv'\r\n\r\nNow, I'm defining `__finalize__` for series, which I understand is able to propagate metadata from one series to the other, for example when I want to merge. But when I define a `__finalize__` that prints off the metadata that I've already assigned, it looks like by the time it calls `__finalize__`, it no longer has the metadata.\r\n\r\n    def finalize_ser(self, other, method=None, **kwargs):\r\n      print 'Self meta: {}'.format(getattr(self, 'filename', None))\r\n      print 'Other meta: {}'.format(getattr(other, 'filename', None))\r\n      \r\n      for name in self._metadata:\r\n          object.__setattr__(self, name, getattr(other, name, ''))\r\n      return self\r\n  \r\n    pd.Series.__finalize__ = finalize_ser\r\n\r\nWhen I call `merge`, I never see the correct metadata printed off\r\n\r\n    df1.merge(df2, left_on=['a'], right_on=['c'], how='inner')\r\n      Self meta: None\r\n      Other meta: None\r\n      Self meta: None\r\n      Other meta: None\r\n      Self meta: None\r\n      Other meta: None\r\n      Self meta: None\r\n      Other meta: None\r\n      Out[5]:\r\n         a  b  c  d\r\n      0  1  1  1  1\r\n      1  0  3  0  1\r\n      2  0  1  0  1\r\n\r\nIt appears the metadata is lost before it gets to the `__finalize__` call, though it's still in the original series\r\n\r\n    df1.a.filename  # => 'fname1.csv'\r\n    mgd.a.filename  # => AttributeError\r\n\r\nIs this expected or is there a bug?"""
6915,31857566,zoof,jreback,2014-04-20 00:34:32,2014-04-24 15:16:31,2014-04-23 20:08:32,closed,,0.14.0,45,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/6915,b'Bug in pd.Series.mean()',"b""In some cases, the mean is computed incorrectly.  Numpy however does the correct calculation.  There is no problem with the standard deviation calculation.  The following is an example.\r\n\r\n```\r\nIn [11]: np.array(stateemp.area.tolist()).mean()\r\nOut[11]: 23785.447812211703\r\n\r\nIn [12]: stateemp.area.mean()\r\nOut[12]: 58.927762478114879\r\n\r\nIn [13]: np.array(stateemp.area.tolist()).std()\r\nOut[13]: 22883.862745218048\r\n\r\nIn [14]: stateemp.area.std()\r\nOut[14]: 22883.864924811925\r\n\r\nIn [15]: pd.__version__\r\nOut[15]: '0.13.1'\r\n\r\nIn [16]: np.__version__\r\nOut[16]: '1.8.1'\r\n```"""
6914,31851725,sinhrks,jreback,2014-04-19 18:23:39,2014-06-14 08:31:02,2014-04-28 14:07:40,closed,,0.14.0,5,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6914,b'BUG: GroupBy.get_group doesnt work with TimeGrouper',"b""`get_group` raises `AttributeError` when the group is created by `TimeGrouper`.\r\n\r\n```\r\n>>> df = pd.DataFrame({'Branch' : 'A A A A A A A B'.split(),\r\n                   'Buyer': 'Carl Mark Carl Carl Joe Joe Joe Carl'.split(),\r\n                   'Quantity': [1,3,5,1,8,1,9,3],\r\n                   'Date' : [\r\n                    datetime(2013,1,1,13,0), datetime(2013,1,1,13,5),\r\n                    datetime(2013,10,1,20,0), datetime(2013,10,2,10,0),\r\n                    datetime(2013,10,1,20,0), datetime(2013,10,2,10,0),\r\n                    datetime(2013,12,2,12,0), datetime(2013,12,2,14,0),]})\r\n\r\n>>> grouped = df.groupby(pd.Grouper(freq='1M',key='Date'))\r\n>>> grouped.get_group(pd.Timestamp('2013-12-31'))\r\nAttributeError: 'DataFrameGroupBy' object has no attribute 'indices'\r\n```"""
6909,31797894,TomAugspurger,TomAugspurger,2014-04-18 13:10:39,2014-06-23 23:13:25,2014-04-21 14:15:49,closed,,0.14.0,9,API Design;Bug;Missing-data;Numeric;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6909,b'ENH: Implement Panel pct_change',"b""Closes https://github.com/pydata/pandas/issues/6904\r\n\r\nThere's just a bit of extra index handling that needs to be done before moving on to `generic.pct_change()`. I had to adjust that to use the `.div` and `.sub` ops instead of `/` and `-` to work with panels.\r\n\r\nI wasn't sure why axis wasn't included as an actual names keyword arg. `generic` just looks for it in **kwargs. I did the same in panel.\r\n\r\nA related issue was the `Panel.shift()` has a different argument signature than `generic.shift()`. I can make those consistent and put in a deprecation warning in this issue or in a new one."""
6908,31796548,sinhrks,jreback,2014-04-18 12:34:42,2014-06-21 21:47:54,2014-04-19 12:47:08,closed,,0.14.0,5,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6908,b'BUG: TimeGrouper outputs different result by column order',"b'closes #6764\r\n\r\n`TimeGrouper` may output incorrect results depending on the target column order. The problem seems to be caused by 2 parts.\r\n- `TimeGrouper._get_time_bins` and related methods expects sorted values input.\r\n- `BinGrouper.get_iterator` expects sorted data input. \r\n\r\n```\r\n>>> df = pd.DataFrame({\'Branch\' : \'A A A A A A A B\'.split(),\r\n                   \'Buyer\': \'Carl Mark Carl Carl Joe Joe Joe Carl\'.split(),\r\n                   \'Quantity\': [1,3,5,1,8,1,9,3],\r\n                   \'Date\' : [\r\n                    datetime(2013,1,1,13,0), datetime(2013,1,1,13,5),\r\n                    datetime(2013,10,1,20,0), datetime(2013,10,2,10,0),\r\n                    datetime(2013,10,1,20,0), datetime(2013,10,2,10,0),\r\n                    datetime(2013,12,2,12,0), datetime(2013,12,2,14,0),]})\r\n\r\n# correct\r\n>>> df.groupby([pd.Grouper(freq=\'1M\',key=\'Date\'),\'Buyer\']).sum()\r\n                  Quantity\r\nDate       Buyer          \r\n2013-01-31 Carl          1\r\n           Mark          3\r\n2013-10-31 Carl          6\r\n           Joe           9\r\n2013-12-31 Carl          3\r\n           Joe           9\r\n\r\n[6 rows x 1 columns]\r\n\r\n>>> df_sorted = df.sort(\'Quantity\')          # change ""Date"" column unsorted\r\n# incorrect\r\n>>> df_sorted.groupby([pd.Grouper(freq=\'1M\',key=\'Date\'),\'Buyer\']).sum()\r\n                  Quantity\r\nDate       Buyer          \r\n2013-01-31 Carl          1\r\n2013-10-31 Carl          1\r\n           Joe           1\r\n           Mark          3\r\n2013-12-31 Carl          8\r\n           Joe          17\r\n\r\n[6 rows x 1 columns]\r\n\r\n>>> df_sorted.groupby([pd.Grouper(freq=\'1M\',key=\'Date\', sort=True),\'Buyer\']).sum()\r\n# same incorrect result\r\n```\r\n\r\n```\r\n# correct\r\n>>> df.groupby([pd.Grouper(freq=\'6M\',key=\'Date\'),\'Buyer\']).sum()\r\n                  Quantity\r\nDate       Buyer          \r\n2013-01-31 Carl          1\r\n           Mark          3\r\n2014-01-31 Carl          9\r\n           Joe          18\r\n\r\n[4 rows x 1 columns]\r\n\r\n# incorrect\r\n>>> df_sorted.groupby([pd.Grouper(freq=\'6M\',key=\'Date\'),\'Buyer\']).sum()\r\n                  Quantity\r\nDate       Buyer          \r\n2013-01-31 Carl          1\r\n2014-01-31 Carl          9\r\n           Joe          18\r\n           Mark          3\r\n\r\n[4 rows x 1 columns]\r\n```'"
6904,31746963,shura-v,TomAugspurger,2014-04-17 17:43:19,2014-04-21 14:15:49,2014-04-21 14:15:49,closed,,0.14.0,3,Algos;API Design;Bug;Missing-data;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6904,b'Panel pct_change bug',"b'```\r\n  File ""z:\\code\\m.py"", line 420, in get_panel\r\n    self.source_panel.pct_change()\r\n  File ""c:\\anaconda\\lib\\site-packages\\pandas\\core\\generic.py"", line 3418, in pct_change\r\n    rs = data / data.shift(periods=periods, freq=freq, **kwds) - 1\r\nTypeError: shift() got an unexpected keyword argument \'periods\'\r\n```\r\n\r\n> pandas.version.version\r\nOut[5]: \'0.13.1\''"
6889,31605337,mcwitt,jreback,2014-04-16 01:16:37,2014-06-16 17:30:28,2014-04-23 22:23:11,closed,,0.14.0,32,API Design;Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/6889,b'BUG/ENH: Add fallback warnings and correctly handle leading whitespace in C parser',"b""closes #6607\r\ncloses #3374\r\n\r\nCurrently, specifying options that are incompatible with the C parser in `read_csv` and `read_table` causes a silent fallback to the python engine. This can be confusing if the user has also passed options that are only supported by the C engine, which are then silently ignored. (See #6607)\r\n\r\nFor example, the commonly used option `sep='\\s+'` causes a fallback to python which could be avoided by automatically translating this to the equivalent `delim_whitespace=True`, which is supported by the C engine.\r\n\r\nThere are some issues with the C parser that need to be fixed in order not to break tests with `sep='\\s+'` which previously fell back to python:\r\n\r\nThe C parser does not correctly handle leading whitespace with `delim_whitespace=True` (#3374).\r\n\r\nThere is a related bug when parsing files with \\r-delimited lines and missing values:\r\n\r\n```python\r\nIn [5]: data = 'a b c\\r2 3\\r4 5 6'\r\n\r\nIn [6]: pd.read_table(StringIO(data), delim_whitespace=True)\r\nOut[6]: \r\n    a  b  c\r\n0   2  3  4\r\n1 NaN  5  6\r\n\r\n[2 rows x 3 columns]\r\n```\r\n\r\n# Summary of changes\r\n\r\n- **Raise `ValueError` when user specifies `engine='c'` with C-unsupported options:**\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: from pandas.compat import StringIO\r\n\r\nIn [3]: data = ' a\\tb c\\n 1\\t2 3\\n 4\\t5 6'\r\n\r\nIn [4]: pd.read_table(StringIO(data), engine='c', sep='\\s')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n. . .\r\nValueError: the 'c' engine does not support regex separators\r\n```\r\n\r\n- **Raise `ValueError` when fallback to python parser causes python-unsupported options to be ignored:**\r\n```python\r\nIn [5]: pd.read_table(StringIO(data), sep='\\s', dtype={'a': float})\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n. . .\r\nValueError: Falling back to the 'python' engine because the 'c' engine does not support regex separators, but this causes 'dtype' to be ignored as it is not supported by the 'python' engine. (Note the 'converters' option provides similar functionality.)\r\n```\r\n\r\n- **Warn (new class ParserWarning) when C-unsupported options cause a fallback to python:**\r\n```python\r\nIn [6]: pd.read_table(StringIO(data), skip_footer=1)\r\npandas/io/parsers.py:615: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skip_footer; you can avoid this warning by specifying engine='python'.\r\n  ParserWarning)\r\nOut[6]: \r\n    a  b c\r\n0   1  2 3\r\n\r\n[1 rows x 2 columns]\r\n```\r\n\r\n- **Raise ValueError when the user specifies both `sep` and `delim_whitespace=True`:**\r\n```python\r\nIn [7]: pd.read_table(StringIO(data), sep='\\s', delim_whitespace=True)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n. . .\r\nValueError: Specified a delimiter with both sep and delim_whitespace=True; you can only specify one.\r\n```\r\n\r\n- **Translate `sep='\\s+'` to `delim_whitespace=True` when there are no other C-unsupported options:**\r\n```python\r\nIn [8]: pd.read_table(StringIO(data), sep='\\s+', engine='c')\r\nOut[8]: \r\n   a  b  c\r\n0  1  2  3\r\n1  4  5  6\r\n\r\n[2 rows x 3 columns]\r\n```\r\n\r\n- **Fix handling of leading whitespace in the C parser (#3374) (add test)**\r\n```python\r\n# Old behavior\r\nIn [3]: pd.__version__\r\nOut[3]: '0.13.1-663-g21565a3'\r\n\r\nIn [4]: pd.read_table(StringIO(data), delim_whitespace=True)\r\nOut[4]: \r\n   Unnamed: 0  a  b   c\r\n0           1  2  3 NaN\r\n1           4  5  6 NaN\r\n\r\n[2 rows x 4 columns]\r\n```\r\n```python\r\n# New behavior\r\nIn [9]: pd.read_table(StringIO(data), delim_whitespace=True)\r\nOut[9]: \r\n   a  b  c\r\n0  1  2  3\r\n1  4  5  6\r\n\r\n[2 rows x 3 columns]\r\n```\r\n\r\n- **Fix bug in handling of \\r-delimited files (add test)**\r\n(Old behavior shown above)\r\n```python\r\n# New behavior\r\nIn [10]: data = 'a b c\\r2 3\\r4 5 6'\r\n\r\nIn [11]: pd.read_table(StringIO(data), delim_whitespace=True)\r\nOut[11]: \r\n   a  b   c\r\n0  2  3 NaN\r\n1  4  5   6\r\n\r\n[2 rows x 3 columns]\r\n```\r\n- **Copy tests in `ParserTests` that fall back to python to `TestPythonParser`; leave copies of these tests in `ParserTests` with the assertion that they raise a `ValueError` when run under other engines**\r\n\r\n- **Add description of `engine` option to docstrings of `read_table` and `read_csv`**"""
6888,31602589,cpcloud,cpcloud,2014-04-16 00:11:03,2014-07-16 09:01:58,2014-04-16 01:22:38,closed,cpcloud,0.14.0,1,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6888,b'BUG: properly rename single group match in Series.str.extract()',
6886,31568557,nspies,jreback,2014-04-15 16:24:16,2014-09-24 12:59:05,2014-04-23 23:24:14,closed,,0.14.0,17,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/6886,"b""Series.rank() doesn't handle small floats correctly""","b'Okay, this fixes #6868 but does so with a bit of a performance penalty. The current pandas version performs just a tad slower than `scipy.stats.rankdata()`, but after these changes, it\'s about 2-3x slower. On the plus side, it does (what I think is) the right thing.\r\n\r\nI\'m no cython expert, so there may well be things that can be done to improve the speed.\r\n\r\nHere\'s some quick benchmarking code (couldn\'t figure out how to work with the vbench suite):\r\n\r\n```\r\nimport timeit\r\nsetup2 = """"""import pandas\r\nimport scipy.stats\r\nimport numpy\r\nnumpy.random.seed(154)\r\ns = pandas.Series(numpy.random.normal(size=10000))""""""\r\n\r\nprint ""pandas:"", timeit.repeat(stmt=\'s.rank()\', setup=setup2, repeat=3, number=10000)\r\nprint ""scipy:"", timeit.repeat(stmt=\'scipy.stats.rankdata(s)\', \r\n    setup=setup2, repeat=3, number=10000)\r\n```'"
6885,31537239,maxgrenderjones,jreback,2014-04-15 09:12:43,2014-04-30 12:40:32,2014-04-30 12:40:32,closed,,0.14.0,12,Bug;Strings;Unicode,https://api.github.com/repos/pydata/pandas/issues/6885,b'get_dummies chokes on unicode values',"b""(Context: `pandas version 0.13.1 running on 2.7.6 |Anaconda 1.9.1 (64-bit)| (default, Nov 11 2013, 10:49:15) [MSC v.1500 64 bit (AMD64)]`)\r\n\r\nIn my code I have a category containing lots of non-English names and want to create dummies out of it.\r\n\r\nSo I call: \r\n```python\r\ndummies=pandas.get_dummies(data[cat], prefix=prefix)\r\n```\r\nand get:\r\n```\r\nc:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape.pyc in get_dummies(data, prefix, prefix_sep, dummy_na)\r\n    971     if prefix is not None:\r\n    972         dummy_cols = ['%s%s%s' % (prefix, prefix_sep, str(v))\r\n--> 973                       for v in levels]\r\n    974     else:\r\n    975         dummy_cols = levels\r\n\r\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xe9' in position 19: ordinal not in range(128)\r\n```\r\n\r\nIssue would appear to be the call to `str(v)` - if `v` is a unicode string with non-ascii, this is liable to explode.\r\n"""
6873,31369999,sinhrks,jreback,2014-04-11 23:28:59,2014-06-24 10:25:33,2014-04-13 07:14:55,closed,,0.14.0,5,Bug;Missing-data;Timedelta;Timezones,https://api.github.com/repos/pydata/pandas/issues/6873,"b'BUG: Arithmetic, timezone and offsets operations affecting to NaT'","b""NaT affected by some datetime related ops unexpectedly.\r\n\r\n# Arithmetic\r\nApplying arithmetic ops to `NaT` is not handled properly. Based on numpy results, I understand that results should be all `NaT` as long as valid data is passed.\r\n\r\n```\r\n# current results\r\n>>> pd.NaT + pd.offsets.Hour(1)\r\n2262-04-11 01:12:43.145224192\r\n>>> pd.NaT - pd.offsets.Hour(1))\r\nOverflowError: Python int too large to convert to C long\r\n>>> pd.NaT - pd.Timestamp('2011-01-01')\r\n-734779 days, 0:00:00\r\n\r\n# numpy\r\n>>> np.datetime64('nat') + np.timedelta64(1, 'h')\r\nNaT\r\n>>> np.datetime64('nat') - np.timedelta64(1, 'h')\r\nNaT\r\n>>> np.datetime64('nat') - np.datetime64('2011-01-01')\r\nNaT\r\n```\r\n\r\n# Timezone\r\nCloses #5546. \r\n```\r\n# current results\r\n>>> idx = pd.DatetimeIndex(['2011-01-01 00:00', '2011-01-02 00:00', pd.NaT])\r\n>>> idx.tz_localize('Asia/Tokyo')\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2011-01-01 00:00:00+09:00, ..., 2262-04-10 00:12:43.145224192+09:00]\r\nLength: 3, Freq: None, Timezone: Asia/Tokyo\r\n\r\n>>> idx = pd.DatetimeIndex(['2011-01-01 00:00', '2011-01-02 00:00', pd.NaT], tz='US/Eastern')\r\n>>> idx.tz_convert('Asia/Tokyo')\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2011-01-01 14:00:00+09:00, ..., 2262-04-11 14:12:43.145224192+09:00]\r\n```\r\n*Note* I fixed `DatetimeIndex`, and I leave `NatType` still doesn't have `tz_localize` and `tz_convert` methods `Timestamp` has. Is it should be added?\r\n\r\n# Offsets\r\nThese have `apply` method which accepts `Timestamp`, but it cannot handle `Nat`.\r\n\r\n```\r\n# current result\r\n>>> pd.offsets.Hour(1).apply(pd.NaT)\r\n2262-04-11 01:12:43.145224192\r\n```\r\n\r\n"""
6872,31353948,jreback,jreback,2014-04-11 19:08:27,2014-06-27 15:47:52,2014-04-11 22:33:18,closed,,0.14.0,1,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6872,"b""BUG: Bug in groupby.get_group where a datetlike wasn't always accepted (GH5267)""",b'closes #5267'
6868,31293222,nspies,jreback,2014-04-11 00:09:10,2014-04-23 23:24:50,2014-04-23 23:24:50,closed,,0.14.0,6,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/6868,"b""Series.rank() doesn't handle small floats correctly""","b'Floats below 1e-10 seem to all be receiving the same rank, incorrectly:\r\n\r\n```\r\nIn [1]: import pandas\r\n\r\nIn [3]: import numpy\r\n\r\nIn [4]: series = pandas.Series([1e-100, 1e-25, 1e-20, 1e-15, 1e-10, \r\n                                1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\r\n\r\nIn [5]: series\r\nOut[5]: \r\n0    1.000000e-100\r\n1     1.000000e-25\r\n2     1.000000e-20\r\n3     1.000000e-15\r\n4     1.000000e-10\r\n5     1.000000e-05\r\n6     1.000000e-04\r\n7     1.000000e-03\r\n8     1.000000e-02\r\n9     1.000000e-01\r\ndtype: float64\r\n\r\nIn [6]: series.rank()\r\nOut[6]: \r\n0     2.5\r\n1     2.5\r\n2     2.5\r\n3     2.5\r\n4     5.0\r\n5     6.0\r\n6     7.0\r\n7     8.0\r\n8     9.0\r\n9    10.0\r\ndtype: float64\r\n\r\nIn [7]: from scipy import stats\r\n\r\nIn [8]: stats.rankdata(series)\r\nOut[8]: array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])\r\n\r\nIn [13]: pandas.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 10.8.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.1\r\nCython: 0.19.1\r\nnumpy: 1.8.0\r\nscipy: 0.12.0.dev-1d5c886\r\nstatsmodels: 0.5.0\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: 0.2.0\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2013b\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.2.0\r\nopenpyxl: 1.5.7\r\nxlrd: 0.7.1\r\nxlwt: None\r\nxlsxwriter: None\r\nsqlalchemy: 0.6.6\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\n```'"
6863,31254594,unutbu,jreback,2014-04-10 15:41:18,2014-06-12 16:16:48,2014-04-10 21:01:17,closed,,0.14.0,14,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/6863,b'BUG: _tidy_repr should not be called when max_rows is None',"b""This issue was raised in http://stackoverflow.com/q/22824104/190597:\r\n\r\n```\r\nimport pandas as pd\r\npd.options.display.max_rows = None\r\nresult = pd.Series(range(1001))\r\nprint(result)\r\n```\r\n\r\nraises `TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'`.\r\n\r\nThe problem occurs in series.py  (line 832) when `max_rows` is `None`:\r\n\r\n        if len(self.index) > (max_rows or 1000):\r\n            result = self._tidy_repr(min(30, max_rows - 4))\r\n\r\nSince the doc string for `get_options` says\r\n\r\n```\r\ndisplay.max_rows: [default: 60] [currently: 60]\r\n        ...\r\n        'None' value means unlimited.\r\n```\r\n\r\nI think `_tidy_repr` should not be called when `max_rows` is None.\r\n\r\nThis PR seems simple enough but my main concern is that  this PR stomps on GH1467 which explicitly changed `> max_rows` to `> (max_rows or 1000)` and I don't know what the purpose of this was."""
6858,31235426,jreback,jreback,2014-04-10 11:49:28,2014-06-29 14:30:56,2014-04-10 12:08:34,closed,,0.14.0,0,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/6858,b'BUG: to_timedelta not properly converting some units (GH6855)',b'closes #6855'
6846,31147414,Acanthostega,jorisvandenbossche,2014-04-09 10:59:37,2014-04-13 19:39:44,2014-04-13 19:39:44,closed,,0.14.0,18,Bug;IO SQL;Regression,https://api.github.com/repos/pydata/pandas/issues/6846,b'BUG: data written with to_sql legacy mode (sqlite/mysql) not persistent',"b'UPDATE: this issue seemed actually to be a bug in the new legacy code so all databases written were only written in memory and not really committed to the database itself (see discussion below).\r\n\r\n---\r\n\r\nHi everybody,\r\n\r\nI still have a problem with writing data into a SQL database. With the following example, the resulting database file isn\'t written, but the structure of the table is created (I assume a pylab environment set with ipython...):\r\n\r\n```python\r\n\r\n>>> import pandas as pd\r\n>>> import sqlite3\r\n\r\n>>> data = pd.DataFrame({""galid"":randint(2**63-1, size=100), ""objid"": randint(2**63-1, size=100), ""alpha"": rand(100)})\r\n\r\n>>> conn = sqlite3.connect(""/tmp/bidulechouette.db"")\r\n\r\n>>> data.to_sql(""DATA"", conn, if_exists=""replace"", flavor=""sqlite"", index=False)\r\n\r\n>>> # just to be sure...\r\n>>> conn.close()\r\n>>> conn = sqlite3.connect(""/tmp/bidulechouette.db"")\r\n\r\n>>> result = pd.read_sql(""SELECT objid FROM DATA;"", conn)\r\n>>> len(result)\r\n0\r\n\r\n```\r\n\r\nI tried it on two different systems with different versions of sqlite3  and python(3.2 and 3.4).\r\n\r\nIf I kill ipython and redo the same without the if_exists option on the same database file, it complains that the table already exists, even if I manually remove the database file of sqlite3. This lets me suppose, that somewhere, a reference to the database is kept, but it\'s weird because ipython is killed... Or the file in which it write isn\'t the good one, since with a lot of data, it takes a long time as it is writing the data somewhere, explaining the problem of existing table in a deleted database...\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.8-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.utf8\r\n\r\npandas: 0.13.1-605-g61ea0a3\r\nCython: 0.20.1\r\nnumpy: 1.8.1\r\nscipy: 0.13.3\r\nstatsmodels: None\r\nIPython: 2.0.0\r\nsphinx: 1.2.2\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.2\r\nbottleneck: None\r\ntables: 3.1.0\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None'"
6845,31123951,jsexauer,jreback,2014-04-09 01:47:22,2014-06-27 21:48:09,2014-04-22 13:09:37,closed,,0.14.0,14,API Design;Bug;Frequency;Reshaping;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6845,b'BUG/ENH: Add how kwarg to rolling_* functions [fix #6297]',"b'Fixes #6297\r\n\r\nFigured while I was in the area, would submit a PR to do this too.  Let me know if you want to see more unit tests than the one.'"
6824,30942583,jreback,jreback,2014-04-06 17:59:24,2014-06-25 21:09:37,2014-04-06 18:16:27,closed,,0.14.0,0,Bug;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6824,b'BUG: Regression from 0.13 with fillna and a Series on datetime-like (6344)',b'closes #6344'
6821,30941296,jreback,jreback,2014-04-06 16:58:07,2014-06-30 14:04:14,2014-04-06 17:35:50,closed,,0.14.0,0,Bug;Compat;IO HDF5;Strings,https://api.github.com/repos/pydata/pandas/issues/6821,b'BUG: Fix unconverting of long strings from HDF (GH6166)',b'superseeds #6166'
6820,30940812,cpcloud,cpcloud,2014-04-06 16:31:13,2014-07-16 09:00:43,2014-04-08 22:55:12,closed,cpcloud,0.14.0,4,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6820,b'BUG: fix replace bug where different dtypes in a nested dict would only replace the first value',b'closes #6689'
6818,30938039,sinhrks,jreback,2014-04-06 13:59:40,2014-06-26 02:50:17,2014-04-07 12:37:33,closed,,0.14.0,2,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/6818,b'BUG: adding np.timedelta64 to DatetimeIndex with tz outputs incorrect',"b""```\r\n>>> idx = pd.date_range(start='2010-11-02 01:00:00', periods=3, tz='US/Pacific', freq='1H')\r\n\r\n>>> idx + offsets.Hour(3)\r\n[2010-11-02 04:00:00-07:00, ..., 2010-11-02 06:00:00-07:00]\r\nLength: 3, Freq: H, Timezone: US/Pacific\r\n\r\n>>> idx + datetime.timedelta(hours=3)\r\n[2010-11-02 04:00:00-07:00, ..., 2010-11-02 06:00:00-07:00]\r\nLength: 3, Freq: H, Timezone: US/Pacific\r\n\r\n>>> idx + np.timedelta64(3, 'h')          # incorrect\r\n[2010-11-02 11:00:00-07:00, ..., 2010-11-02 13:00:00-07:00]\r\nLength: 3, Freq: H, Timezone: US/Pacific\r\n```"""
6814,30927608,dalejung,jreback,2014-04-06 00:45:11,2014-06-23 13:27:19,2014-04-06 18:13:51,closed,,0.14.0,8,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/6814,b'BUG: DataFrame._reduce was converting integers to strings in mixed-type case.',b'closes #6806'
6811,30917085,jreback,jreback,2014-04-05 14:52:26,2014-06-30 14:04:14,2014-04-05 15:23:20,closed,,0.14.0,0,Bug;Timedelta;Windows,https://api.github.com/repos/pydata/pandas/issues/6811,b'BUG: bug in timedelta ops on 32-bit platforms (GH6808)',b'closes #6808'
6808,30915399,yosuah,jreback,2014-04-05 13:22:04,2014-04-06 00:45:11,2014-04-05 15:24:51,closed,,0.14.0,6,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/6808,b'Bug when adding a timedelta to a datetime',"b""Hello,\r\n\r\nI wanted to add a specified offset (10 seconds) to and existing datetime (an index previously created by pd.to_datetime), so I thought creating a timedelta and adding it would do the trick. As you can see it doesn't yield an error but produces a rather strange results.\r\n```\r\n@buddha[J:T26]|5> base = pd.to_datetime(datetime.datetime.now())\r\n@buddha[J:T26]|6> offset = pd.to_timedelta(10, unit='s')\r\n@buddha[J:T26]|7> offset\r\n              <7> numpy.timedelta64(10000000000,'ns')\r\n@buddha[J:T26]|8> base\r\n              <8> Timestamp('2014-04-05 13:13:40.374000', tz=None)\r\n@buddha[J:T26]|9> base + offset\r\n              <9> Timestamp('2014-04-05 13:13:41.784065408', tz=None)\r\n```\r\n\r\nIf I try to do the same by creating a datetime.timedelta then everything works as expected.\r\n```\r\n@buddha[J:T26]|10> offset2 = datetime.timedelta(seconds = 10)\r\n@buddha[J:T26]|11> base + offset2\r\n              <11> Timestamp('2014-04-05 13:13:50.374000', tz=None)\r\n```\r\n\r\nThis is really confusing and hard to spot. I think that pd.to_datetime and pd.to_timedelta should be compatible, but if they are not, please make the error easier to spot.\r\n\r\nThanks,\r\nAdam"""
6806,30902064,dalejung,jreback,2014-04-04 23:17:45,2014-04-06 18:13:51,2014-04-06 18:13:51,closed,,0.14.0,3,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/6806,b'_reduce promotes numerics to strings when axis=0 and mixed type.',"b""```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({'num':[1,2,3], 'num2': np.random.randn(3), 'strings':list('abc')})\r\ndf.sum().values # array(['6', '-1.47818918058', 'abc'], dtype=object)\r\n```\r\n\r\nThe issue is that axis=0 tries to coerce the dtypes and `np.array` will promote the numerics to strings. \r\n\r\nNote that:\r\n\r\n```python\r\ndf.T.sum(axis=1).values # array([6, -1.4781891805780178, 'abc'], dtype=object)\r\n```\r\n\r\nMaybe `com._coerce_to_dtypes` should just return the list?"""
6803,30889403,cpcloud,cpcloud,2014-04-04 19:51:55,2014-06-21 09:55:59,2014-04-05 20:18:58,closed,,0.14.0,8,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/6803,b'BUG/API: disallow boolean arithmetic operations',b'closes #6762'
6802,30886352,PKEuS,jreback,2014-04-04 19:09:34,2014-06-21 14:30:35,2014-04-06 18:32:34,closed,,0.14.0,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/6802,b'StataWriter: Replace missing values in string columns by an empty string',b'Otherwise writing fails with errors about len() applied to a float or NoneType.'
6799,30862921,jreback,jreback,2014-04-04 14:03:09,2014-06-16 05:11:38,2014-04-04 14:42:33,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6799,b'BUG: duplicate (getitem) indexing with iloc (GH6766)',b'fix the getitem issues for iloc on #6766\r\n\r\n'
6797,30842365,markrichardson,jreback,2014-04-04 08:31:43,2015-11-08 19:46:21,2015-11-08 19:45:26,closed,,0.17.1,2,Bug;Enhancement;MultiIndex;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/6797,b'to_csv date format flag not behaving as expected with multiindex',"b""Using pandas 0.13.1 ...\r\n\r\nWhen df is a 2-level datetime indexed dataframe, to_csv prints 00:00 following the dates, even when you specify date_format = '%Y-%m-%d'.\r\n\r\ndf.to_csv(date_format = '%Y-%m-%d')\r\n\r\nTo get around this, I'm currently having to use reset_index; then to_csv works as expected. \r\n\r\n```\r\nIn [4]: df = DataFrame(np.random.randn(6,2),index=MultiIndex.from_product([['A','B'],date_range('20130101',periods=3)]))\r\n\r\nIn [5]: df\r\nOut[5]: \r\n                     0         1\r\nA 2013-01-01 -0.180284 -0.632280\r\n  2013-01-02  2.986923 -0.422540\r\n  2013-01-03 -1.435938  1.700168\r\nB 2013-01-01 -0.163560 -1.047000\r\n  2013-01-02  1.852092 -0.552963\r\n  2013-01-03  1.375277  0.737837\r\n\r\n[6 rows x 2 columns]\r\n\r\nIn [6]: df.to_csv('test.csv',mode='w')\r\n\r\nIn [7]: !cat test.csv\r\n,,0,1\r\nA,2013-01-01 00:00:00,-0.1802842406155066,-0.6322803999738553\r\nA,2013-01-02 00:00:00,2.9869225314020604,-0.42253974812704775\r\nA,2013-01-03 00:00:00,-1.435937967237287,1.700167741194332\r\nB,2013-01-01 00:00:00,-0.1635604685556706,-1.0469998404335645\r\nB,2013-01-02 00:00:00,1.8520922142464138,-0.5529633735794578\r\nB,2013-01-03 00:00:00,1.3752768989190265,0.7378371099828236\r\n\r\nIn [8]: df.to_csv('test.csv',mode='w',date_format='%Y-%m-%d')\r\n\r\nIn [9]: !cat test.csv\r\n,,0,1\r\nA,2013-01-01 00:00:00,-0.1802842406155066,-0.6322803999738553\r\nA,2013-01-02 00:00:00,2.9869225314020604,-0.42253974812704775\r\nA,2013-01-03 00:00:00,-1.435937967237287,1.700167741194332\r\nB,2013-01-01 00:00:00,-0.1635604685556706,-1.0469998404335645\r\nB,2013-01-02 00:00:00,1.8520922142464138,-0.5529633735794578\r\nB,2013-01-03 00:00:00,1.3752768989190265,0.7378371099828236\r\n\r\nIn [10]: df.reset_index().to_csv('test.csv',mode='w',date_format='%Y-%m-%d')\r\n\r\nIn [11]: !cat test.csv\r\n,level_0,level_1,0,1\r\n0,A,2013-01-01,-0.1802842406155066,-0.6322803999738553\r\n1,A,2013-01-02,2.9869225314020604,-0.42253974812704775\r\n2,A,2013-01-03,-1.435937967237287,1.700167741194332\r\n3,B,2013-01-01,-0.1635604685556706,-1.0469998404335645\r\n4,B,2013-01-02,1.8520922142464138,-0.5529633735794578\r\n5,B,2013-01-03,1.3752768989190265,0.7378371099828236\r\n\r\n```"""
6790,30816932,jreback,jreback,2014-04-03 21:46:40,2014-07-16 09:00:21,2014-04-03 22:11:49,closed,,0.14.0,0,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6790,b'BUG: bug in taking all on a multi-index when only level 0 is specified (GH6788)',b'closes #6788'
6788,30812911,rdooley,jreback,2014-04-03 20:55:05,2014-04-03 22:18:18,2014-04-03 22:11:49,closed,,0.14.0,10,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6788,b'BUG: Slicing multi Index with one column by column errors out',"b'The following code errors out. Seems like it should just return the original dataframe.\r\npandas: 0.13.1-552-g8120a59\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom itertools import product\r\n \r\nattributes = [\'Attribute\' + str(i) for i in range(1)]\r\nattribute_values = [\'Value\' + str(i) for i in range(1000)]\r\n \r\nindex = pd.MultiIndex.from_tuples(list(product(attributes, attribute_values)))\r\ndf = 0.1 * np.random.randn(10, 1 * 1000) + 0.5\r\ndf = pd.DataFrame(df, columns=index)\r\n \r\ndf[attributes]\r\n```\r\n\r\nStacktrace\r\n```\r\n(pandas)pandas git:master  python example_failure.py                                                                                                                                                                                \r\nTraceback (most recent call last):\r\n  File ""example_failure.py"", line 12, in <module>\r\n    df[attributes]\r\n  File ""PATH/pandas/pandas/core/frame.py"", line 1672, in __getitem__\r\n    return self._getitem_array(key)\r\n  File ""PATH/pandas/pandas/core/frame.py"", line 1717, in _getitem_array\r\n    return self.take(indexer, axis=1, convert=True)\r\n  File ""PATH/pandas/pandas/core/generic.py"", line 1224, in take\r\n    indices, len(self._get_axis(axis)))\r\n  File ""PATH/pandas/pandas/core/indexing.py"", line 1564, in _maybe_convert_indices\r\n    if mask.any():\r\nAttributeError: \'bool\' object has no attribute \'any\'\r\n```'"
6786,30809691,jreback,jreback,2014-04-03 20:17:23,2014-06-30 14:03:57,2014-04-03 21:07:11,closed,,0.14.0,0,Bug;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/6786,b'BUG: Bug in setting a tz-aware index directly via .index (GH6785)',b'closes #6785'
6785,30803675,kramimus,jreback,2014-04-03 19:03:24,2014-04-03 21:07:22,2014-04-03 21:07:11,closed,,0.14.0,4,Bug;Indexing;Timezones,https://api.github.com/repos/pydata/pandas/issues/6785,b'Inconsistent behavior when setting DataFrame index to datetime Series',"b'Starting with a DataFrame like this:\r\n```\r\ndf = pandas.DataFrame([{\'ts\':datetime(2014, 4, 1, tzinfo=pytz.utc), \'foo\':1}])\r\n```\r\nThis works fine:\r\n```\r\ndf = df.set_index(\'ts\')\r\nprint(df)\r\n```\r\n```\r\n                           foo\r\nts                            \r\n2014-04-01 00:00:00+00:00    1\r\n```\r\n\r\n\r\nThis yields an AttributeError:\r\n```\r\ndf.index = df[\'ts\']\r\nprint(df)\r\n```\r\n\r\n```\r\nIn [75]: print(df)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-75-1152f648e0af> in <module>()\r\n----> 1 print(df)\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/base.pyc in __str__(self)\r\n     33         if compat.PY3:\r\n     34             return self.__unicode__()\r\n---> 35         return self.__bytes__()\r\n     36 \r\n     37     def __bytes__(self):\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/base.pyc in __bytes__(self)\r\n     45 \r\n     46         encoding = get_option(""display.encoding"")\r\n---> 47         return self.__unicode__().encode(encoding, \'replace\')\r\n     48 \r\n     49     def __repr__(self):\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/frame.pyc in __unicode__(self)\r\n    458             width = None\r\n    459         self.to_string(buf=buf, max_rows=max_rows, max_cols=max_cols,\r\n--> 460                        line_width=width, show_dimensions=show_dimensions)\r\n    461 \r\n    462         return buf.getvalue()\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/frame.pyc in to_string(self, buf, columns, col_space, colSpace, header, index, na_rep, formatters, float_format, sparsify, nanRep, index_names, justify, force_unicode, line_width, max_rows, max_cols, show_dimensions)\r\n   1297                                            max_cols=max_cols,\r\n   1298                                            show_dimensions=show_dimensions)\r\n-> 1299         formatter.to_string()\r\n   1300 \r\n   1301         if buf is None:\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/format.pyc in to_string(self, force_unicode)\r\n    378             text = info_line\r\n    379         else:\r\n--> 380             strcols = self._to_str_columns()\r\n    381             if self.line_width is None:\r\n    382                 text = adjoin(1, *strcols)\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/format.pyc in _to_str_columns(self)\r\n    310 \r\n    311         # may include levels names also\r\n--> 312         str_index = self._get_formatted_index()\r\n    313         str_columns = self._get_formatted_column_labels()\r\n    314 \r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/format.pyc in _get_formatted_index(self)\r\n    583                                      formatter=fmt)\r\n    584         else:\r\n--> 585             fmt_index = [index.format(name=show_index_names, formatter=fmt)]\r\n    586 \r\n    587         adjoined = adjoin(1, *fmt_index).split(\'\\n\')\r\n\r\n/home/sucram/.virtualenvs/ehubdata/local/lib/python2.7/site-packages/pandas/core/generic.pyc in __getattr__(self, name)\r\n   1813                 return self[name]\r\n   1814             raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\r\n-> 1815                                  (type(self).__name__, name))\r\n   1816 \r\n   1817     def __setattr__(self, name, value):\r\n\r\nAttributeError: \'Series\' object has no attribute \'format\'\r\n```\r\n'"
6782,30794875,arthurgerigk-rocket,jtratner,2014-04-03 17:17:05,2014-04-05 18:06:13,2014-04-05 18:06:13,closed,jtratner,0.16.0,8,Bug;Dtypes;IO Excel,https://api.github.com/repos/pydata/pandas/issues/6782,b'engine xlsxwriter fails for np.inf',"b'this works for openpyxl but it fails for xlsxwriter\r\n\r\nTypeError(""NAN/INF not supported in write_number()"")'"
6778,30782212,cpcloud,cpcloud,2014-04-03 14:55:05,2014-07-16 09:00:14,2014-04-04 20:56:16,closed,,0.14.0,6,API Design;Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6778,b'BUG: fix metacharacter replacement bug in DataFrame.replace()',b'closes #6777'
6777,30772540,yosuah,cpcloud,2014-04-03 12:57:06,2014-04-05 10:13:32,2014-04-04 20:56:16,closed,cpcloud,0.14.0,2,API Design;Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6777,b'Ambiguity in DataFrame.replace regex handling',"b""Dear developers,\r\n\r\nI think that it is confusing how DataFrame.replace interprets the to_replace values as a pure string or a regexp. \r\n\r\nWhen you pass a dictionary of from and to values, these are interpreted as pure string literals and work as expected. If you pass a nested dictionary with the same values, it is interpreted as a regex even if regex = False is specifically added. This causes a problem if you want to replace values that have special characters in them.\r\n\r\nLet's see an example:\r\n```python\r\n\r\n@buddha[J:T26]|19> df = pd.DataFrame({'a' : ['()', 'something else']})\r\n@buddha[J:T26]|20> df\r\n              <20>\r\n                a\r\n0              ()\r\n1  something else\r\n\r\n[2 rows x 1 columns]\r\n@buddha[J:T26]|21> df.replace({'()' : 'parantheses'})\r\n              <21>\r\n                a\r\n0     parantheses\r\n1  something else\r\n\r\n[2 rows x 1 columns]\r\n@buddha[J:T26]|22> df.replace({'a' : {'()' : 'parantheses'}})\r\n              <22>\r\n                                                   a\r\n0                parantheses(parantheses)parantheses\r\n1  paranthesessparanthesesoparanthesesmparanthese...\r\n\r\n[2 rows x 1 columns]\r\n```\r\n\r\nAs you can see, in the first case the () got replaced as expected. In the second case, even though the only thing I changed was to specify the column in which the replace should occur, the parentheses got treated as a regex. The same happens if I add regex = False to the options.\r\n\r\nThe current workaround for me is to make sure that the from/to values are regexes.\r\n\r\nCheers,\r\nAdam\r\n\r\nPs.: Pandas is awesome, thanks a lot for all the effort!\r\n\r\n@buddha[J:T26]|24> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: x86\r\nprocessor: x86 Family 6 Model 23 Stepping 10, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.13.1\r\nCython: None\r\nnumpy: 1.8.0\r\nscipy: 0.13.3\r\nstatsmodels: 0.5.0\r\nIPython: 1.2.0\r\nsphinx: 1.2.1\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2013.9\r\nbottleneck: 0.8.0\r\ntables: 3.1.0\r\nnumexpr: 2.3\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nsqlalchemy: 0.8.4\r\nlxml: 3.3.1\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nbq: None\r\napiclient: None"""
6769,30703253,fonnesbeck,jreback,2014-04-02 16:27:48,2014-05-16 20:15:53,2014-05-16 20:15:53,closed,,0.14.1,5,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6769,b'groupby boxplot creates more subplots than are needed',"b""When calling `boxplot` on a `DataFrameGroupBy` object, it does not seem to be able to properly infer the number of subplots required to account for the number of groups. For example, the following:\r\n\r\n    lsl_dr[lsl_dr.test_type==t].groupby('type').boxplot(column='score', grid=False)\r\n\r\ngenerates this plot:\r\n\r\n![bad boxplot](http://d.pr/i/YReI+)\r\n\r\nNotice that it generates a 2x2 grid, even though there are only three groups. I thought I could deal with this manually by calling `pyplot.subplots(1,3)`, but the `boxplot` method does not appear to be able to accept an axis argument like the `DataFrame.boxplot` method does. Is there a work-around for this?\r\n"""
6766,30693538,bergtholdt,jreback,2014-04-02 14:41:17,2014-04-30 01:05:17,2014-04-30 01:05:17,closed,,0.14.0,13,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6766,b'BUG: iloc can create columns',"b""After a concat of two DataFrames with the same columns. I want to consolidate some data and remove NaNs in some columns by values in other columns. I ended up with a DataFrame that magically had additional columns.\r\n\r\nThis is the minimum example that I can give to reproduce the faulty behaviour using current master (70de129):\r\n\r\n```\r\ndf1 = pd.DataFrame([{'A':None, 'B':1},{'A':2, 'B':2}])\r\ndf2 = pd.DataFrame([{'A':3, 'B':3},{'A':4, 'B':4}])\r\ndf = pd.concat([df1, df2], axis=1)\r\n```\r\n\r\n\r\n```\r\n>>> df1\r\n    A  B\r\n0 NaN  1\r\n1   2  2\r\n\r\n[2 rows x 2 columns]\r\n\r\n>>> df2\r\n   A  B\r\n0  3  3\r\n1  4  4\r\n\r\n[2 rows x 2 columns]\r\n\r\n>>> df\r\n    A  B  A  B\r\n0 NaN  1  3  3\r\n1   2  2  4  4\r\n\r\n[2 rows x 4 columns]\r\n```\r\n\r\nNow replacing NaNs in the 0 column with (corresponding) values in the 2 column ('A'), I expected to simply write a 3 into NaN (which it did), but it actually added a column '0' at the end of the DataFrame even though iloc is not supposed to enlarge the dataset. Clearly a bug.\r\n```\r\ninds = np.isnan(df.iloc[:, 0])\r\ndf.iloc[:, 0][inds] = df.iloc[:, 2][inds]\r\n```\r\n\r\n```\r\n>>> df\r\n   A  B  A  B  0\r\n0  3  1  3  3  3\r\n1  2  2  4  4  2\r\n\r\n[2 rows x 5 columns]\r\n```"""
6764,30683146,jreback,jreback,2014-04-02 12:29:16,2014-04-19 12:47:08,2014-04-19 12:47:08,closed,,0.14.0,2,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6764,b'BUG: multiple grouping with a TimeGrouper requires sort',"b""resampling has been fixed, so this is only with 2 or more groupers (#6516)\r\n\r\n```\r\nIn [9]: \r\ndf = DataFrame({\r\n            'date' : pd.to_datetime([\r\n                '20121002','20121007','20130130','20130202','20130305','20121002',\r\n                '20121207','20130130','20130202','20130305','20130202','20130305']),\r\n            'user_id' : [1,1,1,1,1,3,3,3,5,5,5,5],\r\n            'whole_cost' : [1790,364,280,259,201,623,90,312,359,301,359,801],\r\n            'cost1' : [12,15,10,24,39,1,0,90,45,34,1,12] }).set_index('date')\r\n\r\n        expected = df.groupby('user_id')['whole_cost'].resample(\r\n            'M', how='sum').dropna().reorder_levels(['date','user_id']).sortlevel().astype('int64')\r\n        expected.name = 'whole_cost'\r\n\r\nIn [10]: expected\r\nOut[10]: \r\ndate        user_id\r\n2012-10-31  1          2154\r\n            3           623\r\n2012-12-31  3            90\r\n2013-01-31  1           280\r\n            3           312\r\n2013-02-28  1           259\r\n            5           718\r\n2013-03-31  1           201\r\n            5          1102\r\nName: whole_cost, dtype: int64\r\n```\r\n\r\nThese should be equivalent\r\n```\r\nIn [11]: df.sort_index().groupby([pd.TimeGrouper(freq='M'), 'user_id'])['whole_cost'].sum()\r\nOut[11]: \r\ndate        user_id\r\n2012-10-31  1          2154\r\n            3           623\r\n2012-12-31  3            90\r\n2013-01-31  1           280\r\n            3           312\r\n2013-02-28  1           259\r\n            5           718\r\n2013-03-31  1           201\r\n            5          1102\r\nName: whole_cost, dtype: int64\r\n\r\nIn [13]: df.groupby([pd.TimeGrouper(freq='M'), 'user_id'])['whole_cost'].sum()\r\nValueError: cannot reindex from a duplicate axis\r\n```"""
6762,30636411,bluefir,cpcloud,2014-04-01 20:48:21,2014-05-22 19:53:06,2014-04-05 20:18:58,closed,,0.14.0,24,Bug;Compat;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/6762,b'numexpr 2.3.1 error with pandas 0.13.1',"b'I just installed numexpr 2.3.1 with pandas 0.13.1 and got the following error:\r\n\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\ops.py"", line 496, in wrapper\r\n    arr = na_op(lvalues, rvalues)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\ops.py"", line 443, in na_op\r\n    raise_on_error=True, **eval_kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\computation\\expressions.py"", line 176, in evaluate\r\n    **eval_kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\computation\\expressions.py"", line 104, in _evaluate_numexpr\r\n    **eval_kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\numexpr\\necompiler.py"", line 738, in evaluate\r\n    NumExpr(ex, signature, **context)\r\n  File ""C:\\Python27\\lib\\site-packages\\numexpr\\necompiler.py"", line 554, in NumExpr\r\n    precompile(ex, signature, context)\r\n  File ""C:\\Python27\\lib\\site-packages\\numexpr\\necompiler.py"", line 498, in precompile\r\n    ast = typeCompileAst(ast)\r\n  File ""C:\\Python27\\lib\\site-packages\\numexpr\\necompiler.py"", line 163, in typeCompileAst\r\n    % (ast.value + \'_\' + retsig+basesig))\r\nNotImplementedError: couldn\'t find matching opcode for \'mul_bbb\'\r\n'"
6748,30493339,immerrr,jreback,2014-03-31 08:33:07,2014-06-12 21:22:30,2014-03-31 13:33:15,closed,,0.14.0,0,Bug;Internals;Sparse,https://api.github.com/repos/pydata/pandas/issues/6748,b'BUG: fix NDFrame.as_blocks() for sparse containers',"b""SparseBlocks don't consolidate, so previous implementation silently dropped all but the last blocks for given dtype:\r\n\r\n```python\r\nIn [1]: pd.__version__\r\nOut[1]: '0.13.1-527-g73506cb'\r\n\r\nIn [2]: pd.SparseDataFrame({'a': [1,2,3, np.nan, np.nan], 'b': [1,2,3, np.nan, np.nan]})\r\nOut[2]: \r\n    a   b\r\n0   1   1\r\n1   2   2\r\n2   3   3\r\n3 NaN NaN\r\n4 NaN NaN\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [3]: _2.blocks\r\nOut[3]: \r\n{'float64':     b\r\n0   1\r\n1   2\r\n2   3\r\n3 NaN\r\n4 NaN\r\n\r\n[5 rows x 1 columns]}\r\n```\r\n\r\nwhen the last output should be:\r\n\r\n```python\r\nIn [3]: _2.blocks\r\nOut[3]: \r\n{'float64':     a   b\r\n0   1   1\r\n1   2   2\r\n2   3   3\r\n3 NaN NaN\r\n4 NaN NaN\r\n\r\n[5 rows x 2 columns]}\r\n\r\n\r\n```\r\n\r\nThis also drops the `columns` kwarg of as_blocks since it doubles reindex functionality."""
6737,30444852,jreback,jreback,2014-03-29 14:28:55,2014-07-16 08:59:47,2014-03-29 14:46:24,closed,,0.14.0,0,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/6737,b'BUG: Bug in downcasting inference with empty arrays (GH6733)',b'closes #6733'
6733,30439748,paolini,jreback,2014-03-29 08:25:42,2014-03-29 14:46:24,2014-03-29 14:46:24,closed,,0.14.0,1,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/6733,b'groupby looses dtype on empty columns',"b'In this snippet:\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    \r\n    print \'numpy\', np.__version__\r\n    print \'pandas\', pd.__version__\r\n    \r\n    df = pd.DataFrame({\'x\': [],\'range\': np.arange(0)})\r\n\r\n    t = df.sort(\'x\').groupby(\'x\').first()\r\n\r\n    print \'types\', df[\'range\'].dtype, t[\'range\'].dtype\r\n\r\nit is shown that after the groupby, an empty ""int"" column becomes an empty ""float"". This is the output on my machine:\r\n\r\n    numpy 1.6.2\r\n    pandas 0.13.1\r\n    types int32 float64\r\n'"
6731,30433140,jreback,jreback,2014-03-29 00:41:15,2014-07-16 08:59:36,2014-03-29 01:53:36,closed,,0.14.0,0,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/6731,b'BUG: Bug in resample when how=None resample freq is the same as the axis freq (GH5955)',b'closes #5955'
6730,30419815,jreback,jreback,2014-03-28 20:26:01,2014-07-16 08:59:35,2014-03-28 23:05:30,closed,,0.14.0,0,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/6730,"b'BUG: bug in BlockManager._get_numeric_data, with invalid combine'",
6722,30339157,sinhrks,jreback,2014-03-27 20:30:06,2014-07-09 08:24:18,2014-03-30 14:42:26,closed,TomAugspurger,0.14.0,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6722,b'BUG: MPLPlot cannot make loglog keyword worked',"b'Changes made in #5638 make `loglog` keyword not worked, and it fixes the problem.'"
6718,30298922,jreback,jreback,2014-03-27 13:18:58,2014-07-16 08:59:25,2014-03-28 12:34:13,closed,,0.14.0,4,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6718,b'BUG: Bug in consistency of groupby aggregation when passing a custom function (GH6715)',b'closes #6715'
6716,30291615,jreback,jreback,2014-03-27 11:23:10,2014-05-13 14:29:35,2014-05-13 14:29:35,closed,,0.14.0,0,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/6716,b'BUG: partial string indexing with PeriodIndex',"b""partial indexing works with time series, but not period index\r\n\r\n\r\n```\r\nIn [27]: df2 = DataFrame(np.arange(20).reshape(10,2),index=pd.date_range('20130101 09:00',freq='T',periods=10))\r\n\r\nIn [28]: df = DataFrame(np.arange(20).reshape(10,2),index=pd.period_range('20130101 09:00',freq='T',periods=10))\r\n\r\nIn [29]: df2['20130101']\r\nOut[29]: \r\n                      0   1\r\n2013-01-01 09:00:00   0   1\r\n2013-01-01 09:01:00   2   3\r\n2013-01-01 09:02:00   4   5\r\n2013-01-01 09:03:00   6   7\r\n2013-01-01 09:04:00   8   9\r\n2013-01-01 09:05:00  10  11\r\n2013-01-01 09:06:00  12  13\r\n2013-01-01 09:07:00  14  15\r\n2013-01-01 09:08:00  16  17\r\n2013-01-01 09:09:00  18  19\r\n\r\n[10 rows x 2 columns]\r\n\r\nIn [30]: df['20130101']\r\nKeyError: u'no item named 20130101'\r\n\r\nIn [31]: df2['20130101 09']\r\nOut[31]: \r\n                      0   1\r\n2013-01-01 09:00:00   0   1\r\n2013-01-01 09:01:00   2   3\r\n2013-01-01 09:02:00   4   5\r\n2013-01-01 09:03:00   6   7\r\n2013-01-01 09:04:00   8   9\r\n2013-01-01 09:05:00  10  11\r\n2013-01-01 09:06:00  12  13\r\n2013-01-01 09:07:00  14  15\r\n2013-01-01 09:08:00  16  17\r\n2013-01-01 09:09:00  18  19\r\n\r\n[10 rows x 2 columns]\r\n\r\nIn [32]: df['20130101 09']\r\nKeyError: u'no item named 20130101 09'\r\n```\r\n\r\nWorks fine for exact start points\r\n```\r\nIn [34]: df2['20130101 09:05':]\r\nOut[34]: \r\n                      0   1\r\n2013-01-01 09:05:00  10  11\r\n2013-01-01 09:06:00  12  13\r\n2013-01-01 09:07:00  14  15\r\n2013-01-01 09:08:00  16  17\r\n2013-01-01 09:09:00  18  19\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [35]: df['20130101 09:05':]\r\nOut[35]: \r\n                   0   1\r\n2013-01-01 09:05  10  11\r\n2013-01-01 09:06  12  13\r\n2013-01-01 09:07  14  15\r\n2013-01-01 09:08  16  17\r\n2013-01-01 09:09  18  19\r\n\r\n[5 rows x 2 columns]\r\n```"""
6715,30284400,jorisvandenbossche,jreback,2014-03-27 09:28:09,2014-03-28 12:34:13,2014-03-28 12:34:13,closed,,0.14.0,5,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6715,b'Inconsistent result with applying function between dataframe/groupby apply and 0.12/0.13/master',"b'I stumbled upon an inconsistent behaviour in (groupby/dataframe) apply when updating some code from 0.12 to 0.13.\r\n\r\n\r\nSay you have following custom function:\r\n```\r\ndef P1(a):\r\n    try:\r\n        return np.percentile(a.dropna(), q=1)\r\n    except:\r\n        return np.nan\r\n        \r\ndef P1_withouttry(a):\r\n    return np.percentile(a.dropna(), q=1)\r\n```\r\nWhen you apply this function to a dataframe:\r\n```\r\nIn [3]: df = pd.DataFrame({\'col1\':[1,2,3,4],\'col2\':[10,25,26,31],                   \r\n     ...:                    \'date\':[dt.date(2013,2,10),dt.date(2013,2,10),dt.date(2013,2,11),dt.date(2013,2,11)]})\r\n\r\nIn [4]: df\r\nOut[4]:\r\n   col1  col2        date\r\n0     1    10  2013-02-10\r\n1     2    25  2013-02-10\r\n2     3    26  2013-02-11\r\n3     4    31  2013-02-11\r\n\r\nIn [136]: df.apply(P1)\r\nOut[136]: \r\ncol1     1.03\r\ncol2    10.45\r\ndate      NaN\r\ndtype: float64\r\n\r\nIn [138]: df.apply(P1_withouttry)\r\nTraceback (most recent call last):\r\n  ...\r\nTypeError: (""unsupported operand type(s) for *: \'datetime.date\' and \'float\'"", u\'occurred at index date\')\r\n```\r\nthis does work with `P1`, but not with `P1_withouttry`. So I constructed my original function with a try/except to be able to apply this on a dataframe with also non-numeric columns.\r\n\r\nHowever, when applying this on a groupby, it does not work anymore like this:\r\n```\r\nIn [6]: g = df.groupby(\'date\')\r\n\r\nIn [7]: g.apply(P1)\r\nOut[7]:\r\ndate\r\n2013-02-10   NaN\r\n2013-02-11   NaN\r\ndtype: float64\r\n\r\nIn [8]: g.apply(P1_withouttry)\r\nTraceback (most recent call last):\r\n   ...\r\nTypeError: can\'t compare datetime.date to long\r\n\r\n\r\nIn [8]: g.agg(P1)\r\nOut[8]:\r\n            col1  col2\r\ndate\r\n2013-02-10   NaN   NaN\r\n2013-02-11   NaN   NaN\r\n\r\nIn [143]: g.agg(P1_withouttry)\r\nOut[143]: \r\n            col1   col2\r\ndate                   \r\n2013-02-10  1.01  10.15\r\n2013-02-11  3.01  26.05\r\n```\r\nSo, with `apply` it does not work, with `aggregate` it does, but only the `P1_withouttry` that didn\'t work with `df.apply()`. \r\nWhen using `g.agg([P1]`) this does work again on master, but not with 0.13.1 (then it gives the same as `g.agg(P1)`), although this did work\r\nin 0.12:\r\n```\r\nIn [11]: g.agg([P1])\r\nOut[11]:\r\n            col1   col2\r\n              P1     P1\r\ndate\r\n2013-02-10  1.01  10.15\r\n2013-02-11  3.01  26.05\r\n```\r\nIt was this last pattern I was using in my code in 0.12 that does not work anymore in 0.13.1 (I had something like `g.agg([P1, P5, P10, P25, np.median, np.mean])`).\r\n'"
6709,30181283,ghost,jorisvandenbossche,2014-03-26 02:34:29,2014-07-01 14:57:33,2014-03-31 16:49:52,closed,,0.14.0,13,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/6709,b'BUG: Allow mapping as parameters for SQL DBAPI2',"b""According to the DBAPI2.0 the parameters of the execute method can be a list or a mapping. The code in the master branch assume that this parameter is a list which can break working code. That's a regression compared to the pandas 0.13.1\r\n\r\nCloses #6708"""
6708,30180796,ghost,jorisvandenbossche,2014-03-26 02:20:28,2014-03-31 16:49:52,2014-03-31 16:49:52,closed,,0.14.0,0,Bug;IO SQL,https://api.github.com/repos/pydata/pandas/issues/6708,b'DBAPI 2.0 broken when using dictionary as parameters',b'The function [pandas.io.sql._convert_params](https://github.com/pydata/pandas/blob/b77eb1f11bcd65515ac33e1510fe03fd365bc433/pandas/io/sql.py#L30-L35) assume that the parameters are passed as a list. However the [DBAPI2.0](http://legacy.python.org/dev/peps/pep-0249/#id14) specify that the parameters can be a list or a mapping. This is a regression compared to the current stable version 0.13.1.'
6690,29975230,jreback,jreback,2014-03-22 23:25:25,2014-06-27 12:22:23,2014-03-23 13:49:21,closed,,0.14.0,0,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/6690,b'BUG: Bug in resample with extra bins when using an evenly divisible freq (GH4076)',"b'closes #4076\r\n\r\nthis only happened on the cythonized methods and not the python agg ones (this is why count worked, but sum did not)'"
6689,29973923,fonnesbeck,cpcloud,2014-03-22 22:11:29,2014-04-08 22:55:12,2014-04-08 22:55:12,closed,cpcloud,0.14.0,8,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6689,b'DataFrame.replace only replaces the first occurrence of replacement pattern',"b""This is best explained by a screenshot:\r\n\r\n![bad_replace](http://d.pr/i/vVgH+)\r\n\r\nI'm running a pretty recent build of Pandas ('0.13.1-213-gc174c3d') on Python 2.7.5 on OS X 0.9.2.\r\n"""
6685,29944202,bashtage,jreback,2014-03-21 22:22:24,2014-06-13 03:14:11,2014-03-23 13:41:24,closed,,0.14.0,13,Bug;Data IO;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6685,b'BUG: NaN values not converted to Stata missing values (GH6684)',"b'closes #6684\r\n\r\nStata does not correctly handle NaNs, and so these must be replaced with Stata\r\nmissing values (. by default).  The fix checks floating point columns for nan\r\nand replaces these with the Stata numeric code for (.).\r\n\r\nThe write_index option was also being ignored by omission. This has been fixed and\r\nnumerous tests which were not correct have been fixed.'"
6684,29923749,ozak,jreback,2014-03-21 18:30:20,2014-03-23 13:41:24,2014-03-23 13:41:24,closed,,0.14.0,9,Bug;Data IO;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6684,"b'Bug: Export to Stata NaN not converted to "".""'","b'Hi,\r\n\r\nI noticed that when exporting data to stata the ``NaN`` values are not always converted to Stata missing values but instead left blank. This somehow confuses Stata which does not allow using the ``destring`` command to solve the problem nor using ``replace value=. if value==.``.\r\n\r\nAs an Example I downloaded the [World Development Indicators](http://databank.worldbank.org/data/download/WDI_excel.zip) and used the following commands to export National Savings to an the excel and csv file:\r\n\r\n```\r\nimport pandas as pd\r\nimport os\r\ndfwdi=pd.read_excel(\'WDI.xlsx\',\'Data\')\r\ndfwdi.columns\r\ndfout=dfwdi.ix[dfwdi[\'Indicator Code\']==\'NY.GDS.TOTL.ZS\']\r\ndfout\r\ncols=[\'savyr\'+str(i) for i in xrange(1960,dfwdi.columns.values[-1]+1)]\r\ndfout.reset_index(inplace=True, drop=True)\r\ndfout.to_csv(\'sav.csv\', index=False)\r\ndfout.to_stata(\'sav.dta\', write_index=False)\r\n```\r\n\r\nIf you import the data into Stata (I am using v.13) and run the following commands, things fail.\r\n\r\n```\r\nuse ""sav.dta"", clear\r\n\r\n* Correct number of missing values\r\nsumm savyr2000\r\nreg savyr2010 savyr 2000\r\n\r\n* Correct countries identified as missing\r\ntab code if savyr==.\r\n\r\n* replace missing values to "".""\r\n* One cannot replace the missing not presented as "".""\r\nreplace savyr2010==. if savyr==""""\r\n* Use ""."" to identify\r\nreplace savyr2010==. if savyr==.\r\n\r\n* Perform analysis again\r\nsumm savyr2000\r\nreg savyr2010 savyr 2000\r\n\r\n* Still fails\r\n```\r\n\r\nAs you can see Stata does not perform the analysis, even though it correctly recognizes the missing values. But not all of them are presented as ""."". If one imports the the csv version into Stata and runs the same initial commands it works fine.\r\n\r\n```\r\nimport delimited ""sav.csv""\r\n\r\n* Correct number of missing values\r\nsumm savyr2000\r\nreg savyr2010 savyr 2000\r\n```\r\n\r\nFurthermore, for some reason the index is still present in the stata file, even though I had used the ``write_index=False`` option.\r\n\r\nI am using Enthought\'s Canopy distribution on OSX Mavericks with Pandas \'0.13.1\'. Haven\'t tried on other  Python dists.'"
6682,29919095,jreback,jreback,2014-03-21 17:30:43,2014-06-23 20:21:08,2014-03-21 20:10:13,closed,,0.14.0,0,Bug;MultiIndex;Numeric,https://api.github.com/repos/pydata/pandas/issues/6682,b'BUG: Bug in binary operations with a rhs of a Series not aligning (GH6681)',"b""closes #6681\r\n\r\n\r\n```\r\nIn [11]:         index=MultiIndex.from_product([list('abc'),\r\n                                       ['one','two','three'],\r\n                                       [1,2,3]],\r\n                                      names=['first','second','third'])\r\n\r\nIn [12]:         df = DataFrame(np.arange(27*3).reshape(27,3),\r\n                       index=index,\r\n                       columns=['value1','value2','value3']).sortlevel()\r\n\r\nIn [13]: df\r\nOut[13]: \r\n                    value1  value2  value3\r\nfirst second third                        \r\na     one    1           0       1       2\r\n             2           3       4       5\r\n             3           6       7       8\r\n      three  1          18      19      20\r\n             2          21      22      23\r\n             3          24      25      26\r\n      two    1           9      10      11\r\n             2          12      13      14\r\n             3          15      16      17\r\nb     one    1          27      28      29\r\n             2          30      31      32\r\n             3          33      34      35\r\n      three  1          45      46      47\r\n             2          48      49      50\r\n             3          51      52      53\r\n      two    1          36      37      38\r\n             2          39      40      41\r\n             3          42      43      44\r\nc     one    1          54      55      56\r\n             2          57      58      59\r\n             3          60      61      62\r\n      three  1          72      73      74\r\n             2          75      76      77\r\n             3          78      79      80\r\n      two    1          63      64      65\r\n             2          66      67      68\r\n             3          69      70      71\r\n\r\n[27 rows x 3 columns]\r\n\r\nIn [15]: x = Series([ 1.0, 10.0], ['two','three'])\r\n\r\nIn [16]: x\r\nOut[16]: \r\ntwo       1\r\nthree    10\r\ndtype: float64\r\n```\r\n\r\nPassing a level now aligns by that level\r\n```\r\nIn [14]: df.mul(x,level='second',axis=0)\r\nOut[14]: \r\n                    value1  value2  value3\r\nfirst second third                        \r\na     one    1         NaN     NaN     NaN\r\n             2         NaN     NaN     NaN\r\n             3         NaN     NaN     NaN\r\n      three  1         180     190     200\r\n             2         210     220     230\r\n             3         240     250     260\r\n      two    1           9      10      11\r\n             2          12      13      14\r\n             3          15      16      17\r\nb     one    1         NaN     NaN     NaN\r\n             2         NaN     NaN     NaN\r\n             3         NaN     NaN     NaN\r\n      three  1         450     460     470\r\n             2         480     490     500\r\n             3         510     520     530\r\n      two    1          36      37      38\r\n             2          39      40      41\r\n             3          42      43      44\r\nc     one    1         NaN     NaN     NaN\r\n             2         NaN     NaN     NaN\r\n             3         NaN     NaN     NaN\r\n      three  1         720     730     740\r\n             2         750     760     770\r\n             3         780     790     800\r\n      two    1          63      64      65\r\n             2          66      67      68\r\n             3          69      70      71\r\n\r\n[27 rows x 3 columns]\r\n```"""
6681,29918780,jreback,jreback,2014-03-21 17:26:56,2014-03-21 20:10:13,2014-03-21 20:10:13,closed,,0.14.0,0,Bug;MultiIndex;Numeric,https://api.github.com/repos/pydata/pandas/issues/6681,b'BUG: align on level for binary ops',b'http://stackoverflow.com/questions/22559172/multiply-all-columns-of-a-multi-indexed-dataframe-by-appropriate-values-in-a-ser\r\n\r\nrelated to #5645 as well\r\n'
6678,29883192,sinhrks,TomAugspurger,2014-03-21 07:30:24,2014-06-12 19:37:46,2014-04-22 13:10:31,closed,,0.14.0,15,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6678,b'BUG: legend behaves inconsistently when plotting to the same axes ',"b""There seems to be some inconsistencies related to `DataFrame.plot` and `Series.plot` legend behaviors.\r\n\r\n*Problems:*\r\n- When `DataFrame.plot` or `Series.plot` plots data on the same axes repeatedly:\r\n\r\n\t- If the target axes already has a legend, line plot always appends its legend to existing one ignoring `legend` kw and existing legend will be drawn as line artist regardless of actual artist type. Also, legend cannot be `reverse`ed if the axes already has a legend.\r\n\t- Bar/BarH plot deletes the existing legend and overwrites with latest one.\r\n\t- KDE plot appends its legend to the existing one, and will apply `reverse` to all artists including the existing one.\r\n- When `subplots` is enabled, line plot draws legends on each axes but bar plot doesn't.\r\n- Scatter plot does not draw legend even if `label` keyword is passed.\r\n\r\n*Fix:*\r\nI've prepared a fix based on following concept, except `hexbin` which doesn't use legend.\r\n- Legend should be drawn by the order of the artists drawn.\r\n- Each legend should be drawn according to the passed `legend` value.\r\n\t- If df1 plots with `legend=True` and df2 with `legend=False`, only df1's legend should appear.\r\n\t- If df2 plots with `legend='reverse'`d, only df2's legend should be reversed.\r\n- When `subplots=True` and `legend=True`, each subplot axes should have its own legend (standardize current line plot behavior). \r\n\r\n*Example Code*\r\n```\r\ndf1 = DataFrame(randn(6, 3), index=range(6), columns=['a', 'b', 'c'])\r\ndf2 = DataFrame(randn(6, 3), index=range(6), columns=['d', 'e', 'f'])\r\ndf3 = DataFrame(randn(6, 3), index=range(6), columns=['x', 'y', 'z'])\r\n\r\nfig, axes = plt.subplots(1, 5, figsize=(14, 3))\r\nfor i, (legend, df) in enumerate(zip([True, False, 'reverse'], [df1, df2, df3])):\r\n    df.plot(ax=axes[0],  legend=legend, title='line')\r\n    df.plot(kind='bar', ax=axes[1], legend=legend, title='bar')\r\n    df.plot(kind='barh', ax=axes[2],  legend=legend, title='barh')\r\n    df.plot(kind='kde', ax=axes[3],  legend=legend, title='kde')\r\n    df.plot(kind='scatter', ax=axes[4], x=df.columns[0], y=df.columns[1], label=i, \r\n        legend=legend, title='scatter')\r\nplt.show()\r\n```\r\n\r\n*Output using current repository*\r\n- For line, bar, kde plot, expected legend is `a, b, c, z, y x`. Because df2 was plot by `legend=False`, and df3 was plot by `legend='reverse'`.\r\n- For scatter plot, `0, 2` is expected because df2 was plot by `legend=False`.\r\n\r\n![figure_legend_current](https://f.cloud.github.com/assets/1696302/2480801/ba3c694a-b0c9-11e3-8f5b-d2ebec0798e3.png)\r\n\r\n*Output after fix*\r\n![figure_fixed](https://f.cloud.github.com/assets/1696302/2480773/b1bd8f70-b0c8-11e3-9654-3e3c2b20157b.png)\r\n\r\nIf there is anything should be considered, please let me know. Thanks.\r\n\r\n"""
6659,29640384,jreback,jreback,2014-03-18 12:18:47,2014-06-21 09:06:49,2014-03-18 14:39:47,closed,,0.14.0,0,Bug;Compat;Indexing,https://api.github.com/repos/pydata/pandas/issues/6659,"b'BUG: Bug in compat with np.compress, surfaced in (GH6658)'",b'closes #6658'
6658,29618419,dsm054,jreback,2014-03-18 03:50:47,2014-03-18 14:39:47,2014-03-18 14:39:47,closed,,0.14.0,4,Bug;Compat;Indexing,https://api.github.com/repos/pydata/pandas/issues/6658,b'BUG: test_boxplot failure',"b""`test_graphics.py` seems to be failing for me:\r\n\r\n```\r\ntest_boxplot (__main__.TestDataFramePlots) ... > /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/internals.py(64)__init__()\r\n-> '%d' % (len(values), len(items)))\r\n(Pdb) bt\r\n  /usr/lib/python2.7/unittest/case.py(331)run()\r\n-> testMethod()\r\n  /home/dsm/sys/pandas/pandas/tests/test_graphics.py(633)test_boxplot()\r\n-> _check_plot_works(df.boxplot, column=['one', 'two'], by='indic')\r\n  /home/dsm/sys/pandas/pandas/tests/test_graphics.py(1226)_check_plot_works()\r\n-> ret = f(*args, **kwargs)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/frame.py(4865)boxplot()\r\n-> fontsize=fontsize, grid=grid, rot=rot, **kwds)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/tools/plotting.py(1995)boxplot()\r\n-> ax=ax)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/tools/plotting.py(2409)_grouped_plot_by_column()\r\n-> plotf(gp_col, ax, **kwargs)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/tools/plotting.py(1973)plot_group()\r\n-> bp = ax.boxplot(values, **kwds)\r\n  /usr/local/lib/python2.7/dist-packages/matplotlib-1.4.x-py2.7-linux-i686.egg/matplotlib/axes/_axes.py(3003)boxplot()\r\n-> labels=labels)\r\n  /usr/local/lib/python2.7/dist-packages/matplotlib-1.4.x-py2.7-linux-i686.egg/matplotlib/cbook.py(2029)boxplot_stats()\r\n-> np.compress(x < stats['whislo'], x),\r\n  /usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py(1560)compress()\r\n-> return _wrapit(a, 'compress', condition, axis, out)\r\n  /usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py(47)_wrapit()\r\n-> result = wrap(result)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/series.py(374)__array_wrap__()\r\n-> copy=False).__finalize__(self)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/series.py(232)__init__()\r\n-> data = SingleBlockManager(data, index, fastpath=True)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/internals.py(3670)__init__()\r\n-> block = make_block(block, axis, axis, ndim=1, fastpath=True)\r\n  /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/internals.py(2058)make_block()\r\n-> placement=placement)\r\n> /usr/local/lib/python2.7/dist-packages/pandas-0.13.1_436_g4216178-py2.7-linux-i686.egg/pandas/core/internals.py(64)__init__()\r\n-> '%d' % (len(values), len(items)))\r\n(Pdb) print values\r\n[]\r\n(Pdb) print items\r\nIndex([u'b', u'd', u'f'], dtype='object')\r\n(Pdb) print ref_items\r\nIndex([u'b', u'd', u'f'], dtype='object')\r\n(Pdb) print ndim\r\n1\r\n(Pdb) print fastpath\r\nTrue\r\n(Pdb) print placement\r\nNone\r\n```\r\n\r\nStandard TMI dump:\r\n\r\n```\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 32\r\nOS: Linux\r\nOS-release: 3.11.0-18-generic\r\nmachine: i686\r\nprocessor: i686\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.13.1-436-g4216178\r\nCython: 0.20.1\r\nnumpy: 1.9.0.dev-e792e15\r\nscipy: 0.15.0.dev-4aa534f\r\nstatsmodels: 0.6.0.dev-f3f5467\r\nIPython: 1.2.1\r\nsphinx: 1.2.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2014.1\r\nbottleneck: 0.8.0\r\ntables: None\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.4.x\r\nopenpyxl: 1.8.4\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: None\r\nlxml: 3.3.3\r\nbs4: 4.3.2\r\nhtml5lib: 1.0b3\r\nbq: None\r\napiclient: 1.2\r\nrpy2: None\r\nsqlalchemy: 0.9.3\r\npymysql: None\r\npsycopg2: None\r\n```"""
6657,29607435,rosnfeld,jreback,2014-03-17 23:10:30,2014-07-16 08:58:46,2014-03-18 10:09:59,closed,,0.14.0,3,Bug;Dtypes;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6657,"b'REGR: fixing Timestamp/Series subtraction, resolves #6648'","b""closes #6648.\r\n\r\nAdmittedly the conditional in ```__sub__``` is a little gross now.\r\n\r\nI didn't add release notes as this is really just patching up a yet-to-be released item, but please let me know if I should.\r\n\r\nAlso, the test only runs on numpy >= 1.7, is that okay?\r\n"""
6656,29560435,sinhrks,jreback,2014-03-17 13:38:17,2014-06-12 05:20:06,2014-05-01 15:14:42,closed,,0.14.0,16,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6656,"b""ENH/VIS: Area plot is now supported by kind='area'. ""","b""Area plot is added to plotting method. The AreaPlot class is created as a subclass of LinePlot, thus it works also in time series. \r\n\r\nBy default, area plot is being stacked. When area plot is not stacked (`stacked=False`), alpha value is set to 0.5 to show overlapped area if not configured specifically. As a side benefit, line plot also can be stacked by specifying `stacked=True` (disabled by default). Different from stacked bar plot, I don't know good visualization for positive/negative mixed data. Thus, input must be all positive or all negative when `stacked=True`. I'll try to implement it if there is a good way. Also, area plot doesn't support logy and loglog plot because filling area starts from 0.\r\n\r\nNote: Area plot's legend is implemented based on the answer described in:\r\nhttp://stackoverflow.com/questions/14534130/legend-not-showing-up-in-matplotlib-stacked-area-plot\r\n\r\n*Example:*\r\n![figure_1](https://f.cloud.github.com/assets/1696302/2436288/dee8f4e6-add8-11e3-9297-a7403935844d.png)\r\n"""
6649,29498429,jreback,jreback,2014-03-15 18:26:46,2014-07-22 18:41:02,2014-03-17 12:25:22,closed,,0.14.0,0,Bug;Windows,https://api.github.com/repos/pydata/pandas/issues/6649,b'PLAT: platform sorting issue surfaced with time_grouper',
6647,29494870,jreback,jreback,2014-03-15 15:36:32,2015-01-18 21:38:51,2015-01-18 21:38:51,closed,,0.16.0,3,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6647,"b'BUG: Bug in DataFrame.reindex(labels,level=0) with with reordered labels (GH4088)'",b'closes #4088'
6634,29413273,hayd,hayd,2014-03-14 06:21:17,2014-04-30 16:29:24,2014-04-30 16:29:24,closed,hayd,0.16.0,11,API Design;Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6634,b'str.get_dummies uses astype(str)',"b'I wrote this so my fault. This probably means some stuff with break when passing stuff with unicode.\r\n\r\nWill add example.\r\n\r\nThe reason is to include integers as strings before getting dummies, maybe should just drop that functionality?'"
6631,29389813,jseabold,jreback,2014-03-13 21:10:09,2014-05-12 13:34:58,2014-05-12 13:34:58,closed,,0.14.0,2,Bug;Dtypes;Period,https://api.github.com/repos/pydata/pandas/issues/6631,b'set_index/reindex bugs with time indices',"b""    [14]: pd.version.version\r\n    [14]: '0.13.1-426-ge19b2eb'\r\n\r\nNoticed these. This one's minor. `set_index` seems to drop frequency information for DatetimeIndex. I'm not sure what other metadata is ignored.\r\n\r\n    import statsmodels.api as sm\r\n    import pandas as pd\r\n    index = pd.PeriodIndex(start='1959Q1', end='2009Q3')\r\n    dta = sm.datasets.macrodata.load_pandas().data\r\n\r\n    assert dta.set_index(index.to_timestamp()).index.freq == index.to_timestamp().freq\r\n\r\nThis one is less minor. `set_index` and `reindex` are broken for a PeriodIndex. Treats it as regular integer index I guess. Not sure if it ever worked, though I don't recall running into this before.\r\n    \r\n    dta.set_index(index).head()\r\n\r\n    dta.reindex(index).head()\r\n\r\n"""
6628,29350178,jreback,jreback,2014-03-13 13:32:40,2014-06-25 08:40:04,2014-03-13 14:37:40,closed,,0.14.0,2,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6628,b'BUG: Bug in fillna with limit and value specified',b'from SO: http://stackoverflow.com/questions/22379343/change-first-occurrence-of-nan-in-a-specific-dataframe-row-to-a-new-value/22380225#22380225'
6623,29323059,rosnfeld,rosnfeld,2014-03-13 03:12:00,2014-06-12 21:04:04,2014-04-30 03:54:36,closed,,0.16.0,2,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/6623,b'BUG: timeseries plot mouse-over indicates incorrect time',"b'In pandas 0.13.1 this code produces a plot that, when ""moused over"", indicates incorrect dates on the \'t\' (x) axis, despite having the correct axis label.\r\n```python\r\npd.Series([1, 2, 3], index=pd.date_range(\'2014-01-01\', periods=3, freq=\'D\')).plot()\r\n```\r\n![screenshot](https://f.cloud.github.com/assets/5356340/2405879/8f7d2ef6-aa5c-11e3-9c68-f1383efc6b52.png)\r\n\r\n(matplotlib version is 1.3.1)\r\n\r\n'"
6622,29317093,bashtage,jreback,2014-03-13 00:37:24,2014-07-16 08:58:09,2014-03-13 22:19:16,closed,,0.14.0,14,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/6622,b'BUG: Error in to_stata when DataFrame contains non-string column names',"b'closes #4558\r\n\r\nto_stata does not work correctly when used with non-string names.  Since\r\nStata requires string names, the proposed fix attempts to rename columns using\r\nthe string representation of the column name used.  A warning is raised if\r\nthe column name is changed.'"
6621,29303312,hayd,jreback,2014-03-12 21:20:11,2014-05-10 19:49:51,2014-05-10 19:49:51,closed,hayd,0.14.0,10,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6621,b'groupby nth for multiindex regression',"b'regression in groupby nth, it raises an index error on MI in master. This is my fault (need to add tests, should also test for axis!=0).\r\n\r\nhttp://stackoverflow.com/questions/22361385/integer-position-based-indexing-of-multi-index-dataframes\r\n\r\nreason this raise is as I use obj.loc[[[]] :s'"
6620,29292735,jbezaire,jreback,2014-03-12 19:11:53,2015-02-14 03:10:28,2015-02-14 03:10:28,closed,,0.16.0,1,Bug;Dtypes;Duplicate;Groupby;Numeric,https://api.github.com/repos/pydata/pandas/issues/6620,b'groupby producing incorrect results applying functions to int64 columns due to internal cast to float',"b""dupe of #3707\r\n\r\ngroupby often casts int64 data to floats before applying functions to the grouped data, and then casts the result back to an int64. This produces incorrect results.\r\nHere is a simple example:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nlabel = np.array([1, 2, 2, 3],dtype=np.int)\r\ndata = np.array([1, 2, 3,4], dtype=np.int64) + 24650000000000000\r\nz = pd.DataFrame({'a':label,'data':data})\r\nf = z.groupby('a').first()\r\n\r\nIn [40]: print z\r\n   a               data\r\n0  1  24650000000000001\r\n1  2  24650000000000002\r\n2  2  24650000000000003\r\n3  3  24650000000000004\r\n\r\nIn [41]: print f\r\n                data\r\na                   \r\n1  24650000000000000\r\n2  24650000000000000\r\n3  24650000000000004\r\n\r\n```\r\n\r\nThe result is clearly incorrect and should be\r\n\r\n```python\r\n                data\r\na                   \r\n1  24650000000000001\r\n2  24650000000000002\r\n3  24650000000000004\r\n```\r\n\r\nz.data and f.data are both numpy.int64, which masks the fact that z.data was converted to a float with a loss of precision during the groupby operation.\r\nTests of software using groupby on int64 will only show a problem if the tests include integers large enough to cause this loss of precision. Otherwise the tests will pass and the software will just quietly produce incorrect results in production.\r\n\r\n"""
6611,29239883,hayd,hayd,2014-03-12 06:11:39,2014-06-20 20:48:38,2014-03-14 06:18:09,closed,,0.14.0,10,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6611,b'FIX str.match uses na flag',b'fixes #6609'
6609,29236668,hayd,hayd,2014-03-12 04:16:55,2014-03-14 06:18:09,2014-03-14 06:18:09,closed,hayd,0.14.0,0,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6609,b'string match returns NaN on non strings',"b""Or just not respecting na flag.\r\n\r\n```\r\nIn [44]: s = pd.Series(['a', 0, np.nan])\r\n\r\nIn [45]: s.str.contains('a', na=False)\r\nOut[45]:\r\n0     True\r\n1    False\r\n2    False\r\ndtype: bool\r\n\r\nIn [46]: s.str.match('a', na=False)\r\nOut[46]:\r\n0    True\r\n1     NaN\r\n2     NaN\r\ndtype: object\r\n```"""
6607,29219934,mcwitt,jreback,2014-03-11 22:19:14,2014-04-23 22:23:11,2014-04-23 22:23:11,closed,,0.14.0,26,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/6607,b'BUG: read_table() ignores dtype argument when multi-character separator is specified',"b""related #4363\r\ncloses #3374\r\n\r\nHere is a minimal example:\r\n\r\n```ipython\r\nIn [21]: df = pd.DataFrame({'A': range(5), 'B': rand(5)})\r\n\r\nIn [22]: df\r\nOut[22]: \r\n   A         B\r\n0  0  0.402616\r\n1  1  0.880696\r\n2  2  0.184491\r\n3  3  0.832732\r\n4  4  0.393917\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [23]: df.to_csv('test.csv', sep=' ')\r\n\r\nIn [24]: pd.read_csv('test.csv', sep=' ', dtype={'A': np.float64}).dtypes\r\nOut[24]: \r\nUnnamed: 0      int64\r\nA             float64\r\nB             float64\r\ndtype: object\r\n```\r\n\r\nHere the dtype argument behaves as expected, and column A has type float. However with sep='\\s' the dtype argument appears to be ignored:\r\n\r\n```ipython\r\nIn [25]: pd.read_csv('test.csv', sep='\\s', dtype={'A': np.float64}).dtypes\r\nOut[25]: \r\nA      int64\r\nB    float64\r\ndtype: object\r\n```\r\n\r\nVersion information\r\n-------------------\r\n```ipython\r\nIn [27]: show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 10.8.0\r\nmachine: i386\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.1-413-ga71ede3\r\nCython: 0.20.1\r\nnumpy: 1.8.0\r\nscipy: 0.13.3\r\nstatsmodels: 0.5.0\r\nIPython: 2.0.0-dev\r\nsphinx: 1.2.1\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2013b\r\nbottleneck: None\r\ntables: 3.1.0\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.2\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.2\r\nlxml: 3.3.1\r\nbs4: 4.3.1\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\nrpy2: None\r\nsqlalchemy: 0.9.2\r\npymysql: None\r\npsycopg2: None\r\n```"""
6606,29213920,sleibman,jreback,2014-03-11 20:25:44,2014-06-24 14:50:40,2014-06-04 00:12:39,closed,,0.14.1,4,Bug;MultiIndex;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/6606,b'tzinfo lost when concatenating multiindex arrays',"b""This may be related to https://github.com/pydata/pandas/issues/3950 though my symptoms are a bit different. In that issue, the tzinfo was lost during the creation of a MultiIndex array. In my case, I'm able to create a MultiIndex array with intact tzinfo, but tzinfo gets lost when I concatenate one or more such arrays.\r\n\r\nHere's an example. Note the lack of tzinfo in the final line:\r\n```python\r\nIn [47]: import pandas\r\nIn [48]: import datetime\r\nIn [49]: import pytz\r\nIn [50]: array = pandas.DataFrame({'a':[datetime.datetime(2014,1,1,tzinfo=pytz.UTC), datetime.datetime(2014,1,2,tzinfo=pytz.UTC)], 'b':[1, 2], 'c':[3, 4]})\r\nIn [51]: a2 = array.set_index('a')\r\nIn [52]: a3 = array.set_index(['a', 'b'])\r\nIn [53]: a2_concatenated = pandas.concat([a2])\r\nIn [54]: a3_concatenated = pandas.concat([a3])\r\nIn [55]: array.iloc[0,0]\r\nOut[55]: datetime.datetime(2014, 1, 1, 0, 0, tzinfo=<UTC>)\r\nIn [56]: a2.index[0]\r\nOut[56]: Timestamp('2014-01-01 00:00:00+0000', tz='UTC')\r\nIn [57]: a3.index[0]\r\nOut[57]: (Timestamp('2014-01-01 00:00:00+0000', tz='UTC'), 1)\r\nIn [58]: a2_concatenated.index[0]\r\nOut[58]: Timestamp('2014-01-01 00:00:00+0000', tz='UTC')\r\nIn [59]: a3_concatenated.index[0]\r\nOut[59]: (Timestamp('2014-01-01 00:00:00', tz=None), 1)\r\n```"""
6601,29200541,jreback,jreback,2014-03-11 17:43:46,2014-06-30 13:50:31,2014-03-11 19:00:54,closed,,0.14.0,0,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/6601,b'BUG: Bug in popping from a Series (GH6600)',b'closes #6600'
6600,29197567,data-raccoon,jreback,2014-03-11 17:10:23,2014-03-11 19:00:54,2014-03-11 19:00:54,closed,,0.14.0,1,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/6600,b'Series.pop() removes additional keys not specified',"b""```python\r\nimport pandas as pd\r\ndf = pd.DataFrame({\r\n    'A': 0, \r\n    'B': range(5), \r\n    'C': 0, \r\n    })\r\nk = df.iloc[4]\r\nk.pop('B')\r\nprint k\r\n```\r\n```\r\nout:\r\nA    0\r\nName: 4, dtype: int64\r\n```\r\n```\r\nexpected:\r\nA    0\r\nC    0\r\nName: 4, dtype: int64\r\n\r\n```\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.4.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.8.0-33-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: de_DE.UTF-8\r\n\r\npandas: 0.13.1\r\nCython: 0.17.4\r\nnumpy: 1.7.1\r\nscipy: 0.11.0\r\nstatsmodels: 0.5.0\r\nIPython: 0.13.2\r\nsphinx: 1.1.3\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2012c\r\nbottleneck: None\r\ntables: 2.4.0\r\nnumexpr: 2.0.1\r\nmatplotlib: 1.2.1\r\nopenpyxl: 1.7.0\r\nxlrd: 0.6.1\r\nxlwt: 0.7.4\r\nxlsxwriter: None\r\nsqlalchemy: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\n```"""
6591,29132019,jorisvandenbossche,jorisvandenbossche,2014-03-10 20:57:44,2014-06-20 10:46:15,2014-03-11 12:05:51,closed,,0.14.0,5,Bug;Dtypes;IO SQL,https://api.github.com/repos/pydata/pandas/issues/6591,b'BUG: replace iterrows with itertuples in sql insert (GH6509)',b'Fixes #6509.\r\n\r\nShould still add a test'
6588,29113817,jreback,jreback,2014-03-10 17:16:19,2014-07-16 08:57:38,2014-03-10 18:14:52,closed,,0.14.0,0,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6588,b'BUG: Bug in fillna with method = bfill/ffill and datetime64[ns] dtype (GH6587)',b'closes #6587'
6587,29106013,yonil7,jreback,2014-03-10 15:47:31,2014-03-10 18:14:52,2014-03-10 18:14:52,closed,,0.14.0,2,Bug;Dtypes;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6587,b'fillna() on Series or DataFrame containing datetime64 mess the values',"b""s = pd.Series([pd.NaT, pd.NaT, '2013-08-05 15:30:00.**000001**'])\r\nprint s\r\n0                          NaT\r\n1                          NaT\r\n2   2013-08-05 15:30:00.**000001**\r\ndtype: datetime64[ns]\r\n\r\nprint s.fillna(method='backfill')\r\n0   2013-08-05 15:30:00.**000001024**\r\n1   2013-08-05 15:30:00.**000001024**\r\n2   2013-08-05 15:30:00.**000001024**\r\ndtype: datetime64[ns]\r\n\r\nThis happens with pandas 0.13.1 and numpy 1.8.0\r\n"""
6585,29100559,behzadnouri,jreback,2014-03-10 14:47:55,2014-06-17 12:08:52,2014-06-17 12:08:52,closed,,0.14.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6585,b'groupby - transform not working with multiple columns',"b""I came across this issue while looking at the [this](http://stackoverflow.com/questions/22298542) question on stackoverflow;\r\n\r\nsay I have a data-frame like:\r\n\r\n    >>> df\r\n      tag  val1  val2\r\n    0   B     0     0\r\n    1   A     1     2\r\n    2   A     2     4\r\n    3   B     3     6\r\n    4   B     4     8\r\nand I want to reverse columns `val1` and `val2` within each `tag`; this works like a charm:\r\n\r\n    >>> grb = df.groupby('tag')\r\n    >>> df['val1'] = grb['val1'].transform(lambda ts: ts.loc[::-1])\r\n    >>> df['val2'] = grb['val2'].transform(lambda ts: ts.loc[::-1])\r\n    >>> df\r\n      tag  val1  val2\r\n    0   B     4     8\r\n    1   A     2     4\r\n    2   A     1     2\r\n    3   B     3     6\r\n    4   B     0     0\r\nbut, if I want to do this at the same time for columns `val1` and `val2`, it breaks:\r\n\r\n    >>> grb = df.groupby('tag')\r\n    >>> grb['val1', 'val2'].transform(lambda obj: obj.loc[::-1])\r\n       val1  val2\r\n    0     0     0\r\n    1     1     2\r\n    2     2     4\r\n    3     3     6\r\n    4     4     8\r\n\r\n    [5 rows x 2 columns]\r\n\r\nobviously I will not get the correct answer if I overwrite `df` with the new data ( or append the new columns ); however, this\r\n\r\n    >>> grb = df.groupby('tag')\r\n    >>> grb['val1', 'val2'].transform(lambda obj: obj.values[::-1])\r\n       val1  val2\r\n    0     4     8\r\n    1     2     4\r\n    2     1     2\r\n    3     3     6\r\n    4     0     0\r\n\r\n    [5 rows x 2 columns]\r\n\r\nworks as expected; which makes me guess this has something to do with the index being aligned with the original data-frame, whereas in here it should be ignored;"""
6583,29087971,nipunbatra,jreback,2014-03-10 11:26:56,2014-04-23 16:26:27,2014-04-23 16:26:27,closed,,0.14.0,5,API Design;Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/6583,b'DOC: Cookbook example on reading CSV by chunking gives error',"b'URL linked in the cookbook: http://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas/12193309#12193309\r\n\r\nTraceback:\r\n\r\n```python\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 929, in concat\r\n    verify_integrity=verify_integrity)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 944, in __init__\r\n    \'""{0}""\'.format(type(objs).__name__))\r\nAssertionError: first argument must be a list-like of pandas objects, you passed an object of type ""TextFileReader""\r\n```\r\n\r\nTo be precise: Concatenation does not work in this example.\r\n'"
6579,29047080,jreback,jreback,2014-03-09 14:46:56,2014-06-22 14:52:08,2014-03-09 15:15:00,closed,,0.14.0,0,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6579,b'BUG: Bug in .xs with a nan in level when dropped (GH6574)',b'closes #6574'
6578,29039689,hayd,jreback,2014-03-09 09:14:10,2014-07-16 08:57:30,2014-05-02 12:51:52,closed,,0.14.0,15,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6578,b'FIX raise when groupby selecting cols not in frame',b'KeyError part of #5264'
6574,28995935,leungwk,jreback,2014-03-07 20:29:09,2014-03-09 15:15:14,2014-03-09 15:15:00,closed,,0.14.0,2,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6574,"b'xs is filling nan in index with its last item, as if sorted ascending, in the resulting index'","b""Illustration:\r\n\r\n```python\r\n\r\nacc = [\r\n    ('a','abcde',1),\r\n    ('b','bbcde',2),\r\n    ('y','yzcde',25),\r\n    ('z','xbcde',24),\r\n    ('z',None,26),\r\n    ('z','zbcde',25),\r\n    ('z','ybcde',26),\r\n]\r\ndf1 = pd.DataFrame(acc, columns=['a1','a2','cnt']).set_index(['a1','a2'])\r\n```\r\n\r\n```\r\nIn [476]: df1\r\nOut[476]: \r\n          cnt\r\na1 a2        \r\na  abcde    1\r\nb  bbcde    2\r\ny  yzcde   25\r\nz  xbcde   24\r\n   NaN     26\r\n   zbcde   25\r\n   ybcde   26\r\n\r\n[7 rows x 1 columns]\r\n\r\nIn [477]: df1.xs('z',level='a1')\r\nOut[477]: \r\n       cnt\r\na2        \r\nxbcde   24\r\nzbcde   26\r\nzbcde   25\r\nybcde   26\r\n\r\n[4 rows x 1 columns]\r\n```\r\n\r\nI was expecting:\r\n```\r\n       cnt\r\na2        \r\nxbcde   24\r\nNaN     26\r\nzbcde   25\r\nybcde   26\r\n```\r\nbecause I thought it would preserve the index of df1.\r\n\r\n\r\n\r\nSorting explicitly doesn't seem to affect the result:\r\n```\r\nIn [478]: df1.sort('cnt',ascending=False)\r\nOut[478]: \r\n          cnt\r\na1 a2        \r\nz  ybcde   26\r\n   NaN     26\r\n   zbcde   25\r\ny  yzcde   25\r\nz  xbcde   24\r\nb  bbcde    2\r\na  abcde    1\r\n\r\n[7 rows x 1 columns]\r\n\r\nIn [479]: df1.sort('cnt',ascending=False).xs('z',level='a1')\r\nOut[479]: \r\n       cnt\r\na2        \r\nybcde   26\r\nzbcde   26\r\nzbcde   25\r\nxbcde   24\r\n\r\n[4 rows x 1 columns]\r\n```\r\n\r\nIt might be related to [forward filling](https://github.com/pydata/pandas/issues/5669#issuecomment-30170618), but then I think it would be:\r\n```\r\n       cnt\r\na2        \r\nybcde   26\r\nybcde   26\r\nzbcde   25\r\nxbcde   24\r\n```\r\nwhich still isn't what I was expecting.\r\n\r\nVersions and dependencies:\r\n```\r\nIn [480]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 12.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.13.1\r\nCython: 0.17.2\r\nnumpy: 1.6.2\r\nscipy: 0.13.3\r\nstatsmodels: 0.5.0\r\nIPython: 1.1.0\r\nsphinx: 1.2\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: 2012h\r\nbottleneck: None\r\ntables: 3.0.0\r\nnumexpr: 2.0.1\r\nmatplotlib: 1.3.1\r\nopenpyxl: None\r\nxlrd: 0.8.0\r\nxlwt: None\r\nxlsxwriter: None\r\nsqlalchemy: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\n```"""
6572,28990282,kontinuity,jreback,2014-03-07 19:10:21,2014-07-21 11:44:04,2014-07-21 11:43:55,closed,,0.15.0,2,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/6572,b'ValueError: Cannot compare tz-naive and tz-aware timestamps',"b'Trying to generate a time series based on timezone raises exception.\r\n\r\n```\r\n>>> import pandas\r\n>>> pandas.date_range(\'2013-01-01T00:00:00+05:30\',\'2014-03-07T23:59:59+05:30\',freq=\'AS\')\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/cruiser/work/metroleads/lib/python2.7/site-packages/pandas/tseries/index.py"", line 1794, in date_range\r\n    closed=closed)\r\n  File ""/Users/cruiser/work/metroleads/lib/python2.7/site-packages/pandas/tseries/index.py"", line 196, in __new__\r\n    infer_dst=infer_dst)\r\n  File ""/Users/cruiser/work/metroleads/lib/python2.7/site-packages/pandas/tseries/index.py"", line 406, in _generate\r\n    index = _generate_regular_range(start, end, periods, offset)\r\n  File ""/Users/cruiser/work/metroleads/lib/python2.7/site-packages/pandas/tseries/index.py"", line 1750, in _generate_regular_range\r\n    dates = list(xdr)\r\n  File ""/Users/cruiser/work/metroleads/lib/python2.7/site-packages/pandas/tseries/offsets.py"", line 1871, in generate_range\r\n    if periods is None and end < start:\r\n  File ""tslib.pyx"", line 611, in pandas.tslib._Timestamp.__richcmp__ (pandas/tslib.c:10872)\r\n  File ""tslib.pyx"", line 640, in pandas.tslib._Timestamp._assert_tzawareness_compat (pandas/tslib.c:11186)\r\nValueError: Cannot compare tz-naive and tz-aware timestamps\r\n```\r\n\r\n```\r\npandas==0.13.1\r\n```'"
6570,28987607,hayd,hayd,2014-03-07 18:33:48,2014-06-30 09:34:37,2014-03-09 19:09:40,closed,,0.14.0,15,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6570,b'FIX use selected_obj rather the obj throughout groupby',b'fixes many parts of for #5264... Changes lots of `self.obj` to `self._selected_obj`.\r\n\r\nprobably should vbench before merging?\r\n\r\ncc @jreback @TomAugspurger '
6569,28984684,hayd,hayd,2014-03-07 17:50:43,2014-07-03 18:44:46,2014-03-07 20:24:22,closed,,0.14.0,8,API Design;Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6569,"b'ENH/BUG groupby nth now filters, works with DataFrames'","b""fixes #5552\r\n\r\npartial for #5264\r\n```\r\nIn [101]: df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\r\n\r\nIn [102]: g = df.groupby('A')\r\n\r\nIn [103]: g.nth(0)\r\nOut[103]:\r\n   A   B\r\n0  1 NaN\r\n2  5   6\r\n\r\nIn [104]: g.nth(1)\r\nOut[104]:\r\n   A  B\r\n1  1  4\r\n\r\nIn [105]: g.nth(-1)\r\nOut[105]:\r\n   A  B\r\n1  1  4\r\n2  5  6\r\n\r\nIn [106]: g.nth(0, dropna='any')  #old behaviour-like\r\nOut[106]:\r\n   B\r\nA\r\n1  4\r\n5  6\r\n\r\nIn [107]: g.nth(1, dropna='any')  #old behaviour-like\r\nOut[107]:\r\n    B\r\nA\r\n1 NaN\r\n5 NaN\r\n```"""
6560,28839788,rosnfeld,jreback,2014-03-06 00:23:30,2014-06-27 01:15:56,2014-03-07 00:04:40,closed,,0.14.0,14,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/6560,b'BUG: preserve frequency across Timestamp addition/subtraction (#4547)',"b""closes #4547\r\n\r\nMy second PR, on basically the same code.\r\n\r\nAssuming people agree with my comments on #4547 (that users adding timedeltas that don't match frequencies better know what they are doing), then this would be my submission.\r\n\r\nI'm also assuming that something is up with Travis and it is not anything I have done - nosetests passes fine for me though Travis still dies more or less at startup, while it didn't on older code. That problem is true of builds not including any code I have added, e.g. https://travis-ci.org/pydata/pandas/jobs/20167915 . Maybe something in PR #6068, or does this kind of thing happen from time to time?"""
6558,28832763,jreback,jreback,2014-03-05 22:35:59,2014-06-16 07:04:38,2014-03-06 23:35:30,closed,,0.14.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6558,b'BUG: Series.quantile raising on an object dtype (GH6555)',b'closes #6555'
6555,28814222,nspies,jreback,2014-03-05 18:41:50,2014-03-06 23:35:30,2014-03-06 23:35:30,closed,,0.14.0,1,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/6555,b'Series.quantile() should convert input dtype to float',"b""Would love it if Series.quantile() converted the dtype to float (as DataFrame.quantile() appears to do). As it is, I get the following error with dtype=object:\r\n\r\n````\r\nIn [8]: low\r\nOut[8]: \r\n...\r\nName: p, Length: 2218, dtype: object\r\n\r\nIn [9]: low.quantile(0.33)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/ase/src/ase/analysis/conservation.py in <module>()\r\n----> 1 low.quantile(0.33)\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/series.pyc in quantile(self, q)\r\n   1323             return pa.NA\r\n   1324         result = _quantile(valid_values, q * 100)\r\n-> 1325         if result.dtype == _TD_DTYPE:\r\n   1326             from pandas.tseries.timedeltas import to_timedelta\r\n   1327             return to_timedelta(result)\r\n\r\nAttributeError: 'float' object has no attribute 'dtype'\r\n````"""
6552,28787541,immerrr,jreback,2014-03-05 12:59:50,2014-10-04 20:02:08,2014-10-04 20:02:08,closed,,0.15.0,13,Bug;Indexing;Internals,https://api.github.com/repos/pydata/pandas/issues/6552,b'BUG: loc/ix indexers with empty list key lose names of respective axes',"b""This issue was encountered when fixing #6536. It seems that `.loc` indexer with empty list key seems to lose names of respective axes:\r\n\r\n```python\r\n\r\nIn [2]: pd.DataFrame(np.arange(9).reshape(3,3), columns=pd.Series(list('abc'), name='foobar'))\r\nOut[2]: \r\nfoobar  a  b  c\r\n0       0  1  2\r\n1       3  4  5\r\n2       6  7  8\r\n\r\n[3 rows x 3 columns]\r\n\r\nIn [3]: _2.iloc[:,[]]\r\nOut[3]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [0, 1, 2]\r\n\r\n[3 rows x 0 columns]\r\n\r\nIn [4]: _2.iloc[:,[]].columns.names\r\nOut[4]: FrozenList([u'foobar'])\r\n\r\nIn [5]: _2.loc[:,[]].columns.names\r\nOut[5]: FrozenList([None])\r\n\r\n```\r\n\r\nIt's not important, but currently #6551 has two tests skipped because of this bug."""
6551,28784702,immerrr,jreback,2014-03-05 12:03:51,2014-10-15 13:40:48,2014-03-06 23:01:26,closed,,0.14.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6551,b'BUG: fix fancy indexing with empty list',"b""This should fix #6536.\r\n\r\nThere's a side issue though with loc/ix indexers that somehow discard column names for empty indexers."""
6549,28756119,flatzo,jreback,2014-03-05 00:52:15,2015-08-29 01:39:57,2015-08-29 01:39:57,closed,,Next Major Release,3,Bug;Difficulty Intermediate;Effort Low;Testing;Timedelta;Windows,https://api.github.com/repos/pydata/pandas/issues/6549,b'Overflow adding pd.timedelta to pd.Timestamp',"b""xref #9442\r\n\r\n# Assume following code\r\n\r\n``` python\r\nimport pandas as pd\r\n\r\nadditions = [1e9, 1e10]\r\nfor ns in additions:\r\n    added = pd.Timestamp(0) + pd.to_timedelta(str(ns), unit='ns')\r\n    print added\r\n```\r\n\r\nThis will work fine on Linux 64 bits. However, it won't work on Windows (both 32 and 64 bits).\r\n\r\nAs for now, the following solution works fine but is not as elegant as the API nothing tells me that Timestamp(0).value will be nanoseconds in the next versions of API\r\n\r\n``` python\r\nimport pandas as pd\r\n\r\nadditions = [1e9, 1e10]\r\nfor ns in additions:\r\n    added = pd.Timestamp(pd.Timestamp(0).value + ns)\r\n    print added\r\n```\r\n\r\nI believe to know what is causing the bug, but I have no clue on how to set up the compilation environment on windows. I would suggest to change the ```astype(int)``` to ```astype(long)``` on the following line :\r\n\r\nhttps://github.com/pydata/pandas/blob/v0.13.1/pandas/tslib.pyx#L659\r\n\r\nAnybody could try that fix or giving me insight on how to setup compilation environement on windows so I could try it myself."""
6548,28754817,jreback,jreback,2014-03-05 00:25:42,2014-06-12 11:59:19,2014-03-05 01:02:29,closed,,0.14.0,0,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/6548,b'BUG: Bug in setitem with loc on mixed integer Indexes (GH6546)',b'closes #6546'
6546,28748040,alexfields,jreback,2014-03-04 22:38:57,2014-03-05 01:02:29,2014-03-05 01:02:29,closed,,0.14.0,3,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/6546,b'Inconsistent integer column indexing',"b""When columns have an integer as their name, indexing them becomes inconsistent:\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({1:[1,1],2:[2,2],'a':['a','a']})\r\n>>> df\r\n   1  2  a\r\n0  1  2  a\r\n1  1  2  a\r\n\r\n[2 rows x 3 columns]\r\n>>> df[1]\r\n0    1\r\n1    1\r\nName: 1, dtype: int64\r\n>>> df[['a',1]]\r\n   a  1\r\n0  a  1\r\n1  a  1\r\n\r\n[2 rows x 2 columns]\r\n>>> df[[1,2]]\r\n   2  a\r\n0  2  a\r\n1  2  a\r\n\r\n[2 rows x 2 columns]\r\n```\r\n\r\nDesired behavior would be:\r\n```python\r\n\r\n>>> df[[1,2]]\r\n   1  2\r\n0  1  2\r\n1  1  2\r\n\r\n[2 rows x 2 columns]\r\n```\r\nThis is in pandas 0.13.1"""
6544,28736562,rosnfeld,jreback,2014-03-04 20:11:49,2014-06-16 22:01:56,2014-03-05 22:15:50,closed,,0.14.0,14,Bug;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6544,b'Fix irregular Timestamp arithmetic types #6543',"b'closes #6543\r\n\r\nThis is my first PR so I may have done a few things wrong. If I should have added my tests to a different file/location, please let me know.\r\n\r\nI should point out that this code is also tested by many other sections of the pandas codebase, on my first pass I hadn\'t captured the ""Timestamp - datetime"" case and all sorts of things blew up. Better now. :)\r\n'"
6543,28729975,rosnfeld,jreback,2014-03-04 18:43:39,2014-03-05 22:16:05,2014-03-05 22:16:05,closed,,0.14.0,2,Bug;Frequency;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6543,b'inconsistent types resulting from Timestamp arithmetic',"b""Adding vs subtracting a datetime.timedelta from a Timestamp yields different return types:\r\n\r\n```python\r\nIn [1]: timestamp = pd.Timestamp(datetime.datetime(2014, 3, 4))\r\n\r\nIn [2]: timestamp + datetime.timedelta(seconds=1)\r\nOut[2]: Timestamp('2014-03-04 00:00:01', tz=None)\r\n\r\nIn [3]: timestamp - datetime.timedelta(seconds=1)\r\nOut[3]: datetime.datetime(2014, 3, 3, 23, 59, 59)\r\n```\r\n\r\nIt's an easy fix, I have the code change done and I am just polishing up additional tests to document this behavior and ensure nothing else is broken.\r\n\r\nIn #4547 I commented I was looking at another issue but hadn't created an issue for it, this is that issue. I realized it's probably better to have an issue, for release notes and the like.\r\n"""
6542,28729152,jreback,jreback,2014-03-04 18:33:03,2014-07-11 04:46:38,2014-03-04 19:10:20,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6542,b'BUG: Bug in setitem with a duplicate index and an alignable rhs (GH6541)',b'closes #6541'
6541,28725821,jreback,jreback,2014-03-04 17:49:22,2014-03-04 19:10:20,2014-03-04 19:10:20,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6541,b'BUG: duplicate indexed assignement',"b""from SO: http://stackoverflow.com/questions/22178642/assignment-modification-of-values-in-an-indexed-subframe-in-pandas\r\n\r\n```\r\ndata = {'me':list('rttti'),'foo': list('aaade'), 'bar': arange(5)*1.34+2, 'bar2': arange(5)*-.34+2}\r\ndf = pd.DataFrame(data).set_index('me')\r\n\r\ndf.loc['r',['bar','bar2']]*=2.0\r\ndf.loc['t','bar']*=2.5\r\ndf.loc['t',['bar','bar2']]*=2.0\r\n```"""
6536,28705875,dbew,jreback,2014-03-04 13:46:17,2014-03-06 23:01:26,2014-03-06 23:01:26,closed,,0.14.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6536,b'iloc with empty list raises IndexError',"b'When you slice a dataframe with an empty list you get an IndexError instead of an empty dataframe. Slicing with a non-empty list works fine.  Tested in pandas 0.13.1.  This is a regression from pandas 0.11.0.\r\n\r\n\r\n``` python\r\ndf = pd.DataFrame(np.arange(25.0).reshape((5,5)), columns=list(\'abcde\'))\r\n```\r\nSlice with non-empty list.\r\n``` python\r\n[5 rows x 5 columns]\r\ndf.iloc[[1,2,3]]\r\nOut[34]: \r\n    a   b   c   d   e\r\n1   5   6   7   8   9\r\n2  10  11  12  13  14\r\n3  15  16  17  18  19\r\n\r\n[3 rows x 5 columns]\r\n```\r\n\r\nSlice with empty list.\r\n``` python\r\ndf.iloc[[]]\r\nTraceback (most recent call last):\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/ipython-1.1.0_1_ahl1-py2.7.egg/IPython/core/interactiveshell.py"", line 2830, in run_code\r\n    exec code_obj in self.user_global_ns, self.user_ns\r\n  File ""<ipython-input-35-8806137779f8>"", line 1, in <module>\r\n    df.iloc[[]]\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/indexing.py"", line 1028, in __getitem__\r\n    return self._getitem_axis(key, axis=0)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/indexing.py"", line 1238, in _getitem_axis\r\n    return self._get_loc(key, axis=axis)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/indexing.py"", line 73, in _get_loc\r\n    return self.obj._ixs(key, axis=axis)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 1588, in _ixs\r\n    result = self.reindex(i, takeable=True)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 2162, in reindex\r\n    **kwargs)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/generic.py"", line 1565, in reindex\r\n    takeable=takeable).__finalize__(self)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 2117, in _reindex_axes\r\n    fill_value, limit, takeable=takeable)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 2126, in _reindex_index\r\n    takeable=takeable)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/index.py"", line 1233, in reindex\r\n    return self[target], target\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.1-py2.7-linux-x86_64.egg/pandas/core/index.py"", line 624, in __getitem__\r\n    result = arr_idx[key]\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```'"
6531,28637400,immerrr,jreback,2014-03-03 17:04:55,2014-06-12 19:22:01,2014-03-05 11:20:16,closed,,0.14.0,12,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6531,b'BUG/TST: fix several issues with slice bound checking code',"b""This patch removes more of what's left after removing pandas-specific customizations to slice indexing.\r\n\r\nBUG/TST: `obj.iloc[:,-4:-10]` should return empty slice (because slice.step > 0 and slice.start > slice.stop)\r\nThere's no way to describe this by setting stop to a value in  `[0; len]` interval, it should remain negative (see [this ticket in python bugtracker](http://bugs.python.org/issue11842)]. Proposed fix to this issue is to remove the custom logic altogether. A side-effect of that is correct handling of negative steps in presence of out-of-bounds slices, e.g. `obj.iloc[:,10:4:-1]` would've incorrectly returned empty slice.\r\n\r\nBUG: fix exception raised by `Series([1,2,3]).iloc[10:]`.\r\nAfter I've cleaned up suspicious code that handled empty-sequence keys in #6440, this piece of code started causing trouble:\r\n\r\n```python\r\nif start >= l:\r\n    return self._getitem_axis(tuple(),axis=axis)\r\n```\r\n\r\nCLN: #6299 & #6443 rendered `core.indexing._check_slice_bounds` function useless and it's now gone.  Next step would be to drop unused `raise_on_error` parameter in slice-related functions.\r\n\r\n\r\nNot sure, how do I put down all these relations into changelog, advice on that is welcome."""
6530,28636442,jreback,jreback,2014-03-03 16:53:04,2014-06-27 23:40:56,2014-03-03 19:13:31,closed,,0.14.0,0,Bug;Dtypes;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6530,b'BUG: Regression from 0.13 in the treatment of numpy datetime64 non-ns dtypes in Series creation (GH6429)',b'closes #6529'
6518,28574596,hayd,hayd,2014-03-02 06:59:48,2014-03-02 19:15:41,2014-03-02 19:15:41,closed,,,2,Bug;Dtypes;Won't Fix,https://api.github.com/repos/pydata/pandas/issues/6518,b'applymap returns empty list even though function is boolean',"b""Weird behaviour with apply map, the below function should always return a bool.\r\n\r\n```\r\nIn [11]: df = pd.DataFrame({'feat 1': {0: [], 1: 6, 2: None}, 'feat 2': {0: [], 1: 8, 2: 10}, 'feat 3': {0: 5.0, 1: 3.0, 2: np.nan}})\r\n\r\nIn [14]: df.applymap(lambda x: x == [] or x is None)\r\nOut[14]:\r\n  feat 1 feat 2 feat 3\r\n0   True   True  False\r\n1  False  False  False\r\n2   True  False  False\r\n\r\nIn [15]: df.applymap(lambda x: x is None or x == [])\r\nOut[15]:\r\n  feat 1 feat 2 feat 3\r\n0   True   True     []\r\n1  False  False     []\r\n2   True  False     []\r\n```\r\nStrange use case, but maybe there is something weird here.\r\n\r\nhttp://stackoverflow.com/questions/22124537/read-json-populates-with-empty-lists-how-to-remove-those-rows/22124735#22124735"""
6516,28567591,jreback,jreback,2014-03-01 22:37:45,2014-06-15 01:09:02,2014-03-15 14:33:32,closed,,0.14.0,32,API Design;Bug;Enhancement;Groupby;Resample,https://api.github.com/repos/pydata/pandas/issues/6516,b'BUG/API: allow TimeGrouper with other columns in a groupby (GH3794)',"b""closes #3794\r\n\r\n```\r\nIn [3]: df\r\nOut[3]: \r\n  Branch Buyer                Date  Quantity\r\n0      A  Carl 2013-01-01 13:00:00         1\r\n1      A  Mark 2013-01-01 13:05:00         3\r\n2      A  Carl 2013-10-01 20:00:00         5\r\n3      A  Carl 2013-10-02 10:00:00         1\r\n4      A   Joe 2013-10-01 20:00:00         8\r\n5      A   Joe 2013-10-02 10:00:00         1\r\n6      A   Joe 2013-12-02 12:00:00         9\r\n7      B  Carl 2013-12-02 14:00:00         3\r\n\r\n[8 rows x 4 columns]\r\n\r\nIn [4]:    df.groupby([pd.Grouper(freq='1M',key='Date'),'Buyer']).sum()\r\nOut[4]: \r\n                  Quantity\r\nDate       Buyer          \r\n2013-01-31 Carl          1\r\n           Mark          3\r\n2013-10-31 Carl          6\r\n           Joe           9\r\n2013-12-31 Carl          3\r\n           Joe           9\r\n\r\n[6 rows x 1 columns]\r\n\r\nIn [5]:    df = df.set_index('Date')\r\n\r\nIn [6]:    df['Date'] = df.index + pd.offsets.MonthEnd(2)\r\n\r\nIn [9]:    df.groupby([pd.Grouper(freq='6M',key='Date'),'Buyer']).sum()\r\nOut[9]: \r\n                  Quantity\r\nDate       Buyer          \r\n2013-02-28 Carl          1\r\n           Mark          3\r\n2014-02-28 Carl          9\r\n           Joe          18\r\n\r\n[4 rows x 1 columns]\r\n\r\nIn [10]:    df.groupby([pd.Grouper(freq='6M',level='Date'),'Buyer']).sum()\r\nOut[10]: \r\n                  Quantity\r\nDate       Buyer          \r\n2013-01-31 Carl          1\r\n           Mark          3\r\n2014-01-31 Carl          9\r\n           Joe          18\r\n\r\n[4 rows x 1 columns]\r\n\r\n```"""
6512,28538217,hayd,hayd,2014-02-28 22:25:05,2014-03-11 00:54:30,2014-03-11 00:54:21,closed,,0.14.0,5,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6512,"b""groupby filter can't access all columns""","b""Unexpectedly? don't have access to the grouped columns.\r\n\r\nAlso seems to allow returning a boolean Series (and takes the first item as the condition).\r\n\r\n```\r\nIn [11]: df = pd.DataFrame([[1, 2], [1, 3], [5, 6]], columns=['A', 'B'])\r\n\r\nIn [12]: g = df.groupby('A')  # same with as_index=False (which *correctly* has no effect)\r\n\r\nIn [13]: g.filter(lambda x: x['A'].sum() == 2)\r\nKeyError: u'no item named A'\r\n\r\nIn [14]: g.filter(lambda x: x['B'].sum() == 5)  # works\r\nOut[14]:\r\n   A  B\r\n0  1  2\r\n1  1  3\r\n\r\nnamed A'\r\n\r\nIn [14]: g.filter(lambda x: x['B'].sum() == 5)  # works\r\nOut[14]:\r\n   A  B\r\n0  1  2\r\n1  1  3\r\n\r\nIn [15]: g.filter(lambda x: x.sum() == 5)  # weird that this works (excepted raise)\r\nOut[15]:\r\n   A  B\r\n0  1  2\r\n1  1  3\r\n\r\nIn [16]: g = df.groupby(df['A'])  # hack/workaround\r\n\r\nIn [16]: g.filter(lambda x: x.sum() == 5)  # seems to look at first col\r\nOut[16]:\r\n   A  B\r\n2  5  6\r\n\r\nIn [17]: g.filter(lambda x: x['A'].sum() == 5)  # works\r\nOut[17]:\r\n   A  B\r\n2  5  6\r\n\r\nIn [18]: g.filter(lambda x: x['B'].sum() == 5)  # works\r\nOut[18]:\r\n   A  B\r\n0  1  2\r\n1  1  3\r\n```\r\n\r\ncc @danielballan"""
6509,28517347,bashtage,jorisvandenbossche,2014-02-28 17:13:47,2014-03-11 12:05:51,2014-03-11 12:05:51,closed,,0.14.0,11,Bug;Dtypes;IO SQL,https://api.github.com/repos/pydata/pandas/issues/6509,b'BUG: SQL writing can lead to data loss',"b""`to_sql` uses `iterrows` which leads to data conversion, and in the case of mixed data types, can lead to data loss.\r\n\r\n```python\r\ns1 = pd.Series([0],dtype=np.float32)\r\ns2 = pd.Series([2**27 + 1],dtype=np.int32)\r\ndf = pd.DataFrame({'float':s1, 'int':s2})\r\n\r\nfor row in df.iterrows():\r\n    print row[1][1] - (2**27 + 1)\r\n```\r\n(The same issue applies when using `df.to_sql`)\r\n\r\nI found the same bug in `to_stata` and have submitted a PR on it.  \r\n\r\n`to_sql` is the only other location in pandas the uses `iterrows`, which should probably be avoided in all non-user code.\r\n\r\nIt should be an easy fix using something like `itertuple` - I don't use the SQL code so I'm not in a good position to fix this problem."""
6506,28508990,jreback,jreback,2014-02-28 15:32:01,2014-07-01 14:57:33,2014-02-28 16:06:28,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6506,b'BUG: Bug in multi-axis indexing using .loc on non-unique indices (GH6504)',b'closes #6504'
6505,28506899,wabu,jreback,2014-02-28 15:06:06,2014-05-27 12:16:05,2014-04-06 17:43:30,closed,,0.14.0,11,Bug;Can't Repro;IO HDF5;Unicode,https://api.github.com/repos/pydata/pandas/issues/6505,b'BUG: HDFStore utf8 corrupted reads',"b""When reading back utf8 encoded data, it randomly is corrupted when read back.\r\nThe output contains random values or fails with an ``UnicodeDecodeError``.\r\n\r\nAfter commenting out the fast version (``astype``) in ``_unconvert_string_array`` the issue did not show up anymore.\r\n\r\npyton 3.3, numpy 1.8.0, pytables 3.1.0\r\n\r\nI'll try to get a reproducible test up in the next days."""
6504,28501267,jorisvandenbossche,jreback,2014-02-28 13:44:27,2014-02-28 16:06:28,2014-02-28 16:06:28,closed,,0.14.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6504,b'KeyError when indexing both axes with .loc[] with duplicate indices',"b""The last line of the following code snippet does not work, while it seems it should. Am I overlooking something?\r\n```\r\ndf = pd.DataFrame(np.random.randn(9,2), index=[1,1,1,2,2,2,3,3,3], columns=['a', 'b'])\r\ndf\r\ndf.loc[[1, 2]]    # works\r\ndf.loc[:,['a', 'b']]    # works\r\ndf.loc[[1, 2], ['a', 'b']]    # does not work\r\n```\r\nYou don't have this with an index without duplicates.\r\n```\r\nIn [1]: df = pd.DataFrame(np.random.randn(9,2), index=[1,1,1,2,2,2,3,3,3], colum\r\nns=['a', 'b'])\r\n\r\nIn [2]: df\r\nOut[2]:\r\n          a         b\r\n1  0.714735 -1.125141\r\n1  0.061066 -0.737312\r\n1  0.269988  1.434862\r\n2  0.336829  1.040681\r\n2  0.929521 -1.142978\r\n2  0.272813 -0.370184\r\n3 -0.520348  1.126233\r\n3  0.232261 -0.075287\r\n3 -0.314570 -2.635400\r\n\r\n[9 rows x 2 columns]\r\n\r\nIn [3]: df.loc[[1, 2]]\r\nOut[3]:\r\n          a         b\r\n1  0.714735 -1.125141\r\n1  0.061066 -0.737312\r\n1  0.269988  1.434862\r\n2  0.336829  1.040681\r\n2  0.929521 -1.142978\r\n2  0.272813 -0.370184\r\n\r\n[6 rows x 2 columns]\r\n\r\nIn [4]: df.loc[:,['a', 'b']]\r\nOut[4]:\r\n          a         b\r\n1  0.714735 -1.125141\r\n1  0.061066 -0.737312\r\n1  0.269988  1.434862\r\n2  0.336829  1.040681\r\n2  0.929521 -1.142978\r\n2  0.272813 -0.370184\r\n3 -0.520348  1.126233\r\n3  0.232261 -0.075287\r\n3 -0.314570 -2.635400\r\n\r\n[9 rows x 2 columns]\r\n\r\nIn [5]: df.loc[[1, 2], ['a', 'b']]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-617013ef905a> in <module>()\r\n----> 1 df.loc[[1, 2], ['a', 'b']]\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in __getitem__(sel\r\nf, key)\r\n   1152     def __getitem__(self, key):\r\n   1153         if type(key) is tuple:\r\n-> 1154             return self._getitem_tuple(key)\r\n   1155         else:\r\n   1156             return self._getitem_axis(key, axis=0)\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in _getitem_tuple(\r\nself, tup)\r\n    680         # ugly hack for GH #836\r\n    681         if self._multi_take_opportunity(tup):\r\n--> 682             return self._multi_take(tup)\r\n    683\r\n    684         # no shortcut needed\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in _multi_take(sel\r\nf, tup)\r\n    726             return o.reindex(**d)\r\n    727         except:\r\n--> 728             raise self._exception\r\n    729\r\n    730     def _convert_for_reindex(self, key, axis=0):\r\n\r\nKeyError:\r\n```"""
6495,28420434,jreback,jreback,2014-02-27 13:58:22,2014-06-19 18:33:42,2014-02-27 14:45:14,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6495,b'BUG: Bug in iat/iloc with duplicate indices on a Series (6493)',b'closes #6493'
6493,28394427,zhangruoyu,jreback,2014-02-27 05:07:07,2014-02-27 14:45:25,2014-02-27 14:45:14,closed,,0.14.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6493,"b""iat, iloc don't work with no unique index""","b'```\r\nimport pandas as pd\r\n\r\ns = pd.Series(range(5), index=[1,1,2,2,3])\r\ns.iat[2]\r\n```\r\nwhich returns `array([2, 3], dtype=int64)`, I think the result should be `2`.\r\n\r\n`s.iloc[2]` works, but `s.iloc[[2, 3]]` raise error.'"
6477,28285889,jreback,jreback,2014-02-25 21:26:09,2014-06-16 06:11:09,2014-02-25 21:29:11,closed,,0.14.0,0,Bug;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/6477,b'BUG: Bug in resample with a timezone and certain offsets (GH6397)',b'closes #6397'
6476,28284891,jseabold,jreback,2014-02-25 21:12:43,2015-10-20 22:24:57,2015-10-20 22:24:57,closed,,,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/6476,b'xsel not working?',"b""I was doing maintenance on an old box, and I just noticed that `clipboard_set` didn't seem to work for me when I only had `xsel` installed and not `xclip`. Installing `xclip` things work fine. Possible that this was a local issue, but just wanted to park this here in case anyone comes across it. """
6475,28279989,jreback,jreback,2014-02-25 20:25:11,2014-07-02 17:34:49,2014-02-25 20:43:00,closed,,0.14.0,0,Bug;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/6475,b'BUG: Bug in sum of a timedelta64[ns] series (GH6462)',b'closes #6462'
6470,28229869,kdiether,jreback,2014-02-25 08:25:23,2014-06-18 12:15:40,2014-02-25 18:55:55,closed,,0.14.0,5,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/6470,"b'BUG: Fixes error in DataReader when getting ""F-F_Momentum_Factor"" data'","b'Hopefully I didn\'t screw-up using git and github. This PR fixes the minor issue outlined in `GH6460`. I didn\'t write any new tests but I added the ""F-F_Momentum_Factor"" data request to the existing tests of the famafrench data reader.\r\n\r\ncloses #6460'"
6467,28201426,cpcloud,cpcloud,2014-02-24 22:01:52,2014-07-07 14:22:54,2014-02-24 23:35:20,closed,cpcloud,0.14.0,0,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6467,b'BUG: split should respect maxsplit when no pat is given',b'closes #6466'
6466,28197745,behzadnouri,cpcloud,2014-02-24 21:17:53,2014-02-24 23:35:20,2014-02-24 23:35:20,closed,cpcloud,0.14.0,1,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6466,b'str.split ignores n if pat is None',"b'as observed in: http://stackoverflow.com/questions/21998590/\r\n`str.split` ignores `n` if `pat` is `None`\r\n\r\nrelevant source code:\r\n\r\n    # pandas/core/strings.py\r\n    def str_split(arr, pat=None, n=None):\r\n        if pat is None:\r\n            if n is None or n == 0:\r\n                n = -1\r\n            f = lambda x: x.split()\r\n        ....'"
6465,28194408,jreback,jreback,2014-02-24 20:39:32,2014-07-09 18:05:14,2014-02-24 21:32:47,closed,,0.14.0,0,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/6465,b'COMPAT: infer_freq compat on passing an Index of strings (GH6463)',b'closes #6463'
6463,28176933,jseabold,jreback,2014-02-24 16:54:02,2014-02-24 21:34:44,2014-02-24 21:32:47,closed,jreback,0.14.0,3,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/6463,b'infer_freq fails on Index',"b""Continuing with the my projects are the pandas test suite theme of issue reports. This broke for me on a recent upgrade. \r\n\r\nOk.\r\n\r\n    pd.infer_freq(['2004-01', '2004-02', '2004-03', '2004-04'])\r\n\r\nNo longer works.\r\n\r\n    pd.infer_freq(pd.Index(['2004-01', '2004-02', '2004-03', '2004-04']))\r\n\r\nSeems like this should still work.\r\n\r\n    [12]: pd.version.version\r\n    [12]: '0.13.1-254-g150f323'\r\n\r\n\r\n"""
6462,28156697,jreback,jreback,2014-02-24 12:12:04,2014-02-25 20:43:00,2014-02-25 20:43:00,closed,jreback,0.14.0,0,Bug;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/6462,b'BUG: inconsistency in some timedelta64[ns] reduction operations',"b""```\r\nIn [1]: s = pd.to_timedelta(range(4),unit='d')\r\n\r\nIn [2]: s\r\nOut[2]: \r\n0   0 days\r\n1   1 days\r\n2   2 days\r\n3   3 days\r\ndtype: timedelta64[ns]\r\n\r\nIn [3]: s.mean()\r\nOut[3]: \r\n0   1 days, 12:00:00\r\ndtype: timedelta64[ns]\r\n\r\nIn [4]: s.sum()\r\nOut[4]: 518400000000000\r\n```"""
6460,28141425,kdiether,jreback,2014-02-24 06:48:21,2014-04-02 14:16:49,2014-02-25 18:56:13,closed,,0.14.0,5,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/6460,"b""famafrench, pandas.io.data.DataReader error for 'F-F_Momentum_Factor' data""","b'I think there is a very minor issue in `pandas.io.data.DataReader` for pandas 0.13.1 caused by an inconsistent extension in Ken French\'s data library. The DataReader can\'t process the \'F-F_Momentum_Factor\' data from Ken French\'s website:\r\n\r\n```python\r\nimport pandas as pd\r\nimport pandas.io.data as web\r\n\r\nmom = web.DataReader(""F-F_Momentum_Factor"", ""famafrench"")\r\n```\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File ""./example.py"", line 6, in <module>\r\n    mom = web.DataReader(""F-F_Momentum_Factor"", ""famafrench"")\r\n  File ""/usr/lib64/python2.7/site-packages/pandas/io/data.py"", line 85, in DataReader\r\n    return get_data_famafrench(name)\r\n  File ""/usr/lib64/python2.7/site-packages/pandas/io/data.py"", line 497, in get_data_famafrench\r\n    data = zf.open(name + \'.txt\').readlines()\r\n  File ""/usr/lib64/python2.7/zipfile.py"", line 957, in open\r\n    zinfo = self.getinfo(name)\r\n  File ""/usr/lib64/python2.7/zipfile.py"", line 905, in getinfo\r\n    \'There is no item named %r in the archive\' % name)\r\nKeyError: ""There is no item named \'F-F_Momentum_Factor.txt\' in the archive""\r\n```\r\n\r\nThe issue appears to be caused by the fact that when \'F-F_Momentum_Factor.zip\' is unzipped the underlying file is \'F-F_Momentum_Factor.TXT\' and  `get_data_famafrench(name)`  in `data.py` assumes the extension will be lower case (I believe this is true for all the other data files on Ken\'s website but for whatever reason has never been true for the momentum factor file). Here is the relevant code in `get_data_famafrench(name)`:\r\n```python\r\nwith ZipFile(tmpf, \'r\') as zf:\r\n    data = zf.open(name + \'.txt\').readlines()\r\n```\r\nThere is probably a better solution to the issue but I changed the preceding to the following and it seems to work:\r\n```python\r\nwith ZipFile(tmpf, \'r\') as zf:\r\n    data = zf.open(zf.namelist()[0]).readlines()\r\n```\r\nExample:\r\n```python\r\nmom = web.DataReader(""F-F_Momentum_Factor"", ""famafrench"")[1]\r\nprint mom.head(10)\r\n```\r\nOutput:\r\n\r\n|        |   1 Mom |\r\n|-------:|--------:|\r\n| 192701 |    0.49 |\r\n| 192702 |   -0.69 |\r\n| 192703 |    5.41 |\r\n| 192704 |    3.83 |\r\n| 192705 |    3.73 |\r\n| 192706 |   -0.65 |\r\n| 192707 |    5.03 |\r\n| 192708 |    1.15 |\r\n| 192709 |    1.55 |\r\n| 192710 |   -0.07 |\r\n\r\nAlso here is the output from `pd.show_versions()`:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.3-201.fc20.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.1\r\nCython: 0.20\r\nnumpy: 1.8.0\r\nscipy: 0.12.1\r\nstatsmodels: 0.5.0\r\nIPython: 0.13.2\r\nsphinx: 1.1.3\r\npatsy: 0.2.1\r\nscikits.timeseries: None\r\ndateutil: 2.2\r\npytz: 2013.9\r\nbottleneck: 0.8.0\r\ntables: 3.0.0\r\nnumexpr: 2.3\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.3\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.2\r\nsqlalchemy: None\r\nlxml: 3.2.4\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\n\r\n```\r\n\r\n\r\n'"
6451,28123860,shoyer,jreback,2014-02-23 18:30:44,2014-06-17 01:05:26,2014-02-23 21:59:25,closed,,0.14.0,1,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6451,b'BUG FIX: cartesian_product now converts all arguments to ndarrays',b'Fixes #6439.'
6443,28097003,jreback,jreback,2014-02-22 14:39:38,2014-06-25 15:14:08,2014-02-22 15:13:36,closed,,0.14.0,0,Bug;Indexing;Testing,https://api.github.com/repos/pydata/pandas/issues/6443,b'BUG/TST: iloc will now raise IndexError on out-of-bounds list indexers (GH6296 / GH6299)',"b'TST: fix spurious tests for test_groupby, closes #6436\r\nBUG/TST: iloc will now raise IndexError on out-of-bounds list indexers promotoing consistency with python/numpy syntax. The out-of-bounds for slice indexers will continue to work (again for consistency) (#6296 / #6299)\r\nDOC: makes tz attribute default to ``None`` to remove build-doc warnings'"
6439,28085211,shoyer,jreback,2014-02-22 01:09:16,2014-07-01 07:16:44,2014-02-23 21:59:25,closed,,0.14.0,5,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6439,"b""BUG: pandas.tools.util.cartesian_product doesn't work on DatetimeIndex objects""","b""This breaks the new `pandas.MultiIndex.from_product` function when one of the product arrays is DatetimeIndex. Observe:\r\n```\r\n>>> import pandas as pd\r\n>>> from pandas.tools.util import cartesian_product\r\n>>> x, y = cartesian_product([[1, 2], pd.date_range('2000-01-01', periods=2)])\r\n>>> print x\r\n[1 1 2 2]\r\n>>> print y.values\r\n['1999-12-31T16:00:00.000000000-0800' '1999-12-31T16:00:00.000000000-0800'\r\n '2000-01-01T16:00:00.000000000-0800' '2000-01-01T16:00:00.000000000-0800']\r\n```\r\nIf `cartesian_product` was working properly, the dates should actually be [1999, 2000, 1999, 2000].\r\n\r\nThe simple fix would be to convert everything into a numpy array before calculating the cartesian product."""
6437,28071571,jseabold,jreback,2014-02-21 20:45:03,2015-10-20 22:24:43,2015-10-20 22:24:43,closed,,,7,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6437,b'Losing index and sort on merge',"b'I\'m almost positive I\'ve filed an issue about this before, but I don\'t find it. This is a pretty common operation when working with panel data. You want to merge in some constant values into a DataFrame that has repeated keys but unique index.\r\n\r\n    df = pd.util.testing.makeDataFrame() # yes!\r\n    df[""group""] = 0\r\n    df.ix[range(0, 20, 2), ""group""] = 1.\r\n    df[""time""] = np.random.randint(5, size=len(df))\r\n    df_x = pd.DataFrame(np.c_[np.r_[np.arange(5), np.arange(5)], \r\n                            np.repeat([0,1], 5)], \r\n                        columns=[""time"", ""group""])\r\n    df_x[""constant_info""] = np.random.randn(len(df_x))\r\n\r\n    df.merge(df_x, on=[""time"", ""group""])\r\n\r\nWhere\'d my index go? Where\'d my sort go, so I could even recover the index?\r\n\r\n    [~/]\r\n    [65]: pd.version.version\r\n    [65]: \'0.13.1-254-g150f323\'\r\n\r\n\r\n'"
6436,28069393,cpcloud,jreback,2014-02-21 20:11:55,2014-02-22 15:13:36,2014-02-22 15:13:36,closed,,0.14.0,3,Bug;Groupby;Testing,https://api.github.com/repos/pydata/pandas/issues/6436,b'Possibly spurious GroupBy failure on Travis',"b""Can't really investigate right now\r\n\r\n\r\nhttps://travis-ci.org/pydata/pandas/jobs/19331373"""
6421,27975073,jreback,jreback,2014-02-20 16:32:36,2014-06-13 10:02:08,2014-02-20 16:48:06,closed,,0.14.0,0,Bug;Indexing;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6421,b'REGR: Bug in Series.reindex when specifying a method with some nan values was inconsistent (GH6418)',b'closes #6418'
6419,27962589,jreback,jreback,2014-02-20 13:50:35,2014-06-18 04:07:44,2014-02-20 14:22:57,closed,,0.14.0,0,Bug;Dtypes;Timezones,https://api.github.com/repos/pydata/pandas/issues/6419,b'BUG: inconcistency in contained values of a Series created from a DatetimeIndex with a tz',b'related #6032'
6418,27962228,meelmaar,jreback,2014-02-20 13:44:56,2014-02-20 16:54:29,2014-02-20 16:48:06,closed,,0.14.0,5,Bug;Indexing;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6418,b'BUG: reindex() and reindex_like() fill behavior is different in pandas 12.0 and 13.1?',"b'I just came across an issue which caused me serious troubles since upgrading from pandas 12.0 to 13.1. It happens when using a fill method with a reindex() or reindex_like() method. Moreover, those method are not giving consistent results anymore! I have not tested how this issue or originates from changed .ffill() and similar method, but I see it propagates to resample(). Could not find any recent mentioning of the strange behavior and no hints in the docs or What\'s New section. \r\n\r\nThis is the problem I encounter using pandas 12.0 (with numpy 1.7.1, in both, 32bit Python 2.7.5 Python x,y and 64bit, WinPython-64bit-2.7.4.1; windows 7) and pandas 13.1 (D:\\PortableApps\\WinPython-64bit-2.7.6.2, numpy 1.8.0). Pandas 12.0 behavior is the same for the 32 bit and 64 bit versions, so this cannot explain the problem.\r\n\r\nCode:\r\n```\r\nimport pandas as pd\r\n# Make low frequency timeseries:\r\ni30 = index=pd.date_range(\'2002-02-02\', periods=4, freq=\'30T\')\r\ns=pd.Series(np.arange(4.), index=i30)\r\ns[2] = np.NaN \r\n\r\n# Upsample by factor 3 with reindex() and resample() methods:\r\ni10 = pd.date_range(i30[0], i30[-1], freq=\'10T\')\r\ns10 = s.reindex(index=i10, method=\'bfill\')\r\ns10_2 = s.reindex(index=i10, method=\'bfill\', limit=2)\r\nr10 = s.resample(\'10Min\', fill_method=\'bfill\')\r\nr10_2 = s.resample(\'10Min\', fill_method=\'bfill\', limit=2)\r\n```\r\nIn pandas 12.0: s10 equals s10_2 equals r10 equals r10_2\r\n```\r\ns10\r\nOut[60]: \r\n2002-02-02 00:00:00     0\r\n2002-02-02 00:10:00     1\r\n2002-02-02 00:20:00     1\r\n2002-02-02 00:30:00     1\r\n2002-02-02 00:40:00   NaN\r\n2002-02-02 00:50:00   NaN\r\n2002-02-02 01:00:00   NaN\r\n2002-02-02 01:10:00     3\r\n2002-02-02 01:20:00     3\r\n2002-02-02 01:30:00     3\r\nFreq: 10T, dtype: float64\r\n```\r\nIn pandas 13.1: s10 does not equal s10_2; s10 has all NaN\'s filled\r\n```\r\ns10\r\nOut[120]: \r\n2002-02-02 00:00:00    0\r\n2002-02-02 00:10:00    1\r\n2002-02-02 00:20:00    1\r\n2002-02-02 00:30:00    1\r\n2002-02-02 00:40:00    3\r\n2002-02-02 00:50:00    3\r\n2002-02-02 01:00:00    3\r\n2002-02-02 01:10:00    3\r\n2002-02-02 01:20:00    3\r\n2002-02-02 01:30:00    3\r\nFreq: 10T, dtype: float64\r\n```\r\nSame holds for resampled series r10\r\nConclusion: in pandas 13.1, all is filled if limit=None which breaks with the pandas 12.0 behavior. I think the 12.0 behavior is mre sensible; only fill the gaps created from upsampling. \r\nThis even more import for the reindex_like method because there the ""limit"" key cannot limit which gaps are filled in pandas 13.1:\r\n```\r\ns.reindex_like(s10, method=\'bfill\', limit=2)\r\nOut[121]: \r\n2002-02-02 00:00:00    0\r\n2002-02-02 00:10:00    1\r\n2002-02-02 00:20:00    1\r\n2002-02-02 00:30:00    1\r\n2002-02-02 00:40:00    3\r\n2002-02-02 00:50:00    3\r\n2002-02-02 01:00:00    3\r\n2002-02-02 01:10:00    3\r\n2002-02-02 01:20:00    3\r\n2002-02-02 01:30:00    3\r\nFreq: 10T, dtype: float64\r\n```\r\nHope this is clear and I can be reproduced? I hope this can be fixed soon. But of course, if you can reproduce this behavior and it has indeed change from 12.0 to 13.1, this should be in the docs\r\n \r\n\r\n'"
6410,27878409,jreback,jreback,2014-02-19 14:29:29,2014-02-19 20:37:41,2014-02-19 17:48:54,closed,,0.14.0,10,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/6410,b'BUG: 32/64-bit issue with indexing',"b'https://groups.google.com/forum/#!topic/pydata/90QNz8EY74Q\r\n\r\nfails on 32-bit\r\n```\r\nIn [1]: df = pd.DataFrame({""A"": range(2), ""B"": [pd.Timestamp(\'2000-01-1\')]*2})\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A          B\r\n0  0 2000-01-01\r\n1  1 2000-01-01\r\n\r\n[2 rows x 2 columns]\r\n\r\nIn [3]: df.groupby(""A"")[""B""].transform(min)\r\nOut[3]: \r\n0   2000-01-01\r\n1   2000-01-01\r\nName: B, dtype: datetime64[ns]\r\n```'"
6403,27843453,myourshaw,jreback,2014-02-19 01:12:46,2015-06-20 16:20:11,2015-06-20 16:20:11,closed,,0.17.0,2,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/6403,"b'ExcelFile parse of empty sheet fails with ""IndexError: list index out of range""'","b'Using pandas 0.13.1 on OS X Mavericks to parse a blank Excel spreadsheet causes ""IndexError: list index out of range"". Apparently the default header=0 in ```_parse_excel``` causes the execution of ```_trim_excel_header(data[header])```. Perhaps when nrows==0 this should not be executed.\r\n\r\n```python\r\nimport pandas as pd\r\nxl_file = pd.ExcelFile(\'blank.xlsx\')\r\nxl_file.parse(\'Sheet1\') #Sheet1 has no data\r\n```\r\n\r\nSTDERR:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/Users/myourshaw/lab/pypeline/python2/excel_example.py"", line 10, in <module>\r\n    xl_file.parse(\'Sheet1\')\r\n  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/excel.py"", line 208, in parse\r\n    **kwds)\r\n  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/excel.py"", line 291, in _parse_excel\r\n    data[header] = _trim_excel_header(data[header])\r\nIndexError: list index out of range\r\n```'"
6401,27834612,jreback,jreback,2014-02-18 22:35:45,2014-06-15 18:24:42,2014-02-19 00:13:47,closed,,0.14.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6401,b'BUG: Float64Index with nans not comparing correctly',b'relalted to #5231'
6400,27823966,jdreaver,jreback,2014-02-18 20:15:01,2014-07-17 17:35:09,2014-02-18 20:41:44,closed,,0.14.0,2,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/6400,b'BUG: Fix mergesort unstable when ascending=False (GH6399)',"b'closes #6399. I hope I followed all of the correct procedures.\r\n\r\nI ran the full test suite (test.sh), and there were no errors.'"
6399,27818224,jdreaver,jreback,2014-02-18 18:59:08,2014-03-10 13:29:59,2014-02-18 20:41:55,closed,,0.14.0,2,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/6399,b'BUG: Mergesort is unstable when ascending=False',"b""### The Issue\r\n\r\nWhen using `DataFrame.sort_by(kind='mergesort')`, sorting is supposed to be stable. Unfortunately, that is not the case when `ascending=False`.\r\n\r\n```python\r\n# Create a DataFrame where the first column is already in descending order.\r\nIn [96]: df = pd.DataFrame([[2, 'first'], [2, 'second'], [1, 'a'], [1, 'b']], columns=['sort_col', 'order'])\r\n\r\nIn [97]: df\r\nOut[97]: \r\n   sort_col   order\r\n0         2   first\r\n1         2  second\r\n2         1       a\r\n3         1       b\r\n\r\n[4 rows x 2 columns]\r\n\r\n# Look at the 'order' column. Clearly not stable.\r\nIn [98]: df.sort_index(by='sort_col', kind='mergesort', ascending=False)\r\nOut[98]: \r\n   sort_col   order\r\n1         2  second\r\n0         2   first\r\n3         1       b\r\n2         1       a\r\n\r\n[4 rows x 2 columns]\r\n```\r\n\r\n### More Info\r\nInside the `sort_by()` source code, `argsort()` is called on the sorted column. Then, `ascending` is checked and if it is False, the indexes are simply reversed. Here is the relevant code snippet in `pandas/core/frame.py`:\r\n\r\n```python\r\nk = self[by].values\r\n...\r\nindexer = k.argsort(kind=kind)\r\n...\r\nif not ascending:\r\n    indexer = indexer[::-1]\r\n```\r\n\r\nSince numpy always sorts in ascending order, this actually guarantees sorting is always *unstable*! Check this out:\r\n\r\n```python\r\nIn [110]: df['sort_col'].values.argsort(kind='mergesort')[::-1]\r\nOut[110]: array([1, 0, 3, 2])\r\n```\r\n\r\nClearly, simply reversing the indices doesn't work. We need to sort a reversed `k`, reverse the indices, and then subtract the indices from the highest index so they correspond to the original `k`:\r\n\r\n```python\r\nIn [112]: 3 - df['sort_col'].values[::-1].argsort(kind='mergesort')[::-1]\r\nOut[112]: array([0, 1, 2, 3])\r\n```\r\n\r\nThe workaround in my code is to stable sort descending is reverse the DataFrame, sort ascending, and reverse again.\r\n\r\nWhat is the best way to fix this? This is probably an easy fix, but I've never contributed to pandas, so I need to set up my fork and make sure I can run tests before working on a pull request.\r\n\r\n### My Versions\r\nHere are my versions:\r\n\r\n```python\r\n\r\nIn [75]: show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.3.3.final.0\r\nOS: Linux\r\nRelease: 3.12.9-2-ARCH\r\nProcessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.0\r\nCython: 0.20\r\nNumpy: 1.8.0\r\nScipy: 0.13.0\r\nstatsmodels: Not installed\r\n    patsy: Not installed\r\nscikits.timeseries: Not installed\r\ndateutil: 2.2\r\npytz: 2013.9\r\nbottleneck: Not installed\r\nPyTables: Not Installed\r\n    numexpr: Not Installed\r\nmatplotlib: 1.3.1\r\nopenpyxl: 1.8.2\r\nxlrd: 0.9.2\r\nxlwt: Not installed\r\nxlsxwriter: Not installed\r\nsqlalchemy: Not installed\r\nlxml: Not installed\r\nbs4: Not installed\r\nhtml5lib: Not installed\r\nbigquery: Not installed\r\napiclient: Not installed\r\n\r\n```"""
6397,27800614,jpdus,jreback,2014-02-18 15:18:06,2014-02-25 21:29:11,2014-02-25 21:29:11,closed,jreback,0.14.0,3,Bug;Resample;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/6397,"b'Resample with period=\'AS\'/\'A\' does not work on localized indexes for multiple methods in ""how""'","b'I encountered a similar issue to (closed) #3592 .\r\n\r\nWhen trying to resample a dataframe with a localized Datetime Index using the how method:\r\n\r\n```python\r\nrng = pd.date_range(\'1/1/2011\', periods=20000, freq=\'H\')\r\nrng = rng.tz_localize(\'EST\')\r\nts = pd.DataFrame(index=rng)\r\nts[\'first\']=np.random.randn(len(rng))\r\nts[\'second\']=np.cumsum(np.random.randn(len(rng)))\r\nts2=ts.resample(\'A\', how={\'first\':np.sum, \'second\':np.mean})\r\n\r\n*File ""tslib.pyx"", line 465, in pandas.tslib._Timestamp.__richcmp__ (pandas\\tslib.c:9313)\r\n*TypeError: can\'t compare offset-naive and offset-aware datetimes\r\n```\r\n\r\nFor other (,lower) frequencies like ""M"" the same code works fine and if I just use \r\n\r\n    how=np.mean\r\n\r\nor just use 1 column in the dict\r\n\r\n    how={\'first\':np.mean}\r\n\r\nit works too.\r\n\r\npython 2.7.5\r\npandas 0.13.1\r\n\r\nEDIT: Changed to include full example'"
6396,27794480,jreback,jreback,2014-02-18 13:54:20,2014-06-27 14:26:56,2014-02-18 14:41:25,closed,,0.14.0,11,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6396,b'BUG: Regression in chained getitem indexing with embedded list-like from 0.12 (GH6394)',b'closes #6394'
6394,27787649,dhirschfeld,jreback,2014-02-18 11:46:19,2014-02-21 02:11:39,2014-02-18 14:41:25,closed,,0.14.0,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6394,b'Indexing Regression in 0.13.0',"b'In pandas 0.12 the order you indexed a DataFrame didn\'t matter, which I think is the correct behaviour:\r\n```python\r\nIn [6]: df = pd.DataFrame({\'A\': 5*[np.zeros(3)], \'B\':5*[np.ones(3)]})\r\n\r\nIn [7]: df\r\nOut[7]: \r\n\r\n\tA\tB\r\n0\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n1\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n2\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n3\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n4\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n\r\nIn [8]: df[\'A\'].iloc[2]\r\nOut[8]: array([ 0., 0., 0.])\r\n\r\nIn [9]: df.iloc[2][\'A\']\r\nOut[9]: array([ 0., 0., 0.])\r\n\r\nIn [10]: pd.__version__\r\nOut[10]: \'0.12.0\'\r\n\r\nIn [11]: assert type(df.ix[2, \'A\']) == type(df[\'A\'].iloc[2]) == type(df.iloc[2][\'A\'])\r\n\r\nIn [12]: \r\n```\r\nIn pandas 0.13 if you index in a different order you can get a different type out which can be problematic for code expecting an array, especially because of the difference between array indexing and label indexing.\r\n```python\r\nIn [1]: df = pd.DataFrame({\'A\': 5*[np.zeros(3)], \'B\':5*[np.ones(3)]})\r\n\r\nIn [2]: df\r\nOut[2]: \r\n\r\n\tA\tB\r\n0\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n1\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n2\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n3\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n4\t[0.0, 0.0, 0.0]\t[1.0, 1.0, 1.0]\r\n5 rows \xa1\xc1 2 columns \r\n\r\nIn [3]: df[\'A\'].iloc[2]\r\nOut[3]: array([ 0., 0., 0.])\r\n\r\nIn [4]: df.iloc[2][\'A\']\r\nOut[4]: \r\nA 0\r\nA 0\r\nA 0\r\nName: 2, dtype: float64\r\n\r\nIn [5]: pd.__version__\r\nOut[5]: \'0.13.1\'\r\n\r\nIn [6]: assert type(df.ix[2, \'A\']) == type(df[\'A\'].iloc[2]) == type(df.iloc[2][\'A\'])\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-11-946e15564ee1>"", line 1, in <module>\r\n    assert type(df.ix[2, \'A\']) == type(df[\'A\'].iloc[2]) == type(df.iloc[2][\'A\'])\r\n\r\nAssertionError\r\n```\r\n'"
6391,27760791,ischwabacher,jreback,2014-02-18 00:53:19,2014-06-21 12:31:07,2014-02-18 19:27:13,closed,,0.14.0,5,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/6391,b'BUG: Fix behavior of to_offset with leading zeroes',"b""Currently, `pandas.tseries.frequencies.to_offset` erroneously returns a zero time offset when the first part of its argument has a numerical value of zero, even if later parts have nonzero values.  For instance,\r\n\r\n    In [123]: pandas.tseries.frequencies.to_offset('00H 00T 01S')\r\n    Out[123]: <0 * Days>\r\n\r\nThis change checks the sign of the numeric part of the argument before converting to `int` in order to support offsets like `'-00H 00T 01S'`."""
6388,27753800,jreback,jreback,2014-02-17 22:32:42,2014-06-12 16:28:01,2014-02-17 23:09:10,closed,,0.14.0,0,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6388,b'BUG: Bug in DataFrame.dropna with duplicate indices (GH6355)',b'closes #6355'
6385,27746626,jreback,jreback,2014-02-17 20:18:46,2014-06-14 14:30:48,2014-02-17 20:34:48,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6385,"b'BUG: Bug in Series.get, was using a buggy access method (GH6383)'",b'closes #6383'
6383,27743301,jseabold,jreback,2014-02-17 19:22:43,2014-02-17 20:34:48,2014-02-17 20:34:48,closed,,0.14.0,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6383,b'Bug in series.get?',"b'This showed up in the test suite for a PR for statsmodels. I have very little idea where this Series came from or why it\'s using get like this, but here it is anyway.\r\n\r\n    df = pd.Series(np.array([43, 48, 60, 48, 50, 51, 50, 45, 57, 48, 56, \r\n                            45, 51, 39, 55, 43, 54, 52, 51, 54]), \r\n                      index=pd.Float64Index([25.0, 36.0, 49.0, 64.0, 81.0, 100.0, \r\n                                      121.0, 144.0, 169.0, 196.0, 1225.0, \r\n                                      1296.0, 1369.0, 1444.0, 1521.0, 1600.0,\r\n                                      1681.0, 1764.0, 1849.0, 1936.0], \r\n                                      dtype=\'object\'))\r\n    df.get(25, 0)\r\n\r\nTraceback:\r\n\r\n    ----> 1 df.get(25, 0)\r\n\r\n    /home/skipper/src/pandas-skipper/pandas/core/series.pyc in get(self, label, default)\r\n        756         """"""\r\n        757         try:\r\n    --> 758             return self.get_value(label)\r\n        759         except KeyError:\r\n        760             return default\r\n\r\n    /home/skipper/src/pandas-skipper/pandas/core/series.pyc in get_value(self, label)\r\n        776         value : scalar value\r\n        777         """"""\r\n    --> 778         return self.index.get_value(self.values, label)\r\n        779 \r\n        780     def set_value(self, label, value):\r\n\r\n    /home/skipper/src/pandas-skipper/pandas/core/index.pyc in get_value(self, series, key)\r\n    1821         k = _values_from_object(key)\r\n    1822         loc = self.get_loc(k)\r\n    -> 1823         new_values = series.values[loc]\r\n    1824         if np.isscalar(new_values):\r\n    1825             return new_values\r\n\r\n    AttributeError: \'numpy.ndarray\' object has no attribute \'values\'\r\n\r\nVersion\r\n\r\n    [~/]\r\n    [11]: pd.version.version\r\n    [11]: \'0.13.1-105-g8119991\''"
6375,27686107,jreback,jreback,2014-02-17 00:09:26,2014-07-08 08:47:03,2014-02-17 00:31:23,closed,,0.14.0,0,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/6375,b'BUG: Bug in take with duplicate columns not consolidated (GH6240)',b'closes #6240'
6373,27676705,jreback,jreback,2014-02-16 17:16:03,2014-06-16 05:12:11,2014-02-16 18:00:40,closed,,0.14.0,0,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6373,b'BUG: DataFrame.shift with axis=1 was raising (GH6371)',b'closes #6371'
6371,27674416,gouthambs,jreback,2014-02-16 15:11:28,2014-02-16 18:00:50,2014-02-16 18:00:40,closed,,0.14.0,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6371,b'DataFrame Shift with axis=1 gives error',"b' I was playing with axis 0 and 1 in DataFrame shift. I get the following error:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\ndf = pd.DataFrame(np.random.rand(10,5))\r\ndf.shift(1,axis=1)\r\n```\r\n\r\nThis gives me an error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""c:\\WinPython-32bit-2.7.6.3\\python-2.7.6\\lib\\site-packages\\pandas\\core\\generic.py"", line 3175, in shift\r\n    new_data = self._data.shift(indexer, periods, axis=block_axis)\r\n  File ""c:\\WinPython-32bit-2.7.6.3\\python-2.7.6\\lib\\site-packages\\pandas\\core\\internals.py"", line 2407, in shift\r\n    return self.apply(\'shift\', *args, **kwargs)\r\n  File ""c:\\WinPython-32bit-2.7.6.3\\python-2.7.6\\lib\\site-packages\\pandas\\core\\internals.py"", line 2375, in apply\r\n    applied = getattr(blk, f)(*args, **kwargs)\r\n  File ""c:\\WinPython-32bit-2.7.6.3\\python-2.7.6\\lib\\site-packages\\pandas\\core\\internals.py"", line 918, in shift\r\n    new_values = self.values.take(indexer, axis=axis)\r\nIndexError: index 5 is out of bounds for size 5\r\n```\r\n\r\nI am using pandas 0.13.1'"
6366,27661766,cpcloud,cpcloud,2014-02-16 01:18:50,2014-06-13 08:36:17,2014-02-20 14:24:06,closed,cpcloud,0.14.0,29,API Design;Bug;Experimental;Internals,https://api.github.com/repos/pydata/pandas/issues/6366,b'ENH: fix eval scoping issues',"b'closes #5987\r\ncloses #5087\r\n\r\nRelevant user-facing changes:\r\n\r\n```\r\nIn [9]: df = DataFrame(randn(5, 2), columns=list(\'ab\'))\r\n\r\nIn [10]: a, b = 1, 2\r\n\r\nIn [11]: # column names take precedence\r\n\r\nIn [12]: df.query(\'a < b\')\r\nOut[12]:\r\n          a         b\r\n0 -0.805549 -0.090572\r\n1 -1.782325 -1.594079\r\n2 -0.984364  0.934457\r\n3 -1.963798  1.122112\r\n\r\n[4 rows x 2 columns]\r\n\r\nIn [13]: # we must use @ whenever we want a local variable\r\n\r\nIn [14]: df.query(\'@a < b\')\r\nOut[14]:\r\n          a         b\r\n3 -1.963798  1.122112\r\n\r\n[1 rows x 2 columns]\r\n\r\nIn [15]: # we cannot use @ in eval calls\r\n\r\nIn [16]: pd.eval(\'@a + b\')\r\n  File ""<string>"", line unknown\r\nSyntaxError: The \'@\' prefix is not allowed in top-level eval calls, please refer to your variables by name without the \'@\' prefix\r\n\r\nIn [17]: pd.eval(\'@a + b\', parser=\'python\')\r\n  File ""<string>"", line unknown\r\nSyntaxError: The \'@\' prefix is only supported by the pandas parser\r\n```\r\n\r\n- [x] update query/eval docstrings/indexing/perfenhancing\r\n- [x] make sure docs build\r\n- [x] release notes\r\n- [x] pytables\r\n- [x] make the `repr` of `Scope` objects work or revert to previous version\r\n- [x] more tests for new local variable scoping API\r\n- [x] disallow (and provide a useful error message for) locals in expressions like `pd.eval(\'@a + b\')`\r\n- [x] Raise when your variables have the same name as the [builtin math functions that `numexpr` supports](https://code.google.com/p/numexpr/wiki/UsersGuide#Supported_functions), since you cannot override them in `numexpr.evaluate`, even when explicitly passing them. For example\r\n\r\n```python\r\nimport numexpr as ne\r\nsin = randn(10)\r\nd = {\'sin\': sin}\r\nresult = ne.evaluate(\'sin > 1\', local_dict=d, global_dict=d)\r\nresult == array(True)\r\n```\r\n\r\nFor reference, after this PR local variables are given lower precedence than column names. For example\r\n\r\n```python\r\na, b = 1, 2\r\ndf = DataFrame(randn(10, 2), columns=list(\'ab\'))\r\nres = df.query(\'a > b\')\r\n```\r\nwill no longer raise an exception about overlapping variable names. If you want the local `a` (as opposed to the column `a`) you must do\r\n\r\n```python\r\ndf.query(\'@a > b\')\r\n```'"
6355,27604138,fonnesbeck,jreback,2014-02-14 15:48:13,2014-02-17 23:09:10,2014-02-17 23:09:10,closed,,0.14.0,13,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/6355,b'ValueError on dropna call',"b'I\'m running into a problem with using `dropna` with a list of columns specified by `subset`. I wish to remove rows with missing values:\r\n\r\n![missing](http://d.pr/i/5wgR+)\r\n\r\nThe dataset looks like this:\r\n\r\n![dataset](http://d.pr/i/wRYE+)\r\n\r\nHowever, when I try to call `dropna` using these columns, it complains about a duplicate axis:\r\n\r\n    variables.dropna(subset=[\'cigarette_smokers\', \'birth_wt_child\', \r\n                                     \'oxygen\', \'length_of_stay\', \'hospitalized_vitamin_d\', \'breastfed\'])\r\n\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-168-96be0b0b00e8> in <module>()\r\n          1 variables.dropna(subset=[\'cigarette_smokers\', \'birth_wt_child\', \r\n    ----> 2                                      \'oxygen\', \'length_of_stay\', \'hospitalized_vitamin_d\', \'breastfed\'])\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/frame.pyc in dropna(self, axis, how, thresh, subset, inplace)\r\n       2407             if subset is not None:\r\n       2408                 agg_axis_name = self._get_axis_name(agg_axis)\r\n    -> 2409                 agg_obj = self.reindex(**{agg_axis_name: subset})\r\n       2410 \r\n       2411             count = agg_obj.count(axis=agg_axis)\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/frame.pyc in reindex(self, index, columns, **kwargs)\r\n       2160     def reindex(self, index=None, columns=None, **kwargs):\r\n       2161         return super(DataFrame, self).reindex(index=index, columns=columns,\r\n    -> 2162                                               **kwargs)\r\n       2163 \r\n       2164     @Appender(_shared_docs[\'reindex_axis\'] % _shared_doc_kwargs)\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/generic.pyc in reindex(self, *args, **kwargs)\r\n       1561         return self._reindex_axes(axes, level, limit,\r\n       1562                                   method, fill_value, copy,\r\n    -> 1563                                   takeable=takeable).__finalize__(self)\r\n       1564 \r\n       1565     def _reindex_axes(self, axes, level, limit, method, fill_value, copy,\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/frame.pyc in _reindex_axes(self, axes, level, limit, method, fill_value, copy, takeable)\r\n       2110         if columns is not None:\r\n       2111             frame = frame._reindex_columns(columns, copy, level, fill_value,\r\n    -> 2112                                            limit, takeable=takeable)\r\n       2113 \r\n       2114         index = axes[\'index\']\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/frame.pyc in _reindex_columns(self, new_columns, copy, level, fill_value, limit, takeable)\r\n       2137         return self._reindex_with_indexers({1: [new_columns, indexer]},\r\n       2138                                            copy=copy, fill_value=fill_value,\r\n    -> 2139                                            allow_dups=takeable)\r\n       2140 \r\n       2141     def _reindex_multi(self, axes, copy, fill_value):\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/generic.pyc in _reindex_with_indexers(self, reindexers, method, fill_value, limit, copy, allow_dups)\r\n       1687                 new_data = new_data.reindex_indexer(index, indexer, axis=baxis,\r\n       1688                                                     fill_value=fill_value,\r\n    -> 1689                                                     allow_dups=allow_dups)\r\n       1690 \r\n       1691             elif (baxis == 0 and index is not None and\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.13.0_395_gef55e60-py2.7-macosx-10.9-intel.egg/pandas/core/internals.pyc in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups)\r\n       3227         # trying to reindex on an axis with duplicates\r\n       3228         if not allow_dups and not self.axes[axis].is_unique:\r\n    -> 3229             raise ValueError(""cannot reindex from a duplicate axis"")\r\n       3230 \r\n       3231         if not self.is_consolidated():\r\n\r\n    ValueError: cannot reindex from a duplicate axis\r\n\r\nRunning a current build from master in Python 2.7.5 on OS X 10.9.1.'"
6351,27571445,jreback,jreback,2014-02-14 03:20:42,2014-02-14 12:24:29,2014-02-14 12:24:29,closed,cpcloud,0.14.0,0,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/6351,b'BUG: impossible to select string with special character from HDFStore via query',"b'http://stackoverflow.com/questions/21769406/pandas-and-hdf5-querying-a-table-string-containing-character\r\n\r\n```\r\n>>> from pandas import HDFStore, DataFrame\r\n>>> df = DataFrame({\'a\': [\'a\', \'a\', \'c\', \'b\', \'test & test\', \'c\' , \'b\', \'e\'], \r\n                   \'b\': [1, 2, 3, 4, 5, 6, 7, 8]})\r\n>>> store = HDFStore(\'test.h5\')\r\n>>> store.append(\'test\', df, format=\'table\', data_columns=True)\r\n>>> df[df.a == \'test & test\']\r\n     a              b\r\n4    test & test    5\r\n>>> store.select(\'test\', \'a=""test & test""\')\r\n```'"
6350,27570838,jreback,jreback,2014-02-14 03:02:34,2014-07-15 21:00:55,2014-02-14 03:24:56,closed,,0.14.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6350,"b'BUG: TimeGrouper sortedness / API fix (GH4161,GH3881)'","b'BUG: Bug in TimeGrouper/resample when presented with a non-monotonic DatetimeIndex would return invalid results,\r\nBUG: Bug in index name propogation in TimeGrouper/resample\r\ncloses #4161\r\n\r\nAPI: TimeGrouper has a more compatible API to the rest of the groupers (e.g. ``groups`` was missing) \r\ncloses #3881'"
6349,27568331,andrewkittredge,jreback,2014-02-14 01:45:30,2014-06-27 23:38:23,2014-02-23 22:09:34,closed,,0.14.0,15,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6349,"b'str_extract should work for timeseries, (GH6348)'",b'closes https://github.com/pydata/pandas/issues/6348'
6348,27568028,andrewkittredge,jreback,2014-02-14 01:38:00,2014-05-18 14:13:45,2014-02-23 22:09:45,closed,,0.14.0,6,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/6348,b'series.str.extract does not work for timeseries',b'series.str.extract does not work for time-series because core.strings.str_extract does not preserve the index.  I am submitting a unittest and patch that demonstrates and hopefully fixes the issue.'
6347,27560240,jreback,jreback,2014-02-13 23:00:52,2014-07-16 08:53:29,2014-02-14 00:24:28,closed,,0.14.0,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6347,b'BUG: Bug in setting complex dtypes via boolean indexing (GH6345)',b'closes #6345'
6346,27560102,cancan101,hayd,2014-02-13 22:58:36,2014-06-08 03:02:35,2014-06-08 03:02:20,closed,cpcloud,0.14.1,5,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/6346,b'Groupby max erroneously returns NaN',"b'Using:\r\n```\r\ndf =pd.read_csv(StringIO("""""",Date,app,File\r\n2013-04-23,2013-04-23 00:00:00,,log080001.log\r\n2013-05-06,2013-05-06 00:00:00,,log.log\r\n2013-05-07,2013-05-07 00:00:00,OE,xlsx""""""), parse_dates=[0])\r\n```\r\nThis does not work:\r\n```\r\nIn [8]: df.groupby(""Date"")[[""File""]].max()\r\nOut[8]:\r\n                     File\r\nDate\r\n2013-04-23 00:00:00   NaN\r\n2013-05-06 00:00:00   NaN\r\n2013-05-07 00:00:00  xlsx\r\n\r\n[3 rows x 1 columns]\r\n```\r\nbut this does:\r\n```\r\nIn [9]: df.groupby(""Date"")[""File""].max()\r\nOut[9]:\r\nDate\r\n2013-04-23 00:00:00    log080001.log\r\n2013-05-06 00:00:00          log.log\r\n2013-05-07 00:00:00             xlsx\r\nName: File, dtype: object\r\n```\r\n'"
6345,27557255,hayd,jreback,2014-02-13 22:14:32,2014-02-15 01:30:34,2014-02-14 00:24:28,closed,,0.14.0,11,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6345,b'inplace where with complex column changes dtype to float',b'http://stackoverflow.com/questions/21766182/replacing-out-of-bounds-complex-values-in-a-pandas-dataframe/21766586#21766586'
6344,27555399,cancan101,jreback,2014-02-13 21:48:33,2014-04-06 18:16:27,2014-04-06 18:16:27,closed,jreback,0.14.0,4,Bug;Missing-data;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6344,b'fillna with datetime and value=dictlike not working on v0.13.1',"b'I believe that this worked on v0.13:\r\n```\r\ndf = pd.DataFrame({\r\n              \'Date\':[pd.NaT, pd.Timestamp(""2014-1-1"")],\r\n              \'Date2\':[ pd.Timestamp(""2013-1-1""),pd.NaT]\r\n              })\r\n\r\nIn [8]: df.fillna(value={\'Date\':df[\'Date2\']})\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-d5273c4f5a7f> in <module>()\r\n----> 1 df.fillna(value={\'Date\':df[\'Date2\']})\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast)\r\n   2172                         continue\r\n   2173                     obj = result[k]\r\n-> 2174                     obj.fillna(v, inplace=True)\r\n   2175                 return result\r\n   2176             else:\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast)\r\n   2159 \r\n   2160                 new_data = self._data.fillna(value, inplace=inplace,\r\n-> 2161                                              downcast=downcast)\r\n   2162 \r\n   2163             elif isinstance(value, (dict, com.ABCSeries)):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.py in fillna(self, *args, **kwargs)\r\n   2408 \r\n   2409     def fillna(self, *args, **kwargs):\r\n-> 2410         return self.apply(\'fillna\', *args, **kwargs)\r\n   2411 \r\n   2412     def downcast(self, *args, **kwargs):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.py in apply(self, f, *args, **kwargs)\r\n   2373 \r\n   2374             else:\r\n-> 2375                 applied = getattr(blk, f)(*args, **kwargs)\r\n   2376 \r\n   2377             if isinstance(applied, list):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.py in fillna(self, value, inplace, downcast)\r\n   1633         values = self.values if inplace else self.values.copy()\r\n   1634         mask = com.isnull(self.values)\r\n-> 1635         value = self._try_fill(value)\r\n   1636         np.putmask(values, mask, value)\r\n   1637         return [self if inplace else\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.py in _try_fill(self, value)\r\n   1625     def _try_fill(self, value):\r\n   1626         """""" if we are a NaT, return the actual fill value """"""\r\n-> 1627         if isinstance(value, type(tslib.NaT)) or isnull(value):\r\n   1628             value = tslib.iNaT\r\n   1629         return value\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/generic.py in __nonzero__(self)\r\n    674         raise ValueError(""The truth value of a {0} is ambiguous. ""\r\n    675                          ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""\r\n--> 676                          .format(self.__class__.__name__))\r\n    677 \r\n    678     __bool__ = __nonzero__\r\n\r\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n```'"
6342,27524844,estevopaz,cpcloud,2014-02-13 15:15:44,2014-02-16 21:08:52,2014-02-16 21:08:52,closed,cpcloud,0.14.0,3,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/6342,"b""DataFrame.replace() doesn't work correctly when passing a nested dict""","b'```\r\nIn [47]: import pandas as pd\r\n\r\nIn [48]: df = pd.DataFrame({\'col\': [1,2,3,4]})\r\n\r\nIn [49]: df.replace({\'col\': {1: \'a\', 4: \'b\'}})\r\nOut[49]: \r\n  col\r\n0   a\r\n1   2\r\n2   3\r\n3   b\r\n\r\nIn [50]: df.replace({\'col\': {-1: \'-\', 1: \'a\', 4: \'b\'}})\r\nOut[50]: \r\n  col\r\n0   a\r\n1   2\r\n2   3\r\n3   a\r\n^^^^ Value 4 must be mapped to ""b"" instead of ""a""\r\n\r\nIn [51]: pd.__version__\r\nOut[51]: \'0.12.0\'\r\n```'"
6338,27479554,jreback,jreback,2014-02-12 23:13:31,2014-06-21 23:36:58,2014-02-13 00:02:09,closed,,0.14.0,0,Bug;Error Reporting;Groupby,https://api.github.com/repos/pydata/pandas/issues/6338,b'BUG: Issue with groupby agg with a single function and a a mixed-type frame (GH6337)',b'closes #6337'
6337,27479210,jreback,jreback,2014-02-12 23:07:50,2014-02-13 00:02:09,2014-02-13 00:02:09,closed,,0.14.0,0,Bug;Error Reporting;Groupby,https://api.github.com/repos/pydata/pandas/issues/6337,b'BUG: groupby agg with a single function and mixed-types raises',b'http://stackoverflow.com/questions/21706030/pandas-groupby-agg-function-column-dtype-error'
6335,27462659,bashtage,jreback,2014-02-12 19:40:00,2014-06-12 13:25:29,2014-03-05 01:12:05,closed,,0.14.0,49,Bug;Data IO;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6335,b'BUG: Fixed incorrect type in integer conversion in to_stata',b'Simple fix for bug #6327. All conversions were off by a factor of 2.\r\nAdded test for this case.'
6332,27449693,jreback,cpcloud,2014-02-12 17:04:26,2014-02-13 23:11:12,2014-02-13 12:36:24,closed,cpcloud,0.14.0,20,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6332,b'BUG: replace should handle Bool blocks correctly',"b""http://stackoverflow.com/questions/21733802/pandas-series-replace-on-boolean-series\r\n\r\nby definition they don't have missing values"""
6330,27437413,jreback,jreback,2014-02-12 14:35:03,2014-07-02 15:55:41,2014-02-12 21:41:08,closed,,0.14.0,0,Bug;Regression,https://api.github.com/repos/pydata/pandas/issues/6330,b'BUG: Regression in join of non_unique_indexes (GH6329)',b'closes #6329'
6329,27430951,dhirschfeld,jreback,2014-02-12 12:51:45,2014-02-12 21:41:08,2014-02-12 21:41:08,closed,,0.14.0,4,Bug;Regression;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6329,b'Memory Performance Regression in 0.13+',"b""The following snippit runs fine in pandas 0.12 on a machine with 8GB of RAM but throws a `MemoryError` on a machine with 16GB of RAM when using pandas 0.13\r\n\r\n```python\r\ndate_index = pd.date_range('01-Jan-2012', '23-Jan-2013', freq='T')\r\ndaily_dates = date_index.to_period('D').to_timestamp('S','S')\r\nfracofday = date_index.view(np.ndarray) - daily_dates.view(np.ndarray)\r\nfracofday = fracofday.astype('timedelta64[ns]').astype(np.float64)/864e11\r\nfracofday = pd.TimeSeries(fracofday, daily_dates)\r\nindex = pd.date_range(date_index.min().to_period('A').to_timestamp('D','S'),\r\n                      date_index.max().to_period('A').to_timestamp('D','E'),\r\n                      freq='D')\r\ntemp = pd.TimeSeries(1.0, index)\r\nfracofday *= temp[fracofday.index]\r\n```"""
6327,27430346,bashtage,jreback,2014-02-12 12:41:01,2014-03-05 01:12:44,2014-03-05 01:12:44,closed,,0.14.0,7,Bug;Data IO;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6327,b'BUG: DataFrame.to_stata() uses wrong struct formats and crashes in int64',"b""The relevant code is\r\n\r\n```python\r\nself.DTYPE_MAP = \\\r\n    dict(\r\n        lzip(range(1, 245), ['a' + str(i) for i in range(1, 245)]) +\r\n        [\r\n            (251, np.int16),\r\n            (252, np.int32),\r\n            (253, np.int64),\r\n            (254, np.float32),\r\n            (255, np.float64)\r\n        ]\r\n    )\r\n```\r\n\r\nand\r\n\r\n```python\r\nself.TYPE_MAP = lrange(251) + list('bhlfd')\r\n```\r\nwhich maps `h` to int32 and `l` to int64.  \r\n\r\nhttp://docs.python.org/2/library/struct.html#format-characters\r\n\r\nshows that `h` is 2 bytes and `l` is 4, and so trying to run \r\n\r\n```python\r\nstruct.pack('<l',2**40)\r\n```\r\nproduces an error.\r\n\r\nThe obvious fix is to use\r\n\r\n```python\r\nself.TYPE_MAP = lrange(251) + list('blqfd')\r\n```\r\n\r\nbut this will probably produce errors on 32-bit platforms."""
6313,27234591,hhuuggoo,jreback,2014-02-09 23:35:45,2014-06-20 14:12:59,2014-02-17 20:39:18,closed,,0.14.0,13,Bug;Compat;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/6313,"b""FIX: hdfstore queries of the form where=[('date', '>=', datetime(2013,1,...""","b""    FIX: hdfstore queries of the form where=[('date', '>=', datetime(2013,1,1)), ('date', '<=', datetime(2014,1,1))] were broken\r\n    - modified Expr.parse_back_compat to check for tuples, in w, and unpack into w, op, value\r\n    - modified Expr.__init__ to modify the where list/tuple with the parsed result\r\n"""
6303,27196860,MiloszD,jreback,2014-02-08 09:01:50,2014-02-08 15:09:17,2014-02-08 15:08:23,closed,,,2,Bug;Duplicate;Groupby,https://api.github.com/repos/pydata/pandas/issues/6303,b'error with groupby and transform while using datetime64[ns] as part of a key',"b'from pandas.util.testing import choice\r\nfrom pandas import DataFrame, date_range\r\nfrom numpy.random import randn\r\n\r\nn = 8\r\ndates = date_range(\'1/1/2000\', periods=n, freq=\'1T\')\r\ncolors = choice([\'red\', \'green\'], size=n)\r\n\r\ndf = DataFrame({\'value\': randn(n), \'date\': dates, \'color\': colors})\r\n\r\nzscore = lambda x: (x - x.mean()) / x.std()\r\n\r\ndf\r\nOut[2]: \r\n   color                date     value\r\n0    red 2000-01-01 00:00:00 -0.382902\r\n1  green 2000-01-01 00:01:00  0.991723\r\n2  green 2000-01-01 00:02:00 -0.442476\r\n3  green 2000-01-01 00:03:00 -0.776372\r\n4    red 2000-01-01 00:04:00 -1.372511\r\n5    red 2000-01-01 00:05:00 -0.276006\r\n6    red 2000-01-01 00:06:00 -0.182162\r\n7    red 2000-01-01 00:07:00 -0.232967\r\n\r\n[8 rows x 3 columns]\r\n\r\ndf.dtypes\r\nOut[3]: \r\ncolor            object\r\ndate     datetime64[ns]\r\nvalue           float64\r\ndtype: object\r\n\r\ndf.groupby([\'color\',\'date\'])[\'value\'].transform(zscore)\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-97e715db56f2> in <module>()\r\n----> 1 df.groupby([\'color\',\'date\'])[\'value\'].transform(zscore)\r\n\r\nC:\\Users\\MD\\Anaconda\\lib\\site-packages\\pandas-0.13.1.dev-py2.7-win-amd64.egg\\pandas\\core\\groupby.pyc in transform(self, func, *args, **kwargs)\r\n   1845             # this needs to be an ndarray\r\n   1846             result = Series(result)\r\n-> 1847             result.iloc[self._get_index(name)] = res\r\n   1848             result = result.values\r\n   1849 \r\n\r\nC:\\Users\\MD\\Anaconda\\lib\\site-packages\\pandas-0.13.1.dev-py2.7-win-amd64.egg\\pandas\\core\\groupby.pyc in _get_index(self, name)\r\n    265         """""" safe get index """"""\r\n    266         try:\r\n--> 267             return self.indices[name]\r\n    268         except:\r\n    269             if isinstance(name, Timestamp):\r\n\r\nKeyError: (\'green\', Timestamp(\'2000-01-01 00:01:00\', tz=None))'"
6297,27173094,sleibman,jreback,2014-02-07 21:18:04,2014-04-22 13:09:37,2014-04-22 13:09:37,closed,,0.14.0,2,API Design;Bug;Frequency;Reshaping;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6297,"b""rolling_max on a TimeSeries with freq='D' returns incorrect results""","b""rolling_max on a TimeSeries with freq='D' appears to actually compute the rolling mean, and not the rolling max.\r\n\r\n```python\r\nIn [118]: import pandas\r\n\r\nIn [119]: indices = [datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)]\r\n\r\nIn [120]: indices.append(datetime.datetime(1975, 1, 3, 6, 0))  # So that we can have 2 datapoints on one of the days\r\n\r\nIn [121]: series = pandas.Series(range(1, 7), index=indices)\r\n\r\nIn [122]: series = series.map(lambda x: float(x))  # Use floats instead of ints as values\r\n\r\nIn [123]: series = series.sort_index()  # Sort chronologically\r\n\r\nIn [124]: expected_result = pandas.Series([1.0, 2.0, 6.0, 4.0, 5.0], index=[datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)])\r\n\r\nIn [125]: actual_result = pandas.rolling_max(series, window=1, freq='D')\r\n\r\nIn [126]: assert((actual_result==expected_result).all())\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-126-cc436c4798a7> in <module>()\r\n----> 1 assert((actual_result==expected_result).all())\r\n\r\nAssertionError: \r\n\r\nIn [127]: expected_result\r\nOut[127]: \r\n1975-01-01 12:00:00    1\r\n1975-01-02 12:00:00    2\r\n1975-01-03 12:00:00    6\r\n1975-01-04 12:00:00    4\r\n1975-01-05 12:00:00    5\r\ndtype: float64\r\n\r\nIn [128]: actual_result\r\nOut[128]: \r\n1975-01-01    1.0\r\n1975-01-02    2.0\r\n1975-01-03    4.5\r\n1975-01-04    4.0\r\n1975-01-05    5.0\r\nFreq: D, dtype: float64\r\n```\r\n\r\nWith a window of size 2 days, it still looks like the rolling mean:\r\n```python\r\nIn [130]: pandas.rolling_max(series, window=2, freq='D')\r\nOut[130]: \r\n1975-01-01    NaN\r\n1975-01-02    2.0\r\n1975-01-03    4.5\r\n1975-01-04    4.5\r\n1975-01-05    5.0\r\nFreq: D, dtype: float64\r\n```\r\n"""
6284,27049238,TomAugspurger,jreback,2014-02-06 13:32:22,2014-07-16 08:52:32,2014-02-06 21:41:11,closed,,0.14.0,2,API Design;Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6284,b'BUG: Fix interpolate with inplace=True',"b""Closes https://github.com/pydata/pandas/issues/6281\r\n\r\nI didn't have the non inplace return in an else block. And I clearly didn't test this :/\r\n"""
6281,27033572,abrakababra,jreback,2014-02-06 08:35:41,2014-02-06 21:41:11,2014-02-06 21:41:11,closed,,0.14.0,13,API Design;Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6281,b'Series (DataFrame column) inplace interpolate UnboundLocalError',"b""Hello, I recently stumbled across this:\r\n\r\n```python\r\ndf1 = pd.DataFrame({'a':[1.,2.,3.,4.,nan,6,7,8]}, dtype='<f4')\r\ndf1.a.interpolate(inplace=True, downcast=None)\r\n\r\n\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-9-bf0994f72664> in <module>()\r\n----> 1 df1.a.interpolate(inplace=True, downcast=None)\r\n\r\nC:\\Program Files\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in interpolate(self, method, axis, limit, inplace, downcast, **kwargs)\r\n   2304         if axis == 1:\r\n   2305             res = res.T\r\n-> 2306         return res\r\n   2307 \r\n   2308     #----------------------------------------------------------------------\r\n\r\nUnboundLocalError: local variable 'res' referenced before assignment\r\n```\r\n\r\nThe problem arises with setting inplace to True regardless of 'downcast'. \r\n\r\nBut speaking of which: running interpolate() without any options, I would excpect to keep data types by default! I often chunk through datasets and glue them together with to_hdf. It took me a while to figure out that a float column in one chunk had just zeros in it so interpolate downcasted it to int => to_hdf raised on appending.\r\n\r\nThx in advance"""
6275,26983573,jreback,jreback,2014-02-05 18:03:20,2014-06-14 23:51:41,2014-02-05 18:31:59,closed,,0.14.0,0,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6275,b'BUG: Bug in conversion of a string types to a DatetimeIndex with a specifed frequency (GH6273)',b'closes #6273\r\ncloses #6274 '
6274,26981279,jseabold,jreback,2014-02-05 17:32:52,2014-02-05 18:31:59,2014-02-05 18:31:59,closed,,,1,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/6274,b'infer_freq on a series',"b""Somewhat related to #6273 . I'm fairly certain though not positive that this also used to work on 0.12.x.\r\n\r\n```\r\ndf = pd.DataFrame(np.random.rand(5,3))\r\ndf['date'] = ['1-1-1990', '2-1-1990', '3-1-1990', '4-1-1990', '5-1-1990']\r\npd.infer_freq(df['date'])\r\n```\r\n\r\nRaises Series has no attribute `is_monotonic`. I guess `infer_freq` now expects either an Index or ....?\r\n\r\n    [22]: pd.version.version\r\n    [22]: '0.13.0-472-g9109812'"""
6273,26980793,jseabold,jreback,2014-02-05 17:27:17,2014-02-05 18:31:59,2014-02-05 18:31:59,closed,,0.14.0,2,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/6273,b'DatetimeIndex from Series worked in 0.12.x?',"b""I'm fairly certain this worked before a recent upgrade. Too busy to check right now. Maybe it shouldn't work and it's just been fixed, but ... I have something like this in a lot of places in my code.\r\n\r\n```\r\ndf = pd.DataFrame(np.random.rand(5,3))\r\ndf['date'] = ['1-1-1990', '2-1-1990', '3-1-1990', '4-1-1990', '5-1-1990']\r\npd.DatetimeIndex(df['date'], freq='MS')\r\n```\r\n\r\nRaises about Series  not having an attribute inferred_freq. Why is it trying to infer a frequency when I passed freq? Is it a new check?\r\n\r\n\r\n    [22]: pd.version.version\r\n    [22]: '0.13.0-472-g9109812'\r\n"""
6265,26951443,andygoldschmidt,jreback,2014-02-05 10:58:34,2014-03-05 21:58:54,2014-03-05 21:58:54,closed,,0.14.0,3,Bug;Groupby;Indexing,https://api.github.com/repos/pydata/pandas/issues/6265,b'Missing Series name when using count() on groupby object',"b""Consider this very basic group by operation:\r\n``` python\r\ndf = pd.DataFrame({'col1': ['a', 'b', 'a', 'c'], 'col2': [1, 3, 2, 5]})\r\ndf_grp = df.groupby('col1')['col2']\r\n```\r\n\r\nWhen aggregating using ```mean```, ```min```, ... (everything but ```count```) the resulting Series has ```col2``` as its name:\r\n``` python\r\ndf_grp.mean()\r\n\r\ncol1\r\na       1.5\r\nb       3.0\r\nc       5.0\r\nName: col2, dtype: float64\r\n```\r\nHowever, if I use ```count``` for aggregation, the name is not set:\r\n\r\n```python\r\ndf_grp.count()\r\n\r\ncol1\r\na       2\r\nb       1\r\nc       1\r\ndtype: int64\r\n```\r\n\r\nNot a big problem for a simple case like that, but I stumbled over that while working with a MultiIndex that needed to be reindexed and led to a ```KeyError``` due to that missing column name."""
6263,26938743,fonnesbeck,jreback,2014-02-05 05:30:12,2014-03-22 21:32:16,2014-03-22 21:31:56,closed,,,5,Bug;Compat;Visualization,https://api.github.com/repos/pydata/pandas/issues/6263,b'ValueError on groupby boxplot',"b""I have a pretty simple data frame of two columns, containing dates and velocities\r\n\r\n    \tvelocity\tdate\r\n    0\t 83.9\t 2007-08-04\r\n    1\t 89.6\t 2007-08-04\r\n    2\t 88.2\t 2007-08-04\r\n    3\t 88.9\t 2007-08-04\r\n    4\t 89.4\t 2007-08-04\r\n\r\nYet, when I try to make a box plot of velocities grouped by date, I get a ValueError:\r\n\r\n    pitches.boxplot(column='velocity', by='date')\r\n\r\n    ValueError: Wrong number of items passed 109, indices imply 108 \r\n\r\nI've confirmed that `velocity` are floats and `date` are np.datetime64."""
6259,26923920,jreback,jreback,2014-02-04 23:03:45,2014-06-16 02:27:27,2014-02-05 02:08:12,closed,,0.14.0,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6259,b'BUG: bug in xs for a Series with a multiindex (GH6258)',b'closes #6258\r\ncloses #5684'
6258,26917730,cancan101,jreback,2014-02-04 21:37:56,2014-02-05 02:08:12,2014-02-05 02:08:12,closed,jreback,0.14.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6258,b'xs on MultiIndex Does not Work For Series',"b""This works:\r\n```\r\nIn [141]:\r\n\r\ndates = pd.to_datetime(['2013-09-03', '2013-09-04', '2013-09-05'])\r\npd.concat({'A':pd.Series([1,3,4], index=dates), 'B':pd.Series([1,3,4], index=dates)}).to_frame().xs(key='2013-09-03', level=1)\r\n\r\n   0\r\nA  1\r\nB  1\r\n\r\n[2 rows x 1 columns]\r\n```\r\n\r\nbut this does not:\r\n```\r\nIn [142]:\r\n\r\ndates = pd.to_datetime(['2013-09-03', '2013-09-04', '2013-09-05'])\r\n\r\npd.concat({'A':pd.Series([1,3,4], index=dates), 'B':pd.Series([1,3,4], index=dates)}).xs(key='2013-09-03', level=1)\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-142-06aba7d44961> in <module>()\r\n      1 dates = pd.to_datetime(['2013-09-03', '2013-09-04', '2013-09-05'])\r\n----> 2 pd.concat({'A':pd.Series([1,3,4], index=dates), 'B':pd.Series([1,3,4], index=dates)}).xs(key='2013-09-03', level=1)\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.py in _xs(self, key, axis, level, copy)\r\n    437 \r\n    438     def _xs(self, key, axis=0, level=None, copy=True):\r\n--> 439         return self.__getitem__(key)\r\n    440 \r\n    441     xs = _xs\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.py in __getitem__(self, key)\r\n    482     def __getitem__(self, key):\r\n    483         try:\r\n--> 484             result = self.index.get_value(self, key)\r\n    485             if isinstance(result, np.ndarray):\r\n    486                 return self._constructor(result,index=[key]*len(result)).__finalize__(self)\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/index.py in get_value(self, series, key)\r\n   2294                     raise InvalidIndexError(key)\r\n   2295                 else:\r\n-> 2296                     raise e1\r\n   2297             except Exception:  # pragma: no cover\r\n   2298                 raise e1\r\n\r\nKeyError: '2013-09-03'\r\n```"""
6256,26909043,jreback,jreback,2014-02-04 19:36:49,2014-07-16 08:51:48,2014-02-04 19:54:21,closed,,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6256,"b'BUG: Indexing bugs with reordered indexes (GH6252, GH6254)'","b""closes #6252\r\ncloses #6254\r\n\r\n```\r\nIn [1]:  df = DataFrame(index=[3, 5, 4], columns=['A'])\r\n\r\nIn [2]: df.loc[[4, 3, 5], 'A'] = [1, 2, 3]\r\n\r\nIn [3]: df\r\nOut[3]: \r\n   A\r\n3  2\r\n5  3\r\n4  1\r\n\r\n[3 rows x 1 columns]\r\n```\r\n\r\n```\r\nIn [4]: keys1 = ['@' + str(i) for i in range(5)]\r\n\r\nIn [5]: val1 = np.arange(5)\r\n\r\nIn [6]: keys2 = ['@' + str(i) for i in range(4)]\r\n\r\nIn [7]: val2 = np.arange(4)\r\n\r\nIn [8]: index = list(set(keys1).union(keys2))\r\n\r\nIn [9]: df = DataFrame(index = index)\r\n\r\nIn [10]: df['A'] = nan\r\n\r\nIn [11]: df.loc[keys1, 'A'] = val1\r\n\r\nIn [12]: df['B'] = nan\r\n\r\nIn [13]: df.loc[keys2, 'B'] = val2\r\n\r\nIn [14]: df\r\nOut[14]: \r\n    A   B\r\n@2  2   2\r\n@3  3   3\r\n@0  0   0\r\n@1  1   1\r\n@4  4 NaN\r\n\r\n[5 rows x 2 columns]\r\n```"""
6254,26902167,sadruddin,jreback,2014-02-04 18:02:19,2014-02-12 12:59:46,2014-02-04 19:54:21,closed,,0.14.0,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6254,"b""assigning using df.loc and unsorted labels doesn't put values in the right places""","b""Using 0.13.1 (gohlke binaries, Win64, using numpy 1.8):\r\n\r\nWhen using .loc, when passing unsorted labels, pandas doesn't assign the values in the designed labels' locations. Example:\r\n```df = pandas.DataFrame(index=[3, 5, 4], columns=['A'])```\r\n```df.loc[[4, 3, 5], 'A'] = [1, 2, 3]```\r\nResult is:\r\n```\r\nOut[30]: \r\n   A\r\n3  1\r\n5  2\r\n4  3\r\n```\r\n\r\nOne would have expected:\r\n```\r\nOut[30]: \r\n   A\r\n3  2\r\n5  3\r\n4  1\r\n```"""
6252,26880775,jreback,jreback,2014-02-04 13:20:20,2014-02-04 19:54:21,2014-02-04 19:54:21,closed,jreback,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6252,b'BUG: loc indexing when starting with an empty frame',"b""from ML: https://groups.google.com/forum/#!topic/pydata/P3JFE3sob1U\r\nempty indexing the frame\r\n\r\n```\r\nkeys1 = ['@' + str(i) for i in range(1000)]\r\nval1 = arange(1000)\r\n\r\nkeys2 = ['@' + str(i) for i in range(990)]\r\nval2 = arange(990)\r\n\r\nallKeys = list(set(keys1).union(keys2))\r\ndf = pandas.DataFrame(index = allKeys)\r\ndf['A'] = nan\r\ndf.loc[keys1, 'A'] = val1\r\n\r\ndf['B'] = nan\r\ndf.loc[keys2, 'B'] = val2\r\n\r\ndf.head()\r\n\r\n## -- End pasted text --\r\n                        <43>\r\n        A    B\r\n@679  679  679\r\n@678  678  678\r\n@337  337  337\r\n@744  744  744\r\n@673  673  673\r\n```"""
6247,26866244,y-p,jreback,2014-02-04 08:28:59,2014-02-04 14:17:37,2014-02-04 14:17:37,closed,,0.14.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/6247,b'TestHDFStore: test_select_dtypes fails on py3.4',"b'Caught while testing 185b3f11 on Win/x86_64 with PY3.4.0/64:\r\n\r\n```\r\n======================================================================\r\nERROR: test_select_dtypes (pandas.io.tests.test_pytables.TestHDFStore)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File &quot;c:\\python34-AMD64\\lib\\unittest\\case.py&quot;, line 57, in testPartExecutor\r\n    yield\r\n  File &quot;c:\\python34-AMD64\\lib\\unittest\\case.py&quot;, line 574, in run\r\n    testMethod()\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\tests\\test_pytables.py&quot;, line 2939, in test_select_dtypes\r\n    result = store.select(&#x27;df&#x27;, Term(&#x27;boolv == %s&#x27; % str(v)), columns = [&#x27;A&#x27;,&#x27;boolv&#x27;])\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 664, in select\r\n    auto_close=auto_close).get_values()\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 1338, in get_values\r\n    results = self.func(self.start, self.stop)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 653, in func\r\n    columns=columns, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 3788, in read\r\n    if not self.read_axes(where=where, **kwargs):\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 3047, in read_axes\r\n    self.selection = Selection(self, where=where, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 4263, in __init__\r\n    self.terms = self.generate(where)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\io\\pytables.py&quot;, line 4276, in generate\r\n    return Expr(where, queryables=q, encoding=self.table.encoding)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\pytables.py&quot;, line 518, in __init__\r\n    self.terms = self.parse()\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 689, in visit_Compare\r\n    return self.visit(binop)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 536, in visit_BinOp\r\n    op, op_class, left, right = self._possibly_transform_eq_ne(node)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 494, in _possibly_transform_eq_ne\r\n    right = self.visit(node.right, side=&#x27;right&#x27;)\r\n  File &quot;C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\34\\pandas\\computation\\expr.py&quot;, line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: &#x27;ExprVisitor&#x27; object has no attribute &#x27;visit_NameConstant&#x27;\r\n```\r\nxref https://github.com/pydata/pandas/pull/6243\r\n'"
6243,26837350,cgohlke,jreback,2014-02-03 21:08:43,2014-06-14 10:19:39,2014-02-04 14:31:19,closed,cpcloud,,8,Bug;IO HDF5;Python 3.5,https://api.github.com/repos/pydata/pandas/issues/6243,b'ENH: add support for Python 3.4 ast.NameConstant',"b'Python 3.4 created a ""NameConstant AST class to represent None, True, and False literals"" (http://docs.python.org/3.4/whatsnew/changelog.html). \r\nPandas-0.13.1.win-amd64-py3.4 fails a few tests with `AttributeError: \'...ExprVisitor\' object has no attribute \'visit_NameConstant\'`. \r\nThe suggested change fixes those test errors but I am not sure that it is the best/correct fix.\r\n\r\n```\r\n======================================================================\r\nERROR: test_scalar_unary (pandas.computation.tests.test_eval.TestEvalNumexprPandas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 678, in test_scalar_unary\r\n    pd.eval(\'~True\', parser=self.parser, engine=self.engine), ~True)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 545, in visit_UnaryOp\r\n    operand = self.visit(node.operand)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PandasExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_scalar_unary (pandas.computation.tests.test_eval.TestEvalNumexprPython)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 678, in test_scalar_unary\r\n    pd.eval(\'~True\', parser=self.parser, engine=self.engine), ~True)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 545, in visit_UnaryOp\r\n    operand = self.visit(node.operand)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PythonExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_scalar_unary (pandas.computation.tests.test_eval.TestEvalPythonPandas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 678, in test_scalar_unary\r\n    pd.eval(\'~True\', parser=self.parser, engine=self.engine), ~True)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 545, in visit_UnaryOp\r\n    operand = self.visit(node.operand)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PandasExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_scalar_unary (pandas.computation.tests.test_eval.TestEvalPythonPython)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 678, in test_scalar_unary\r\n    pd.eval(\'~True\', parser=self.parser, engine=self.engine), ~True)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 545, in visit_UnaryOp\r\n    operand = self.visit(node.operand)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PythonExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_bool_ops_with_constants (pandas.computation.tests.test_eval.TestOperationsNumExprPandas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1086, in test_bool_ops_with_constants\r\n\r\n    res = self.eval(ex)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1046, in eval\r\n    return pd.eval(*args, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 716, in visit_BoolOp\r\n    return reduce(visitor, operands)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 708, in visitor\r\n    lhs = self._try_visit_binop(x)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 704, in _try_visit_binop\r\n    return self.visit(bop)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PandasExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_simple_bool_ops (pandas.computation.tests.test_eval.TestOperationsNumExprPandas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1078, in test_simple_bool_ops\r\n    res = self.eval(ex)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1046, in eval\r\n    return pd.eval(*args, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 716, in visit_BoolOp\r\n    return reduce(visitor, operands)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 708, in visitor\r\n    lhs = self._try_visit_binop(x)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 704, in _try_visit_binop\r\n    return self.visit(bop)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PandasExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_bool_ops_with_constants (pandas.computation.tests.test_eval.TestOperationsNumExprPython)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1389, in test_bool_ops_with_constants\r\n\r\n    res = self.eval(ex)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1046, in eval\r\n    return pd.eval(*args, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 536, in visit_BinOp\r\n    op, op_class, left, right = self._possibly_transform_eq_ne(node)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 492, in _possibly_transform_eq_ne\r\n    left = self.visit(node.left, side=\'left\')\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PythonExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_bool_ops_with_constants (pandas.computation.tests.test_eval.TestOperationsPythonPandas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1086, in test_bool_ops_with_constants\r\n\r\n    res = self.eval(ex)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1046, in eval\r\n    return pd.eval(*args, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 716, in visit_BoolOp\r\n    return reduce(visitor, operands)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 708, in visitor\r\n    lhs = self._try_visit_binop(x)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 704, in _try_visit_binop\r\n    return self.visit(bop)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PandasExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_simple_bool_ops (pandas.computation.tests.test_eval.TestOperationsPythonPandas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1078, in test_simple_bool_ops\r\n    res = self.eval(ex)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1046, in eval\r\n    return pd.eval(*args, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 716, in visit_BoolOp\r\n    return reduce(visitor, operands)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 708, in visitor\r\n    lhs = self._try_visit_binop(x)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 704, in _try_visit_binop\r\n    return self.visit(bop)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PandasExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_bool_ops_with_constants (pandas.computation.tests.test_eval.TestOperationsPythonPython)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1389, in test_bool_ops_with_constants\r\n\r\n    res = self.eval(ex)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\tests\\test_eval.py"", line 1046, in eval\r\n    return pd.eval(*args, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\eval.py"", line 207, in eval\r\n    truediv=truediv)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 762, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 536, in visit_BinOp\r\n    op, op_class, left, right = self._possibly_transform_eq_ne(node)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 492, in _possibly_transform_eq_ne\r\n    left = self.visit(node.left, side=\'left\')\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'PythonExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n\r\n======================================================================\r\nERROR: test_select_dtypes (pandas.io.tests.test_pytables.TestHDFStore)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\tests\\test_pytables.py"", line 2939, in test_select_dtypes\r\n    result = store.select(\'df\', Term(\'boolv == %s\' % str(v)), columns = [\'A\',\'boolv\'])\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 664, in select\r\n    auto_close=auto_close).get_values()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 1338, in get_values\r\n    results = self.func(self.start, self.stop)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 653, in func\r\n    columns=columns, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 3788, in read\r\n    if not self.read_axes(where=where, **kwargs):\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 3047, in read_axes\r\n    self.selection = Selection(self, where=where, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 4263, in __init__\r\n    self.terms = self.generate(where)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\io\\pytables.py"", line 4276, in generate\r\n    return Expr(where, queryables=q, encoding=self.table.encoding)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\pytables.py"", line 518, in __init__\r\n    self.terms = self.parse()\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 781, in parse\r\n    return self._visitor.visit(self.expr)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 455, in visit_Module\r\n    return self.visit(expr, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 458, in visit_Expr\r\n    return self.visit(node.value, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 689, in visit_Compare\r\n    return self.visit(binop)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 449, in visit\r\n    return visitor(node, **kwargs)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 536, in visit_BinOp\r\n    op, op_class, left, right = self._possibly_transform_eq_ne(node)\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 494, in _possibly_transform_eq_ne\r\n    right = self.visit(node.right, side=\'right\')\r\n  File ""X:\\Python34\\lib\\site-packages\\pandas\\computation\\expr.py"", line 448, in visit\r\n    visitor = getattr(self, method)\r\nAttributeError: \'ExprVisitor\' object has no attribute \'visit_NameConstant\'\r\n```'"
6240,26821919,jreback,jreback,2014-02-03 17:48:31,2014-02-17 00:31:22,2014-02-17 00:31:22,closed,jreback,0.14.0,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6240,b'BUG: duplicate columns and drop buggy',"b""```\r\nIn [21]: df = DataFrame({'A' : np.random.randn(5),\r\n 'B' : np.random.randn(5),'C' : np.random.randn(5), \r\n  'D':['a','b','c','d','e'] })\r\n\r\nIn [22]: df\r\nOut[22]: \r\n          A         B         C  D\r\n0 -0.941264  0.272726 -0.547948  a\r\n1  0.069432  1.398414  0.039093  b\r\n2 -0.073638 -1.264676 -1.339994  c\r\n3  1.520017 -0.562979 -0.739326  d\r\n4 -0.395157  0.542807  0.766582  e\r\n\r\n[5 rows x 4 columns]\r\n\r\nIn [23]: df.take([2,0,1,2,3], axis=1).drop('C',axis=1)\r\nOut[23]: \r\n          A         B  D\r\n0 -0.941264  0.272726  a\r\n1  0.069432  1.398414  b\r\n2 -0.073638 -1.264676  c\r\n3  1.520017 -0.562979  d\r\n4 -0.395157  0.542807  e\r\n\r\n[5 rows x 3 columns]\r\n\r\nIn [24]: df.take([2,0,1,2,1], axis=1).drop('C',axis=1)\r\nIndexError: index 3 is out of bounds for axis 0 with size 3\r\n```\r\n"""
6222,26741662,jreback,jreback,2014-02-01 16:18:48,2014-06-26 12:23:43,2014-02-01 18:12:17,closed,,0.13.1,0,2/3 Compat;Algos;Bug,https://api.github.com/repos/pydata/pandas/issues/6222,b'BUG/TST: groupby with mixed string/int grouper failing in python3 (GH6212)',b'closes #6212'
6212,26717199,phaebz,jreback,2014-01-31 21:33:37,2014-02-03 20:50:05,2014-02-01 18:12:17,closed,,0.13.1,2,Algos;Bug,https://api.github.com/repos/pydata/pandas/issues/6212,b'DOC/PY3: DataFrame.groupby fails in docs examples',"b'This is a follow-up on #5530 and the general Python 3 compat for docs issue. It is the first episode of my journey into the unknown Python-3-compat-for-doc-examples-land.\r\n\r\nWhile some errors should be rather easy to fix (e.g. `xrange`) I just stumbled upon this one (`doc/source/comparison_with_r.rst` line 54) which fails under Python 3:\r\n``` python\r\nimport pandas as pd\r\nimport numpy as np\r\noptions.display.max_rows=15\r\nfrom pandas import DataFrame\r\n\r\ndf = DataFrame({\r\n   \'v1\': [1,3,5,7,8,3,5,np.nan,4,5,7,9],\r\n   \'v2\': [11,33,55,77,88,33,55,np.nan,44,55,77,99],\r\n   \'by1\': [""red"", ""blue"", 1, 2, np.nan, ""big"", 1, 2, ""red"", 1, np.nan, 12],\r\n   \'by2\': [""wet"", ""dry"", 99, 95, np.nan, ""damp"", 95, 99, ""red"", 99, np.nan,\r\n           np.nan]\r\n   })\r\n\r\ng = df.groupby([\'by1\',\'by2\'])\r\ng[[\'v1\',\'v2\']].mean()\r\n```\r\n``` python\r\nTypeError: unorderable types: str() > int()\r\n```\r\nIs the usage wrong here or is it a bug?'"
6210,26708680,TomAugspurger,jreback,2014-01-31 19:25:17,2014-06-13 02:06:23,2014-01-31 21:30:11,closed,,0.13.1,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6210,b'BUG: dont ignore use_index kw in bar plot',"b""Closes https://github.com/pydata/pandas/issues/6209\r\n\r\nHere's the fixed output for the same example from the issue: `df.plot(kind='bar', use_index=False)`:\r\n\r\n![line_false_fixed](https://f.cloud.github.com/assets/1312546/2053850/603f044e-8aad-11e3-86e3-3f3bf38a5683.png)\r\n\r\nI thew it in `.13.1`, can push to `.14` if need be."""
6209,26708016,TomAugspurger,jreback,2014-01-31 19:15:57,2014-01-31 21:30:11,2014-01-31 21:30:11,closed,,0.13.1,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/6209,"b""BUG: plot(kind='bar') ignores `use_index` keyword""","b""with `df = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])`.\r\n\r\nFrom `df.plot(kind='bar', use_index=False)`:\r\n\r\n![bar_false](https://f.cloud.github.com/assets/1312546/2053712/c892ee7c-8aab-11e3-9ab4-2810cf9adbd5.png)\r\n\r\nFrom `df.plot(use_index=False)`:\r\n\r\n![line_false](https://f.cloud.github.com/assets/1312546/2053715/d6ad8328-8aab-11e3-8495-604fdaffe9f2.png)\r\n\r\nMost of the time it doesn't make sense to do a bar plot and ignore the groups formed by the index, but I had an index with a bunch of long strings and just wanted to see the distribution (pretty much a histogram), so I didn't care about the index.\r\n\r\nPR inbound.\r\n"""
6205,26685978,havoc-io,jreback,2014-01-31 14:54:46,2014-06-12 21:23:08,2014-02-06 14:59:23,closed,,0.14.0,23,Bug;Dtypes;Internals,https://api.github.com/repos/pydata/pandas/issues/6205,b'BUG: Add type promotion support for eval() expressions with many properties',"b""This commit modifies the call to numpy.result_type to get around the\r\nNPY_MAXARGS limit, which at the moment is 32.  Instead of passing a\r\ngenerator of all types involved in an expression, the type promotion\r\nis done on a pair-wise basis with a call to reduce.\r\n\r\nThis fixes bugs for code such as the following:\r\n\r\n    from numpy.random import randn\r\n    from pandas import DataFrame\r\n\r\n    d = DataFrame(randn(10, 2), columns=list('ab'))\r\n\r\n    # Evaluates fine\r\n    print(d.eval('*'.join(['a'] * 32)))\r\n\r\n    # Fails to evaluate due to NumPy argument limits\r\n    print(d.eval('*'.join(['a'] * 33)))"""
6202,26679799,jreback,jreback,2014-01-31 13:05:52,2014-06-19 08:34:35,2014-01-31 13:32:00,closed,,0.13.1,0,Bug;IO HDF5;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6202,b'BUG: correctly select on a multi-index even in the prescence of under specificed column spec (GH6169)',b'closes #6169'
6186,26584317,masu33,hayd,2014-01-30 09:26:35,2014-04-21 05:45:10,2014-04-21 05:45:10,closed,,0.16.0,7,Bug;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/6186,b'to_csv: index and header parameter interference',"b""In the following snippet the two to_csv calls differs only in the index argument. Although the output header in the first case is (index), 'X', 'Y' but in the second case is 'A', 'B'. The documentation didn't suggest that sort of difference.\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])\r\ndf.to_csv('testtocsv1.csv', header=['X', 'Y'])\r\ndf.to_csv('testtocsv2.csv', index=False, header=['X', 'Y'])\r\n```"""
6184,26574746,dsm054,jreback,2014-01-30 04:44:14,2014-07-16 08:50:50,2014-01-31 14:53:15,closed,,0.13.1,1,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/6184,b'TST: fix test_reshape.py',"b""Fixes #6081 issues in test_reshape.py -- most of `test_custom_var_and_value_name` wasn't being executed (hiding some bugs in the test code too) because it was being shadowed by another method of the same name."""
6178,26562264,jreback,jreback,2014-01-29 23:15:41,2014-02-15 22:05:03,2014-02-15 22:05:03,closed,jtratner,0.14.0,2,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/6178,b'BUG: div by integer 0 returning non-signed infs (and nan issue)',"b""#6149\r\nexposed this bug; going to update the tests (and i'll link there that show it)\r\ninitially only showed up on py2.6...but it is real\r\n\r\nthe signs of the inf under integer division are wrong.\r\nfor the most part the np.nan are preserved, not sure if this is right or not\r\n```\r\nIn [1]: Series([np.nan,1,-1])/0\r\nOut[1]: \r\n0    inf\r\n1    inf\r\n2    inf\r\ndtype: float64\r\n\r\nIn [2]: Series([np.nan,1,-1])/0.\r\nOut[2]: \r\n0    NaN\r\n1    inf\r\n2   -inf\r\ndtype: float64\r\n```"""
6177,26552153,wabu,jreback,2014-01-29 20:47:11,2014-06-13 12:19:31,2014-02-09 13:31:35,closed,,0.14.0,10,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/6177,b'ENH: select column/coordinates/multiple with start/stop/selection',"b'select_as_multiple/column/coordinate and remove behaves strange on combinations of where clauses and start/stop keyword arguments so I started fixing this.\r\n\r\nI changed select_as_multiple to select coordinates inside the TableIterator, so it will work on large tables. Moreover select_as_multiple always throws a KeyError when either a key or the selector is not available in the file.\r\n\r\nCloses #4835.'"
6172,26534412,jreback,jreback,2014-01-29 16:48:25,2014-06-24 03:11:45,2014-01-29 17:23:04,closed,,0.13.1,0,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/6172,b'BUG: Consistency with dtypes in setting an empty DataFrame (GH6171)',b'closes #6171'
6171,26530006,aldanor,jreback,2014-01-29 15:56:50,2014-01-29 19:48:08,2014-01-29 17:23:04,closed,,0.13.1,20,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/6171,"b'Assignments: numerics, strings and .loc'","b""Why would assigning an entire column to an array of values work differently with numbers vs strings?\r\n\r\n**Assigning numeric values:**\r\n```python\r\n>>> df = pd.DataFrame(columns=['x', 'y'])\r\n>>> df['x'] = [1, 2]\r\n>>> df\r\n   x    y\r\n0  1  NaN\r\n1  2  NaN\r\n```\r\n\r\n**Assigning string values:**\r\n```python\r\n>>> df = pd.DataFrame(columns=['x', 'y'])\r\n>>> df['x'] = ['1', '2']\r\nValueError: could not broadcast input array from shape (2) into shape (0)\r\n```\r\n\r\nBtw according to latest docs `.loc` can append, but can it append more than one value at once?\r\n\r\n**Setting multiple via `.loc`:**\r\n```python\r\n>>> df = pd.DataFrame(columns=['x', 'y'])\r\n>>> df.loc[:, 'x'] = [1, 2]\r\n>>> df\r\nEmpty DataFrame\r\nColumns: [x, y]\r\nIndex: []\r\n>>> df.loc[[0, 1], 'x'] = [1, 2]\r\n>>> df\r\nEmpty DataFrame\r\nColumns: [x, y]\r\nIndex: []\r\n>>> df.loc[0:2, 'x'] = [1, 2]\r\n>>> df\r\nEmpty DataFrame\r\nColumns: [x, y]\r\nIndex: []\r\n```\r\n\r\n**Setting single via `.loc`:** (this works ofc)\r\n```python\r\n>>> df = pd.DataFrame(columns=['x', 'y'])\r\n>>> df.loc[0, 'x'] = 1\r\n>>> df\r\n   x    y\r\n0  1  NaN\r\n```"""
6170,26529262,waitingkuo,jreback,2014-01-29 15:47:23,2015-01-18 21:40:11,2015-01-18 21:40:11,closed,,0.16.0,22,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/6170,b'BUG: parsing multi-column headers in read_csv (GH6051)',"b""closes #6051 \r\n\r\nBug: `mangle_dupe_cols` cannot work while the header is a list. \r\n\r\nOriginally, `has_mi_columns` will be set as 1 while the header is a list. And the `mangle_dupe_cols` things would not work when `has_mi_columns` == 1. \r\n\r\nThis pull request is to \r\n\r\n1. for the list with single element, for example [0], the `has_mi_columns` will be set as 0, and the header will do the original `mangle_dupe_cols` things as the header is a single integer.\r\n2. for the list with more than one element, append the sequence number in the last element of the duplicated multi-column\r\n\r\nFor example\r\n\r\n    Male,Male,Male,Male,Male,Female,Female\r\n    A,B,B,B,C,D,D\r\n    1,2,3,4,5,6,7\r\n    1,2,3,4,5,6,7\r\n    1,2,3,4,5,6,7\r\n    1,2,3,4,5,6,7\r\n    1,2,3,4,5,6,7\r\n    1,2,3,4,5,6,7\r\n\r\nwould be converted to \r\n\r\n    In [2]: pd.read_csv('woo.csv', header=[0,1])\r\n    Out[2]: \r\n       Male                  Female     \r\n          A  B  B.1  B.2  C       D  D.1\r\n    0     1  2    3    4  5       6    7\r\n    1     1  2    3    4  5       6    7\r\n    2     1  2    3    4  5       6    7\r\n    3     1  2    3    4  5       6    7\r\n    4     1  2    3    4  5       6    7\r\n    5     1  2    3    4  5       6    7\r\n\r\n    [6 rows x 7 columns]"""
6169,26527397,glyg,jreback,2014-01-29 15:24:50,2014-01-31 13:32:00,2014-01-31 13:32:00,closed,,0.13.1,6,Bug;IO HDF5;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/6169,"b""can't select a specific column from a HDFStore table with a MultiIndex DataFrame""","b'I\'m running in what seems to be a bug.\r\nI\'m using pandas version \'0.13.0rc1-29-ga0a527b\' from github, python 3.3 on a linux Mint 15 64 bits.\r\n\r\nHere\'s a minimal example that fails:\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\nindex = pd.MultiIndex(levels=[[\'foo\', \'bar\', \'baz\', \'qux\'],\r\n                              [\'one\', \'two\', \'three\']],\r\n                      labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],\r\n                              [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\r\n                      names=[\'foo_name\', \'bar_name\'])\r\n\r\n\r\ndf_mi = pd.DataFrame(np.random.randn(10, 3), index=index,\r\n                     columns=[\'A\', \'B\', \'C\'])\r\n\r\nwith pd.get_store(\'minimal_io.h5\') as store:\r\n    store.put(\'df_mi\', df_mi, format=\'table\')\r\n\r\nwith pd.get_store(\'minimal_io.h5\') as store:\r\n    ixs = store.select(\'df_mi\', ""columns=[\'A\']"")\r\n```\r\n\r\nAnd here is the error message:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-32-005cf4e724e0> in <module>()\r\n     17 \r\n     18 with pd.get_store(\'minimal_io.h5\') as store:\r\n---> 19     ixs = store.select(\'df_mi\', ""columns=[\'A\']"")\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/io/pytables.py in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\r\n    622 \r\n    623         return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop,\r\n--> 624                              auto_close=auto_close).get_values()\r\n    625 \r\n    626     def select_as_coordinates(\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/io/pytables.py in get_values(self)\r\n   1252 \r\n   1253     def get_values(self):\r\n-> 1254         results = self.func(self.start, self.stop)\r\n   1255         self.close()\r\n   1256         return results\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/io/pytables.py in func(_start, _stop)\r\n    611         def func(_start, _stop):\r\n    612             return s.read(where=where, start=_start, stop=_stop,\r\n--> 613                           columns=columns, **kwargs)\r\n    614 \r\n    615         if iterator or chunksize is not None:\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/io/pytables.py in read(self, columns, **kwargs)\r\n   3796         df = super(AppendableMultiFrameTable, self).read(\r\n   3797             columns=columns, **kwargs)\r\n-> 3798         df = df.set_index(self.levels)\r\n   3799 \r\n   3800         # remove names for \'level_%d\'\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/frame.py in set_index(self, keys, drop, append, inplace, verify_integrity)\r\n   2327                 names.append(None)\r\n   2328             else:\r\n-> 2329                 level = frame[col].values\r\n   2330                 names.append(col)\r\n   2331                 if drop:\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/frame.py in __getitem__(self, key)\r\n   1626             return self._getitem_multilevel(key)\r\n   1627         else:\r\n-> 1628             return self._getitem_column(key)\r\n   1629 \r\n   1630     def _getitem_column(self, key):\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/frame.py in _getitem_column(self, key)\r\n   1633         # get column\r\n   1634         if self.columns.is_unique:\r\n-> 1635             return self._get_item_cache(key)\r\n   1636 \r\n   1637         # duplicate columns & possible reduce dimensionaility\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/generic.py in _get_item_cache(self, item)\r\n    976         res = cache.get(item)\r\n    977         if res is None:\r\n--> 978             values = self._data.get(item)\r\n    979             res = self._box_item_values(item, values)\r\n    980             cache[item] = res\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/internals.py in get(self, item)\r\n   2738                 return self.get_for_nan_indexer(indexer)\r\n   2739 \r\n-> 2740             _, block = self._find_block(item)\r\n   2741             return block.get(item)\r\n   2742         else:\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/internals.py in _find_block(self, item)\r\n   3049 \r\n   3050     def _find_block(self, item):\r\n-> 3051         self._check_have(item)\r\n   3052         for i, block in enumerate(self.blocks):\r\n   3053             if item in block:\r\n\r\n/home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/internals.py in _check_have(self, item)\r\n   3056     def _check_have(self, item):\r\n   3057         if item not in self.items:\r\n-> 3058             raise KeyError(\'no item named %s\' % com.pprint_thing(item))\r\n   3059 \r\n   3060     def reindex_axis(self, new_axis, indexer=None, method=None, axis=0,\r\n\r\nKeyError: \'no item named foo_name\'\r\n\r\n> /home/guillaume/python3/lib/python3.3/site-packages/pandas-0.13.0rc1_29_ga0a527b-py3.3-linux-x86_64.egg/pandas/core/internals.py(3058)_check_have()\r\n   3057         if item not in self.items:\r\n-> 3058             raise KeyError(\'no item named %s\' % com.pprint_thing(item))\r\n   3059\r\n```\r\n'"
6167,26512128,wabu,jreback,2014-01-29 10:59:31,2014-06-16 09:44:30,2014-01-29 15:39:38,closed,,0.13.1,2,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/6167,b'append to existing table with mulitindex fix',b'Appending to an existing table with a multiindex failed as data_columns is set to [] but compared with None'
6166,26505467,wabu,jreback,2014-01-29 08:45:05,2014-06-24 10:39:07,2014-04-06 16:59:32,closed,,0.14.0,15,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/6166,b'BUG: long str unconvert fix ',"b'When storing long strings in hdf5, they get truncated at 64bytes when converted back to strings. The fix computes the itemsize of the strings and uses it before converting to strings. It would be possible to use the attribute information of the table, but that would require changes in the calling code.\r\n\r\nTest that failed without the patch is included.\r\n\r\n'"
6155,26489069,michaelbilow,jreback,2014-01-29 00:24:40,2014-01-29 06:19:54,2014-01-29 03:24:59,closed,,0.13.1,10,Bug;Dtypes;Experimental;Strings,https://api.github.com/repos/pydata/pandas/issues/6155,b'String comparison in query()',"b'Hi, it seems that string comparisons aren\'t supported in query() yet, so maybe this isn\'t a bug yet. Anyway, hopefully this behavior will be fixed for future editions of pandas.\r\n\r\n```python\r\nimport pandas as pd\r\nimport numexpr as ne\r\n\r\na = list(\'abcdef\')\r\nb = range(6)\r\ndf = pd.DataFrame({\'X\':pd.Series(a),\'Y\': pd.Series(b)})\r\n\r\ndf_Y = df.query(\'Y < 3\')             ## Works fine.\r\nne_works = ne.evaluate(\'""a"" < ""d""\')  ## ne_works == np.array([True])\r\ndf_X = df.query(\'X < ""d""\')           ## RuntimeError: max recursion depth exceeded\r\n```'"
6149,26472603,jreback,jreback,2014-01-28 20:17:55,2014-02-15 23:16:11,2014-02-15 23:16:11,closed,,0.14.0,6,API Design;Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6149,"b""API/BUG: should df.loc[:,'col'] be the same as df['col'] for assignment""","b""see also: http://stackoverflow.com/questions/21415432/pandas-v0-13-0-setting-dataframe-values-of-type-datetime64ns\r\n\r\n~~I believe this changed from 0.12.~~ was the same in 0.12\r\n\r\nBoils down to if the row indexer is a null-slice (IOW all rows are selected), should\r\ndtype conversion be done or not (as it current)\r\n\r\n```\r\nIn [45]: df = pd.DataFrame({'date':pd.date_range('2000-01-01','2000-01-5'),'val' : np.arange(5)})\r\n\r\nIn [46]: df\r\nOut[46]: \r\n        date  val\r\n0 2000-01-01    0\r\n1 2000-01-02    1\r\n2 2000-01-03    2\r\n3 2000-01-04    3\r\n4 2000-01-05    4\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [47]: df['date'] = 0\r\n\r\nIn [48]: df\r\nOut[48]: \r\n   date  val\r\n0     0    0\r\n1     0    1\r\n2     0    2\r\n3     0    3\r\n4     0    4\r\n\r\n[5 rows x 2 columns]\r\n\r\nIn [49]: df = pd.DataFrame({'date':pd.date_range('2000-01-01','2000-01-5'),'val' : np.arange(5)})\r\n\r\nIn [53]: df.loc[:,'date'] = np.array([0])\r\n\r\nIn [54]: df\r\nOut[54]: \r\n        date  val\r\n0 1970-01-01    0\r\n1 1970-01-01    1\r\n2 1970-01-01    2\r\n3 1970-01-01    3\r\n4 1970-01-01    4\r\n\r\n[5 rows x 2 columns]\r\n```"""
6148,26469518,jreback,cpcloud,2014-01-28 19:37:34,2014-01-29 03:54:13,2014-01-29 03:54:13,closed,cpcloud,0.13.1,5,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6148,b'BUG: boolean query evaluation',b'http://stackoverflow.com/questions/21414959/querying-a-single-row-dataframe-with-anded-conditionals/21415119#21415119\r\n\r\ncc @cpcloud \r\n'
6145,26455707,vfilimonov,jreback,2014-01-28 16:36:00,2016-07-24 13:52:46,2016-07-24 13:52:46,closed,,0.19.0,1,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/6145,b'Describe() does not work properly on timedelta64',"b""When being `describe`d, series of `timedelta64` resulted in misformatted output:\r\n\r\nTest example:\r\n```\r\nxx = pd.DataFrame({'t1':pd.date_range('2010-01-01 00:00:00', freq='T', periods=10),\r\n                   't2':pd.date_range('2010-01-01 00:00:00', freq='2T', periods=10)})\r\nxx['dt'] = xx['t2']-xx['t1']\r\nprint xx\r\n```\r\n\r\nThen `xx.dt.describe()` returns:\r\n```\r\n>>> xx.dt.describe()\r\ncount                                     10\r\nmean     0   00:04:30\r\ndtype: timedelta64[ns]\r\nstd                              1.81659e+11\r\nmin      0   00:00:00\r\ndtype: timedelta64[ns]\r\n25%                 135000000000 nanoseconds\r\n50%      0   00:04:30\r\ndtype: timedelta64[ns]\r\n75%                 405000000000 nanoseconds\r\nmax      0   00:09:00\r\ndtype: timedelta64[ns]\r\nName: dt, dtype: object\r\n```\r\n\r\nDescribe of datetime64 works fine:\r\n```\r\n>>> xx.t1.describe()\r\ncount                      10\r\nunique                     10\r\nfirst     2010-01-01 00:00:00\r\nlast      2010-01-01 00:09:00\r\ntop       2010-01-01 00:00:00\r\nfreq                        1\r\nName: t1, dtype: object\r\n```\r\n\r\nIssue was found both in version 0.13.0 and master (0.13.0-417-g1ed5c3e)."""
6140,26430593,dbew,jreback,2014-01-28 10:54:15,2014-01-29 00:59:07,2014-01-28 15:39:16,closed,,0.13.1,3,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/6140,"b""DataFrame.from_records doesn't handle missing dates (None)""","b'When you construct a DataFrame from a numpy recarray with datetime data and None for missing dates you get an error.\r\n\r\n```python\r\narrdata = [np.array([datetime.datetime(2005, 3, 1, 0, 0), None])]\r\ndtypes = [(\'EXPIRY\', \'<M8[m]\')]\r\nrecarray = np.core.records.fromarrays(arrdata, dtype=dtypes)\r\n\r\ndf = pd.DataFrame.from_records(recarray)\r\nTraceback (most recent call last):\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/ipython-1.1.0_1_ahl1-py2.7.egg/IPython/core/interactiveshell.py"", line 2830, in run_code\r\n    exec code_obj in self.user_global_ns, self.user_ns\r\n  File ""<ipython-input-33-ae01f48c3b82>"", line 1, in <module>\r\n    df = pd.DataFrame.from_records(recarray)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_408_g464c1f9-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 841, in from_records\r\n    columns)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_408_g464c1f9-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 4473, in _arrays_to_mgr\r\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_408_g464c1f9-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 3748, in create_block_manager_from_arrays\r\n    construction_error(len(arrays), arrays[0].shape[1:], axes, e)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_408_g464c1f9-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 3720, in construction_error\r\n    passed,implied))\r\nValueError: Shape of passed values is (1,), indices imply (1, 2)\r\n```\r\n\r\nThis is a regression from 0.11.0. Stepping through the code it looks the initial error is raised in tslib.cast_to_nanoseconds and then caught and re-raised in create_block_manager_from_arrays\r\n\r\nIncidentally, construction does work from a simple array instead of a recarray:\r\n\r\n```python\r\npd.DataFrame(np.array([datetime.datetime(2005, 3, 1, 0, 0), None]))\r\nOut[36]: \r\n           0\r\n0 2005-03-01\r\n1        NaT\r\n\r\n[2 rows x 1 columns]\r\n```\r\n'"
6136,26421388,y-p,jreback,2014-01-28 07:42:52,2014-01-31 21:49:46,2014-01-31 16:25:24,closed,,0.13.1,5,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/6136,"b'Failing test on win/py2.6/64, TestResample:test_how_lambda_functions'","b'This has failed intermittently a couple of times in the last week or two.\r\n\r\n```\r\n=============================================================\r\n=======================\r\nFAILURE: test_how_lambda_functions (pandas.tseries.tests.test_resample.TestResample)\r\n------------------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\Python27-AMD64\\Lib\\unittest\\case.py"", line 331, in run\r\n    testMethod()\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\tseries\\tests\\test_resample.py"", line 640, in test_how_lambda_functions\r\n    tm.assert_series_equal(result[\'bar\'], bar_exp)\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\util\\testing.py"", line 448, in assert_series_equal\r\n    assert_almost_equal(left.values, right.values, check_less_precise)\r\n  File ""testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:2561)\r\n  File ""testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:1803)\r\n  File ""testing.pyx"", line 107, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:2023)\r\nAssertionError: First object is not null, second is null: inf != nan\r\n```'"
6129,26374874,brandjon,jreback,2014-01-27 17:24:19,2014-01-27 17:58:18,2014-01-27 17:58:10,closed,,0.13.1,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6129,b'DataFrame.append() does not create missing columns',"b""Appending a Series to a DataFrame does not create new columns in the DataFrame for unmatched labels in the Series.\r\n\r\n    df = pd.DataFrame({'a': {'x': 1, 'y': 2}, 'b': {'x': 3, 'y': 4}})\r\n    row = pd.Series([5, 6, 7], index=['a', 'b', 'c'], name='z')\r\n\r\n    df = df.append(row)\r\n    print(df.to_string())\r\n\r\nResult:\r\n\r\n       a  b\r\n    x  1  3\r\n    y  2  4\r\n    z  5  6\r\n\r\nThis behavior is contrary to the documentation for append().\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.3.0.final.0\r\nOS: Windows\r\nRelease: 7\r\nProcessor: x86 Family 6 Model 37 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.13.0\r\nCython: Not installed\r\nNumpy: 1.7.0rc1\r\nScipy: Not installed\r\nstatsmodels: Not installed\r\n    patsy: Not installed\r\nscikits.timeseries: Not installed\r\ndateutil: 2.2\r\npytz: 2013.9\r\nbottleneck: Not installed\r\nPyTables: Not Installed\r\n    numexpr: Not Installed\r\nmatplotlib: 1.3.0\r\nopenpyxl: Not installed\r\nxlrd: Not installed\r\nxlwt: Not installed\r\nxlsxwriter: Not installed\r\nsqlalchemy: Not installed\r\nlxml: Not installed\r\nbs4: Not installed\r\nhtml5lib: Not installed\r\nbigquery: Not installed\r\napiclient: Not installed"""
6125,26359308,dbew,jreback,2014-01-27 15:19:18,2014-01-28 10:55:22,2014-01-27 16:22:38,closed,,0.13.1,2,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6125,b'DataFrame.apply not working with datetimes',"b""When you use apply on a DataFrame with datetimes in, the result is unexpected. This is a dataframe with just integers and strings and the result is that we get the market names back out.\r\n\r\n```python\r\npositions = pd.DataFrame([[1, 'ABC', 50], [1, 'YUM', 20], \r\n                          [1, 'DEF', 20], [2, 'ABC', 50],\r\n                          [2, 'YUM', 20], [2, 'DEF', 20]],\r\n                         columns=['a', 'market', 'position'])\r\npositions.apply(lambda r: r['market'], axis=1)\r\nOut[210]: \r\n0    ABC\r\n1    YUM\r\n2    DEF\r\n3    ABC\r\n4    YUM\r\n5    DEF\r\ndtype: object\r\n```\r\n\r\nIf we replace the data in column 'a' with datetimes, then we get the wrong result - the first value in the market column is repeated:\r\n\r\n``` python\r\nimport datetime\r\n\r\npositions = pd.DataFrame([[datetime.datetime(2013, 1, 1), 'ABC', 50], \r\n                           [datetime.datetime(2013, 1, 1), 'YUM', 20],\r\n                           [datetime.datetime(2013, 1, 1), 'DEF', 20],\r\n                           [datetime.datetime(2013, 1, 2), 'ABC', 50],\r\n                           [datetime.datetime(2013, 1, 2), 'YUM', 20], \r\n                           [datetime.datetime(2013, 1, 2), 'DEF', 20]],\r\n                          columns=['a', 'market', 'position'])\r\npositions.apply(lambda r: r['market'], axis=1)\r\nOut[213]: \r\n0    ABC\r\n1    ABC\r\n2    ABC\r\n3    ABC\r\n4    ABC\r\n5    ABC\r\ndtype: object\r\n```\r\n\r\nIf you replace the lambda function with a function which prints the object passed in, then you can see that you only ever receive the first row of the dataframe:\r\n\r\n``` python\r\ndef print_input(r):\r\n    print r\r\n    return 1\r\n\r\npositions.apply(print_input, axis=1)\r\na           2013-01-01 00:00:00\r\nmarket                      ABC\r\nposition                     50\r\nName: 0, dtype: object\r\na           2013-01-01 00:00:00\r\nmarket                      ABC\r\nposition                     50\r\nName: 1, dtype: object\r\na           2013-01-01 00:00:00\r\nmarket                      ABC\r\nposition                     50\r\nName: 2, dtype: object\r\na           2013-01-01 00:00:00\r\nmarket                      ABC\r\nposition                     50\r\nName: 3, dtype: object\r\na           2013-01-01 00:00:00\r\nmarket                      ABC\r\nposition                     50\r\nName: 4, dtype: object\r\na           2013-01-01 00:00:00\r\nmarket                      ABC\r\nposition                     50\r\nName: 5, dtype: object\r\nOut[215]: \r\n0    1\r\n1    1\r\n2    1\r\n3    1\r\n4    1\r\n5    1\r\ndtype: int64\r\n```\r\n\r\nThis is new in the master, I didn't see it in pandas 0.11.0 or 0.13.0. """
6124,26356382,bburan-galenea,jreback,2014-01-27 14:33:39,2014-03-05 21:58:54,2014-03-05 21:58:54,closed,,0.14.0,6,Bug;Groupby;Reshaping,https://api.github.com/repos/pydata/pandas/issues/6124,b'Propagate Series.name attribute when merging series into data frame',"b""See #6068\r\n\r\nUse case\r\n--------------\r\nFacilitate DataFrame group/apply transformations when using a function that returns a Series.  Right now, if we perform the following:\r\n\r\n    import pandas\r\n    df = pandas.DataFrame(\r\n            {'a':  [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\r\n             'b':  [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\r\n             'c':  [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\r\n             'd':  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\r\n             })\r\n\r\n    def count_values(df):\r\n        return pandas.Series({'count': df['b'].sum(), 'mean': df['c'].mean()}, name='metrics')\r\n\r\n    result = df.groupby('a').apply(count_values)\r\n    print result.stack().reset_index()\r\n\r\nWe get the following output:\r\n\r\n       a level_1    0\r\n    0  0   count  2.0\r\n    1  0    mean  0.5\r\n    2  1   count  2.0\r\n    3  1    mean  0.5\r\n    4  2   count  2.0\r\n    5  2    mean  0.5\r\n\r\n    [6 rows x 3 columns]\r\n\r\nIdeally, the series name should be preserved and propagated through these operations such that we get the following output:\r\n\r\n       a metrics    0\r\n    0  0   count  2.0\r\n    1  0    mean  0.5\r\n    2  1   count  2.0\r\n    3  1    mean  0.5\r\n    4  2   count  2.0\r\n    5  2    mean  0.5\r\n\r\n    [6 rows x 3 columns]\r\n\r\nThe only way to achieve this (currently) is:\r\n\r\n    result = df.groupby('a').apply(count_values)\r\n    result.columns.name = 'metrics'\r\n    print result.stack().reset_index()\r\n\r\nHowever, the key issue here is 1) this adds an extra line of code and 2) the name of the series created in the applied function may not be known in the outside block (so we can't properly fix the result.columns.name attribute).\r\n\r\nThe other work-around is to name the index of the series:\r\n\r\n    def count_values(df):\r\n        series = pandas.Series({'count': df['b'].sum(), 'mean': df['c'].mean()})\r\n        series.index.name = 'metrics'\r\n        return series\r\n\r\nDuring the group/apply operation, one approach is to check to see whether series.index has the name attribute set.  If the name attribute is not set, it will set the index.name attribute to the name of the series (thus ensuring the name propagates)."""
6121,26348530,dbew,jreback,2014-01-27 12:10:08,2014-01-28 11:00:46,2014-01-27 14:17:59,closed,,0.13.1,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6121,b'Slice by column then by index fails if columns/rows are repeated.',"b'We\'ve found a problem where repeating a row and a column in a DataFrame fails with a ""Cannot create BlockManager.\\_ref\\_locs"" assertion error.\r\n\r\nThe dataframe is very simple:\r\n\r\n```python\r\ndf = pd.DataFrame(np.arange(25.).reshape(5,5),\r\n                            index=[\'a\', \'b\', \'c\', \'d\', \'e\'],\r\n                            columns=[\'a\', \'b\', \'c\', \'d\', \'e\'])\r\n```\r\n\r\nAnd we pull the data out like this:\r\n``` python\r\nz = df[[\'a\', \'c\', \'a\']]\r\nz.ix[[\'a\', \'c\', \'a\']]\r\nTraceback (most recent call last):\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/ipython-1.1.0_1_ahl1-py2.7.egg/IPython/core/interactiveshell.py"", line 2830, in run_code\r\n    exec code_obj in self.user_global_ns, self.user_ns\r\n  File ""<ipython-input-87-3bdc0aacc4b5>"", line 1, in <module>\r\n    z.ix[[\'a\', \'c\', \'a\']]\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/indexing.py"", line 56, in __getitem__\r\n    return self._getitem_axis(key, axis=0)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/indexing.py"", line 744, in _getitem_axis\r\n    return self._getitem_iterable(key, axis=axis)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/indexing.py"", line 816, in _getitem_iterable\r\n    convert=False)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/generic.py"", line 1164, in take\r\n    new_data = self._data.take(indices, axis=baxis)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 3366, in take\r\n    ref_items=new_axes[0], axis=axis)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 2337, in apply\r\n    do_integrity_check=do_integrity_check)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 1990, in __init__\r\n    self._set_ref_locs(do_refs=True)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 2130, in _set_ref_locs\r\n    \'have _ref_locs set\' % (block, labels))\r\nAssertionError: Cannot create BlockManager._ref_locs because block [FloatBlock: [a], 1 x 3, dtype: float64] with duplicate items [Index([u\'a\', u\'c\', u\'a\'], dtype=\'object\')] does not have _ref_locs set\r\n```\r\n\r\nIf instead we take a copy of the intermediate step, then it works:\r\n\r\n``` python\r\nz = df[[\'a\', \'c\', \'a\']].copy()\r\nz.ix[[\'a\', \'c\', \'a\']]\r\nOut[89]: \r\n    a   c   a\r\na   0   2   0\r\nc  10  12  10\r\na   0   2   0\r\n\r\n[3 rows x 3 columns]\r\n```\r\n\r\nThis means that if you several functions which each do a part of the data processing, you need to know the history of an object to know whether what you\'re doing works. I think .ix should *always* succeed on a DataFrame or Series, regardless of how it was constructed.\r\n\r\n(I\'ve read the discussion at https://github.com/pydata/pandas/issues/6056 about chained operations - but it\'s not something you can avoid if you have a pipeline of small steps instead of one big step).\r\n\r\n\r\nThis wasn\'t an issue in 0.11.0 but is failing in 0.13.0 and the latest master. Here\'s the output of installed versions when running on the master:\r\n\r\ncommit: None\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-308.el5\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB\r\n\r\npandas: 0.13.0-292-g4dcecb0\r\nCython: 0.16\r\nnumpy: 1.7.1\r\nscipy: 0.9.0\r\nstatsmodels: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: None\r\nbottleneck: 0.6.0\r\ntables: 2.3.1-1\r\nnumexpr: 2.0.1\r\nmatplotlib: 1.1.1\r\nopenpyxl: None\r\nxlrd: 0.8.0\r\nxlwt: None\r\nxlsxwriter: None\r\nsqlalchemy: None\r\nlxml: 2.3.6\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None\r\n'"
6120,26345637,dbew,jreback,2014-01-27 11:09:40,2014-01-27 13:04:29,2014-01-27 12:57:15,closed,,0.13.1,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6120,b'Assign to df with repeated column fails',"b'If you have a DataFrame with a repeated or non-unique column, then some assignments fail.\r\n\r\n``` python\r\ndf = pd.DataFrame(np.random.randn(10,2), columns=[\'that\', \'that\'])\r\n\r\ndf2\r\nOut[10]: \r\n   that  that\r\n0     1     1\r\n1     1     1\r\n2     1     1\r\n3     1     1\r\n4     1     1\r\n5     1     1\r\n6     1     1\r\n7     1     1\r\n8     1     1\r\n9     1     1\r\n\r\n[10 rows x 2 columns]\r\n```\r\n\r\nThis is float data and the following works:\r\n\r\n``` python\r\ndf[\'that\'] = 1.0\r\n```\r\n\r\nHowever, this fails with an error and breaks the dataframe (e.g. a subsequent repr will also fail.)\r\n\r\n``` python\r\ndf2[\'that\'] = 1\r\nTraceback (most recent call last):\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/ipython-1.1.0_1_ahl1-py2.7.egg/IPython/core/interactiveshell.py"", line 2830, in run_code\r\n    exec code_obj in self.user_global_ns, self.user_ns\r\n  File ""<ipython-input-11-8701f5b0efe4>"", line 1, in <module>\r\n    df2[\'that\'] = 1\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 1879, in __setitem__\r\n    self._set_item(key, value)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 1960, in _set_item\r\n    NDFrame._set_item(self, key, value)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/generic.py"", line 1057, in _set_item\r\n    self._data.set(key, value)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 2968, in set\r\n    _set_item(item, arr[None, :])\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 2927, in _set_item\r\n    self._add_new_block(item, arr, loc=None)\r\n  File ""/users/is/dbew/pyenvs/timeseries/lib/python2.7/site-packages/pandas-0.13.0_292_g4dcecb0-py2.7-linux-x86_64.egg/pandas/core/internals.py"", line 3108, in _add_new_block\r\n    new_block = make_block(value, self.items[loc:loc + 1].copy(),\r\nTypeError: unsupported operand type(s) for +: \'slice\' and \'int\'\r\n```\r\n\r\nI stepped through the code and it looked like most places handle repeated columns ok except the code that reallocates arrays when the dtype changes.\r\n\r\nI\'ve tested this against pandas 0.13.0 and the latest master. Here\'s the output of installed versions when running on the master:\r\n\r\ncommit: None\r\npython: 2.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.18-308.el5\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB\r\n\r\npandas: 0.13.0-292-g4dcecb0\r\nCython: 0.16\r\nnumpy: 1.7.1\r\nscipy: 0.9.0\r\nstatsmodels: None\r\npatsy: None\r\nscikits.timeseries: None\r\ndateutil: 1.5\r\npytz: None\r\nbottleneck: 0.6.0\r\ntables: 2.3.1-1\r\nnumexpr: 2.0.1\r\nmatplotlib: 1.1.1\r\nopenpyxl: None\r\nxlrd: 0.8.0\r\nxlwt: None\r\nxlsxwriter: None\r\nsqlalchemy: None\r\nlxml: 2.3.6\r\nbs4: None\r\nhtml5lib: None\r\nbq: None\r\napiclient: None \r\n\r\n'"
6105,26316148,jburroni,y-p,2014-01-26 15:58:54,2014-01-27 03:13:55,2014-01-27 03:11:54,closed,y-p,0.13.1,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/6105,b'get_options_data fails when given only the month',"b""if you call this method using only the month, an UnboundedLocalError exception is trown\r\n```\r\ngoog_opt.get_options_data(month=3)\r\n```\r\n\r\n```\r\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/data.pyc in _get_option_data(self, month, year, expiry, table_loc, name)\r\n    639 \r\n    640         if month:\r\n--> 641             name += m1 + str(year)[-2:]\r\n    642         setattr(self, name, option_data)\r\n    643         return option_data\r\n\r\nUnboundLocalError: local variable 'm1' referenced before assignment\r\n```\r\n\r\nApparently the issue is in here: https://github.com/pydata/pandas/blob/master/pandas/io/data.py#L639"""
6098,26301485,aclemen1,y-p,2014-01-25 21:26:44,2014-01-27 03:13:01,2014-01-27 02:36:53,closed,,0.13.1,1,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/6098,"b'BUG: in HTMLFormatter._write_header(), str() fails on column names in unicode'","b""IPython snippet reproducing the problem\r\n-------------------\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame({u'cl\xa8\xa61': [u'a', u'a', u'b', u'b', u'a'],\r\n                   u'cl\xa8\xa62': [u'1er', u'2\xa8\xa8me', u'1er', u'2\xa8\xa8me', u'1er'],\r\n                   'donn\xa8\xa6es1': np.random.randn(5),\r\n                   'donn\xa8\xa6es2': np.random.randn(5)})\r\ndf.pivot_table(rows=[u'cl\xa8\xa61'], cols=[u'cl\xa8\xa62'])\r\n```\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 2.7.5.final.0\r\nOS: Linux\r\nRelease: 2.6.32-358.14.1.el6.x86_64\r\nProcessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.13.0\r\nCython: 0.19.2\r\nNumpy: 1.8.0\r\nScipy: Not installed\r\nstatsmodels: Not installed\r\n    patsy: Not installed\r\nscikits.timeseries: Not installed\r\ndateutil: 2.2\r\npytz: 2013.9\r\nbottleneck: Not installed\r\nPyTables: Not Installed\r\n    numexpr: Not Installed\r\nmatplotlib: Not installed\r\nopenpyxl: Not installed\r\nxlrd: 0.9.2\r\nxlwt: 0.7.5\r\nxlsxwriter: Not installed\r\nsqlalchemy: Not installed\r\nlxml: 3.2.5\r\nbs4: Not installed\r\nhtml5lib: Not installed\r\nbigquery: Not installed\r\napiclient: Not installed\r\n\r\nExpected behavior\r\n--------------\r\nHTML formatted table.\r\n\r\nSeen instead\r\n--------------\r\nWarning message in IPython:\r\n```\r\nWARNING: Exception in text/html formatter: 'ascii' codec can't encode character u'\\xe9' in position 2: ordinal not in range(128)\r\n```"""
6091,26293568,jreback,jreback,2014-01-25 14:15:18,2014-02-16 23:10:44,2014-02-16 23:10:44,closed,,0.14.0,2,Bug;Data Reader;Network IO;Unreliable Test,https://api.github.com/repos/pydata/pandas/issues/6091,b'NWK: investigate unreliable tests for get_components_yahoo',b'disable here: \r\n\r\nhttps://github.com/jreback/pandas/commit/928402e2bd69514a115a0274c06bde30f4057759\r\n\r\n'
6076,26280292,jreback,jreback,2014-01-24 23:58:22,2014-01-24 23:59:39,2014-01-24 23:59:16,closed,,0.14.0,0,Bug;Enhancement;Indexing;Sparse,https://api.github.com/repos/pydata/pandas/issues/6076,b'BUG/ENH: cross sectional on a Sparse DataFrame is unimplemented/buggy',"b""http://stackoverflow.com/questions/21328945/pandas-accessing-rows-of-a-sparsedataframe\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(np.arange(15).reshape(5,3), index=list('abcde'))\r\ndf.loc['b',1] = np.nan  # for good measure...\r\nsparse = df.to_sparse()\r\n\r\nsparse[1]  # This is OK.\r\ndf.loc['b']  # This is also OK.\r\nsparse.loc['b'] # This blows up\r\n```"""
6071,26273586,rcossa,jreback,2014-01-24 21:48:40,2014-02-22 13:40:44,2014-02-22 13:40:44,closed,,0.14.0,3,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/6071,b'BUG: DataFrame.corrwith returns a vector of NaN when DataFrame is square',"b""In [1]: import pandas as pd\r\n   ...: import numpy as np\r\n\r\n\r\nIn [2]: mat = np.random.randn(5,6)\r\n   ...: q = pd.DataFrame(mat, columns=['a','b','c','d','e','f'])\r\n\r\nIn [3]: a = q.corrwith(q['a'])\r\n   ...: print(a)\r\n\r\na    1.000000\r\nb    0.003617\r\nc   -0.025690\r\nd    0.586906\r\ne    0.654205\r\nf   -0.159210\r\ndtype: float64\r\n\r\nIn [4]: a = q[['a','b','c','d','e']].corrwith(q['a'])\r\n   ...: print(a)\r\n\r\na   NaN\r\nb   NaN\r\nc   NaN\r\nd   NaN\r\ne   NaN\r\ndtype: float64\r\n\r\nIn [5]: a = q[['a','b','c','d']].corrwith(q['a'])\r\n   ...: print(a)\r\n\r\na    1.000000\r\nb    0.003617\r\nc   -0.025690\r\nd    0.586906\r\ndtype: float64\r\n"""
6068,26263629,bburan-galenea,jreback,2014-01-24 19:10:46,2014-07-06 10:27:46,2014-03-05 21:58:54,closed,,0.14.0,18,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/6068,b'ENH: Keep series name when merging GroupBy result',"b""closes #6124\r\ncloses #6265 \r\n\r\nUse case\r\n--------------\r\nThis will facilitate DataFrame group/apply transformations when using a function that returns a Series.  Right now, if we perform the following:\r\n\r\n    import pandas\r\n    df = pandas.DataFrame(\r\n            {'a':  [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\r\n             'b':  [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\r\n             'c':  [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\r\n             'd':  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\r\n             })\r\n\r\n    def count_values(df):\r\n        return pandas.Series({'count': df['b'].sum(), 'mean': df['c'].mean()}, name='metrics')\r\n\r\n    result = df.groupby('a').apply(count_values)\r\n    print result.stack().reset_index()\r\n\r\nWe get the following output:\r\n\r\n       a level_1    0\r\n    0  0   count  2.0\r\n    1  0    mean  0.5\r\n    2  1   count  2.0\r\n    3  1    mean  0.5\r\n    4  2   count  2.0\r\n    5  2    mean  0.5\r\n\r\n    [6 rows x 3 columns]\r\n\r\nIdeally, the series name should be preserved and propagated through these operations such that we get the following output:\r\n\r\n       a metrics    0\r\n    0  0   count  2.0\r\n    1  0    mean  0.5\r\n    2  1   count  2.0\r\n    3  1    mean  0.5\r\n    4  2   count  2.0\r\n    5  2    mean  0.5\r\n\r\n    [6 rows x 3 columns]\r\n\r\nThe only way to achieve this (currently) is:\r\n\r\n    result = df.groupby('a').apply(count_values)\r\n    result.columns.name = 'metrics'\r\n    print result.stack().reset_index()\r\n\r\nHowever, the key issue here is 1) this adds an extra line of code and 2) the name of the series created in the applied function may not be known in the outside block (so we can't properly fix the result.columns.name attribute).\r\n\r\nThe other work-around is to name the index of the series:\r\n\r\n    def count_values(df):\r\n        series = pandas.Series({'count': df['b'].sum(), 'mean': df['c'].mean()})\r\n        series.index.name = 'metrics'\r\n        return series\r\n\r\nDuring the group/apply operation, this pull request will check to see whether series.index has the name attribute set.  If the name attribute is not set, it will set the index.name attribute to the name of the series (thus ensuring the name propagates)."""
6043,26135038,cyrusmaher,jreback,2014-01-23 00:19:54,2014-01-23 08:20:10,2014-01-23 01:30:42,closed,,0.13.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6043,b'Inappropriate/inconsistent behavior in data assignment (v. 0.13)',"b""I find it convenient to store lists or series as elements in a dataframe. However, it seems panda gets confused if the iterable is the same length as the number of rows in the dataframe. Here's a minimal example:\r\n\r\n```python\r\nmatchmat_test = pd.DataFrame(index=[0,1], \r\n                             columns=[1], dtype=list)\r\nmatchmat_test.ix[0,0] = [1,2,3]\r\n```\r\nReturns:\r\n```\r\n           0\r\n0  [1, 2, 3]\r\n1        NaN\r\n```\r\n\r\nIf we then assign to the same element again:\r\n```python\r\nmatchmat_test.ix[0,0] = [1,2]\r\n```\r\nwe get:\r\n```  \r\n 0\r\n0  1\r\n1  2\r\n```\r\n"""
6026,26016253,bburan-galenea,jreback,2014-01-21 17:09:39,2014-01-24 12:57:56,2014-01-21 23:06:39,closed,,0.13.1,6,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/6026,b'Segfault when modifying pandas.DataFrame in-place after creating from numpy recarray',"b""The following code generates a segfault when `use_records` is True.  This segfault only occurs when the DataFrame is generated from a record array and then I attempt to modify the series in-place.  This was not an issue in the previous release (v0.12) of Pandas.  Tested this code against 0.13.0-268-g08c1302.\r\n\r\n    import numpy as np\r\n    import pandas\r\n\r\n    data = [('right', 'left', 'left', 'left', 'right', 'left', 'timeout')]\r\n\r\n    use_records = True\r\n    if use_records:\r\n        recarray = np.rec.fromarrays(data, names=['response'])\r\n        df = pandas.DataFrame(recarray)\r\n    else:\r\n        df = pandas.DataFrame({'response': data[0]})\r\n    mask = df.response == 'timeout'\r\n    df.response[mask] = 'none'"""
6018,25950673,theandygross,jreback,2014-01-20 22:18:16,2014-01-21 14:42:07,2014-01-21 14:42:07,closed,,0.13.1,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/6018,b'Multi-Index Slice Selection in Series',"b'Slicing in Series with MultiIndex does not seem to work on master with .ix or .loc.  This is not comparable with previous versions. \r\n\r\n```python\r\nIn [54]:  \r\n\r\ns = pd.Series([1,2,3])\r\ns.index = pd.MultiIndex.from_tuples([(0,0),(1,1), (2,1)])\r\ns[:,0]\r\n\r\nOut[55]:  \r\n\r\n1    2\r\n2    3\r\n\r\nIn [56]:  \r\n\r\ns.ix[:,1]\r\n\r\n\r\nIndexingError                             \r\n\r\nTraceback (most recent call last)\r\n<ipython-input-56-3163789d3245> in <module>()\r\n----> 1 s.ix[:,1]\r\n\r\n/cellar/users/agross/anaconda2/lib/python2.7/site-packages/pandas-0.13.0_120_gdd89ce4-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in __getitem__(self, key)\r\n     52                 pass\r\n     53 \r\n---> 54             return self._getitem_tuple(key)\r\n     55         else:\r\n     56             return self._getitem_axis(key, axis=0)\r\n\r\n/cellar/users/agross/anaconda2/lib/python2.7/site-packages/pandas-0.13.0_120_gdd89ce4-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    593 \r\n    594         # no multi-index, so validate all of the indexers\r\n--> 595         self._has_valid_tuple(tup)\r\n    596 \r\n    597         # ugly hack for GH #836\r\n\r\n/cellar/users/agross/anaconda2/lib/python2.7/site-packages/pandas-0.13.0_120_gdd89ce4-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in _has_valid_tuple(self, key)\r\n    103         for i, k in enumerate(key):\r\n    104             if i >= self.obj.ndim:\r\n--> 105                 raise IndexingError(\'Too many indexers\')\r\n    106             if not self._has_valid_type(k, i):\r\n    107                 raise ValueError(""Location based indexing can only have [%s] ""\r\n\r\nIndexingError: Too many indexers\r\n```'"
6012,25932054,joejohnson111,jreback,2014-01-20 16:58:10,2014-10-01 12:09:00,2014-10-01 12:09:00,closed,,0.16.0,6,Bug;Data Reader,https://api.github.com/repos/pydata/pandas/issues/6012,b'BUG: get_option_data',"b""looked at this this morning some. In the _get_option_data function, when building the url for yahoo, if I try to get current month but forward year, I get the +Options url which returns nearest expiry. The way I quickly fixed it, but i'm not sure its the best way\r\n\r\n            if year != CUR_YEAR:\r\n                    url += '&m={year}-{m1}'.format(year=year, m1=m1)\r\n                \r\n                \r\n            else:\r\n                    if m1 != CUR_MONTH and m2 != CUR_MONTH:\r\n                        url += '&m={year}-{m1}'.format(year=year, m1=m1)\r\n                    else:\r\n                        url += '+Options'\r\n\r\n    else:  # Default to current month\r\n          url += '+Options'\r\n\r\nI apologize in advance for any bad formatting or etiquette as I'm only just starting to use github.\r\n\r\nThanks"""
6006,25896523,cpcloud,y-p,2014-01-20 03:38:37,2014-01-20 22:19:02,2014-01-20 22:19:02,closed,,0.13.1,2,Bug;Build;CI,https://api.github.com/repos/pydata/pandas/issues/6006,b'latest version of dateutil requires six',"b""https://travis-ci.org/pydata/pandas/jobs/17252239\r\n\r\nBut why doesn't this show up on py3.x"""
5999,25874566,yarikoptic,jreback,2014-01-19 04:56:52,2014-01-19 19:41:13,2014-01-19 19:02:13,closed,,0.13.1,0,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5999,b's390 test failure: file descriptor cannot be a negative integer',"b'full log https://buildd.debian.org/status/fetch.php?pkg=pandas&arch=s390x&ver=0.13.0-2&stamp=1390095775&file=log\r\n\r\n```\r\n======================================================================\r\nERROR: test_flush (pandas.io.tests.test_pytables.TestHDFStore)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/PKGBUILDDIR/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_pytables.py"", line 523, in test_flush\r\n    store.flush(fsync=True)\r\n  File ""/PKGBUILDDIR/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/pytables.py"", line 571, in flush\r\n    os.fsync(self._handle.fileno())\r\nValueError: file descriptor cannot be a negative integer (-1)\r\n```\r\nmay be due to a recent update to pytables 3.0.0-2'"
5986,25818782,jreback,jreback,2014-01-17 16:31:02,2016-02-27 16:03:24,2016-02-27 15:22:59,closed,,0.18.0,1,Algos;Bug;Difficulty Novice;Numeric;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5986,b'BUG: algos with timedelta',"b'check that these work / fix (prob just need tests and conversion to i8), datetime64\r\n\r\n- [x] rank #12465\r\n- [x] unique\r\n- [x] factorize #12465\r\n- [x] quantile\r\n- [x] mode #12465\r\n- [x] value_counts\r\n\r\n'"
5975,25746434,TomAugspurger,y-p,2014-01-16 18:14:57,2014-01-19 09:43:24,2014-01-19 09:43:24,closed,,0.13.1,2,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/5975,b'BUG: Scipy interpolate methods are not datetime64 aware',"b""From [this SO question](http://stackoverflow.com/questions/21169182/interpolation-for-some-methods-is-failing-with-a-type-error).\r\n\r\nIs this something we should worry about here, or report upstream to scipy? The workaround I posted on SO is just to explicitly recast the index as an int64, interpolate on that, and then reset the original index. Any reason we shouldn't do this internally?"""
5968,25725340,y-p,y-p,2014-01-16 13:42:26,2014-01-16 22:14:44,2014-01-16 17:58:40,closed,,0.13.1,21,Bug;Performance,https://api.github.com/repos/pydata/pandas/issues/5968,b'df.dtypes.values is not O(1) and repr(df) is therefore slow for large frames',"b'For the FEC dataset, it takes about 1.5 sec to get a repr. and prun\r\nputs it all in `infer_dtype`.\r\n\r\n'"
5963,25702862,cancan101,jreback,2014-01-16 04:33:47,2014-11-26 02:30:02,2014-11-26 02:30:02,closed,,0.15.2,4,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5963,b'Unable to compare timedelta64 to timedelta',"b'```\r\nIn [426]:\r\ns = pd.Series([timedelta(days=1), timedelta(days=2)])\r\ns > timedelta(days=1)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-426-9094b7c93677> in <module>()\r\n      1 s = pd.Series([timedelta(days=1), timedelta(days=2)])\r\n----> 2 s > timedelta(days=1)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0rc1_78_g142ca62-py2.7-linux-x86_64.egg/pandas/core/ops.pyc in wrapper(self, other)\r\n    561 \r\n    562             # scalars\r\n--> 563             res = na_op(values, other)\r\n    564             if np.isscalar(res):\r\n    565                 raise TypeError(\'Could not compare %s type with Series\'\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0rc1_78_g142ca62-py2.7-linux-x86_64.egg/pandas/core/ops.pyc in na_op(x, y)\r\n    528 \r\n    529             try:\r\n--> 530                 result = getattr(x, name)(y)\r\n    531                 if result is NotImplemented:\r\n    532                     raise TypeError(""invalid type comparison"")\r\n\r\nTypeError: can\'t compare datetime.timedelta to long\r\n```\r\n\r\nThe comparison does work for ```timedelta64```:\r\n\r\n```\r\nIn [45]: s > np.timedelta64(timedelta(days=1))\r\nOut[45]: \r\n0    False\r\n1     True\r\ndtype: bool\r\n```'"
5961,25701517,cancan101,jreback,2014-01-16 03:46:06,2014-01-16 11:54:39,2014-01-16 11:54:39,closed,,0.13.1,0,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/5961,b'to_datetime broken for Tz-aware datetimes and NaTs in same array',"b'This works:\r\n```\r\nIn [400]: \r\npd.to_datetime([pd.Timestamp(""2013-1-1"", tz=pytz.timezone(\'US/Eastern\'))])\r\n\r\nOut[400]:\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2013-01-01 00:00:00-05:00]\r\nLength: 1, Freq: None, Timezone: US/Eastern\r\n```\r\n\r\nbut this (adding an NaT) does not. At the very least, the error message is misleading:\r\n```\r\nIn [401]:\r\npd.to_datetime([pd.Timestamp(""2013-1-1"", tz=pytz.timezone(\'US/Eastern\')), pd.NaT])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-401-3b5b8a67d2e1> in <module>()\r\n----> 1 pd.to_datetime([pd.Timestamp(""2013-1-1"", tz=pytz.timezone(\'US/Eastern\')), pd.NaT])\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0rc1_78_g142ca62-py2.7-linux-x86_64.egg/pandas/tseries/tools.pyc in to_datetime(arg, errors, dayfirst, utc, box, format, coerce, unit)\r\n    137         return Series(values, index=arg.index, name=arg.name)\r\n    138     elif com.is_list_like(arg):\r\n--> 139         return _convert_listlike(arg, box=box)\r\n    140 \r\n    141     return _convert_listlike(np.array([ arg ]), box=box)[0]\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0rc1_78_g142ca62-py2.7-linux-x86_64.egg/pandas/tseries/tools.pyc in _convert_listlike(arg, box)\r\n    127                 return DatetimeIndex._simple_new(values, None, tz=tz)\r\n    128             except (ValueError, TypeError):\r\n--> 129                 raise e\r\n    130 \r\n    131     if arg is None:\r\n\r\nValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True\r\n```\r\n\r\nI tracked down this issue to a bug in ```datetime_to_datetime64```\r\n\r\nWhen iterating over the elements in ```datetime_to_datetime64``` the check for nullness is ```util._checknull(val)``` which is ```False``` for ```NaT```.\r\n\r\nThe correct null check is to use  ```checknull``` from  ```lib```\r\n\r\nPR on the way.'"
5955,25679361,sleibman,jreback,2014-01-15 20:30:31,2014-03-29 01:53:36,2014-03-29 01:53:36,closed,,0.14.0,2,Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/5955,"b""rolling_mean with freq='D' returns all NaNs when there is exactly 1 data point per day""","b'related to #3020\r\n\r\n```python\r\n$ python\r\nPython 2.7.4 (default, Apr 23 2013, 12:22:04) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import datetime\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n\'0.12.0\'\r\n>>> indices = [datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)]\r\n>>> series = pd.Series(range(1, 6), index=indices)\r\n>>> series = series.map(lambda x: float(x))  # range() returns ints, so force to float\r\n>>> series = series.sort_index()  # already sorted, but just to be clear\r\n>>> series  # here\'s what our input series looks like\r\n1975-01-01 12:00:00    1\r\n1975-01-02 12:00:00    2\r\n1975-01-03 12:00:00    3\r\n1975-01-04 12:00:00    4\r\n1975-01-05 12:00:00    5\r\ndtype: float64\r\n>>> pd.rolling_mean(series, window=2, freq=\'D\')  # these results will be wrong\r\n1975-01-01   NaN\r\n1975-01-02   NaN\r\n1975-01-03   NaN\r\n1975-01-04   NaN\r\n1975-01-05   NaN\r\nFreq: D, dtype: float64\r\n>>> better_series = series.append(pd.Series([3.0], index=[datetime.datetime(1975, 1, 3, 6, 0)]))\r\n>>> better_series = better_series.sort_index()\r\n>>> better_series  # here\'s a revised input with more than one datapoint on one of the days\r\n1975-01-01 12:00:00    1\r\n1975-01-02 12:00:00    2\r\n1975-01-03 06:00:00    3\r\n1975-01-03 12:00:00    3\r\n1975-01-04 12:00:00    4\r\n1975-01-05 12:00:00    5\r\ndtype: float64\r\n>>> pd.rolling_mean(better_series, window=2, freq=\'D\')  # These results will be correct and are what I expected above\r\n1975-01-01    NaN\r\n1975-01-02    1.5\r\n1975-01-03    2.5\r\n1975-01-04    3.5\r\n1975-01-05    4.5\r\nFreq: D, dtype: float64\r\n```\r\n'"
5947,25659763,acowlikeobject,jreback,2014-01-15 16:27:40,2014-01-15 19:25:07,2014-01-15 19:25:07,closed,,0.13.1,1,Bug;Msgpack,https://api.github.com/repos/pydata/pandas/issues/5947,b'Msgpack serialization fails for DatetimeIndex with only two rows',"b""A rare/corner case, but thought I'd document it.\r\n\r\npandas.read_msgpack() throws a ```ValueError: Dates do not conform to passed frequency``` if the index is a DatetimeIndex and there are only two rows of data.\r\n\r\n```python\r\nIn [66]: df = pd.DataFrame([1, 2], index=pd.date_range('1/1/2013', '1/2/2013'))\r\n\r\nIn [67]: df\r\nOut[67]: \r\n            0\r\n2013-01-01  1\r\n2013-01-02  2\r\n\r\n[2 rows x 1 columns]\r\n\r\nIn [68]: pd.read_msgpack(df.to_msgpack())\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-68-fae93928effe> in <module>()\r\n----> 1 pd.read_msgpack(df.to_msgpack())\r\n\r\n/home/user/environments/python3/src/pandas/pandas/io/packers.py in read_msgpack(path_or_buf, iterator, **kwargs)\r\n    158         try:\r\n    159             fh = compat.BytesIO(path_or_buf)\r\n--> 160             return read(fh)\r\n    161         finally:\r\n    162             fh.close()\r\n\r\n/home/user/environments/python3/src/pandas/pandas/io/packers.py in read(fh)\r\n    136 \r\n    137     def read(fh):\r\n--> 138         l = list(unpack(fh))\r\n    139         if len(l) == 1:\r\n    140             return l[0]\r\n\r\n/home/user/environments/python3/src/pandas/pandas/msgpack.cpython-33m.so in pandas.msgpack.Unpacker.__next__ (pandas/msgpack.cpp:7846)()\r\n\r\n/home/user/environments/python3/src/pandas/pandas/msgpack.cpython-33m.so in pandas.msgpack.Unpacker._unpack (pandas/msgpack.cpp:6981)()\r\n\r\n/home/user/environments/python3/src/pandas/pandas/io/packers.py in decode(obj)\r\n    448     elif typ == 'datetime_index':\r\n    449         data = unconvert(obj['data'], np.int64, obj.get('compress'))\r\n--> 450         result = globals()[obj['klass']](data, freq=obj['freq'], name=obj['name'])\r\n    451         tz = obj['tz']\r\n    452 \r\n\r\n/home/user/environments/python3/src/pandas/pandas/tseries/index.py in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, **kwds)\r\n    283                 inferred = subarr.inferred_freq\r\n    284                 if inferred != offset.freqstr:\r\n--> 285                     raise ValueError('Dates do not conform to passed '\r\n    286                                      'frequency')\r\n    287 \r\n\r\nValueError: Dates do not conform to passed frequency\r\n```\r\n\r\nAdding a third row makes the problem go away:\r\n```python\r\nIn [13]: df = pd.DataFrame([1, 2, 3], index=pd.date_range('1/1/2013', '1/3/2013'))\r\n\r\nIn [14]: df\r\nOut[14]: \r\n            0\r\n2013-01-01  1\r\n2013-01-02  2\r\n2013-01-03  3\r\n\r\n[3 rows x 1 columns]\r\n\r\nIn [15]: pd.read_msgpack(df.to_msgpack())\r\nOut[15]: \r\n            0\r\n2013-01-01  1\r\n2013-01-02  2\r\n2013-01-03  3\r\n\r\n[3 rows x 1 columns]\r\n```"""
5945,25652791,dershow,jreback,2014-01-15 14:57:06,2014-04-24 01:24:31,2014-04-24 01:24:31,closed,,0.14.0,8,Bug;IO Excel;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5945,"b""Can't read excel decimal seconds""","b'related to #4332\r\n\r\nI have an excel spread sheet (.xls) that contains a time column. The time is displayed in Excel as minutes:seconds.tenths of seconds. Such as ""50:59.2"" ""50:59.4"". The raw data contains hours:minutes:seconds.decimalseconds.\r\n\r\nIt seems that Pandas uses xldate_as_tuple() which apparently rounds all seconds, so the decimal part is dropped.  So the above two data points both import as ""50:59"".  \r\n\r\nI suggest  using a different conversion method, at least as an option.  That way the data will not be dropped, as it is now.\r\nI did also post this same issue to xlrd.  '"
5932,25560807,filmor,jreback,2014-01-14 09:05:22,2014-01-14 13:45:46,2014-01-14 13:45:01,closed,,0.13.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5932,b'BUG: Copying an empty DataFrame with index results in a broken BlockManager',"b'The following line illustrates the problem:\r\n\r\n    df = pd.DataFrame(index=[0]).copy()\r\n    df[""a""] = 0\r\n\r\nThis results in an `AttributeError` since the function `BlockManager.make_empty` initialises `df._data.blocks` with a numpy array object instead of an empty list. Thus subsequent calls to `blocks.pop` and `blocks.append` fail.\r\n\r\nI have a patch coming up.'"
5928,25533548,twiecki,jreback,2014-01-13 22:21:55,2014-01-14 03:05:23,2014-01-14 00:24:21,closed,,0.13.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5928,b'Assignment using .ix[] raises ValueError',"b""This works with `0.12` but not with master:\r\n\r\n```python\r\nIn [8]: import pandas as pd\r\nIn [10]: x = pd.DataFrame({'a': [1, 2, 3]})\r\nIn [13]: x['a'].ix[[0, 1, 2]] = -x['a'].ix[[0, 1, 2]]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-30fcfd21bb22> in <module>()\r\n----> 1 x['a'].ix[[0, 1, 2]] = -x['a'].ix[[0, 1, 2]]\r\n\r\n/home/wiecki/envs/hddm/local/lib/python2.7/site-packages/pandas/core/indexing.py in __setitem__(self, key, value)\r\n     94             indexer = self._convert_to_indexer(key, is_setter=True)\r\n     95 \r\n---> 96         self._setitem_with_indexer(indexer, value)\r\n     97 \r\n     98     def _has_valid_type(self, k, axis):\r\n\r\n/home/wiecki/envs/hddm/local/lib/python2.7/site-packages/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)\r\n    409 \r\n    410             if isinstance(value, ABCSeries):\r\n--> 411                 value = self._align_series(indexer, value)\r\n    412 \r\n    413             elif isinstance(value, ABCDataFrame):\r\n\r\n/home/wiecki/envs/hddm/local/lib/python2.7/site-packages/pandas/core/indexing.py in _align_series(self, indexer, ser)\r\n    516             return ser.reindex(ax).values\r\n    517 \r\n--> 518         raise ValueError('Incompatible indexer with Series')\r\n    519 \r\n    520     def _align_frame(self, indexer, df):\r\n\r\nValueError: Incompatible indexer with Series\r\n> /home/wiecki/envs/hddm/local/lib/python2.7/site-packages/pandas/core/indexing.py(518)_align_series()\r\n    517 \r\n--> 518         raise ValueError('Incompatible indexer with Series')\r\n    519 \r\n```\r\n\r\nThis is where it showed up: https://github.com/hddm-devs/hddm/issues/35"""
5917,25491378,gbakalian,jreback,2014-01-13 11:34:08,2014-05-10 11:32:16,2014-05-10 11:32:16,closed,,0.14.0,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5917,b'dayfirst non working when constructing a pd.DatetimeIndex with an array',"b""assert pd.DatetimeIndex(['10/02/2014'], dayfirst=True) == pd.DatetimeIndex(np.asarray(['10/02/2014'], dtype=object), dayfirst=True)\r\n"""
5914,25473496,theandygross,jreback,2014-01-13 00:36:27,2014-01-13 13:16:33,2014-01-13 13:16:06,closed,,0.13.1,4,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/5914,b'Float Index Interpreted as Object dtype',"b""When I cast a FloatIndex to a numpy array, I get an array of objects.  The desired behavior should be float.  This is backwards incompatible with pre-FloatIndex behavior.  \r\n\r\n```python\r\nIn [119]:   s = pd.Series([1,2,3],[1.,2.,3.])\r\n               array(s.index).dtype\r\n\r\nOut[119]:  dtype('O')\r\n```\r\n\r\nThe Int64Index works as expected.\r\n```python\r\nIn [114]:    s = pd.Series([1,2,3],[1,2,3])\r\n                array(s.index).dtype\r\n\r\nOut[114]:  dtype('int64')\r\n````"""
5913,25471261,CarstVaartjes,jreback,2014-01-12 22:21:56,2015-08-20 02:20:53,2015-08-20 02:20:53,closed,,Next Major Release,21,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5913,b'HDF5 Select with Filter gives incorrect results when using Iteration',"b""Linked Issue: https://github.com/PyTables/PyTables/issues/319\r\n\r\nHi,\r\n\r\nI have a dataframe saved in HDF5 with 6.7 million records (about 425MB). As you can see below, it gives an incorrect result when it has to do multiple iterations (which is a serious issue for me actually). ->\r\n\r\n    # prepare\r\n    fact_hdf = pd.HDFStore('iteration_test3.h5', mode='r')\r\n    store_name = 'store_0'\r\n    column_list = ['o', 'm101']\r\n    where = [('o', '=', [-15534, -16280, -17113, -14786, -14790, -18074])]\r\n    \r\n    # describe\r\n    fact_hdf[store_name]\r\n    '''\r\n    <class 'pandas.core.frame.DataFrame'>\r\n    Int64Index: 6708525 entries, 0 to 6708524\r\n    Columns: 27 entries, a1007 to m105_b\r\n    dtypes: float64(11), int64(16)\r\n    '''\r\n    \r\n    # non iterated\r\n    fact_hdf.select(store_name, columns=column_list, where=where)['o'].unique()\r\n    '''\r\n    array([-14790, -16280, -15534, -14786, -18074, -17113])\r\n    '''\r\n    \r\n    # iterated by 1mln\r\n    chunksize = 1000000 # 1 mln\r\n    for sub_df in fact_hdf.select(store_name, columns=column_list, where=where, iterator=True, chunksize=chunksize):\r\n        sub_df['o'].unique()\r\n    \r\n    '''\r\n    array([-14790, -16280, -15534, -14786, -18074, -17113])\r\n    array([-14786, -14790, -15534, -18074, -16280, -17113])\r\n    array([-14790, -18074, -15534, -14786, -16280, -17113])\r\n    array([-16280, -14790, -15534, -18074, -14786, -17113])\r\n    array([-14790, -14786, -15534, -18074, -16280, -17113])\r\n    array([-18511, -17074, -17099, -16876, -18060, -15965, -14733, -16300,\r\n           -15534, -14790, -16280, -14786, -18074, -17113])\r\n    array([-16280, -14790, -18074, -15534, -14786, -17113])\r\n    '''\r\n    \r\n    # iterated by 10mln\r\n    chunksize = 10000000 # 1 mln\r\n    for sub_df in fact_hdf.select(store_name, columns=column_list, where=where, iterator=True, chunksize=chunksize):\r\n        sub_df['o'].unique()\r\n    \r\n    '''\r\n    array([-14790, -16280, -15534, -14786, -18074, -17113])\r\n    '''\r\n\r\nReproducing the bug can be hard (not all files have the same issue), but this specific case happens again (it's not a corrupted HDF5 file, if you write away the dataframe it happens again).\r\n\r\nI can share the file with pandas/pytables developers if needed (it has to be personal though / not uploaded to a public location; message me for a sftp link)\r\n\r\nSome technical stuff\r\n - Pandas 0.12\r\n - Pytables 3.0\r\n - Ubuntu 12.04LTS 64bit\r\n - File is saved with BLOSC, complevel=9\r\n\r\nKind regards,\r\n\r\nCarst"""
5905,25444644,jreback,jreback,2014-01-11 14:49:50,2014-01-11 15:15:28,2014-01-11 15:15:28,closed,,,0,Bug;Indexing;Missing-data,https://api.github.com/repos/pydata/pandas/issues/5905,b'BUG: fully reindexing panel is incorrect when missing items',"b""Fully reindexing a panel copies sub-frames when missing top-level items\r\n\r\nIOW, this should have a ``nan`` frame for 'Item2'\r\n\r\n```\r\nIn [1]: df = DataFrame(np.random.randn(4,3))\r\n [4]: p = Panel({ 'Item1' : df })\r\n\r\nIn [12]: p['Item2'] = np.nan\r\n\r\nIn [13]: p\r\nOut[13]: \r\n<class 'pandas.core.panel.Panel'>\r\nDimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)\r\nItems axis: Item1 to Item2\r\nMajor_axis axis: 0 to 3\r\nMinor_axis axis: 0 to 2\r\n\r\nIn [5]:  items = ['Item1','Item2']\r\n\r\nIn [8]: major_axis = np.arange(4)\r\n\r\nIn [9]: minor_axis = np.arange(3)\r\n\r\nIn [10]: result = p.reindex(items=items,major_axis=major_axis,minor_axis=minor_axis)\r\n\r\nIn [11]: result.ix['Item2']\r\nOut[11]: \r\n          0         1         2\r\n0  1.420715  0.972559 -1.334794\r\n1 -0.165063 -1.811950 -1.107652\r\n2 -1.412676  0.376777 -0.521125\r\n3  2.119229  1.221648 -1.078299\r\n\r\n[4 rows x 3 columns]\r\n\r\nIn [14]: p['Item2']\r\nOut[14]: \r\n    0   1   2\r\n0 NaN NaN NaN\r\n1 NaN NaN NaN\r\n2 NaN NaN NaN\r\n3 NaN NaN NaN\r\n\r\n[4 rows x 3 columns]\r\n```"""
5897,25368993,saffsd,jreback,2014-01-10 01:13:38,2014-07-19 14:13:11,2014-07-01 15:28:58,closed,,0.14.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/5897,"b""'rot' parameter to DataFrame.boxplot not applied to some axes when multi-panel plot is produced using 'by'""","b""Minimal snippet:\r\n```python\r\nimport pandas as pd\r\nfrom numpy.random import randn\r\nimport matplotlib.pyplot as plt\r\n\r\nd = pd.DataFrame({'one':randn(5), 'two':randn(5), 'three':randn(5), 'label':['label'] * 5},\r\n        columns = ['one','two','three', 'label'])\r\nbp= d.boxplot(by='label', rot=45)\r\nplt.savefig('xxx.png')\r\n```\r\n\r\nOutput produced:\r\n![xxx](https://f.cloud.github.com/assets/128034/1884064/fcb7cd40-7993-11e3-8a07-58d7613407e2.png)\r\n\r\nIssue:\r\nThe label on the left is rotated but the label on the right is not.\r\n\r\nVersions: \r\npandas '0.12.0'\r\nmatplotlib '1.3.1'"""
5895,25332570,y-p,y-p,2014-01-09 16:40:22,2014-01-27 09:52:34,2014-01-15 19:00:50,closed,,0.13.1,2,Bug;Testing;Unreliable Test,https://api.github.com/repos/pydata/pandas/issues/5895,b'Clipboard test failure on windows',"b'Intermittent, seen on 2.7 64bits\r\n\r\n```python\r\n======================================================================\r\nERROR: test_round_trip_frame_string (classname)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\Python27-AMD64\\Lib\\unittest\\case.py"", line 331, in run\r\n    testMethod()\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\io\\tests\\test_clipboard.py"", line 63, in test_round_trip_frame_string\r\n    self.check_round_trip_frame(dt,excel=False)\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\io\\tests\\test_clipboard.py"", line 50, in check_round_trip_frame\r\n    data.to_clipboard(excel=excel, sep=sep)\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\core\\generic.py"", line 934, in to_clipboard\r\n    clipboard.to_clipboard(self, excel=excel, sep=sep, **kwargs)\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\io\\clipboard.py"", line 76, in to_clipboard\r\n    clipboard_set(objstr)\r\n  File ""C:\\workspace\\pandas_tests\\BITS\\64\\PYTHONVER\\27\\pandas\\util\\clipboard.py"", line 72, in winSetClipboard\r\n    ctypes.cdll.msvcrt.strcpy(ctypes.c_char_p(pchData), bytes(text))\r\nWindowsError: exception: access violation writing 0x0000000000000000\r\n```'"
5891,25306915,deanjones,jreback,2014-01-09 10:03:52,2016-03-25 19:44:54,2014-02-18 02:27:11,closed,,0.14.0,34,Bug;Dtypes;IO CSV;IO Excel,https://api.github.com/repos/pydata/pandas/issues/5891,b'Cannot convert numbers to strings when reading an Excel spreadsheet',"b""I'm reading some excel spreadsheets (xlsx format) into pandas using ```read_excel```, which generally works great. The problem I have is that when a column contains numbers, pandas converts these to float64 type, and I would like them to be treated as strings. After reading them in, I can convert the column to str:\r\n\r\n     my_frame.my_col = my_frame.my_col.astype('str') \r\n\r\nThis works as far as assigning the right type to the column, but when I view the values in this column, the strings are formatted in scientific-format e.g. 8.027770e+14, which is not what I want. I like to be able to tell Pandas to read the columns as strings. My current solution involves dropping down to xlrd to read the spreadsheet.\r\n\r\nSee stackoverflow question: http://stackoverflow.com/q/20970483/690890"""
5890,25301615,dorandeluz,jreback,2014-01-09 08:00:54,2014-01-13 16:18:13,2014-01-13 16:18:13,closed,,0.13.1,2,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/5890,"b'dt+BDay(n) gives the wrong date if n>5, n%5==0 and dt not on offset'","b""This issue is closely related to bug #2680 and I believe it is also a bug.\r\n\r\nThe problem appears when adding a number of BDay n with n > 5, n % 5 == 0 to \r\na starting date which is not on offset.\r\n\r\nFor instance\r\n\r\n``` python\r\ndt = Timestamp('20140105') # Sunday\r\nprint (dt + BDay(10)).strftime('%A')\r\n```\r\n```\r\nSunday\r\n```\r\n\r\nWe except Friday and not Sunday. Likewise, Friday is 'gone' from the week in the following loop:\r\n\r\n``` python\r\nfor i in range(8,12):\r\n    print i, (Timestamp('20140104') + BDay(i)).strftime('%A')\r\n```\r\n```\r\n8 Wednesday\r\n9 Thursday\r\n10 Saturday\r\n11 Monday\r\n```\r\n\r\nIt does yield the correct results for substraction however:\r\n\r\n``` python\r\nfor i in range(9,13):\r\n    print i, (Timestamp('20140105') - BDay(i)).strftime('%A')\r\n```\r\n```\r\n9 Tuesday\r\n10 Monday\r\n11 Friday\r\n12 Thursday\r\n```\r\n\r\nI have patched tseries/offsets.py with a couple of lines and it corrects the problem without apparently breaking any of the existing tests (version 0.13.0-94-g0bab303). I am not super familiar with git but can try to write a PR about this if appropriate."""
5888,25298049,cancan101,jreback,2014-01-09 05:55:57,2016-05-26 23:56:09,2016-05-26 23:56:09,closed,,Next Major Release,6,Bug;Docs;IO CSV,https://api.github.com/repos/pydata/pandas/issues/5888,b'API/DOC: status of low_memory kwarg of read_csv/table',b'I am getting the following warning:\r\n```\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0rc1_78_g142ca62-py2.7-linux-x86_64.egg/pandas/io/parsers.py:1050: \r\nDtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\r\n  data = self._reader.read(nrows)\r\n```\r\n\r\nbut I can find no documentation for ```low_memory```'
5878,25243331,mirage007,jreback,2014-01-08 14:33:35,2014-05-12 13:34:58,2014-05-12 13:34:58,closed,,0.14.0,1,Bug;Reshaping;Timezones,https://api.github.com/repos/pydata/pandas/issues/5878,b'pivot function on timezone aware objects does not preserve timezone info in resulting dataframe index',"b'This bug is in 0.12.0\r\n\r\nUsing example DataFrame like below:\r\n```\r\n   col  data                       time\r\n0    1     0  2013-03-22 11:00:00-04:00\r\n1    2     1  2013-03-22 15:00:00-04:00\r\n2    2     2  2013-03-22 11:00:00-04:00\r\n3    1     3  2013-03-22 15:00:00-04:00\r\n```\r\nAfter pivoting, the old behavior in 0.10.1 properly preserved the timezone info in the index, resulting in a new DataFrame like such:\r\n\r\n```\r\ncol                        1  2\r\ntime                           \r\n2013-03-22 11:00:00-04:00  0  2\r\n2013-03-22 15:00:00-04:00  3  1\r\n```\r\n\r\nHowever in 0.12.0 this behavior is lost resulting in an index that does not have the timezone information\r\n\r\n```\r\ncol                  1  2\r\ntime                     \r\n2013-03-22 15:00:00  0  2\r\n2013-03-22 19:00:00  3  1\r\n```\r\n\r\nBelow is the code to reproduce this issue:\r\n```python\r\nimport pandas\r\nprint pandas.__version__\r\nimport datetime\r\nimport pandas as pn\r\nimport pytz\r\nest = pytz.timezone(\'US/Eastern\')\r\ndt1 = est.localize(datetime.datetime(2013,3,22,11,0,0))\r\ndt2 = est.localize(datetime.datetime(2013,3,22,15,0,0))\r\ndf = pn.DataFrame({\'time\': [dt1, dt2, dt1, dt2], \'col\': [1, 2, 2, 1], \'data\': range(4)})\r\npivotDf =  df.pivot(\'time\', \'col\', \'data\')\r\nprint df\r\nprint pivotDf\r\nprint pivotDf.index\r\n```\r\n\r\nthe output from 0.10.1 is:\r\n\r\n```\r\n0.10.1\r\n   col  data                       time\r\n0    1     0  2013-03-22 11:00:00-04:00\r\n1    2     1  2013-03-22 15:00:00-04:00\r\n2    2     2  2013-03-22 11:00:00-04:00\r\n3    1     3  2013-03-22 15:00:00-04:00\r\ncol                        1  2\r\ntime                           \r\n2013-03-22 11:00:00-04:00  0  2\r\n2013-03-22 15:00:00-04:00  3  1\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2013-03-22 11:00:00, 2013-03-22 15:00:00]\r\nLength: 2, Freq: None, Timezone: US/Eastern\r\n```\r\n\r\nthe output from 0.12.0 is:\r\n```\r\n0.12.0\r\n   col  data                       time\r\n0    1     0  2013-03-22 11:00:00-04:00\r\n1    2     1  2013-03-22 15:00:00-04:00\r\n2    2     2  2013-03-22 11:00:00-04:00\r\n3    1     3  2013-03-22 15:00:00-04:00\r\ncol                  1  2\r\ntime                     \r\n2013-03-22 15:00:00  0  2\r\n2013-03-22 19:00:00  3  1\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2013-03-22 15:00:00, 2013-03-22 19:00:00]\r\nLength: 2, Freq: None, Timezone: None\r\n```\r\nNotice the None in the ""Timezone: "" infor of the DatetimeIndex.'"
5877,25239541,sadruddin,jreback,2014-01-08 13:38:56,2014-01-08 17:23:27,2014-01-08 17:23:12,closed,,0.13.1,6,API Design;Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/5877,"b'""IndexingError: Unalignable boolean Series key provided"" when passing an empty object Series'","b""For some reason, with 0.13, an empty object Series is interpreted as a boolean one, when using to fetch elements in another Series:\r\n \r\n    pandas.Series(['A', 'B'])[pandas.Series([], dtype=object)]\r\n\r\nThe following works though, so the dtype confusion seems to be confined to empty Series:\r\n\r\n    pandas.Series(['A', 'B'])[pandas.Series(['C'], dtype=object)]\r\n"""
5873,25221489,felixlawrence,jreback,2014-01-08 06:33:14,2015-01-02 15:41:39,2015-01-02 15:41:39,closed,,0.16.0,5,Bug;Dtypes;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/5873,b'BUG: has_duplicates misbehaves when multiindex has a NaN',"b""When (at least) one element in a MultiIndex contains a NaN, has_duplicates starts to behave strangely:\r\n\r\n```python\r\n>>> idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])\r\n>>> idx\r\nMultiIndex\r\n[(101, 3.5), (102, nan)]\r\n>>> idx.has_duplicates\r\nTrue\r\n>>> idx.get_duplicates()\r\n[]\r\n```\r\n\r\nI would expect has_duplicates to return False here, because 102 is not the same as 101.\r\n\r\nI would also expect it to return false for the MultiIndex\r\n```python\r\nMultiIndex\r\n[(101, 3.5), (101, nan)]\r\n```\r\nsince 3.5 != NaN, but this case is more debatable.\r\n\r\nThis is important because you can't call .unstack() on a series with a MultiIndex for which has_duplicates is True, even if the MultiIndex is of high dimension and the dimensions containing the NaN(s) are not involved in the operation.\r\n\r\nThis is with pandas 0.12.0"""
5869,25193654,jreback,jreback,2014-01-07 20:10:08,2014-01-27 11:32:09,2014-01-07 20:44:31,closed,,0.13.1,0,Bug;Dtypes;Groupby;Regression;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5869,b'BUG: groupby with datetime and cythonized functions regression from 0.12',"b""```\r\nIn [30]: DataFrame(dict(A = Timestamp('20130101'), B = np.arange(5))).groupby('A')['A'].max()\r\nOut[30]: \r\nA\r\n2013-01-01    1356998400000000000\r\nName: A, dtype: int64\r\n```\r\n\r\nshould match\r\n```\r\nIn [31]: DataFrame(dict(A = Timestamp('20130101'), B = np.arange(5))).groupby('A')['A'].apply(lambda x: x.max())\r\nOut[31]: \r\nA\r\n2013-01-01   2013-01-01 00:00:00\r\ndtype: datetime64[ns]\r\n```"""
5863,25131370,CarstVaartjes,jreback,2014-01-06 21:34:29,2014-01-07 00:41:00,2014-01-07 00:41:00,closed,,0.13.1,7,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5863,"b'""high"" datetime conversion incorrect'","b'Hi,\r\n\r\nI have a situation where I have ""far away"" end dates (3 examples included below). Normal Python strptime conversion handles this correctly, but the to_datetime functionality does a Martin McFly and ends up in the 1800s. See this example that reproduces it:\r\n\r\n    # input\r\n    time_ser = pd.Series([np.nan, \'2013-04-08 00:00:00.000\', \'2013-06-04 00:00:00.000\', \'2013-09-06 00:00:00.000\', \'2013-10-02 00:00:00.000\', \'2013-10-03 00:00:00.000\', \'2013-10-30 00:00:00.000\', \'2013-10-31 00:00:00.000\', \'2013-11-30 00:00:00.000\', \'2013-12-02 00:00:00.000\', \'2013-12-17 00:00:00.000\', \'2013-12-31 00:00:00.000\', \'2014-01-15 00:00:00.000\', \'2014-01-31 00:00:00.000\', \'2014-02-15 00:00:00.000\', \'2014-02-28 00:00:00.000\', \'2014-03-15 00:00:00.000\', \'2014-03-31 00:00:00.000\', \'2014-06-15 00:00:00.000\', \'2014-06-30 00:00:00.000\', \'2099-12-31 00:00:00.000\', \'2999-12-31 00:00:00.000\', \'9990-12-31 00:00:00.000\', \'9999-12-31 00:00:00.000\'])\r\n    \r\n    # see last 3 results\r\n    pd.to_datetime(time_ser, \'%Y-%m-%d %H:%M:%S.%f\')\r\n    \r\n    # normal python\r\n    import math\r\n    for x in sorted(time_ser.unique()):\r\n        if isinstance(x, basestring):\r\n            print \'{} converts to {}\'.format( x, datetime.datetime.strptime(x, \'%Y-%m-%d %H:%M:%S.%f\'))\r\n    \r\n    # This is the erroneous result:\r\n    21   1830-11-22 00:50:52.580896768\r\n    22   1807-03-30 05:56:08.066277376\r\n    23   1816-03-29 05:56:08.066277376\r\n\r\nI\'m running Pandas 0.12 on Ubuntu 12.04LTS with Python 2.7.3. I could not find this issue registered yet.\r\n\r\nKind regards,\r\n\r\nCarst'"
5862,25108393,JackKelly,jreback,2014-01-06 15:38:36,2014-01-15 11:56:41,2014-01-15 02:08:37,closed,jreback,0.13.1,7,API Design;Bug;Resample,https://api.github.com/repos/pydata/pandas/issues/5862,b'API: implement __finalize__ for resample et al.',"b'I\'m really salivating at the chance to use the `._metadata` and `__finalize__` mechanisms in pandas-0.13-dev to create my own subclass of DataFrame and to have metadata propagate after calling functions inherited from DataFrame like `dropna()`, `resample()` etc.\r\n\r\nUsing the latest 0.13-dev version of Pandas, I think I might have bumped into a small bug (although I\'m not sure if this is a bug or not??):\r\n\r\n`._metadata` propagates after calling `.resample(rule=\'D\')` and `dropna()` but not after calling `.resample(rule=\'D\', how=\'max\')`.\r\n\r\nMore details, including the full code of my subclass, are given under the ""experiments"" heading of this issue: https://github.com/nilmtk/nilmtk/issues/83\r\n'"
5852,25062434,jseabold,jreback,2014-01-04 23:31:59,2014-04-09 03:03:32,2014-04-09 03:03:32,closed,,0.14.0,4,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/5852,b'Frequency alias bug? MS -> millisecond or MonthBegin?',"b""Likely take me a while to work my way through this again completely. Maybe I'm just misremembering this code, but should MS map to millisecond or month begin? (I was looking for a function to easily and generally say that M, MS, M-Jan, M-Dec, etc. are all monthly frequencies.)\r\n\r\n```\r\npd.tseries.frequencies.get_standard_freq('MS')\r\npd.tseries.frequencies.get_freq_group('MS')\r\n```\r\n\r\nvs. things like\r\n\r\n```\r\npd.tseries.frequencies.get_offset('MS')\r\n```\r\n\r\nand all the rest of the code that gives freq and freqstr MS for month start code like.\r\n\r\n```\r\npd.tseries.offsets.MonthBegin().freqstr\r\n```\r\n"""
5846,25055014,MichaelWS,jreback,2014-01-04 15:22:19,2014-01-15 04:31:05,2014-01-15 00:34:38,closed,,0.13.1,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5846,b'tail throws exception on empty frame',"b'currently, tail throws an exception on an empty frame but head does not.\r\n    \r\n    import pandas as pd\r\n    df = pd.DataFrame()\r\n    df.tail()\r\n\r\ntraceback:\r\nhttps://gist.github.com/MichaelWS/8256143\r\n\r\nShould this be fixed by returning the frame if empty on calls to iloc?  \r\n\r\nWe can check for empty in tail on length as well or create a new slice with start being None)'"
5839,25035664,dragoljub,jreback,2014-01-03 21:02:49,2016-05-09 20:37:17,2016-05-09 20:37:17,closed,,Next Major Release,18,Bug;Groupby;MultiIndex;Won't Fix,https://api.github.com/repos/pydata/pandas/issues/5839,b'df.groupby().apply() with only one group returns wrong shape!',"b'I have reached a corner case in the wonderful groupby().apply() method. A groupby with only __one__ group causes the apply method to return the wrong output shape. Instead of a series with a multi-index the result is retunred as a DataFrame with last row index level as columns. :frowning: \r\n\r\n```python\r\n\r\nIn [1]: import numpy as np\r\n   ...: import pandas as pd\r\n   ...: from sklearn.cluster import DBSCAN as DBSCAN\r\n   ...: \r\n   ...: print pd.__version__\r\n   ...: \r\n   ...: # Generate Test DataFrame\r\n   ...: NUM_ROWS = 1000\r\n   ...: NUM_COLS = 10\r\n   ...: col_names = [\'A\'+num for num in map(str,np.arange(NUM_COLS).tolist())]\r\n   ...: index_cols = col_names[:5] \r\n   ...: \r\n   ...: # Set DataFrame to have 5 level Hierarchical Index.\r\n   ...: # Sort the index!\r\n   ...: df = pd.DataFrame(np.random.randint(5, size=(NUM_ROWS,NUM_COLS)), dtype=np.int64, columns=col_names)\r\n   ...: df = df.set_index(index_cols).sort_index()\r\n   ...: df\r\n   ...: \r\n   ...: # Group by first 4 index columns.\r\n   ...: grp = df.groupby(level=index_cols[:4])\r\n   ...: \r\n   ...: # Find index of largest group.\r\n   ...: big_loc = grp.size().idxmax()\r\n   ...: \r\n   ...: # Create function to apply clustering on groups\r\n   ...: def grp_func(df):\r\n   ...:     """"""Run clustering on subgroup and return series of results.""""""\r\n   ...:     db = DBSCAN(eps=1, min_samples=1, metric=\'euclidean\').fit(df.values)\r\n   ...:     return pd.Series(db.labels_, name=\'cluster_id\', index=df.index.get_level_values(4))\r\n   ...: \r\n0.12.0\r\n\r\nIn [2]: # Apply clustering on each subgroup of DataFrame\r\n   ...: out_good = grp.apply(grp_func)\r\n   ...: out_good\r\n   ...: out_good.shape\r\nOut[2]: (1000L,)\r\n\r\nIn [3]: # Select out biggest group wihile keeping index levels and try same apply\r\n   ...: out_bad = df[[big_loc == a[:4] for a in df.index.values]].groupby(level=index_cols[:4]).apply(grp_func)\r\n   ...: out_bad\r\n   ...: out_bad.shape\r\nOut[3]: (1, 7)\r\n\r\nIn [4]: out_good\r\nOut[4]: \r\nA0  A1  A2  A3  A4\r\n0   0   0   0   0     1\r\n                3     0\r\n            1   3     0\r\n            2   1     1\r\n                3     2\r\n                3     0\r\n            3   3     1\r\n                3     0\r\n        1   1   1     0\r\n            2   0     0\r\n                2     1\r\n                4     2\r\n            3   4     0\r\n                4     1\r\n            4   0     0\r\n...\r\n4   4   3   0   2     0\r\n            1   1     1\r\n                3     2\r\n                4     0\r\n            3   1     0\r\n                2     1\r\n            4   4     0\r\n        4   0   4     0\r\n            1   1     3\r\n                1     1\r\n                2     2\r\n                2     0\r\n            3   1     1\r\n                3     0\r\n            4   4     0\r\nName: cluster_id, Length: 1000, dtype: float64\r\n\r\nIn [5]: out_bad\r\n\r\nOut[5]: \r\nA4           1  1  2  3  3  3  4\r\nA0 A1 A2 A3                     \r\n3  0  0  3   6  5  3  0  1  4  2\r\n\r\n# If you stack the bad result it comes out looking OK, but now I need a workaround for this corner case to use apply.\r\nIn [17]: out_bad.stack()\r\n\r\nOut[17]: A0  A1  A2  A3  A4\r\n3   0   0   3   1     3\r\n                1     6\r\n                2     5\r\n                3     1\r\n                3     4\r\n                3     0\r\n                4     2\r\ndtype: float64\r\n```'"
5835,25023174,jreback,jreback,2014-01-03 16:47:19,2014-01-04 18:53:09,2014-01-04 18:53:09,closed,,0.13.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5835,b'BUG: duplicate selection with missing values raises',"b""Hard to reprod.\r\n\r\nSelect from a duplicate indexed axis in a frame with ix where part of the\r\nselection set is missing\r\n\r\n```\r\nIn [22]: df = DataFrame(np.random.randn(5,5),columns=['A.1','B.1','B.2','B.3','A.2'],index=date_range('20130101',periods=5))\r\n\r\nIn [23]: df2 = df.rename(columns=lambda x: x.split('.')[0])\r\n\r\nIn [24]: df2\r\nOut[24]: \r\n                   A         B         B         B         A\r\n2013-01-01 -1.029245 -0.782139  0.584956  1.097301 -0.150675\r\n2013-01-02 -0.723246 -0.356150 -0.441952  0.027012 -1.851583\r\n2013-01-03 -1.001412  0.129464  0.093433  0.952615 -1.338390\r\n2013-01-04  0.165987  0.227918  0.557940 -0.102501 -1.194053\r\n2013-01-05  0.249493 -1.102096 -0.977755 -0.529540  0.783277\r\n\r\n[5 rows x 5 columns]\r\n\r\nIn [25]: df2.ix[:,['A','B','C']]\r\n\r\nAssertionError: Number of manager items must equal union of block items\r\n# manager items: 6, # tot_items: 14\r\n```\r\n               """
5824,24979132,gdraps,jreback,2014-01-02 18:30:31,2014-04-16 13:04:41,2014-04-16 13:04:41,closed,,0.14.0,20,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/5824,b'Numpy 1.8 `DeprecationWarning` in compat/scipy.py',"b'Not sure how pressing this is, but with `DeprecationWarning` enabled, I notice that numpy 1.8 is raising a warning during the following call to `describe()`.  [side note: enabled DeprecationWarning in my test suite after learning that it was changed in py2.7 to ""ignore"" by default.]\r\n\r\n    import pandas as pd\r\n    import warnings\r\n    warnings.simplefilter(""once"", DeprecationWarning)\r\n\r\n    df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [1.2, 4.2, 5.2]})\r\n    print df.groupby(\'A\')[\'B\'].describe()\r\n\r\nstdout:\r\n\r\n    $ python test_fail.py \r\n    .../pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer\r\n    number instead of an integer will result in an error in the future\r\n      score = values[idx]\r\n\r\nHere\'s the full traceback with DeprecationWarning escalated to an error (`warnings.simplefilter(""error"", DeprecationWarning)`):\r\n\r\n    Traceback (most recent call last):\r\n      File ""test_fail.py"", line 6, in <module>\r\n        print df.groupby(\'A\')[\'B\'].describe()\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/groupby.py"", line 343, in wrapper\r\n        return self.apply(curried)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/groupby.py"", line 424, in apply\r\n        return self._python_apply_general(f)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/groupby.py"", line 427, in _python_apply_general\r\n        keys, values, mutated = self.grouper.apply(f, self.obj, self.axis)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/groupby.py"", line 883, in apply\r\n        res = f(group)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/groupby.py"", line 422, in f\r\n        return func(g, *args, **kwargs)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/groupby.py"", line 329, in curried\r\n        return f(x, *args, **kwargs)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/series.py"", line 1386, in describe\r\n        lb), self.median(), self.quantile(ub),\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/core/series.py"", line 1316, in quantile\r\n        result = _quantile(valid_values, q * 100)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0_29_g97860a1-py2.7-linux-i686.egg/pandas/compat/scipy.py"", line 68, in scoreatpercentile\r\n        score = values[idx]\r\n    IndexError: cannot convert index to integer\r\n'"
5818,24953057,acowlikeobject,jreback,2014-01-02 06:38:36,2014-01-24 22:20:51,2014-01-24 22:20:51,closed,,0.13.1,1,Bug;Dtypes;Reshaping,https://api.github.com/repos/pydata/pandas/issues/5818,b'DatetimeIndex columns cause reset_index() to throw AttributeError',"b""It appears that if a dataframe has column headers of type DatetimeIndex, calling reset_index() throws ```AttributeError: 'str' object has no attribute 'view'```.  I see this both in v0.12 and the master branch.\r\n\r\nIf column headers are strings or integers, reset_index() works fine.\r\n\r\nI'm guessing it's treating the new column array as type DatetimeIndex whereas it now has a string in the 0th position ('index').  Maybe the reset_index should first cast the columns as object and convert the DatetimeIndex values to strings?\r\n\r\n```python\r\nIn [48]: df = pd.DataFrame(data=np.random.rand(2, 2), columns=pd.date_range('1/1/2013', '1/2/2013'), index=['A', 'B'])\r\n\r\nIn [49]: df.reset_index()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-49-6983677cc901> in <module>()\r\n----> 1 df.reset_index()\r\n\r\n/home/user/environments/python3/src/pandas/pandas/core/frame.py in reset_index(self, level, drop, inplace, col_level, col_fill)\r\n   2447             else:\r\n   2448                 values = _maybe_cast(self.index.values)\r\n-> 2449             new_obj.insert(0, name, values)\r\n   2450 \r\n   2451         new_obj.index = new_index\r\n\r\n/home/user/environments/python3/src/pandas/pandas/core/frame.py in insert(self, loc, column, value, allow_duplicates)\r\n   1940         value = self._sanitize_column(column, value)\r\n   1941         self._data.insert(\r\n-> 1942             loc, column, value, allow_duplicates=allow_duplicates)\r\n   1943 \r\n   1944     def _sanitize_column(self, key, value):\r\n\r\n/home/user/environments/python3/src/pandas/pandas/core/internals.py in insert(self, loc, item, value, allow_duplicates)\r\n   2899 \r\n   2900         try:\r\n-> 2901             new_items = self.items.insert(loc, item)\r\n   2902             self.set_items_norename(new_items)\r\n   2903 \r\n\r\n/home/user/environments/python3/src/pandas/pandas/tseries/index.py in insert(self, loc, item)\r\n   1539 \r\n   1540         new_index = np.concatenate((self[:loc].asi8,\r\n-> 1541                                     [item.view(np.int64)],\r\n   1542                                     self[loc:].asi8))\r\n   1543         return DatetimeIndex(new_index, freq='infer')\r\n\r\nAttributeError: 'str' object has no attribute 'view'\r\n\r\nIn [50]: df = pd.DataFrame(data=np.random.rand(2, 2), columns=[1, 2], index=['A', 'B'])\r\nIn [51]: df.reset_index()\r\nOut[51]: \r\n  index         1         2\r\n0     A  0.947575  0.370406\r\n1     B  0.664856  0.686524\r\n\r\n[2 rows x 3 columns]\r\n\r\nIn [52]: df = pd.DataFrame(data=np.random.rand(2, 2), columns=['C', 'D'], index=['A', 'B'])\r\n\r\nIn [53]: df.reset_index()\r\nOut[53]: \r\n  index         C         D\r\n0     A  0.053455  0.599483\r\n1     B  0.776364  0.680425\r\n\r\n[2 rows x 3 columns]\r\n```"""
5808,24922800,jreback,jreback,2013-12-31 15:31:03,2014-01-04 22:30:39,2014-01-04 22:30:39,closed,,0.13.1,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/5808,b'BUG: boolean op with empty frames should not raise',"b""\r\n```\r\nIn [4]: DataFrame(index=[1]) & DataFrame(index=[1])\r\nTypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\r\n```\r\nshould just return an empty frame (also raises if one side is non-empty)"""
5797,24896600,MichaelWS,jreback,2013-12-30 19:22:28,2014-01-02 22:59:01,2014-01-02 22:59:01,closed,,0.13.1,9,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/5797,b'Series replace values using timestamps in a dict',"b'I was working with financial data and was replacing dates that were not valid trading days.  It seems that replace does not work on any dict between two timestamps.  Here is an example.  This simply throws an exception on 0.12.\r\n\r\n    import pandas.io.data as web\r\n    ibm = web.DataReader(""IBM"", ""yahoo"", ""1993/01/01"").reset_index()\r\n    next_date_dict = dict(zip(ibm[""Date""], ibm[""Date""].shift()))\r\n    ibm_next_date = ibm[""Date""].replace(next_date_dict)\r\n    print (ibm_next_date == ibm[""Date""]).all()\r\nin 0.12 there is an exception:\r\nhttps://gist.github.com/MichaelWS/8186689'"
5789,24853217,jorisvandenbossche,jreback,2013-12-28 23:44:21,2013-12-29 16:23:22,2013-12-29 16:23:22,closed,,0.13,0,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/5789,b'BUG: hour string automatically converted to datetime in groupby apply',"b'(Encountered with same SO [question](http://stackoverflow.com/questions/20819702/hourly-frequency-count-with-python/20820166#20820166) as #5786 and #5788. With following dataframe:\r\n```\r\nfrom StringIO import StringIO\r\ns=""""""2011.05.16,00:00,1.40893\r\n2011.05.16,01:00,1.40760\r\n2011.05.16,02:00,1.40750\r\n2011.05.16,03:00,1.40649\r\n2011.05.17,02:00,1.40893\r\n2011.05.17,03:00,1.40760\r\n2011.05.17,04:00,1.40750\r\n2011.05.17,05:00,1.40649\r\n2011.05.18,02:00,1.40893\r\n2011.05.18,03:00,1.40760\r\n2011.05.18,04:00,1.40750\r\n2011.05.18,05:00,1.40649""""""\r\ndf = pd.read_csv(StringIO(s), header=None, names=[\'date\', \'time\', \'value\'])\r\n```\r\nwhere all dates or hours are strings. When doing a groupby on the date and then accessing the value in column `time` where `value` is maximal, the hour string is converted to a datetime:\r\n```\r\nIn [21]: df.groupby(\'date\').apply(lambda x: x[\'time\'][x[\'value\'].idxmax()])\r\nOut[21]: \r\ndate\r\n2011.05.16   2013-12-29 00:00:00\r\n2011.05.17   2013-12-29 02:00:00\r\n2011.05.18   2013-12-29 02:00:00\r\ndtype: datetime64[ns]\r\n```\r\nwhile in 0.12 this remains a string:\r\n```\r\nIn [63]: df.groupby(\'date\').apply(lambda x: x[\'time\'][x[\'value\'].idxmax()])\r\nOut[63]: \r\ndate\r\n2011.05.16    00:00\r\n2011.05.17    02:00\r\n2011.05.18    02:00\r\ndtype: object\r\n```\r\nI think in this case this conversion can be highly inappropriate (and you have a chance that just because a string is looking like a date/time component it is converted, while this will not always be your intention).\r\n\r\n'"
5782,24840769,y-p,jreback,2013-12-28 03:55:12,2013-12-29 23:35:34,2013-12-29 23:35:06,closed,,0.13,2,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/5782,b' test_aggregate_item_by_item fails on SPARC py3',"b'```\r\n=============================================================\r\n=========\r\nFAIL: test_aggregate_item_by_item (pandas.tests.test_groupby.TestGroupBy)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/buildslave/nd-bb-slave-sparc-wheezy/pandas-py3_x-wheezy-sparc/build/venv/lib/python3.2/site-packages/pandas-0.13.0rc1_128_g375a66e-py3.2-linux-sparc64.egg/pandas/tests/test_groupby.py"", line 556, in test_aggregate_item_by_item\r\n    assert_almost_equal(result.xs(\'foo\'), [foo] * K)\r\n  File ""testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2554)\r\n  File ""testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas/src/testing.c:1796)\r\n  File ""testing.pyx"", line 113, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2077)\r\nAssertionError: First object is numeric, second is not: 5.0 != 5\r\n```\r\n\r\nhttp://nipy.bic.berkeley.edu/builders/pandas-py3.x-wheezy-sparc\r\nhttp://nipy.bic.berkeley.edu/builders/pandas-py3.x-wheezy-sparc/builds/330/steps/shell_4/logs/stdio\r\n\r\nI\'ve got a patch for a seperate unicode issue awaiting travis, with it and this fixed (cc @jreback), sparc should be green across the board.\r\n\r\n'"
5781,24831581,jreback,jreback,2013-12-27 19:53:51,2014-05-29 11:10:05,2014-05-29 11:10:05,closed,,0.14.0,8,Bug;Data IO;Testing,https://api.github.com/repos/pydata/pandas/issues/5781,b'TST/BUG: stata failures on big endian',b'related #5778\r\n\r\nInvestigate/fix whether the test failures (currently skipped on big-endian) are actual bugs or just an odd test.\r\n\r\ncc @PKEuS '
5779,24827399,y-p,jreback,2013-12-27 17:36:11,2013-12-27 21:06:44,2013-12-27 20:08:13,closed,,0.13,5,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/5779,b'Failing timeseries tests on debian sparc buildbot',"b'```\r\n======================================================================\r\nERROR: test_to_timedelta_on_missing_values (pandas.tseries.tests.test_timedeltas.TestTimedeltas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/tseries/tests/test_timedeltas.py"", line 204, in test_to_timedelta_on_missing_values\r\n    expected = Series([np.timedelta64(1000000000, \'ns\'), timedelta_NaT], dtype=\'<m8[ns]\')\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/core/series.py"", line 218, in __init__\r\n    raise_cast_failure=True)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/core/series.py"", line 2542, in _sanitize_array\r\n    subarr = _try_cast(data, False)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/core/series.py"", line 2503, in _try_cast\r\n    arr = _possibly_cast_to_datetime(arr, dtype)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/core/common.py"", line 1579, in _possibly_cast_to_datetime\r\n    ""cannot convert timedeltalike to dtype [%s]"" % dtype)\r\nTypeError: cannot convert timedeltalike to dtype [<m8[ns]]\r\n\r\n======================================================================\r\nFAIL: test_timedelta_ops_with_missing_values (pandas.tseries.tests.test_timedeltas.TestTimedeltas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/tseries/tests/test_timedeltas.py"", line 281, in test_timedelta_ops_with_missing_values\r\n    assert_frame_equal(actual, dfn)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/util/testing.py"", line 493, in assert_frame_equal\r\n    check_less_precise=check_less_precise)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/util/testing.py"", line 442, in assert_series_equal\r\n    assert_almost_equal(left.values, right.values, check_less_precise)\r\n  File ""testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2561)\r\n  File ""testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas/src/testing.c:1803)\r\n  File ""testing.pyx"", line 107, in pandas._testing.assert_almost_equal (pandas/src/testing.c:2023)\r\nAssertionError: First object is not null, second is null: numpy.timedelta64(9223372036854775807,\'ns\') != numpy.timedelta64(\'NaT\',\'ns\')\r\n```\r\n\r\nhttp://nipy.bic.berkeley.edu/builders/pandas-py3.x-sid-sparc/builds/491/steps/shell_4/logs/stdio'"
5778,24827358,y-p,jreback,2013-12-27 17:34:50,2013-12-27 21:14:53,2013-12-27 20:08:13,closed,,0.13,4,Bug;Data IO;Testing,https://api.github.com/repos/pydata/pandas/issues/5778,b'Failing stata tests on debian SPARC buildbot',"b'```\r\n======================================================================\r\nERROR: test_read_write_dta11 (pandas.io.tests.test_stata.TestStata)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/tests/test_stata.py"", line 247, in test_read_write_dta11\r\n    written_and_read_again = self.read_dta(path)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/tests/test_stata.py"", line 41, in read_dta\r\n    return read_stata(file, convert_dates=True)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 47, in read_stata\r\n    reader = StataReader(filepath_or_buffer, encoding)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 343, in __init__\r\n    self._read_header()\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 468, in _read_header\r\n    for i in range(self.nvar)]\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 468, in <listcomp>\r\n    for i in range(self.nvar)]\r\nTypeError: ord() expected a character, but string of length 0 found\r\n\r\n======================================================================\r\nERROR: test_read_write_dta12 (pandas.io.tests.test_stata.TestStata)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/tests/test_stata.py"", line 263, in test_read_write_dta12\r\n    written_and_read_again = self.read_dta(path)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/tests/test_stata.py"", line 41, in read_dta\r\n    return read_stata(file, convert_dates=True)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 47, in read_stata\r\n    reader = StataReader(filepath_or_buffer, encoding)\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 343, in __init__\r\n    self._read_header()\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 468, in _read_header\r\n    for i in range(self.nvar)]\r\n  File ""/home/buildslave/nd-bb-slave-sparc-sid/pandas-py3_x-sid-sparc/build/venv/lib/python3.3/site-packages/pandas-0.13.0rc1_124_g8cd34c6-py3.3-linux-sparc64.egg/pandas/io/stata.py"", line 468, in <listcomp>\r\n    for i in range(self.nvar)]\r\nTypeError: ord() expected a character, but string of length 0 found\r\n```\r\n\r\ncc @PKEuS \r\nhttp://nipy.bic.berkeley.edu/builders/pandas-py3.x-sid-sparc/builds/491/steps/shell_4/logs/stdio\r\n'"
5771,24767212,yarivm,jreback,2013-12-25 11:41:55,2013-12-25 13:08:03,2013-12-25 13:07:52,closed,,0.13,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5771,b'Cannot increment after loc indexing',"b'```python\r\nimport pandas as pd\r\na = pd.Series(index=[4,5,6], data=0)\r\nprint a.loc[4:5]\r\na.loc[4:5] += 1\r\n```\r\n\r\nYields:\r\n```\r\n4    0\r\n5    0\r\nTraceback (most recent call last):\r\n  File ""temp1.py"", line 9, in <module>\r\ndtype: int64\r\n    a.loc[4:5] += 1\r\n  File ""lib\\site-packages\\pandas\\core\\indexing.py"", line 88, in __setitem__\r\n    self._setitem_with_indexer(indexer, value)\r\n  File ""lib\\site-packages\\pandas\\core\\indexing.py"", line 177, in _setitem_with_indexer\r\n    value = self._align_series(indexer, value)\r\n  File ""lib\\site-packages\\pandas\\core\\indexing.py"", line 206, in _align_series\r\n    raise ValueError(\'Incompatible indexer with Series\')\r\nValueError: Incompatible indexer with Series\r\n```'"
5766,24723889,wrenoud,y-p,2013-12-23 20:57:58,2013-12-25 02:05:13,2013-12-25 02:05:13,closed,,0.13,4,Bug;Regression,https://api.github.com/repos/pydata/pandas/issues/5766,b'BUG: IndexError on read_csv/read_table when using usecols/names parameters and omitting last column',"b'Example code:\r\n```python\r\nfrom StringIO import StringIO\r\nimport pandas as pd\r\n\r\nnames = [""a"",""b"",""c""]\r\n\r\ndata = """"""\\\r\n0,1,2\r\n3,4,5\r\n6,7,8""""""\r\n\r\n# usecols works as expected if all columns are named\r\nprint pd.read_csv(StringIO(data), header=None, usecols=[1,2], names=names)\r\nprint pd.read_csv(StringIO(data), header=None, usecols=[0,1], names=names)\r\n\r\n# naming only columns selected with usecols works when last column is included\r\nprint pd.read_csv(StringIO(data), header=None, usecols=[1,2], names=names[1:])\r\n# causes IndexError\r\nprint pd.read_csv(StringIO(data), header=None, usecols=[0,1], names=names[:-1])\r\n```\r\nOutput:\r\n```\r\n   b  c\r\n0  1  2\r\n1  4  5\r\n2  7  8\r\n\r\n[3 rows x 2 columns]\r\n   a  b\r\n0  0  1\r\n1  3  4\r\n2  6  7\r\n\r\n[3 rows x 2 columns]\r\n   b  c\r\n0  1  2\r\n1  4  5\r\n2  7  8\r\n\r\n[3 rows x 2 columns]\r\nTraceback (most recent call last):\r\n  File ""pandas_test2.py"", line 18, in <module>\r\n    print pd.read_csv(StringIO(data), header=None, usecols=[0,1], names=names[:-1])\r\n  File ""/home/weston/pandas/pandas/io/parsers.py"", line 404, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File ""/home/weston/pandas/pandas/io/parsers.py"", line 212, in _read\r\n    return parser.read()\r\n  File ""/home/weston/pandas/pandas/io/parsers.py"", line 610, in read\r\n    ret = self._engine.read(nrows)\r\n  File ""/home/weston/pandas/pandas/io/parsers.py"", line 1050, in read\r\n    data = self._reader.read(nrows)\r\n  File ""parser.pyx"", line 727, in pandas.parser.TextReader.read (pandas/parser.c:6475)\r\n  File ""parser.pyx"", line 749, in pandas.parser.TextReader._read_low_memory (pandas/parser.c:6695)\r\n  File ""parser.pyx"", line 824, in pandas.parser.TextReader._read_rows (pandas/parser.c:7517)\r\n  File ""parser.pyx"", line 902, in pandas.parser.TextReader._convert_column_data (pandas/parser.c:8296)\r\n  File ""parser.pyx"", line 1139, in pandas.parser.TextReader._get_column_name (pandas/parser.c:11353)\r\nIndexError: list index out of range\r\n```\r\n`print_versions.py` output:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 2.7.3.final.0\r\nOS: Linux 3.2.0-51-generic #77-Ubuntu SMP Wed Jul 24 20:21:10 UTC 2013 i686\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\n\r\npandas: 0.13.0rc1-119-g2485e09\r\nCython: 0.15.1\r\nNumpy: 1.6.1\r\nScipy: 0.9.0\r\nstatsmodels: Not installed\r\n    patsy: Not installed\r\nscikits.timeseries: Not installed\r\ndateutil: 1.5\r\npytz: 2011k\r\nbottleneck: Not installed\r\nPyTables: Not Installed\r\n    numexpr: Not Installed\r\nmatplotlib: 1.1.1rc\r\nopenpyxl: Not installed\r\nxlrd: Not installed\r\nxlwt: Not installed\r\nxlsxwriter: Not installed\r\nsqlalchemy: Not installed\r\nlxml: Not installed\r\nbs4: Not installed\r\nhtml5lib: Not installed\r\nbigquery: Not installed\r\napiclient: Not installed\r\n```'"
5756,24619536,immerrr,jreback,2013-12-20 12:33:30,2013-12-20 14:08:05,2013-12-20 14:08:05,closed,,0.13,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/5756,b'BUG: creating dataframe from empty series raises exception',"b""Here's a snippet to show what I'm talking about:\r\n```python\r\n\r\nIn [1]: pd.__version__\r\nOut[1]: '0.13.0rc1-95-gfec2ff6'\r\n\r\nIn [2]: pd.DataFrame(pd.Series())\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-31c6b8fd30b1> in <module>()\r\n----> 1 pd.DataFrame(pd.Series())\r\n\r\n/home/immerrr/sources/pandas/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)\r\n    230             else:\r\n    231                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n--> 232                                          copy=copy)\r\n    233         elif isinstance(data, (list, types.GeneratorType)):\r\n    234             if isinstance(data, types.GeneratorType):\r\n\r\n/home/immerrr/sources/pandas/pandas/core/frame.py in _init_ndarray(self, values, index, columns, dtype, copy)\r\n    334 \r\n    335             # zero len case (GH #2234)\r\n--> 336             if not len(values) and len(columns):\r\n    337                 values = np.empty((0, 1), dtype=object)\r\n    338 \r\n\r\nTypeError: object of type 'NoneType' has no len()\r\n\r\n```\r\n\r\nApparently, it's a regression from 0.12.0, because \r\n\r\n```python\r\nIn [1]: pd.__version__\r\nOut[1]: '0.12.0'\r\n\r\nIn [2]: pd.DataFrame(pd.Series())\r\nOut[2]: \r\nEmpty DataFrame\r\nColumns: [0]\r\nIndex: []\r\n\r\n```\r\n\r\nI'd guess that one of failing checks previously read `if smth ...`, but since implicit conversion to boolean for ndarrays are now forbidden, the check was replaced by `len(smth)`, but I didn't dig into this."""
5754,24605139,MichaelWS,jreback,2013-12-20 04:50:51,2013-12-20 16:09:03,2013-12-20 14:08:05,closed,,0.13,6,Bug;Dtypes;Reshaping,https://api.github.com/repos/pydata/pandas/issues/5754,b'pandas concat working in 0.12 but not 0.13',"b""I have a long list of dataframes that concat fine in 0.12 but not in 0.13.  Has anyone seen this error?\r\nI am concatenating using the following\r\n\r\n    pd.concat(all_gics, ignore_index=True) \r\n\r\n\r\nMy traceback:\r\n\r\n\r\n/usr/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity)\r\n    925                        keys=keys, levels=levels, names=names,\r\n    926                        verify_integrity=verify_integrity)\r\n--> 927     return op.get_result()\r\n    928 \r\n    929 \r\n\r\n/usr/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in get_result(self)\r\n   1016             return tmpdf\r\n   1017         else:\r\n-> 1018             new_data = self._get_concatenated_data()\r\n   1019             new_data = self._post_merge(new_data)\r\n   1020             return self.objs[0]._from_axes(new_data, self.new_axes)\r\n\r\n/usr/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_concatenated_data(self)\r\n   1075             new_data = {}\r\n   1076             for item in self.new_axes[0]:\r\n-> 1077                 new_data[item] = self._concat_single_item(rdata, item)\r\n   1078 \r\n   1079         return new_data\r\n\r\n/usr/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _concat_single_item(self, objs, item)\r\n   1184                     elif isnull(v).all():\r\n   1185 \r\n-> 1186                         alls.remove('other')\r\n   1187                         alls.remove('object')\r\n   1188 \r\n\r\nKeyError: 'other'\r\n"""
5749,24585599,yieldsfalsehood,jreback,2013-12-19 20:30:14,2014-01-16 14:27:39,2014-01-16 14:27:39,closed,,0.13.1,2,Algos;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/5749,b'Rolling skewness and kurtosis fail on a sample of all equal values',"b""For a sample of data like this:\r\n\r\n    d = pd.Series([1] * 25)\r\n\r\nBoth of these throw an exception (during an attempt to divide by zero):\r\n\r\n    pd.rolling_skew(d, window=25)\r\n    pd.rolling_kurt(d, window=25)\r\n\r\nThe issue is in algos.pyx. There are no checks for what amounts to zero variance in the data. If one value occurs more times in a row than than the size of the window, the entire rolling computation fails, rather than just returning NaN for that one period (which is what I'd expect). For reference, scipy gives a kurtosis of -3 and a skewness of 0 (plus a warning) for this situation, which is not what I'd expect (since the higher moments are all zero, implying a division by zero).\r\n\r\n    >>> from scipy import stats\r\n    >>> stats.kurtosis([1,1,1,1,1,1,1])\r\n    -3.0\r\n    >>> stats.skew([1,1,1,1,1,1,1])\r\n    /usr/lib/python2.7/dist-packages/scipy/stats/stats.py:1067: RuntimeWarning: invalid value encountered in double_scalars\r\n      vals = np.where(zero, 0, m3 / m2**1.5)\r\n    0.0\r\n\r\nBelow is the approach I was taking to weed out any possible divide by zero issues. I'll submit a proper pull request tomorrow, in the meantime this is here in case I can get any feedback, preferably on whether these added conditions are enough (I think the kurtosis could still break) and how to add some tests for both of these.\r\n\r\n    diff --git a/pandas/algos.pyx b/pandas/algos.pyx\r\n    index 08ec707..78b619f 100644\r\n    --- a/pandas/algos.pyx\r\n    +++ b/pandas/algos.pyx\r\n    @@ -1160,7 +1160,7 @@ def roll_skew(ndarray[double_t] input, int win, int minp):\r\n\r\n                     nobs -= 1\r\n\r\n    -        if nobs >= minp:\r\n    +        if nobs >= minp and not (x == 0 and xx == 0) and nobs != 2:\r\n                 A = x / nobs\r\n                 B = xx / nobs - A * A\r\n                 C = xxx / nobs - A * A * A - 3 * A * B\r\n    @@ -1227,7 +1227,7 @@ def roll_kurt(ndarray[double_t] input,\r\n\r\n                     nobs -= 1\r\n\r\n    -        if nobs >= minp:\r\n    +        if nobs >= minp and not (x == 0 and xx == 0) and nobs != 2:\r\n                 A = x / nobs\r\n                 R = A * A\r\n                 B = xx / nobs - R"""
5744,24558825,socheon,jreback,2013-12-19 14:22:56,2013-12-19 17:32:02,2013-12-19 15:20:21,closed,,0.13,2,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5744,b'BUG: Column indexing does not work in this case',"b'I am using Pandas 0.13.0rc1-23-g286811a, Python 2.7 and Windows XP.\r\n\r\nThe space in column names is apparently causing a problem. On IPython,\r\n```\r\ndf = pd.DataFrame(columns=[\'a\', \'b\', \'c c\'])\r\ndf[\'d\'] = 3\r\ndf[\'c c\']\r\n```\r\n\r\nError\r\n```\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in __getitem__(self, key)\r\n   1626             return self._getitem_multilevel(key)\r\n   1627         else:\r\n-> 1628             return self._getitem_column(key)\r\n   1629\r\n   1630     def _getitem_column(self, key):\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in _getitem_column(self, key)\r\n   1633         # get column\r\n   1634         if self.columns.is_unique:\r\n-> 1635             return self._get_item_cache(key)\r\n   1636\r\n   1637         # duplicate columns & possible reduce dimensionaility\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\generic.pyc in _get_item_cache(self, item)\r\n    977         if res is None:\r\n    978             values = self._data.get(item)\r\n--> 979             res = self._box_item_values(item, values)\r\n    980             cache[item] = res\r\n    981             res._cacher = (item, weakref.ref(self))\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in _box_item_values(self, key, values)\r\n   1834             return self._constructor(values.T, columns=items, index=self.index)\r\n   1835         else:\r\n-> 1836             return self._box_col_values(values, items)\r\n   1837\r\n   1838     def _box_col_values(self, values, items):\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in _box_col_values(self, values, items)\r\n   1839         """""" provide boxed values for a column """"""\r\n   1840         return self._constructor_sliced.from_array(values, index=self.index,\r\n-> 1841                                                    name=items, fastpath=True)\r\n   1842\r\n   1843     def __setitem__(self, key, value):\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\series.pyc in from_array(cls, arr, index, name, copy, fastpath)\r\n    233             cls = SparseSeries\r\n    234\r\n--> 235         return cls(arr, index=index, name=name, copy=copy, fastpath=fastpath)\r\n    236\r\n    237     @property\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\series.pyc in __init__(self, data, index, dtype, name, copy, fastpath\r\n)\r\n    130             # data is an ndarray, index is defined\r\n    131             if not isinstance(data, SingleBlockManager):\r\n--> 132                 data = SingleBlockManager(data, index, fastpath=True)\r\n    133             if copy:\r\n    134                 data = data.copy()\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, block, axis, do_integrity_check, fast\r\npath)\r\n   3413                 block = block[0]\r\n   3414             if not isinstance(block, Block):\r\n-> 3415                 block = make_block(block, axis, axis, ndim=1, fastpath=True)\r\n   3416\r\n   3417         else:\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in make_block(values, items, ref_items, klass, ndim, dt\r\nype, fastpath, placement)\r\n   1893\r\n   1894     return klass(values, items, ref_items, ndim=ndim, fastpath=fastpath,\r\n-> 1895                  placement=placement)\r\n   1896\r\n   1897\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastp\r\nath, placement)\r\n   1297         super(ObjectBlock, self).__init__(values, items, ref_items, ndim=ndim,\r\n   1298                                           fastpath=fastpath,\r\n-> 1299                                           placement=placement)\r\n   1300\r\n   1301     @property\r\n\r\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastp\r\nath, placement)\r\n     63         if len(items) != len(values):\r\n     64             raise ValueError(\'Wrong number of items passed %d, indices imply \'\r\n---> 65                              \'%d\' % (len(items), len(values)))\r\n     66\r\n     67         self.set_ref_locs(placement)\r\n\r\nValueError: Wrong number of items passed 1, indices imply 0\r\n```'"
5732,24496571,nmichaud,y-p,2013-12-18 15:28:47,2014-01-03 01:09:56,2014-01-03 01:09:56,closed,,0.13.1,0,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/5732,b'read_csv/read_table have a prefix argument which is not respected',"b""According to the docs, `prefix` can either be a `str` or None. If it is a string then it is used as the prefix of the column name. However, no matter what string is passed, the prefix used is 'X'."""
5727,24478638,Marigold,jreback,2013-12-18 09:37:32,2014-08-28 00:30:53,2013-12-18 20:30:27,closed,,0.13,8,API Design;Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/5727,b'IndexError: index out of bounds for large Series with MultiIndex',"b'I am on `0.13.0rc1-64-gceec8bf` (master). I get the `IndexError` when trying to print large dataframe with MultiIndex full of NaNs. Following script raises the error\r\n``` python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nn = 3500000\r\narrays = [range(n), np.empty(n)]\r\nindex = pd.MultiIndex.from_tuples(zip(*arrays))\r\ns = pd.Series(np.zeros(n), index=index)\r\n\r\nprint(s)\r\n```\r\nAnd here is a Traceback\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home/mvinkler/Documents/analysis/models/test.py"", line 12, in <module>\r\n    print(s)\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/base.py"", line 35, in __str__\r\n    return self.__bytes__()\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/base.py"", line 47, in __bytes__\r\n    return self.__unicode__().encode(encoding, \'replace\')\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/series.py"", line 857, in __unicode__\r\n    result = self._tidy_repr(min(30, max_rows - 4))\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/series.py"", line 876, in _tidy_repr\r\n    head = self.iloc[:num]._get_repr(print_header=True, length=False,\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/generic.py"", line 946, in _indexer\r\n    setattr(self, iname, indexer(self, name))\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/generic.py"", line 1607, in __setattr__\r\n    elif name in self._info_axis:\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/index.py"", line 2498, in __contains__\r\n    self.get_loc(key)\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/index.py"", line 2986, in get_loc\r\n    return self._get_level_indexer(key, level=0)\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/index.py"", line 3123, in _get_level_indexer\r\n    loc = level_index.get_loc(key)\r\n  File ""/home/mvinkler/Downloads/pandas/pandas/core/index.py"", line 1017, in get_loc\r\n    return self._engine.get_loc(_values_from_object(key))\r\n  File ""index.pyx"", line 129, in pandas.index.IndexEngine.get_loc (pandas/index.c:3560)\r\n  File ""index.pyx"", line 138, in pandas.index.IndexEngine.get_loc (pandas/index.c:3314)\r\n  File ""util.pxd"", line 40, in util.get_value_at (pandas/index.c:13101)\r\nIndexError: index out of bounds\r\n```'"
5725,24466997,behzadnouri,jreback,2013-12-18 03:10:21,2013-12-19 12:33:09,2013-12-18 20:30:27,closed,,0.13,7,2/3 Compat;Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/5725,b'inconsistent indexing behaviour with multi index',"b""i have a dataframe with multi-index as \r\n\r\n    >>> df\r\n                  val  diffs\r\n    tag day                 \r\n    A   26   0.208105    NaN\r\n        37   2.709899    NaN\r\n        57   0.933981    NaN\r\n    B   75   0.624029    NaN\r\n    C   0   -1.645489    NaN\r\n        27  -1.749618    NaN\r\n        27  -0.725497    NaN\r\n        37  -0.381485    NaN\r\n        67   0.110426    NaN\r\n        82   1.253857    NaN\r\n    \r\n    [10 rows x 2 columns]\r\n\r\nhowever\r\n\r\n    >>> df.val[ 'A' ]\r\n    A   NaN\r\n    Name: val, dtype: float64\r\n    >>> df.val[ 'X' ]\r\n    X   NaN\r\n    Name: val, dtype: float64\r\n\r\n\r\n\r\n    >>> pd.__version__\r\n    '0.13.0rc1-92-gf6fd509'"""
5720,24402760,gdraps,jreback,2013-12-17 06:42:27,2013-12-19 17:32:02,2013-12-17 21:32:24,closed,,0.13,4,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5720,b'Empty dataframe corrupted by column add (0.13rc1)',"b'Noticed the following difference in behavior between 0.12 and 0.13rc1 when adding a column to an empty dataframe.  Obviously, a weird case, and can be worked around easily.\r\n\r\n    df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [1.2, 4.2, 5.2]})\r\n    y = df[df.A > 5]\r\n    y[\'New\'] = np.nan\r\n    print y\r\n    print y.values\r\n\r\n&nbsp;\r\n\r\n    (pandas-0.12)$ python test.py \r\n    Empty DataFrame\r\n    Columns: [A, B, New]\r\n    Index: []\r\n    []\r\n\r\n&nbsp;\r\n\r\n    (pandas-master)$ python test.py \r\n         A    B  New\r\n    0  NaN  NaN  NaN\r\n\r\n    [1 rows x 3 columns]\r\n    Traceback (most recent call last):\r\n      File ""do_fail.py"", line 8, in <module>\r\n        print y.values\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1705, in values\r\n        return self.as_matrix()\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1697, in as_matrix\r\n        self._consolidate_inplace()\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1622, in _consolidate_inplace\r\n        self._data = self._protect_consolidate(f)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1660, in _protect_consolidate\r\n        result = f()\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1621, in <lambda>\r\n        f = lambda: self._data.consolidate()\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2727, in consolidate\r\n        bm = self.__class__(self.blocks, self.axes)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 1945, in __init__\r\n        self._verify_integrity()\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2227, in _verify_integrity\r\n        tot_items, block.values.shape[1:], self.axes)\r\n      File ""/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 3561, in construction_error\r\n        tuple(map(int, [len(ax) for ax in axes]))))\r\n    ValueError: Shape of passed values is (3, 0), indices imply (3, 1)\r\n'"
5717,24391999,MichaelWS,jreback,2013-12-17 00:36:14,2013-12-19 16:14:16,2013-12-17 02:35:54,closed,,0.13,23,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5717,b'Problem selecting major_axis of a panel in HDFstore from a list if data_columns are used.',"b'Is there any way to use Term to select membership  in a large list for a Panel in HDFstore?\r\n\r\n I have a hdfstore of the following format: https://gist.github.com/MichaelWS/7997225\r\n\r\nMy major axis is ""id"" and minor axis is ""date"".  I am trying to give a large python list into Term and select if an id is in it.   This works for a small list and does not work for a larger one.  This fails for a list of 338 items.  \r\n\r\n    store = pd.HDFStore(store_path, ""r"")\r\n    terms = []\r\n    if not members is None:\r\n        terms.append(Term(""major_axis"", ""="", members))\r\n    if not additional_terms is None:\r\n        terms += additional_terms\r\n    if len(terms) > 0:\r\n        panel = store.select(country, where=terms)\r\n    else:\r\n        panel = store.select(country)\r\n    store.close()\r\n\r\nHere is the traceback:\r\n\r\n\r\n     File ""/home/michael/repos/franklin/factset/reference_data.py"", line 1037, in get_frame\r\n      panel = store.select(country, where=terms)\r\n      File ""/local/install/pandas_test/lib/python2.7/site- packages/pandas/io/pytables.py"", line 637, in select\r\n      auto_close=auto_close).get_values()\r\n      File ""/local/install/pandas_test/lib/python2.7/site-packages/pandas/io/pytables.py"", line 1311, in get_values\r\n    results = self.func(self.start, self.stop)\r\n      File ""/local/install/pandas_test/lib/python2.7/site-packages/pandas/io/pytables.py"", line 626, in func\r\n    columns=columns, **kwargs)\r\n      File ""/local/install/pandas_test/lib/python2.7/site-packages/pandas/io/pytables.py"", line 3453, in read\r\n    sorted_values, items, tuple(N), take_labels)\r\n      File ""/local/install/pandas_test/lib/python2.7/site-packages/pandas/core/reshape.py"", line 1046, in block2d_to_blocknd\r\n    pvalues[i].flat[mask] = values[:, i]\r\n     IndexError: too many indices\r\n'"
5712,24368565,MichaelWS,jreback,2013-12-16 18:26:45,2013-12-16 20:21:33,2013-12-16 19:53:58,closed,,0.13,5,Bug;Groupby;Regression,https://api.github.com/repos/pydata/pandas/issues/5712,b'groupby.transform failing in 0.13',"b'I have used groupby.transform to compute  a number of statistics on data that is grouped by date.  It seems to fail on indexing\r\n\r\nAll of my tests are failing for 0.13 but work in 0.12.  \r\n\r\nHere is one example:\r\nthe following three lines work fine in 0.12 and fails in 0.13.\r\n    print cik_df.head()\r\n    cik_df[""portfolio_rank""] = cik_df.groupby(""ex_date"")[""value""].transform(lambda x: x.rank(ascending=False))\r\n    print cik_df.head()\r\n\r\n\r\nHere is the output: https://gist.github.com/MichaelWS/7991767\r\n\r\n\r\n(I sent this to the pydata group but was asked to post it here by a more active contributor)'"
5703,24322064,miketkelly,jreback,2013-12-15 23:16:17,2013-12-16 16:23:30,2013-12-16 16:23:30,closed,,0.13,8,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/5703,"b""Series.fillna doesn't work with series input""","b""Test code:\r\n\r\n```\r\nimport pandas as pd\r\n\r\nprint pd.__version__\r\n\r\ns1 = pd.Series([np.nan])\r\ns2 = pd.Series([1])\r\n\r\ns1.fillna(s2)\r\n```\r\n\r\nResults:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-28706fea97bb> in <module>()\r\n      6 s2 = pd.Series([1])\r\n      7 \r\n----> 8 s1.fillna(s2)\r\n\r\n/usr/local/anaconda/lib/python2.7/site-packages/pandas-0.13.0rc1_64_gceec8bf-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in fillna(self, value, method, axis, inplace, limit, downcast)\r\n   1917                         continue\r\n   1918                     obj = result[k]\r\n-> 1919                     obj.fillna(v, inplace=True)\r\n   1920                 return result\r\n   1921             else:\r\n\r\nAttributeError: 'numpy.float64' object has no attribute 'fillna'\r\n\r\n0.13.0rc1-64-gceec8bf\r\n```"""
5699,24299463,cpcloud,cpcloud,2013-12-14 20:58:43,2013-12-14 23:15:34,2013-12-14 22:57:24,closed,,0.13.1,15,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/5699,b'Odd StringMixin behavior',"b""A unicode-indexed frame:\r\n\r\n```\r\nIn [2]: df = mkdf(3, 2, r_idx_type='u')\r\n\r\nIn [3]: df\r\nOut[3]:\r\nC0         C_l0_g0 C_l0_g1\r\nR0\r\n74    R0C0    R0C1\r\n441    R1C0    R1C1\r\n355132    R2C0    R2C1\r\n\r\n[3 rows x 2 columns]\r\n\r\nIn [4]: <Ctrl-D>\r\n\r\nsqlite3.ProgrammingError: You must not use 8-bit bytestrings unless you use a text_factory that can interpret 8-bit bytestrings (like text_factory = str). It is highly recommended that you instead just switch your application to Unicode strings.\r\n```\r\n\r\nIf I don't `repr` it, things exit IPython smoothly.\r\n\r\nA trivial subclass of `StringMixin` is fine\r\n\r\n```\r\nIn [1]: class Blob(pandas.core.base.StringMixin):\r\n   ...:     def __unicode__(self):\r\n   ...:         return type(self).__name__\r\n   ...:\r\n\r\nIn [2]: Blob()\r\nOut[2]: Blob\r\n\r\nIn [3]: <Ctrl-D> # <- Fine\r\n```\r\n\r\nHowever, when I add unicode characters to the repr, mega-borkage follows:\r\n\r\n```\r\nIn [1]: class Blob(pandas.core.base.StringMixin):\r\n   ...:     def __unicode__(self):\r\n   ...:         return pandas.compat.u('\\u22ee')\r\n   ...:\r\n\r\nIn [2]: Blob()\r\nOut[2]: \r\n\r\nIn [3]: <Ctrl-D> # same traceback as above\r\n```\r\n\r\nLooks like could be IPython too. I'll see if I can't track it down."""
5695,24256820,cancan101,jreback,2013-12-13 16:31:35,2014-01-20 23:37:17,2014-01-20 23:37:17,closed,,0.13.1,0,Bug;Reshaping;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5695,b'Left Join with timedelta64 does not produce correct nulls',"b'related example: http://stackoverflow.com/questions/20789976/python-pandas-dataframe-1st-line-issue-with-datetime-timedelta/20802902?noredirect=1#comment31195305_20802902\r\n\r\n```\r\nimport datetime\r\nimport pandas as pd\r\nparms = {\'d\':  datetime.datetime(2013, 11, 5, 5, 56), \'t\':datetime.timedelta(0, 22500)}\r\ndf = pd.DataFrame(columns=list(\'dt\'))\r\ndf = df.append(parms, ignore_index=True)\r\nerroneous code:\r\n>>> df.append(parms, ignore_index=True)\r\n                    d               t\r\n0 2013-11-05 05:56:00  22500000000000\r\n1 2013-11-05 05:56:00         6:15:00\r\n```\r\n\r\n\r\nThe notion of nullness is not handled well for ```timedelta64``` columns when performing a left join:\r\n```\r\nIn [194]:\r\npd.DataFrame(pd.Series([np.timedelta64(300000000),np.timedelta64(300000000)],dtype=\'m8[ns]\',index=[""A"",""B""])).join(\r\n     pd.DataFrame(pd.Series([np.timedelta64(300000000)],dtype=\'m8[ns]\',index=[""A""])),rsuffix=\'r\', how=""left"").info()\r\n\r\nOut [194]:\r\n<class \'pandas.core.frame.DataFrame\'>\r\nIndex: 2 entries, A to B\r\nData columns (total 2 columns):\r\n0     2  non-null values\r\n0r    1  non-null values\r\ndtypes: float64(1), timedelta64[ns](1)\r\n```\r\n\r\nThe column with a mix of timedelta64 and ``nulls`` gets cast to a ```float64```.\r\n\r\nThis seems incorrect since ```NaT``` should be usable to indicate the null:\r\n```\r\nIn [196]:\r\npd.Series([np.timedelta64(300000000), pd.NaT],dtype=\'m8[ns]\')\r\n\r\nOut[196]:\r\n0   00:00:00.300000\r\n1               NaT\r\ndtype: timedelta64[ns]\r\n```'"
5694,24256738,dalbani,jreback,2013-12-13 16:30:20,2014-11-06 12:42:10,2014-11-06 12:42:10,closed,,0.15.2,8,API Design;Bug;Duplicate;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/5694,b'Incorrect resampling due to DST',"b'related #5172\r\n\r\nGiven this DataFrame, with an index containing the moment when DST changes (October 27th in the case of the ""Europe/Paris"" timezone):\r\n\r\n```\r\nindex = pandas.date_range(\'2013-09-30\', \'2013-11-02\', freq = \'30Min\', tz = \'UTC\').tz_convert(\'Europe/Paris\')\r\ncolumn_a = pandas.np.random.random(index.size)\r\ncolumn_b = pandas.np.random.random(index.size)\r\ndf = pandas.DataFrame({ ""a"": column_a, ""b"": column_b }, index = index)\r\n```\r\n\r\nLet\'s say I want to find the ""min"" and ""max"" values for each month:\r\n\r\n```\r\ndf.resample(""MS"", how = { ""a"": ""min"", ""b"": ""max"" })\r\n```\r\n\r\nHere\'s the incorrect result:\r\n```\r\n                                  a         b\r\n2013-09-01 00:00:00+02:00  0.015856  0.979541\r\n2013-10-01 00:00:00+02:00  0.002039  0.999960\r\n2013-10-31 23:00:00+01:00       NaN       NaN\r\n```\r\n\r\nSame problem with a ""W-MON"" frequency:\r\n\r\n```\r\n                                  a         b\r\n2013-09-30 00:00:00+02:00  0.015856  0.979541\r\n2013-10-07 00:00:00+02:00  0.007961  0.999734\r\n2013-10-14 00:00:00+02:00  0.002614  0.993354\r\n2013-10-21 00:00:00+02:00  0.005655  0.999960\r\n2013-10-27 23:00:00+01:00       NaN       NaN\r\n2013-11-03 23:00:00+01:00       NaN       NaN\r\n```\r\n\r\nWhereas it works fine with a ""D"" frequency.\r\n\r\n```\r\n                                  a         b\r\n...\r\n2013-10-26 00:00:00+02:00  0.004645  0.983281\r\n2013-10-27 00:00:00+02:00  0.030151  0.986827\r\n2013-10-28 00:00:00+01:00  0.015891  0.981455\r\n2013-10-29 00:00:00+01:00  0.024176  0.999306\r\n...\r\n```\r\n\r\nShould I resample only the ""a"" column, it also works fine:\r\n\r\n```\r\ndf[""a""].resample(""MS"", how = ""min"")\r\n```\r\n```\r\n2013-09-01 00:00:00+02:00    0.015856\r\n2013-10-01 00:00:00+02:00    0.002039\r\n2013-11-01 00:00:00+01:00    0.000747\r\nFreq: MS, dtype: float64\r\n```\r\n\r\nTested with latest pandas from GIT master.'"
5693,24256737,gtakacs,jreback,2013-12-13 16:30:20,2013-12-13 16:33:33,2013-12-13 16:33:19,closed,,,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5693,b'incorrect behavior of string-indexed Series',"b'The following code gives ""IndexError: Out of bounds on buffer access (axis 0)""\r\n(pandas version: 0.12.0, Python version: 3.3.2, OS: Linux):\r\n\r\npandas.Series([0], [""y""])[[""x"", ""x"", ""y""]]\r\n\r\nIf ""x"" is replaced to 1 and ""y"" to 2, then the error disappears.\r\n'"
5690,24217963,cancan101,jreback,2013-12-12 23:50:07,2013-12-13 15:20:00,2013-12-13 15:20:00,closed,,0.13.1,4,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5690,b'min and max of timedelta64',"b'related #5689\r\n\r\nOn pandas v12:\r\n\r\nI thought this was fixed in #2990, but:\r\n\r\nboth of these:\r\n```\r\npd.DataFrame(pd.Series((np.array([np.timedelta64(3000000000),np.timedelta64(3000000000)])))).min()\r\n```\r\nand\r\n```\r\npd.Series((np.array([np.timedelta64(3000000000),np.timedelta64(3000000000)]))).min()\r\n```\r\n\r\nproduce:\r\n```\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.py in min(self, axis, out, skipna, level)\r\n   1509         if level is not None:\r\n   1510             return self._agg_by_level(\'min\', level=level, skipna=skipna)\r\n-> 1511         return nanops.nanmin(self.values, skipna=skipna)\r\n   1512 \r\n   1513     @Substitution(name=\'maximum\', shortname=\'max\',\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/nanops.py in f(values, axis, skipna, **kwds)\r\n     78                     result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n     79             except Exception:\r\n---> 80                 result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n     81 \r\n     82             return result\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/nanops.py in nanmin(values, axis, skipna)\r\n    279 @bottleneck_switch()\r\n    280 def nanmin(values, axis=None, skipna=True):\r\n--> 281     values, mask, dtype = _get_values(values, skipna, fill_value_typ = \'+inf\')\r\n    282 \r\n    283     # numpy 1.6.1 workaround in Python 3.x\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/nanops.py in _get_values(values, skipna, fill_value, fill_value_typ, isfinite, copy)\r\n    130         mask = _isfinite(values)\r\n    131     else:\r\n--> 132         mask = isnull(values)\r\n    133 \r\n    134     dtype    = values.dtype\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/common.py in isnull(obj)\r\n     59         given which of the element is null.\r\n     60     """"""\r\n---> 61     return _isnull(obj)\r\n     62 \r\n     63 \r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/common.py in _isnull_new(obj)\r\n     68     from pandas.core.generic import PandasContainer\r\n     69     if isinstance(obj, np.ndarray):\r\n---> 70         return _isnull_ndarraylike(obj)\r\n     71     elif isinstance(obj, PandasContainer):\r\n     72         # TODO: optimize for DataFrame, etc.\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/common.py in _isnull_ndarraylike(obj)\r\n    157     else:\r\n    158         # -np.isfinite(obj)\r\n--> 159         result = np.isnan(obj)\r\n    160     return result\r\n    161 \r\n\r\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\r\n```\r\n\r\nI get the same Error with max as well.\r\n\r\nmedian appears to work, although the type is incorrect:\r\n```\r\nIn [144]:\r\npd.DataFrame(pd.Series((np.array([np.timedelta64(3000000000),np.timedelta64(3000000000)])))).median()\r\n\r\nOut[144]:\r\n0    3000000000\r\ndtype: float64\r\n```'"
5689,24216004,cancan101,jreback,2013-12-12 23:09:22,2014-02-04 15:53:19,2014-01-20 23:37:17,closed,,0.13.1,3,API Design;Bug;Dtypes;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5689,b'BUG: timedelta64 dtype inference in Series',"b""related #5690\r\n\r\nThis works:\r\n```\r\npd.Series(np.array([np.timedelta64(300000000)]))\r\n```\r\n```\r\n0   00:00:00.300000\r\ndtype: timedelta64\r\n```\r\n\r\nbut this does not:\r\n```\r\npd.Series([np.timedelta64(300000000)])\r\n```\r\n\r\non Numpy 1.7:\r\n```\r\nTypeError: don't know how to convert scalar number to float\r\n```\r\n\r\nand on Numpy 1.8:\r\n```\r\n0    300000000\r\ndtype: int64\r\n```"""
5688,24209728,cancan101,jreback,2013-12-12 21:27:31,2013-12-12 21:59:22,2013-12-12 21:59:22,closed,,0.13.1,1,Bug;Duplicate,https://api.github.com/repos/pydata/pandas/issues/5688,b'NaT Not Correctly showing up as null value',"b""On pandas 0.12:\r\n```\r\npd.DataFrame( pd.Series([datetime(2012,12,12).replace(tzinfo=pytz.utc)] + [pd.NaT]*100))\r\n```\r\nI would argue this should show 1 non null-value:\r\n```\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 101 entries, 0 to 100\r\nData columns (total 1 columns):\r\n0    101  non-null values\r\ndtypes: object(1)\r\n```"""
5684,24182757,TomAugspurger,jreback,2013-12-12 14:47:14,2014-02-05 02:08:12,2014-02-05 02:08:12,closed,,0.14.0,4,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/5684,b'BUG: Series.xs() inconsistent with DataFrame.xs() with MultiIndex',"b""*Edited to clarify the bug*\r\n\r\n`Series.xs` slice fails with string index labels and MultiIndex:\r\n\r\n```python\r\nIn [1]: idx = pd.MultiIndex.from_tuples([('a', 'one'), ('a', 'two'), ('b', 'one'), ('b', 'two')])\r\n\r\nIn [2]: df = pd.Series(np.random.randn(4), index=idx)\r\n\r\nIn [4]: df.index.set_names(['L1', 'L2'], inplace=True)\r\n\r\nIn [5]: df\r\nOut[5]: \r\nL1  L2 \r\na   one   -0.136418\r\n    two   -0.346941\r\nb   one   -1.468534\r\n    two    1.217693\r\ndtype: float64\r\n\r\nIn [6]: df.xs('one', level='L2')\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-6-52601adf5184> in <module>()\r\n----> 1 df.xs('one', level='L2')\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.13.0rc1_27_g4d5ca5c-py2.7-macosx-10.8-x86_64.egg/pandas/core/series.pyc in _xs(self, key, axis, level, copy)\r\n    437 \r\n    438     def _xs(self, key, axis=0, level=None, copy=True):\r\n--> 439         return self.__getitem__(key)\r\n    440 \r\n    441     xs = _xs\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.13.0rc1_27_g4d5ca5c-py2.7-macosx-10.8-x86_64.egg/pandas/core/series.pyc in __getitem__(self, key)\r\n    482     def __getitem__(self, key):\r\n    483         try:\r\n--> 484             return self.index.get_value(self, key)\r\n    485         except InvalidIndexError:\r\n    486             pass\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.13.0rc1_27_g4d5ca5c-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in get_value(self, series, key)\r\n   2294                     raise InvalidIndexError(key)\r\n   2295                 else:\r\n-> 2296                     raise e1\r\n   2297             except Exception:  # pragma: no cover\r\n   2298                 raise e1\r\n\r\nKeyError: 'one'\r\n```\r\n\r\nThe same slice works on a DataFrame.\r\n\r\nPrevious post below:\r\n\r\n```python\r\nIn [12]: idx = pd.MultiIndex.from_tuples([('a', 0), ('a', 1), ('b', 0), ('b', 1)])\r\n\r\nIn [13]: df = pd.Series(np.random.randn(4), index=idx)\r\n\r\nIn [14]: df\r\nOut[14]: \r\na  0    0.876121\r\n   1    0.638050\r\nb  0    0.965934\r\n   1    1.061716\r\ndtype: float64\r\n\r\nIn [15]: df.xs(0, level=1)   # returns scaler\r\nOut[15]: 0.87612104445620753\r\n\r\nIn [16]: df.index.names = ['L1', 'L2']\r\n\r\nIn [27]: df.xs(0, level='L2')   # returns scaler\r\nOut[27]: -0.98585685847339011\r\n\r\nIn [28]: df.xs(0, level='L1')   # No key error\r\nOut[28]: -0.98585685847339011\r\n```\r\n\r\nWorks for DataFrames:\r\n\r\n```python\r\nIn [30]: df.xs(0, level='L2')\r\nOut[30]: \r\n           0\r\nL1          \r\na  -0.985857\r\nb   0.648114\r\n\r\n[2 rows x 1 columns]\r\n```\r\n\r\n`Series.xs` also seems to fail on string index labels?\r\n\r\n```python\r\nIn [50]: idx = pd.MultiIndex.from_tuples([('a', 'one'), ('a', 'two'), ('b', 'one'), ('b', 'two')])\r\nIn [53]: df = pd.Series(np.random.randn(4), index=idx)\r\nIn [56]: df.xs('one', level='L2')\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-56-52601adf5184> in <module>()\r\n----> 1 df.xs('one', level='L2')\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.13.0rc1_27_g4d5ca5c-py2.7-macosx-10.8-x86_64.egg/pandas/core/series.pyc in _xs(self, key, axis, level, copy)\r\n    437 \r\n    438     def _xs(self, key, axis=0, level=None, copy=True):\r\n--> 439         return self.__getitem__(key)\r\n    440 \r\n    441     xs = _xs\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.13.0rc1_27_g4d5ca5c-py2.7-macosx-10.8-x86_64.egg/pandas/core/series.pyc in __getitem__(self, key)\r\n    482     def __getitem__(self, key):\r\n    483         try:\r\n--> 484             return self.index.get_value(self, key)\r\n    485         except InvalidIndexError:\r\n    486             pass\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.13.0rc1_27_g4d5ca5c-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in get_value(self, series, key)\r\n   2294                     raise InvalidIndexError(key)\r\n   2295                 else:\r\n-> 2296                     raise e1\r\n   2297             except Exception:  # pragma: no cover\r\n   2298                 raise e1\r\n\r\nKeyError: 'one'\r\n```\r\n\r\nSo I guess this is about 3 errors on `Series.xs` (possibly related?):\r\n\r\n1. Returning scalers when it should return a Series when the label is an integer\r\n2. Not raising key errors when the label is an integer\r\n3. Failing on slices for `level>1` when the label is a string.\r\n\r\nEDIT: Oh, and I know that `.loc` / `.ix` will work for these. I was just surprised by the results."""
5683,24168273,omar-masmoudi,jreback,2013-12-12 10:15:10,2013-12-12 12:44:17,2013-12-12 12:44:17,closed,,0.13,1,Bug;Duplicate;Indexing,https://api.github.com/repos/pydata/pandas/issues/5683,b'Failed to assign new value to masked serie',"b'Hello,\r\n\r\n```\r\n#!/bin/env python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport sys\r\nprint sys.version\r\nprint ""NUMPY:"", np.__version__\r\nprint ""PANDAS:"", pd.__version__\r\n\r\ndf = pd.DataFrame([[0, None], [4, 3]], columns=[\'a\',\'b\'])\r\nprint df\r\n\r\ndf[""c""] = 1.0\r\n#print df #uncomment and result change\r\n\r\ndf[""c""][~df.all(axis=1)] = 2\r\nprint df\r\nprint df.c\r\n``` \r\n\r\ngives\r\n\r\n``` \r\n2.7.2 (default, Jul 30 2012, 17:32:47)\r\n[GCC 4.1.2 20080704 (Red Hat 4.1.2-46)]\r\nNUMPY: 1.7.1\r\nPANDAS: 0.12.0\r\n   a   b\r\n0  0 NaN\r\n1  4   3\r\n   a   b  c\r\n0  0 NaN  1\r\n1  4   3  1\r\n0    2\r\n1    1\r\nName: c, dtype: float64\r\n``` \r\n\r\nAs you can see, I try to change value of the c columns to ""2"" for the element where one of the column is null and it does not work.\r\n\r\nThe serie is however modified.\r\nWhat is even stranger is that adding an intermediate print (uncomment above) changes the behavior.\r\n\r\nI was not able to find any duplicate of this issue.\r\nThanks in advance for having a look.\r\n'"
5678,24105710,floux,jreback,2013-12-11 13:10:33,2013-12-11 18:05:38,2013-12-11 15:26:47,closed,,0.13,4,API Design;Bug;Indexing;Regression,https://api.github.com/repos/pydata/pandas/issues/5678,b'API: selection from a duplicated index of a Series return a Series?',"b""See below for discussion\r\n\r\n-----\r\n\r\n```\r\nIn [19]: import pandas as pd\r\n\r\nIn [20]: import numpy as np\r\n\r\nIn [21]: import random\r\n\r\nIn [22]: df = pd.DataFrame(np.random.random_sample((20,5)), index=[random.choice('ABCDE') for x in range(20)])\r\n\r\nIn [23]: df.loc[:,0].ix['A'].median()\r\nOut[23]: 0.57704085832236685\r\n\r\nIn [24]: pd.version.version\r\nOut[24]: '0.12.0'\r\n\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: import random\r\n\r\nIn [4]: df = pd.DataFrame(np.random.random_sample((20,5)), index=[random.choice('ABCDE') for x in range(20)])\r\n\r\nIn [5]: df.loc[:,0].ix['A'].median()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-57f9fc9d1583> in <module>()\r\n----> 1 df.loc[:,0].ix['A'].median()\r\n\r\nAttributeError: 'numpy.ndarray' object has no attribute 'median'\r\n\r\nIn [6]: pd.version.version\r\nOut[6]: '0.13.0rc1-43-g4f9fefc'\r\n```"""
5664,23917926,dsm054,jreback,2013-12-08 03:52:35,2015-04-12 13:37:27,2015-04-12 13:37:27,closed,,0.16.1,11,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/5664,b'BUG: read_csv segfault',"b'Came across this when trying to parse a poorly-formatted csv file and got the number of columns wrong.  Mostly it segfaults; sometimes it produces a broken last row.\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> from StringIO import StringIO\r\n>>> pd.__version__\r\n\'0.13.0rc1-43-g4f9fefc\'\r\n>>> \r\n>>> f = StringIO(\'1,1,1,1,0\\n\'*2 + \'\\n\'*2)\r\n>>> df = pd.read_csv(f,names=list(""abcd""))\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nWhen it doesn\'t segfault it tends to produce\r\n\r\n```\r\n      a   b   c    d\r\n 1    1   1   1    0\r\n 1    1   1   1    0\r\nNaN NaN NaN NaN  NaN\r\nNaN NaN NaN NaN  [unicode box: 0 0 0 1]\r\n```\r\n\r\n\r\n\r\n'"
5662,23912545,whoburg,TomAugspurger,2013-12-07 21:46:47,2015-03-31 21:06:09,2015-03-31 21:06:09,closed,,Next Major Release,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/5662,b'unexpected axis ticks in tools.plotting.scatter_matrix',"b""The upper left y axis tick marks in a scatter_matrix appear to have units/values pulled from the (0, 0) histogram counts, instead of the data values that are used for all other axis tick values.\r\n\r\nExample:\r\n\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: pd.__version__\r\nOut[3]: '0.12.0'\r\n\r\nIn [4]: df = pd.DataFrame(np.random.randn(100,3), columns=['a', 'b', 'c'])\r\n\r\nIn [5]: pd.tools.plotting.scatter_matrix(df)\r\nOut[5]: \r\narray([[<matplotlib.axes.AxesSubplot object at 0x1071d5d90>,\r\n        <matplotlib.axes.AxesSubplot object at 0x1089aa0d0>,\r\n        <matplotlib.axes.AxesSubplot object at 0x1071e36d0>],\r\n       [<matplotlib.axes.AxesSubplot object at 0x10a7d4f90>,\r\n        <matplotlib.axes.AxesSubplot object at 0x10a7fd450>,\r\n        <matplotlib.axes.AxesSubplot object at 0x10a8197d0>],\r\n       [<matplotlib.axes.AxesSubplot object at 0x10a83bf90>,\r\n        <matplotlib.axes.AxesSubplot object at 0x10a84fe50>,\r\n        <matplotlib.axes.AxesSubplot object at 0x10a886310>]], dtype=object)\r\n```\r\n\r\n![figure_1](https://f.cloud.github.com/assets/6042395/1699217/0ceb08aa-5f89-11e3-9cf2-0b6f4fed6fee.png)\r\n"""
5652,23871675,janschulz,jreback,2013-12-06 17:16:00,2014-01-24 20:07:55,2014-01-23 22:03:09,closed,,0.13.1,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5652,"b'""df.get(None)"" throws error in 0.13 -> regression?'","b'This works with pandas 0.12:\r\n```python\r\nfrom ggplot import meat\r\nmeat.get(None)\r\n[returns None]\r\n```\r\nBut throws an error in 0.13 (RC):\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-86be6a7317b8> in <module>()\r\n----> 1 meat.get(None)\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in get(self, key, default)\r\n    961         """"""\r\n    962         try:\r\n--> 963             return self[key]\r\n    964         except KeyError:\r\n    965             return default\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __getitem__(self, key)\r\n   1626             return self._getitem_multilevel(key)\r\n   1627         else:\r\n-> 1628             return self._getitem_column(key)\r\n   1629 \r\n   1630     def _getitem_column(self, key):\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _getitem_column(self, key)\r\n   1633         # get column\r\n   1634         if self.columns.is_unique:\r\n-> 1635             return self._get_item_cache(key)\r\n   1636 \r\n   1637         # duplicate columns & possible reduce dimensionaility\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in _get_item_cache(self, item)\r\n    972         res = cache.get(item)\r\n    973         if res is None:\r\n--> 974             values = self._data.get(item)\r\n    975             res = self._box_item_values(item, values)\r\n    976             cache[item] = res\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in get(self, item)\r\n   2737             if isnull(item):\r\n   2738                 indexer = np.arange(len(self.items))[isnull(self.items)]\r\n-> 2739                 return self.get_for_nan_indexer(indexer)\r\n   2740 \r\n   2741             _, block = self._find_block(item)\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in get_for_nan_indexer(self, indexer)\r\n   2789                 indexer = indexer.item()\r\n   2790             else:\r\n-> 2791                 raise ValueError(""cannot label index with a null key"")\r\n   2792 \r\n   2793         # take a nan indexer and return the values\r\n\r\nValueError: cannot label index with a null key\r\n```\r\n\r\nNot sure if that is intentional as this broke some code in the facet_wrap in ggplot (which now will get some update against this :-) )'"
5637,23671684,jikamens,jreback,2013-12-03 20:33:11,2014-04-06 15:03:05,2014-04-06 15:03:01,closed,,0.14.0,1,Bug;Duplicate;Frequency;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/5637,b'datetools.BDay() does the wrong thing when given a Friday - Saturday range',"b""With a daily frequency, when you specify a range from a Friday morning UTC to Saturday morning UTC, you get back both days as expected:\r\n\r\n    (Pdb) pd.DatetimeIndex(start=datetime.datetime(2013,11,22,5,0,0,tzinfo=pytz.utc), end=datetime.datetime(2013,11,23,4,59,59,tzinfo=pytz.utc), freq=pd.datetools.Day())\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2013-11-22 05:00:00, 2013-11-23 05:00:00]\r\n    Length: 2, Freq: D, Timezone: UTC\r\n\r\nWith a BDay frequency, however, you get back *neither* day, even though you should get back the Friday since it's a business dasy:\r\n\r\n    (Pdb) pd.DatetimeIndex(start=datetime.datetime(2013,11,22,5,0,0,tzinfo=pytz.utc), end=datetime.datetime(2013,11,23,4,59,59,tzinfo=pytz.utc), freq=pd.datetools.BDay())\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    Length: 0, Freq: B, Timezone: UTC\r\n\r\nThe correct thing happens if you shift both dates back by a day:\r\n\r\n    (Pdb) pd.DatetimeIndex(start=datetime.datetime(2013,11,21,5,0,0,tzinfo=pytz.utc), end=datetime.datetime(2013,11,22,4,59,59,tzinfo=pytz.utc), freq=pd.datetools.BDay())\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2013-11-21 05:00:00]\r\n    Length: 1, Freq: B, Timezone: UTC\r\n\r\nYou also get back the Friday if you extend the range to the next business day:\r\n\r\n    (Pdb) pd.DatetimeIndex(start=datetime.datetime(2013,11,22,5,0,0,tzinfo=pytz.utc), end=datetime.datetime(2013,11,25,4,59,59,tzinfo=pytz.utc), freq=pd.datetools.BDay())\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2013-11-22 05:00:00]\r\n    Length: 1, Freq: B, Timezone: UTC\r\n\r\nRef: https://github.com/quantopian/zipline/issues/243"""
5636,23669681,cancan101,jreback,2013-12-03 20:03:36,2016-04-19 18:36:05,2016-04-19 18:36:05,closed,,0.18.1,3,API Design;Bug;Difficulty Novice;Effort Low;Error Reporting;IO CSV,https://api.github.com/repos/pydata/pandas/issues/5636,b'read_csv not correctly parsing dates when parse_dates is string and index_col not set',"b'With:\r\n```\r\ncsv=""""""A,B,C\r\n1,2,2003-11-1\r\n""""""\r\n```\r\nThese all work as expected:\r\n```\r\nIn [40]: pd.read_csv(StringIO(csv), parse_dates=""C"",index_col=""C"").index[0]\r\n\r\nOut[40]: Timestamp(\'2003-11-01 00:00:00\', tz=None)\r\n\r\nIn [41]: pd.read_csv(StringIO(csv), parse_dates=[""C""],index_col=""C"").index[0]\r\n\r\nOut[41]: Timestamp(\'2003-11-01 00:00:00\', tz=None)\r\n\r\nIn [42]: pd.read_csv(StringIO(csv), parse_dates=[""C""]).C[0]\r\n\r\nOut[42]: Timestamp(\'2003-11-01 00:00:00\', tz=None)\r\n```\r\n\r\nbut this does not parse the string:\r\n\r\n```\r\nIn [39]: pd.read_csv(StringIO(csv), parse_dates=""C"",).C[0]\r\n\r\nOut[39]: \'2003-11-1\'\r\n```'"
5635,23620886,TomAugspurger,jreback,2013-12-03 04:36:41,2014-02-15 21:00:15,2014-02-15 21:00:15,closed,,0.14.0,6,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5635,b'BUG: empty result for HDFStore where with leading NaNs',"b'https://github.com/PyTables/PyTables/issues/282 is the culprit \r\n\r\nI may be missing something here, but I think this is an odd result. When reading from a `table` HDFStore, if the first value is NaN then any `where`s based on that column will return an empty selection.\r\n\r\n```python\r\nIn [58]: sm\r\nOut[58]: \r\n            HRHHID  earnings\r\n0  251395039600155       NaN\r\n1  251396749500840    188461\r\n2  251396749500840     57692\r\n3  251408550479107       NaN\r\n4  251465059600832     61500\r\n\r\nIn [63]: tst = pd.HDFStore(\'tst.h5\')\r\n\r\n# leading NaN, empty\r\nIn [64]: sm.to_hdf(tst, \'sm\', data_columns=True, format=\'table\')\r\n\r\nIn [65]: tst.select(\'sm\', where=""earnings>0"")\r\nOut[65]: \r\nEmpty DataFrame\r\nColumns: [HRHHID, earnings]\r\nIndex: []\r\n\r\n# leading isn\'t NaN, works\r\nIn [66]: sm.tail(4).to_hdf(tst, \'sm\', data_columns=True, format=\'table\')\r\n\r\nIn [67]: tst.select(\'sm\', where=""earnings>0"")\r\nOut[67]: \r\n            HRHHID  earnings\r\n1  251396749500840    188461\r\n2  251396749500840     57692\r\n4  251465059600832     61500\r\n\r\n# Leading NaN again, empty\r\nIn [69]: sm.tail(2).to_hdf(tst, \'sm\', data_columns=True, format=\'table\')\r\n\r\nIn [70]: tst.select(\'sm\', where=""earnings>0"")\r\nOut[70]: \r\nEmpty DataFrame\r\nColumns: [HRHHID, earnings]\r\nIndex: []\r\n\r\n\r\nIn [71]: pd.__version__\r\nOut[71]: \'0.12.0-1153-g476b6e3\'\r\n\r\nIn [72]: import tables\r\n\r\nIn [73]: tables.__version__\r\nOut[73]: \'3.0.0\'\r\n```\r\n\r\n\r\n'"
5632,23603011,FragLegs,jreback,2013-12-02 21:26:59,2013-12-03 14:53:32,2013-12-03 01:59:04,closed,,0.13,6,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5632,b'Setting empty column on empty DataFrame',"b'This issue appears in 0.13.0-rc1 and did not appear in 0.12.0-854-gde63e00\r\n\r\nLet us say we have an empty DataFrame. Then the following calls (which worked in pandas 0.12) no longer work:\r\n\r\n```python\r\ndf = pd.DataFrame()\r\ndf[\'foo\'] = []\r\n\r\ndf = pd.DataFrame()\r\ndf[\'foo\'] = df.index\r\n\r\ndf = pd.DataFrame()\r\ndf[\'foo\'] = range(len(df))\r\n```\r\n\r\nThey all throw an exception:\r\n\r\n```\r\nFile ""/usr/local/lib/python3.2/dist-packages/pandas-0.13.0rc1-py3.2-linux-x86_64.egg/pandas/core/frame.py"", line 1855, in __setitem__\r\n    self._set_item(key, value)\r\n  File ""/usr/local/lib/python3.2/dist-packages/pandas-0.13.0rc1-py3.2-linux-x86_64.egg/pandas/core/frame.py"", line 1915, in _set_item\r\n    self._ensure_valid_index(value)\r\n\r\nFile ""/usr/local/lib/python3.2/dist-packages/pandas-0.13.0rc1-py3.2-linux-x86_64.egg/pandas/core/frame.py"", line 1899, in _ensure_valid_index\r\n    raise ValueError(\'Cannot set a frame with no defined index \'\r\nValueError: Cannot set a frame with no defined index and a non-series\r\n```\r\n\r\nThe following will work:\r\n\r\n```python\r\ndf = pd.DataFrame()\r\ndf[\'foo\'] = pd.Series([])\r\n\r\ndf = pd.DataFrame()\r\ndf[\'foo\'] = pd.Series(df.index)\r\n\r\ndf = pd.DataFrame()\r\ndf[\'foo\'] = pd.Series(range(len(df)))\r\n```\r\n\r\nThe issue appears to be on lines 1897-1899 of pandas.core.frame:\r\n\r\n```python\r\nif not len(self.index):\r\n    if not isinstance(value, Series):\r\n        raise ValueError(\'Cannot set a frame with no defined index \'\r\n```\r\n\r\nPerhaps it could be changed to:\r\n\r\n```python\r\nif not len(self.index) and len(value) > 0:\r\n    if not isinstance(value, Series):\r\n        raise ValueError(\'Cannot set a frame with no defined index \'\r\n```'"
5628,23550720,michaelaye,jreback,2013-12-02 03:40:47,2013-12-06 18:36:25,2013-12-02 12:53:01,closed,,0.13,17,Bug,https://api.github.com/repos/pydata/pandas/issues/5628,"b'Dropping rows inplace, weird bug with 0.13.0rc1'","b""In c239b06e I have the following weird behavior. I am even unsure how to describe it, but somehow the dropping of rows selected via a boolean leads to a failure of dropping the rows only for the Series of a DataFrame that met the condition in the first place.\r\nPlease have a look at the MWE I figured out after some hacking and trying out:\r\n\r\nhttp://nbviewer.ipython.org/gist/michaelaye/7744592\r\n\r\nPlease tell me I'm not being dumb again?\r\n"""
5616,23501543,aldanor,jtratner,2013-11-29 16:50:12,2013-12-10 13:19:30,2013-12-10 13:19:30,closed,,0.13,8,Bug;IO Excel;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/5616,b'BUG: DataFrame.to_excel(index=False) misplaces data/columns in MultiIndex case',"b""Example:\r\n```python\r\n>>> df = pd.DataFrame({'a': [10, 20], 'b': [30, 40], 'c': [50, 60]}, \r\n...                   index=pd.MultiIndex.from_tuples([(70, 80), (90, 100)]))\r\n>>> writer = pd.ExcelWriter('test.xls')\r\n>>> df.to_excel(writer, sheet_name='1', index=False)\r\n>>> writer.save()\r\n>>> pd.read_excel('test.xls', '1')\r\n        a   b   c\r\n10 30  50 NaN NaN\r\n20 40  60 NaN NaN\r\n```\r\nThe column names are obviously misplaced (erroneously shifted to the right by the number of levels in the MultiIndex). Or is it the data that is misplaced? Wonder why no one noticed this before."""
5614,23466381,hayd,jreback,2013-11-28 20:37:33,2014-05-01 22:23:50,2014-05-01 15:13:57,closed,hayd,0.14.0,16,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/5614,b'cumsum sums the groupby column',"b""It shouldn't sum the groupby'd col (in fact index col should be the index, if groupby as_index).\r\n```\r\nIn [13]: df = pd.DataFrame([[1, 2, np.nan], [1, np.nan, 9], [3, 4, 9]], columns=['A', 'B', 'C'])\r\n\r\nIn [14]: g = df.groupby('A')\r\n\r\nIn [16]: g.cumsum()\r\nOut[16]: \r\n   A   B   C\r\n0  1   2 NaN\r\n1  2 NaN   9\r\n2  3   4   9\r\n\r\n[3 rows x 3 columns]\r\n```\r\nNature of it being dispatch. Should fix up for 0.14 possibly along with some other whitelisted groupby functions."""
5613,23458643,jorisvandenbossche,jreback,2013-11-28 16:30:09,2016-01-11 13:28:35,2016-01-11 13:28:35,closed,,0.18.0,15,Bug;Difficulty Intermediate;Effort Low;Frequency;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5613,b'asfreq / resample do not set freq with regular timeseries',"b'If your timeseries has a regularly spaced index, so if a frequency can be inferred, using `asfreq` or `resample` with this frequency does not set the frequency (it remains None):\r\n\r\n```\r\nIn [1]: df = pd.DataFrame({\'date\':[""2012-01-01"", ""2012-01-02"", ""2012-01-03""], \'col\':[1,2,3]})\r\nIn [2]: df2 = df.set_index(pd.to_datetime(df.date))\r\nIn [3]: df2.index\r\nOut[3]:\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2012-01-01 00:00:00, ..., 2012-01-03 00:00:00]\r\nLength: 3, Freq: None, Timezone: None\r\n\r\nIn [4]: df2.index.inferred_freq\r\nOut[4]: \'D\'\r\n\r\nIn [5]: df2.asfreq(\'D\').index\r\nOut[5]:\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2012-01-01 00:00:00, ..., 2012-01-03 00:00:00]\r\nLength: 3, Freq: None, Timezone: None\r\n\r\nIn [6]: df2.resample(\'D\').index\r\nOut[6]:\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2012-01-01 00:00:00, ..., 2012-01-03 00:00:00]\r\nLength: 3, Freq: None, Timezone: None\r\n```\r\nNote: this is the same with a Series, however in 0.12 it works for Series (as then it is still a TimeSeries)\r\n\r\n\r\n\r\n'"
5610,23445537,jorisvandenbossche,jreback,2013-11-28 11:52:46,2014-04-29 20:12:46,2014-04-29 20:12:46,closed,,0.14.0,16,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/5610,b'GroupBy.count() returns the grouping column as both index and column',"b""``GroupBy.count()`` (with the default `as_index=True`) return the grouping column both as index and as column, while other methods as `first` and `sum` keep it only as the index (which is most logical I think). This seems a minor inconsistency to me:\r\n\r\n```\r\nIn [41]: data = pd.DataFrame({'name' : ['a', 'a', 'b', 'd'], 'counts' : [3,4,3,2]})\r\nIn [42]: data\r\nOut[42]:\r\n   counts name\r\n0       3    a\r\n1       4    a\r\n2       3    b\r\n3       2    d\r\n\r\nIn [43]: g = data.groupby('name')\r\nIn [45]: g.count()\r\nOut[45]:\r\n      counts  name\r\nname\r\na          2     2\r\nb          1     1\r\nd          1     1\r\n\r\nIn [46]: g.first()\r\nOut[46]:\r\n      counts\r\nname\r\na          3\r\nb          3\r\nd          2\r\n\r\nIn [47]: g.sum()\r\nOut[47]:\r\n      counts\r\nname\r\na          7\r\nb          3\r\nd          2\r\n```"""
5605,23425517,michaelaye,jreback,2013-11-28 01:18:28,2013-11-28 05:29:28,2013-11-28 03:19:24,closed,,0.13,7,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/5605,b'df.set_index() broken',"b'http://nbviewer.ipython.org/gist/michaelaye/7685834\r\n\r\nVersion: d057fc9  (\'0.12.0-1178-gd057fc9\')\r\n\r\nI tried a sequence of ""python setup.py clean && python setup.py install"" without success.\r\n\r\n'"
5597,23379811,jseabold,jreback,2013-11-27 10:57:51,2015-03-07 22:51:45,2013-11-29 19:13:20,closed,,0.13,19,API Design;Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5597,b'spurious SettingWithCopyWarning ',"b""I'm getting spurious warnings on some old code that I'm running with new pandas. You can replicate by doing something like this (you have to take a subset of the data first, that's the key)\r\n\r\n```\r\nimport pandas as pd\r\nfrom pandas.core.common import SettingWithCopyWarning\r\nfrom string import letters\r\nimport warnings\r\nwarnings.simplefilter('error', SettingWithCopyWarning)\r\n\r\ndef random_text(nobs=100):\r\n    df = []\r\n    for i in range(nobs):\r\n        idx= np.random.randint(len(letters), size=2)\r\n        idx.sort()\r\n        df.append([letters[idx[0]:idx[1]]])\r\n\r\n    return pd.DataFrame(df, columns=['letters'])\r\n\r\ndf = random_text(100000)\r\n\r\ndf = df.ix[df.letters.apply(lambda x : len(x) > 10)]\r\ndf['letters'] = df['letters'].apply(str.lower)\r\n```"""
5596,23376193,Ranadheer1,jreback,2013-11-27 09:44:38,2013-11-27 16:13:57,2013-11-27 16:13:49,closed,,,7,Bug;Can't Repro,https://api.github.com/repos/pydata/pandas/issues/5596,b'Indexing with hierarchical index issue',"b""I'm using pandas 0.12.0 and having issue with accessing hierarchically indexed data frame elements using the ix. As a note, the issue is coming only when I have datetime objects in level 0 of hierarchical index. \r\n\r\narrays = [date_range('2013-11-15', periods=4).repeat(2),\r\n             ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\r\ntuples = list(zip(*arrays))\r\nindex = MultiIndex.from_tuples(tuples, names=['first', 'second'])\r\ndf = DataFrame(randn(8, 3), index=index, columns=['A', 'B', 'C'])\r\ndf.ix['2013-11-15', 'one']\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-99-c55bdd03164b> in <module>()\r\n----> 1 df.ix['2013-11-15', 'one']\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n     45                 pass\r\n     46 \r\n---> 47             return self._getitem_tuple(key)\r\n     48         else:\r\n     49             return self._getitem_axis(key, axis=0)\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    251     def _getitem_tuple(self, tup):\r\n    252         try:\r\n--> 253             return self._getitem_lowerdim(tup)\r\n    254         except IndexingError:\r\n    255             pass\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in _getitem_lowerdim(self, tup)\r\n    384                         new_key, = new_key\r\n    385 \r\n--> 386                 return getattr(section,self.name)[new_key]\r\n    387 \r\n    388         raise IndexingError('not applicable')\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n     45                 pass\r\n     46 \r\n---> 47             return self._getitem_tuple(key)\r\n     48         else:\r\n     49             return self._getitem_axis(key, axis=0)\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    251     def _getitem_tuple(self, tup):\r\n    252         try:\r\n--> 253             return self._getitem_lowerdim(tup)\r\n    254         except IndexingError:\r\n    255             pass\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in _getitem_lowerdim(self, tup)\r\n    361         for i, key in enumerate(tup):\r\n    362             if _is_label_like(key) or isinstance(key, tuple):\r\n--> 363                 section = self._getitem_axis(key, axis=i)\r\n    364 \r\n    365                 # we have yielded a scalar ?\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in _getitem_axis(self, key, axis)\r\n    411                     return self._get_loc(key, axis=axis)\r\n    412 \r\n--> 413             return self._get_label(key, axis=axis)\r\n    414 \r\n    415     def _getitem_iterable(self, key, axis=0):\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/indexing.pyc in _get_label(self, label, axis)\r\n     59             return self.obj._xs(label, axis=axis, copy=False)\r\n     60         except Exception:\r\n---> 61             return self.obj._xs(label, axis=axis, copy=True)\r\n     62 \r\n     63     def _get_loc(self, key, axis=0):\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/frame.pyc in xs(self, key, axis, level, copy)\r\n   2358 \r\n   2359         if axis == 1:\r\n-> 2360             data = self[key]\r\n   2361             if copy:\r\n   2362                 data = data.copy()\r\n\r\n/u/pullurur/dig/quantinvest/env/lib/python2.6/site-packages/pandas/core/frame.pyc in __getitem__(self, key)\r\n   2001             # get column\r\n   2002             if self.columns.is_unique:\r\n-> 2003                 return self._get_item_cache(key)\r\n   2004 \r\n   2005             # duplicate columns\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/generic.pyc in _get_item_cache(self, item)\r\n    665             return cache[item]\r\n    666         except Exception:\r\n--> 667             values = self._data.get(item)\r\n    668             res = self._box_item_values(item, values)\r\n    669             cache[item] = res\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/internals.pyc in get(self, item)\r\n   1653     def get(self, item):\r\n   1654         if self.items.is_unique:\r\n-> 1655             _, block = self._find_block(item)\r\n   1656             return block.get(item)\r\n   1657         else:\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/internals.pyc in _find_block(self, item)\r\n   1933 \r\n   1934     def _find_block(self, item):\r\n-> 1935         self._check_have(item)\r\n   1936         for i, block in enumerate(self.blocks):\r\n   1937             if item in block:\r\n\r\n~/env/lib/python2.6/site-packages/pandas/core/internals.pyc in _check_have(self, item)\r\n   1940     def _check_have(self, item):\r\n   1941         if item not in self.items:\r\n-> 1942             raise KeyError('no item named %s' % com.pprint_thing(item))\r\n   1943 \r\n   1944     def reindex_axis(self, new_axis, method=None, axis=0, copy=True):\r\n\r\nKeyError: u'no item named one'"""
5595,23367540,changhiskhan,jreback,2013-11-27 05:09:57,2013-11-27 13:28:17,2013-11-27 13:28:17,closed,,0.13,2,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/5595,b'Test failures on windows',"b'The included error message is on 32-bit windows for python 2.6 but it\'s the same for 64-bit (waiting for other python versions to finish)\r\n\r\nthis is a blocker for 0.13 rc1\r\n\r\n```\r\n======================================================================\r\nFAIL: test_groupby_return_type (pandas.tests.test_groupby.TestGroupBy)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test-py26\\pandas\\tests\\test_groupby.py"", line 331, in test_groupby_return_type\r\n    assert_frame_equal(result,expected)\r\n  File ""C:\\workspace\\pandas-windows-test-py26\\pandas\\util\\testing.py"", line 475, in assert_frame_equal\r\n    check_less_precise=check_less_precise)\r\n  File ""C:\\workspace\\pandas-windows-test-py26\\pandas\\util\\testing.py"", line 423, in assert_series_equal\r\n    assert_attr_equal(\'dtype\', left, right)\r\n  File ""C:\\workspace\\pandas-windows-test-py26\\pandas\\util\\testing.py"", line 407, in assert_attr_equal\r\n    assert_equal(left_attr,right_attr,""attr is not equal [{0}]"" .format(attr))\r\n  File ""C:\\workspace\\pandas-windows-test-py26\\pandas\\util\\testing.py"", line 390, in assert_equal\r\n    assert a == b, ""%s: %r != %r"" % (msg.format(a,b), a, b)\r\nAssertionError: attr is not equal [dtype]: dtype(\'int64\') != dtype(\'int32\')\r\n```'"
5592,23336324,jreback,jreback,2013-11-26 18:41:04,2014-06-05 17:19:54,2013-11-26 19:49:55,closed,,0.13,8,API Design;Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/5592,b'BUG/API: inconsistent results in a groupby-apply when mix of scalar/Series are returned',b'http://stackoverflow.com/questions/20224564/how-does-pandas-grouped-apply-decide-on-output-and-why-does-this-depend-on-w/20225276#20225276'
5591,23326552,whoburg,jreback,2013-11-26 16:29:40,2014-02-18 19:24:24,2014-02-18 19:24:24,closed,,0.14.0,7,API Design;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/5591,b'Series.__contains__ gives unexpected results',"b'``__contains__`` should validate the key to be of a correctly castable type, e.g. 0.1 should raise a ``TypeError`` with an ``Int64Index``\r\n\r\n-------------------\r\nI may well be missing something, but the following behavior seems unexpected:\r\n\r\n""""""\r\nIn [58]: 5 in pd.Series([1.5, 2.5, 3.5])\r\nOut[58]: False\r\n\r\nIn [59]: 5.0 in pd.Series([1.5, 2.5, 3.5])\r\nOut[59]: False\r\n\r\nIn [60]: 0.1 in pd.Series([1.5, 2.5, 3.5])\r\nOut[60]: True\r\n\r\nIn [61]: 0 in pd.Series([1.5, 2.5, 3.5])\r\nOut[61]: True\r\n\r\nIn [62]: 3.5 in pd.Series([1.5, 2.5, 3.5])\r\nOut[62]: False\r\n\r\nIn [63]: 3.4 in pd.Series([1.5, 2.5, 3.5])\r\nOut[63]: False\r\n\r\nIn [64]: 0.2 in pd.Series([1.5, 2.5, 3.5])\r\nOut[64]: True\r\n\r\nIn [65]: 1.6 in pd.Series([1.5, 2.5, 3.5])\r\nOut[65]: True\r\n\r\nIn [66]: 2.6 in pd.Series([1.5, 2.5, 3.5])\r\nOut[66]: True\r\n\r\nIn [67]: 3.0 in pd.Series([1.5, 2.5, 3.5])\r\nOut[67]: False\r\n\r\nIn [68]: 2.5 in pd.Series([1.5, 2.5, 3.5])\r\nOut[68]: True\r\n\r\nIn [69]: 0.1 in pd.Series([1.5, 2.5, 3.5])\r\nOut[69]: True\r\n""""""\r\n'"
5577,23180698,dieterv77,jreback,2013-11-23 03:27:08,2013-11-24 01:08:16,2013-11-24 01:08:16,closed,,0.13,2,Bug,https://api.github.com/repos/pydata/pandas/issues/5577,b'ma.mrecords not found',"b'Running into the issue below with pandas master and numpy compiled from 1.8.x maintenance branch.  If i do ""import numpy.ma.mrecords"" in ipython and try again, then creating the DataFrame works fine.  Maybe the best thing to do is to add ""import numpy.ma.mrecords"" in core/frame.py?\r\n\r\n```\r\nimport numpy.ma as ma\r\nimport pandas\r\nx = ma.masked_all((3,4))\r\ndf = pandas.DataFrame(x)\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-8935d2e6ea81> in <module>()\r\n----> 1 df = pandas.DataFrame(x)\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas-0.12.0_1147_g39ad4cb-py2.7-win-amd64.egg\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\r\n    203 \r\n    204             # masked recarray\r\n--> 205             if isinstance(data, ma.mrecords.MaskedRecords):\r\n    206                 mgr = _masked_rec_array_to_mgr(data, index, columns, dtype,\r\n    207                                                copy)\r\n\r\nAttributeError: \'module\' object has no attribute \'mrecords\'\r\n```'"
5557,23015054,dsm054,jreback,2013-11-20 19:16:25,2013-11-20 21:10:31,2013-11-20 20:15:58,closed,,0.13,3,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/5557,b'BUG: KeyErrors during value_counts',"b'I\'m getting occasional KeyErrors during `.value_counts()` on trunk.  It\'s pretty flaky, and this is the smallest example I had at hand (had one with only ~60 but lost it.)\r\n\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> print pd.__version__\r\n0.12.0-1128-ge6aaf5e\r\n>>> \r\n>>> ser = {256: 2321.0, 1: 78.0, 2: 2716.0, 3: 0.0, 4: 369.0, 5: 0.0, 6: 269.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 3536.0, 11: 0.0, 12: 24.0, 13: 0.0, 14: 931.0, 15: 0.0, 16: 101.0, 17: 78.0, 18: 9643.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 63761.0, 23: 0.0, 24: 446.0, 25: 0.0, 26: 34773.0, 27: 0.0, 28: 729.0, 29: 78.0, 30: 0.0, 31: 0.0, 32: 3374.0, 33: 0.0, 34: 1391.0, 35: 0.0, 36: 361.0, 37: 0.0, 38: 61808.0, 39: 0.0, 40: 0.0, 41: 0.0, 42: 6677.0, 43: 0.0, 44: 802.0, 45: 0.0, 46: 2691.0, 47: 0.0, 48: 3582.0, 49: 0.0, 50: 734.0, 51: 0.0, 52: 627.0, 53: 70.0, 54: 2584.0, 55: 0.0, 56: 324.0, 57: 0.0, 58: 605.0, 59: 0.0, 60: 0.0, 61: 0.0, 62: 3989.0, 63: 10.0, 64: 42.0, 65: 0.0, 66: 904.0, 67: 0.0, 68: 88.0, 69: 70.0, 70: 8172.0, 71: 0.0, 72: 0.0, 73: 0.0, 74: 64902.0, 75: 0.0, 76: 347.0, 77: 0.0, 78: 36605.0, 79: 0.0, 80: 379.0, 81: 70.0, 82: 0.0, 83: 0.0, 84: 3001.0, 85: 0.0, 86: 1630.0, 87: 7.0, 88: 364.0, 89: 0.0, 90: 67404.0, 91: 9.0, 92: 0.0, 93: 0.0, 94: 7685.0, 95: 0.0, 96: 1017.0, 97: 0.0, 98: 2831.0, 99: 0.0, 100: 2963.0, 101: 0.0, 102: 854.0, 103: 0.0, 104: 0.0, 105: 0.0, 106: 0.0, 107: 0.0, 108: 0.0, 109: 0.0, 110: 0.0, 111: 0.0, 112: 0.0, 113: 0.0, 114: 0.0, 115: 0.0, 116: 0.0, 117: 0.0, 118: 0.0, 119: 0.0, 120: 0.0, 121: 0.0, 122: 0.0, 123: 0.0, 124: 0.0, 125: 0.0, 126: 67744.0, 127: 22.0, 128: 264.0, 129: 0.0, 260: 197.0, 268: 0.0, 265: 0.0, 269: 0.0, 261: 0.0, 266: 1198.0, 267: 0.0, 262: 2629.0, 258: 775.0, 257: 0.0, 263: 0.0, 259: 0.0, 264: 163.0, 250: 10326.0, 251: 0.0, 252: 1228.0, 253: 0.0, 254: 2769.0, 255: 0.0}\r\n>>> s = pd.Series(ser)\r\n>>> s.value_counts()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/base.py"", line 55, in __repr__\r\n    return str(self)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/base.py"", line 35, in __str__\r\n    return self.__bytes__()\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/base.py"", line 47, in __bytes__\r\n    return self.__unicode__().encode(encoding, \'replace\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/series.py"", line 854, in __unicode__\r\n    result = self._tidy_repr(min(30, max_rows - 4))\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/series.py"", line 873, in _tidy_repr\r\n    head = self[:num]._get_repr(print_header=True, length=False,\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/series.py"", line 698, in __getslice__\r\n    return self._slice(slobj)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/series.py"", line 478, in _slice\r\n    slobj = self.index._convert_slice_indexer(slobj, typ=typ or \'getitem\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/index.py"", line 1811, in _convert_slice_indexer\r\n    return self.slice_indexer(key.start, key.stop, key.step)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/index.py"", line 1493, in slice_indexer\r\n    start_slice, end_slice = self.slice_locs(start, end)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/index.py"", line 1555, in slice_locs\r\n    end_slice = self.get_loc(end)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1128_ge6aaf5e-py2.7-linux-i686.egg/pandas/core/index.py"", line 1017, in get_loc\r\n    return self._engine.get_loc(_values_from_object(key))\r\n  File ""index.pyx"", line 129, in pandas.index.IndexEngine.get_loc (pandas/index.c:3548)\r\n  File ""index.pyx"", line 149, in pandas.index.IndexEngine.get_loc (pandas/index.c:3428)\r\n  File ""hashtable.pyx"", line 674, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:10784)\r\n  File ""hashtable.pyx"", line 682, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:10737)\r\nKeyError: 15\r\n>>> 15 in s\r\nTrue\r\n>>> s[15]\r\n0.0\r\n```'"
5553,22969438,TomAugspurger,jreback,2013-11-20 03:50:01,2013-11-20 16:07:05,2013-11-20 16:07:05,closed,,0.13,11,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5553,b'BUG: segfault with .loc indexing',"b""I found that segfault that messed up my HDFStore earlier.\r\n\r\nYou'll need to csv files:\r\n\r\nhttps://www.dropbox.com/s/55gcpkp3b8byfka/df1.csv\r\nhttps://www.dropbox.com/s/mqqhfgocdg4qggp/df2.csv\r\n\r\nTo reproduce:\r\n\r\n```python\r\nIn [1]: df1 = pd.read_csv('df1.csv', index_col=[0,1,2])\r\n\r\nIn [2]: df2 = pd.read_csv('df2.csv', index_col=[0,1,2])\r\n\r\nIn [3]: df2.loc[df1.index]\r\nSegmentation fault: 11\r\n(pandas-dev)dhcp80fff527:Desktop tom$ \r\n```\r\n\r\nI've never debugged a segfault before, but I gather there are some [methods](https://wiki.python.org/moin/DebuggingWithGdb).  Can anyone reproduce this?\r\n\r\nAlso, that's a valid use of `.loc` right?\r\n\r\nIn [1]: pd.__version__\r\nOut[1]: '0.12.0-1084-ged64416'\r\n\r\nIn [2]: np.__version__\r\nOut[2]: '1.7.1'\r\n"""
5546,22890616,cancan101,jreback,2013-11-19 05:55:38,2014-04-13 07:14:55,2014-04-13 07:14:55,closed,,0.14.0,0,Bug;Missing-data;Timezones,https://api.github.com/repos/pydata/pandas/issues/5546,b'tz_localize does not handle NaT',"b""```DatetimeIndex``` correctly handles ```NaT```:\r\n```\r\nIn [2]: pd.DatetimeIndex([pd.NaT])\r\nOut[2]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[NaT]\r\nLength: 1, Freq: None, Timezone: None\r\n```\r\n\r\nBut calling ```tz_localize``` leads to garbage:\r\n```\r\nIn [1]: pd.DatetimeIndex([pd.NaT]).tz_localize('US/Pacific')\r\nOut[1]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2262-04-10 00:12:43.145224192]\r\nLength: 1, Freq: None, Timezone: US/Pacific\r\n```"""
5545,22888658,hhuuggoo,jreback,2013-11-19 04:59:29,2013-11-20 14:28:49,2013-11-20 14:28:49,closed,,0.13,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/5545,"b'bug in groupby.apply?, goes away if I copy the data.'","b""Tested pandas 0.11, and 0.12, both exhibit this problem\r\n\r\n```\r\nimport pandas as pd\r\n\r\ndata = pd.DataFrame({'id_field' : [100, 100, 200, 300], 'category' : ['a','b','c','c'], 'value' : [1,2,3,4]})\r\n\r\ndef filt1(x):\r\n    if x.shape[0] == 1:\r\n        return x.copy()\r\n    else:\r\n        return x[x.category == 'c']\r\n\r\n\r\ndef filt2(x):\r\n    if x.shape[0] == 1:\r\n        return x\r\n    else:\r\n        return x[x.category == 'c']\r\n\r\n#works fine                                                                                                                                                                                                                                    \r\nprint data.groupby('id_field').apply(filt1)\r\n\r\n#throws error                                                                                                                                                                                                                                  \r\nprint data.groupby('id_field').apply(filt2)\r\n```"""
5542,22872408,dsm054,jreback,2013-11-18 22:25:07,2013-11-19 01:49:54,2013-11-19 01:49:54,closed,,0.13,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5542,b'BUG: del in Series gives strange errors ',"b'I\'m not sure whether `del someseries[something]` is supposed to work or not, although there is a `Series.__delitem__` method so maybe at some point it was.  And `.drop()` seems to work, so it\'s not super-important.  But ISTM users shouldn\'t see this:\r\n\r\n```\r\n>>> pd.__version__\r\n\'0.12.0-1114-g4d0632b\'\r\n>>> ser = pd.Series(range(5))\r\n>>> del ser[0]\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-62-e967e45427c2>"", line 1, in <module>\r\n    del ser[0]\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1114_g4d0632b-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1057, in __delitem__\r\n    self._data.delete(key)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1114_g4d0632b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2818, in delete\r\n    self.set_items_norename(new_items)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1114_g4d0632b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2923, in set_items_norename\r\n    self.set_axis(0, value, maybe_rename=False, check_axis=False)\r\nTypeError: set_axis() got an unexpected keyword argument \'maybe_rename\'\r\n\r\n>>> del ser[0]\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-63-e967e45427c2>"", line 1, in <module>\r\n    del ser[0]\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1114_g4d0632b-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1057, in __delitem__\r\n    self._data.delete(key)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1114_g4d0632b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2814, in delete\r\n    self._delete_from_all_blocks(loc, item)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1114_g4d0632b-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2945, in _delete_from_all_blocks\r\n    i, _ = self._find_block(item)\r\nTypeError: \'NoneType\' object is not iterable\r\n```\r\n\r\nThat the error message isn\'t the same both times is a little disturbing in any case, because that suggests something stateful changed, which it\'s hard to believe can be right.\r\n'"
5528,22776191,zhangruoyu,jreback,2013-11-16 11:04:18,2013-11-16 19:13:59,2013-11-16 19:13:47,closed,,0.13,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5528,b'BUG: iloc getitem with DataFrame with MultiIndex',"b""```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ntup = zip(*[['a','a','b','b'],['x','y','x','y']])\r\nindex = pd.MultiIndex.from_tuples(tup)\r\ndf = pd.DataFrame(np.random.randn(4, 4), index=index)\r\ndf.iloc[[0, 1]]\r\n```\r\nThis will raise:\r\n\r\n    ValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\r\n\r\nI am using 0.12 version."""
5527,22766407,bluefir,jreback,2013-11-15 23:38:29,2013-12-03 03:00:46,2013-12-03 03:00:46,closed,,0.13,2,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5527,b'BUG: validate multi index names in HDFStore writing',"b""    store_id_map\r\n>\\<class 'pandas.io.pytables.HDFStore'>\r\nFile path: C:\\output\\identifier_map.h5\r\n/identifier_map            frame_table  (typ->appendable_multi,nrows->26779823,ncols->9,indexers->[index],dc->[RefIdentifierID])\r\n\r\n    store_id_map.select('identifier_map')\r\n>\\---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-21-4b0da9c6ab3c> in <module>()\r\n----> 1 store_id_map.select('identifier_map')\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.pyc in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\r\n    456             return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop, chunksize=chunksize, auto_close=auto_close)\r\n    457 \r\n--> 458         return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop, auto_close=auto_close).get_values()\r\n    459 \r\n    460     def select_as_coordinates(self, key, where=None, start=None, stop=None, **kwargs):\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.pyc in get_values(self)\r\n    982 \r\n    983     def get_values(self):\r\n--> 984         results = self.func(self.start, self.stop)\r\n    985         self.close()\r\n    986         return results\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.pyc in func(_start, _stop)\r\n    449         # what we are actually going to do for a chunk\r\n    450         def func(_start, _stop):\r\n--> 451             return s.read(where=where, start=_start, stop=_stop, columns=columns, **kwargs)\r\n    452 \r\n    453         if iterator or chunksize is not None:\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.pyc in read(self, columns, **kwargs)\r\n   3259                     columns.insert(0, n)\r\n   3260         df = super(AppendableMultiFrameTable, self).read(columns=columns, **kwargs)\r\n-> 3261         df.set_index(self.levels, inplace=True)\r\n   3262         return df\r\n   3263 \r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in set_index(self, keys, drop, append, inplace, verify_integrity)\r\n   2827                 names.append(None)\r\n   2828             else:\r\n-> 2829                 level = frame[col].values\r\n   2830                 names.append(col)\r\n   2831                 if drop:\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __getitem__(self, key)\r\n   2001             # get column\r\n   2002             if self.columns.is_unique:\r\n-> 2003                 return self._get_item_cache(key)\r\n   2004 \r\n   2005             # duplicate columns\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in _get_item_cache(self, item)\r\n    665             return cache[item]\r\n    666         except Exception:\r\n--> 667             values = self._data.get(item)\r\n    668             res = self._box_item_values(item, values)\r\n    669             cache[item] = res\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in get(self, item)\r\n   1653     def get(self, item):\r\n   1654         if self.items.is_unique:\r\n-> 1655             _, block = self._find_block(item)\r\n   1656             return block.get(item)\r\n   1657         else:\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _find_block(self, item)\r\n   1933 \r\n   1934     def _find_block(self, item):\r\n-> 1935         self._check_have(item)\r\n   1936         for i, block in enumerate(self.blocks):\r\n   1937             if item in block:\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _check_have(self, item)\r\n   1940     def _check_have(self, item):\r\n   1941         if item not in self.items:\r\n-> 1942             raise KeyError('no item named %s' % com.pprint_thing(item))\r\n   1943 \r\n   1944     def reindex_axis(self, new_axis, method=None, axis=0, copy=True):\r\n>\r\n>KeyError: u'no item named None'\r\n\r\nI am at a loss. What does that mean? I successfully saved the DataFrame but I cannot read it back."""
5517,22699440,aseyboldt,TomAugspurger,2013-11-14 22:45:52,2014-08-30 14:10:48,2014-08-30 14:10:48,closed,,0.15.0,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/5517,b'BUG: boxplot does not set limits of axis',"b'`DataFrame.boxplot` fails to properly set the limits of the y-axis is this example:\r\n\r\n```python\r\ndf = pd.DataFrame({\'X\': [1, 0, 1, 0],\r\n                   \'Y\': [1, 0, 1, 0],\r\n                   \'Z\': [1, 0, 1, 0],\r\n                   #\'M\': [1, 0, 1, 0],\r\n                   \'B\': list(""XXYY"")}, \r\n                  index=list(""UUVV""))\r\n\r\nprint df\r\n""""""\r\n   B  X  Y  Z\r\nU  X  1  1  1\r\nU  X  0  0  0\r\nV  Y  1  1  1\r\nV  Y  0  0  0\r\n""""""\r\n\r\ndf.boxplot(by=\'B\')\r\n```\r\nI would expect the y-axis to go from 0 to 1, not -0.06 to 0.06.\r\nIf I uncomment the `M` it works.\r\n\r\nI use pandas 0.12.0 and matplotlib 1.3.1\r\n\r\n![Uploading boxplot.png . . .]()\r\n'"
5508,22622596,thisch,jreback,2013-11-13 21:10:28,2013-11-14 19:02:05,2013-11-14 01:06:30,closed,,0.13,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5508,b'unexpected behavior when assigning data to a multicolumn dataframe',"b""Using the latest git version of pandas (705b677) I have exerienced the\r\nfollowing problems:\r\n\r\n\r\n```python\r\nIn [1]: a = pd.DataFrame(index=pd.Index(xrange(1,11)))\r\n\r\nIn [2]: a['foo'] = np.zeros(10, dtype=np.float)\r\n\r\nIn [3]: a['bar'] = np.zeros(10, dtype=np.complex)\r\n\r\nIn [4]: a.ix[2:5, 'bar']\r\nOut[4]:\r\n2    0j\r\n3    0j\r\n4    0j\r\n5    0j\r\nName: bar, dtype: complex128\r\n\r\nIn [5]: a.ix[2:5, 'bar'] = np.array([2.33j, 1.23+0.1j, 2.2]) \r\n# invalid input (RHS has wrong size) -> does not throw an exception! \r\n# (The reason why no exception is thrown is because of the different \r\n# dtype of a['foo'] - see ``In[9]-In[10]``\r\n\r\nIn [6]: a\r\nOut[6]:\r\n    foo    bar\r\n1     0     0j\r\n2     0  2.33j\r\n3     0  2.33j\r\n4     0  2.33j\r\n5     0  2.33j\r\n6     0     0j\r\n7     0     0j\r\n8     0     0j\r\n9     0     0j\r\n10    0     0j\r\n\r\nIn [7]: a.ix[2:5, 'bar'] = np.array([2.33j, 1.23+0.1j, 2.2, 1.0]) # valid\r\n\r\nIn [8]: a\r\nOut[8]:\r\n    foo          bar\r\n1     0           0j\r\n2     0        2.33j\r\n3     0  (1.23+0.1j)\r\n4     0     (2.2+0j)\r\n5     0       (1+0j)\r\n6     0           0j\r\n7     0           0j\r\n8     0           0j\r\n9     0           0j\r\n10    0           0j\r\n\r\n\r\nIn [9]: a = pd.DataFrame(index=pd.Index(xrange(1,11)))\r\n\r\nIn [10]: a['bar'] = np.zeros(10, dtype=np.complex)\r\n\r\nIn [11]: a.ix[2:5, 'bar'] = np.array([2.33j, 1.23+0.1j, 2.2]) \r\n# invalid RHS-> exception raised  OK!\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-bde45910cde6> in <module>()\r\n----> 1 a.ix[2:5, 'bar'] = np.array([2.33j, 1.23+0.1j, 2.2])\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n     92             indexer = self._convert_to_indexer(key, is_setter=True)\r\n     93\r\n---> 94         self._setitem_with_indexer(indexer, value)\r\n     95\r\n     96     def _has_valid_type(self, k, axis):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n    387                 value = self._align_panel(indexer, value)\r\n    388\r\n--> 389             self.obj._data = self.obj._data.setitem(indexer,value)\r\n    390             self.obj._maybe_update_cacher(clear=True)\r\n    391\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in setitem(self, *args, **kwargs)\r\n   2182\r\n   2183     def setitem(self, *args, **kwargs):\r\n-> 2184         return self.apply('setitem', *args, **kwargs)\r\n   2185\r\n   2186     def putmask(self, *args, **kwargs):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in apply(self, f, *args, **kwargs)\r\n   2162\r\n   2163             else:\r\n-> 2164                 applied = getattr(blk, f)(*args, **kwargs)\r\n   2165\r\n   2166             if isinstance(applied, list):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in setitem(self, indexer, value)\r\n    580         try:\r\n    581             # set and return a block\r\n--> 582             values[indexer] = value\r\n    583\r\n    584             # coerce and try to infer the dtypes of the result\r\n\r\nValueError: could not broadcast input array from shape (3) into shape (4)\r\n```\r\n\r\n\r\nHere is the 2nd problem:\r\n\r\n```python\r\nIn [1]: b = pd.DataFrame(index=pd.Index(xrange(1,11)))\r\n\r\nIn [2]: b['foo'] = np.zeros(10, dtype=np.float)\r\n\r\nIn [3]: b['bar'] = np.zeros(10, dtype=np.complex)\r\n\r\nIn [4]: b\r\nOut[4]:\r\n    foo  bar\r\n1     0   0j\r\n2     0   0j\r\n3     0   0j\r\n4     0   0j\r\n5     0   0j\r\n6     0   0j\r\n7     0   0j\r\n8     0   0j\r\n9     0   0j\r\n10    0   0j\r\n\r\nIn [5]: b[2:5]\r\nOut[5]:\r\n   foo  bar\r\n3    0   0j\r\n4    0   0j\r\n5    0   0j\r\n\r\nIn [6]: b[2:5] = np.arange(1,4)*1j \r\n# invalid input (wrong size on RHS)\r\n\r\nIn [7]: b\r\nOut[7]:\r\n    foo  bar\r\n1    0j   0j\r\n2    0j   0j\r\n3    1j   2j\r\n4    1j   2j\r\n5    1j   2j\r\n6    0j   0j\r\n7    0j   0j\r\n8    0j   0j\r\n9    0j   0j\r\n10   0j   0j\r\n\r\n# why does the expression in ``In [6]`` change the dtype of b['foo']. \r\n# Is this intended ?\r\n\r\nIn [8]: b[2:5] = np.arange(1,4)*1j # invalid input (wrong size on RHS)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-a729140e3f7f> in <module>()\r\n----> 1 b[2:5] = np.arange(1,4)*1j # invalid input (wrong size on RHS)\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in __setitem__(self, key, value)\r\n   1831         indexer = _convert_to_index_sliceable(self, key)\r\n   1832         if indexer is not None:\r\n-> 1833             return self._setitem_slice(indexer, value)\r\n   1834\r\n   1835         if isinstance(key, (Series, np.ndarray, list)):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in _setitem_slice(self, key, value)\r\n   1842\r\n   1843     def _setitem_slice(self, key, value):\r\n-> 1844         self.ix._setitem_with_indexer(key, value)\r\n   1845\r\n   1846     def _setitem_array(self, key, value):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n    387                 value = self._align_panel(indexer, value)\r\n    388\r\n--> 389             self.obj._data = self.obj._data.setitem(indexer,value)\r\n    390             self.obj._maybe_update_cacher(clear=True)\r\n    391\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in setitem(self, *args, **kwargs)\r\n   2182\r\n   2183     def setitem(self, *args, **kwargs):\r\n-> 2184         return self.apply('setitem', *args, **kwargs)\r\n   2185\r\n   2186     def putmask(self, *args, **kwargs):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in apply(self, f, *args, **kwargs)\r\n   2162\r\n   2163             else:\r\n-> 2164                 applied = getattr(blk, f)(*args, **kwargs)\r\n   2165\r\n   2166             if isinstance(applied, list):\r\n\r\n/home/thomas/.local/lib/python2.7/site-packages/pandas-0.12.0_1098_g705b677-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in setitem(self, indexer, value)\r\n    580         try:\r\n    581             # set and return a block\r\n--> 582             values[indexer] = value\r\n    583\r\n    584             # coerce and try to infer the dtypes of the result\r\n\r\nValueError: could not broadcast input array from shape (3) into shape (3,2)\r\n```\r\n"""
5506,22611023,acowlikeobject,jreback,2013-11-13 18:24:58,2013-11-13 19:54:35,2013-11-13 19:53:52,closed,,0.13,3,Bug;Msgpack,https://api.github.com/repos/pydata/pandas/issues/5506,"b""to_msgpack() with DateTimeIndex: AttributeError: 'NoneType' object has no attribute 'freqstr'""","b""Trying df.to_msgpack() on a DataFrame with a DateTimeIndex produces an AttributeError.\r\n\r\nSetting the index to an Int64Index fixes the issue, so it seems like it has to do with DateTimeIndex. \r\n\r\npd.__version__ = '0.12.0-1095-g3239b29'\r\n\r\n\r\n\r\nIn [130]: from pandas.io.data import DataReader\r\n\r\nIn [131]: from pandas.io.packers import pack, unpack\r\n\r\nIn [132]: goog = DataReader('goog', 'yahoo')\r\n\r\nIn [133]: buf = pack(goog)\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-133-361b2be95a50> in <module>()\r\n----> 1 buf = pack(goog)\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/io/packers.py in pack(o, default, encoding, unicode_errors, use_single_float)\r\n    507     return Packer(default=default, encoding=encoding,\r\n    508                   unicode_errors=unicode_errors,\r\n--> 509                   use_single_float=use_single_float).pack(o)\r\n    510 \r\n    511 \r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/msgpack.cpython-33m.so in pandas.msgpack.Packer.pack (pandas/msgpack.cpp:3509)()\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/msgpack.cpython-33m.so in pandas.msgpack.Packer.pack (pandas/msgpack.cpp:3372)()\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/msgpack.cpython-33m.so in pandas.msgpack.Packer._pack (pandas/msgpack.cpp:3251)()\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/msgpack.cpython-33m.so in pandas.msgpack.Packer._pack (pandas/msgpack.cpp:2882)()\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/msgpack.cpython-33m.so in pandas.msgpack.Packer._pack (pandas/msgpack.cpp:3189)()\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/msgpack.cpython-33m.so in pandas.msgpack.Packer._pack (pandas/msgpack.cpp:3236)()\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/io/packers.py in encode(obj)\r\n    273                     'dtype': obj.dtype.num,\r\n    274                     'data': convert(obj.asi8),\r\n--> 275                     'freq': obj.freqstr,\r\n    276                     'tz': obj.tz}\r\n    277         elif isinstance(obj, MultiIndex):\r\n\r\n/home/user/environments/python3/lib/python3.3/site-packages/pandas/tseries/index.py in freqstr(self)\r\n   1394     @property\r\n   1395     def freqstr(self):\r\n-> 1396         return self.offset.freqstr\r\n   1397 \r\n   1398     year = _field_accessor('year', 'Y')\r\n\r\nAttributeError: 'NoneType' object has no attribute 'freqstr'\r\n\r\nIn [134]: goog\r\nOut[134]: \r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 973 entries, 2010-01-04 00:00:00 to 2013-11-12 00:00:00\r\nData columns (total 6 columns):\r\nOpen         973  non-null values\r\nHigh         973  non-null values\r\nLow          973  non-null values\r\nClose        973  non-null values\r\nVolume       973  non-null values\r\nAdj Close    973  non-null values\r\ndtypes: float64(5), int64(1)\r\n\r\nIn [135]: goog.index = np.arange(973)\r\n\r\nIn [136]: buf = pack(goog)\r\n\r\nIn [137]: len(buf)\r\nOut[137]: 54908\r\n"""
5476,22368149,kghose,kghose,2013-11-08 22:22:21,2013-12-03 10:23:04,2013-11-09 01:59:57,closed,,,6,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5476,b'BUG: NaN in DataFame column causes select with condition to always return empty',"b""```\r\nimport pandas as pd, numpy\r\n\r\nr = numpy.array([\r\n[1,2],\r\n[2,1],\r\n[1,2],\r\n[2,1]],dtype=float)\r\nr[0,1] = numpy.nan\r\n\r\ndf = pd.DataFrame(data=r,columns=['s1','s2'])\r\nprint 'Original:'\r\nprint df\r\nprint 'Selected:'\r\nprint df[df.s2==1]\r\n\r\n# As expected\r\n#   s1  s2\r\n#1   2   1\r\n#3   2   1\r\n\r\ndf.to_hdf('float_bug.h5','data',table=True,data_columns=['s1','s2'])\r\nwith pd.get_store('float_bug.h5') as store:\r\n  print 'Selected store'\r\n  print store.select('data',['s2=1'])\r\n\r\n# Unexpected\r\n# Empty DataFrame\r\n#Columns: [s1, s2]\r\n#Index: []\r\n\r\nIn [72]: pd.__version__\r\nOut[72]: '0.12.0'\r\n```\r\nYou can verify this does not happen in the column without the NaN or when there are no NaNs at all."""
5475,22365095,dsm054,jreback,2013-11-08 21:26:41,2013-11-09 01:56:27,2013-11-09 01:56:27,closed,,0.13,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5475,b'missing _is_copy causes strangeness',"b'Note the duplication of the assignment is not a copy/paste error-- the first one works, the second one fails, presumably because a different path is taken if the column already exists than if it doesn\'t.\r\n\r\n```\r\n>>> pd.__version__\r\n\'0.12.0-1081-ge684bdc\'\r\n>>> df = pd.DataFrame({""A"": [1,2]})\r\n>>> df._is_copy\r\nFalse\r\n>>> df.to_pickle(""tmp.pk"")\r\n>>> df2 = pd.read_pickle(""tmp.pk"")\r\n>>> hasattr(df2, ""_is_copy"")\r\nFalse\r\n>>> df2[""B""] = df2[""A""]\r\n>>> df2[""B""] = df2[""A""]\r\nTraceback (most recent call last):\r\n  File ""<ipython-input-155-e1fb2db534a8>"", line 1, in <module>\r\n    df2[""B""] = df2[""A""]\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1081_ge684bdc-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1841, in __setitem__\r\n    self._set_item(key, value)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1081_ge684bdc-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1907, in _set_item\r\n    self._check_setitem_copy()\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1081_ge684bdc-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1001, in _check_setitem_copy\r\n    if self._is_copy:\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_1081_ge684bdc-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1525, in __getattr__\r\n    (type(self).__name__, name))\r\nAttributeError: \'DataFrame\' object has no attribute \'_is_copy\'\r\n```\r\n'"
5468,22311494,changhiskhan,jtratner,2013-11-08 02:16:30,2013-11-08 05:01:49,2013-11-08 04:48:37,closed,,0.13,5,Bug;Windows,https://api.github.com/repos/pydata/pandas/issues/5468,b'Error in test suite for Series.mode',"b'This is on the VM running pandas windows builds.\r\nWindows XP\r\nPython 2.7 32-bit\r\nnumpy 1.7.1\r\n\r\n```\r\n======================================================================\r\nFAIL: test_mode (pandas.tests.test_series.TestSeries)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\tests\\test_series.py"", line 1727, in test_mode\r\n    assert_series_equal(Series([1, 2, 3]).mode(), Series([], dtype=int))\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 417, in assert_series_equal\r\n    assert_attr_equal(\'dtype\', left, right)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 401, in assert_attr_equal\r\n    assert_equal(left_attr,right_attr,""attr is not equal [{0}]"" .format(attr))\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 384, in assert_equal\r\n    assert a == b, ""%s: %r != %r"" % (msg.format(a,b), a, b)\r\nAssertionError: attr is not equal [dtype]: dtype(\'int64\') != dtype(\'int32\')\r\n```'"
5467,22311440,changhiskhan,jreback,2013-11-08 02:14:43,2013-11-08 18:18:25,2013-11-08 18:18:25,closed,,0.13,1,Bug;Windows,https://api.github.com/repos/pydata/pandas/issues/5467,b'Error in test suite for eval',"b'This is on the VM running pandas windows builds.\r\nWindows XP\r\nPython 2.7 32-bit\r\nnumpy 1.7.1\r\n\r\n\r\n```\r\n======================================================================\r\nFAIL: pandas.computation.tests.test_eval.TestAlignment.test_basic_series_frame_alignment(\'numexpr\', \'python\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python27\\x32\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\computation\\tests\\test_eval.py"", line 873, in check_basic_series_frame_alignment\r\n    testit(r_idx_type, c_idx_type, index_name)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\computation\\tests\\test_eval.py"", line 868, in testit\r\n    assert_frame_equal(res, expected)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 448, in assert_frame_equal\r\n    assert_index_equal(left.columns, right.columns)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 394, in assert_index_equal\r\n    right.dtype))\r\nAssertionError: [index] left [object Index([0, 2, 2000-01-03 00:00:00, 2000-01-04 00:00:00, 2000-01-05 00:00:00, 2000-01-06 00:00:00, 2000-01-07 00:00:00, 1, 2000-01-10 00:00:00, 3, 2000-01-11 00:00:00, 4], dtype=\'object\')], right [Index([2000-01-03 00:00:00, 2000-01-04 00:00:00, 2000-01-05 00:00:00, 2000-01-06 00:00:00, 2000-01-07 00:00:00, 2000-01-10 00:00:00, 2000-01-11 00:00:00, 0, 1, 2, 3, 4], dtype=\'object\') object]\r\n```'"
5466,22311312,changhiskhan,jreback,2013-11-08 02:11:07,2013-11-08 18:19:00,2013-11-08 18:18:25,closed,,0.13,1,Bug;Prio-high;Windows,https://api.github.com/repos/pydata/pandas/issues/5466,b'Error in test suite for indexing',"b'This is on the VM running pandas windows builds.\r\nWindows XP\r\nPython 2.7 32-bit\r\nnumpy 1.7.1\r\n\r\n```\r\n======================================================================\r\nERROR: test_multiindex_assignment (pandas.tests.test_indexing.TestIndexing)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\tests\\test_indexing.py"", line 1003, in test_multiindex_assignment\r\n    df.ix[name, \'new_col\'] = new_vals\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\indexing.py"", line 94, in __setitem__\r\n    self._setitem_with_indexer(indexer, value)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\indexing.py"", line 363, in _setitem_with_indexer\r\n    setter(labels[0], value)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\indexing.py"", line 318, in setter\r\n    self.obj[item] = s\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\frame.py"", line 1841, in __setitem__\r\n    self._set_item(key, value)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\frame.py"", line 1907, in _set_item\r\n    self._check_setitem_copy()\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\generic.py"", line 1006, in _check_setitem_copy\r\n    raise SettingWithCopyError(t)\r\nSettingWithCopyError: A value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_index,col_indexer] = value instead\r\n```\r\n\r\n\r\nand also:\r\n\r\n```\r\n======================================================================\r\nFAIL: test_inplace_mutation_resets_values (pandas.tests.test_index.TestMultiIndex)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\tests\\test_index.py"", line 1352, in test_inplace_mutation_resets_values\r\n    assert_almost_equal(exp_values, new_values)\r\n  File ""testing.pyx"", line 58, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:2440)\r\n  File ""testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:1692)\r\n  File ""testing.pyx"", line 93, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:1692)\r\n  File ""testing.pyx"", line 113, in pandas._testing.assert_almost_equal (pandas\\src\\testing.c:1973)\r\nAssertionError: First object is numeric, second is not: 1 != 1L\r\n```\r\n\r\nand also:\r\n\r\n```\r\n======================================================================\r\nFAIL: test_detect_chained_assignment (pandas.tests.test_indexing.TestIndexing)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\tests\\test_indexing.py"", line 1695, in test_detect_chained_assignment\r\n    assert_frame_equal(df, expected)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 469, in assert_frame_equal\r\n    check_less_precise=check_less_precise)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 417, in assert_series_equal\r\n    assert_attr_equal(\'dtype\', left, right)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 401, in assert_attr_equal\r\n    assert_equal(left_attr,right_attr,""attr is not equal [{0}]"" .format(attr))\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 384, in assert_equal\r\n    assert a == b, ""%s: %r != %r"" % (msg.format(a,b), a, b)\r\nAssertionError: attr is not equal [dtype]: dtype(\'int32\') != dtype(\'int64\')\r\n```'"
5465,22311244,changhiskhan,jreback,2013-11-08 02:09:25,2013-11-08 18:18:25,2013-11-08 18:18:25,closed,,0.13,2,Bug;Prio-high;Windows,https://api.github.com/repos/pydata/pandas/issues/5465,b'Error in test suite for timedelta operations',"b'This is on the VM running pandas windows builds.\r\nWindows XP\r\nPython 2.7 32-bit\r\nnumpy 1.7.1\r\n\r\n```\r\n======================================================================\r\nERROR: test_timedelta_ops_with_missing_values (pandas.tseries.tests.test_timedeltas.TestTimedeltas)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\tseries\\tests\\test_timedeltas.py"", line 247, in test_timedelta_ops_with_missing_values\r\n    actual = s1 + timedelta_NaT\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\ops.py"", line 461, in wrapper\r\n    time_converted = _TimeOp.maybe_convert_for_time_op(left, right, name)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\ops.py"", line 428, in maybe_convert_for_time_op\r\n    return cls(left, right, name)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\ops.py"", line 246, in __init__\r\n    rvalues = self._convert_to_array(right, name=name, other=lvalues)\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\core\\ops.py"", line 363, in _convert_to_array\r\n    "" operation"".format(pa.array(values).dtype))\r\nTypeError: incompatible type [timedelta64[ns]] for a datetime/timedelta operation\r\n```'"
5464,22311184,changhiskhan,jreback,2013-11-08 02:07:55,2013-11-08 18:19:17,2013-11-08 18:19:17,closed,,0.13,1,Bug;Prio-high;Windows,https://api.github.com/repos/pydata/pandas/issues/5464,b'Error setting locale in test suite',"b'This is on the VM running pandas windows builds.\r\nWindows XP\r\nPython 2.7 32-bit\r\n\r\n```\r\nERROR: test suite for <class \'pandas.io.tests.test_data.TestGoogle\'>\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python27\\x32\\lib\\site-packages\\nose\\suite.py"", line 208, in run\r\n    self.setUp()\r\n  File ""c:\\python27\\x32\\lib\\site-packages\\nose\\suite.py"", line 291, in setUp\r\n    self.setupContext(ancestor)\r\n  File ""c:\\python27\\x32\\lib\\site-packages\\nose\\suite.py"", line 314, in setupContext\r\n    try_run(context, names)\r\n  File ""c:\\python27\\x32\\lib\\site-packages\\nose\\util.py"", line 478, in try_run\r\n    return func()\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\io\\tests\\test_data.py"", line 41, in setUpClass\r\n    cls.locales = tm.get_locales(prefix=\'en_US\')\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 181, in get_locales\r\n    raw_locales = locale_getter()\r\n  File ""C:\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 151, in _default_locale_getter\r\n    ""system"" % e)\r\nTypeError: __init__() takes at least 3 arguments (2 given)\r\n```'"
5461,22241197,zhangruoyu,jreback,2013-11-07 02:48:08,2013-11-08 12:46:51,2013-11-08 12:46:51,closed,,,10,Bug;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/5461,"b""DatetimeIndex.normalize doesn't clear ns field""","b'    import pandas as pd\r\n    import numpy as np\r\n    didx = pd.DatetimeIndex(np.array([1380585623454345752, 1380585612343234312]).astype(""datetime64[ns]""))\r\n    didx.normalize()\r\n\r\nthe output is:\r\n\r\n    <class \'pandas.tseries.index.DatetimeIndex\'>\r\n    [2013-10-01 00:00:00.000000752, 2013-10-01 00:00:00.000000312]\r\n    Length: 2, Freq: None, Timezone: None'"
5453,22200481,willfurnass,TomAugspurger,2013-11-06 15:15:53,2014-06-12 21:07:21,2014-05-01 17:50:35,closed,,0.14.0,4,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/5453,b'Matplotlib cursor position wrong after using asfreq method to change freq of DateTimeIndex from None to something',"b""After using the `asfreq` method to change the frequency of a time-series DataFrame from `None` to something e.g. `15Min` the cursor position in matplotlib graphs of that DataFrame is no longer correct (usually shows a datetime just after the unix epoch).  The following demonstrates this (NB dt in df1 is not a constant):\r\n\r\n````\r\ndf1 = pandas.read_csv('tseries1.csv', names=['tstamp', 'Q'], parse_dates=True, \r\n    index_col='tstamp').clip_lower(0).fillna(0)\r\ndf1['T'] = pandas.read_csv('tseries2.csv', names=['tstamp', 'T'], parse_dates=True, \r\n    index_col='tstamp', squeeze=True).clip_lower(0).fillna(0)\r\n\r\ndf2 = df1.asfreq(freq='15Min', method='ffill')\r\n# NB df1.index.freq is None\r\n# NB df2.index.freq is <15 * Minutes>\r\ndf1.plot()\r\ndf2.plot()\r\nplt.show()\r\n```\r\n\r\nI find the Matplotlib cursor position to be invaluable when looking for features in very long time-series.\r\n\r\nVersions:\r\n\r\n - pandas master (commit ID 764b444)\r\n - numpy 1.8\r\n - matplotlib 1.3.0\r\n\r\n"""
5449,22194284,jreback,jreback,2013-11-06 13:34:03,2013-11-06 13:52:27,2013-11-06 13:52:27,closed,,,5,Bug;Indexing;IO Excel,https://api.github.com/repos/pydata/pandas/issues/5449,b'BUG: dup columns in to_excel when writing',"b'@jtratner , cc @jmcnamara\r\n\r\npretty sure this is fixed with xlsxwriter...maybe need a warning with other writers?\r\n\r\nhttp://stackoverflow.com/questions/19812934/pandas-dataframe-concat-and-to-excel-output'"
5443,22142844,jreback,jreback,2013-11-05 19:07:26,2014-01-23 03:55:50,2014-01-06 23:44:29,closed,,0.13.1,0,Bug;Dtypes;Timedelta;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5443,b'BUG: isnull of an object array failing when NaT',"b'```\r\nIn [1]: pd.isnull(np.array([pd.NaT]))\r\nOut[1]: array([False], dtype=bool)\r\n```\r\n\r\nthis should be true'"
5437,22093205,danielballan,jreback,2013-11-05 01:28:24,2013-11-06 23:51:20,2013-11-06 23:51:20,closed,,0.13,2,Bug;Missing-data;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5437,b'BUG: pd.to_timedelta raises instead of returning NaT',"b""    In [1]: Series(['2011-01-01', np.nan]).apply(pd.to_datetime)\r\n    Out[1]: \r\n    0   2011-01-01 00:00:00\r\n    1                   NaT\r\n    dtype: datetime64[ns]\r\n\r\n    In [2]: Series(['00:00:01', np.nan]).apply(pd.to_timedelta)\r\n    AssertionError: Invalid type for timedelta scalar: <type 'float'>\r\n\r\nMay not be a trivial fix. So far I have seen that inserting np.nan and pd.tslib.iNaT into the result turns the dtype to object, which is no good."""
5436,22089436,jreback,jreback,2013-11-04 23:52:29,2014-11-26 02:30:02,2014-11-26 02:30:02,closed,,0.15.2,0,API Design;Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5436,b'API: timedelta64[ns] elements operate with timedelta/offsets',"b""have to return timedelta64[ns] scalars\r\n\r\n```\r\ntype: timedelta64[ns]\r\n\r\nIn [23]: s = pd.to_timedelta(np.arange(5),unit='d')\r\n\r\nIn [24]: s\r\nOut[24]: \r\n0           00:00:00\r\n1   1 days, 00:00:00\r\n2   2 days, 00:00:00\r\n3   3 days, 00:00:00\r\n4   4 days, 00:00:00\r\ndtype: timedelta64[ns]\r\n\r\nIn [25]: s + timedelta(hours=1)\r\nOut[25]: \r\n0           01:00:00\r\n1   1 days, 01:00:00\r\n2   2 days, 01:00:00\r\n3   3 days, 01:00:00\r\n4   4 days, 01:00:00\r\ndtype: timedelta64[ns]\r\n\r\nIn [26]: s + pd.offsets.Hour(1)\r\nOut[26]: \r\n0           01:00:00\r\n1   1 days, 01:00:00\r\n2   2 days, 01:00:00\r\n3   3 days, 01:00:00\r\n4   4 days, 01:00:00\r\ndtype: timedelta64[ns]\r\n\r\nIn [27]: s.iloc[2] + pd.offsets.Hour(1)\r\nTypeError: ufunc add cannot use operands with types dtype('<m8[ns]') and dtype('O')\r\n\r\nIn [29]: s.iloc[2] + timedelta(hours=1)\r\nTypeError: ufunc add cannot use operands with types dtype('<m8[ns]') and dtype('O')\r\n```\r\n\r\nThis works (wrapping in a np.timedeltat64\r\n```\r\nIn [31]: s.iloc[2] += np.timedelta64(timedelta(hours=1))\r\n\r\nIn [32]: s\r\nOut[32]: \r\n0           00:00:00\r\n1   1 days, 00:00:00\r\n2   2 days, 01:00:00\r\n3   3 days, 00:00:00\r\n4   4 days, 00:00:00\r\ndtype: timedelta64[ns]\r\n```\r\n\r\nReevaluate whether summary ops should just return the np.timdelta64 scalar\r\n(not as pretty though)\r\n```\r\nIn [33]: s.mean()\r\nOut[33]: \r\n0   2 days, 00:12:00\r\ndtype: timedelta64[ns]\r\n\r\nIn [34]: s.mean().iloc[0]\r\nOut[34]: numpy.timedelta64(173520000000000,'ns')\r\n```\r\n"""
5430,22032156,kevinastone,jtratner,2013-11-04 02:07:13,2013-11-06 01:07:39,2013-11-06 00:23:40,closed,,0.13,23,Bug;Groupby;Period;Timezones,https://api.github.com/repos/pydata/pandas/issues/5430,"b""Resampling a Series with a timezone using kind='period' Crashes with ~6000 Values""","b""I wrote a test case that consistently crashes the entire process.  It looks like it requires a Series with data localized to a timezone that has a DST and the data crosses the DST boundary.  Finally, you have to use `kind='period'`for the `resample()` operation.  Oddly, it's not just the actual boundary, because I can create a smaller dataset, and it resamples fine (included in the test case with the `_works` suffix.\r\n\r\nWith that combination, the code crashes the entire process with a glibc error.\r\n\r\n### Crashing Test Case\r\nhttps://gist.github.com/kevinastone/7297033\r\n\r\n```\r\n>>> series.resample('D', how='sum', kind='period')\r\n*** glibc detected *** ${VENV_DIR}/bin/python: double free or corruption (!prev): 0x00007fd6845cbb60 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x7eb96)[0x7fd693178b96]\r\n${VENV_DIR}/local/lib/python2.7/site-packages/numpy/core/multiarray.so(+0x5f7a6)[0x7fd68f0f67a6]\r\n${VENV_DIR}/local/lib/python2.7/site-packages/numpy/core/multiarray.so(+0xc4026)[0x7fd68f15b026]\r\n${VENV_DIR}/local/lib/python2.7/site-packages/pandas/algos.so(+0x1184b2)[0x7fd689b004b2]\r\n${VENV_DIR}/local/lib/python2.7/site-packages/pandas/algos.so(+0x11933c)[0x7fd689b0133c]\r\n...\r\n```"""
5427,22031192,jmcnamara,jtratner,2013-11-04 01:11:18,2013-11-06 22:16:21,2013-11-06 22:16:21,closed,,0.13,1,API Design;Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/5427,"b'BUG: Excel writer doesn\'t handle ""cols"" option correctly'","b""This is an issue that I introduced when fixing #5235.\r\n\r\nThe `cols` option in `to_excel` no longer works correctly after the fix for the above issue.  For example:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'A': ['a', 'a', 'a'],\r\n                   'B': ['b', 'b', 'b']})\r\n\r\ndf.to_excel('frame.xlsx', sheet_name='Sheet1', cols=['B', 'A'])\r\n\r\n```\r\n\r\nGives:\r\n\r\n![screenshot](https://f.cloud.github.com/assets/94267/1462277/a4f1cd20-44ed-11e3-96be-993c49f342a6.png)\r\n\r\nNote, the headers are changed but not the column data.\r\n\r\nI have a proposed fix and test for this. Should I create a new branch/PR or merge it into the Excel MultiIndex PR: #5423\r\n"""
5424,22026674,janschulz,jreback,2013-11-03 20:58:31,2013-11-04 00:40:53,2013-11-04 00:40:53,closed,,0.13,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5424,"b'print df.ix[..,..] changes dataframe?'","b'These two codeblocks result in different dataframes:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\ncont = [\'one\', \'two\',\'three\', \'four\', \'five\', \'six\', \'seven\']\r\ndfb = pd.DataFrame({\'a\' : cont, ""b"":cont[3:]+cont[:3] ,\'c\' : np.arange(7)})\r\n#print dfb.ix[0,""c""]\r\ndfb.ix[7,\'c\'] = 1\r\nprint dfb\r\nprint dfb.ix[7,""c""]\r\nprint dfb.ix[0,""c""]\r\n\r\n       a      b  c\r\n0    one   four  0\r\n1    two   five  1\r\n2  three    six  2\r\n3   four  seven  3\r\n4   five    one  4\r\n5    six    two  5\r\n6  seven  three  6\r\n7    NaN    NaN  1\r\n1.0\r\n0.0\r\n```\r\n\r\nand \r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\ncont = [\'one\', \'two\',\'three\', \'four\', \'five\', \'six\', \'seven\']\r\ndfb = pd.DataFrame({\'a\' : cont, ""b"":cont[3:]+cont[:3] ,\'c\' : np.arange(7)})\r\nprint dfb.ix[0,""c""]\r\ndfb.ix[7,\'c\'] = 1\r\nprint dfb\r\nprint dfb.ix[7,""c""]\r\nprint dfb.ix[0,""c""]\r\n\r\n0\r\n       a      b   c\r\n0    one   four   0\r\n1    two   five   1\r\n2  three    six   2\r\n3   four  seven   3\r\n4   five    one   4\r\n5    six    two   5\r\n6  seven  three   6\r\n7    NaN    NaN NaN\r\nnan\r\n0.0\r\n```\r\npd.__version__ = \'0.12.0-922-gac1609e\'\r\n\r\nIn the first case I also don\'t understand why the last column is changed to a float when I set an int.'"
5420,22013025,jreback,jreback,2013-11-03 01:42:11,2014-06-19 00:29:52,2014-06-19 00:29:52,closed,,0.16.0,16,Bug;Docs;Indexing,https://api.github.com/repos/pydata/pandas/issues/5420,b'BUG: loc should not fallback for integer indexing for multi-index',b'https://groups.google.com/forum/m/#!topic/pydata/W0e3l0UvNwI'
5418,22011805,cancan101,jreback,2013-11-02 23:51:00,2015-07-14 15:52:23,2015-07-14 15:52:23,closed,,Next Major Release,1,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/5418,b'parse_time_string raises key error for certain frequencies',"b'I will fix as part of #5148 \r\n\r\nFor example:\r\n```\r\nparse_time_string(\'2013Q1\', freq=""INVLD-L-DEC-SAT"")\r\n```'"
5410,21987717,danielballan,jreback,2013-11-01 22:12:36,2013-11-04 21:19:02,2013-11-04 21:19:02,closed,,0.13,7,Bug;Timedelta,https://api.github.com/repos/pydata/pandas/issues/5410,"b'pd.to_timedelta(single_string) returns a Series, which ruins broadcasting.'","b""Currently\r\n\r\n    In [10]: pd.to_timedelta(Series(['00:03:37', '00:05:05'])) - pd.to_timedelta('00:03:00')\r\n    Out[10]: \r\n    0   00:00:37\r\n    1        NaT\r\n    dtype: timedelta64[ns]\r\n\r\nbecause\r\n\r\n    In [13]: pd.to_timedelta('00:03:00')\r\n    Out[13]: \r\n    0   00:03:00\r\n    dtype: timedelta64[ns]\r\n\r\nMore expected and consistent behavior, in my opinion, would be\r\n\r\n    In [12]: pd.to_timedelta(Series(['00:03:37', '00:05:05'])) - pd.to_timedelta('00:03:00')\r\n    Out[12]: \r\n    0   00:00:37\r\n    1   00:02:05\r\n    dtype: timedelta64[ns]\r\n\r\nIf, instead, ``pd.to_timedelta('00:03:00')`` gave ``numpy.timedelta64(180000000000,'ns')``, it would broadcast properly, as demonstrated by this work-around\r\n\r\n    In [12]: pd.to_timedelta(Series(['00:03:37', '00:05:05'])) - pd.to_timedelta('00:03:00')[0]\r\n    Out[12]: \r\n    0   00:00:37\r\n    1   00:02:05\r\n    dtype: timedelta64[ns]\r\n\r\nI have not regularly used timedeltas until now. Am I misjudging the expected usage?\r\n\r\n**Update**, OK, one can also get the expected result by setting ``box=False``. But ``box=False`` should be default for a scalar, no?\r\n\r\n\r\n"""
5407,21967464,jreback,jreback,2013-11-01 16:37:12,2014-05-18 16:56:27,2014-05-18 16:56:27,closed,jreback,0.14.0,0,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/5407,b'BUG: period indexing via [] for label indexingshould return partial ranges',b'http://stackoverflow.com/questions/19730653/slicing-on-a-perod-index-in-pandas-when-slice-start-and-end-may-be-out-of-bounds/19731145#19731145'
5402,21929013,TomAugspurger,jreback,2013-10-31 21:41:07,2014-01-15 16:12:41,2014-01-15 16:12:41,closed,,0.13.1,8,API Design;Bug;Indexing;Internals;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/5402,b'BUG/Not Implemented Panel.to_frame() with MultiIndex',"b'Should this be doable?\r\n\r\n```python\r\nIn [39]: df = pd.DataFrame({\'A\': [1, 2], \'B\': pd.to_datetime([\'a\', \'b\'])},\r\n                  index=pd.MultiIndex.from_tuples([(1, \'one\'), (1, \'two\')]))\r\n\r\nIn [40]: df\r\nOut[40]: \r\n       A  B\r\n1 one  1  a\r\n  two  2  b\r\n\r\nIn [41]: wp = pd.Panel({\'i1\': df, \'i2\': df})\r\n\r\nIn [42]: wp.to_frame()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-42-e49d9f2f9609> in <module>()\r\n----> 1 wp.to_frame()\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/panel.pyc in to_frame(self, filter_observations)\r\n    846         index = MultiIndex(levels=[self.major_axis, self.minor_axis],\r\n    847                            labels=[major_labels, minor_labels],\r\n--> 848                            names=[maj_name, min_name], verify_integrity=False)\r\n    849 \r\n    850         return DataFrame(data, index=index, columns=self.items)\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in __new__(cls, levels, labels, sortorder, names, copy, verify_integrity)\r\n   1880         if names is not None:\r\n   1881             # handles name validation\r\n-> 1882             subarr._set_names(names)\r\n   1883 \r\n   1884         if sortorder is not None:\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in _set_names(self, values, validate)\r\n   2150         # set the name\r\n   2151         for name, level in zip(values, self.levels):\r\n-> 2152             level.rename(name, inplace=True)\r\n   2153 \r\n   2154     names = property(\r\n\r\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in set_names(self, names, inplace)\r\n    333         """"""\r\n    334         if not com.is_list_like(names):\r\n--> 335             raise TypeError(""Must pass list-like as `names`."")\r\n    336         if inplace:\r\n    337             idx = self\r\n\r\nTypeError: Must pass list-like as `names`.\r\n```\r\n\r\nI think the issue comes when the index of the lower dimensional DataFrame (`df` in this case) is already a MultiIndex. These two work:\r\n\r\n```python\r\nIn [45]: wp.transpose(1, 0, 2).to_frame()\r\nOut[45]: \r\n              1    \r\n            one two\r\nmajor minor        \r\ni1    A       1   2\r\n      B       a   b\r\ni2    A       1   2\r\n      B       a   b\r\n\r\nIn [46]: wp.transpose(1, 2, 0).to_frame()\r\nOut[46]: \r\n              1    \r\n            one two\r\nmajor minor        \r\nA     i1      1   2\r\n      i2      1   2\r\nB     i1      a   b\r\n      i2      a   b\r\n```\r\n\r\nI was expecting that `wp.to_frame()` would create a new MultiIndex with 3 levels:\r\n\r\n```python\r\nIn [63]: df = pd.DataFrame({\'A\': [1, 2, 1, 2], \'B\': pd.to_datetime([\'a\', \'b\', \'a\', \'b\'])},\r\n                  index=pd.MultiIndex.from_tuples([(\'i1\', 1, \'one\'), (\'i1\', 1, \'two\'), (\'i2\', 1, \'one\'), (\'i2\', 1, \'two\')]))\r\n\r\nIn [64]: df\r\nOut[64]: \r\n          A  B\r\ni1 1 one  1  a\r\n     two  2  b\r\ni2 1 one  1  a\r\n     two  2  b\r\n```\r\n\r\nThe ordering of the new MultiIndex (with `wp.items` inserted) is ambiguous... But something like that. You could always swaplevels later.\r\n\r\n(side note to myself: check on if `verify_integrity` is `validate` in MultiIndex land. It doesn\'t get passed to `_set_names`).'"
5391,21862576,behzadnouri,jreback,2013-10-30 22:01:55,2014-08-21 22:11:28,2014-08-21 22:11:28,closed,,0.15.0,4,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/5391,b'left join fails in case of non-unique indices',"b""It seems to me that join operation fails if the index is not of unique values. The particular circumastance that I observed this was with multi-index:\r\n\r\n    df1.set_index( [ 'col1', 'col2', 'col3' ], inplace=True )\r\n    df2.join ( df1, on=['cola', 'colb', 'colc' ], how='left' )\r\n\r\nI understand that the above join operation is not well-defined for non-unique index values, but pandas gives wrong values even for unique matches. ( no warnings, error messages whatsoever )\r\n\r\nIn case checking for index integrity has a heavy performance cost, it should be documented that this method fails if the index is not unique. ( or alternatively have the optional argument to enforce integrity check )\r\n\r\nI could get correct join by doing below:\r\n    \r\n    df1.drop_duplicates( cols=[ 'col1', 'col2', 'col3' ], inplace=True )\r\n    df1.set_index( [ 'col1', 'col2', 'col3' ], inplace=True )\r\n    df2.join ( df1, on=['cola', 'colb', 'colc' ], how='left' )\r\n\r\n"""
5361,21688759,RomanPekar,jreback,2013-10-28 13:27:29,2014-06-07 06:13:38,2014-01-24 22:16:35,closed,,0.13.1,1,Bug;Enhancement;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5361,"b""BUG: to_datetime returns KeyError: 'p' when %p specified in format string""","b'Here\'s a question on SO - http://stackoverflow.com/questions/19635704/keyerror-in-pandas-to-datetime-using-custom-format, also possible to check it like this:\r\n\r\n```\r\npd.to_datetime(\'09/30/2013 04:14 PM\', format=""%m/%d/%Y %I:%M %p"")\r\n```\r\n\r\nIt works for python time:\r\n\r\n```\r\ntime.strptime(\'09/30/2013 04:14 PM\', ""%m/%d/%Y %I:%M %p"")\r\n```'"
5358,21667148,jtratner,jreback,2013-10-28 01:59:23,2014-04-04 18:08:48,2014-04-04 18:08:41,closed,,0.14.0,1,Bug;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/5358,"b""BUG: set_index doesn't work with MI.""","b""Changes it to tuples instead (so it's probably just converting to list or going to values somewhere)\r\n```python\r\nIn [9]: df = DataFrame(list(zip(range(3), range(3), range(3), range(3))))\r\n\r\nIn [10]: ind = MultiIndex.from_arrays([['x', 'y', 'z'], [4, 4, 5]], names=['ind1', 'ind2'])\r\n\r\nIn [11]: df.set_index(ind)\r\nOut[11]:\r\n        0  1  2  3\r\n(x, 4)  0  0  0  0\r\n(y, 4)  1  1  1  1\r\n(z, 5)  2  2  2  2\r\nIn [12]: df.reindex(ind) # works\r\nOut[12]:\r\n            0   1   2   3\r\nind1 ind2\r\nx    4    NaN NaN NaN NaN\r\ny    4    NaN NaN NaN NaN\r\nz    5    NaN NaN NaN NaN\r\nIn [13]: df2 = df.copy()\r\n\r\nIn [14]: df2.index = ind # works too\r\n\r\nIn [15]: df2\r\nOut[15]:\r\n           0  1  2  3\r\nind1 ind2\r\nx    4     0  0  0  0\r\ny    4     1  1  1  1\r\nz    5     2  2  2  2\r\n```"""
5346,21652562,y-p,takluyver,2013-10-27 13:27:20,2013-11-26 01:04:01,2013-11-26 01:04:01,closed,,0.13.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/5346,b'to_clipboard() should ignore display options',"b""When the frame is too wide or too long, summary/expand_repr kick in\r\nand the contents of the clipboard are unusable.\r\n\r\nCould perhaps introduce an option to enforce a limit on row count (10,000? 100,000? )\r\nand/or add a `force` keyword in `to_clipboard()` since copying a huge amount into the \r\nclipboard can hang.\r\n\r\nnow that it uses to_csv by default (#5070 ), it's only relevent with `excel=false`\r\n"""
5344,21644077,jreback,jreback,2013-10-27 00:29:56,2013-10-27 00:45:07,2013-10-27 00:45:07,closed,,0.13,0,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/5344,b'BUG: set_index not resetting ref_locs',b'http://stackoverflow.com/questions/19612650/pandas-multiindex-names-not-working'
5338,21632015,janschulz,cpcloud,2013-10-26 11:04:26,2015-11-20 06:44:33,2014-02-21 12:00:51,closed,cpcloud,0.14.0,24,API Design;Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/5338,b'DataFrame.replace(dict) has weird behaviour in some cases',"b'```python\r\nimport pandas as pd\r\ndf = pd.DataFrame({""color"":[1,2,3,4]})\r\nprint df\r\n   color\r\n0      1\r\n1      2\r\n2      3\r\n3      4\r\nprint df.replace({""color"":{""1"":""2"",""3"":""4"",}}) # works but shouldn\'t?\r\n  color\r\n0     2\r\n1     2\r\n2     4\r\n3     4\r\nprint df.replace({""color"":{""1"":""2"",""2"":""3"",""3"":""4"",""4"":""5""}}) # strange\r\n  color\r\n0     2\r\n1     4\r\n2     3\r\n3     5\r\nprint df.replace({""color"":{1:""2"",2:""3"",3:""4"",4:""5""}}) # works by replacing each cell once\r\n  color\r\n0     2\r\n1     3\r\n2     4\r\n3     5\r\n\r\ndf = pd.DataFrame({""color"":[""1"",""2"",""3"",""4""]})\r\nprint df\r\n  color\r\n0     1\r\n1     2\r\n2     3\r\n3     4\r\nprint df.replace({""color"":{""1"":""2"",""3"":""4"",}}) # works\r\n  color\r\n0     2\r\n1     2\r\n2     4\r\n3     4\r\nprint df.replace({""color"":{""1"":""2"",""2"":""3"",""3"":""4"",""4"":""5""}}) # works not\r\n  color\r\n0     3\r\n1     3\r\n2     5\r\n3     5\r\nprint df.replace({""color"":{1:""2"",2:""3"",3:""4"",4:""5""}}) # works as expected: shouldn\'t replace anything!\r\n  color\r\n0     1\r\n1     2\r\n2     3\r\n3     4\r\n```\r\n\r\nSo, my expected behaviour would be:\r\n* don\'t replace a cell if the type of the cell does not match the key (as it is the case when a string cell is replaced by a int key)\r\n* if a value of a cell is replaced, the cell shouldn\'t be replaced a second time in the same replace call\r\n\r\nI found the problem when I tried to match string values to colors and got blown up color values: like `{""3"":""#123456"",""4"":""#000000""}` wouldn\'t convert `""3""` into `""#123#00000056""`\r\n\r\n*Edit*: insert string cell cases and my expected behaviour and deleted the intial comments which had these examples\r\n'"
5329,21611650,michaelaye,jreback,2013-10-25 19:38:33,2014-04-09 13:53:48,2014-04-09 13:53:48,closed,,0.14.0,14,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/5329,b'Possible memory leak with HDFStores and dataframes?',"b""I am running this loop over approx  48 large HDFstores, each with approx 17 million rows. After roughly half the list done, it crashes with a memory error on my 96 GB RAM machine running CentOS6 64-bit.\r\n\r\n```python\r\nfor fname in fnames:\r\n    print fname\r\n    df = pd.read_hdf(fname, 'df', columns=[colname])\r\n    df['mybool'] = df[colname] == 90\r\n    df['label'] = label(df.mybool)[0]\r\n    df['time'] = df.index\r\n    g = df.groupby('label')['time']\r\n    pd.DataFrame({'start':g.first(),\r\n                  'duration':g.last() - g.first()})[1:].to_hdf('./earth_scans.h5',\r\n                                                           'df',\r\n                                                           mode='a',\r\n                                                           format='table',\r\n                                                           append=True)\r\n```\r\nThe error is:\r\n```python\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\n<ipython-input-11-6c43f9f5c6be> in <module>()\r\n      3 for fname in fnames:\r\n      4     print fname\r\n----> 5     df = pd.read_hdf(fname, 'df', columns=[colname])\r\n      6     df['mybool'] = df[colname] == 90\r\n      7     df['label'] = label(df.mybool)[0]\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/io/pytables.pyc in read_hdf(path_or_buf, key, **kwargs)\r\n    308         store = HDFStore(path_or_buf, **kwargs)\r\n    309         try:\r\n--> 310             return f(store, True)\r\n    311         except:\r\n    312 \r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/io/pytables.pyc in <lambda>(store, auto_close)\r\n    300 \r\n    301     f = lambda store, auto_close: store.select(\r\n--> 302         key, auto_close=auto_close, **kwargs)\r\n    303 \r\n    304     if isinstance(path_or_buf, compat.string_types):\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/io/pytables.pyc in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\r\n    595 \r\n    596         return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop,\r\n--> 597                              auto_close=auto_close).get_values()\r\n    598 \r\n    599     def select_as_coordinates(\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/io/pytables.pyc in get_values(self)\r\n   1225 \r\n   1226     def get_values(self):\r\n-> 1227         results = self.func(self.start, self.stop)\r\n   1228         self.close()\r\n   1229         return results\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/io/pytables.pyc in func(_start, _stop)\r\n    584         def func(_start, _stop):\r\n    585             return s.read(where=where, start=_start, stop=_stop,\r\n--> 586                           columns=columns, **kwargs)\r\n    587 \r\n    588         if iterator or chunksize is not None:\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/io/pytables.pyc in read(self, where, columns, **kwargs)\r\n   3611             df = frames[0]\r\n   3612         else:\r\n-> 3613             df = concat(frames, axis=1, verify_integrity=False).consolidate()\r\n   3614 \r\n   3615         # apply the selection filters & axis orderings\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in consolidate(self, inplace)\r\n   1505             cons_data = self._protect_consolidate(f)\r\n   1506             if cons_data is self._data:\r\n-> 1507                 cons_data = cons_data.copy()\r\n   1508             return self._constructor(cons_data).__finalize__(self)\r\n   1509 \r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in copy(self, deep)\r\n   2458             new_axes = list(self.axes)\r\n   2459         return self.apply('copy', axes=new_axes, deep=deep,\r\n-> 2460                         ref_items=new_axes[0], do_integrity_check=False)\r\n   2461 \r\n   2462     def as_matrix(self, items=None):\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in apply(self, f, *args, **kwargs)\r\n   2142                 applied = f(blk, *args, **kwargs)\r\n   2143             else:\r\n-> 2144                 applied = getattr(blk, f)(*args, **kwargs)\r\n   2145 \r\n   2146             if isinstance(applied, list):\r\n\r\n/usr/local/epd/lib/python2.7/site-packages/pandas-0.12.0_963_g383010f-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in copy(self, deep, ref_items)\r\n    522         values = self.values\r\n    523         if deep:\r\n--> 524             values = values.copy()\r\n    525         if ref_items is None:\r\n    526             ref_items = self.ref_items\r\n\r\nMemoryError: \r\n```\r\n\r\nThis loop is running inside an iPython notebook via a remote notebook server.\r\n\r\nTo my understanding, the automatic garbage collection should get rid of the intermediate dataframes I am creating inside the loop, am I doing anything stupid or is this leaking somehow?"""
5324,21599207,jreback,jreback,2013-10-25 15:56:32,2013-10-25 19:24:09,2013-10-25 19:24:09,closed,,0.13,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/5324,b'BUG: embedded pandas object cause formatting exceptions',b'http://stackoverflow.com/questions/19594003/should-it-be-supported-to-store-a-pandas-dataframe-in-another-pandas-dataframe'
5292,21344163,adamgreenhall,jreback,2013-10-21 21:04:06,2014-09-10 17:46:09,2014-09-04 22:18:50,closed,,0.15.0,32,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/5292,b'BUG: DateOffset weekday around DST produces unexpected results. fixes #5175',"b""closes #5175\r\n\r\nWound up working on this in preparation for the DST change next week. Here's a first pass at a solution and a test."""
5267,21266946,jreback,jreback,2013-10-19 18:51:04,2014-04-21 12:22:23,2014-04-11 22:33:18,closed,,0.14.0,0,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5267,b'BUG: groupby get_group should be more datelike friendly',"b""http://stackoverflow.com/questions/19458361/python-pandas-groupby-date-and-accessing-each-group-by-timestamp\r\n\r\n```\r\nIn [2]: df= pd.DataFrame({'DATE' : ['10-Oct-2013', '10-Oct-2013', '10-Oct-2013', '11-Oct-2013', '11-Oct-2013', '11-Oct-2013'],'VAL' : [1,2,3,4,5,6]})\r\n\r\nIn [3]: df['DATE'] = pd.to_datetime(df['DATE'])\r\n\r\nIn [4]: df\r\nOut[4]: \r\n                 DATE  VAL\r\n0 2013-10-10 00:00:00    1\r\n1 2013-10-10 00:00:00    2\r\n2 2013-10-10 00:00:00    3\r\n3 2013-10-11 00:00:00    4\r\n4 2013-10-11 00:00:00    5\r\n5 2013-10-11 00:00:00    6\r\n\r\nIn [5]: g = df.groupby('DATE')\r\n\r\nIn [8]: key = g.groups.keys()[0]\r\n\r\nIn [9]: key\r\nOut[9]: numpy.datetime64('2013-10-09T20:00:00.000000000-0400')\r\n\r\nIn [10]: g.indices\r\nOut[10]: \r\n{1381363200000000000L: array([0, 1, 2]),\r\n 1381449600000000000L: array([3, 4, 5])}\r\n\r\nIn [11]: g.get_group(key.astype('i8'))\r\nOut[11]: \r\n                 DATE  VAL\r\n0 2013-10-10 00:00:00    1\r\n1 2013-10-10 00:00:00    2\r\n2 2013-10-10 00:00:00    3\r\n```\r\n\r\nso works, but not very friendly\r\nin addition on windows, the following is needed for the selection to succeed\r\n\r\n``g.get_group(long(key.astype('i8')))``\r\n\r\n\r\n"""
5266,21265007,cpcloud,cpcloud,2013-10-19 16:49:01,2014-06-15 16:56:32,2014-06-03 13:36:59,closed,cpcloud,0.14.1,63,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/5266,b'BUG: union should not try to sort inplace because platform impls differ as to when sorting occurs for objects that cannot be compared',b'closes #5039'
5264,21263178,TomAugspurger,TomAugspurger,2013-10-19 14:59:21,2014-04-11 14:57:13,2014-04-11 14:57:13,closed,,0.14.0,30,API Design;Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/5264,b'BUG: groupby sub-selection ignored with some methods',"b'related #6512, #6524, #6346\r\n\r\n*Update from @hayd I think these should reference `_selected_obj` rather than `obj`.*\r\n\r\nLooking through some others, looks these also ignore the selection\r\n\r\n- [x] count #6570\r\n- [x] corr #6570\r\n- [x] cummax #6570\r\n- [x] cummin #6570 \r\n- [x] cumsum #6570 \r\n- [x] cumprod  #6570 \r\n- [x] describe #6570\r\n- [x] fillna #6570\r\n- [x] quantile #6570\r\n- [x] head #6533\r\n- [x] hist? the output is ok but the plots have all\r\n- [ ] ohlc? possibly fixed with #6570 (resample with ohlc is tested), should this method exist? see #6594\r\n- [x] plot\r\n- [x] rank #6570\r\n- [x] tail #6533 \r\n- [x] filter, #6570 (tested in #6593)\r\n- [x] resample, #6570\r\n- [x] nth #6569\r\n- [x] diff/shift, #6570\r\n- [x] all/any #6570\r\n- [x] ffill, #6570\r\n- [x] pct_change  #6570\r\n- [x] idxmin/idxmax, #6570\r\n- [x] dtypes #6570 \r\n- [ ] apply  #6570 (could be tested more / different paths?)\r\n\r\nAggregation functions like (they already *kind* of do, but they allow bad selections ie column names not in columns, may be sep issue?):\r\n- [x] sum/max/min/median/mean/var/std/.. (not tested)\r\n- [x] agg (not tested)\r\n(these ""work"" with the described bug)\r\n\r\nAtm selecting a column not in df doesn\'t raise:\r\n\r\n- [ ]  it should raise a Key Error, #6578\r\n\r\nwhat about ``iloc/loc/ix`` (current all disabled)?\r\n- [x] iloc (very similar to head/tail)\r\n- [ ] loc/ix (maybe push off for now, this is pretty tricky)\r\n\r\n- [x] iterate over all (whitelisted) functions to check they adhere to this\r\n\r\nThe column selection on a groupby object is being ignored when `.quantile()` is called. So it computes the quantile on all the (numeric) columns and returns the full DataFrame.\r\n\r\n```python\r\nIn [92]: t = pd.DataFrame(np.random.randn(10, 4)); t[0] = np.hstack([np.ones(5), np.zeros(5)])\r\n\r\nIn [95]: t.groupby(0)[[1, 2]].quantile()  # shows other cols\r\nOut[95]: \r\n   0         1         2         3\r\n0                                 \r\n0  0  0.127152  0.108908  0.369601\r\n1  1 -0.321279  0.265550 -0.382398\r\n\r\nIn [96]: t[[1, 2]].groupby(t[0]).quantile()  # Should be equivalent to:\r\nOut[96]: \r\n          1         2\r\n0                    \r\n0  0.127152  0.108908\r\n1 -0.321279  0.265550\r\n```\r\n\r\nSeeing all these, I\'m wondering if this is a bug or just how some of the methods are implementer. The [docs](http://pandas.pydata.org/pandas-docs/dev/groupby.html#dataframe-column-selection-in-groupby) don\'t mention anything about only supporting some methods though.\r\n\r\nversion: \'0.12.0-883-g988d4be\''"
5248,21142694,ruidc,jreback,2013-10-17 09:54:14,2013-10-17 13:08:34,2013-10-17 11:59:24,closed,,0.13,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5248,b'Series.drop regression and unexpected behaviour',"b'On 0.12 stable this works:\r\n\r\n```\r\n>>> s = pandas.Series([1,1,2],index=[""one"",""one"",""two""])\r\n>>> s.drop([""one""])\r\ntwo    2\r\ndtype: int64\r\n```\r\n\r\nbut a singleton silently fails:\r\n\r\n```\r\n>>> s.drop(""one"")\r\none    1\r\none    1\r\ntwo    2\r\ndtype: int64\r\n```\r\n\r\nexcept when index is unique:\r\n\r\n```\r\n>>> s = pandas.Series([1,2],index=[""one"",""two""])\r\n>>> s.drop(""one"")\r\ntwo    2\r\ndtype: int64\r\n```\r\naccording to the docs, the input to drop should be array like, so i\'m not sure this should be permissible.\r\n\r\nOn head, with unique index, it matches, but with non-unique:\r\n```\r\n>>> pandas.__version__\r\n\'0.12.0-892-g6830600\'\r\n>>> s = pandas.Series([1,1,2],index=[""one"",""one"",""two""])\r\n>>> s2 = s.drop([""one""])\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/core/generic.py"", line 1133, in drop\r\n    return self.ix[tuple(slicer)]\r\n  File ""pandas/core/indexing.py"", line 53, in __getitem__\r\n    return self._getitem_tuple(key)\r\n  File ""pandas/core/indexing.py"", line 558, in _getitem_tuple\r\n    return self._multi_take(tup)\r\n  File ""pandas/core/indexing.py"", line 598, in _multi_take\r\n    raise self._exception\r\nKeyError\r\n```\r\n'"
5235,21048558,jmcnamara,jreback,2013-10-15 22:19:45,2013-11-04 01:38:29,2013-10-16 18:50:39,closed,,0.13,7,Bug;IO Excel,https://api.github.com/repos/pydata/pandas/issues/5235,b'Issue with Excel writers when column names are duplicated',"b""There appears to be an issue with Excel writers when DataFrame column names are duplicated. This issue that was initially reported on [StackOverflow](http://stackoverflow.com/questions/19363779/problems-writing-to-file-in-pandas).\r\n\r\nFor example consider the following program: \r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas import DataFrame\r\n\r\ndf = DataFrame([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\r\n\r\ndf.columns = ['A', 'B', 'B']  # !!!\r\n\r\ndf.to_csv('output.csv')\r\ndf.to_excel('output.xlsx')\r\n\r\n```\r\nNote the duplicated column name. The `df` for this looks like this:\r\n\r\n````python\r\n>>> df\r\n   A  B  B\r\n0  1  2  3\r\n1  1  2  3\r\n2  1  2  3\r\n````\r\n\r\nThe corresponding output of the CSV is as expected:\r\n\r\n```\r\n$ cat output.csv\r\n,A,B,B\r\n0,1,2,3\r\n1,1,2,3\r\n2,1,2,3\r\n```\r\n\r\nHowever, the output of the any of the Excel writers is incorrect:\r\n\r\n![screenshot](https://f.cloud.github.com/assets/94267/1338347/b42803a2-35e5-11e3-912e-4923d3382f3d.png)\r\n\r\nThe issue appears to be in `pandas/core/format.py`. The output data is gathered based on column names, as shown below, which causes issues with duplicate names.\r\n\r\n```python\r\n    def _format_regular_rows(self):\r\n        ...\r\n        for colidx, colname in enumerate(self.columns):\r\n            series = self.df[colname]\r\n            ... \r\n```\r\n\r\nI initially thought that this might be the correct behaviour and that column names shouldn't be duplicated but given that the output is different to the csv writer it looks like a bug.\r\n\r\nI'll write a test case but I'm not sure of the best way to fix the issue."""
5231,21030591,unutbu,jreback,2013-10-15 17:15:17,2014-09-10 00:22:49,2014-03-27 22:23:50,closed,,0.14.0,71,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/5231,b'EHN/FIX: Add na_last parameter to DataFrame.sort. Fixes GH3917',"b""closes  #3917\r\n\r\nThis is an attempt to fix the Nested Sort with NaN bug (https://github.com/pydata/pandas/issues/3917).\r\n\r\nI've added tests to `test_frame.py` and `test_hashtable.py` to demonstrate the problem.\r\n\r\n`hashtable.Factorizer.factorize` has been modified to map `nan` to `na_sentinel`. Before it was mapping `nan` to a label which was already being used. This, I believe is the origin of the bug. \r\n\r\nMy first idea was to mimick/reuse code from `Series.order`, since this method already handles `nan`s nicely, and allows the user to choose if nans should be placed at the beginning or the end of the sort via the `na_last` parameter. Although I found a solution using code from `Series.order`, I eventually abandoned this when I realized this patches the problem at too high a level and that it could be handled more generally with a modification of `factorize`. I retained the idea that `df.sort` should have a `na_last` parameter, however.\r\n\r\nTo that end, `groupby._lexsort_indexer` has been modified to handle all possible combinations of `na_last` and `orders` settings. There are four tests (assertions) in `test_frame.py` to exercise the possibilities, one of which demonstrates that `df.sort(['A','B'])` now behaves correctly for the DataFrame shown in GH3917."""
5226,20992677,danielballan,jreback,2013-10-15 01:54:31,2013-10-23 14:51:25,2013-10-15 13:11:40,closed,,0.13,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5226,"b""New appending behavior doesn't work on an empty DataFrame""","b'Consider this working example from the docs:\r\n\r\n    In [9]: df1 = DataFrame(np.arange(6).reshape(3,2),\r\n       ...:                 columns=[\'A\',\'B\'])\r\n\r\n    In [10]: df1.loc[3] = [6, 7]\r\n\r\n    In [11]: df1\r\n    Out[11]: \r\n       A  B\r\n    0  0  1\r\n    1  2  3\r\n    2  4  5\r\n    3  6  7\r\n\r\nAnd now watch what happens when the DataFrame is empty:\r\n\r\n    In [12]: df2 = DataFrame(columns=[\'A\',\'B\'])\r\n\r\n    In [13]: df2.loc[3] = [6, 7]\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-13-53d84383e8d4> in <module>()\r\n    ----> 1 df2.loc[3] = [6, 7]\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n         90             indexer = self._convert_tuple(key, is_setter=True)\r\n         91         else:\r\n    ---> 92             indexer = self._convert_to_indexer(key, is_setter=True)\r\n         93 \r\n         94         self._setitem_with_indexer(indexer, value)\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/indexing.pyc in _convert_to_indexer(self, obj, axis, is_setter)\r\n        821             if is_setter:\r\n        822                 if obj >= len(self.obj) and not isinstance(labels, MultiIndex):\r\n    --> 823                     raise ValueError(""cannot set by positional indexing with enlargement"")\r\n        824 \r\n        825             return obj\r\n\r\n    ValueError: cannot set by positional indexing with enlargement\r\n\r\nI haven\'t read the discussion that generated this new behavior of loc/ix. Perhaps there is a good reason why this can\'t work. But I can\'t think of one.\r\n\r\n(This was prompted by [this SO question](http://stackoverflow.com/questions/19365513/how-to-add-an-extra-row-to-a-pandas-dataframe/19368360?noredirect=1#comment28705229_19368360).)'"
5216,20957036,glyg,jreback,2013-10-14 13:40:58,2013-12-12 12:44:16,2013-10-14 15:20:35,closed,,0.13,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5216,b'BUG: AttributeError when trying to assign a value to an element of a MultiIndex DataFrame',"b""Code snipet producing the (unexpected) ValueError (versions of pandas and numpy printed after the traceback)\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nprint('pandas : %s' % pd.__version__)\r\nprint('numpy : %s' % np.__version__)\r\na = np.random.rand(10, 3)\r\ndf = pd.DataFrame(a, columns=['x', 'y', 'z'])\r\ntuples = [(i, j) for i in range(5) for j in range(2)]\r\nindex = pd.MultiIndex.from_tuples(tuples)\r\ndf.index = index\r\ndf.loc[0]['z'].iloc[0] = 1.\r\n```\r\n\r\noutput:\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-8152e4c5a53b> in <module>()\r\n      8 index = pd.MultiIndex.from_tuples(tuples)\r\n      9 df.index = index\r\n---> 10 df.loc[0]['z'].iloc[0] = 1.\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_852_gb76b265-py2.7-linux-i686.egg/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n     92             indexer = self._convert_to_indexer(key, is_setter=True)\r\n     93 \r\n---> 94         self._setitem_with_indexer(indexer, value)\r\n     95 \r\n     96     def _has_valid_type(self, k, axis):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_852_gb76b265-py2.7-linux-i686.egg/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n    364 \r\n    365             self.obj._data = self.obj._data.setitem(indexer,value)\r\n--> 366             self.obj._maybe_update_cacher(clear=True)\r\n    367 \r\n    368     def _align_series(self, indexer, ser):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.12.0_852_gb76b265-py2.7-linux-i686.egg/pandas/core/generic.pyc in _maybe_update_cacher(self, clear)\r\n    944         cacher = getattr(self,'_cacher',None)\r\n    945         if cacher is not None:\r\n--> 946             cacher[1]()._maybe_cache_changed(cacher[0],self)\r\n    947         if clear:\r\n    948             self._clear_item_cache()\r\n\r\nAttributeError: 'NoneType' object has no attribute '_maybe_cache_changed'\r\n\r\npandas : 0.12.0-852-gb76b265\r\nnumpy : 1.8.0.dev-d310678\r\n```"""
5203,20919296,jreback,jreback,2013-10-12 23:49:29,2013-10-15 13:28:58,2013-10-15 13:28:58,closed,,0.13.1,18,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/5203,b'BUG: period on weekends rolling to odd business days',"b""For biz days, seems saturday is rolling to monday, but sunday to friday\r\nlooks like a bug (though it might just be ambiguous/undefined behavior).\r\n```\r\nIn [11]: pd.Period('2013-10-4', freq='B')\r\nOut[11]: Period('2013-10-04', 'B')\r\n\r\nIn [12]: pd.Period('2013-10-5', freq='B')\r\nOut[12]: Period('2013-10-07', 'B')\r\n\r\nIn [13]: pd.Period('2013-10-6', freq='B')\r\nOut[13]: Period('2013-10-04', 'B')\r\n\r\nIn [14]: pd.Period('2013-10-7', freq='B')\r\nOut[14]: Period('2013-10-07', 'B')\r\n```"""
5198,20918345,TomAugspurger,cpcloud,2013-10-12 22:20:12,2014-03-17 14:18:12,2014-03-17 14:18:12,closed,cpcloud,0.14.0,1,Bug;Experimental,https://api.github.com/repos/pydata/pandas/issues/5198,b'BUG: recursion limit on eval when operating on different types',"b'Me being dumb discovered this:\r\n\r\n```python\r\nIn [209]: bug = pd.DataFrame({""A"": [1, 2], \'B\': [\'c\', \'d\']})\r\n\r\nIn [210]: bug\r\nOut[210]: \r\n   A  B\r\n0  1  c\r\n1  2  d\r\n\r\nIn [211]: bug.eval(\'A + B\')\r\n\r\n# stack trace truncated ...\r\n\r\nRuntimeError: maximum recursion depth exceeded\r\n```\r\n\r\ncc @cpcloud '"
5196,20918216,jtratner,sinhrks,2013-10-12 22:10:46,2015-09-11 14:28:24,2015-09-11 14:28:23,closed,jtratner,0.17.0,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5196,b'BUG/ENH: Passing explicit dtype should delegate to appropriate Index subclass (if available)',"b""e.g. here's wrong behavior on current master\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\nIn [2]: ind = pd.Index([1, 2, 3], dtype='int64')\r\nIn [3]: ind\r\nOut[3]: Index([1, 2, 3], dtype='object')\r\n```\r\n\r\nProblem is ordering of how passed dtypes work in Index constructor.  Make sure this gets fixed in 0.14."""
5195,20917787,vpatel34,jreback,2013-10-12 21:39:54,2013-10-12 23:53:46,2013-10-12 23:53:46,closed,,0.13,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/5195,b'pd.to_datetime ValueError or TypeError',"b""Hi,\r\n\r\nI am using pandas 0.12.0 and numpy 1.7.1.\r\n\r\nNot sure if this is the expected functionality or not.  I thought if coerce = True or errors='ignore' I wouldn't get this result:\r\n\r\n```python\r\nIn [372]: td = pd.Series(['May 04', 'Jun 02', 'Dec 11'], index=[1,2,3])\r\n\r\nIn [373]: td.apply(pd.to_datetime, format='%b %y', errors='ignore', coerce=True)\r\nOut[373]:\r\n1   2004-05-01 00:00:00\r\n2   2002-06-01 00:00:00\r\n3   2011-12-01 00:00:00\r\ndtype: datetime64[ns]\r\n\r\nIn [374]: td = pd.Series(['May 04', 'Jun 02', ''], index=[1,2,3])\r\n\r\nIn [375]: td.apply(pd.to_datetime, format='%b %y', errors='ignore', coerce=True)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-375-0c44a3c36f8e> in <module>()\r\n----> 1 td.apply(pd.to_datetime, format='%b %y', errors='ignore', coerce=True)\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in apply(self, func, convert_dtype, args, **kwds)\r\n   2534             values = lib.map_infer(values, lib.Timestamp)\r\n   2535\r\n-> 2536         mapped = lib.map_infer(values, f, convert=convert_dtype)\r\n   2537         if isinstance(mapped[0], Series):\r\n   2538             from pandas.core.frame import DataFrame\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.map_infer (pandas\\lib.c:42320)()\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in <lambda>(x)\r\n   2523\r\n   2524         if kwds or args and not isinstance(func, np.ufunc):\r\n-> 2525             f = lambda x: func(x, *args, **kwds)\r\n   2526         else:\r\n   2527             f = func\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\tseries\\tools.pyc in to_datetime(arg, errors, dayfirst, utc, box, format, coerce, u\r\nnit)\r\n    127         return _convert_listlike(arg, box=box)\r\n    128\r\n--> 129     return _convert_listlike(np.array([ arg ]), box=box)[0]\r\n    130\r\n    131 class DateParseError(ValueError):\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\tseries\\tools.pyc in _convert_listlike(arg, box)\r\n    115                 return DatetimeIndex._simple_new(values, None, tz=tz)\r\n    116             except (ValueError, TypeError):\r\n--> 117                 raise e\r\n    118\r\n    119     if arg is None:\r\n\r\nValueError: time data '' does not match format '%b %y'\r\n\r\nIn [376]:\r\n\r\n```\r\n"""
5191,20913448,agravier,jreback,2013-10-12 16:37:13,2013-10-12 17:45:28,2013-10-12 17:45:28,closed,,0.13,3,Bug;Error Reporting,https://api.github.com/repos/pydata/pandas/issues/5191,b'Segmentation fault when conctructing DataFrame with specified datetime dtype of one column',"b'## Description\r\n\r\nWhen building a DataFrame with specified column names and dtypes, one might expect one of two possible behaviours:\r\n\r\n* The column names and dtypes specs are perfectly cromulent, and Pandas goes on to build the object.\r\n* The column names or dtypes don\'t match the data shape, or the dtypes are badly specified, and Pandas gives an error message.\r\n\r\nInstead, I have encountered a segmentation fault.\r\n\r\nNow, it is unclear to me whether my column names spec and dtypes are correctly written and if my data is proper too (see example below). But in any case, it should not crash.\r\n\r\n## Reproducing\r\n\r\nTo reproduce, please run:\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime as dt\r\nimport itertools as it\r\n\r\ndf_test = pd.DataFrame(data = list(it.repeat((dt.datetime(2001, 1, 1), ""aa"", 20), 9)),\r\n                       columns=[""A"", ""B"", ""C""],\r\n                       dtype=[(""A"",""datetime64[h]""), (""B"",""str""), (""C"",""int32"")])\r\n```\r\n\r\n## Modes of failure\r\n\r\nI have found that the above script always crashes on my machine (see next section for detailed configuration information). It does it in 2 possible ways:\r\n\r\n### First mode of failure: hanging\r\n\r\n```\r\nPython 2.7.5 (default, Sep  6 2013, 09:55:21) \r\n[GCC 4.8.1 20130725 (prerelease)] on linux2\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas as pd\r\n>>> import datetime as dt\r\n>>> import itertools as it\r\n>>> \r\n>>> df_test = pd.DataFrame(data = list(it.repeat((dt.datetime(2001, 1, 1), ""aa"", 20), 9)),\r\n...                        columns=[""A"", ""B"", ""C""],\r\n...                        dtype=[(""A"",""datetime64[h]""), (""B"",""str""), (""C"",""int32"")])\r\n*** Error in `python\': corrupted double-linked list: 0x0000000001bfd8e0 ***\r\n```\r\nAfter that line, the terminal is dead. \r\n\r\n### Second mode of failure: segfault\r\n\r\n```\r\nPython 2.7.5 (default, Sep  6 2013, 09:55:21)\r\n[GCC 4.8.1 20130725 (prerelease)] on linux2\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas as pd\r\n>>> import datetime as dt\r\n>>> import itertools as it\r\n>>>\r\n>>> df_test = pd.DataFrame(data = list(it.repeat((dt.datetime(2001, 1, 1), ""aa"", 20), 9)),\r\n...                        columns=[""A"", ""B"", ""C""],\r\n...                        dtype=[(""A"",""datetime64[h]""), (""B"",""str""), (""C"",""int32"")])\r\n*** Error in `python2\': double free or corruption (!prev): 0x00000000027161d0 ***\r\n======= Backtrace: =========\r\n/usr/lib/libc.so.6(+0x72ecf)[0x7f2bd7ab9ecf]\r\n/usr/lib/libc.so.6(+0x7869e)[0x7f2bd7abf69e]\r\n/usr/lib/libc.so.6(+0x79377)[0x7f2bd7ac0377]\r\n/home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/numpy/core/multiarray.so(_field_transfer_data_free+0x2e)[0x7f2bd634d47e]\r\n/home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/numpy/core/multiarray.so(+0x9a1c9)[0x7f2bd63a61c9]\r\n/home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/numpy/core/multiarray.so(+0xa4a3a)[0x7f2bd63b0a3a]\r\n/home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/numpy/core/multiarray.so(+0xab0a1)[0x7f2bd63b70a1]\r\n/home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/numpy/core/multiarray.so(+0xb838b)[0x7f2bd63c438b]\r\n/home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/numpy/core/multiarray.so(+0xb8643)[0x7f2bd63c4643]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x4c2f)[0x7f2bd80ec2ef]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x850)[0x7f2bd80ed290]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x4dc9)[0x7f2bd80ec489]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x850)[0x7f2bd80ed290]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x4dc9)[0x7f2bd80ec489]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x850)[0x7f2bd80ed290]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x4dc9)[0x7f2bd80ec489]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x850)[0x7f2bd80ed290]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x4dc9)[0x7f2bd80ec489]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x850)[0x7f2bd80ed290]\r\n/usr/lib/libpython2.7.so.1.0(+0x6dbdd)[0x7f2bd807cbdd]\r\n/usr/lib/libpython2.7.so.1.0(PyObject_Call+0x43)[0x7f2bd8058c13]\r\n/usr/lib/libpython2.7.so.1.0(+0x5841d)[0x7f2bd806741d]\r\n/usr/lib/libpython2.7.so.1.0(PyObject_Call+0x43)[0x7f2bd8058c13]\r\n/usr/lib/libpython2.7.so.1.0(+0x9de57)[0x7f2bd80ace57]\r\n/usr/lib/libpython2.7.so.1.0(+0x9cbcf)[0x7f2bd80abbcf]\r\n/usr/lib/libpython2.7.so.1.0(PyObject_Call+0x43)[0x7f2bd8058c13]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1321)[0x7f2bd80e89e1]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x850)[0x7f2bd80ed290]\r\n/usr/lib/libpython2.7.so.1.0(PyEval_EvalCode+0x32)[0x7f2bd80ed392]\r\n/usr/lib/libpython2.7.so.1.0(+0xf708f)[0x7f2bd810608f]\r\n/usr/lib/libpython2.7.so.1.0(PyRun_InteractiveOneFlags+0x140)[0x7f2bd8107fb0]\r\n/usr/lib/libpython2.7.so.1.0(PyRun_InteractiveLoopFlags+0x4e)[0x7f2bd810819e]\r\n/usr/lib/libpython2.7.so.1.0(PyRun_AnyFileExFlags+0x3e)[0x7f2bd81087fe]\r\n/usr/lib/libpython2.7.so.1.0(Py_Main+0xc7f)[0x7f2bd8118c2f]\r\n/usr/lib/libc.so.6(__libc_start_main+0xf5)[0x7f2bd7a68bc5]\r\npython2[0x400741]\r\n======= Memory map: ========\r\n00400000-00401000 r-xp 00000000 08:11 1886483                            /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/bin/python2\r\n00600000-00601000 r--p 00000000 08:11 1886483                            /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/bin/python2\r\n00601000-00602000 rw-p 00001000 08:11 1886483                            /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/bin/python2\r\n012d1000-029b7000 rw-p 00000000 00:00 0                                  [heap]\r\n7f2bced0d000-7f2bced11000 r-xp 00000000 08:01 923895                     /usr/lib/python2.7/lib-dynload/termios.so\r\n7f2bced11000-7f2bcef10000 ---p 00004000 08:01 923895                     /usr/lib/python2.7/lib-dynload/termios.so\r\n7f2bcef10000-7f2bcef11000 r--p 00003000 08:01 923895                     /usr/lib/python2.7/lib-dynload/termios.so\r\n7f2bcef11000-7f2bcef13000 rw-p 00004000 08:01 923895                     /usr/lib/python2.7/lib-dynload/termios.so\r\n7f2bcef13000-7f2bcef26000 r-xp 00000000 08:11 57747                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/json.so\r\n7f2bcef26000-7f2bcf125000 ---p 00013000 08:11 57747                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/json.so\r\n7f2bcf125000-7f2bcf126000 r--p 00012000 08:11 57747                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/json.so\r\n7f2bcf126000-7f2bcf127000 rw-p 00013000 08:11 57747                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/json.so\r\n7f2bcf127000-7f2bcf171000 r-xp 00000000 08:11 57858                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/parser.so\r\n7f2bcf171000-7f2bcf370000 ---p 0004a000 08:11 57858                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/parser.so\r\n7f2bcf370000-7f2bcf371000 r--p 00049000 08:11 57858                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/parser.so\r\n7f2bcf371000-7f2bcf376000 rw-p 0004a000 08:11 57858                      /home/agravier/metahome/.local-common/share/python2.7/venvs/finance64/lib/python2.7/site-packages/pandas/parser.so\r\n7f2bcf376000-7f2bcf377000 rw-p 00000000 00:00 0\r\n7f2bcf377000-7f2bcf3d9000 r-xp 00000000 08:01 798526                     /usr/lib/libssl.so.1.0.0\r\n7f2bcf3d9000-7f2bcf5d8000 ---p 00062000 08:01 798526                     /usr/lib/libssl.so.1.0.0\r\n7f2bcf5d8000-7f2bcf5dc000 r--p 00061000 08:01 798526                     /usr/lib/libssl.so.1.0.0\r\n7f2bcf5dc000-7f2bcf5e3000 rw-p 00065000 08:01 798526                     /usr/lib/libssl.so.1.0.0\r\n7f2bcf5e3000-7f2bcf5eb000 r-xp 00000000 08:01 923889                     /usr/lib/python2.7/lib-dynload/_ssl.soAborted (core dumped)\r\n```\r\n\r\n## Configuration information\r\n\r\nPython:\r\n```\r\nPython 2.7.5\r\n```\r\n\r\nuname -a:\r\n```\r\nLinux agravier-archvm 3.10.10-1-ARCH #1 SMP PREEMPT Fri Aug 30 11:30:06 CEST 2013 x86_64 GNU/Linux\r\n```\r\n\r\npip freeze --local:\r\n```\r\nQSTK==0.2.6\r\nmatplotlib==1.3.0\r\nnose==1.3.0\r\nnumpy==1.7.1\r\npandas==0.12.0\r\npyparsing==2.0.1\r\npython-dateutil==2.1\r\npytz==2013.7\r\nscikit-learn==0.14.1\r\nscipy==0.12.1\r\nsix==1.4.1\r\nyolk==0.4.3\r\n```\r\n\r\n## Concluding remarks\r\n\r\nNote that in the line that I use to create the data `list(it.repeat((dt.datetime(2001, 1, 1), ""aa"", 20), 9))`, the number of rows has an influence on whether Python crashes. If less than 9, there is the output:\r\n\r\n```\r\nPython 2.7.5 (default, Sep  6 2013, 09:55:21)\r\n[GCC 4.8.1 20130725 (prerelease)] on linux2\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas as pd\r\n>>> import datetime as dt\r\n>>> import itertools as it\r\n>>>\r\n>>> df_test = pd.DataFrame(data = list(it.repeat((dt.datetime(2001, 1, 1), ""aa"", 20), 8)),\r\n...                        columns=[""A"", ""B"", ""C""],\r\n...                        dtype=[(""A"",""datetime64[h]""), (""B"",""str""), (""C"",""int32"")])\r\n>>> df_test\r\n                     A                           B                            C\r\n0  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n1  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n2  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n3  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n4  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n5  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n6  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n7  2001-01-01 00:00:00  (1972-11-04 17:00:00, , 0)  (1970-01-01 20:00:00, , 20)\r\n```\r\n\r\nNow, this output doesn\'t make much sense to me, it doesn\'t seem to respect the dtype spec that I give, but it\'s very possible that I don\'t understand the dtype spec well and that it\'s actually perfectly sensible output.'"
5185,20898979,danielballan,jreback,2013-10-11 22:41:54,2013-10-13 15:06:00,2013-10-12 13:54:10,closed,,0.13,3,Bug,https://api.github.com/repos/pydata/pandas/issues/5185,b'BUG: Formerly useful behavior now raises.',"b'I did this:\r\n\r\nIn [1]: df1 = DataFrame([1, 2, 3, 4, 5], index=[1, 2, 1, 2, 3])\r\n\r\nIn [2]: df2 = DataFrame([1, 2, 3], index=[1, 2, 3])\r\n\r\nIn [3]: df1.sub(df2)\r\nOut[3]: \r\n   0\r\n1  0\r\n1  2\r\n2  0\r\n2  2\r\n3  2\r\n\r\nwhich is possible up to and including [this commit](https://github.com/jreback/pandas/commit/725b1951249a795fe01896dff4ce46bd9206021f). Then, [this ""bug fix""](https://github.com/jreback/pandas/commit/3f76205f1d4a42ba971ff5be8770b52172b9f822#diff-0) seems to have broken the above usage. It now raises like so:\r\n\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-12-e7e549a4e818> in <module>()\r\n    ----> 1 df1.sub(df2)\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/ops.pyc in f(self, other, axis, level, fill_value)\r\n        710     def f(self, other, axis=default_axis, level=None, fill_value=None):\r\n        711         if isinstance(other, pd.DataFrame):    # Another DataFrame\r\n    --> 712             return self._combine_frame(other, na_op, fill_value, level)\r\n        713         elif isinstance(other, pd.Series):\r\n        714             return self._combine_series(other, na_op, fill_value, axis, level)\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/frame.pyc in _combine_frame(self, other, func, fill_value, level)\r\n       2759 \r\n       2760     def _combine_frame(self, other, func, fill_value=None, level=None):\r\n    -> 2761         this, other = self.align(other, join=\'outer\', level=level, copy=False)\r\n       2762         new_index, new_columns = this.index, this.columns\r\n       2763 \r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/generic.pyc in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\r\n       2331                                      copy=copy, fill_value=fill_value,\r\n       2332                                      method=method, limit=limit,\r\n    -> 2333                                      fill_axis=fill_axis)\r\n       2334         elif isinstance(other, Series):\r\n       2335             return self._align_series(other, join=join, axis=axis, level=level,\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/generic.pyc in _align_frame(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\r\n       2362         left = self._reindex_with_indexers({0: [join_index,   ilidx],\r\n       2363                                             1: [join_columns, clidx]},\r\n    -> 2364                                            copy=copy, fill_value=fill_value)\r\n       2365         right = other._reindex_with_indexers({0: [join_index,   iridx],\r\n       2366                                               1: [join_columns, cridx]},\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/generic.pyc in _reindex_with_indexers(self, reindexers, method, fill_value, limit, copy, allow_dups)\r\n       1320                 indexer = com._ensure_int64(indexer)\r\n       1321                 new_data = new_data.reindex_indexer(index, indexer, axis=baxis,\r\n    -> 1322                                                     fill_value=fill_value, allow_dups=allow_dups)\r\n       1323 \r\n       1324             elif baxis == 0 and index is not None and index is not new_data.axes[baxis]:\r\n\r\n    /Users/danielallan/Documents/Repos/pandas-danielballan/pandas/core/internals.pyc in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups)\r\n       2884         # trying to reindex on an axis with duplicates\r\n       2885         if not allow_dups and not self.axes[axis].is_unique:\r\n    -> 2886             raise ValueError(""cannot reindex from a duplicate axis"")\r\n       2887 \r\n       2888         if axis == 0:\r\n\r\n    ValueError: cannot reindex from a duplicate axis\r\n\r\nI found this very handy. Should it really raise? Can you suggest what I should be doing instead? Thanks.'"
5175,20820881,adamgreenhall,jreback,2013-10-10 17:03:01,2014-09-04 22:18:50,2014-09-04 22:18:50,closed,,0.15.0,35,Bug;Timezones,https://api.github.com/repos/pydata/pandas/issues/5175,b'BUG: DateOffset weekday around DST produces unexpected results',"b'I want next Monday:\r\n\r\n    def next_week_start(t):\r\n        return t + pd.DateOffset(weekday=0)\r\n\r\nBut for DST (fall back is Sunday 2013-11-03):\r\n    \r\n     >> next_week_start(pd.Timestamp(""2013-10-30"", tz=""US/Pacific""))\r\n     Timestamp(\'2013-11-03 23:00:00-0800\', tz=\'US/Pacific\')\r\n     \r\nwhich is Sunday at 11pm, not Monday.'"
5174,20820084,cpcloud,jreback,2013-10-10 16:49:41,2013-10-28 22:58:08,2013-10-28 19:52:51,closed,,0.13,42,Bug;Numeric;Testing,https://api.github.com/repos/pydata/pandas/issues/5174,b'Failing interpolation test',"b'```\r\n$ nosetests pandas/tests/test_generic.py:TestSeries.test_interp_quad\r\nF\r\n======================================================================\r\nFAIL: test_interp_quad (pandas.tests.test_generic.TestSeries)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/phillip/Documents/code/py/pandas/pandas/tests/test_generic.py"", line 339, in test_interp_quad\r\n    assert_series_equal(result, expected)\r\n  File ""/home/phillip/Documents/code/py/pandas/pandas/util/testing.py"", line 452, in assert_series_equal\r\n    assert_attr_equal(\'dtype\', left, right)\r\n  File ""/home/phillip/Documents/code/py/pandas/pandas/util/testing.py"", line 369, in assert_attr_equal\r\n    assert_equal(left_attr,right_attr,""attr is not equal [{0}]"" .format(attr))\r\n  File ""/home/phillip/Documents/code/py/pandas/pandas/util/testing.py"", line 354, in assert_equal\r\n    assert a == b, ""%s: %r != %r"" % (msg.format(a,b), a, b)\r\nAssertionError: attr is not equal [dtype]: dtype(\'int64\') != dtype(\'float64\')\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.041s\r\n\r\nFAILED (failures=1)\r\n```'"
5173,20819153,moonout,jreback,2013-10-10 16:33:06,2014-04-09 12:19:40,2014-04-09 12:19:40,closed,,0.14.0,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/5173,b'BUG: Creating pivot with margins crashes',"b'I\'m trying to create simple pivot\r\n```python\r\nimport pandas as pd\r\nimport numpy\r\nimport datetime\r\n\r\ns1 = pd.Series([datetime.datetime.now() - datetime.timedelta(days=i) for i in range(5)])\r\ns2 = pd.Series([\'a1\', \'a1\', \'a2\', \'a2\', \'a3\'])\r\ns3 = pd.Series([i for i in range(5)])\r\nframe = pd.DataFrame({\'s1\': s1, \'s2\': s1, \'s3\': s3})\r\npivot = pd.pivot_table(frame, rows=\'s2\', cols=\'s1\', values=\'s3\', margins=True)\r\n```\r\nIt happens if I set ""margins=True"" and cols for pivot is datetime.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""./aa.py"", line 12, in <module>\r\n    p = pd.pivot_table(b, rows=\'s2\', cols=\'s1\', values=\'s3\', margins=True)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/tools/pivot.py"", line 135, in pivot_table\r\n    cols=cols, aggfunc=aggfunc)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/tools/pivot.py"", line 174, in _add_margins\r\n    piece[all_key] = margin[key]\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/core/frame.py"", line 2119, in __setitem__\r\n    self._set_item(key, value)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/core/frame.py"", line 2166, in _set_item\r\n    NDFrame._set_item(self, key, value)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/core/generic.py"", line 679, in _set_item\r\n    self._data.set(key, value)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/core/internals.py"", line 1781, in set\r\n    self.insert(len(self.items), item, value)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/core/internals.py"", line 1801, in insert\r\n    new_items = self.items.delete(loc)\r\n  File ""/usr/local/lib/python2.6/dist-packages/pandas/core/index.py"", line 2610, in delete\r\n    new_labels = [np.delete(lab, loc) for lab in self.labels]\r\n  File ""/usr/local/lib/python2.6/dist-packages/numpy/lib/function_base.py"", line 3456, in delete\r\n    ""invalid entry"")\r\nValueError: invalid entry\r\n```'"
5172,20816269,dflatow,jreback,2013-10-10 15:51:04,2015-03-11 12:11:10,2015-03-11 12:10:19,closed,,0.16.0,6,API Design;Bug;Master Tracker;Resample;Timezones,https://api.github.com/repos/pydata/pandas/issues/5172,b'BUG/API: master issue for resample on DST days',"b""- [x] #5694\r\n```\r\ndf = pd.DataFrame([0], index=[datetime(2012, 11, 4, 23, 0, 0)])\r\ndf = df.tz_localize('America/New_York')\r\ndf.resample(rule='D', how='sum')\r\n```\r\nraises a AmbiguousTimeError even though datetime(2012, 11, 4, 23, 0, 0) is not ambiguous.\r\n\r\n- [x] #8601 \r\n```\r\nidx = date_range('2014-10-08 00:00','2014-10-09 00:00', freq='D', tz='Europe/Berlin')\r\npd.Series(5, idx).resample('MS')\r\n```\r\n- [ ] #8744\r\n```\r\nindex = pd.to_datetime(pd.Series([\r\n'2014-10-26 07:35:49',\r\n'2014-10-26 07:45:08',\r\n'2014-10-26 08:04:58'\r\n]))\r\n\r\ndf = pd.DataFrame(np.arange(len(index)), index=index)\r\ndf = df.tz_localize('Asia/Krasnoyarsk', ambiguous='NaT')\r\ndf.resample('D')\r\n\r\nAmbiguousTimeError: Cannot infer dst time from Timestamp('2014-10-26 01:00:00'), try using the 'ambiguous' argument\r\n```\r\n\r\n- [x]  #8653 (might be separate issue)\r\n```\r\nimport datetime\r\nimport pytz as tz\r\nimport pandas as pd\r\n\r\nrome = tz.timezone('Europe/Rome')\r\n\r\ndr = []\r\nfor i in range(2):\r\n    dp = datetime.datetime(2014, 10, 25) + datetime.timedelta(days=i)\r\n    dr.append(rome.localize(dp))\r\n\r\nseries = {}\r\nfor i, ddr in enumerate(dr):\r\n    series[ddr] = i * 10\r\n\r\ns1 = pd.Series(series)\r\ns1 = s1.resample('D', how='mean')\r\n```\r\n\r\n- [x] #9119 \r\n- [ ] #9173\r\n- [x] #9468 """
5156,20680486,jreback,jreback,2013-10-08 14:00:04,2013-11-02 05:32:30,2013-10-19 21:13:49,closed,,0.13,6,Bug;IO CSV,https://api.github.com/repos/pydata/pandas/issues/5156,b'BUG: read_csv with bad file coreing',"b'from ML: \r\nhttps://groups.google.com/forum/#!topic/pydata/KO-PmQdBUZI\r\n```\r\nIn [1]: data = """"""c1\\ntext11,text12\\ntext21,text22""""""\r\n\r\nIn [2]: read_csv(StringIO(data))\r\nOut[2]: \r\n            c1\r\ntext11  text12\r\ntext21  text22\r\n\r\nIn [3]: read_csv(StringIO(data),header=0,names=list(\'abc\'))\r\nBus error (core dumped)\r\n```'"
5148,20648423,cancan101,jreback,2013-10-08 00:39:32,2014-10-23 17:15:44,2014-07-28 14:59:20,closed,,0.15.0,88,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/5148,b'ENH: Cleanup backend for Offsets and Period',b'This PR is a work in progress\r\n\r\nFixes #5306 - Removing unused imports\r\nFixes #5082 - Added `inferred_freq_offset`\r\nFixes #5028 - fixed issues with `_offset_map`\r\nFixes #4878 - `freq` parameter for `Period` init supports full set of Offsets.\r\nFixes #5418 - raise `DateParseError` rather than incorrect `KeyError` for invalid frequencies\r\n'
5143,20641179,TomAugspurger,cpcloud,2013-10-07 21:43:36,2013-10-08 04:35:21,2013-10-08 04:35:21,closed,cpcloud,0.13,8,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/5143,b'Replace partial matches behavior',"b""I ran into some unexpected behavior with `replace` today.\r\n\r\nI want to replace the answer with a numeric value according to the dictionary `weights`\r\n```python\r\nIn [33]: df.answer.head()\r\nOut[33]: \r\n0       Strongly Agree\r\n1                Agree\r\n2              Neutral\r\n3             Disagree\r\n4    Strongly Disagree\r\nName: answer, dtype: object\r\n\r\nIn [34]: weights\r\nOut[34]: \r\n{'Agree': 4,\r\n 'Disagree': 2,\r\n 'Neutral': 3,\r\n 'Strongly Agree': 5,\r\n 'Strongly Disagree': 1}\r\n\r\nIn [35]: df.answer.replace(weights).head()\r\nOut[35]: \r\n0    4\r\n1    4\r\n2    3\r\n3    2\r\n4    2\r\ndtype: int64\r\n```\r\n\r\nIt looks like replace matches on the first part, `Agree`, or `Disagree` and doesn't make it through the dict to the `Strongly`s.  Am I just being a noob with how regexes work, or is this a bug?"""
5129,20585239,cancan101,cpcloud,2013-10-06 20:02:32,2014-02-27 19:02:44,2014-02-27 19:02:44,closed,cpcloud,0.16.0,23,Bug;Data IO;IO HTML,https://api.github.com/repos/pydata/pandas/issues/5129,b'Exception with read_html and header',"b'```python\r\nIn [5]: pd.read_html(""http://pastebin.com/raw.php?i=7mAF0Ei6"",infer_types=False, header=[0,1])[0]\r\n---------------------------------------------------------------------------\r\n\r\n----> 1 pd.read_html(""http://pastebin.com/raw.php?i=7mAF0Ei6"",infer_types=False, header=[0,1])[0]\r\n\r\n/home/alex/git/pandas/pandas/io/html.pyc in read_html(io, match, flavor, header, index_col, skiprows, infer_types, attrs, parse_dates, tupleize_cols, thousands)\r\n    838                          \'data (you passed a negative value)\')\r\n    839     return _parse(flavor, io, match, header, index_col, skiprows, infer_types,\r\n--> 840                   parse_dates, tupleize_cols, thousands, attrs)\r\n\r\n/home/alex/git/pandas/pandas/io/html.pyc in _parse(flavor, io, match, header, index_col, skiprows, infer_types, parse_dates, tupleize_cols, thousands, attrs)\r\n    710     return [_data_to_frame(table, header, index_col, skiprows, infer_types,\r\n    711                            parse_dates, tupleize_cols, thousands)\r\n--> 712             for table in tables]\r\n    713 \r\n    714 \r\n\r\n/home/alex/git/pandas/pandas/io/html.pyc in _data_to_frame(data, header, index_col, skiprows, infer_types, parse_dates, tupleize_cols, thousands)\r\n    600                     skiprows=_get_skiprows(skiprows),\r\n    601                     parse_dates=parse_dates, tupleize_cols=tupleize_cols,\r\n--> 602                     thousands=thousands)\r\n    603     df = tp.read()\r\n    604 \r\n\r\n/home/alex/git/pandas/pandas/io/parsers.pyc in TextParser(*args, **kwds)\r\n   1171     """"""\r\n   1172     kwds[\'engine\'] = \'python\'\r\n-> 1173     return TextFileReader(*args, **kwds)\r\n   1174 \r\n   1175 \r\n\r\n/home/alex/git/pandas/pandas/io/parsers.pyc in __init__(self, f, engine, **kwds)\r\n    481             self.options[\'has_index_names\'] = kwds[\'has_index_names\']\r\n    482 \r\n--> 483         self._make_engine(self.engine)\r\n    484 \r\n    485     def _get_options_with_defaults(self, engine):\r\n\r\n/home/alex/git/pandas/pandas/io/parsers.pyc in _make_engine(self, engine)\r\n    596             elif engine == \'python-fwf\':\r\n    597                 klass = FixedWidthFieldParser\r\n--> 598             self._engine = klass(self.f, **self.options)\r\n    599 \r\n    600     def _failover_to_python(self):\r\n\r\n/home/alex/git/pandas/pandas/io/parsers.pyc in __init__(self, f, **kwds)\r\n   1294         if len(self.columns) > 1:\r\n   1295             self.columns, self.index_names, self.col_names, _ = self._extract_multi_indexer_columns(\r\n-> 1296                 self.columns, self.index_names, self.col_names)\r\n   1297         else:\r\n   1298             self.columns = self.columns[0]\r\n\r\n/home/alex/git/pandas/pandas/io/parsers.pyc in _extract_multi_indexer_columns(self, header, index_names, col_names, passed_names)\r\n    734         # if we find \'Unnamed\' all of a single level, then our header was too long\r\n    735         for n in range(len(columns[0])):\r\n--> 736             if all([ \'Unnamed\' in c[n] for c in columns ]):\r\n    737                 raise _parser.CParserError(""Passed header=[%s] are too many rows for this ""\r\n    738                                            ""multi_index of columns"" % \',\'.join([ str(x) for x in self.header ]))\r\n\r\nTypeError: argument of type \'float\' is not iterable\r\n```'"
5123,20582492,TomAugspurger,jtratner,2013-10-06 16:54:17,2013-10-11 12:01:34,2013-10-11 12:01:34,closed,,0.13,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/5123,b'BUG: Segfault on isnull(MultiIndex)',"b""```\r\nIn [1]: from pandas import *\r\n\r\nIn [2]: idx = MultiIndex.from_tuples([(0, 'a'), (1, 'b'), (2, 'c')])\r\n\r\nIn [3]: pd.isnull(idx)\r\nSegmentation fault: 11\r\n(pandas-dev)dhcp80fff527:pandas tom$ \r\n```\r\n\r\nThis is on latest master (`'0.12.0-757-g69b67f3'`). Numpy 1.7.1."""
5087,20403029,cpcloud,cpcloud,2013-10-02 15:49:19,2014-02-20 14:24:06,2014-02-20 14:24:06,closed,cpcloud,0.14.0,1,Bug;Error Reporting;Experimental,https://api.github.com/repos/pydata/pandas/issues/5087,b'query fails with local variables in expressions',"b""Proper message for overlapping locals/frame (tested):\r\n```\r\nIn [28]: a = Series(randint(3, size=5), name='a')\r\n\r\nIn [29]: b = Series(randint(10, size=5), name='b')\r\n\r\nIn [30]: df = DataFrame({'a': a, 'b': b})\r\n\r\nIn [31]: df.query('b - 1 in a')\r\nNameResolutionError: resolvers and locals overlap on names ['b']\r\n```\r\n\r\nExpression works when there's no local (tested)\r\n```\r\nIn [50]: del b\r\n\r\nIn [51]: a\r\nOut[51]:\r\n0    2\r\n1    2\r\n2    0\r\n3    1\r\n4    2\r\nName: a, dtype: int64\r\n\r\nIn [52]: df.b - 1\r\nOut[52]:\r\n0    3\r\n1    4\r\n2    7\r\n3    7\r\n4   -1\r\nName: b, dtype: int64\r\n\r\nIn [53]: df.query('b - 1 in a')\r\nOut[53]:\r\nEmpty DataFrame\r\nColumns: [a, b]\r\nIndex: []\r\n```\r\n\r\nWorks fine when I use the local by itself (tested):\r\n```\r\nIn [34]: df.query('@b in a')\r\nOut[34]:\r\n   a  b\r\n4  2  0\r\n```\r\n\r\nHowever, when I try to reference the local `b` using `@b` *in an expression*, things go awry (not tested):\r\n```\r\nIn [55]: b = Series(randint(10, size=5), name='b')\r\n\r\nIn [56]: df.query('@b - 1 in a')\r\nUndefinedVariableError: local variable '@b' is not defined\r\n```"""
5074,20360279,goyodiaz,jtratner,2013-10-01 21:30:23,2014-05-15 18:54:17,2013-10-07 21:25:49,closed,,0.13,12,Bug;Indexing;Missing-data,https://api.github.com/repos/pydata/pandas/issues/5074,b'MultiIndex.get_level_values() replaces NA by another value',"b""Test case:\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: index = pd.MultiIndex.from_arrays([\r\n    ['a', 'b', 'b'],\r\n    [1, np.nan, 2]\r\n])\r\n\r\nIn [4]: index.get_level_values(1)\r\nOut[4]: Float64Index([1.0, 2.0, 2.0], dtype=object)\r\n```\r\nThe expected output is\r\n```Float64Index([1.0, nan, 2.0], dtype=object)```\r\n\r\nThis happens because NA values are not stored in the MultiIndex levels and the corresponding label is set to -1. Then when labels are used as indexes to values in ```get_level_values()``` that -1 points to the last (not null) value.\r\n\r\nI tried to fix this by appending a NA to the values if -1 is in levels.\r\nhttps://github.com/goyodiaz/pandas/commit/f028513ad96a\r\nIt needs to be improved in order to return the proper NA value (NaN, None, maybe NaT?) depending on the index type. Does this approach makes sense?\r\n"""
5048,20237441,alefnula,cpcloud,2013-09-29 22:35:59,2013-10-03 02:26:05,2013-10-03 02:26:05,closed,cpcloud,0.13,2,Bug;IO HTML,https://api.github.com/repos/pydata/pandas/issues/5048,b'BUG: read_html does not parse correctly the header of non-string columns',"b""I presume that the problem is that the data is first parsed and then the header is selected out. But when the dtype of the column is a number type the item that should become the column name, since it's not a valid number, becomes `NaN`.\r\n\r\nSample data:\r\n```python\r\ndata1 = io.StringIO(u'''<table>\r\n    <thead>\r\n        <tr>\r\n            <th>Country</th>\r\n            <th>Municipality</th>\r\n            <th>Year</th>\r\n        </tr>\r\n    </thead>\r\n    <tbody>\r\n        <tr>\r\n            <td>Ukraine</td>\r\n            <th>Odessa</th>\r\n            <td>1944</td>\r\n        </tr>\r\n    </tbody>\r\n</table>''')\r\ndata2 = io.StringIO(u'''\r\n<table>\r\n    <tbody>\r\n        <tr>\r\n            <th>Country</th>\r\n            <th>Municipality</th>\r\n            <th>Year</th>\r\n        </tr>\r\n        <tr>\r\n            <td>Ukraine</td>\r\n            <th>Odessa</th>\r\n            <td>1944</td>\r\n        </tr>\r\n    </tbody>\r\n</table>''')\r\n```\r\nOutput:\r\n```python\r\n>>> pd.read_html(data1)[0]\r\n   Country Municipality  Year\r\n0  Ukraine       Odessa  1944\r\n>>> pd.read_html(data2, header=0)[0]\r\n0  Country Municipality   NaN\r\n1  Ukraine       Odessa  1944\r\n```"""
5029,20221846,cancan101,cpcloud,2013-09-29 00:51:11,2013-10-03 02:26:05,2013-10-03 02:26:05,closed,cpcloud,0.13,15,Bug;Data IO;IO HTML;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/5029,b'read_html does not correctly parse table cells with commas',"b'```read_html```, find the correct table, parses the structure of the table (inclusing row and header labels), but does not parse the data:\r\n\r\n```tables = pd.read_html(""http://www.camacau.com/changeLang?lang=en_US&url=/statistic_list"")```\r\n\r\n```\r\nIn [119]: tables[7]\r\nOut[119]: \r\n                     0     1     2     3     4     5     6\r\n0                  NaT  2013  2012  2011  2010  2009  2008\r\n1  2013-01-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n2  2013-02-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n3  2013-03-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n4  2013-04-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n5  2013-05-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n6  2013-06-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n7  2013-07-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n8  2013-08-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n9  2013-09-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n10 2013-10-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n11 2013-11-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n12 2013-12-28 00:00:00   NaN   NaN   NaN   NaN   NaN   NaN\r\n13                 NaT   NaN   NaN   NaN   NaN   NaN   NaN\r\n```'"
5006,20176069,hmeine,jreback,2013-09-27 15:08:36,2013-10-07 00:10:08,2013-10-07 00:10:08,closed,,0.13,8,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/5006,b'.iloc with 1-element integer array behaves badly',"b""\r\nThere is an inconsistency in the returned (python) types when indexing .iloc with an integer array.  The result should always be a sequence (Series), but if the array contains exactly one index, it returns a scalar value:\r\n\r\n```python\r\n\r\n>>> import pandas, numpy\r\n>>> column = pandas.Series(numpy.arange(10))\r\n>>> indices, = (column > 4).nonzero()\r\n>>> column.iloc[indices]\r\n5    5\r\n6    6\r\n7    7\r\n8    8\r\n9    9\r\ndtype: int64\r\n>>> indices, = (column == 5).nonzero()\r\n>>> column.iloc[indices]\r\n5\r\n>>> indices, = (column == 99).nonzero()\r\n>>> column.iloc[indices]\r\nSeries([], dtype: int64)\r\n```\r\n\r\nI am working around that using `numpy.atleast_1d`, but it's ugly."""
4996,20120495,d10genes,jreback,2013-09-26 16:27:34,2013-10-02 21:16:57,2013-10-02 21:16:57,closed,,0.13,27,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4996,b'Changing columns with `+=` gives UnboundLocalError',"b""    In [1]: import numpy as np\r\n    In [2]: import pandas as pd\r\n    In [3]: np.random.seed(10)\r\n    In [4]: a = np.random.randint(0, 20, (6, 5))\r\n    In [5]: df = pd.DataFrame(a, columns=list('abcde'))\r\n    In [6]: df\r\n    Out[6]:\r\n        a   b   c   d   e\r\n    0   9   4  15   0  17\r\n    1  16  17   8   9   0\r\n    2  10   8   4  19  16\r\n    3   4  15  11  11   1\r\n    4   8   4  14  17  19\r\n    5  13   5  13  19  13\r\nChanging columns with `+` is fine\r\n\r\n    In [7]: df.columns = df.columns + '_x'\r\n    In [8]: df\r\n    Out[8]:\r\n       a_x  b_x  c_x  d_x  e_x\r\n    0    9    4   15    0   17\r\n    1   16   17    8    9    0\r\n    2   10    8    4   19   16\r\n    3    4   15   11   11    1\r\n    4    8    4   14   17   19\r\n    5   13    5   13   19   13\r\nThe `+=` operator on the other hand correctly changes the columns,\r\n\r\n    In [9]: df.columns += '_y'\r\n    In [10]: df.columns\r\n    Out[10]: Index([u'a_x_y', u'b_x_y', u'c_x_y', u'd_x_y', u'e_x_y'], dtype=object)\r\nbut gives an error when trying to display the whole dataframe:\r\n\r\n    In [11]: df\r\n    Out[11]: ---------------------------------------------------------------------------\r\n    UnboundLocalError                         Traceback (most recent call last)\r\n\r\nTraceback is [here](https://gist.github.com/d10genes/6716510). Not sure if it's an ipython or pandas issue. `pd.__version__ == '0.12.0-371-g1434776'`"""
4993,20101985,patrickhoebeke,jreback,2013-09-26 10:41:38,2013-09-26 18:31:38,2013-09-26 18:31:38,closed,,0.13,1,Bug;Reshaping;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4993,b'append : NaT should be the default for missing values for datetime64 columns',"b'Hello,\r\n\r\nHere is a issue I discovered in pandas version \'0.12.0\' (already present in preview version I think)\r\n\r\nIssue:\r\nWhen appending a DataFrame (with a new datetime64 column) to an existing one, the default value for missing values should be a pandas.tslib.NaT.\r\n\r\nExample:\r\n```python\r\nimport pandas as pd\r\nimport datetime as dt\r\nfrom pandas.tslib import NaT\r\ndf1 = pd.DataFrame(index=[1,2],\\\r\n    data=[dt.datetime(2013,1,1,0,0),dt.datetime(2013,1,2,0,0)],\\\r\n    columns=[\'start_time\'])\r\n```\r\n![df1](https://f.cloud.github.com/assets/1043822/1216925/e9bfa8a2-2696-11e3-8098-d7c1d902c6a5.jpg)\r\n```python\r\ndf2 = pd.DataFrame(index=[4,5],\\\r\n    data=[[dt.datetime(2013,1,3,0,0),dt.datetime(2013,1,3,6,10)],[dt.datetime(2013,1,4,0,0),dt.datetime(2013,1,4,7,10)]],\\\r\n    columns=[\'start_time\',\'end_time\'])\r\n```\r\n![df2](https://f.cloud.github.com/assets/1043822/1216942/5a2acd24-2697-11e3-8552-c3ee2089a451.jpg)\r\n```python\r\ndf3=df1.append(df2,ignore_index=True)\r\n```\r\n![df3_nan](https://f.cloud.github.com/assets/1043822/1216943/5ce3de66-2697-11e3-97a6-1e553fbb9677.jpg)\r\n\r\nWhile in reallity we want: \r\n![df3_wanted](https://f.cloud.github.com/assets/1043822/1216944/5eabcf7e-2697-11e3-9505-f5b24baa7208.jpg)\r\n\r\nA simple work around is:\r\n```python\r\ndf3[\'end_time\']=df3[\'end_time\'].apply(pd.to_datetime)\r\n```\r\n\r\nCould be nice if, be default, when a new ""datetime64"" column is added, the default for missing values is NaT. Otherwise this creates problem when, for example, saving as HDF5 using pytable which does not accept mixed types per column.\r\n\r\nHave a nice day,\r\n\r\nPatrick\r\n'"
4986,20067175,dalejung,jreback,2013-09-25 20:17:56,2014-02-15 21:16:56,2014-02-15 21:16:56,closed,,0.14.0,2,Bug;Dtypes;Missing-data,https://api.github.com/repos/pydata/pandas/issues/4986,b'pd.isnull(Panel) errors when Panel is empty',"b'related #3551\r\n\r\n```python\r\nimport pandas as pd\r\n\r\npd.isnull(pd.DataFrame()) # works\r\npd.isnull(pd.Panel()) # IndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\n\r\nWhile the above is contrived. I hit the error when building out a `slice(0,0)` programmatically like the following:\r\n\r\n```python\r\npanel = pd.util.testing.makePanel()\r\nnp.all(pd.isnull(panel.iloc[0:0])) # IndexError: index 0 is out of bounds for axis 0 with size 0\r\n```'"
4982,20051798,cpcloud,cpcloud,2013-09-25 16:02:41,2014-08-18 17:03:33,2013-09-27 15:53:26,closed,,0.13,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4982,b'Timestamp comparisons to Series should work on either side of the comparison operator',"b""```\r\nIn [2]: df = DataFrame({'dates': date_range('20010101', periods=10)})\r\n\r\nIn [3]: Timestamp('20010109') < df.dates\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-713f2b228d0e> in <module>()\r\n----> 1 Timestamp('20010109') < df.dates\r\n\r\n/home/phillip/Documents/code/py/pandas/pandas/tslib.so in pandas.tslib._Timestamp.__richcmp__ (pandas/tslib.c:9555)()\r\n\r\nTypeError: Cannot compare Timestamp with 'Series'\r\n```\r\nbut when a `Series` is on the left hand side, things go swimmingly:\r\n\r\n```\r\nIn [4]: df.dates < Timestamp('20010109')\r\nOut[4]:\r\n0     True\r\n1     True\r\n2     True\r\n3     True\r\n4     True\r\n5     True\r\n6     True\r\n7     True\r\n8    False\r\n9    False\r\nName: dates, dtype: bool\r\n```"""
4977,20019934,kongscn,cpcloud,2013-09-25 03:11:01,2014-01-29 04:10:38,2014-01-29 04:10:38,closed,cpcloud,0.14.0,10,Bug;IO HTML;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/4977,b'read_html failed parsing financial statements on WSJ.com',"b""read_html can not parse financial statements on WSJ.com\r\n\r\n```\r\nimport pandas as pd\r\nurl = 'http://quotes.wsj.com/CN/XSHG/600000/financials/annual/balance-sheet'\r\ntb = pd.read_html(url)\r\nprint(tb[0])\r\n```\r\nCheck the contents. Row and column names are parsed correctly, but it seems that all numbers including ',' or in parentheses are ignored.\r\n\r\nI'm using Python 3.3.2 and pandas 0.12.0"""
4975,20014971,jseabold,jreback,2013-09-25 00:06:26,2013-09-26 00:21:08,2013-09-26 00:21:08,closed,,0.13,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/4975,b'BUG: merge fails to rename when name already exists',"b'Trying to merge a bunch of files in a loop. I realize there\'s a better way to do this and a workaround, but it took me a second to realize what was going on here.\r\n\r\n```\r\nw = pd.DataFrame(np.random.random((4,2)), columns=[""x"", ""y""]) \r\nx = pd.DataFrame(np.random.random((4,2)), columns=[""x"", ""y""])\r\ny = pd.DataFrame(np.random.random((4,2)), columns=[""x"", ""y""])\r\nz = pd.DataFrame(np.random.random((4,2)), columns=[""x"", ""y""])\r\n\r\ndta = x.merge(y, left_index=True, right_index=True).merge(z, left_index=True, right_index=True, how=""outer"")\r\n\r\ndta.merge(w, left_index=True, right_index=True)\r\n```'"
4960,19955264,dalejung,jreback,2013-09-24 03:57:24,2013-10-03 21:14:18,2013-10-03 21:14:18,closed,jreback,0.13,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4960,b'Duplicate items on Panel supported?',"b'```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndata = np.random.randn(5, 100, 5)\r\npanel = pd.Panel(data, items=list(""AACDE""))\r\n\r\npanel.iloc[0] # ValueError: Wrong number of dimensions\r\n\r\nfr = panel.loc[""E""]\r\nfr.values.shape # (5, 100, 5) wrong\r\n```\r\n\r\nSo this is another contrived example that I got from just running a battery of tests on random objects. Basically having a duplicate item key will bork the `Panel`. \r\n\r\nI\'m actually not sure whether Panel is meant to be a collection of uniquely identified DataFrames, or more like a generic 3d container that has convenient semantics for its axes. I think at one point it was the former, but as more things pull from generic it\'s becoming the later? \r\n\r\n'"
4947,19890069,hayd,jreback,2013-09-23 06:05:05,2013-10-01 13:28:40,2013-10-01 13:28:40,closed,,0.13,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4947,b'Series boolean intersection not label based',"b""http://stackoverflow.com/questions/18952544/in-python-pandas-boolean-operation\r\n\r\n```\r\nIn [1]: a = pd.Series([True, False, True], list('bca'))\r\n\r\nIn [2]: b = pd.Series([False, True, False], list('abc'))\r\n\r\nIn [3]: a & b\r\nOut[3]:\r\nb    False\r\nc    False\r\na    False\r\ndtype: bool\r\n```\r\n\r\nExpected to use labels:\r\n\r\n```\r\nIn [6]: index = a.index | b.index\r\n\r\nIn [7]: a.reindex(index) & b.reindex(index)\r\nOut[7]:\r\na    False\r\nb     True\r\nc    False\r\ndtype: bool\r\n```"""
4940,19880937,dalejung,jreback,2013-09-22 22:45:42,2014-01-02 07:37:14,2013-09-23 11:38:40,closed,,,4,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/4940,b'Adding new row on TimeDataFrame silently does nothing for integers.',"b""```python\r\ndf = pd.util.testing.makeTimeDataFrame()\r\ndf.ix[100,:] = df.ix[0] # does nothing\r\ndf.ix[100.0, :] = df.ix[0] # adds a new row\r\n```\r\n\r\nNot sure why someone would do such lunacy, but figured I'd report it :P"""
4939,19880888,dalejung,jreback,2013-09-22 22:41:13,2013-09-23 00:25:20,2013-09-23 00:24:59,closed,,0.13,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4939,b'ix setting with enlargement results in stale caches.',"b'When doing setting with enlargement like:\r\n\r\n```python\r\nimport pandas as pd\r\npanel = pd.util.testing.makePanel()\r\npd.ix[0] # get first item into cache\r\npanel.ix[:, :, \'A+1\'] = panel.ix[:, :, \'A\'] + 1\r\nassert ""A+1"" in panel.ix[0].columns # fails due to stale item cache\r\nassert ""A+1"" in panel.ix[1].columns # succeeds\r\n```\r\n\r\nThe same thing happens with DataFrames. \r\n\r\n```python\r\ndf = pd.util.testing.makeDataFrame()\r\ndf[\'A\'] # cache series\r\ndf.ix[""Hello Friend""] = df.ix[0]\r\nassert ""Hello Friend"" in df[\'A\'].index # fails\r\nassert ""Hello Friend"" in df[\'B\'].index # success\r\n```'"
4937,19875871,jreback,jreback,2013-09-22 17:22:12,2014-03-20 12:34:07,2013-09-22 19:28:16,closed,jreback,0.13,0,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/4937,b'BUG: convert_objects on > 2ndim raises',"b""```\r\nIn [40]: x = Panel(dict(A = dict(a = ['1','1.0'])))\r\n\r\nIn [41]: x\r\nOut[41]: \r\n<class 'pandas.core.panel.Panel'>\r\nDimensions: 1 (items) x 2 (major_axis) x 1 (minor_axis)\r\nItems axis: A to A\r\nMajor_axis axis: 0 to 1\r\nMinor_axis axis: a to a\r\n\r\nIn [42]: x.values\r\nOut[42]: \r\narray([[['1'],\r\n        ['1.0']]], dtype=object)\r\n\r\nIn [43]: x.convert_objects(convert_numeric='force')\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n```"""
4932,19866885,jtratner,jreback,2013-09-22 02:40:11,2015-03-03 01:45:05,2015-03-03 01:45:05,closed,,0.16.0,4,Bug;Dtypes;Enhancement;IO Excel,https://api.github.com/repos/pydata/pandas/issues/4932,b'Integers converted to floats when round-tripping through Excel',"b""0.12 and current master have this problem (problem is that we were only testing frames with floats):\r\n\r\n```\r\n>>> df = DataFrame(range(10))\r\n>>> df.dtypes\r\n0    int64\r\ndtype: object\r\n>>> df.index\r\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)\r\n>>> df.to_excel('test1.xlsx')\r\n>>> new_df = read_excel('test1.xlsx', 'sheet1')\r\n>>> new_df.index\r\nIndex([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], dtype=object)\r\n>>> new_df.dtypes\r\n0    float64\r\ndtype: object\r\n```\r\n\r\nCan we do some type inference here or something? `read_csv` handles it better:\r\n\r\n```\r\n>>> df.to_csv('out.csv')\r\n>>> new_df = read_csv('out.csv')\r\n>>> new_df.index\r\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)\r\n>>> new_df.dtypes\r\nUnnamed: 0    int64\r\n0             int64\r\ndtype: object\r\n```"""
4914,19856614,cpcloud,cpcloud,2013-09-21 13:36:05,2014-11-04 22:50:04,2013-12-15 21:00:17,closed,cpcloud,0.13.1,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/4914,b'plot() only shows line or bar with a combined bar and line graph',"b""Here's a notebook:\r\n\r\nhttp://nbviewer.ipython.org/6650724"""
4903,19834215,nehalecky,jorisvandenbossche,2013-09-20 18:47:18,2016-07-21 16:03:04,2015-10-16 07:53:21,closed,,No action,0,Bug;Data IO;IO Excel;Prio-high,https://api.github.com/repos/pydata/pandas/issues/4903,"b""BUG: ExcelFile.parse() skiprows arg doesn't play nice""","b""related/dup #4340\r\n\r\nWas building on [this example on SO](http://stackoverflow.com/questions/15555005/get-inferred-dataframe-types-iteratively-using-chunksize), and found a bug in `ExcelFile().parse` option `skiprows`, when passed an index.\r\n\r\n```python\r\nIn [2]: xls = pd.ExcelFile('example.xlsx')\r\nIn [3]: df = xls.parse(xls.sheet_names[0])\r\nIn [4]: print df\r\n              Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n0   2011-10-01 00:00:00  1892213     E      8111            29      ALTOU\r\n1   2011-11-01 00:00:00  1892213     E      8111            29      ALTOU\r\n2                   NaN      NaN   NaN       NaN           NaN        NaN\r\n3                   NaN      NaN   NaN       NaN           NaN        NaN\r\n4             Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n5   2011-10-01 00:00:00   553961     G      8111            29        GN3\r\n6   2011-11-01 00:00:00   553961     G      8111            29        GN3\r\n7                   NaN      NaN   NaN       NaN           NaN        NaN\r\n8             Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n9   2011-10-01 00:00:00  6322158     E                      29          A\r\n10  2011-11-01 00:00:00  6322158     E                      29          A\r\n```\r\n\r\nWanting to avoid the repeated headers and empty rows, I parse with `skiprows`, but no dice (no difference with previously parsed sheet):\r\n\r\n```python\r\nIn [5]: skip_idx = np.array([2,3,4,7,8])\r\nIn [6]: df = xls.parse(xls.sheet_names[0], skiprows=skip_idx+1)\r\nIn [8]: df\r\nOut[8]: \r\n              Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n0   2011-10-01 00:00:00  1892213     E      8111            29      ALTOU\r\n1   2011-11-01 00:00:00  1892213     E      8111            29      ALTOU\r\n2                   NaN      NaN   NaN       NaN           NaN        NaN\r\n3                   NaN      NaN   NaN       NaN           NaN        NaN\r\n4             Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n5   2011-10-01 00:00:00   553961     G      8111            29        GN3\r\n6   2011-11-01 00:00:00   553961     G      8111            29        GN3\r\n7                   NaN      NaN   NaN       NaN           NaN        NaN\r\n8             Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n9   2011-10-01 00:00:00  6322158     E                      29          A\r\n10  2011-11-01 00:00:00  6322158     E                      29          A\r\n```\r\n\r\nMeanwhile, our little friend `pd.read_csv()`, doesn't seem to have the same hangups. Creating a csv directly from the original `.xlsx` and performing the same operations:\r\n\r\n```python\r\nIn [9]: df = pd.read_csv('example.csv')\r\nIn [10]: df\r\nOut[10]: \r\n    Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n0      Oct-11  1892213     E      8111            29      ALTOU\r\n1      Nov-11  1892213     E      8111            29      ALTOU\r\n2         NaN      NaN   NaN       NaN           NaN        NaN\r\n3         NaN      NaN   NaN       NaN           NaN        NaN\r\n4   Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n5      Oct-11   553961     G      8111            29        GN3\r\n6      Nov-11   553961     G      8111            29        GN3\r\n7         NaN      NaN   NaN       NaN           NaN        NaN\r\n8   Bill Date  Meter #  Type  SIC Code  Billing Days  Rate Code\r\n9      Oct-11  6322158     E                      29          A\r\n10     Nov-11  6322158     E                      29          A\r\n```\r\nLooks fine, and now passing the `skiprows` option, and I get a correctly parsed `df` with all the guilty lines missing:\r\n\r\n```python\r\nIn [11]: df = pd.read_csv('example.csv', skiprows=skip_id+1)\r\nIn [12]: df\r\nOut[12]: \r\n  Bill Date  Meter # Type SIC Code  Billing Days Rate Code\r\n0    Oct-11  1892213    E     8111            29     ALTOU\r\n1    Nov-11  1892213    E     8111            29     ALTOU\r\n2    Oct-11   553961    G     8111            29       GN3\r\n3    Nov-11   553961    G     8111            29       GN3\r\n4    Oct-11  6322158    E                     29         A\r\n5    Nov-11  6322158    E                     29         A\r\n```\r\n\r\nStarted tracing for troubleshooting, but after the 5th handoff of the `skiprows` parameter all over in `.io`, I gave up. :(\r\n\r\nThoughts? """
4902,19834199,alefnula,jreback,2013-09-20 18:46:55,2013-09-21 12:03:26,2013-09-21 12:03:26,closed,,0.13,0,Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/4902,b'_ensure_numeric does not check for complex numbers',"b'`_ensure_numeric` from `pandas.core.nanops` does not include a check for complex numbers, just converts them to floats.\r\n\r\nRelevant StackOverflow question that found the bug: [python pandas complex number](http://stackoverflow.com/questions/18919699/python-pandas-complex-number/)'"
4897,19824203,cpcloud,cpcloud,2013-09-20 15:34:23,2013-09-27 19:34:46,2013-09-27 19:34:46,closed,cpcloud,0.13,0,Bug,https://api.github.com/repos/pydata/pandas/issues/4897,b'Remove datetime comparisons from numexpr in eval (evaluate in Python)',"b""doesn't work correctly with `NaT` values"""
4879,19747335,gdraps,jreback,2013-09-19 10:33:28,2013-09-19 12:55:15,2013-09-19 12:55:15,closed,,0.13,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4879,b'AssertionError while boolean indexing with non-unique columns',"b'Hit this `AssertionError: cannot create BlockManager._ref_locs ...` while processing a dataframe with multiple empty column names. Appears to fail with 0.12/master and pass with 0.11.\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    df = pd.DataFrame(np.random.rand(3,4), columns=[\'\', \'\', \'C\', \'D\'])\r\n    df[df.C > .5]\r\n\r\n&nbsp;\r\n\r\n    Traceback (most recent call last):\r\n      File ""test_fail.py"", line 5, in <module>\r\n        df[df.C > .5]\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1830, in __getitem__\r\n        return self._getitem_frame(key)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1898, in _getitem_frame\r\n        return self.where(key)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/generic.py"", line 2372, in where\r\n        cond = cond.reindex(**self._construct_axes_dict())\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1118, in reindex\r\n        return self._reindex_axes(axes, level, limit, method, fill_value, copy, takeable=takeable)._propogate_attributes(self)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/frame.py"", line 2413, in _reindex_axes\r\n        fill_value, limit, takeable=takeable)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/frame.py"", line 2436, in _reindex_columns\r\n        copy=copy, fill_value=fill_value, allow_dups=takeable)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/generic.py"", line 1218, in _reindex_with_indexers\r\n        fill_value=fill_value, allow_dups=allow_dups)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2840, in reindex_indexer\r\n        return self._reindex_indexer_items(new_axis, indexer, fill_value)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2885, in _reindex_indexer_items\r\n        return self.__class__(new_blocks, new_axes)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/internals.py"", line 1735, in __init__\r\n        self._set_ref_locs(do_refs=True)\r\n      File ""/home/gmd/.virtualenvs/pandas-master/local/lib/python2.7/site-packages/pandas-0.12.0_476_gb889fda-py2.7-linux-i686.egg/pandas/core/internals.py"", line 1863, in _set_ref_locs\r\n        ""does not have _ref_locs set"" % (block, labels))\r\n    AssertionError: cannot create BlockManager._ref_locs because block [BoolBlock: [C], 1 x 3, dtype: bool] with duplicate items [Index([u\'\', u\'\', u\'C\', u\'D\'], dtype=object)] does not have _ref_locs set\r\n\r\n'"
4862,19625605,brianboonstra,jreback,2013-09-17 16:32:44,2015-01-26 01:29:07,2015-01-26 01:29:07,closed,,0.16.0,2,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/4862,b'DataFrame.unstack() fails when some index column values are NaN',"b'{Python 2.6.6, pandas 0.12}\r\n\r\nA DataFrame will fail to unstack() when one of the columns retained as an index has NaN values.  The code below sets up a dataframe with NaN in some index entries, at which point calling unstack() will fail.\r\n\r\nIn the first failure, the exception message is that the index ""has duplicate entries"" which is patently false.   In the second failure, where a given id only has one NaN, the error message becomes cannot convert float NaN to integer.\r\n\r\nA final try, with NaN converted to a sentinel value of 42, shows proper behavior.\r\n\r\n```python\r\nimport pandas\r\nfrom numpy import nan\r\n\r\ndf = pandas.DataFrame(\r\n    {\'agent\': {\r\n                      17263: \'Hg\',\r\n                      17264: \'U\',\r\n                      17265: \'Pb\',\r\n                      17266: \'Sn\',\r\n                      17267: \'Ag\',\r\n                      17268: \'Hg\'},\r\n    \'change\': {\r\n                      17263: nan,\r\n                      17264: 0.0,\r\n                      17265: 7.070e-06,\r\n                      17266: 2.3614e-05,\r\n                      17267: 0.0,\r\n                      17268: -0.00015},\r\n    \'dosage\': {\r\n                      17263: nan,\r\n                      17264: nan,\r\n                      17265: nan,\r\n                      17266: 0.0133,\r\n                      17267: 0.0133,\r\n                      17268: 0.0133},\r\n    \'s_id\': {\r\n                      17263: 680585148,\r\n                      17264: 680585148,\r\n                      17265: 680585148,\r\n                      17266: 680607017,\r\n                      17267: 680607017,\r\n                      17268: 680607017}}\r\n            )\r\ntry:\r\n    dupe = df.copy().set_index([\'s_id\',\'dosage\',\'agent\'])\r\n    badDupe = dupe.unstack()\r\nexcept Exception as e:\r\n    print( \'Error with all data was: %s\'%(e,) )\r\ntry:\r\n    getnan = df.ix[17264:].copy().set_index([\'s_id\',\'dosage\',\'agent\'])\r\n    badNan = getnan.unstack()\r\nexcept Exception as e:\r\n    print( \'Error dropping first entry was: %s\'%(e,) )\r\ndf.dosage[:3]=42\r\nwillWork = df.copy().set_index([\'s_id\',\'dosage\',\'agent\'])\r\nu = willWork.unstack()\r\nprint(u) \r\n```\r\n\r\n\r\nOverall output:\r\n```\r\nError with all data was: Index contains duplicate entries, cannot reshape\r\nError dropping first entry was: cannot convert float NaN to integer\r\n\r\n                   change                                 \r\nagent                  Ag       Hg        Pb        Sn   U\r\ns_id      dosage                                          \r\n680585148 42.0000     NaN      NaN  0.000007       NaN   0\r\n680607017 0.0133        0 -0.00015       NaN  0.000024 NaN\r\n```'"
4853,19564276,dalejung,jreback,2013-09-16 17:21:21,2014-04-06 15:14:01,2013-09-18 13:40:57,closed,,0.13,3,API Design;Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4853,b'tshift does not work for Panels',b'`tshift` is broken for `Panel` because `Panel.shift` does not handle `freq`.\r\n\r\n'
4839,19503731,jburroni,jtratner,2013-09-14 15:27:56,2013-09-17 05:40:40,2013-09-17 05:40:40,closed,jtratner,0.13,13,Bug,https://api.github.com/repos/pydata/pandas/issues/4839,b'DataFrame.sort_index does not use ascending when then value is a list with a single element',"b""    In [7]: d\r\n    Out[7]: {'one': [1.0, 2.0, 3.0, 4.0], 'two': [4.0, 3.0, 2.0, 1.0]}\r\n\r\n\r\n\r\n    In [11]: pd.DataFrame(d).sort_index(by=['two'], ascending=[0,])\r\n    Out[11]: \r\n       one  two\r\n    3    4    1\r\n    2    3    2\r\n    1    2    3\r\n    0    1    4\r\n\r\n    In [12]: pd.DataFrame(d).sort_index(by=['two'], ascending=0)\r\n    Out[12]: \r\n       one  two\r\n    0    1    4\r\n    1    2    3\r\n    2    3    2\r\n    3    4    1\r\n"""
4835,19474655,wabu,wabu,2013-09-13 20:34:48,2014-06-13 07:51:18,2014-02-08 14:50:32,closed,,0.14.0,13,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4835,b'io.pytables: handle start/stop in remove and select_coords',"b'I realized that the start/stop arguments to HDFStore.remove are ignored, so I have a patch to make them work as expected. Moreover I changed Selection.select_coords to also apply start and stop.'"
4827,19419872,jtratner,jtratner,2013-09-12 22:41:57,2013-09-20 08:01:33,2013-09-20 08:01:33,closed,,0.13,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/4827,b'BUG: Fred tests failing - possible error with Fred data IO.',"b'These two tests are failing (but they\'ve been hidden for a while). They are (going to be) skipped on master for the moment, but need to be fixed/removed at some pont.\r\n\r\nIs someone responsible/using the Fred data reader right now?\r\n```\r\n======================================================================\r\nERROR: test_fred_nan (pandas.io.tests.test_data.TestFred)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/pandas-0.0.0-py2.7-linux-x86_64.egg/pandas/util/testing.py"", line 842, in network_wrapper\r\n    return t(*args, **kwargs)\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/pandas-0.0.0-py2.7-linux-x86_64.egg/pandas/io/tests/test_data.py"", line 387, in test_fred_nan\r\n    assert pd.isnull(df.ix[\'2010-01-01\'])\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/pandas-0.0.0-py2.7-linux-x86_64.egg/pandas/core/generic.py"", line 559, in __nonzero__\r\n    raise ValueError(""The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all()."")\r\nValueError: The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all().\r\n-------------------- >> begin captured stdout << ---------------------\r\nFailed: ValueError(\'The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all().\',)\r\n--------------------- >> end captured stdout << ----------------------\r\n```\r\n```\r\n======================================================================\r\nERROR: test_fred_parts (pandas.io.tests.test_data.TestFred)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/pandas-0.0.0-py2.7-linux-x86_64.egg/pandas/util/testing.py"", line 842, in network_wrapper\r\n    return t(*args, **kwargs)\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/pandas-0.0.0-py2.7-linux-x86_64.egg/pandas/io/tests/test_data.py"", line 394, in test_fred_parts\r\n    self.assertEqual(df.ix[\'2010-05-01\'], 217.23)\r\n  File ""/usr/lib/python2.7/unittest/case.py"", line 511, in assertEqual\r\n    assertion_func(first, second, msg=msg)\r\n  File ""/usr/lib/python2.7/unittest/case.py"", line 501, in _baseAssertEqual\r\n    if not first == second:\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/pandas-0.0.0-py2.7-linux-x86_64.egg/pandas/core/generic.py"", line 559, in __nonzero__\r\n    raise ValueError(""The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all()."")\r\nValueError: The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all().\r\n-------------------- >> begin captured stdout << ---------------------\r\nFailed: ValueError(\'The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all().\',)\r\n--------------------- >> end captured stdout << ----------------------\r\n----------------------------------------------------------------------\r\n```'"
4825,19392819,ruidc,jreback,2013-09-12 15:05:41,2013-09-16 13:01:20,2013-09-16 12:56:55,closed,,0.13,29,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4825,b'bug or unhelpful error message in 0.12 and trunk: IndexError: Out of bounds on buffer access (axis 0)',"b'The following raises the error\r\n```\r\npandas.Series([0.1, 0.2], index=[1, 2]).loc[[2, 3, 2]]\r\n```\r\n\r\nwhilst\r\n```\r\npandas.Series([0.1, 0.2], index=[1, 2]).loc[[2, 2, 3]]\r\n```\r\n\r\ndoes not. Or is there something invalid with this code?'"
4815,19338914,willrc,jreback,2013-09-11 18:52:07,2013-09-11 19:38:24,2013-09-11 19:38:12,closed,,0.13,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/4815,b'Column multi-index not restored correctly on load from HDF5 table',"b""v 0.12.0\r\n\r\ncols = pd.MultiIndex.from_tuples([(1,2), (3,4)])\r\ndata = pd.DataFrame(index=[1,2,3], columns=cols, dtype=float)\r\ndata.values.fill(0.)\r\n\r\ns = store('/tmp/test', mode='a')\r\ns.append('mi_frame', data)\r\n\r\nreloaded = s['mi_frame']     # also applies to s.select...\r\nprint type(data.columns)\r\nOut: pandas.core.index.MultiIndex\r\n\r\nprint type(reloaded.columns)\r\nOut: pandas.core.index.Index\r\n\r\nIt's easy enough to re-convert upon reload, but would be nice not to have to"""
4812,19331977,jreback,jreback,2013-09-11 16:54:59,2013-09-13 00:08:05,2013-09-13 00:08:05,closed,,0.13,0,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4812,b'BUG: resample with dup index broken',"b""\r\n\r\n```\r\nIn [2]: df = DataFrame(randn(4,12),index=[2000,2000,2000,2000],columns=[ pd.Period(year=2000,month=i+1,freq='M') for i in range(12) ])\r\n\r\nIn [3]: df\r\nOut[3]: \r\n       2000-01   2000-02   2000-03   2000-04   2000-05   2000-06   2000-07   2000-08   2000-09   2000-10   2000-11   2000-12\r\n2000 -0.630327  0.698388 -0.678858 -1.289394  1.541255  1.828321 -0.157585 -0.362442  1.315526  0.783812  1.195390 -0.350228\r\n2000 -0.213631 -0.759442 -0.418679  0.199200  0.449716 -1.936404 -1.040448 -1.867548  1.066477 -0.303183  1.982605  0.138027\r\n2000  1.052005  1.291127  1.311332 -0.215721 -1.075875  0.209229 -0.159184 -0.662904 -1.812327  0.830689 -1.720358  1.323978\r\n2000 -1.266226 -0.508003 -0.382924 -0.165822 -0.876964 -0.767572 -0.334778 -1.576188  0.378332 -0.617412 -1.053356  1.843076\r\n\r\nIn [4]: df.resample('Q',axis=1)\r\nAttributeError: 'DataFrame' object has no attribute 'flags'\r\n```\r\n"""
4807,19287069,jorisvandenbossche,jreback,2013-09-10 21:47:11,2013-10-11 21:09:22,2013-10-11 21:09:22,closed,,0.13,6,Bug;Docs;Prio-high,https://api.github.com/repos/pydata/pandas/issues/4807,"b""DOC: example in 'Custom Business Days' shows errors in the built dev docs""",b'The example code seems not to run on the computer where the documentation is build: http://pandas.pydata.org/pandas-docs/dev/timeseries.html#custom-business-days-experimental'
4795,19235382,jtratner,jreback,2013-09-10 02:59:40,2013-09-10 11:42:07,2013-09-10 11:42:07,closed,,0.13,2,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/4795,b'BUG: _coerce_to_dtypes: Timestamp is not defined.',"b'@jreback - `Timestamp` isn\'t actually defined/imported in core/common. Did you mean `lib.Timestamp`?\r\n\r\n```python\r\n# snippet from core/common.py\r\ndef _coerce_to_dtypes(result, dtypes):\r\n    """""" given a dtypes and a result set, coerce the result elements to the dtypes """"""\r\n    if len(result) != len(dtypes):\r\n        raise AssertionError(""_coerce_to_dtypes requires equal len arrays"")\r\n\r\n    def conv(r,dtype):\r\n        try:\r\n            if isnull(r):\r\n                pass\r\n            elif dtype == _NS_DTYPE:\r\n                r = Timestamp(r)\r\n            elif dtype == _TD_DTYPE:\r\n                r = _coerce_scalar_to_timedelta_type(r)\r\n            elif dtype == np.bool_:\r\n                r = bool(r)\r\n            elif dtype.kind == \'f\':\r\n                r = float(r)\r\n            elif dtype.kind == \'i\':\r\n                r = int(r)\r\n        except:\r\n            pass\r\n\r\n        return r\r\n\r\n    return np.array([ conv(r,dtype) for r, dtype in zip(result,dtypes) ])\r\n```'"
4794,19231661,jtratner,jtratner,2013-09-10 00:31:05,2013-10-14 01:43:01,2013-10-14 01:43:01,closed,jtratner,0.13,6,Bug,https://api.github.com/repos/pydata/pandas/issues/4794,b'BUG: MultiIndex needs to validate length of levels on setting.',"b'related #4687\r\n\r\nBad errors and/or not checking:\r\n\r\n```\r\n>>> ind = tm.makeCustomIndex(5, 5)\r\n>>> ind.set_levels([[1, 2]]) # Wrong Error\r\nTraceback\r\n    ...\r\nValueError: Length of names (5) must be same as level (1)\r\n>>> ind = tm.makeCustomIndex(5, 5)\r\n>>> ind.set_levels([[1, 2]], inplace=True) # Wrong Error\r\nTraceback\r\n    ...\r\nValueError: Length of names (5) must be same as level (1)\r\n>>> ind = tm.makeCustomIndex(5, 5)\r\n>>> ind.levels = [[1, 2]] # Wrong Error\r\nTraceback\r\n    ...\r\nValueError: Length of names (5) must be same as level (1)\r\n```'"
4789,19221682,yarikoptic,cpcloud,2013-09-09 20:45:14,2013-09-14 20:31:47,2013-09-14 20:31:47,closed,cpcloud,0.13,13,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/4789,b'test_bar_log fights back with matplotlib 1.3.0',"b'```\r\n======================================================================\r\nFAIL: test_bar_log (pandas.tests.test_graphics.TestDataFramePlots)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/pandas/tests/test_graphics.py"", line 568, in test_bar_log\r\n    self.assertEqual(ax.yaxis.get_ticklocs()[0], 1.0)\r\nAssertionError: 0.10000000000000001 != 1.0\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.341s\r\n```\r\n\r\nneeds to be fixed up for debian where mpl now 1.3.0'"
4788,19221174,d10genes,jreback,2013-09-09 20:36:09,2013-09-10 14:04:41,2013-09-10 13:05:46,closed,,0.13,24,Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/4788,"b""read_pickle error for multi-index: 'FrozenList' does not support mutable operations.""","b""I'm in a bit of a pickle here. If I try to save and read back a multi-indexed dataframe, I get this error (and in some situations, can't reproduce when, I get a `TypeError: Required argument 'shape' (pos 1) not found` error).\r\n\r\nThe gist with the full traceback is [here](https://gist.github.com/d10genes/6500977).\r\n\r\n```\r\nIn [3]: import numpy as np\r\n\r\nIn [4]: import pandas as pd\r\n\r\nIn [5]: np.random.seed(10)\r\n\r\nIn [6]: a = np.random.randint(0, 20, (6, 5))\r\n\r\nIn [7]: df = pd.DataFrame(a).set_index([2,3,4])\r\n\r\nIn [8]: df\r\nOut[8]:\r\n           0   1\r\n2  3  4\r\n15 0  17   9   4\r\n8  9  0   16  17\r\n4  19 16  10   8\r\n11 11 1    4  15\r\n14 17 19   8   4\r\n13 19 13  13   5\r\n\r\nIn [9]: df.to_pickle('~/Desktop/dummy.df')\r\n\r\nIn [10]: df2 = pd.read_pickle('~/Desktop/dummy.df')\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n...\r\nTypeError: 'FrozenList' does not support mutable operations.\r\n```\r\n\r\nWhen I `reset_index` it saves fine. I'm on Mac OSX, numpy 1.7, pandas 0.12.0-361-g53eec08.\r\n\r\nIn the meantime, if there's no quick fix, anyone know another way to save multi-indexes to file? csv doesn't look like it can preserve them. I could do something like appending `_index` to the columns and un-rename them after reading but would prefer something less hacky."""
4785,19179685,y-p,jtratner,2013-09-09 07:10:50,2013-09-14 02:00:38,2013-09-14 02:00:38,closed,jtratner,0.13,7,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/4785,b'read_fwf/table on py3 has trouble with BytesIO',"b'```\r\n>>> import pandas as pd\r\n>>> from io import BytesIO\r\n>>> pd.read_fwf(BytesIO("""".encode(\'utf8\')),widths=[2])\r\n>>>pandas/io/parsers.py"", line 1944, in <listcomp>\r\n>>>    for (fromm, to) in self.colspecs]\r\n>>>TypeError: Type str doesn\'t support the buffer API\r\n```\r\n\r\nBy another path:\r\n```\r\n>>> from io import BytesIO\r\n>>> pd.read_table(BytesIO(""::1234\\n"".encode(\'cp1255\')),sep=""::"", engine=\'python\', encoding=\'cp1255\')\r\n  File ""/usr/local/lib/python3.3/dist-packages/pandas-0.12.0_357_g218f334-py3.3-linux-x86_64.egg/pandas/io/parsers.py"", line 1324, in _read\r\n    yield pat.split(line.decode(\'utf-8\').strip())\r\nUnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xf9 in position 0: invalid start byte\r\n```\r\nis broken. Note that len(sep)>1 activates the python engine anyway right now.\r\n \r\n\r\nrelated #4784 \r\n\r\nEdit: fixed incorrect encoding and updated error\r\nEdit: Updated examples'"
4771,19145492,jreback,jreback,2013-09-07 14:33:38,2013-09-25 23:53:33,2013-09-07 20:57:14,closed,,0.13,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4771,b'BUG: concatenation of dups across dtypes fails for axis=0',"b""Addmitedly an odd case....\r\n\r\n```\r\nIn [5]: df = concat([DataFrame(np.random.randn(10,4),columns=['A','A','B','B']),\r\n   ...:                      DataFrame(np.random.randint(0,10,size=20).reshape(10,2),columns=['A','C'])],\r\n   ...:                     axis=1)\r\n\r\nIn [6]: df\r\nOut[6]: \r\n          A         A         B         B  A  C\r\n0 -0.931048  1.068066 -0.006521 -0.031878  4  7\r\n1  2.440559  1.279710 -0.272613 -1.387456  8  4\r\n2 -0.220729  0.730665 -0.999338 -1.063784  1  1\r\n3 -0.395295 -0.868506 -1.945702  1.531287  4  1\r\n4  0.218480 -1.907875  0.354556 -0.277721  9  3\r\n5 -0.518193 -1.292118 -0.250087  0.750773  3  8\r\n6  1.908601 -1.736724 -1.156278  0.552929  3  7\r\n7 -0.996528  0.081386 -1.030960  0.959769  7  1\r\n8  0.349220  0.904122 -0.033104 -1.469096  1  4\r\n9  0.798829 -0.882321 -1.969144 -0.237184  4  7\r\n\r\nIn [7]: concat([df,df],axis=1)\r\nOut[7]: \r\n          A         A         B         B  A  C         A         A         B         B  A  C\r\n0 -0.931048  1.068066 -0.006521 -0.031878  4  7 -0.931048  1.068066 -0.006521 -0.031878  4  7\r\n1  2.440559  1.279710 -0.272613 -1.387456  8  4  2.440559  1.279710 -0.272613 -1.387456  8  4\r\n2 -0.220729  0.730665 -0.999338 -1.063784  1  1 -0.220729  0.730665 -0.999338 -1.063784  1  1\r\n3 -0.395295 -0.868506 -1.945702  1.531287  4  1 -0.395295 -0.868506 -1.945702  1.531287  4  1\r\n4  0.218480 -1.907875  0.354556 -0.277721  9  3  0.218480 -1.907875  0.354556 -0.277721  9  3\r\n5 -0.518193 -1.292118 -0.250087  0.750773  3  8 -0.518193 -1.292118 -0.250087  0.750773  3  8\r\n6  1.908601 -1.736724 -1.156278  0.552929  3  7  1.908601 -1.736724 -1.156278  0.552929  3  7\r\n7 -0.996528  0.081386 -1.030960  0.959769  7  1 -0.996528  0.081386 -1.030960  0.959769  7  1\r\n8  0.349220  0.904122 -0.033104 -1.469096  1  4  0.349220  0.904122 -0.033104 -1.469096  1  4\r\n9  0.798829 -0.882321 -1.969144 -0.237184  4  7  0.798829 -0.882321 -1.969144 -0.237184  4  7\r\n\r\nIn [8]: concat([df,df],axis=0)\r\nAttributeError: 'BlockManager' object has no attribute 'dtype'\r\n```"""
4767,19134651,jreback,jreback,2013-09-07 00:05:13,2013-09-07 00:20:06,2013-09-07 00:20:06,closed,,0.13,0,Bug;Indexing;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4767,b'BUG: readback of duplicate columns in a HDFStore table is buggy',b'http://stackoverflow.com/questions/18646668/pandas-hdfstore-duplicate-items-error/18668217#18668217'
4758,19072906,jorisvandenbossche,hayd,2013-09-05 20:33:10,2013-11-27 10:14:52,2013-09-06 08:16:56,closed,,0.13,4,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4758,b'Datetime as string in indexing a MultiIndex not always working',"b'See http://stackoverflow.com/questions/18643030/groupby-and-multi-indexing/18644920#18644920\r\n\r\nBasically, indexing a DateTimeIndex with a string does work in a single index, but not always in a MultiIndex:\r\n```\r\nIn [7]: df= pd.DataFrame({\'ACCOUNT\':[""ACCT1"", ""ACCT1"", ""ACCT1"", ""ACCT2""],\r\n   ...:                   \'TICKER\':[""ABC"", ""MNP"", ""XYZ"", ""XYZ""],\r\n   ...:                   \'val\':[1,2,3,4]},\r\n   ...:                  index=pd.date_range(""2013-06-19 09:30:00"", periods=4, freq=\'5T\'))\r\nIn [8]: df_multi = df.set_index([\'ACCOUNT\', \'TICKER\'], append=True)\r\nIn [9]: df\r\nOut[9]:\r\n                    ACCOUNT TICKER  val\r\n2013-06-19 09:30:00   ACCT1    ABC    1\r\n2013-06-19 09:35:00   ACCT1    MNP    2\r\n2013-06-19 09:40:00   ACCT1    XYZ    3\r\n2013-06-19 09:45:00   ACCT2    XYZ    4\r\n\r\nIn [10]: df_multi\r\nOut[10]:\r\n                                    val\r\n                    ACCOUNT TICKER\r\n2013-06-19 09:30:00 ACCT1   ABC       1\r\n2013-06-19 09:35:00 ACCT1   MNP       2\r\n2013-06-19 09:40:00 ACCT1   XYZ       3\r\n2013-06-19 09:45:00 ACCT2   XYZ       4\r\n```\r\nAccessing a row with string in single index works:\r\n```\r\nIn [11]: df.loc[\'2013-06-19 09:30:00\']\r\nOut[11]:\r\nACCOUNT    ACCT1\r\nTICKER       ABC\r\nval            1\r\nName: 2013-06-19 09:30:00, dtype: object\r\n```\r\nIn a MultiIndex it doesn\'t work:\r\n```\r\nIn [12]: df_multi.loc[(\'2013-06-19 09:30:00\', \'ACCT1\', \'ABC\')]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last) <ipython-input-12-cbb2ba302ffa> in <module>()\r\n----> 1 df_multi.loc[(\'2013-06-19 09:30:00\', \'ACCT1\', \'ABC\')]\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in __getitem__(self, key)\r\n    722     def __getitem__(self, key):\r\n    723         if type(key) is tuple:\r\n--> 724             return self._getitem_tuple(key)\r\n    725         else:\r\n    726             return self._getitem_axis(key, axis=0)\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in _getitem_tuple(self, tup)\r\n    285\r\n    286         # no multi-index, so validate all of the indexers\r\n--> 287         self._has_valid_tuple(tup)\r\n    288\r\n    289         # ugly hack for GH #836\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in _has_valid_tuple(self, key)\r\n    717             if i >= self.obj.ndim:\r\n    718                 raise ValueError(\'Too many indexers\')\r\n--> 719             if not self._has_valid_type(k,i):\r\n    720                 raise ValueError(""Location based indexing can only have [%s] types"" % self._valid_types)\r\n    721\r\n\r\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\indexing.pyc in _has_valid_type(self, key, axis)\r\n    792\r\n    793             if not key in ax:\r\n--> 794                 raise KeyError(""the label [%s] is not in the [%s]"" % (key,self.obj._get_axis_name(axis)))\r\n    795\r\n    796         return True\r\n\r\nKeyError: \'the label [ACCT1] is not in the [columns]\'\r\n```\r\nBut when only accessing the first two levels of the three levels of the MultiIndex, it does work:\r\n```\r\nIn [13]: df_multi.loc[(\'2013-06-19 09:30:00\', \'ACCT1\')]\r\nOut[13]:\r\n        val\r\nTICKER\r\nABC       1\r\n```\r\nAnd when using a Timestamp it also works (as it should be):\r\n```\r\nIn [14]: df_multi.loc[(pd.Timestamp(\'2013-06-19 09:30:00\', tz=None), \'ACCT1\', \'ABC\')]\r\nOut[14]:\r\nval    1\r\nName: (2013-06-19 09:30:00, ACCT1, ABC), dtype: int64\r\n```'"
4743,18950100,cpcloud,cpcloud,2013-09-04 02:21:00,2013-09-06 23:40:44,2013-09-06 23:40:44,closed,cpcloud,0.13,7,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/4743,b'Bug in Series.replace',"b""http://pandas.pydata.org/pandas-docs/dev/missing_data.html#replacing-generic-values\r\n\r\nand scroll down a bit. Caught this in a doc build. I'll take a look at it."""
4726,18839894,jtratner,jreback,2013-09-01 17:42:42,2013-09-02 13:07:12,2013-09-02 13:07:12,closed,,0.13,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4726,b'BUG: Indexing with duplicate columns',"b""Currently failing test case:\r\n\r\n```python\r\ndf = DataFrame([[1,2,'foo','bar']], columns=['a','a','a','a'])\r\ns = Series([1, 2, 'foo', 'bar'], index=['a', 'a', 'a', 'a'], name=0)\r\nassert_series_equal(df.irow(0), s)\r\n```\r\n\r\nHere's what's going on right now.\r\n\r\n```python\r\nIn [3]: from pandas import *\r\nIn [4]: df = DataFrame([[1,2,'foo','bar']], columns=['a','a','a','a'])\r\nIn [5]: df\r\nOut[5]:\r\n   a  a    a    a\r\n0  1  2  foo  bar\r\n\r\nIn [6]: df.irow(0)\r\nOut[6]:\r\na    bar\r\na    bar\r\na    bar\r\na    bar\r\nName: 0, dtype: object\r\n\r\nIn [7]: df.icol(0)\r\nOut[7]:\r\n0    1\r\nName: a, dtype: int64\r\n\r\nIn [8]: df.columns\r\nOut[8]: Index([u'a', u'a', u'a', u'a'], dtype=object)\r\n```"""
4710,18795303,kghose,jreback,2013-08-30 15:52:54,2013-08-31 20:57:53,2013-08-31 20:00:14,closed,,0.13,6,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4710,b'hierarchical index + frame_table + data_columns=True -> TypeError',"b""```\r\nimport pandas as pd, numpy\r\n\r\nr = numpy.empty((3,4))\r\nindex = pd.MultiIndex.from_tuples([('A','a'), ('A','b'), ('B','a'), ('B','b')])\r\ndf = pd.DataFrame(r, columns=index)\r\n\r\nstore = pd.HDFStore('df.h5')\r\nstore.put('data',df) #->OK\r\nstore.put('data1',df,table=True) #-> Ok\r\nstore.put('data2',df,table=True,data_columns=['A']) #-> Ok\r\nstore.put('data3',df,table=True,data_columns=True) #-> raises hell \r\n#TypeError: not all arguments converted during string formatting\r\n\r\nstore['data']['A'] #->OK\r\nstore['data1']['A'] #-> KeyError KeyError: u'no item named A'\r\n\r\nIn [43]: df = store['data']\r\n\r\nIn [44]: df\r\nOut[44]: \r\n   A                            B               \r\n   a             b              a              b\r\n0  2 -1.727234e-77  2.964394e-323   0.000000e+00\r\n1  0  0.000000e+00   0.000000e+00   0.000000e+00\r\n2  0  0.000000e+00   0.000000e+00  8.344027e-309\r\n\r\nIn [45]: df = store['data1']\r\n\r\nIn [46]: df\r\nOut[46]: \r\n   (A, a)        (A, b)         (B, a)         (B, b)\r\n0       2 -1.727234e-77  2.964394e-323   0.000000e+00\r\n1       0  0.000000e+00   0.000000e+00   0.000000e+00\r\n2       0  0.000000e+00   0.000000e+00  8.344027e-309\r\n\r\n\r\npd.__version__ -> '0.12.0'\r\n```\r\n"""
4708,18792219,prossahl,jreback,2013-08-30 14:56:41,2013-10-15 12:42:35,2013-10-15 12:42:35,closed,,0.13.1,0,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4708,b'BUG: A zero length series written to HDF cannot be read back.',"b'This happens with and empty series with numpy arrays for both the values and the index.\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> \r\n>>> with pd.get_store(\'foo.h5\') as store:\r\n...     s = pd.Series(np.array([], dtype=np.int64), index=np.array([], dtype=np.int64))\r\n...     store[\'s\'] = s\r\n...     s = store[\'s\']\r\n... \r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 4, in <module>\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/io/pytables.py"", line 349, in __getitem__\r\n    return self.get(key)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/io/pytables.py"", line 507, in get\r\n    return self._read_group(group)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/io/pytables.py"", line 1093, in _read_group\r\n    return s.read(**kwargs)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/io/pytables.py"", line 2247, in read\r\n    return Series(values, index=index, name=self.name)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/core/series.py"", line 657, in __init__\r\n    data = SingleBlockManager(data, index, fastpath=True)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/core/internals.py"", line 2942, in __init__\r\n    block = make_block(block, axis, axis, ndim=1, fastpath=True)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/core/internals.py"", line 1535, in make_block\r\n    return klass(values, items, ref_items, ndim=ndim, fastpath=fastpath, placement=placement)\r\n  File ""/users/is/pross/workspace/pandas/git/pandas/pandas/core/internals.py"", line 62, in __init__\r\n    % (len(items), len(values)))\r\nValueError: Wrong number of items passed 1, indices imply 0\r\n```\r\n\r\nI have fixed this locally and will put in a PR.'"
4707,18771334,wesm,jreback,2013-08-30 05:10:35,2014-02-17 02:35:55,2014-02-17 02:35:55,closed,,0.16.0,6,Bug;Data IO;IO JSON;Numeric,https://api.github.com/repos/pydata/pandas/issues/4707,b'pandas.json handling of integers in floating point arrays',"b'cc @Komnomnomnom \r\n\r\nI noticed a discrepancy between `JSON.stringify` in Chrome and both the built-in `json` package (python 2.7 at least) and `ultrajson`. In short the value `4.0`, say, encodes as `""4""` in Chrome and `""4.0""` in the others. In JavaScript-land, this has no practical impact, but I thought I\'d bring it up in case others felt we should tweak ultrajson to conform. '"
4705,18714713,davaco,jreback,2013-08-29 06:49:08,2013-09-07 21:28:09,2013-09-07 21:28:09,closed,,0.13,4,Bug;Performance;Regression;Visualization,https://api.github.com/repos/pydata/pandas/issues/4705,b'Plotting performance deterioration on DataFrames with date-index',"b'The below code-sample gives a big performance drop on plotting in pandas 0.12, compared to pandas 0.11:\r\n\r\nIn Pandas 0.12 : ""Ran in 0:00:29.542475 secs""\r\nIn Pandas 0.11 : ""Ran in 0:00:06.653506 secs""\r\n\r\nIt only happens on a date-indexed DataFrame:\r\n\r\n\r\nfrom pandas import *\r\nfrom numpy.random import randn\r\n\r\nN = 10000\r\nM = 25\r\n\r\ndf = DataFrame(randn(N,M), index=date_range(\'1/1/1975\', periods=N))\r\n\r\nt0 = datetime.now()\r\ndf.plot()\r\nprint(""Ran in %s secs"" % (datetime.now() - t0))'"
4692,18622518,hayd,hayd,2013-08-27 17:43:59,2013-08-27 17:56:10,2013-08-27 17:56:10,closed,,,2,Bug;Data IO;IO CSV,https://api.github.com/repos/pydata/pandas/issues/4692,b'read_csv example tricks parser dtypes',"b""Poster has an example which tricks read_csv into thinking column a is a int, but then throws it lots of strings (and it then infers 1s as strings).\r\n\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame({'a':['1']*100000 + ['X']*100000 + ['1']*100000, 'b':['b']*300000})\r\ndf.to_csv('test', sep='\\t', index=False, na_rep='NA')\r\ndf2 = pd.read_csv('test', sep='\\t')\r\nprint df2['a'].unique()\r\n```\r\n\r\nhttp://stackoverflow.com/questions/18471859/pandas-read-csv-dtype-inference-issue\r\n\r\nI think this is rather an edge case tbh. :)"""
4690,18620759,sboehler,jreback,2013-08-27 17:11:33,2014-06-14 14:30:36,2014-06-14 14:30:36,closed,,0.14.1,0,Bug;Frequency;Timezones,https://api.github.com/repos/pydata/pandas/issues/4690,b'DatetimeIndex.intersection should preserve timezone',"b""pandas' intersection method on DatetimeIndex should preserve the timezone. I had some cases where it works, and some where it doesn't. Example that does not work:\r\n\r\n    In [75]: a = pd.date_range('2012-01-01', '2012-01-20', freq='D', tz='CET')\r\n\r\n    In [76]: b = pd.date_range('2012-01-01', '2012-01-19', freq='D', tz='CET')\r\n\r\n    In [77]: a.intersection(b)\r\n    Out[77]: \r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2011-12-31 23:00:00, ..., 2012-01-18 23:00:00]\r\n    Length: 19, Freq: D, Timezone: None\r\n    \r\n    In [78]: pd.__version__\r\n    Out[78]: '0.12.0'\r\n\r\n\r\nI would expect Timezone: CET in the output of line [77], and the times to be correctly displayed in CET."""
4686,18589535,gdraps,jreback,2013-08-27 06:06:56,2013-09-10 19:54:07,2013-09-10 19:54:07,closed,jreback,0.13,1,Bug;Dtypes;Error Reporting;Indexing,https://api.github.com/repos/pydata/pandas/issues/4686,"b'""AttributeError: _ref_locs"" while assigning non-contiguous MultiIndex columns'","b'Hitting an exception with the following code on 0.12.0 (and current master).  Copied 0.11.0 behavior for reference, but feel free to reject if assignment to non-contiguous columns of a MultiIndex using a partial label is illegal/unsupported.\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n\r\n    df = pd.DataFrame(\r\n        np.ones((1, 3)),\r\n        columns=pd.MultiIndex.from_tuples(\r\n            [(\'A\', \'1\'), (\'B\', \'1\'), (\'A\', \'2\')]\r\n        ),\r\n        dtype=object\r\n    )\r\n    print \'Before:\'\r\n    print df.dtypes\r\n    df[\'A\'] = df[\'A\'].astype(float)\r\n    print \'After:\'\r\n    print df.dtypes\r\n\r\n0.11.0 output:\r\n\r\n    Before:\r\n    A  1    object\r\n    B  1    object\r\n    A  2    object\r\n    dtype: object\r\n    After:\r\n    A  1    float64\r\n    B  1     object\r\n    A  2    float64\r\n    dtype: object\r\n\r\n0.12.0 output:\r\n\r\n    Before:\r\n    A  1    object\r\n    B  1    object\r\n    A  2    object\r\n    dtype: object\r\n    Traceback (most recent call last):\r\n      File ""repro.py"", line 15, in <module>\r\n        df[\'A\'] = df[\'A\'].astype(float)\r\n      File ""/home/gmd/ENV/pandas-master/lib/python2.7/site-packages/pandas-0.12.0_270_ge3c71f2-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1929, in __setitem__\r\n        self._set_item(key, value)\r\n      File ""/home/gmd/ENV/pandas-master/lib/python2.7/site-packages/pandas-0.12.0_270_ge3c71f2-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1977, in _set_item\r\n        NDFrame._set_item(self, key, value)\r\n      File ""/home/gmd/ENV/pandas-master/lib/python2.7/site-packages/pandas-0.12.0_270_ge3c71f2-py2.7-linux-i686.egg/pandas/core/generic.py"", line 798, in _set_item\r\n        self._data.set(key, value)\r\n      File ""/home/gmd/ENV/pandas-master/lib/python2.7/site-packages/pandas-0.12.0_270_ge3c71f2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 2448, in set\r\n        self._reset_ref_locs()\r\n      File ""/home/gmd/ENV/pandas-master/lib/python2.7/site-packages/pandas-0.12.0_270_ge3c71f2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 1644, in _reset_ref_locs\r\n        self._rebuild_ref_locs()\r\n      File ""/home/gmd/ENV/pandas-master/lib/python2.7/site-packages/pandas-0.12.0_270_ge3c71f2-py2.7-linux-i686.egg/pandas/core/internals.py"", line 1652, in _rebuild_ref_locs\r\n        if self._ref_locs is not None:\r\n    AttributeError: _ref_locs\r\n\r\nThanks!'"
4678,18568899,hayd,jreback,2013-08-26 19:37:21,2013-09-26 01:03:38,2013-09-26 01:03:38,closed,,0.13,5,Bug;Data IO;IO CSV;Prio-high,https://api.github.com/repos/pydata/pandas/issues/4678,b'Dates are parsed with read_csv thousand seperator',"b""When reading a csv with a date column, the date is sometimes parsed as a number:\r\n```\r\nIn [1]: s = '06.02.2013;13:00;1.000,215;0,215;0,185;0,205;0,00'\r\n\r\nIn [2]: pd.read_csv(StringIO(s), sep=';', header=None, parse_dates={'Dates': [0, 1]}, index_col=0, decimal=',', thousands='.')\r\nOut[2]:\r\n                        2      3      4      5  6\r\nDates\r\n6022013 13:00   1.000,215  0.215  0.185  0.205  0\r\n```\r\nHere `06.02.2013` is read as a number `0602013` before the date is parsed (which fails)... I think [dates are sometimes written this way on the continent](http://en.wikipedia.org/wiki/Date_and_time_notation_in_Europe) (along with . thousands).\r\n\r\nThis was found in #4322 (but that issue was more about . being ignored), I guess another test case would be with `-`:\r\n\r\n```\r\nIn [3]: s = '06-02-2013;13:00;1.000,215;0,215;0,185;0,205;0,00'\r\n\r\nIn [4]: pd.read_csv(StringIO(s), sep=';', header=None, parse_dates={'Dates': [0, 1]}, decimal=',', thousands='-')\r\nOut[4]: \r\n           Dates          2      3      4      5  6\r\n0  6022013 13:00  1.000,215  0.215  0.185  0.205  0\r\n```\r\n\r\n@jreback suggests:\r\n>but it should ignore dates columns entirely (for thousands parsing...)\r\n\r\ncc #4598 @guyrt"""
4667,18514313,dalejung,jreback,2013-08-25 00:04:15,2014-07-03 14:52:51,2013-08-26 15:18:22,closed,,0.13,3,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/4667,b'Setting float_series[bool_index] = None errors',"b""```python \r\ns = pd.Series(range(10)).astype(float)\r\ns[s > 8] = None\r\ns\r\n```\r\n\r\n```\r\n/Users/datacaliber/Dropbox/Projects/trading/python/externals/pandas/pandas/core/internals.py in create_block(v, m, n, item, reshape)\r\n    596                 # change the dtype\r\n    597                 if nv is None:\r\n--> 598                     dtype, _ = com._maybe_promote(n.dtype)\r\n    599                     nv = v.astype(dtype)\r\n    600                     try:\r\n\r\nAttributeError: 'NoneType' object has no attribute 'dtype'\r\n```\r\n\r\nThis is on latest master."""
4636,18404545,jorisvandenbossche,cpcloud,2013-08-22 10:12:16,2014-06-05 01:13:47,2014-06-05 01:13:47,closed,,0.14.1,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/4636,b'Cannot update boxplot via axes',"b'When I construct a boxplot with pandas, e.g. like this:\r\n\r\n```python\r\ndf = pd.DataFrame({\'A\':[1,3,2,5,4,7,6], \'B\':[1,1,1,1,2,2,2]})\r\nfig, ax = plt.subplots()\r\ndf.boxplot(\'A\', \'B\', ax=ax)\r\n```\r\nI cannnot change the plot anymore via ``ax``, e.g. ``ax.set_ylim(0,20)`` or ``ax.set_ylabel(""Something"")`` has no effect on the figure.\r\n\r\nComplete example: http://nbviewer.ipython.org/5868420/bug_pandas-boxplot-set-ylim.ipynb\r\n\r\nI tested it on the dev version of pandas (and I know it worked previously, but cannot check which version exactly anymore).\r\n\r\n'"
4633,18390566,cpcloud,jreback,2013-08-22 01:38:33,2014-04-25 17:06:59,2013-08-31 17:51:58,closed,,0.13,22,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4633,b'Now boolean operators work with NDFrames?',"b'Awkward side effect of the new and improved `Series` probably, I typed something wrong and was shocked to not get the usual `numpy` `ValueError` barf:\r\n\r\n```\r\nIn [57]: a, b = Series(rand(10) > 0.5), Series(rand(10) > 0.5)\r\n\r\nIn [58]: a\r\nOut[58]:\r\n0     True\r\n1     True\r\n2    False\r\n3    False\r\n4     True\r\n5    False\r\n6    False\r\n7     True\r\n8     True\r\n9     True\r\ndtype: bool\r\n\r\nIn [59]: b\r\nOut[59]:\r\n0     True\r\n1     True\r\n2     True\r\n3     True\r\n4    False\r\n5     True\r\n6    False\r\n7     True\r\n8     True\r\n9     True\r\ndtype: bool\r\n\r\nIn [60]: a and b # what?! no way!!\r\nOut[60]:\r\n0     True\r\n1     True\r\n2     True\r\n3     True\r\n4    False\r\n5     True\r\n6    False\r\n7     True\r\n8     True\r\n9     True\r\ndtype: bool\r\n\r\nIn [61]: a & b # whew\r\nOut[61]:\r\n0     True\r\n1     True\r\n2    False\r\n3    False\r\n4    False\r\n5    False\r\n6    False\r\n7     True\r\n8     True\r\n9     True\r\ndtype: bool\r\n```'"
4626,18383120,jseabold,jreback,2013-08-21 21:57:28,2013-08-26 15:21:56,2013-08-26 15:21:56,closed,,0.13,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/4626,b'BUG: read_stata ignoring encoding?',"b'I don\'t have time to debug right now, and maybe my expectations are just off, but it looks like `read_stata` doesn\'t respect the encoding keyword. I\'m also not sure it\'s needed. AFAIK, Stata doesn\'t (and likely won\'t) support unicode. It always uses latin-1, so we can always use the latin-1 encoding for strings (maybe not desirable though). \r\n\r\nhttps://www.dropbox.com/s/hq42trq4327ker8/encoding_issue.dta\r\n\r\n```\r\ndta = pd.read_stata(""./encoding_issue.dta"")\r\ndta.head()\r\n\r\ndta = pd.read_stata(""./encoding_issue.dta"", encoding=""latin-1"")\r\ndta.head()\r\n\r\ndta = pd.read_stata(""./encoding_issue.dta"")\r\ndta.kreis1849.str.decode(""latin-1"")\r\n```'"
4621,18362038,hayd,jreback,2013-08-21 15:38:17,2013-10-15 00:55:05,2013-10-15 00:55:05,closed,,0.13,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/4621,b'Groupby filter changes ordering',"b""At the moment filter reorders wrt the groups.\r\n\r\nExample:\r\n\r\n```\r\nIn [1]: data = pd.DataFrame(\r\n    {'pid' : [1,1,1,2,2,3,3,3],\r\n     'tag' : [23,45,62,24,45,34,25,62],\r\n     })\r\n\r\nIn [2]: g = data.groupby('tag')\r\n\r\nIn [3]: g.filter(lambda x: len(x) > 1)\r\nOut[3]: \r\n   pid  tag\r\n1    1   45\r\n4    2   45\r\n2    1   62\r\n7    3   62\r\n```\r\nIf there is a way to efficiently keep the order that would be ideal I think, failing that sort back afterwards (but being wary of sorting with dupe/unordered indexes).\r\n\r\ncc #3680"""
4620,18361750,hayd,jreback,2013-08-21 15:33:38,2013-11-01 18:34:35,2013-11-01 18:34:35,closed,,0.13,6,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/4620,"b""Groupby filter doesn't work with repeated index""","b""Example:\r\n```\r\nIn [1]: data = pandas.DataFrame(\r\n    {'pid' : [1,1,1,2,2,3,3,3],\r\n     'tag' : [23,45,62,24,45,34,25,62],\r\n     }, index=[0] * 8)\r\n\r\nIn [2]: g = data.groupby('tag')\r\n\r\nIn [3]: g.filter(lambda x: len(x) > 1)\r\nException: Reindexing only valid with uniquely valued Index objects\r\n```\r\ncc #3680\r\n\r\n*(I had thought there was also a similar behaviour with transform, but that seems ok.)*"""
4619,18336072,gdraps,jreback,2013-08-21 04:54:02,2013-08-21 18:22:24,2013-08-21 18:22:24,closed,,0.13,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4619,b'Unexpected result/segmentation fault with .ix/.loc (0.12.0)',"b'Hitting a seg fault on a specific usage of .ix/.loc with release 0.12.0 (and current master).  Any help is greatly appreciated.  Let me know if I can provide any more info.\r\n\r\nHere\'s an example produced with pandas 0.12.0 and numpy 1.7.1.\r\n\r\n    >>> df = pd.DataFrame({""A"": [0, 1, 2]})\r\n    >>> print df.ix[[0,8,0]]  # unexpected results\r\n                  A\r\n    0  0.000000e+00\r\n    8           NaN\r\n    0  4.677219e+12\r\n    >>> df = pd.DataFrame({""A"": list(\'abc\')})\r\n    >>> print df.ix[[0,8,0]]\r\n    Segmentation fault (core dumped)\r\n\r\n(.loc in place of .ix fails in the same way)\r\n\r\nFor comparison, here\'s the behavior with 0.11.0:\r\n\r\n    >>> df = pd.DataFrame({""A"": [0, 1, 2]})\r\n    >>> print df.ix[[0,8,0]]\r\n        A\r\n    0   0\r\n    8 NaN\r\n    0   0\r\n    >>> df = pd.DataFrame({""A"": list(\'abc\')})\r\n    >>> print df.ix[[0,8,0]]\r\n         A\r\n    0    a\r\n    8  NaN\r\n    0    a\r\n\r\nSystem info\r\n\r\n    i686 GNU/Linux\r\n\r\nPython info\r\n\r\n    Python 2.7.3 (default, Apr 10 2013, 05:09:49) \r\n    [GCC 4.7.2] on linux2\r\n'"
4605,18243829,alefnula,jreback,2013-08-19 14:53:41,2013-08-21 23:46:11,2013-08-21 23:46:11,closed,,0.13,12,API Design;Bug;Internals,https://api.github.com/repos/pydata/pandas/issues/4605,b'Inconsistent behaviour of rename for MultiIndexed DataFrames and Series',"b'If I try to rename a MultiIndexed DataFrame, everything works correctly:\r\n```python\r\nIn [3]: pd.DataFrame([11,21,31],\r\n            index=pd.MultiIndex.from_tuples([(""A"",x) for x in [""a"",""B"",""c""]])).rename(str.lower)\r\nOut[3]: \r\n      0\r\na a  11\r\n  b  21\r\n  c  31\r\n```\r\nBut if I do the same thing with the Series, it doesn\'t work:\r\n```python\r\nIn [4]: pd.Series([11,21,31],\r\n            index=pd.MultiIndex.from_tuples([(""A"",x) for x in [""a"",""B"",""c""]])).rename(str.lower)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-aa25b537c1df> in <module>()\r\n----> 1 pd.Series([11,21,31],index=pd.MultiIndex.from_tuples([(""A"",x) for x in [""a"",""B"",""c""]])).rename(str.lower)\r\n\r\n/home/viktor/pandas/pandas/core/series.py in rename(self, mapper, inplace)\r\n   3187         result = self if inplace else self.copy()\r\n   3188         result.index = Index([mapper_f(x)\r\n-> 3189                              for x in self.index], name=self.index.name)\r\n   3190 \r\n   3191         if not inplace:\r\n\r\nTypeError: descriptor \'lower\' requires a \'str\' object but received a \'tuple\'\r\n```\r\n\r\nThis behavior is observed in 0.12 and in the master branch.'"
4604,18216392,alefnula,jreback,2013-08-18 22:45:56,2013-08-21 12:44:43,2013-08-21 12:44:43,closed,,0.13,4,Bug,https://api.github.com/repos/pydata/pandas/issues/4604,b'BUG: reindex strange behaviour',"b""When I try to reindex a Series object I get a really weird result.\r\n\r\n```python\r\n>>> s = pd.Series([1,2,3,4,5], index=['a', 'b', 'c', 'd', 'e'])\r\n>>> s.reindex(['a', 'g', 'c', 'f'], method='ffill')\r\na     1\r\ng     5\r\nc   NaN\r\nf     5\r\ndtype: float64\r\n```\r\nAccording to the documentation I should have got something like:\r\n```python\r\na   1\r\ng   1\r\nc   3\r\nf   3\r\ndtype: float64\r\n```\r\nSo either the documentation is not valid, or reindex is not working correctly or I haven't understood how this beast works?\r\n"""
4601,18209640,y-p,jreback,2013-08-18 16:19:58,2013-09-20 22:34:04,2013-09-20 22:34:04,closed,,0.13,1,Bug,https://api.github.com/repos/pydata/pandas/issues/4601,b'Type inference code coerces float column into datetime',"b'```\r\nIn [10]: ix = [-352.737091, 183.575577]\r\n    ...: df=pd.DataFrame([0,1],index=ix)\r\n    ...: df.to_csv(""/tmp/1.csv"")\r\n    ...: df2=pd.DataFrame.from_csv(""/tmp/1.csv"",parse_dates=True)\r\n    ...: print df\r\n    ...: print df2\r\n             0\r\n-352.737091  0\r\n 183.575577  1\r\n                            0\r\n2105-11-21 22:43:41.128654  0\r\n1936-11-21 22:43:41.128654  1\r\n```\r\n\r\nMoved from https://github.com/pydata/pandas/issues/3171.\r\n '"
4596,18201612,cancan101,jreback,2013-08-18 00:58:17,2013-09-19 12:24:50,2013-09-19 12:24:50,closed,jtratner,0.13,1,Bug;Data IO;IO CSV,https://api.github.com/repos/pydata/pandas/issues/4596,b'PythonParser::_check_thousands appears broken',"b'This code appears broken:\r\n```\r\n    def _check_thousands(self, lines):\r\n        if self.thousands is None:\r\n            return lines\r\n        nonnum = re.compile(\'[^-^0-9^%s^.]+\' % self.thousands)\r\n        ret = []\r\n        for l in lines:\r\n            rl = []\r\n            for x in l:\r\n                if (not isinstance(x, compat.string_types) or\r\n                    self.thousands not in x or\r\n                        nonnum.search(x.strip())):\r\n                    rl.append(x)\r\n                else:\r\n                    rl.append(x.replace(\',\', \'\'))\r\n            ret.append(rl)\r\n        return ret\r\n```\r\n\r\nIt looks like the ```thousands``` argument to the class is used to check if the value is ""non numeric"" but then a hard coded comma is used when actually performing the cleaning.\r\n\r\nIn addition to fixing this, I would recommend factoring out this method so that it can be used elsewhere.'"
4584,18166985,bluefir,jreback,2013-08-16 16:53:24,2013-08-26 12:43:52,2013-08-26 12:43:52,closed,,0.13,5,API Design;Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4584,"b""BUG: 0.12.0 using DataFrame.to_hdf() with mode='a' does not append the data""","b'``to_hdf(....., append=True,table=True)`` raising, should accept this\r\n\r\n\r\nIs there a way to use DataFrame.to_hdf() so that data can be appened the data? Somehow to_hdf(filename, table_name, mode=\'a\', table=True) overwrites the data while to_hdf(filename, table_name, table=True, append=True) gives the following error:\r\n\r\n>Traceback (most recent call last):\r\n  File ""M:/Projects/PortfolioAnalytics/Code/Python/attribution/TopContributors.py"", line 1035, in <module>\r\n    portfolio_data.to_hdf(filename, OUTPUT_NAME_PORTFOLIO, table=True, append=Settings.TC_OUTPUT_APPEND)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.py"", line 521, in to_hdf\r\n    return pytables.to_hdf(path_or_buf, key, self, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 194, in to_hdf\r\n    f(store)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 188, in <lambda>\r\n    f = lambda store: store.append(key, value, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 658, in append\r\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\r\nTypeError: _write_to_group() got multiple values for keyword argument \'table\'\r\n'"
4576,18092581,ssalonen,jreback,2013-08-15 06:27:50,2016-07-16 22:41:18,2013-08-26 12:45:17,closed,,0.13,16,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4576,b'Pandas 0.12 unexpected results when comparing dataframe to list or tuple',"b""It seems that when comparing DataFrame to list or tuple of values (lenght of DataFrame columns), the resulting boolean DataFrame is incorrect.\r\n\r\n```python\r\n>>> import pandas\r\n>>> print pandas.__version__\r\n0.12.0\r\n>>> import numpy as np\r\n>>> print np.__version__\r\n1.6.1\r\n>>> df=pandas.DataFrame([ [-1, 0], [1, 2] ])\r\n>>> df > (0, 1)\r\n       0      1\r\n0  False  False\r\n1  False   True\r\n>>> df > [0, 1]\r\n       0      1\r\n0  False  False\r\n1  False   True\r\n\r\n``'\r\n\r\nComparison with numpy array works as expected:\r\n```python\r\n>>> df > np.array([0, 1])\r\n       0      1\r\n0  False  False\r\n1   True   True\r\n``'\r\n\r\nNote that the comparison behaved correctly at least in pandas 0.9.0:\r\n```python\r\n>>> import pandas\r\n>>> print pandas.__version__\r\n0.9.0\r\n>>> df=pandas.DataFrame([ [-1, 0], [1, 2] ])\r\n>>> df > (0, 1)\r\n       0      1\r\n0  False  False\r\n1   True   True\r\n>>> df > [0, 1]\r\n       0      1\r\n0  False  False\r\n1   True   True\r\n``'\r\n"""
4564,18052634,filmor,jreback,2013-08-14 13:42:16,2015-01-06 14:43:46,2013-09-26 00:49:44,closed,,0.13,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4564,b'BUG: DatetimeIndex uses the wrong union if the operands overlap',"b'This pretty much boils down to this line: https://github.com/pydata/pandas/blob/master/pandas/tseries/index.py#L987\r\n\r\nYou can easily reproduce the error using the following code:\r\n\r\n    r = pd.date_range(""2013-01-01"", ""2013-02-01"")\r\n    r2 = r + pd.DateOffset(minutes=15)\r\n    r | r2 # Results in the index r having ""2013-01-31 00:15"" appended\r\n\r\nThis happens because the check on the given line does not take into account, that two indices with the same offset do not necessarily have to be aligned. If they are the `_fast_union`-method can be used, if not it should fall back to the `Index` implementation.'"
4561,18039205,Puggie,jreback,2013-08-14 07:49:06,2013-08-15 11:50:48,2013-08-15 11:50:48,closed,,0.13,0,Bug,https://api.github.com/repos/pydata/pandas/issues/4561,b'BUG: read_clipboard fails on Windows since pandas 0.12',"b'Since I updated pandas from version 0.11 to 0.12, read_clipboard fails:\r\n\r\n    import pandas as pd    \r\n    df = pd.read_clipboard()\r\nResults in:\r\n\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-2-6dead334eb54> in <module>()\r\n    ----> 1 df = pd.read_clipboard()\r\n    \r\n    C:\\Python33\\lib\\site-packages\\pandas\\io\\clipboard.py in read_clipboard(**kwargs)\r\n         16     from pandas.io.parsers import read_table\r\n         17     text = clipboard_get()\r\n    ---> 18     return read_table(StringIO(text), **kwargs)\r\n         19 \r\n         20 \r\n    \r\n    TypeError: initial_value must be str or None, not bytes\r\n\r\nWhat I did was:\r\n\r\n- Open a csv file in Excel 2010\r\n\r\n- Copy a range of cells, including headers\r\n\r\n- Perform read_clipboard in iPython Qt console as described in above code block\r\n\r\nAfter downgrading to 0.11, this procedure worked fine again.\r\n\r\nOS: Win7, 64bit\r\nPython: v3.3.2 32-bit\r\nPandas: 0.12.0\r\nNumpy: 1.7.1\r\n\r\n'"
4554,18004298,goyodiaz,jreback,2013-08-13 15:54:09,2013-10-24 11:52:14,2013-09-29 22:07:31,closed,,0.13,5,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/4554,b'API: Reshaping Series to its own shape raises TypeError',"b'This may be seen like a weird function call but it is actually triggered when plotting with development versions of matplotlib and the case is explicitly (and wrongly as it seems) by pandas:\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> x = pd.Series(range(5))\r\n>>> x.reshape(x.shape)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/lib/pymodules/python2.7/pandas/core/series.py"", line 981, in reshape\r\n    return ndarray.reshape(self, newshape, order)\r\nTypeError: an integer is required\r\n```\r\n\r\nIt is easily fixed just replacing the offending line by this one:\r\n\r\n```return ndarray.reshape(self, newshape, order=order)```\r\n'"
4550,17994747,jgehrcke,jreback,2013-08-13 12:55:03,2013-09-09 02:03:14,2013-09-09 02:03:14,closed,,0.13,4,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/4550,"b'\'ValueError: Cannot assign nan to integer series"" when calling `where` on Series with non-unique index.'","b'Create a Series with non-unique index:\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n\'0.12.0\'\r\n>>> s1 = pd.Series(range(3))\r\n>>> s2 = pd.Series(range(3))\r\n>>> comb = pd.concat([s1,s2])\r\n>>> comb\r\n0    0\r\n1    1\r\n2    2\r\n0    0\r\n1    1\r\n2    2\r\ndtype: int64\r\n```\r\n\r\nAs of https://github.com/pydata/pandas/issues/4548 I cannot use `comb[comb<2] =+ 10`, so I tried working with `where` (according to http://pandas.pydata.org/pandas-docs/stable/indexing.html#where-and-masking, stating ""To guarantee that selection output has the same shape as the original data, you can use the where method in Series and DataFrame""). But already calling `where` on `comb` is problematic:\r\n\r\n```python\r\n>>> comb.where(comb < 2)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/projects/bioinfp_apps/Python-2.7.3/lib/python2.7/site-packages/pandas/core/series.py"", line 745, in where\r\n    ser._set_with(~cond, other)\r\n  File ""/projects/bioinfp_apps/Python-2.7.3/lib/python2.7/site-packages/pandas/core/series.py"", line 886, in _set_with\r\n    self._set_values(key, value)\r\n  File ""/projects/bioinfp_apps/Python-2.7.3/lib/python2.7/site-packages/pandas/core/series.py"", line 904, in _set_values\r\n    values[key] = _index.convert_scalar(values, value)\r\n  File ""index.pyx"", line 547, in pandas.index.convert_scalar (pandas/index.c:9752)\r\n  File ""index.pyx"", line 560, in pandas.index.convert_scalar (pandas/index.c:9639)\r\nValueError: Cannot assign nan to integer series\r\n>>> \r\n```\r\n\r\nIs this expected?\r\n\r\n\r\n'"
4548,17994399,jgehrcke,jreback,2013-08-13 12:46:32,2013-12-04 00:40:35,2013-09-09 14:52:24,closed,jtratner,0.13,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4548,"b'BUG: Series with non-unique index: ""Index length did not match values"" error upon assignment'","b'Create a Series with non-unique index:\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n\'0.12.0\'\r\n>>> s1 = pd.Series(range(3))\r\n>>> s2 = pd.Series(range(3))\r\n>>> comb = pd.concat([s1,s2])\r\n>>> comb\r\n0    0\r\n1    1\r\n2    2\r\n0    0\r\n1    1\r\n2    2\r\ndtype: int64\r\n```\r\n\r\nAssign value by boolean mask:\r\n```python\r\n>>> comb[comb<1] = 5\r\n>>> comb\r\n0    5\r\n1    1\r\n2    2\r\n0    5\r\n1    1\r\n2    2\r\ndtype: int64\r\n```\r\n\r\nThis has worked. Now *add* a value by boolean mask:\r\n```python\r\n>>> comb[comb<2] += 10\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""***/pandas/core/series.py"", line 852, in __setitem__\r\n    self.where(~key,value,inplace=True)\r\n  File ""***/pandas/core/series.py"", line 749, in where\r\n    other = other.reindex(ser.index)\r\n  File ""***/pandas/core/series.py"", line 2646, in reindex\r\n    return self._reindex_with_indexers(new_index, indexer, copy=copy, fill_value=fill_value)\r\n  File ""***/pandas/core/series.py"", line 2650, in _reindex_with_indexers\r\n    return Series(new_values, index=index, name=self.name)\r\n  File ""***/pandas/core/series.py"", line 492, in __new__\r\n    subarr.index = index\r\n  File ""properties.pyx"", line 74, in pandas.lib.SeriesIndex.__set__ (pandas/lib.c:29541)\r\nAssertionError: Index length did not match values\r\n```\r\n\r\nIs this expected behavior? If it is, I am sorry, because this was not clear to me from the docs and I am just wondering why simple assignment via `=` works and special assignment via `+=` does not...\r\n\r\n\r\n\r\n\r\n'"
4547,17984904,mvds314,jreback,2013-08-13 08:32:19,2014-05-22 17:28:50,2014-03-07 00:04:40,closed,,0.14.0,23,Bug;Frequency,https://api.github.com/repos/pydata/pandas/issues/4547,b'BUG: TimeStamp looses frequency info on arithmetic ops',"b'Running the follow code generates an error using pandas 0.11 (with winpython 2.7.5.2 64 bit):\r\nimport pandas as pd\r\nts1=pd.date_range(\'1/1/2000\',periods=1,freq=\'Q\')[0]\r\nts1-1-1\r\n\r\nError:\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""tslib.pyx"", line 566, in pandas.tslib._Timestamp.__sub__ (pandas\\tslib.c:10775)\r\n  File ""tslib.pyx"", line 550, in pandas.tslib._Timestamp.__add__ (pandas\\tslib.c:10477)\r\nValueError: Cannot add integral value to Timestamp without offset.\r\n\r\nReason:\r\nts1.freq has value:\r\n1 QuarterEnd: startingMonth=12, kwds={\'startingMonth\': 12}, offset=3 MonthEnds\r\nbut\r\n(ts1-1).freq has no value'"
4544,17962350,alefnula,jreback,2013-08-12 20:16:47,2013-10-24 12:57:37,2013-10-01 01:21:47,closed,jreback,0.13,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4544,b'BUG: .ix get and set behave differently',"b'I\'m really sorry if this is a duplicate issue of #2997, but it seems to me that this is a little bit different. Here the `.ix` when getting a value interprets the integer as label, and when setting a value it interprets it as a positional index.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame(np.arange(16).reshape((4, 4)),\r\n                      columns=[\'a\', \'b\', 8, \'c\'],\r\n                      index=[\'e\', 7, \'f\', \'g\'])\r\n>>> df\r\n    a   b   8   c\r\ne   0   1   2   3\r\n7   4   5   6   7\r\nf   8   9  10  11\r\ng  12  13  14  15\r\n>>> df.ix[\'e\', 8]\r\n2\r\n>>> df.ix[\'e\', 8] = 42\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/core/indexing.py"", line 90, in __setitem__\r\n    self._setitem_with_indexer(indexer, value)\r\n  File ""pandas/core/indexing.py"", line 190, in _setitem_with_indexer\r\n    values[indexer] = value\r\nIndexError: index 8 is out of bounds for axis 1 with size 4\r\n>>> df.loc[\'e\', 8] = 42\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/core/indexing.py"", line 90, in __setitem__\r\n    self._setitem_with_indexer(indexer, value)\r\n  File ""pandas/core/indexing.py"", line 190, in _setitem_with_indexer\r\n    values[indexer] = value\r\nIndexError: index 8 is out of bounds for axis 1 with size 4\r\n```\r\n\r\nI see this issue with the 0.12 version and also with the latest git checkout.\r\n\r\n_Update: @jreback noticed that the `.loc` is not working either._'"
4538,17913258,cpcloud,cpcloud,2013-08-11 14:46:56,2013-09-28 17:26:33,2013-09-28 17:26:33,closed,,0.13.1,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/4538,"b""fill_between doesn't work with Series objects""","b""[SO question](http://stackoverflow.com/questions/18171079/using-fill-between-with-a-pandas-data-series/18173133#18173133)\r\n\r\nif instead you don't use `.values` then a `KeyError` is thrown"""
4533,17899015,jreback,jreback,2013-08-10 13:56:01,2014-09-13 22:59:30,2014-09-13 22:59:30,closed,,0.15.0,1,API Design;Bug;Internals;Timedelta,https://api.github.com/repos/pydata/pandas/issues/4533,b'BUG: diff on a DataFrame with a datelike broken',"b""```\r\nIn [10]: df = DataFrame(dict(time = [Timestamp('20130101 9:01'),Timestamp('20130101 9:02')],value=[1.0,2.0]))\r\n\r\nIn [11]: df\r\nOut[11]: \r\n                 time  value\r\n0 2013-01-01 09:01:00      1\r\n1 2013-01-01 09:02:00      2\r\n```\r\n\r\nworks on series\r\n```\r\nIn [12]: df.time.diff()\r\nOut[12]: \r\n0        NaT\r\n1   00:01:00\r\nName: time, dtype: timedelta64[ns]\r\n```\r\n\r\nBroken with a datelike dtype inclded\r\n```\r\nIn [13]: df.diff()\r\nValueError: cannot include dtype 'm' in a buffer\r\n```"""
4532,17898561,jreback,jreback,2013-08-10 13:09:31,2014-07-17 22:05:20,2013-08-13 22:19:43,closed,,0.13,0,Bug;Enhancement;Timedelta,https://api.github.com/repos/pydata/pandas/issues/4532,b'BUG/API: timedelta64 series should operate directly with np.timedelta64/pd.offsets',"b""Related #4134 and:\r\n\r\nhttp://stackoverflow.com/questions/18159675/python-pandas-change-duplicate-timestamp-to-unique/18162121#18162121\r\n\r\n```\r\nIn [81]: df = DataFrame(dict(time = [Timestamp('20130101 9:01'),Timestamp('20130101 9:02')]))\r\n```\r\n\r\nThis is buggy\r\n```\r\nIn [82]: df.time + np.timedelta64(1,'ms')\r\nOut[82]: \r\n0   2013-01-01 09:01:00.000000001\r\n1   2013-01-01 09:02:00.000000001\r\nName: time, dtype: datetime64[ns]\r\n\r\nIn [84]: df.time + np.timedelta64(1,'s')\r\nOut[84]: \r\n0   2013-01-01 09:01:00.000000001\r\n1   2013-01-01 09:02:00.000000001\r\nName: time, dtype: datetime64[ns]\r\n```\r\n\r\nDoesn't work\r\n```\r\nIn [83]: df.time + pd.offsets.Milli(5)\r\nValueError: cannot operate on a series with out a rhs of a series/ndarray of type datetime64[ns] or a timedelta\r\n```\r\n\r\nBut this is ok\r\n```\r\nIn [86]: df.time.apply(lambda x: x + pd.offsets.Milli(5))\r\nOut[86]: \r\n0   2013-01-01 09:01:00.005000\r\n1   2013-01-01 09:02:00.005000\r\nName: time, dtype: datetime64[ns]\r\n```\r\n\r\nThis is ok\r\n```\r\nIn [85]: df.time + timedelta(seconds=1)\r\nOut[85]: \r\n0   2013-01-01 09:01:01\r\n1   2013-01-01 09:02:01\r\nName: time, dtype: datetime64[ns]\r\n```\r\n"""
4528,17885281,brechea,jreback,2013-08-09 21:05:54,2014-02-18 19:46:30,2014-02-18 19:46:30,closed,,0.14.0,6,API Design;Bug;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/4528,"b""BUG/API: can't pass parameters to csv module via df.to_csv""","b'Trying to print a data frame as plain, strict tsv (i.e., no quoting and no escaping, because I know none the fields will contain tabs), I wanted to use the ""quoting"" option, which is documented in pandas and is passed through to csv, as well as the ""quotechar"" option, not documented in pandas but also a csv option. But it doesn\'t work:\r\n\r\n```python\r\nIn [1]: import sys, csv\r\n\r\nIn [2]: from pandas import DataFrame\r\n\r\nIn [3]: data = {\'col1\': [\'contents of col1 row1\', \'contents "" of col1 row2\'], \'col2\': [\'contents of col2 row1\', \'contents "" of col2 row2\'] }\r\n\r\nIn [4]: df = DataFrame(data)\r\n\r\nIn [5]: df.to_csv(sys.stdout, sep=\'\\t\', quoting=csv.QUOTE_NONE, quotechar=None)\r\n    col1  col2\r\n0    contents of col1 row1  contents of col2 row1\r\n---------------------------------------------------------------------------\r\nError                   Traceback (most recent call last)\r\n<ipython-input-5-a30d32266fb4> in <module>()\r\n----> 1 df.to_csv(sys.stdout, sep=\'\\t\', quoting=csv.QUOTE_NONE, quotechar=None)\r\n\r\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/frame.pyc in to_csv(self, path_or_buf, sep, na_rep, float_format, cols, header, index, index_label, mode, nanRep, encoding, quoting, line_terminator, chunksize, tupleize_cols, **kwds)\r\n 1409                   tupleize_cols=tupleize_cols,\r\n 1410                   )\r\n-> 1411     formatter.save()\r\n 1412\r\n 1413   def to_excel(self, excel_writer, sheet_name=\'sheet1\', na_rep=\'\',\r\n\r\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/format.pyc in save(self)\r\n  974\r\n  975       else:\r\n--> 976         self._save()\r\n  977\r\n  978\r\n\r\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/format.pyc in _save(self)\r\n 1080         break\r\n 1081\r\n-> 1082       self._save_chunk(start_i, end_i)\r\n 1083\r\n 1084   def _save_chunk(self, start_i, end_i):\r\n\r\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/format.pyc in _save_chunk(self, start_i, end_i)\r\n 1098     ix = data_index.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)\r\n 1099\r\n-> 1100     lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)\r\n 1101\r\n 1102 # from collections import namedtuple\r\n\r\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/lib.so in pandas.lib.write_csv_rows (pandas/lib.c:13871)()\r\n\r\nError: need to escape, but no escapechar set\r\n```\r\n\r\nAdding the parameter\r\n\r\nquotechar=kwds.get(""quotechar"")\r\n\r\nto the\r\n\r\nformatter = fmt.CSVFormatter(...\r\n\r\ncall in to_csv(), and doing corresponding changes to format.CSVFormatter()\'s __init__() and save(), produces the expected output:\r\n\r\n```python\r\nIn [1]: import sys, csv\r\n\r\nIn [2]: from pandas import DataFrame\r\n\r\nIn [3]: data = {\'col1\': [\'contents of col1 row1\', \'contents "" of col1 row2\'], \'col2\': [\'contents of col2 row1\', \'contents "" of col2 row2\'] }\r\n\r\nIn [4]: df = DataFrame(data)\r\n\r\nIn [5]: df.to_csv(sys.stdout, sep=\'\\t\', quoting=csv.QUOTE_NONE, quotechar=None)\r\n        col1    col2\r\n0       contents of col1 row1   contents of col2 row1\r\n1       contents "" of col1 row2 contents "" of col2 row2\r\n```\r\n\r\ni.e., unescaped, unquoted tsv.\r\n\r\nMore generally, there could be many reasons to want more control of the underlying csv writer, so a generic mechanism (as opposed to adding each param one by one) might be called for (e.g., allowign for a csv dialect object or at least a dictionary holding dialect attributes).\r\n'"
4519,17852291,prossahl,jreback,2013-08-09 08:31:41,2013-08-15 17:23:58,2013-08-15 17:23:58,closed,,0.13,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4519,b'Segfault with pd.tslib.get_period_field',"b""```python\r\n>>> import pandas as pd\r\n>>> pd.tslib.get_period_field(11, 1, 1)\r\nException ValueError: ValueError('Unrecognized code: 11',) in 'pandas.tslib._get_accessor_func' ignored\r\nSegmentation fault\r\n```\r\n\r\nI'll push a fix."""
4516,17829521,highpost,jreback,2013-08-08 20:34:29,2013-08-17 15:39:38,2013-08-09 14:36:39,closed,,0.13,13,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4516,b'MultiIndexes and large CSV files',"b'I\'ve run into a data-dependent bug with MultiIndex. Attached are a test script and a data file generation script:\r\n\r\n  * sales-data.py loads CSV data from a CSV data file with read_csv and then displays a multiindexed subset.\r\n  * sales-gen.sh is an AWK script that generates CSV test files. By default, it creates a 600MB file, which causes the failure. To make a smaller test file, which succeeds, just use ""head -1000"".\r\n\r\nMy problem is that sales-data.py breaks with large CSV files but works with smaller CSV files. When it fails it displays the following error message:\r\n    \r\n    \'MultiIndex lexsort depth 0, key was length 4\'\r\n\r\nI know that MultiIndexes must be pre-sorted. So sales-gen.sh makes sure that the first four columns in the auto-generated CSV files are ordered. BTW, I\'m not sure why lexsort is getting called.\r\n\r\n# sales-data.py\r\n\r\n    #!/usr/bin/env python\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    from memory_profiler import profile\r\n\r\n    pd.set_option(\'display.height\',            400)\r\n    pd.set_option(\'display.width\',             400)\r\n    pd.set_option(\'display.max_rows\',         1000)\r\n    pd.set_option(\'display.max_columns\',        30)\r\n    pd.set_option(\'display.line_width\',        200)\r\n\r\n    # @profile\r\n    def load_data():\r\n        try:\r\n            df = pd.read_csv(\r\n                \'./sales-large.csv\',\r\n              # \'./sales-small.csv\',\r\n                header = None,\r\n                na_values = [\'NULL\'],\r\n                names = [\r\n                    \'salesperson\',\r\n                    \'customer\',\r\n                    \'invoice_date\',\r\n                    \'ship_date\',\r\n                    \'product\',\r\n                    \'quantity\',\r\n                    \'price\',\r\n                ],\r\n                index_col = [\r\n                    \'salesperson\',\r\n                    \'customer\',\r\n                    \'invoice_date\',\r\n                    \'ship_date\',\r\n                ],\r\n                parse_dates = [\r\n                    \'invoice_date\',\r\n                    \'ship_date\',\r\n                ],\r\n            )\r\n\r\n            print(df.loc[(\r\n                \'A00000\',                        # salesperson\r\n                \'A\',                             # customer\r\n                pd.datetime(2011,3,1,0,0,0),     # invoice_date\r\n                pd.datetime(2011,3,6,0,0,0),     # ship_date\r\n            )])\r\n\r\n        except Exception as e:\r\n            print(e)\r\n\r\n    if __name__== \'__main__\':\r\n        load_data()\r\n\r\n# sales-gen.sh\r\n\r\n    #!/usr/bin/env bash\r\n\r\n    # \'salesperson\',\r\n    # \'customer\',\r\n    # \'invoice_date\',\r\n    # \'ship_date\',\r\n    # \'product\',\r\n    # \'quantity\',\r\n    # \'price\',\r\n\r\n    awk ""BEGIN {                                                                                              \\\r\n              first = 65;                                                                                     \\\r\n              last = 91;                                                                                      \\\r\n              s_char = first;                                                                                 \\\r\n              c_char = first;                                                                                 \\\r\n                                                                                                              \\\r\n              n_sa = 100;                                                                                     \\\r\n              n_cu = 100;                                                                                     \\\r\n              n_dt = 10;                                                                                      \\\r\n              n_pr = 100;                                                                                     \\\r\n                                                                                                              \\\r\n              for (i = 0; i < n_sa; i++ s_char++) {                                                           \\\r\n                  if (s_char == last) {                                                                       \\\r\n                      s_char = first;                                                                         \\\r\n                  }                                                                                           \\\r\n                  c_char = first;                                                                             \\\r\n                  for (j = 0; j < n_cu; j++ c_char++) {                                                       \\\r\n                      if (c_char == last) {                                                                   \\\r\n                          c_char = first;                                                                     \\\r\n                      }                                                                                       \\\r\n                      for (k = 1; k <= n_dt; k++) {                                                           \\\r\n                          for (l = 0; l < n_pr; l++) {                                                        \\\r\n                              printf(\\""%c%05d,%c,2011-03-%02d 00:00:00,2011-03-%02d 00:00:00,%d,%d,%f\\n\\"",    \\\r\n                                  s_char, i, c_char, k, k+5, l, 1 + int(rand() * 10), rand())                 \\\r\n                          }                                                                                   \\\r\n                      }                                                                                       \\\r\n                  }                                                                                           \\\r\n              }                                                                                               \\\r\n          }""\r\n'"
4504,17757793,mamikonyan,jreback,2013-08-07 16:19:48,2013-08-12 13:23:49,2013-08-12 13:23:49,closed,,0.13,5,Bug;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4504,b'read_hdf() requires write permission',"b""read_hdf() called get_store() with default open mode, which is 'a', and so requires write permissions to read a file. Please change to 'r'."""
4493,17727149,fonnesbeck,cpcloud,2013-08-07 02:32:17,2014-06-12 21:35:37,2014-02-27 17:48:47,closed,cpcloud,0.14.0,34,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/4493,b'KeyError when generating scatter plot of DataFrame columns',"b'\r\nI have two columns of a `DataFrame` that I am trying to plot using `scatter` in Matplotlib. Here are the two series:\r\n\r\n    mmsi\r\n    210234000    52.600000\r\n    211109000    62.250000\r\n    211200350    48.333333\r\n    211201390    52.887500\r\n    211203370    41.962500\r\n    211204500    32.233333\r\n    211205780    87.050000\r\n    211205790    42.500000\r\n    211207740    38.233333\r\n    211220290    53.233333\r\n    211262460    40.100000\r\n    211262630    31.716667\r\n    211327410    54.300000\r\n    211335760    43.175000\r\n    211378120    58.361290\r\n    ...\r\n    636090986    62.812500\r\n    636091049    70.175000\r\n    636091052    45.600000\r\n    636091078    82.725000\r\n    636091137    59.075000\r\n    636091138    61.083333\r\n    636091195    75.030000\r\n    636091278    68.241667\r\n    636091394    57.700000\r\n    636091452    44.275000\r\n    636091501    63.480000\r\n    636091506    67.578571\r\n    636091545    44.500000\r\n    636091595    46.475000\r\n    636091800    61.850000\r\n    Name: pdgt10_early, Length: 235, dtype: float64\r\n\r\n    mmsi\r\n    210234000    True\r\n    211109000    True\r\n    211200350    True\r\n    211201390    True\r\n    211203370    True\r\n    211204500    True\r\n    211205780    True\r\n    211205790    True\r\n    211207740    True\r\n    211220290    True\r\n    211262460    True\r\n    211262630    True\r\n    211327410    True\r\n    211335760    True\r\n    211378120    True\r\n    ...\r\n    636090986    True\r\n    636091049    True\r\n    636091052    True\r\n    636091078    True\r\n    636091137    True\r\n    636091138    True\r\n    636091195    True\r\n    636091278    True\r\n    636091394    True\r\n    636091452    True\r\n    636091501    True\r\n    636091506    True\r\n    636091545    True\r\n    636091595    True\r\n    636091800    True\r\n    Length: 235, dtype: bool\r\n\r\n(Actually the latter is the result of a comparison operation on the column to see if values are greater or less than some threshold value)\r\n\r\nUnfortunately, calling `scatter` on these two axes raises a `KeyError: 0` exception for some reason. \r\n\r\n    ---------------------------------------------------------------------------\r\n    KeyError                                  Traceback (most recent call last)\r\n    <ipython-input-52-4b02ddd54f50> in <module>()\r\n          1 for sma in sma_list:\r\n    ----> 2     plot_logistic(\'Cargo\', sma, nova_traces[np.where(sma_list==sma)[0][0]].traces[1], season_a=-3)\r\n\r\n    <ipython-input-51-94c541930a55> in plot_logistic(ship, sma, trace, dataset, season_a, season_b, nlines)\r\n         14     print x\r\n         15     print y\r\n    ---> 16     scatter(x, y)\r\n         17     title(\'{0} in {1}\'.format(ship, sma))\r\n         18 \r\n\r\n    /Library/Python/2.7/site-packages/matplotlib-1.4.x-py2.7-macosx-10.8-intel.egg/matplotlib/pyplot.pyc in scatter(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, hold, **kwargs)\r\n       3088         ret = ax.scatter(x, y, s=s, c=c, marker=marker, cmap=cmap, norm=norm,\r\n       3089                          vmin=vmin, vmax=vmax, alpha=alpha,\r\n    -> 3090                          linewidths=linewidths, verts=verts, **kwargs)\r\n       3091         draw_if_interactive()\r\n       3092     finally:\r\n\r\n    /Library/Python/2.7/site-packages/matplotlib-1.4.x-py2.7-macosx-10.8-intel.egg/matplotlib/axes/_axes.pyc in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, **kwargs)\r\n       3210             self.cla()\r\n       3211 \r\n    -> 3212         self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)\r\n       3213         x = self.convert_xunits(x)\r\n       3214         y = self.convert_yunits(y)\r\n\r\n    /Library/Python/2.7/site-packages/matplotlib-1.4.x-py2.7-macosx-10.8-intel.egg/matplotlib/axes/_base.pyc in _process_unit_info(self, xdata, ydata, kwargs)\r\n       1652             # we only need to update if there is nothing set yet.\r\n       1653             if not self.xaxis.have_units():\r\n    -> 1654                 self.xaxis.update_units(xdata)\r\n       1655             #print \'\\tset from xdata\', self.xaxis.units\r\n       1656 \r\n\r\n    /Library/Python/2.7/site-packages/matplotlib-1.4.x-py2.7-macosx-10.8-intel.egg/matplotlib/axis.pyc in update_units(self, data)\r\n       1330         """"""\r\n       1331 \r\n    -> 1332         converter = munits.registry.get_converter(data)\r\n       1333         if converter is None:\r\n       1334             return False\r\n\r\n    /Library/Python/2.7/site-packages/matplotlib-1.4.x-py2.7-macosx-10.8-intel.egg/matplotlib/units.pyc in get_converter(self, x)\r\n        135 \r\n        136         if isinstance(x, np.ndarray) and x.size:\r\n    --> 137             converter = self.get_converter(x.ravel()[0])\r\n        138             return converter\r\n        139 \r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/core/series.pyc in __getitem__(self, key)\r\n        617     def __getitem__(self, key):\r\n        618         try:\r\n    --> 619             return self.index.get_value(self, key)\r\n        620         except InvalidIndexError:\r\n        621             pass\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/core/index.pyc in get_value(self, series, key)\r\n        722         """"""\r\n        723         try:\r\n    --> 724             return self._engine.get_value(series, key)\r\n        725         except KeyError as e1:\r\n        726             if len(self) > 0 and self.inferred_type == \'integer\':\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/index.so in pandas.index.IndexEngine.get_value (pandas/index.c:2646)()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/index.so in pandas.index.IndexEngine.get_value (pandas/index.c:2461)()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/index.so in pandas.index.IndexEngine.get_loc (pandas/index.c:3198)()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/hashtable.so in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6422)()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/hashtable.so in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6366)()\r\n\r\n    KeyError: 0\r\n\r\nThis worked like a charm prior to updating pandas and matplotlib a week or so ago.\r\n\r\nRunning Python 2.7.2 on OS X 10.8.4.'"
4486,17715420,fonnesbeck,cpcloud,2013-08-06 21:13:06,2014-04-15 20:21:17,2013-08-07 17:22:27,closed,,0.13,16,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/4486,b'Erroneous AssertionError: Index length did not match values on plot',"b'I have what appears to be a valid `Series` with a time series index as follows:\r\n\r\n    In [41]: inactive\r\n    Out[41]: \r\n    2010-06-10 20:43:07    37.2\r\n    2010-06-21 06:37:28     0.0\r\n    2009-07-20 06:53:38     0.0\r\n    2012-05-17 01:50:13    27.4\r\n    2009-07-27 21:09:15     0.0\r\n    2010-05-04 09:06:54     0.0\r\n    2010-05-06 03:38:54    32.5\r\n    2010-05-09 08:33:16    56.9\r\n    2010-05-26 18:58:42     0.0\r\n    2010-05-31 18:40:41     0.0\r\n    2010-06-13 08:46:17     0.0\r\n    2010-06-16 18:27:51     0.0\r\n    2010-06-19 11:59:48     0.0\r\n    2010-06-24 15:24:26    33.4\r\n    2010-06-28 10:54:27     0.0\r\n    ...\r\n    2008-11-02 07:35:22     71.2\r\n    2008-11-05 17:33:39     97.6\r\n    2009-10-22 23:15:39     75.7\r\n    2009-10-27 20:55:07     67.5\r\n    2009-06-02 13:57:15     35.8\r\n    2009-06-06 14:15:41     25.9\r\n    2008-11-07 01:22:46      0.0\r\n    2009-10-12 17:08:20    100.0\r\n    2010-09-19 10:21:39      0.0\r\n    2010-09-24 15:46:50      0.0\r\n    2010-09-30 16:48:09      0.0\r\n    2010-10-08 21:43:10      0.0\r\n    2010-10-15 19:27:11      0.0\r\n    2010-10-28 19:10:42      0.0\r\n    2010-06-13 10:32:45    100.0\r\n    Name: pdgt10, Length: 13536, dtype: float64\r\n\r\nNote also:\r\n\r\n    In [43]: inactive.isnull().sum()\r\n    Out[43]: 0\r\n\r\n    In [45]: inactive.index.size\r\n    Out[45]: 13536\r\n\r\n    In [46]: inactive.values.size\r\n    Out[46]: 13536\r\n\r\nHowever, when I try calling `inactive.plot()` in order to get a time series plot, I receive a puzzling `AssertionError`:\r\n\r\n    In [40]: inactive.plot()\r\n    ---------------------------------------------------------------------------\r\n    AssertionError                            Traceback (most recent call last)\r\n    <ipython-input-40-cdcfa4d871e5> in <module>()\r\n    ----> 1 inactive.plot()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/tools/plotting.pyc in plot_series(series, label, kind, use_index, rot, xticks, yticks, xlim, ylim, ax, style, grid, legend, logx, logy, secondary_y, **kwds)\r\n       1730                      secondary_y=secondary_y, **kwds)\r\n       1731 \r\n    -> 1732     plot_obj.generate()\r\n       1733     plot_obj.draw()\r\n       1734 \r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/tools/plotting.pyc in generate(self)\r\n        856         self._compute_plot_data()\r\n        857         self._setup_subplots()\r\n    --> 858         self._make_plot()\r\n        859         self._post_plot_logic()\r\n        860         self._adorn_subplots()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/tools/plotting.pyc in _make_plot(self)\r\n       1244             lines = []\r\n       1245             labels = []\r\n    -> 1246             x = self._get_xticks(convert_period=True)\r\n       1247 \r\n       1248             plotf = self._get_plot_function()\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/tools/plotting.pyc in _get_xticks(self, convert_period)\r\n       1034                 x = index._mpl_repr()\r\n       1035             elif is_datetype:\r\n    -> 1036                 self.data = self.data.reindex(index=index.order())\r\n       1037                 x = self.data.index._mpl_repr()\r\n       1038             else:\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/core/series.pyc in reindex(self, index, method, level, fill_value, limit, copy, takeable)\r\n       2649 \r\n       2650         # GH4246 (dispatch to a common method with frame to handle possibly duplicate index)\r\n    -> 2651         return self._reindex_with_indexers(new_index, indexer, copy=copy, fill_value=fill_value)\r\n       2652 \r\n       2653     def _reindex_with_indexers(self, index, indexer, copy, fill_value):\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/core/series.pyc in _reindex_with_indexers(self, index, indexer, copy, fill_value)\r\n       2653     def _reindex_with_indexers(self, index, indexer, copy, fill_value):\r\n       2654         new_values = com.take_1d(self.values, indexer, fill_value=fill_value)\r\n    -> 2655         return Series(new_values, index=index, name=self.name)\r\n       2656 \r\n       2657     def reindex_axis(self, labels, axis=0, **kwargs):\r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/core/series.pyc in __new__(cls, data, index, dtype, name, copy)\r\n        491         else:\r\n        492             subarr = subarr.view(Series)\r\n    --> 493         subarr.index = index\r\n        494         subarr.name = name\r\n        495 \r\n\r\n    /Library/Python/2.7/site-packages/pandas-0.12.0_80_gfcaf9a6_20130729-py2.7-macosx-10.8-intel.egg/pandas/lib.so in pandas.lib.SeriesIndex.__set__ (pandas/lib.c:29092)()\r\n\r\n    AssertionError: Index length did not match values\r\n\r\nAs I have shown above, my index length and my values are the same length.\r\n\r\nRunning a recent (<2 weeks) build from master on OS X 10.8.4 and Python 2.7.2.'"
4484,17704752,jreback,jtratner,2013-08-06 17:55:25,2013-09-15 12:00:28,2013-09-15 12:00:28,closed,,0.13,1,API Design;Bug;Error Reporting;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4484,b'BUG/ER: misleading results when filling with a reversed index',"b'created by @prossahl\r\n\r\nReversing an index and forward filling succeeds but produces misleading data:\r\n\r\n```\r\n>>> dr = pd.date_range(\'2013-08-01\', periods=6, freq=\'B\')\r\n>>> df = pd.DataFrame(np.random.randn(6,1), index=dr, columns=list(\'A\'))\r\n>>> df[\'A\'][3] = np.nan\r\n>>> df.reindex(df.index[::-1], method=\'ffill\')\r\n                   A\r\n2013-08-08  2.127302\r\n2013-08-07       NaN\r\n2013-08-06       NaN\r\n2013-08-05       NaN\r\n2013-08-02       NaN\r\n2013-08-01       NaN\r\n```\r\n\r\nThis change enforces ""monotocity"" (well actually weakly increasing) and causes a ValueError to be raised.'"
4477,17684106,y-p,jreback,2013-08-06 11:03:33,2013-08-07 00:25:47,2013-08-07 00:25:47,closed,,0.13,2,Bug,https://api.github.com/repos/pydata/pandas/issues/4477,b'2to3 refactor broke vbenches',"b""looks like lmap is now called by some vbenches, which breaks\r\nthe tests on all older versions.\r\n\r\n```\r\n.....................................................groupby_transform died with:\r\nname 'lmap' is not defined\r\nSkipping...\r\n\r\ngroupby_transform died with:\r\nname 'lmap' is not defined\r\nSkipping...\r\n\r\n```\r\n\r\n#4384, #4474"""
4474,17671618,y-p,jreback,2013-08-06 03:28:52,2013-08-07 00:27:58,2013-08-07 00:27:58,closed,,0.13,3,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/4474,b'2to3 refactor broke test_perf.py',"b""https://github.com/pydata/pandas/pull/4384\r\n\r\nwhen run in HEAD mode, checking out the test_perf.py revision in git master \r\nshould run without issues no matter what version of pandas you have checked out\r\n(at least as far back as 9.1, probably further).\r\n\r\nNow that the script imports `pandas.compat`, it's broken for anything older \r\nthen a couple of weeks. \r\n"""
4463,17635530,jreback,jreback,2013-08-05 13:48:34,2013-08-16 19:28:19,2013-08-16 19:28:19,closed,,0.13,11,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4463,b'BUG: changing series dtype inplace',"b""All of the following current fail because of in-place dtype conversions\r\n\r\n```\r\nIn [2]: s = Series([1,2,3])\r\nIn [3]: s.iloc[0] = np.nan\r\nValueError: cannot convert float NaN to integer\r\n\r\nIn [4]: s.loc[0] = np.nan\r\nValueError: cannot convert float NaN to integer\r\n\r\nIn [5]: s[0] = np.nan\r\nTypeError: 'int' object is not iterable\r\n```"""
4455,17588867,cpcloud,cpcloud,2013-08-03 01:02:40,2013-08-03 20:01:22,2013-08-03 20:01:22,closed,cpcloud,0.13,0,2/3 Compat;Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/4455,b'awkward repring on python3 bytes types',"b""might be an edge case as i'm not sure how useful storing byte strings in `Series` objects is, nonetheless they are repring like `tuple` objects in Python 3\r\n\r\n```\r\nIn [37]: import string\r\n\r\nIn [38]: d = string.digits * 10\r\n\r\nIn [39]: s = Series([d, tm.rands(10), tm.rands(10000)])\r\n\r\nIn [40]: s\r\nOut[40]:\r\n0    0123456789012345678901234567890123456789012345...\r\n1                                           c0HYcb0BlY\r\n2    IghbSU964QRgNimc0jZn2etcTFhRgHnHQepH1dwAqVZLar...\r\ndtype: object\r\n\r\nIn [41]: b = lambda x, encoding='utf8': bytes(x, encoding=encoding)\r\n\r\nIn [42]: s.map(b)\r\nOut[42]:\r\n0      (48, 49, 50, 51, 52, 53, 54, 55, 56, 57, ...)\r\n1          (99, 48, 72, 89, 99, 98, 48, 66, 108, 89)\r\n2    (73, 103, 104, 98, 83, 85, 57, 54, 52, 81, ...)\r\ndtype: object\r\n```\r\n\r\nPR coming soon with a couple of other output formatting fixes as well regarding py3 compat"""
4447,17562816,mattemathias,jreback,2013-08-02 14:26:57,2013-10-13 23:37:53,2013-10-13 23:37:53,closed,,0.13,15,Bug;Groupby;Testing,https://api.github.com/repos/pydata/pandas/issues/4447,b'Filter of grouped dataframe raises ValueError',"b""see also #4527\r\n\r\nWhen trying to filter a grouped dataframe with more than 2 columns raises an ValueError:\r\n\r\n```python\r\n\r\nIn [90]: dff = pd.DataFrame({'A': np.arange(8), 'B': list('aabbbbcc'), 'C': np.arange(8)})\r\n\r\nIn [91]: dff.groupby('B').filter(lambda x: len(x) > 2)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-91-89d79df28299> in <module>()\r\n----> 1 dff.groupby('B').filter(lambda x: len(x) > 2)\r\n\r\nC:\\Anaconda\\lib\\site-packages\\pandas\\core\\groupby.pyc in filter(self, func, dropna, *args, **kwargs)\r\n   2092                 res = path(group)\r\n   2093\r\n-> 2094             if res:\r\n   2095                 indexers.append(self.obj.index.get_indexer(group.index))\r\n   2096\r\n\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\n\r\nIn [93]: pd.__version__\r\nOut[93]: '0.12.0'\r\n```\r\n"""
4446,17559043,hayd,hayd,2013-08-02 13:06:41,2013-08-26 23:43:59,2013-08-26 23:43:59,closed,,0.13,1,Bug,https://api.github.com/repos/pydata/pandas/issues/4446,b'get_dummies with NaN',"b""get_dummies seems to get caught out by NaNs\r\n```\r\nIn [11]: s1 = pd.Series(['a', 'a', np.nan, 'c', 'c', 'c'])\r\n\r\nIn [12]: s1\r\nOut[12]: \r\n0      a\r\n1      a\r\n2    NaN\r\n3      c\r\n4      c\r\n5      c\r\ndtype: object\r\n\r\nIn [13]: pd.get_dummies(s1)\r\nOut[13]: \r\n   a  c\r\n0  1  0\r\n1  1  0\r\n2  0  1\r\n3  0  1\r\n4  0  1\r\n5  0  1\r\n```\r\n\r\nA rogue c has been used as the NaN value, I think expected is:\r\n```\r\nIn [14]: pd.get_dummies(s1[s1.notnull()])\r\nOut[14]: \r\n   a  c\r\n0  1  0\r\n1  1  0\r\n3  0  1\r\n4  0  1\r\n5  0  1\r\n```\r\n\r\n"""
4413,17431209,amelio-vazquez-reina,y-p,2013-07-31 02:14:13,2013-08-02 09:37:02,2013-08-02 09:37:02,closed,,0.13,1,Bug,https://api.github.com/repos/pydata/pandas/issues/4413,b'DeprecationWarning: height has been deprecated',"b'I just upgraded to the latest stable version of Pandas:\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\npd.DataFrame(np.random.random((10,5)))\r\n```\r\n\r\nreturns the following warning:\r\n```\r\n/opt/python/virtualenvs/work/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.    \r\n```\r\n\r\nWhy? And how can I silence it?'"
4405,17406807,mariusvniekerk,cpcloud,2013-07-30 16:49:30,2013-08-03 03:29:56,2013-08-03 03:29:56,closed,cpcloud,0.13,6,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4405,b'string truncation when using Series.astype(str)',"b""When converting a pandas Series object to type string using `astype(str)`, long strings are truncated to 64 characters silently.\r\n\r\nPandas version:  0.12\r\nNumpy version:  1.7.1\r\n\r\n```python\r\n\r\nIn [1]: s = '0123456789' * 10\r\n\r\nIn [2]: tmp = np.array([s]).astype(str)\r\n         tmp\r\nOut[2]: array([ '0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789'], \r\n      dtype='|S100')\r\n\r\nIn [3]: tmp = pd.Series([s]).astype(str)\r\n       tmp[0]\r\nOut[3]: '0123456789012345678901234567890123456789012345678901234567890123'\r\n\r\nIn [4]: len(tmp[0])\r\nOut[4]: 64\r\n```"""
4390,17316419,cpcloud,jreback,2013-07-28 20:00:35,2013-07-30 00:27:21,2013-07-30 00:27:21,closed,jreback,0.13,2,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4390,b'BUG: iat/at broken for PeriodIndex',"b""```\r\nIn [9]: s = Series(randn(10), index=period_range('1/1/2001', periods=10))\r\n\r\nIn [10]: s\r\nOut[10]:\r\n2001-01-01   -0.677\r\n2001-01-02    0.058\r\n2001-01-03   -1.200\r\n2001-01-04   -1.152\r\n2001-01-05   -2.646\r\n2001-01-06   -1.068\r\n2001-01-07   -1.232\r\n2001-01-08    0.452\r\n2001-01-09    0.476\r\n2001-01-10   -1.387\r\nFreq: D, dtype: float64\r\n\r\nIn [11]: s.iat[0] # throws a KeyError\r\n```"""
4382,17303048,cancan101,jreback,2013-07-27 20:27:09,2013-09-24 20:37:17,2013-09-24 20:37:17,closed,,0.13,3,Bug;Data IO;IO CSV,https://api.github.com/repos/pydata/pandas/issues/4382,b'pd.io.parsers.read_csv ignores skiprows when parse_dates is set to a dict',"b'For example:\r\n\r\n```\r\npd.io.parsers.read_csv(""http://www.datazoa.com/publish/export.asp?hash=yjPceG6fHL&uid=dzadmin&a=exportcsv"",skiprows=range(1,13+1),skipfooter=4,parse_dates={""date"":[0]})\r\n```\r\nhas 907 rows.\r\n\r\nAs does:\r\n```\r\npd.io.parsers.read_csv(""http://www.datazoa.com/publish/export.asp?hash=yjPceG6fHL&uid=dzadmin&a=exportcsv"",skipfooter=4,parse_dates={""date"":[0]})\r\n```\r\n\r\nwhereas:\r\n\r\n```\r\npd.io.parsers.read_csv(""http://www.datazoa.com/publish/export.asp?hash=yjPceG6fHL&uid=dzadmin&a=exportcsv"",skiprows=range(1,13+1),skipfooter=4,parse_dates=[0])\r\n```\r\n\r\nhas 894 rows.\r\n\r\nI am on Pandas v0.11.0\r\n'"
4363,17220085,brentp,jreback,2013-07-25 16:06:06,2016-04-26 17:23:52,2016-04-26 17:22:34,closed,,No action,5,Bug;Duplicate;IO CSV,https://api.github.com/repos/pydata/pandas/issues/4363,"b'BUG: read_table error with tabs, dtype and index_col'","b'This gist demonstrates the problem:\r\nhttps://gist.github.com/brentp/6066942\r\n\r\nIt\'s discussed in this thread:\r\nhttps://groups.google.com/forum/#!topic/pydata/hIIZpZqbY5M\r\n\r\nAs discussed in the thread, there is some interaction when specifying dtype, index_col, and sep in read_csv.\r\n\r\nAs the gist shows, things work find with sep=""\\s+"", but fail for the same set with sep=""\\t"" even though the file is tab-delimited.\r\n'"
4362,17217735,mariusvniekerk,jreback,2013-07-25 15:27:16,2013-08-15 01:14:39,2013-08-15 01:14:12,closed,,0.13,6,Bug;Docs;IO JSON;Prio-high,https://api.github.com/repos/pydata/pandas/issues/4362,b'BUG: Series/Dataframe  to_json not returning miliseconds',"b'According to the documentation for pandas 0.12\r\n\r\n> date_format : type of date conversion (epoch = epoch milliseconds, iso = ISO8601)\r\n    default is epoch\r\n\r\nThe json exporting function seems to be returning the epoch _nanoseconds_ rather than the _milliseconds_ as suggested by the documentation.\r\n\r\n```python\r\n\r\ns = pd.Series(1, index=pd.date_range(\'2013-07-25\', periods=2, ))\r\ns.index.astype(\'int64\')\r\n>> array([1374710400000000000, 1374796800000000000])\r\n\r\ns.to_json(orient=\'split\', date_format=\'epoch\')\r\n>> \'{""name"":null,""index"":[1374710400000000000,1374796800000000000],""data"":[1,1]}\'\r\n```'"
4359,17216325,tdhopper,jreback,2013-07-25 15:03:57,2013-07-29 13:00:30,2013-07-28 15:42:43,closed,,0.13,26,Bug;Data IO;IO JSON;Prio-high,https://api.github.com/repos/pydata/pandas/issues/4359,b'BUG: read_json silently skipping records?',"b""should raise when ``orient='columns'`` and index is non_unique\r\n``orient='index'`` and column is non_unique?\r\n\r\nI'm trying out to_json and read_json on a data frame with 800k rows. However, after calling to_json on the file, read_json gets back only 2k rows. This happens if I call them in series or if I give to_json a filename and call the filename with read_json. Judging by the size of the file, all the data is being written (the json is roughly the size of the pickled data frame object). Any idea what's going on?\r\n\r\n![image](https://f.cloud.github.com/assets/611122/856362/0f8d00f8-f53b-11e2-8af9-265886f7fc03.png)\r\n"""
4342,17155390,ruoyu0088,jreback,2013-07-24 13:10:36,2014-05-10 15:32:36,2014-05-10 15:32:36,closed,,0.14.1,4,Bug;MultiIndex;Period;Prio-medium;Reshaping,https://api.github.com/repos/pydata/pandas/issues/4342,"b""Series with MultiIndex with PeriodIndex can't do unstack()""","b'Here is the code:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nidx1 = pd.PeriodIndex([""2013-01"", ""2013-01"", ""2013-02"", ""2013-02""], freq=""M"")\r\nidx2 = [""a"",""b"",""a"",""b""]\r\nvalue = [1,2,3,4]\r\n\r\nidx = pd.MultiIndex.from_arrays([idx1, idx2])\r\ns = pd.Series(value, index=idx)\r\n\r\ns.unstack()\r\n```\r\n\r\nIt will raise following exception:\r\n\r\n```\r\nanaconda/lib/python2.7/site-packages/pandas-0.12.0.dev_424e187-py2.7-linux-x86_64.egg/pandas/tseries/period.pyc in _from_arraylike(cls, data, freq, tz)\r\n    649 \r\n    650                 if freq is None:\r\n--> 651                     raise ValueError((\'freq not specified and cannot be \'\r\n    652                                       \'inferred from first element\'))\r\n    653 \r\n\r\nValueError: freq not specified and cannot be inferred from first element\r\n```'"
4328,17093601,adamaaaa,jtratner,2013-07-23 10:22:45,2013-09-05 02:43:12,2013-09-05 02:37:08,closed,jtratner,0.13,4,Bug;IO CSV;Prio-high,https://api.github.com/repos/pydata/pandas/issues/4328,b'BUG: Crash using to_csv with QUOTE_NONE',"b'Using release 0.11.0 with Python 2.7.1, but it doesn\'t look like anything has changed in git since.\r\n\r\nI can write DataFrames to a file using to_csv fine unless I pass the option: quoting=pandas.io.parsers.csv.QUOTE_NONE in which case I get exceptions of the form:\r\n\r\npackages/pandas/core/format.py"", line 1054, in _save_chunk\r\n    lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)\r\n  File ""lib.pyx"", line 832, in pandas.lib.write_csv_rows (pandas/lib.c:13466)\r\n_csv.Error: need to escape, but no escapechar set\r\n\r\nA short search reveals links like http://www.velocityreviews.com/forums/t612166-how-can-i-use-quotes-without-escaping-them-using-csv.html implying that due to some csv oddities we should be setting something like quotechar=None when calling csv.writer.\r\n\r\nI tried modifying pandas/core/format.py to add quotechar=None to the call to csv.writer and this does indeed seem to fix the problem. Unfortunately I don\'t have everything set up to build from git so I can\'t submit a patch.\r\n\r\nThanks.'"
4322,17072035,hayd,jreback,2013-07-22 21:36:55,2013-09-24 00:04:00,2013-08-23 17:51:59,closed,,0.13,10,Bug;Data IO;IO CSV;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/4322,b'read csv thousands separator',"b""From [this SO question](http://stackoverflow.com/questions/17797384/date-and-time-in-pandas-read-csv), with input to include thousands:\r\n\r\n```\r\n\r\nIn [1]: s = '06.02.2013;13:00;1.000,215;0,215;0,185;0,205;0,00'\r\n\r\nIn [2]: pd.read_csv(StringIO(s), sep=';', header=None, parse_dates={'Dates': [0, 1]}, index_col=0, decimal=',')\r\nOut[2]:\r\n                              2      3      4      5  6\r\nDates\r\n2013-06-02 13:00:00  10.000,215  0.215  0.185  0.205  0\r\n\r\nIn [3]: pd.read_csv(StringIO(s), sep=';', header=None, parse_dates={'Dates': [0, 1]}, index_col=0, decimal=',', thousands='.')\r\nOut[3]:\r\n                        2      3      4      5  6\r\nDates\r\n6022013 13:00   1.000,215  0.215  0.185  0.205  0\r\n```\r\n*Note: the Dates column (as well as the thousands not being converted.*"""
4319,17069145,cpcloud,cpcloud,2013-07-22 20:37:41,2013-09-05 21:12:52,2013-09-05 21:12:51,closed,cpcloud,0.13,5,Bug;Error Reporting;Prio-low,https://api.github.com/repos/pydata/pandas/issues/4319,"b'""&"" doesn\'t raise ValueError according to nose, but it does in the shell'","b'Strange bug, probably has to do with not propagating errors in Cython\r\n```python\r\nimport numpy as np\r\nimport numbers\r\n\r\ndef randbool(p=0.5, size=None):\r\n    if size is None:\r\n        size = ()\r\n\r\n    if isinstance(size, numbers.Integral):\r\n        size = (size,)\r\n\r\n    return np.random.rand(*size) <= p\r\n```\r\n```\r\nIn [1]: x = randbool(size=(10,5))\r\n\r\nIn [2]: s = Series([False]*5)\r\n\r\nIn [3]: andit = lambda x, s: x & s\r\n\r\nIn [4]: andit(x, s)\r\nValueError: operands could not be broadcast together with shapes (10) (10,5)\r\n\r\nIn [5]: from nose.tools import assert_raises\r\n\r\nIn [6]: assert_raises(ValueError, andit, x, s)\r\nAssertionError: ValueError not raised\r\n```'"
4318,17058898,brainysmurf,jreback,2013-07-22 17:16:59,2013-07-30 00:07:39,2013-07-30 00:07:39,closed,,0.13,13,Bug;Data IO;Docs;IO CSV,https://api.github.com/repos/pydata/pandas/issues/4318,b'DOC/BUG: NA string in data gets read as NaN',"b'better docs for ``keep_default_na`` and ``na_values`` when used together\r\n\r\nI have a field set where ""NA"" is meaningful, but not meaning ""not applicable"" (it refers to the name of my school\'s Homeroom). There\'s no obvious way to handle this edge case...\r\nOn 0.11.0'"
4294,16981064,l736x,jreback,2013-07-19 16:56:42,2013-10-02 19:10:22,2013-10-02 19:10:22,closed,jreback,0.13,5,Bug;Error Reporting;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4294,b'ER: Bug accessing a series MultiIndex with a Timestamp',"b""I have a series with a multiindex (datetime, int).\r\nWhen I try to access a slice corresponding to a date using a Timestamp I have an exception printed to screen if the size of the series is >= 1e6.\r\nNo problem if I access the same slice with a string.\r\nNo problem with an equivalent dataframe.\r\nTested on 0.10.1 and 0.11, I'm sorry but I don't have access to 0.12rc1.\r\n\r\n``` python\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.11.0'\r\n\r\nIn [3]: np.__version__\r\nOut[3]: '1.7.1'\r\n\r\nIn [4]: df = pd.DataFrame(randn(1000, 1000), index=pd.date_range('2000-1-1', periods=1000))\r\n\r\nIn [5]: s = df.stack() # s has 1e6 lines\r\n\r\nIn [6]: s.head()\r\nOut[6]:\r\n2000-01-01  0   -0.555228\r\n            1   -0.152952\r\n            2    0.401490\r\n            3   -1.057106\r\n            4   -0.987895\r\ndtype: float64\r\n\r\nIn [7]: s['2000-1-4'].head()\r\nOut[7]:\r\n0   -1.009647\r\n1    0.094427\r\n2    0.105180\r\n3   -2.166810\r\n4   -0.767165\r\ndtype: float64\r\n\r\nIn [8]: s[pd.Timestamp('2000-1-4')].head()\r\nException TypeError: TypeError('Cannot compare Timestamp with (<Timestamp: 2002-09-26 00:00:00>, 999)',) in 'pandas.index._bin_search' ignored\r\nOut[8]:\r\n0   -1.009647\r\n1    0.094427\r\n2    0.105180\r\n3   -2.166810\r\n4   -0.767165\r\ndtype: float64\r\n\r\nIn [9]: s2 = s[:-1].copy() # s2 has 1e6 - 1 lines\r\n\r\nIn [10]: s2['2000-1-4'].head()\r\nOut[10]:\r\n0   -1.009647\r\n1    0.094427\r\n2    0.105180\r\n3   -2.166810\r\n4   -0.767165\r\ndtype: float64\r\n\r\nIn [11]: s2[pd.Timestamp('2000-1-4')].head()\r\nOut[11]:\r\n0   -1.009647\r\n1    0.094427\r\n2    0.105180\r\n3   -2.166810\r\n4   -0.767165\r\ndtype: float64\r\n\r\nIn [12]: df2 = pd.DataFrame(s)\r\n\r\nIn [13]: df2.ix['2000-1-4'].head()\r\nOut[13]:\r\n          0\r\n0 -1.009647\r\n1  0.094427\r\n2  0.105180\r\n3 -2.166810\r\n4 -0.767165\r\n\r\nIn [14]: df2.ix[pd.Timestamp('2000-1-4')].head()\r\nOut[14]:\r\n          0\r\n0 -1.009647\r\n1  0.094427\r\n2  0.105180\r\n3 -2.166810\r\n4 -0.767165\r\n\r\n```"""
4280,16884808,BAM-BAM-BAM,jreback,2013-07-17 19:33:22,2013-07-18 03:15:42,2013-07-18 03:00:27,closed,,0.12,11,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4280,b'MemoryError when using .loc or .ix',"b'<pre>\r\nfrom pandas import *\r\ndf = read_csv(open(\'mydata.csv.gz\', \'r\'), compression=\'gzip\', index_col=False)\r\ndf = df[(df.land != 1)]\r\nprint df\r\n# <class \'pandas.core.frame.DataFrame\'>\r\n# Int64Index: 977579 entries, 0 to 1100398\r\n# Data columns (total 89 columns):\r\n# <snip>\r\n\r\n# sample 100,000 rows, only use some of the columns\r\nrows = np.random.choice(df.index.values, 100000)\r\nkeep_cols = [\'sq_ft\', \'zip\', \'year\', \'bathrooms\', \'bedrooms\', \'floors\']\r\nsampled_df = df.ix[rows, keep_cols]\r\n\r\nsampled_df.loc[sampled_df.year.notnull()].year        # works fine\r\nsampled_df.loc[sampled_df.year.notnull(),[\'year\']]    # MemoryError\r\n\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\n<ipython-input-114-f9c1e2bc7b58> in <module>()\r\n      1 #sampled_df.loc[sampled_df[\'year\'].notnull(),[\'year\']]\r\n      2 sampled_df.loc[sampled_df.year.notnull()].year\r\n----> 3 sampled_df.loc[sampled_df.year.notnull(),[\'year\']]\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n    695     def __getitem__(self, key):\r\n    696         if type(key) is tuple:\r\n--> 697             return self._getitem_tuple(key)\r\n    698         else:\r\n    699             return self._getitem_axis(key, axis=0)\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    260         # ugly hack for GH #836\r\n    261         if self._multi_take_opportunity(tup):\r\n--> 262             return self._multi_take(tup)\r\n    263 \r\n    264         # no shortcut needed\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/indexing.pyc in _multi_take(self, tup)\r\n    300             index = self._convert_for_reindex(tup[0], axis=0)\r\n    301             columns = self._convert_for_reindex(tup[1], axis=1)\r\n--> 302             return self.obj.reindex(index=index, columns=columns)\r\n    303         elif isinstance(self.obj, Panel4D):\r\n    304             conv = [self._convert_for_reindex(x, axis=i)\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/frame.pyc in reindex(self, index, columns, method, level, fill_value, limit, copy, takeable)\r\n   2623         if index is not None:\r\n   2624             frame = frame._reindex_index(index, method, copy, level,\r\n-> 2625                                          fill_value, limit, takeable)\r\n   2626 \r\n   2627         return frame\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/frame.pyc in _reindex_index(self, new_index, method, copy, level, fill_value, limit, takeable)\r\n   2703         new_index, indexer = self.index.reindex(new_index, method, level,\r\n   2704                                                 limit=limit, copy_if_needed=True,\r\n-> 2705                                                 takeable=takeable)\r\n   2706         return self._reindex_with_indexers(new_index, indexer, None, None,\r\n   2707                                            copy, fill_value)\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/index.pyc in reindex(self, target, method, level, limit, copy_if_needed, takeable)\r\n    930                         raise ValueError(""cannot reindex a non-unique index ""\r\n    931                                          ""with a method or limit"")\r\n--> 932                     indexer, _ = self.get_indexer_non_unique(target)\r\n    933 \r\n    934         return target, indexer\r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer_non_unique(self, target, **kwargs)\r\n    843             tgt_values = target.values\r\n    844 \r\n--> 845         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n    846         return Index(indexer), missing\r\n    847 \r\n\r\n/home/jprior/Scratch/VENV1/lib/python2.7/site-packages/pandas/index.so in pandas.index.IndexEngine.get_indexer_non_unique (pandas/index.c:5049)()\r\n\r\nMemoryError: \r\n</pre>\r\n\r\nSorry I haven\'t figured out how to reproduce the error with a toy example. \r\n'"
4273,16860501,jreback,jreback,2013-07-17 11:54:12,2013-08-23 14:33:19,2013-08-23 14:33:19,closed,,0.13,3,Bug;Error Reporting;IO HDF5,https://api.github.com/repos/pydata/pandas/issues/4273,b'BUG/ER: HDFStore write with empty frame reports an error (rather than suceeding)',"b""writing to an HDFStore with an empty-frame with invalid dtypes raises, maybe should just proceed (or is it the dtypes call that is actually wrong here: see #4272)\r\n\r\ncame up in this question: http://stackoverflow.com/questions/17691912/problems-with-merging-on-disk-tables-with-millions-of-rows/17698740#17698740\r\n\r\n```\r\nIn [26]: df = DataFrame(randn(10,2),columns=list('AB'))\r\n\r\nIn [28]: df['C'] = 'foo'\r\n\r\nIn [33]: df.to_hdf('test.h5','df',mode='w',table=True)\r\n\r\nIn [35]: pd.read_hdf('test.h5','df')\r\nOut[35]: \r\n          A         B    C\r\n0 -1.123712 -1.146515  foo\r\n1  0.921705  1.800419  foo\r\n2 -0.769236 -0.553307  foo\r\n3 -0.747601 -1.783439  foo\r\n4 -1.110340  1.601026  foo\r\n5  0.743869 -2.135140  foo\r\n6  1.033699  2.028479  foo\r\n7 -0.755478 -1.060223  foo\r\n8  0.079326 -2.671624  foo\r\n9 -2.262756  0.406850  foo\r\n\r\nIn [36]: pd.read_hdf('test.h5','df').dtypes\r\nOut[36]: \r\nA    float64\r\nB    float64\r\nC     object\r\ndtype: object\r\n\r\nIn [37]: df[df.C=='bar']\r\nOut[37]: \r\nEmpty DataFrame\r\nColumns: [A, B, C]\r\nIndex: []\r\n\r\nIn [38]: df[df.C=='bar'].dtypes\r\nOut[38]: \r\nA   NaN\r\nB   NaN\r\nC   NaN\r\ndtype: float64\r\n\r\nIn [39]: df[df.C=='bar'].to_hdf('test.h5','df',append=True)\r\nTypeError: Cannot serialize the column [C] because\r\nits data contents are [empty] object dtype\r\n```"""
4272,16860155,jreback,jreback,2013-07-17 11:43:33,2013-09-29 21:22:26,2013-09-29 21:22:26,closed,,0.13,7,API Design;Bug;Dtypes;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/4272,b'BUG: dtypes on empty frame are incorrect',"b""```\r\nIn [1]: df = DataFrame(columns=list('xyz'))\r\n\r\nIn [2]: df\r\nOut[2]: \r\nEmpty DataFrame\r\nColumns: [x, y, z]\r\nIndex: []\r\n\r\nIn [3]: df.dtypes\r\nOut[3]: \r\nx   NaN\r\ny   NaN\r\nz   NaN\r\ndtype: float64\r\n\r\nIn [5]: df['x'].dtype\r\nOut[5]: dtype('O')\r\n```\r\n\r\nexpected\r\n```\r\nIn [11]: Series(dict([ (s,np.dtype('O')) for s in list('xyz')]))\r\nOut[11]: \r\nx    object\r\ny    object\r\nz    object\r\ndtype: object\r\n```"""
4253,16790514,yarikoptic,cpcloud,2013-07-16 02:57:41,2013-07-17 13:29:00,2013-07-17 13:29:00,closed,cpcloud,0.12,2,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/4253,b'test_fallback_success: missing skip conditioning',"b'with 0.12.0~rc1+git79-g50eff60\r\n\r\n```\r\n======================================================================\r\nFAIL: test_fallback_success (pandas.io.tests.test_html.TestReadHtmlLxml)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_html.py"", line 449, in test_fallback_success\r\n    \'html5lib\'])\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_html.py"", line 420, in run_read_html\r\n    return read_html(*args, **kwargs)\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/html.py"", line 906, in read_html\r\n    attrs)\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/html.py"", line 765, in _parse\r\n    parser = _parser_dispatch(flav)\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/html.py"", line 723, in _parser_dispatch\r\n    raise AssertionError(""You\'re using a version""\r\nAssertionError: You\'re using a version of BeautifulSoup4 (4.2.0) that has been known to cause problems on certain operating systems such as Debian. Please install a version of BeautifulSoup4 != 4.2.0, both earlier and later releases will work.\r\n\r\n```'"
4252,16790472,yarikoptic,cpcloud,2013-07-16 02:55:33,2013-07-17 13:28:48,2013-07-17 13:28:48,closed,cpcloud,0.12,8,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/4252,"b""AttributeError: 'TestReadHtmlBase' object has no attribute 'flavor'""","b'Was testing 0.12.0~rc1+git79-g50eff60 on a sparc for the recent fixes:  test_html tests puked before trying with\r\n\r\n```\r\n======================================================================\r\nERROR: test_bad_url_protocol (pandas.io.tests.test_html.TestReadHtmlBase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_html.py"", line 98, in setUp\r\n    self.try_skip()\r\n  File ""/home/yoh/deb/gits/pkg-exppsy/pandas/debian/tmp/usr/lib/python2.7/dist-packages/pandas/io/tests/test_html.py"", line 87, in try_skip\r\n    self.flavor != [\'lxml\']):\r\nAttributeError: \'TestReadHtmlBase\' object has no attribute \'flavor\'\r\n\r\n```'"
4249,16785637,michaelaye,cpcloud,2013-07-15 23:45:03,2013-08-21 13:32:23,2013-07-16 13:19:21,closed,cpcloud,0.12,7,Bug,https://api.github.com/repos/pydata/pandas/issues/4249,b'1e69dad import failure',"b""(master)[maye@luna4 ~/src/pandas]$ git log -1\r\ncommit 1e69dade1309b494e43f26f9831326c2ce63f7de\r\nMerge: 5be1e5a 5b908a5\r\nAuthor: Phillip Cloud <cpcloud@gmail.com>\r\nDate:   Mon Jul 15 16:17:06 2013 -0700\r\n\r\n    Merge pull request #4248 from cpcloud/mpl-1.1.1-build\r\n\r\n    BLD: use mpl 1.1.1 in python 2.7 production travis build\r\n\r\n\r\nThis happened after doing:\r\n\r\n````python\r\npython setup.py clean\r\npython setup.py install\r\n````\r\n\r\nHere's the error:\r\n\r\n````python\r\nIn [1]: import pandas\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-d6ac987968b6> in <module>()\r\n----> 1 import pandas\r\n\r\n/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/pandas-0.12.0.dev_1e69dad-py2.7-macosx-10.5-x86_64.egg/pandas/__init__.pyc in <module>()\r\n     25 import pandas.core.config_init\r\n     26\r\n---> 27 from pandas.core.api import *\r\n     28 from pandas.sparse.api import *\r\n     29 from pandas.stats.api import *\r\n\r\n/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/pandas-0.12.0.dev_1e69dad-py2.7-macosx-10.5-x86_64.egg/pandas/core/api.py in <module>()\r\n     16 from pandas.core.panel4d import Panel4D\r\n     17 from pandas.core.groupby import groupby\r\n---> 18 from pandas.core.reshape import (pivot_simple as pivot, get_dummies,\r\n     19                                  lreshape)\r\n     20\r\n\r\n/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/pandas-0.12.0.dev_1e69dad-py2.7-macosx-10.5-x86_64.egg/pandas/core/reshape.py in <module>()\r\n      6 import numpy as np\r\n      7\r\n----> 8 import six\r\n      9\r\n     10 from pandas.core.series import Series\r\n\r\nImportError: No module named six\r\n````"""
4235,16721697,miketkelly,hayd,2013-07-14 00:08:49,2013-08-23 13:03:32,2013-08-23 13:03:32,closed,,0.13,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4235,b'BUG: Boolean indexing on an empty series loses index names',"b""\r\n\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a': [], 'b': [], 'c': []})\r\ndf = df.set_index(['a', 'b'])\r\n\r\nprint df.c.index.names\r\nprint df.c[df.c.isnull()].index.names\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n['a', 'b']\r\n[None]\r\n```\r\n\r\nThe problem is that the empty boolean mask isn't recognized as a boolean array by infer_dtype in inference.pyx.\r\n\r\nI have a fix here: 87b9f21d0f298e35c5ec965b48b53ff17d40ab7e. If you agree with the approach I'll submit the PR.\r\n"""
4204,16626713,hayd,jreback,2013-07-11 11:48:07,2013-10-03 00:31:53,2013-08-16 19:34:11,closed,,0.13,2,Bug;Dtypes;Indexing;Numeric,https://api.github.com/repos/pydata/pandas/issues/4204,b'BUG: loc setting NaN in an int column should upcast',"b'I think upcasting to float should occur here.\r\n\r\nFor example:\r\n\r\n```\r\nIn [1]: df = pd.DataFrame([[0, 0]])\r\n\r\nIn [2]: df.loc[0] = np.nan\r\n\r\nIn [3]: df\r\nOut[3]:\r\n                     0                    1\r\n0 -9223372036854775808 -9223372036854775808\r\n```\r\n\r\nSometimes this is caught:\r\n\r\n```\r\nIn [11]: df = pd.DataFrame([[0]])\r\n\r\nIn [12]: df.loc[0] = np.nan\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-92653b6a9ef8> in <module>()\r\n----> 1 df.loc[0] = np.nan\r\n\r\n/Users/234BroadWalk/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n     86\r\n     87         self._setitem_with_indexer(indexer, value)\r\n---> 88\r\n     89     def _has_valid_tuple(self, key):\r\n     90         pass\r\n\r\n/Users/234BroadWalk/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\r\n    183             if np.prod(values.shape):\r\n    184                 values[indexer] = value\r\n--> 185\r\n    186     def _align_series(self, indexer, ser):\r\n    187         # indexer to assign Series can be tuple or scalar\r\n\r\nValueError: cannot convert float NaN to integer\r\n```'"
4202,16624784,hayd,jtratner,2013-07-11 10:55:52,2013-09-24 03:43:59,2013-09-24 03:43:59,closed,jtratner,0.13,13,API Design;Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/4202,b'API: Deep copy not copying index/columns',"b""http://stackoverflow.com/questions/17591104/in-pandas-can-i-deeply-copy-a-dataframe-including-its-index-and-column/17591423#17591423\r\n\r\nThis means if we change the index in place, e.g. with name:\r\n\r\n    df1 = df.copy()\r\n    df1.index.name = 'ffg'\r\n\r\nThis changes the name of the df.index."""
4192,16581521,SleepingPills,jreback,2013-07-10 14:52:44,2013-07-10 18:52:48,2013-07-10 18:52:48,closed,,0.12,9,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4192,b'BUG: Boolean indexed assignment',"b'Take the simple example below:\r\n\r\n```\r\ns = pd.Series([1.0, 2.0], dtype=float)\r\ns[[True, False]] = np.array([2.0])\r\n```\r\n\r\nThe expectation would be that the first item in the series will be set to 2. Unfortunately due to a bug in broadcasting, pandas will set the first item in the series to 2*len(s). The culprit appears to be the below code (series.py:757 - where() method definition):\r\n\r\n```\r\nif len(other) == 1:\r\n    other = np.array(other[0]*len(ser))\r\n```\r\n\r\n`other` in this case is the length 1 array we need to broadcast. I believe the code made the assumption that other will be perhaps a list of lists? As it stands, this code will take the single element out of the array, multiply it by the length of the series and then carry out the update.\r\n\r\nI propose that we fix this by using np.repeat which does the correct thing (pending testing).'"
4187,16571741,hayd,hayd,2013-07-10 11:11:43,2013-09-10 19:03:57,2013-07-13 08:21:49,closed,,0.12,9,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/4187,b'ENH: Series from MultiIndex causes segfault',"b'```\r\nIn [3]: m = pd.MultiIndex.from_arrays([[1,2], [3,4]])\r\n\r\nIn [4]: m\r\nOut[4]:\r\nMultiIndex\r\n[(1, 3), (2, 4)]\r\n\r\nIn [5]: pd.Series(m)\r\n[1]    786 segmentation fault  ipython\r\n\r\npd.Series(m.values) has same fault (list of tuples)\r\n```\r\nProbably this should just be disallowed.'"
4170,16524717,hayd,hayd,2013-07-09 13:33:48,2013-08-07 19:23:42,2013-08-07 19:17:18,closed,cpcloud,0.13,18,API Design;Bug;Dtypes;Numeric,https://api.github.com/repos/pydata/pandas/issues/4170,b'API: boolean dtype upsets cumsum',"b""cumsum seems to require skipna=False otherwise it sulks here. (Not investigate which others are also affected, cumprod is though).\r\n\r\n```\r\nIn [10]: b = pd.Series([False, False, False, True, True, False, False])\r\n\r\nIn [11]: b\r\nOut[11]:\r\n0    False\r\n1    False\r\n2    False\r\n3     True\r\n4     True\r\n5    False\r\n6    False\r\ndtype: bool\r\n\r\nIn [12]: b.cumsum()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-f3f684a93525> in <module>()\r\n----> 1 b.cumsum()\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/series.pyc in cumsum(self, axis, dtype, out, skipna)\r\n   1626\r\n   1627         if do_mask:\r\n-> 1628             np.putmask(result, mask, pa.NA)\r\n   1629\r\n   1630         return Series(result, index=self.index)\r\n\r\nValueError: cannot convert float NaN to integer\r\n\r\nIn [13]: b.cumsum(skipna=False)\r\nOut[13]:\r\n0    0\r\n1    0\r\n2    0\r\n3    1\r\n4    2\r\n5    2\r\n6    2\r\ndtype: int64\r\n```\r\n\r\nIf it has nans or you int or object it works as expected:\r\n\r\n```\r\nIn [21]: b.astype(int).cumsum()\r\nIn [22]: b.astype(object).cumsum()  # False at the beginning is expected\r\nIn [23]: b.astype(int).astype(object).cumsum()\r\n```\r\n\r\nAlso, if you try and inset an nan it doesn't work nor raise (!):\r\n\r\n```\r\nIn [31]: b.loc[0] = np.nan\r\n\r\nIn [32]: b\r\nOut[32]:\r\n0     True\r\n1    False\r\n2    False\r\n3     True\r\n4     True\r\n5    False\r\n6    False\r\ndtype: bool\r\n```"""
4161,16482602,aschilling,jreback,2013-07-08 17:10:59,2014-02-14 03:24:56,2014-02-14 03:24:56,closed,,0.14.0,3,Bug;Error Reporting;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4161,b'TimeGrouper only works properly when using a sorted DateTimeIndex',"b'related SO question\r\n\r\nhttp://stackoverflow.com/questions/19636370/dataframe-groupbytimegrouper-d-invalid-length-for-values-or-for-binner/19636874?noredirect=1#comment29155715_19636874\r\n\r\nHi everybody,\r\n\r\ntoday I spend the whole afternoon figuring out that TimeGrouper only works properly when using on a sorted(!) DateTimeIndex.\r\n\r\nIf the DateTimeIndex is not sorted, TimeGrouper throws no error (!) but produces corrupt results.\r\n\r\nI would suggest either to do a sort() call within TimeGrouper or modify it to throw an error message. The fact that it silently produces corrupt results when the DateTimeIndex is not sorted is really disturbing.\r\n\r\nSorry, that I cannot add an example for the bug. All simple examples I created worked fine, it seems that this only leads to corrupt results with DataFrames of size 700 and above.\r\n\r\nBest regards\r\n\r\nAndy'"
4146,16429988,hayd,hayd,2013-07-06 11:39:54,2013-07-06 13:28:09,2013-07-06 13:27:19,closed,,0.12,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4146,b'Not duplicates in multiIndex columns with duplicates not indexed properly when selected',"b""This appears to be a regression since 0.11 in handling duplicates in MultiIndex columns:\r\n\r\n```\r\nIn [11]: df\r\nOut[11]:\r\n  h1 main  h3 sub  h5\r\n0  a    A   1  A1   1\r\n1  b    B   2  B1   2\r\n2  c    B   3  A1   3\r\n3  d    A   4  B2   4\r\n4  e    A   5  B2   5\r\n5  f    B   6  A2   6\r\n\r\nIn [12]: df2 = df.set_index(['main', 'sub']).T.sort_index(1)\r\n\r\nIn [13]: df2\r\nOut[13]:\r\nmain  A        B\r\nsub  A1 B2 B2 A1 A2 B1\r\nh1    a  d  e  c  f  b\r\nh3    1  4  5  3  6  2\r\nh5    1  4  5  3  6  2\r\n```\r\n\r\nIf we grab out successively we get an unexpected result for the non-duplicate:\r\n\r\n```\r\nIn [14]: df2['A']\r\nOut[14]:\r\nsub A1 B2 B2\r\nh1   a  d  e\r\nh3   1  4  5\r\nh5   1  4  5\r\n\r\nIn [15]: df2['A']['B2']\r\nOut[15]:\r\nsub B2 B2\r\nh1   d  e\r\nh3   4  5\r\nh5   4  5\r\n\r\nIn [16]: df2['A']['A1']  # this worked in 0.11\r\nOut[16]:\r\n   0\r\n0  a\r\n1  1\r\n2  1\r\n```\r\n\r\n```\r\nIn [21]: df2['A']['A1']  # pandas 0.11\r\nOut[21]:\r\nh1    a\r\nh3    1\r\nh5    1\r\nName: A1, dtype: object\r\n```\r\n\r\n*FWIW never like how this can return different a type...*"""
4145,16429784,hayd,jreback,2013-07-06 11:11:41,2013-07-06 13:21:40,2013-07-06 13:21:40,closed,,0.12,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4145,b'DataFrame MultiIndex column access (and pop)',"b'Suppose I want to acces a column in df2 (perhaps there is a near way, but I also expect these to work):\r\n\r\n```\r\nIn [11]: df\r\nOut[11]:\r\n  h1 main  h3 sub  h5\r\n0  a    A   1  A1   1\r\n1  b    B   2  B1   2\r\n2  c    B   3  A1   3\r\n3  d    A   4  B2   4\r\n4  e    A   5  B2   5\r\n5  f    B   6  A2   6\r\n\r\nIn [12]: df2 = df.set_index([\'main\', \'sub\']).T.sort_index(1)\r\n\r\nIn [13]: df2\r\nOut[13]:\r\nmain  A        B\r\nsub  A1 B2 B2 A1 A2 B1\r\nh1    a  d  e  c  f  b\r\nh3    1  4  5  3  6  2\r\nh5    1  4  5  3  6  2\r\n```\r\n\r\nI want to access the column `(\'A\', \'A1\')`:\r\n```\r\nIn [14]: df2.iloc[:, 0]  # cheating with iloc\r\nIn [15]: df2.T.loc[(\'A\', \'A1\'), :].iloc[0]  # hacky!\r\nIn [16]: df2.iloc[:, df2.columns.get_loc((\'A\', \'A1\')).start]  # very hacky!\r\n\r\nIn [17]: df2[df2.columns[:1]]  # returns DataFrame\r\n```\r\nI had assumed/hoped this would work:\r\n\r\n```\r\nIn [18]: df2[(\'A\', \'A1\')]\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-18-c307c7bb3bb8> in <module>()\r\n----> 1 df2[(\'A\', \'A1\')]\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in __getitem__(self, key)\r\n   1997             return self._getitem_frame(key)\r\n   1998         elif isinstance(self.columns, MultiIndex):\r\n-> 1999             return self._getitem_multilevel(key)\r\n   2000         else:\r\n   2001             # get column\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in _getitem_multilevel(self, key)\r\n   2036         if isinstance(loc, (slice, np.ndarray)):\r\n   2037             new_columns = self.columns[loc]\r\n-> 2038             result_columns = _maybe_droplevels(new_columns, key)\r\n   2039             if self._is_mixed_type:\r\n   2040                 result = self.reindex(columns=new_columns)\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/indexing.pyc in _maybe_droplevels(index, key)\r\n   1103     if isinstance(key, tuple):\r\n   1104         for _ in key:\r\n-> 1105             index = index.droplevel(0)\r\n   1106     else:\r\n   1107         index = index.droplevel(0)\r\n\r\nAttributeError: \'Index\' object has no attribute \'droplevel\'\r\n\r\nIn [19]: df2[[\'A\', \'A1\']]  # interestingly, slightly different error here\r\nKeyError: ""[\'A1\'] not in index""\r\n```\r\n\r\nAlso this way is buggy (loses the index)... which is *weird*, separated this part of the issue as #4146:\r\n```\r\nIn [21]: df2[\'A\'][\'A1\']  # in master but not in 0.11.0\r\nOut[21]:\r\n   0\r\n0  a\r\n1  1\r\n2  1\r\n```\r\n\r\n*pop uses this in it\'s implementation, so atm it\'s not possible to pop a MultiIndex.*'"
4135,16409430,jreback,jreback,2013-07-05 16:02:40,2013-08-13 22:19:43,2013-08-13 22:19:43,closed,,0.13,3,Bug;Dtypes;Prio-high;Timedelta,https://api.github.com/repos/pydata/pandas/issues/4135,b'BUG: np.timedelta64 arithmetic appears buggy',"b""Buggy timedelta arithmetic with ``np.timedelta64`` in np 1.7.1\r\n\r\n\r\n```\r\nIn [41]: s = Series([Timestamp('20130301'),Timestamp('20130228 23:00:00'),Timestamp('20130228 22:00:00'),Timestamp('20130228 21:00:00')])\r\n\r\nIn [42]: s\r\nOut[42]: \r\n0   2013-03-01 00:00:00\r\n1   2013-02-28 23:00:00\r\n2   2013-02-28 22:00:00\r\n3   2013-02-28 21:00:00\r\ndtype: datetime64[ns]\r\n\r\nIn [43]: s + timedelta(hours=1)\r\nOut[43]: \r\n0   2013-03-01 01:00:00\r\n1   2013-03-01 00:00:00\r\n2   2013-02-28 23:00:00\r\n3   2013-02-28 22:00:00\r\ndtype: datetime64[ns]\r\n```\r\n\r\nShould be the same as above\r\n```\r\nIn [45]: s + np.timedelta64(1,'h')\r\nOut[45]: \r\n0   2013-03-01 00:00:00.000000001\r\n1   2013-02-28 23:00:00.000000001\r\n2   2013-02-28 22:00:00.000000001\r\n3   2013-02-28 21:00:00.000000001\r\ndtype: datetime64[ns]\r\n```"""
4131,16402744,davidshinn,davidshinn,2013-07-05 13:17:04,2013-07-07 04:40:23,2013-07-06 04:17:34,closed,,0.12,3,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/4131,b'Excel parser cannot change *keep_default_na* value',"b'The parse method in the ExcelFile class does not pass the argument *keep_default_na* to the TextParser from pandas.io.parsers.  There is no clean way to override the default na values when parsing excel files with the current code.\r\n\r\nMy specific problem involves ""NA"" as a typical value for ""North America"" in excel files.'"
4125,16371119,jreback,jreback,2013-07-04 14:48:14,2015-09-09 21:41:02,2015-09-09 21:41:01,closed,,0.17.0,1,Bug;Indexing;Period,https://api.github.com/repos/pydata/pandas/issues/4125,b'BUG: indexing with an entire period_index raising',"b""```\r\nIn [57]: pi = pd.period_range('2002-01','2003-12',freq='M')\r\n\r\nIn [58]: pi\r\nOut[58]: \r\n<class 'pandas.tseries.period.PeriodIndex'>\r\nfreq: M\r\n[2002-01, ..., 2003-12]\r\nlength: 24\r\n\r\nIn [59]: p = pd.Panel(randn(24,5,10),items=pi)\r\n\r\nIn [60]: p\r\nOut[60]: \r\n<class 'pandas.core.panel.Panel'>\r\nDimensions: 24 (items) x 5 (major_axis) x 10 (minor_axis)\r\nItems axis: 2002-01 to 2003-12\r\nMajor_axis axis: 0 to 4\r\nMinor_axis axis: 0 to 9\r\n```\r\n\r\nThis is ok\r\n```\r\nIn [61]: p.ix[list(pi)]\r\nOut[61]: \r\n<class 'pandas.core.panel.Panel'>\r\nDimensions: 24 (items) x 5 (major_axis) x 10 (minor_axis)\r\nItems axis: 2002-01 to 2003-12\r\nMajor_axis axis: 0 to 4\r\nMinor_axis axis: 0 to 9\r\n```\r\n\r\nThis raises?\r\n```\r\nIn [62]: p.ix[pi]\r\nDateParseError: day is out of range for month\r\n```"""
4122,16348865,changhiskhan,jreback,2013-07-04 01:46:08,2013-07-04 16:05:03,2013-07-04 15:11:52,closed,,0.12,9,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/4122,b'windows build test failures',"b'getting test failures on the windows builds. this is from py27 64-bit:\r\n\r\n```\r\n======================================================================\r\nERROR: test_replace_convert (pandas.tests.test_frame.TestDataFrame)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\tests\\test_frame.py"", line 6869, in test_replace_convert\r\n    assert_series_equal(expec, res)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\util\\testing.py"", line 197, in assert_series_equal\r\n    assert_almost_equal(left.values, right.values, check_less_precise)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\util\\testing.py"", line 141, in assert_almost_equal\r\n    assert_almost_equal(a[i], b[i], check_less_precise)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\util\\testing.py"", line 146, in assert_almost_equal\r\n    if isnull(a):\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\core\\common.py"", line 60, in isnull\r\n    return _isnull(obj)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\core\\common.py"", line 74, in _isnull_new\r\n    return _isnull_ndarraylike(obj)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\core\\common.py"", line 135, in _isnull_ndarraylike\r\n    values = np.asarray(obj)\r\n  File ""c:\\python27\\lib\\site-packages\\numpy\\core\\numeric.py"", line 235, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nValueError: invalid __array_struct__\r\n\r\n======================================================================\r\nFAIL: test_insert_column_bug_4032 (pandas.tests.test_frame.TestDataFrame)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\tests\\test_frame.py"", line 10084, in test_insert_column_bug_4032\r\n    assert_frame_equal(result,expected)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\util\\testing.py"", line 238, in assert_frame_equal\r\n    check_less_precise=check_less_precise)\r\n  File ""C:\\workspace\\pandas-windows-test64-py27\\pandas\\util\\testing.py"", line 199, in assert_series_equal\r\n    assert(left.dtype == right.dtype)\r\nAssertionError\r\n```\r\n\r\n'"
4119,16340917,hayd,jreback,2013-07-03 21:05:14,2013-07-03 23:43:15,2013-07-03 23:43:15,closed,,0.12,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4119,b'convert_object not converting to NaN',"b""docstring seem to suggest that this ought to convert to NaN:\r\n\r\n```\r\nIn [10]: s = pd.Series([1, 'na', 3 ,4])\r\n\r\nIn [11]: s.convert_objects(convert_numeric=True)\r\nOut[11]:\r\n0     1\r\n1    na\r\n2     3\r\n3     4\r\ndtype: object\r\n```\r\nDocstring:\r\n```\r\nconvert_numeric : boolean, default True\r\n    if True attempt to coerce to numbers (including strings),\r\n    non-convertibles get NaN\r\n```\r\n\r\nHowever, does work if the non-strings are stringified first\r\n```\r\nIn [142]: pd.Series([1, 'na', 3 ,4]).astype(str).convert_objects(convert_numeric=True)\r\nOut[142]: \r\n0     1\r\n1   NaN\r\n2     3\r\n3     4\r\ndtype: float64\r\n```\r\n"""
4116,16324377,jreback,jreback,2013-07-03 15:38:24,2014-02-13 23:12:31,2014-02-13 23:12:31,closed,jreback,0.14.0,0,Bug;Enhancement;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/4116,b'BUG/ENH: multi-index assignment',"b""http://stackoverflow.com/questions/17451843/copying-a-single-index-dataframe-into-a-multiindex-dataframe/17452237#17452237\r\n\r\n```\r\nIIn [3]: import itertools\r\n\r\nIn [4]: inner = ('a','b')\r\n\r\nIn [5]: outer = ((10,20), (1,2))\r\n\r\nIn [6]: cols = ('one','two','three','four')\r\n\r\nIn [7]: sngl = pd.DataFrame(np.random.randn(2,4), index=inner, columns=cols)\r\n\r\nIIn [8]: index_tups = list(itertools.product(*(outer + (inner,))))\r\n\r\nIn [9]: index_mult = pd.MultiIndex.from_tuples(index_tups)\r\n\r\nIn [10]: mult = pd.DataFrame(index=index_mult, columns=cols)\r\n```\r\n\r\nand the solution\r\n```\r\n    In [48]: nm = mult.reset_index().set_index('level_2')\r\n    \r\n    In [49]: nm\r\n    Out[49]: \r\n             level_0  level_1  one  two three four\r\n    level_2                                       \r\n    a             10        1  NaN  NaN   NaN  NaN\r\n    b             10        1  NaN  NaN   NaN  NaN\r\n    a             10        2  NaN  NaN   NaN  NaN\r\n    b             10        2  NaN  NaN   NaN  NaN\r\n    a             20        1  NaN  NaN   NaN  NaN\r\n    b             20        1  NaN  NaN   NaN  NaN\r\n    a             20        2  NaN  NaN   NaN  NaN\r\n    b             20        2  NaN  NaN   NaN  NaN\r\n```\r\n\r\nA bug here????\r\nThis should probably work with a series on the rhs; this might be a buglet\r\n    \r\n```\r\n    In [50]: nm.loc['a',sngl.columns] = sngl.loc['a'].values\r\n    \r\n    In [51]: nm\r\n    Out[51]: \r\n             level_0  level_1        one        two     three        four\r\n    level_2                                                              \r\n    a             10        1  0.3738456 -0.2261926 -1.205177  0.08448757\r\n    b             10        1        NaN        NaN       NaN         NaN\r\n    a             10        2  0.3738456 -0.2261926 -1.205177  0.08448757\r\n    b             10        2        NaN        NaN       NaN         NaN\r\n    a             20        1  0.3738456 -0.2261926 -1.205177  0.08448757\r\n    b             20        1        NaN        NaN       NaN         NaN\r\n    a             20        2  0.3738456 -0.2261926 -1.205177  0.08448757\r\n    b             20        2        NaN        NaN       NaN         NaN\r\n    \r\n    In [52]: nm.reset_index().set_index(['level_0','level_1','level_2'])\r\n    Out[52]: \r\n                                   one        two     three        four\r\n    level_0 level_1 level_2                                            \r\n    10      1       a        0.3738456 -0.2261926 -1.205177  0.08448757\r\n                    b              NaN        NaN       NaN         NaN\r\n            2       a        0.3738456 -0.2261926 -1.205177  0.08448757\r\n                    b              NaN        NaN       NaN         NaN\r\n    20      1       a        0.3738456 -0.2261926 -1.205177  0.08448757\r\n                    b              NaN        NaN       NaN         NaN\r\n            2       a        0.3738456 -0.2261926 -1.205177  0.08448757\r\n                    b              NaN        NaN       NaN         NaN\r\n```\r\n\r\n"""
4115,16324219,vfilimonov,cpcloud,2013-07-03 15:35:54,2013-07-03 22:39:18,2013-07-03 18:59:42,closed,cpcloud,0.12,18,Bug,https://api.github.com/repos/pydata/pandas/issues/4115,b'BUG: DataFrame.replace is broken in 0.12-dev',"b""the ```DataFrame.replace``` seems to be broken in 0.12 development version\r\n\r\nthis code:\r\n```\r\ndf = pd.DataFrame({'Type':['Q','T','Q','Q','T'], 'tmp':2})\r\nprint df\r\nprint df.replace({'Type': {'Q': 0, 'T': 1}})\r\n```\r\nReturn correct result in 0.11.0:\r\n```\r\n  Type  tmp\r\n0    Q    2\r\n1    T    2\r\n2    Q    2\r\n3    Q    2\r\n4    T    2\r\n5    T    2\r\n  Type  tmp\r\n0    0    2\r\n1    1    2\r\n2    0    2\r\n3    0    2\r\n4    1    2\r\n5    1    2\r\n```\r\nand in 0.12.0.dev-795004d it does not work:\r\n```\r\n\r\n  Type  tmp\r\n0    Q    2\r\n1    T    2\r\n2    Q    2\r\n3    Q    2\r\n4    T    2\r\n5    T    2\r\n   Type  tmp\r\n0     0    2\r\n1     1    2\r\n2     0    2\r\n3     1    2\r\n4     0    2\r\n5     1    2\r\n```\r\n\r\nSeries.replace it works well, so the workaround for this bug is the following:\r\n```\r\ndf['Type'] = df['Type'].replace({'Q': 0, 'T': 1})\r\n```\r\n"""
4112,16317969,cpcloud,cpcloud,2013-07-03 13:38:37,2013-09-22 13:32:57,2013-09-22 13:32:57,closed,cpcloud,0.13,1,Bug;Prio-high;Visualization,https://api.github.com/repos/pydata/pandas/issues/4112,b'BUG: double figure when passing by to Series.hist',
4110,16298337,jpdus,jreback,2013-07-03 02:08:25,2014-01-29 20:26:50,2014-01-29 20:26:50,closed,,0.14.0,1,Blocker;Bug;Data IO;IO SQL,https://api.github.com/repos/pydata/pandas/issues/4110,b'BUG: in io.sql.write_frame (replace)',"b'From http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.sql.write_frame.html :\r\n\r\n```\r\nParameters: (...)\r\nif_exists: {\xa1\xaefail\xa1\xaf, \xa1\xaereplace\xa1\xaf, \xa1\xaeappend\xa1\xaf}, default \xa1\xaefail\xa1\xaf :\r\nfail: If table exists, do nothing. replace: If table exists, drop it, recreate it, \r\nand insert data. append: If table exists, insert data. Create if does not exist.\r\n```\r\nEverything works fine as long as the table does not exist.\r\n If table already exists : \r\n```Python\r\nsql.write_frame(sp5002, name=""sp500"", con=conn, if_exists=\'replace\')\r\n``` \r\n-> ""sqlite3.OperationalError: no such table: sp500"" \r\n\r\nReason is that there is no new ""create"" statement after the drop statement:\r\n\r\n```Python\r\n    create = None\r\n    if exists and if_exists == \'replace\':\r\n        create = ""DROP TABLE %s"" % name\r\n    elif not exists:\r\n        create = get_schema(frame, name, flavor)\r\n```\r\n\r\nIn my opinion it should be changed to something like: (?)\r\n\r\n```Python\r\n    create = None\r\n    if exists and if_exists == \'replace\':\r\n        create = ""DROP TABLE %s"" % name + ""\\n""+get_schema(frame, name, flavor)\r\n    elif not exists:\r\n        create = get_schema(frame, name, flavor)\r\n```\r\n\r\nIf i manually work around that, everything works as expected:\r\n\r\n```Python\r\n    exists = sql.table_exists(\'sp500\', conn, \'sqlite\')\r\n    if not exists:\r\n        sql.write_frame(sp5002, name=""sp500"", con=conn, if_exists=\'replace\')\r\n    else:\r\n        create = ""DROP TABLE sp500""\r\n        cur = conn.cursor()\r\n        cur.execute(create)\r\n        cur.close()\r\n        sql.write_frame(sp5002, name=""sp500"", con=conn, if_exists=\'replace\')\r\n```\r\n\r\n#2971'"
4109,16298329,SlyOne,jreback,2013-07-03 02:08:07,2014-03-30 15:14:50,2014-03-30 15:14:50,closed,,0.14.0,1,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4109,b'BUG: Maybe a Bug with BQuarterEnd',"b'prob related to #4069\r\n\r\nimport pandas as pd\r\nfrom pandas.tseries.offsets import *\r\n\r\nhelp(BQuarterEnd)\r\nHelp on class BQuarterEnd in module pandas.tseries.offsets:\r\n\r\nclass BQuarterEnd(DateOffset, CacheableOffset)\r\n |  DateOffset increments between business Quarter dates\r\n |  startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...\r\n |  startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...\r\n |  startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, ...\r\n |  \r\n |  Method resolution order:\r\n |      BQuarterEnd\r\n |      DateOffset\r\n |      CacheableOffset\r\n |      __builtin__.object\r\n |  \r\n |  Methods defined here:\r\n |  \r\n |  __init__(self, n=1, **kwds)\r\n\r\n\r\n\r\nimport datetime as dt\r\nd = dt.datetime(2008, 8, 18)\r\ndw = d+ BQuarterEnd(1)\r\ndw\r\nOut[10]: datetime.datetime(2008, 9, 30, 0, 0)\r\n\r\ndw = d+ BQuarterEnd(1,2)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-2f6514a26e56> in <module>()\r\n----> 1 dw = d+ BQuarterEnd(1,2)\r\n\r\nTypeError: __init__() takes at most 2 arguments (3 given)\r\n\r\n\r\nI am looking for Business quarter end which end APR, and MAY deflaut is JUN, or those three month cycles...\r\n\r\n\r\n\r\n\r\n\r\n'"
4098,16230915,jreback,jreback,2013-07-01 19:27:05,2013-07-01 20:39:08,2013-07-01 20:39:08,closed,,0.12,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/4098,b'BUG: tz in index not working in HDFStore with table',"b""```\r\nIn [1]: df = DataFrame(dict(A = Series(xrange(3), index=date_range('2000-1-1',periods=3,freq='H', tz='US/Eastern'))))\r\n\r\nIn [2]: df.index\r\nOut[2]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2000-01-01 00:00:00, ..., 2000-01-01 02:00:00]\r\nLength: 3, Freq: H, Timezone: US/Eastern\r\n\r\nIn [3]: df.to_hdf('tz.h5','df',mode='w')\r\n\r\nIn [4]: pd.read_hdf('tz.h5','df').index\r\nOut[4]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2000-01-01 00:00:00, ..., 2000-01-01 02:00:00]\r\nLength: 3, Freq: H, Timezone: US/Eastern\r\n\r\nIn [5]: df.to_hdf('tz.h5','df',mode='w',table=True)\r\n\r\nIn [6]: pd.read_hdf('tz.h5','df').index\r\nOut[6]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2000-01-01 00:00:00, ..., 2000-01-01 02:00:00]\r\nLength: 3, Freq: H, Timezone: US/Eastern\r\n```"""
4096,16226632,vfilimonov,jreback,2013-07-01 17:53:33,2013-11-07 19:56:18,2013-07-01 21:27:26,closed,,0.12,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/4096,"b'stochastic bug in saving dataframes with ""int16"" or ""int32"" columns to HDF5 file'","b'There seems to be a strange random bug in saving dataframes with integer columns to HDF5 files. The error sounds like:\r\n```\r\nValueError: invalid combinate of [values_axes] on appending data [name->values_block_1,cname->values_block_1,axis->None,pos->2,kind->integer] vs current table [name->values_block_1,cname->values_block_1,axis->None,pos->2,kind->integer]\r\n```\r\nand the full traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-91a60d72b1ca> in <module>()\r\n     25     print raw.dtypes\r\n     26     store = pd.HDFStore(\'possible_bug9.h5\')\r\n---> 27     store.append(\'raw\', raw)\r\n     28     store.close()\r\n\r\n/Users/v/.virtual_envs/system/lib/python2.7/site-packages/pandas/io/pytables.pyc in append(self, key, value, columns, **kwargs)\r\n    608             raise Exception(""columns is not a supported keyword in append, try data_columns"")\r\n    609 \r\n--> 610         self._write_to_group(key, value, table=True, append=True, **kwargs)\r\n    611 \r\n    612     def append_to_multiple(self, d, value, selector, data_columns=None, axes=None, **kwargs):\r\n\r\n/Users/v/.virtual_envs/system/lib/python2.7/site-packages/pandas/io/pytables.pyc in _write_to_group(self, key, value, index, table, append, complib, **kwargs)\r\n    869             raise ValueError(\'Compression not supported on non-table\')\r\n    870 \r\n--> 871         s.write(obj = value, append=append, complib=complib, **kwargs)\r\n    872         if s.is_table and index:\r\n    873             s.create_index(columns = index)\r\n\r\n/Users/v/.virtual_envs/system/lib/python2.7/site-packages/pandas/io/pytables.pyc in write(self, obj, axes, append, complib, complevel, fletcher32, min_itemsize, chunksize, expectedrows, **kwargs)\r\n   2738         # create the axes\r\n   2739         self.create_axes(axes=axes, obj=obj, validate=append,\r\n-> 2740                          min_itemsize=min_itemsize, **kwargs)\r\n   2741 \r\n   2742         if not self.is_exists:\r\n\r\n/Users/v/.virtual_envs/system/lib/python2.7/site-packages/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\r\n   2482         # validate the axes if we have an existing table\r\n   2483         if validate:\r\n-> 2484             self.validate(existing_table)\r\n   2485 \r\n   2486     def process_axes(self, obj, columns=None):\r\n\r\n/Users/v/.virtual_envs/system/lib/python2.7/site-packages/pandas/io/pytables.pyc in validate(self, other)\r\n   2096                     oax = ov[i]\r\n   2097                     if sax != oax:\r\n-> 2098                         raise ValueError(""invalid combinate of [%s] on appending data [%s] vs current table [%s]"" % (c,sax,oax))\r\n   2099 \r\n   2100                 # should never get here\r\n\r\nValueError: invalid combinate of [values_axes] on appending data [name->values_block_1,cname->values_block_1,axis->None,pos->2,kind->integer] vs current table [name->values_block_1,cname->values_block_1,axis->None,pos->2,kind->integer]\r\n```\r\n\r\nInterestingly, the error is not deterministic and depends at least on (i) set of other (non-integer) columns in dataframe and (ii) size of dataframe.\r\n\r\nUnfortunately I was unable to narrow it down to ""just-code"" report, so I have to attach a piece of data and an ipython notebook. This is a minimal code in which I was able to reproduce the bug.\r\n\r\nhttps://www.dropbox.com/s/myo03sbqbulzvaj/pandas_possible_bug.zip\r\n\r\nthe pandas version is 0.11.0, pytables version: 3.0.0'"
4089,16195168,fonnesbeck,cpcloud,2013-06-30 23:35:16,2013-07-01 16:00:17,2013-07-01 03:28:07,closed,cpcloud,0.12,13,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/4089,b'DataFrame.hist does not honor sharex or sharey set to True',"b'[Here is an example](http://nbviewer.ipython.org/urls/gist.github.com/fonnesbeck/5897459/raw/6181f066f245c7fb489b68f88e4b4731c77a238b/King%27s+Bay+Survey+Analysis.ipynb) of the `hist` method called as a multi-panel plot via the `by` argument, with `sharey=True` and `sharex=True`. As you can observe in the plot at the bottom of the notebook, neither the x nor the y axis is shared among the subplots.\r\n\r\nRunning 0.11.1 on Python 2.7.2 (OS X 10.8.4).'"
4088,16188429,janschulz,jreback,2013-06-30 14:36:02,2014-12-07 00:18:23,2014-12-07 00:18:23,closed,,0.15.2,5,Bug;MultiIndex;Reshaping,https://api.github.com/repos/pydata/pandas/issues/4088,b'Reording a multiindex by only specifiying the order on level 0',"b'starting PR is here: https://github.com/pydata/pandas/pull/6647\r\n\r\nBy reading the docs on `Dataframe.reindex()`, I would have guessed (not sure what ""broadcast"" is...) that this works:\r\n\r\n``` python\r\narrays = [[\'bar\', \'bar\', \'baz\', \'baz\', \'foo\', \'foo\', \'qux\', \'qux\'],\r\n[\'one\', \'two\', \'one\', \'two\', \'one\', \'two\', \'one\', \'two\']]\r\ntuples = zip(*arrays)\r\nindex = MultiIndex.from_tuples(tuples, names=[\'first\', \'second\'])\r\ndf = DataFrame({""a"":list(""abcdefgh""), ""b"":list(""abcdefgh"")}, index=index)\r\norder = [""baz"", ""bar"", ""foo"", ""qux""]\r\ndf.reindex(order, level=0) # still returns the same ordering\r\n```\r\n\r\nIt works that way on a normal index (without the level argument):\r\n```python\r\ndf2 = DataFrame({""a"":list(""ABCD"")}, index=[\'bar\', \'baz\', \'foo\', \'qux\'])\r\ndf2.reindex(order) # new ordering\r\n```\r\n\r\nIt also works when I pass a list of tuples:\r\n```python\r\n_new_index = []\r\nfor _a in order:\r\n    for _b in df.index.levels[1]:\r\n        _new_index.append((_a, _b))\r\ndf.reindex(_new_index) # new ordering\r\n```'"
4080,16169484,midfield,jreback,2013-06-29 01:58:32,2013-08-16 19:27:26,2013-08-16 19:27:26,closed,,0.13,15,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/4080,b'BUG: series update',"b""```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(dict((c, [1,2,3]) for c in ['a', 'b', 'c']))\r\ndf.set_index(['a', 'b', 'c'], inplace=True)\r\ns = pd.Series([1], index=[(2,2,2)])\r\ndf['val'] = 0\r\ndf\r\ndf['val'].update(s)\r\ndf\r\n```\r\n\r\non my machine (pandas 0.11.1.dev-45d298d, linux) i get the rather unexpected output\r\n\r\n```\r\n\r\n                       val\r\na b c\r\n1 1 1                    0\r\n2 2 2  4607182418800017408\r\n3 3 3                    0\r\n\r\n\r\n```"""
4078,16164581,cpcloud,jreback,2013-06-28 22:22:58,2013-10-02 20:54:59,2013-10-02 20:54:59,closed,jreback,0.13,9,Bug;Error Reporting;Indexing,https://api.github.com/repos/pydata/pandas/issues/4078,b'ER: multiindexes in no-data DataFrame constructor should yield a nan frame',"b'as it stands\r\n\r\n```\r\nIn [24]: tuples = [(randint(10), randint(10)) for _ in range(10)]\r\n\r\nIn [25]: mi = MultiIndex.from_tuples(tuples)\r\n\r\nIn [26]: df = DataFrame(index=mi, columns=mi)\r\n```\r\nfails, whereas single level indexes work fine\r\n\r\n```\r\nIn [27]: df = DataFrame(index=range(10), columns=range(10))\r\n\r\nIn [28]: df\r\nOut[28]:\r\n     0    1    2    3    4    5    6    7    8    9\r\n0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n6  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n7  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n8  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n9  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n```'"
4076,16153391,waltaskew,jreback,2013-06-28 17:53:55,2014-03-23 13:49:21,2014-03-23 13:49:21,closed,cpcloud,0.14.0,10,Bug;Prio-high;Resample,https://api.github.com/repos/pydata/pandas/issues/4076,b'Extra Bin with Pandas Resample in 0.11.0',"b""I've got a pandas data frame defined like this, using pandas 0.11.0:\r\n\r\n```\r\n    last_4_weeks_range = pandas.date_range(                                \r\n            start=datetime.datetime(2001, 5, 4), periods=28)               \r\n    last_4_weeks = pandas.DataFrame(                                       \r\n        [{'REST_KEY': 1, 'DLY_TRN_QT': 80, 'DLY_SLS_AMT': 90,              \r\n            'COOP_DLY_TRN_QT': 30, 'COOP_DLY_SLS_AMT': 20}] * 28 +         \r\n        [{'REST_KEY': 2, 'DLY_TRN_QT': 70, 'DLY_SLS_AMT': 10,              \r\n            'COOP_DLY_TRN_QT': 50, 'COOP_DLY_SLS_AMT': 20}] * 28,          \r\n        index=last_4_weeks_range.append(last_4_weeks_range))               \r\n    last_4_weeks.sort(inplace=True)\r\n```\r\n\r\nand when I go to resample it:\r\n\r\n```\r\nIn [265]: last_4_weeks.resample('7D', how='sum')\r\nOut[265]: \r\n            COOP_DLY_SLS_AMT  COOP_DLY_TRN_QT  DLY_SLS_AMT  DLY_TRN_QT  REST_KEY\r\n2001-05-04               280              560          700        1050        21\r\n2001-05-11               280              560          700        1050        21\r\n2001-05-18               280              560          700        1050        21\r\n2001-05-25               280              560          700        1050        21\r\n2001-06-01                 0                0            0           0         0\r\n```\r\n\r\nI end up with an extra empty bin I wouldn't expect to see -- 2001-06-01. I wouldn't expect that bin to be there, as my 28 days are evenly divisible into the 7 day resample I'm performing. I've tried messing around with the closed kwarg, but I can't escape that extra bin.  This seems like a bug, and it messes up my mean calculations when I try to do \r\n```\r\nIn [266]: last_4_weeks.groupby('REST_KEY').resample('7D', how='sum').mean(level=0)\r\nOut[266]: \r\n          COOP_DLY_SLS_AMT  COOP_DLY_TRN_QT  DLY_SLS_AMT  DLY_TRN_QT  REST_KEY\r\nREST_KEY                                                                      \r\n1                      112              168          504         448       5.6\r\n2                      112              280           56         392      11.2\r\n```\r\nas the numbers are being divided by 5 rather than 4.  (I also wouldn't expect REST_KEY to show up in the aggregation columns as it's part of the groupby, but that's really a smaller problem.)"""
4071,16142893,LionelR,cpcloud,2013-06-28 14:02:03,2013-06-28 20:15:50,2013-06-28 18:41:48,closed,cpcloud,0.12,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4071,"b'BUG: AssertionError using mask on a dataframe with shape (1, n)'","b'Using a DataFrame of bools to mask some values in another dataframe works great:\r\n```python\r\nIn [84]: pd.DataFrame([[1,2],[3,4]]).mask(pd.DataFrame([[True,False],[False, True]]))\r\nOut[84]: \r\n    0   1\r\n0 NaN   2\r\n1   3 NaN\r\n```\r\n\r\nbut not if we try on a dataframe with a single index:\r\n```python\r\nIn [85]: pd.DataFrame([[1,2],]).mask(pd.DataFrame([[True,False],]))\r\nAssertionError: Number of Block dimensions (1) must equal number of axes (2)\r\n```'"
4069,16139509,SlyOne,jreback,2013-06-28 12:28:50,2014-03-30 15:23:19,2014-03-30 15:23:19,closed,,0.14.0,1,Bug;Frequency;Timeseries,https://api.github.com/repos/pydata/pandas/issues/4069,b'BUG: Possible BMonthEnd()',"b'```python\r\n>>> import datetime as dt\r\n>>> from pandas.tseries.offsets import *\r\n>>> d = dt.datetime(2013, 6, 27, 8, 10)\r\n>>> d + BMonthEnd()\r\ndatetime.datetime(2013, 6, 28, 0, 0) # correct!!!\r\n>>> d = dt.datetime(2013, 6, 28, 8, 10)\r\n>>> d + BMonthEnd()\r\ndatetime.datetime(2013, 7, 31, 0, 0) ## should 6/28 not 7/31...day not over\r\n```'"
4032,16001219,miketkelly,jreback,2013-06-25 20:56:57,2013-06-26 14:02:11,2013-06-26 14:02:11,closed,,0.12,4,Bug,https://api.github.com/repos/pydata/pandas/issues/4032,b' Some items were not contained in blocks AssertionError',"b'\r\nThe code below causes the DataFrame internals to get confused and throws an AssertionError on `print df.values`. The code looks contrived, but its just a simplified version of something I was trying to do. It doesn\'t seem to matter whether any columns are actually renamed so I simplified that also, but the rename calls do contribute to the problem.\r\n\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame({\'a\': [1, 2],\r\n                   \'b\': [3, 4],\r\n                   \'c\': [5, 6]})\r\n\r\ndf = df.set_index([\'a\', \'b\'])\r\ndf = df.unstack()\r\ndf = df.rename(columns={\'a\': \'d\'})\r\ndf = df.reset_index()\r\nprint df._data\r\ndf = df.rename(columns={})\r\nprint df._data\r\nprint df.values\r\n\r\n```\r\n\r\n```\r\nBlockManager\r\nItems: MultiIndex\r\n[(u\'a\', u\'\'), (u\'c\', 3), (u\'c\', 4)]\r\nAxis 1: Int64Index([0, 1], dtype=int64)\r\nFloatBlock: [(c, 3), (c, 4)], 2 x 2, dtype float64\r\nIntBlock: [(a, )], 1 x 2, dtype int64\r\n\r\nBlockManager\r\nItems: MultiIndex\r\n[(u\'a\', u\'\'), (u\'c\', 3), (u\'c\', 4)]\r\nAxis 1: Int64Index([0, 1], dtype=int64)\r\nFloatBlock: [(a, ), (c, 3)], 2 x 2, dtype float64\r\nIntBlock: [(a, )], 1 x 2, dtype int64\r\n\r\nTraceback (most recent call last):\r\n  File ""/home/mtk/bug.py"", line 13, in <module>\r\n    print df.values\r\n  File ""/Users/mtk/Source/pandas/pandas/core/frame.py"", line 1779, in as_matrix\r\n    return self._data.as_matrix(columns).T\r\n  File ""/Users/mtk/Source/pandas/pandas/core/internals.py"", line 1513, in as_matrix\r\n    mat = self._interleave(self.items)\r\n  File ""/Users/mtk/Source/pandas/pandas/core/internals.py"", line 1549, in _interleave\r\n    raise AssertionError(\'Some items were not contained in blocks\')\r\nAssertionError: Some items were not contained in blocks\r\n\r\n```\r\n\r\n```\r\n>>> pd.__version__\r\n\'0.11.1.dev-8a242d2\'\r\n```'"
4023,15960481,joeb1415,jreback,2013-06-25 04:11:12,2014-09-08 13:47:55,2014-09-08 13:47:55,closed,jtratner,0.16.0,6,Bug;Duplicate;Enhancement;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/4023,b'rename multiindex bug in DataFrame',"b'related / same as #4160\r\n\r\n    from datetime import datetime, time\r\n    from pandas import date_range, Series, DataFrame\r\n    \r\n    time_ind = date_range(datetime(2013, 1, 1), datetime(2013, 1, 10), freq=\'1min\')\r\n    \r\n    s = Series(1, index=time_ind)\r\n    s_g = s.groupby([lambda x: x.hour, lambda x: x.minute]).sum()\r\n    s_g = s_g.rename(lambda (h, m): time(h, m, 0))\r\n    \r\n    df = DataFrame(1, index=time_ind, columns=[\'a\', \'b\'])\r\n    df_g = df.groupby([lambda x: x.hour, lambda x: x.minute]).sum()\r\n    df_g = df_g.rename(lambda (h, m): time(h, m, 0))\r\n\r\nWorks fine on Series, but throws error on DataFrame:\r\n\r\n    Traceback (most recent call last):\r\n\r\n    File ""<ipython-input-25-4e51131ae70d>"", line 2, in <module>\r\n      df_g = df_g.rename(lambda (h, m): time(h, m, 0))\r\n  \r\n    File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 3539, in rename\r\n      result._rename_index_inplace(index_f)\r\n  \r\n    File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 3548, in _rename_index_inplace\r\n      self._data = self._data.rename_axis(mapper, axis=1)\r\n  \r\n    File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 1734, in rename_axis\r\n      new_axis = MultiIndex.from_tuples([tuple(mapper(y) for y in x) for x in index], names=index.names)\r\n  \r\n    File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 1734, in <genexpr>\r\n      new_axis = MultiIndex.from_tuples([tuple(mapper(y) for y in x) for x in index], names=index.names)\r\n  \r\n    File ""<ipython-input-25-4e51131ae70d>"", line 2, in <lambda>\r\n      df_g = df_g.rename(lambda (h, m): time(h, m, 0))\r\n  \r\n    TypeError: \'long\' object is not iterable'"
4017,15952249,rhstanton,jreback,2013-06-24 23:00:55,2013-06-28 13:18:24,2013-06-26 18:15:38,closed,,0.12,1,Bug;Indexing;Performance,https://api.github.com/repos/pydata/pandas/issues/4017,b'Odd behavior from df.iloc',"b'I encountered some strange behavior from df.iloc today (today\'s master). Here\'s an example. First, create a 300,000 element df with ascending integer index and then select 3000 rows from it:\r\n\r\nfrom pandas import *\r\nimport numpy as np\r\n\r\ndf = DataFrame({\'A\' : [0.1] * 300000, \'B\' : [1] * 300000})\r\nidx = array(range(3000)) * 99\r\n%timeit df.A.iloc[idx]\r\n\r\n> 1000 loops, best of 3: 256 s per loop\r\n\r\nThis seems quite reasonable. Now do the same thing, but this time with a df whose index is not ascending:\r\n\r\ndf2 = DataFrame({\'A\' : [0.1] * 100000, \'B\' : [1] * 100000})\r\ndf2 = concat([df, 2*df, 3*df])\r\n%timeit df2.A.iloc[idx]\r\n\r\nAfter waiting several *minutes*, I get the output:\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-47-8f3d2b974bc2> in <module>()\r\n      1 df2 = DataFrame({\'A\' : [0.1] * 100000, \'B\' : [1] * 100000})\r\n      2 df2 = concat([df, 2*df, 3*df])\r\n----> 3 get_ipython().magic(u\'timeit df2.A.iloc[idx]\')\r\n\r\n/RHS/packages/anaconda/ipython/IPython/core/interactiveshell.pyc in magic(self, arg_s)\r\n   2180         magic_name, _, magic_arg_s = arg_s.partition(\' \')\r\n   2181         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\r\n-> 2182         return self.run_line_magic(magic_name, magic_arg_s)\r\n   2183 \r\n   2184     #-------------------------------------------------------------------------\r\n\r\n/RHS/packages/anaconda/ipython/IPython/core/interactiveshell.pyc in run_line_magic(self, magic_name, line)\r\n   2101                 kwargs[\'local_ns\'] = sys._getframe(stack_depth).f_locals\r\n   2102             with self.builtin_trap:\r\n-> 2103                 result = fn(*args,**kwargs)\r\n   2104             return result\r\n   2105 \r\n\r\n/RHS/packages/anaconda/ipython/IPython/core/magics/execution.pyc in timeit(self, line, cell)\r\n\r\n/RHS/packages/anaconda/ipython/IPython/core/magic.pyc in <lambda>(f, *a, **k)\r\n    190     # but it\'s overkill for just that one bit of state.\r\n    191     def magic_deco(arg):\r\n--> 192         call = lambda f, *a, **k: f(*a, **k)\r\n    193 \r\n    194         if callable(arg):\r\n\r\n/RHS/packages/anaconda/ipython/IPython/core/magics/execution.pyc in timeit(self, line, cell)\r\n    885             number = 1\r\n    886             for i in range(1, 10):\r\n--> 887                 if timer.timeit(number) >= 0.2:\r\n    888                     break\r\n    889                 number *= 10\r\n\r\n/Users/stanton/anaconda/lib/python2.7/timeit.pyc in timeit(self, number)\r\n    193         gc.disable()\r\n    194         try:\r\n--> 195             timing = self.inner(it, self.timer)\r\n    196         finally:\r\n    197             if gcold:\r\n\r\n<magic-timeit> in inner(_it, _timer)\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/indexing.pyc in __getitem__(self, key)\r\n    686             return self._getitem_tuple(key)\r\n    687         else:\r\n--> 688             return self._getitem_axis(key, axis=0)\r\n    689 \r\n    690     def _getitem_axis(self, key, axis=0):\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/indexing.pyc in _getitem_axis(self, key, axis)\r\n    838                 raise ValueError(""Cannot index by location index with a non-integer key"")\r\n    839 \r\n--> 840             return self._get_loc(key,axis=axis)\r\n    841 \r\n    842     def _convert_to_indexer(self, obj, axis=0):\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/indexing.pyc in _get_loc(self, key, axis)\r\n     62 \r\n     63     def _get_loc(self, key, axis=0):\r\n---> 64         return self.obj._ixs(key, axis=axis)\r\n     65 \r\n     66     def _slice(self, obj, axis=0, raise_on_error=False):\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/series.pyc in _ixs(self, i, axis)\r\n    599                 label = self.index[i]\r\n    600                 if isinstance(label, Index):\r\n--> 601                     return self.reindex(label)\r\n    602                 else:\r\n    603                     return _index.get_value_at(self, i)\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/series.pyc in reindex(self, index, method, level, fill_value, limit, copy)\r\n   2660                                                 level=level, limit=limit)\r\n   2661         new_values = com.take_1d(self.values, indexer, fill_value=fill_value)\r\n-> 2662         return Series(new_values, index=new_index, name=self.name)\r\n   2663 \r\n   2664     def reindex_axis(self, labels, axis=0, **kwargs):\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/series.pyc in __new__(cls, data, index, dtype, name, copy)\r\n    487         else:\r\n    488             subarr = subarr.view(Series)\r\n--> 489         subarr.index = index\r\n    490         subarr.name = name\r\n    491 \r\n\r\n/RHS/packages/anaconda/pandas/pandas/lib.so in pandas.lib.SeriesIndex.__set__ (pandas/lib.c:29483)()\r\n\r\nAssertionError: Index length did not match values\r\n'"
4016,15950739,dalejung,jreback,2013-06-24 22:18:22,2013-08-07 20:20:41,2013-08-07 20:20:41,closed,,0.13,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/4016,"b'Pandas.ix[:, ""2002""] returns a DataFrame'","b'```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nind = pd.date_range(start=""2000"", freq=""D"", periods=1000)\r\ndf = pd.DataFrame(np.random.randn(len(ind), 5), index=ind, columns=list(\'ABCDE\'))\r\npanel = pd.Panel({\'frame_\'+c:df for c in list(\'ABC\')})\r\n\r\ntest1 = panel.ix[:, ""2002""]\r\ntest1.ndim # 3\r\ntype(test1) # pandas.core.frame.DataFrame\r\n\r\ntest2 = panel.ix[:, ""2002"":""2002-12-31""]\r\ntest2.ndim # 3\r\ntype(test2) # pandas.core.panel.Panel\r\n\r\nprint pd.__version__\r\n# 0.11.1.dev-d7fe745\r\n```\r\n\r\nWhen trying to grab all the data for year 2002, I get back a DataFrame. If I use a range for the major axis, then it returns a Panel. \r\n\r\nhttp://nbviewer.ipython.org/5853887'"
4013,15936374,floux,jreback,2013-06-24 17:22:36,2014-03-28 23:31:24,2014-03-28 23:31:24,closed,jtratner,0.14.0,2,Bug;Groupby;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/4013,b'df.groupby.agg() removes name of column MultiIndex at level 0',"b""When applying different functions to columns with a MultiIndex by supplying a mapping to groupby.agg(), the top-level name of the columns get lost.\r\n\r\nI believe this is a bug, because the names of the columns are unchanged (the total number of columns might be smaller, if not all columns are in the mapping, though).\r\n\r\nIn the example here I am using groupby.agg(), even though technically speaking I want to do a transformation. However, groupby.agg() seems to be the only apply-like method that allows the usage of a mapping for different functions per column. What would be the recommended way?\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame({\r\n   ...:         'exp' : ['A']*6 + ['B']*6,\r\n   ...:         'obj' : [1,1,1,2,2,2]*2,\r\n   ...:         'rep' : [1,2,3] * 4,\r\n   ...:         'var1' : range(12),\r\n   ...:         'var2' : range(12,24),\r\n   ...:         'var3' : range(24,36),\r\n   ...:         })\r\n\r\nIn [3]: df = df.set_index(['exp', 'obj', 'rep'])\r\n\r\nIn [4]: df = df.sort_index()\r\n\r\nIn [5]: df.columns.name = 'vars'\r\n\r\nIn [6]: print('before unstack: ', df.columns.names)\r\n('before unstack: ', ['vars'])\r\n\r\nIn [7]: df = df.unstack('rep')\r\n\r\nIn [8]: print('after unstack: ', df.columns.names)\r\n('after unstack: ', ['vars', 'rep'])\r\n\r\nIn [9]: funcs = {\r\n   ...:                 'var1' : lambda x: x - x.median(),\r\n   ...:                 'var2' : lambda y: y - y.mean(),\r\n   ...:                 'var3' : lambda y: y - y.sum(),\r\n   ...: }\r\n\r\nIn [10]: df1 = df.groupby(level=0).agg(funcs)\r\n\r\nIn [11]: print('after groupby.agg: ', df1.columns.names)\r\n('after groupby.agg: ', [None, 'rep'])\r\n```"""
4012,15934519,amanhanda,jreback,2013-06-24 16:41:40,2014-02-18 02:26:41,2014-02-18 02:26:41,closed,,0.14.0,12,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4012,"b'DataFrame to_record() with string, datetime column broken in 0.11 release'","b'Issue valid ONLY for numpy < 1.7, need to astype to object the datetime64[ns] first, then to_records\r\n\r\n\r\nTo recreate:\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ncurrent = datetime.datetime.combine(datetime.date.today(), datetime.time())\r\ndate_list = [(\'test{0}\'.format(i), current + datetime.timedelta(days=i)) for i in xrange(5)]\r\n\r\n# convert this to pandas df\r\ndf1 = pd.DataFrame(date_list, columns=[\'id\', \'day1\'])\r\n\r\nprint ""Data Frame""\r\nprint df1\r\ndfr = df1.to_records(index=False)\r\nprint dfr\r\nfor r in dfr:\r\n\tprint r[1], type(r[1])\r\n\r\nOutput with 0.10.1 (Good)\r\n\r\nData Frame\r\n      id                 day1\r\n0  test0  2013-06-24 00:00:00\r\n1  test1  2013-06-25 00:00:00\r\n2  test2  2013-06-26 00:00:00\r\n3  test3  2013-06-27 00:00:00\r\n4  test4  2013-06-28 00:00:00\r\n[(\'test0\', datetime.datetime(2013, 6, 24, 0, 0))\r\n (\'test1\', datetime.datetime(2013, 6, 25, 0, 0))\r\n (\'test2\', datetime.datetime(2013, 6, 26, 0, 0))\r\n (\'test3\', datetime.datetime(2013, 6, 27, 0, 0))\r\n (\'test4\', datetime.datetime(2013, 6, 28, 0, 0))]\r\n2013-06-24 00:00:00 <type \'datetime.datetime\'>\r\n2013-06-25 00:00:00 <type \'datetime.datetime\'>\r\n2013-06-26 00:00:00 <type \'datetime.datetime\'>\r\n2013-06-27 00:00:00 <type \'datetime.datetime\'>\r\n2013-06-28 00:00:00 <type \'datetime.datetime\'>\r\n\r\n```\r\nOutput with 0.11 (Bad)\r\n```python\r\nData Frame\r\n      id                day1\r\n0  test0 2013-06-24 00:00:00\r\n1  test1 2013-06-25 00:00:00\r\n2  test2 2013-06-26 00:00:00\r\n3  test3 2013-06-27 00:00:00\r\n4  test4 2013-06-28 00:00:00\r\n[(\'test0\', datetime.datetime(1970, 1, 16, 128, 0))\r\n (\'test1\', datetime.datetime(1970, 1, 16, 152, 0))\r\n (\'test2\', datetime.datetime(1970, 1, 16, 176, 0))\r\n (\'test3\', datetime.datetime(1970, 1, 16, 200, 0))\r\n (\'test4\', datetime.datetime(1970, 1, 16, 224, 0))]\r\n1970-01-16 128:00:00 <type \'numpy.datetime64\'>\r\n1970-01-16 152:00:00 <type \'numpy.datetime64\'>\r\n1970-01-16 176:00:00 <type \'numpy.datetime64\'>\r\n1970-01-16 200:00:00 <type \'numpy.datetime64\'>\r\n1970-01-16 224:00:00 <type \'numpy.datetime64\'>\r\n```\r\n\r\n1. Notice the datetime value.  This is just not a display issue.  The object has the incorrect value.\r\n2. 0.10.1 use to return datetime.datetime type, while 0.11 returns numpy.datetime64.\r\n\r\n\r\n\r\n\r\n\r\n'"
4006,15904144,miketkelly,jreback,2013-06-23 23:27:32,2014-02-18 18:54:28,2014-02-18 18:54:28,closed,,0.16.0,4,Bug;Dtypes;Strings,https://api.github.com/repos/pydata/pandas/issues/4006,b'BUG: Series.min() fails for string series containing None',"b""```\r\n>>> s = pd.Series([None, 'a', 'b'])\r\n>>> s\r\n0    None\r\n1       a\r\n2       b\r\ndtype: object\r\n>>> s.min()\r\ninf\r\n```"""
4005,15904051,miketkelly,jreback,2013-06-23 23:21:51,2013-09-22 17:12:48,2013-09-22 17:12:48,closed,,0.13,2,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/4005,"b""Can't assign NaT to datetime series with iloc""","b'```\r\n>>> s = pd.Series(pd.date_range(\'2000-1-1\', \'2000-1-2\'))\r\n>>> s\r\n0   2000-01-01 00:00:00\r\n1   2000-01-02 00:00:00\r\n```\r\nThis works:\r\n```\r\n>>> s[0] = pd.NaT\r\n>>> s\r\n0                   NaT\r\n1   2000-01-02 00:00:00\r\n````\r\n\r\nThis works:\r\n```\r\n>>> s.iloc[0] = pd.NaT.value\r\n>>> s\r\n0                   NaT\r\n1   2000-01-02 00:00:00\r\ndtype: datetime64[ns]\r\n```\r\n\r\nThis doens\'t:\r\n```\r\n>>> s.iloc[0] = pd.NaT\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""pandas/core/indexing.py"", line 88, in __setitem__\r\n    self._setitem_with_indexer(indexer, value)\r\n  File ""pandas/core/indexing.py"", line 185, in _setitem_with_indexer\r\n    values[indexer] = value\r\nValueError: Invalid date (-1,-1,-1) when converting to NumPy datetime\r\n```'"
4001,15892998,cpcloud,cpcloud,2013-06-23 04:14:42,2013-06-28 23:06:17,2013-06-28 23:06:17,closed,cpcloud,0.12,3,Bug;Data IO;Testing,https://api.github.com/repos/pydata/pandas/issues/4001,b'clean up data.py',"b'- [x] remove `isinstance(x, str)` checks\r\n- [x] handle urls with context managers\r\n- [x] get rid of `Exception` and use appropriate exception types\r\n- [x] pep8\r\n- [x] remove redundant parens in conditionals\r\n- [x] remove `\\` style line joins\r\n- [x] `range` -> `xrange`\r\n- [x] functions should not exit after a warning with `None` they should raise an exception\r\n- [x] turn all the str `+` style concats into format strings\r\n- [x] remove redundant calls to `str`\r\n- [x] remove exception handlers with no exception type\r\n- [x] replace calls to `__setattr__` and `__getattribute__` with `setattr` and `getattr`\r\n- [x] code coverage at 99%\r\n\r\nThis should remove the connection reset by peer testing error and I thought I would perform a little housekeeping while I was at it.'"
3995,15887538,jreback,jreback,2013-06-22 17:41:12,2014-11-14 12:49:34,2014-11-14 12:49:34,closed,,0.15.2,8,Bug;Data IO;Data Reader;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3995,b'BUG: Google finance is returning object dtype data (rather than float)',
3971,15808803,dalejung,jreback,2013-06-20 16:08:42,2013-09-22 17:13:54,2013-09-22 17:13:54,closed,,0.13,4,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3971,b'Series.where return unmodified series in upcasting corner case',"b""```python \r\nimport pandas as pd\r\nimport numpy as np\r\ns = pd.Series(range(10))\r\ns.where(s > 4, np.array([np.nan]))\r\n\r\nOut[4]:\r\n0    0\r\n1    1\r\n2    2\r\n3    3\r\n4    4\r\n5    5\r\n6    6\r\n7    7\r\n8    8\r\n9    9\r\ndtype: int64\r\n```\r\n\r\nThe culprit is that if upcast is required, ser doesn't get changed in the `com` call. Since we're making a copy of self anyways, shouldn't we always pass in change?\r\n\r\n```python\r\n        change = ser if inplace else None\r\n        com._maybe_upcast_putmask(ser,~cond,other,change=change)\r\n```\r\n\r\nI didn't hit this organically and I'm assuming it's a fairly contrived example. Was just reading code and found this.  """
3970,15808172,phobson,jreback,2013-06-20 15:56:19,2013-12-04 00:46:05,2013-06-28 18:41:53,closed,,0.12,13,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3970,b'Strange behavior assigning values to elements',"b'Basically, if you create a Data\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({ ""aa"":range(5), ""bb"":[2.2]*5})\r\n\r\ndf[""cc""] = 0.0 # remove this line, it works\r\n\r\nck = [True]*len(df)\r\n\r\ndf[""bb""].iloc[0] = .13\r\ndf_tmp = df.iloc[ck] # or remove this line and it\'ll work\r\ndf[""bb""].iloc[0] = .15 # doesn\'t happen\r\n\r\nprint df\r\n```\r\nwhich gives:\r\n```\r\n   aa    bb  cc\r\n0   0  0.13   0\r\n1   1  2.20   0\r\n2   2  2.20   0\r\n3   3  2.20   0\r\n4   4  2.20   0\r\n```\r\n\r\nVery strange.\r\n\r\nSpecs:\r\n```\r\npd.version.version\r\nOut[5]: \'0.11.0\'\r\n\r\nnp.version.version\r\nOut[7]: \'1.7.1\'\r\n```'"
3967,15785950,garaud,jreback,2013-06-20 08:03:57,2013-06-21 12:20:03,2013-06-21 01:22:33,closed,,0.12,13,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3967,b'read_csv with iterator=True does not seem to work as expected without chunksize',"b'Hi there,\r\n\r\nI tested:\r\n\r\n```python\r\ndata = """"""A,B,C\r\nfoo,1,2,3\r\nbar,4,5,6\r\nbaz,7,8,9\r\n""""""\r\nreader = pd.read_csv(StringIO(data), iterator=True)\r\n```\r\n\r\nI thought that I could do:\r\n\r\n    for row in reader:\r\n        print(row)\r\n\r\nsince `reader` is iterable. Unfortunately, it calls the generator `TextFileReader.__iter__`:\r\n\r\n```python\r\ndef __iter__(self):\r\n    try:\r\n        while True:\r\n            yield self.read(self.chunksize)\r\n    except StopIteration:\r\n        pass\r\n```\r\n\r\nwhere `self.chunksize` is `None`. Maybe set `self.chunsize` to 1 when it\'s not\r\ndefined and there is `iterator=True`. I\'ll propose a patch as soon as possible\r\n--- today or tomorrow.\r\n\r\nBest regards,\r\nDamien G.\r\n'"
3964,15773307,michaelaye,TomAugspurger,2013-06-19 23:01:28,2014-12-07 15:42:17,2014-12-07 15:42:17,closed,,0.15.2,3,Bug;Prio-medium;Visualization,https://api.github.com/repos/pydata/pandas/issues/3964,b'Plotting using subplots',"b'\r\nI am creating the attached plot like this:\r\n\r\n````python\r\nfig, axes = subplots(3,3, sharex=True, sharey=True)\r\nfor ch in range(1,7):\r\n    axis = axes.flatten()[ch-1]\r\n    cols = some code to map ch to col_names\r\n    df[cols].plot(ax=axis)\r\n# do the same for 3 more channels with different IDs\r\nfor ch in range(1,4):\r\n    axis = axes.flatten()[ch-1+6]\r\n    cols = some code to map ch to col_names, slightly different to above\r\n    df[cols].plot(ax=axis)\r\n````\r\n\r\nI have 2 problems with the result:\r\n1. sharex does not seem to work for time axes\r\n2. The last major x-ticklabel of the last row of axes is 10:00, while for the first 2 rows it is 9:00. I confirmed that all columns have data in the last row (index = 2011-04-02 10:00:00.718000) and not NaN which would have been excluded in the plot, potentially.\r\n\r\nPossibly the 2 probs are related?\r\n\r\n![all_thermal_channels](https://f.cloud.github.com/assets/69774/678376/2f81c2a0-d933-11e2-9d9e-d98d30e04822.png)\r\n'"
3963,15768714,TomAugspurger,jtratner,2013-06-19 21:12:19,2013-09-14 01:59:59,2013-09-14 01:59:59,closed,jtratner,0.13,6,Bug;Data IO;IO CSV,https://api.github.com/repos/pydata/pandas/issues/3963,b'BUG: python 3 compression and read_fwf',"b""Getting a ```TypeError: Type str doesn't support the buffer API``` when using ```read_fwf``` on a compressed file with python 3 only; works in python 2.7\r\n\r\nExample:\r\n\r\nwith a file ```fwf.bug.txt``` like\r\n\r\n```\r\n1111111111\r\n2222222222\r\n3333333333\r\n4444444444\r\n\r\n```\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nprint(pd.__version__)\r\n\r\nwidths = [5, 5]\r\n\r\n!gzip -d fwf_bug.txt\r\n\r\ndf = pd.read_fwf('fwf_bug.txt', widths=widths, names=['one', 'two'])\r\nprint(df)\r\n\r\n!gzip fwf_bug.txt\r\n\r\n# python 3 throws an error here.\r\ndf2 = pd.read_fwf('fwf_bug.txt.gz', widths=widths,\r\n                  names=['one', 'two'], compression='gzip')\r\n\r\nprint(df2)\r\n```\r\n\r\nVersions:\r\npython3: `0.11.1.dev-4d06037` should be most recent\r\npython2: `0.11.1.dev-3ebfef9`\r\n\r\nI can paste the full traceback if you'd like."""
3950,15719545,hayd,jreback,2013-06-18 23:46:13,2014-07-23 22:09:51,2014-06-21 20:10:21,closed,,0.14.1,10,Bug;MultiIndex;Timeseries;Timezones,https://api.github.com/repos/pydata/pandas/issues/3950,b'tz info lost when creating multiindex',"b""see http://stackoverflow.com/questions/17159207/change-timezone-of-date-time-column-in-pandas-and-add-as-hierarchical-index/17159276#17159276\r\n\r\n```\r\ndat = pd.DataFrame({'label':['a', 'a', 'a', 'b', 'b', 'b'], 'datetime':['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00'], 'value':range(6)})\r\ndat.index = pd.to_datetime(dat.pop('datetime'), utc=True)\r\ndat.index = dat.index.tz_localize('UTC').tz_convert('US/Pacific')\r\n\r\ndat\r\n                          label  value\r\ndatetime\r\n2011-07-19 00:00:00-07:00     a      0\r\n2011-07-19 01:00:00-07:00     a      1\r\n2011-07-19 02:00:00-07:00     a      2\r\n2011-07-19 00:00:00-07:00     b      3\r\n2011-07-19 01:00:00-07:00     b      4\r\n2011-07-19 02:00:00-07:00     b      5\r\n```\r\nIf we add another index (we lose the tz):\r\n```\r\nIn [14]: dat.set_index('label', append=True).swaplevel(0, 1)\r\nOut[14]:\r\n                           value\r\nlabel datetime\r\na     2011-07-19 07:00:00      0\r\n      2011-07-19 08:00:00      1\r\n      2011-07-19 09:00:00      2\r\nb     2011-07-19 07:00:00      3\r\n      2011-07-19 08:00:00      4\r\n      2011-07-19 09:00:00      5\r\n```"""
3944,15681728,jreback,y-p,2013-06-18 11:22:42,2014-04-25 12:57:40,2014-02-04 09:24:55,closed,,0.14.0,13,API Design;Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3944,b'API/BUG: inconsistent return in Timestamp/to_datetime for current year',"b""after #3890, still unresolved / inconsistent\r\nnon-trivial to fix, so pushing to 0.12\r\n\r\n```\r\nIn [14]: Timestamp('2012')\r\nOut[14]: Timestamp('2012-06-18 00:00:00', tz=None)\r\n\r\nIn [15]: to_datetime('2012')\r\nOut[15]: Timestamp('2012-01-01 00:00:00', tz=None)\r\n```\r\n```\r\nIn [17]: pd.date_range('2014', '2015', freq='M')\r\nOut[17]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2014-04-30, ..., 2015-03-31]\r\nLength: 12, Freq: M, Timezone: None\r\n```"""
3922,15601818,buckeye1,jreback,2013-06-16 07:24:05,2014-09-08 14:22:11,2013-06-21 20:16:55,closed,,0.12,53,Bug,https://api.github.com/repos/pydata/pandas/issues/3922,b'pandas appears to crash doing pivot table from 10 minute examples',"b'pandas 0.11.0 installed with Python 2.7.2 using ""pip install pandas"" on fully patched mac osx 10.8.5, running in a virtual environment.  running any pivot table against any DataFrame create so for -- nothing happens.  pressing ctrl-c I see the latest lines run are:\r\n\r\n    422         while com._long_prod(sizes) >= 2 ** 63:\r\n    423             i = len(sizes)\r\n--> 424             while com._long_prod(sizes[:i]) >= 2 ** 63:\r\n    425                 i -= 1\r\n\r\nThe problem is reproducible using http://pandas.pydata.org/pandas-docs/stable/10min.html and running:\r\n\r\nfrom pandas import DataFrame, Series\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nthen running lines [97], [98], and [99] in iPython and Python native.\r\n\r\nI can\'t see any way to debug or go further. '"
3917,15595441,hayd,jreback,2013-06-15 18:48:37,2014-04-05 19:32:40,2014-03-27 22:23:50,closed,,0.14.0,4,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/3917,b'BUG: Nested sort with NaN',"b'Nested sort doesn\'t seem to work with NaNs, see [this SO question](http://stackoverflow.com/questions/17126500/pandas-nested-sort-and-nan).\r\n\r\n```\r\nIn [11]: df\r\nOut[11]:\r\n    a   b\r\n0   1   9\r\n1   2 NaN\r\n2 NaN   5\r\n3   1   2\r\n4   6   5\r\n5   8   4\r\n6   4   5\r\n\r\nIn [12]: df.sort(columns=[""a"",""b""])\r\nOut[12]:\r\n    a   b\r\n3   1   2\r\n0   1   9\r\n1   2 NaN\r\n2 NaN   5\r\n6   4   5\r\n4   6   5\r\n5   8   4\r\n```\r\n\r\n(It works as expected using a single columns)'"
3911,15586280,wesm,jreback,2013-06-15 04:04:53,2013-06-15 15:00:25,2013-06-15 12:25:21,closed,,0.12,9,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3911,b'GroupBy.apply type upcasting regression',"b""Blocker for 0.11.1. This got broken sometime between 0.11.0 and master; I haven't had time to bisect yet. Essentially, write a mixed type DataFrame and a groupby function that extracts a row. This used to do type inference and convert object back to numeric in the columns of the resulting DataFrame. Didn't have a test so I don't blame anyone for breaking it by accident =)\r\n\r\n```\r\nIn [151]: cafdata.dtypes\r\nOut[151]:\r\nid            int64\r\nfood         object\r\nfgroup       object\r\nnutrient     object\r\nngroup       object\r\nunits        object\r\nvalue       float64\r\ndtype: object\r\n\r\nIn [152]:\r\ndef max_value(group):\r\n    return group.ix[group['value'].idxmax()]\r\n \r\nmax_value(cafdata)\r\n\r\nOut[152]:\r\nid                                      14366\r\nfood        Tea, instant, unsweetened, powder\r\nfgroup                              Beverages\r\nnutrient                             Caffeine\r\nngroup                                  Other\r\nunits                                      mg\r\nvalue                                    3680\r\nName: 336702, dtype: object\r\n\r\nIn [153]: cafdata.groupby('fgroup').apply(max_value).dtypes\r\nOut[153]:\r\nid          object\r\nfood        object\r\nfgroup      object\r\nnutrient    object\r\nngroup      object\r\nunits       object\r\nvalue       object\r\ndtype: object\r\n```"""
3904,15557357,jreback,jreback,2013-06-14 13:12:33,2013-09-22 17:07:05,2013-09-22 17:07:05,closed,,0.13,1,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/3904,b'BUG/API: date_conversion in read_csv with int vs float component parts',"b'from the ML\r\nhttps://groups.google.com/forum/?fromgroups#!topic/pydata/Lc10-vz4NpU\r\n\r\nnot really sure if this is a bug, more of a convention\r\n\r\n```\r\nIn [58]: data = ""YEAR,DOY,TEMP\\n2001,1,10.5\\n2001,2,11.8\\n2001,3,67.4""\r\n\r\nIn [59]: def date_converter(x):                                                                                                         \r\n        return dt.datetime.strptime(x, \'%Y %j\')\r\n\r\nIn [60]: pandas.read_csv(StringIO(data), parse_dates=[[0,1]], index_col=0, sep="","", keep_date_col=True, date_parser=date_converter)\r\nOut[60]: \r\n            YEAR DOY  TEMP\r\nYEAR_DOY                  \r\n2001-01-01  2001   1  10.5\r\n2001-01-02  2001   2  11.8\r\n2001-01-03  2001   3  67.4\r\n\r\nIn [61]: data = ""YEAR,DOY,TEMP\\n2001.0,1.0,10.5\\n2001.0,2.0,11.8\\n2001.0,3,67.4""\r\n\r\nIn [62]: def date_converter2(*args):                                                                                                     \r\n    return dt.datetime(int(float(args[0])),int(float(args[1])),1)\r\n   ....: \r\n\r\nIn [63]: pandas.read_csv(StringIO(data), parse_dates=[[0,1]], index_col=0, sep="","", keep_date_col=True, date_parser=date_converter2)\r\nOut[63]: \r\n              YEAR  DOY  TEMP\r\nYEAR_DOY                     \r\n2001-01-01  2001.0  1.0  10.5\r\n2001-02-01  2001.0  2.0  11.8\r\n2001-03-01  2001.0    3  67.4\r\n\r\n```'"
3899,15536892,cpcloud,cpcloud,2013-06-14 00:11:08,2013-10-04 02:39:18,2013-10-04 02:39:18,closed,cpcloud,0.13,17,Bug;Error Reporting;Indexing;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3899,b'recursion limit reached when joining period and datetimeindexes',"b""one should probably *not* be trying to join these types of indices. even so, a stack overflow is unacceptable. there should at least be an error message...\r\n\r\ncode:\r\n\r\n```\r\nIn [9]: df = mkdf(10, 10, data_gen_f=lambda *args: randint(2), c_idx_type='p', r_idx_type='dt')\r\n\r\nIn [10]: s = df.iloc[:5, 0]\r\n\r\nIn [11]: s.index.join(df.columns, how='outer')\r\n```"""
3880,15491820,joeb1415,jreback,2013-06-13 07:28:08,2013-06-13 19:46:37,2013-06-13 19:46:37,closed,,0.12,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3880,b'DataFrame.loc[] loses index name',"b""    In [1]: from pandas import DataFrame\r\n    \r\n    In [2]: x = DataFrame([[1, 1], [1, 1]])\r\n    \r\n    In [3]: x.index.name = 'index_name'\r\n    \r\n    In [4]: x.iloc[[0, 1]].index.name\r\n    Out[4]: 'index_name'\r\n    \r\n    In [5]: x.loc[[0, 1]].index.name\r\n    \r\n    In [6]: x.loc[[0, 1]].index.name is None\r\n    Out[6]: True"""
3873,15474702,gtakacs,cpcloud,2013-06-12 21:17:03,2013-06-13 12:22:47,2013-06-13 12:22:47,closed,,0.12,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3873,b'itertuples() fails after selecting a column twice',"b'The following code gives incorrect result (using pandas 0.11):\r\n\r\n```python\r\nimport pandas\r\ndf = pandas.DataFrame(data={""a"": [1, 2, 3], ""b"": [4, 5, 6]})\r\nprint df[[""a"", ""a""]].values # works\r\nprint list(df[[""a"", ""a""]].itertuples()) # FAILS!\r\n```\r\n\r\nThe last lines prints [(0, \'a\', \'a\'), (1, \'a\', \'a\')] instead of [(0, 1, 1), (1, 2, 2), (2, 3, 3)].\r\n'"
3870,15469141,vgod,jreback,2013-06-12 19:26:01,2013-10-02 21:04:46,2013-09-22 17:04:18,closed,,0.13,2,Bug;Duplicate;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3870,b'DataFrame.resample has a bug that produces NaN with some specific indices.',"b""might be related to #3849\r\n\r\nThis bug can be reproduced with 0.11.1.dev-0538c43 and 0.11.0.\r\n\r\nThe following code should keep the values on 2012-09-12, 2012-09-19, and 2012-09-26 and insert NaN between these dates. However, the values of all rows between 2012-09-12 to 2012-09-26 are NaN.\r\n\r\n```python\r\nimport pandas as pd\r\npd.DataFrame({'value':[1,2,3]},index= pd.Series([pd.Timestamp('2012-09-12 10:00:00'),  pd.Timestamp('2012-09-19 10:00:00'), pd.Timestamp('2012-09-26 10:00:00')])).resample('D')\r\n```\r\nOutput:\r\n```\r\n            value\r\n2012-09-12    NaN\r\n2012-09-13    NaN\r\n2012-09-14    NaN\r\n2012-09-15    NaN\r\n2012-09-16    NaN \r\n2012-09-17    NaN\r\n2012-09-18    NaN\r\n2012-09-19    NaN\r\n2012-09-20    NaN\r\n2012-09-21    NaN\r\n2012-09-22    NaN\r\n2012-09-23    NaN\r\n2012-09-24    NaN\r\n2012-09-25    NaN \r\n2012-09-26    NaN\r\n```\r\n\r\nThe weird thing was, if I changed the time of one index slightly, the output became correct.\r\n\r\n```python\r\npd.DataFrame({'value':[1,2,3]},index= pd.Series([pd.Timestamp('2012-09-12 10:00:00'),  pd.Timestamp('2012-09-19 10:00:00'), pd.Timestamp('2012-09-26 00:00:00')])).resample('D')\r\n```\r\n\r\n```\r\n            value\r\n2012-09-12      1\r\n2012-09-13    NaN\r\n2012-09-14    NaN\r\n2012-09-15    NaN\r\n2012-09-16    NaN\r\n2012-09-17    NaN\r\n2012-09-18    NaN\r\n2012-09-19      2\r\n2012-09-20    NaN\r\n2012-09-21    NaN\r\n2012-09-22    NaN\r\n2012-09-23    NaN\r\n2012-09-24    NaN\r\n2012-09-25    NaN\r\n2012-09-26      3\r\n```\r\n"""
3869,15467834,dmlockhart,jtratner,2013-06-12 19:04:13,2013-09-05 02:43:34,2013-09-05 02:18:00,closed,,0.13,9,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3869,b'BUG: Key Error: range exception when printing DataFrame',"b'Here\'s a reprodu\r\n```\r\ndf = pd.DataFrame({ \'A\' : [\'foo\',""~:{range}:0""], \'B\' : [\'bar\',\'bah\'] })\r\ndf\r\n             A    B\r\n0          foo  bar\r\n1  ~:{range}:0  bah\r\n\r\ndf.set_index([\'A\']).info()\r\n*** KeyError: \'range\'\r\n```\r\n\r\nin core/index.py, check if ``head/tail`` is not already an instance of a str\r\n```\r\n    def summary(self, name=None):\r\n        if len(self) > 0:\r\n            head = self[0]\r\n            if hasattr(head,\'format\'):\r\n                head = head.format()\r\n            tail = self[-1]\r\n            if hasattr(tail,\'format\'):\r\n                tail = tail.format()\r\n```\r\n\r\nPrinting a DataFrame created from two Series objects (previously columns in other DataFrames) results in a ""Key Error: \'range\'"" exception being raised.  The DataFrame creation seems to work fine.  Printing other DataFrames with the same ""problematic"" index also works okay.\r\n\r\nTest code:\r\n\r\n    import pstats\r\n    import pandas as pd\r\n    \r\n    # Import some cProfile data\r\n    run1 = pstats.Stats(\'run1.prof\')\r\n    run2 = pstats.Stats(\'run2.prof\')\r\n    \r\n    # Utility function to convert pstats dict into a list of lists\r\n    def pstats_to_list( stats ):\r\n      plist = []\r\n      for key, value in stats.strip_dirs().stats.items():\r\n        filename, lineno, func_name = key\r\n        ccalls, ncalls, total_time, cum_time, callers = value\r\n        name = ""{}:{}:{}"".format( filename, func_name, lineno )\r\n        plist.append( [name, ncalls, total_time, cum_time] )\r\n      return plist\r\n    \r\n    jit_list   = pstats_to_list( run1 )\r\n    nojit_list = pstats_to_list( run2 )\r\n    \r\n    # Create DataFrames for the profile run data\r\n    columns=[\'name\',\'ncalls\',\'ttime\', \'ctime\']\r\n    jdf = pd.DataFrame( jit_list,   columns = columns )\r\n    ndf = pd.DataFrame( nojit_list, columns = columns )\r\n    \r\n    # Set the \'name\' column to be the index (for plotting)\r\n    jdf = jdf.set_index( \'name\' )\r\n    ndf = ndf.set_index( \'name\' )\r\n    \r\n    # These DataFrames print fine\r\n    print jdf\r\n    print ndf\r\n    \r\n    # Extract out the \'ttime\' columns\r\n    x = ndf[\'ttime\']\r\n    y = jdf[\'ttime\']\r\n    \r\n    # Create a new DataFrame using the \'ttime\' Series from jdf and ndf\r\n    z = pd.DataFrame( {\'jit\': x, \'nojit\': y } )\r\n    \r\n    # Print some data.... this works\r\n    print z[0:10]\r\n    \r\n    # Print some data.... this raises ""KeyError: \'range\'""\r\n    print z\r\n    \r\n'"
3866,15459982,martingoodson,jreback,2013-06-12 16:26:25,2013-09-29 19:28:09,2013-09-29 19:28:09,closed,,0.13.1,4,Bug;Data IO;IO CSV;Prio-high,https://api.github.com/repos/pydata/pandas/issues/3866,b'BUG: Unexpected behaviour when reading large text files with mixed datatypes',"b""read_csv gives unexpected behaviour with large files if a column contains both strings and integers. eg\r\n```python\r\n\r\n>>> df=DataFrame({'colA':range(500000-1)+['apple', 'pear']+range(500000-1)})\r\nlen(set(df.colA))\r\n500001\r\n\r\n>>> df.to_csv('testpandas2.txt')\r\n>>> df2=read_csv('testpandas2.txt')\r\n>>> len(set(df2.colA))\r\n762143\r\n\r\n >>> pandas.__version__\r\n'0.11.0'\r\n```\r\nIt seems some of the integers are parsed as integers and others as strings.\r\n\r\n```python\r\n>>> list(set(df2.colA))[-10:]\r\n['282248', '282249', '282240', '282241', '282242', '15679', '282244', '282245', '282246', '282247']\r\n>>> list(set(df2.colA))[:10]\r\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n```\r\n"""
3862,15451121,lodagro,jreback,2013-06-12 13:39:21,2013-10-03 00:32:01,2013-08-16 19:27:34,closed,,0.13,5,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/3862,b'corner case when initializing Series from scalar',"b""```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.11.1.dev-45d298d'\r\n\r\nIn [3]: pd.Series(10)\r\nOut[3]: 10\r\n```\r\n\r\nOk, when setting index explicitly\r\n\r\n```\r\nIn [4]: pd.Series(10, index=[0])\r\nOut[4]:\r\n0    10\r\ndtype: int64\r\n```"""
3852,15416167,jreback,cpcloud,2013-06-11 18:57:05,2013-06-24 22:19:17,2013-06-24 22:19:17,closed,,0.12,8,Bug;Error Reporting,https://api.github.com/repos/pydata/pandas/issues/3852,b'RPT/BUG: yahoo retrieve failure error message is uninformative',b'https://www.travis-ci.org/pydata/pandas/jobs/7992099'
3849,15401275,jreback,jreback,2013-06-11 14:23:16,2013-09-25 12:31:36,2013-09-25 12:31:36,closed,,0.13,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3849,b'BUG: resample in single group case not using how passed function',"b""```\r\nIn [34]: s = Series([30.1,31.6],index=[Timestamp('20070915 15:30:00'),Timestamp('20070915 15:40:00')])\r\n\r\nIn [35]: s\r\nOut[35]: \r\n2007-09-15 15:30:00    30.1\r\n2007-09-15 15:40:00    31.6\r\ndtype: float64\r\n\r\nIn [36]: s.resample('D',how=lambda x: np.std(x))\r\nOut[36]: \r\n2007-09-15   NaN\r\nFreq: D, dtype: float64\r\n\r\nIn [37]: s.groupby(pd.TimeGrouper('D')).apply(lambda x: np.std(x))\r\nOut[37]: \r\n2007-09-15    0.75\r\ndtype: float64\r\n\r\n```"""
3844,15380909,jsudheer,jreback,2013-06-11 03:47:13,2014-01-19 00:47:05,2013-06-11 14:15:58,closed,,0.12,15,Bug,https://api.github.com/repos/pydata/pandas/issues/3844,b'passing np.std to resample does not actually call the numpy function',"b""\r\n    Hi,\r\n    Again the std issue\r\n    When I call numpy.std outside pandas I get the population std where as if I call inside it gives me sample std. How can I ask pandas to allow np.std to behave as it originally does?\r\n    wit best regards,\r\n    Sudheer\r\n\r\n    In [13]: ts[53153:53155]\r\n    Out[13]:\r\n    2007-09-15 15:30:00 30.1\r\n    2007-09-15 15:40:00 31.6\r\n    Freq: 10T\r\n\r\n    In [14]: np.std(ts[53153:53155])\r\n    Out[14]: 0.75\r\n\r\n    In [15]: ts[53153:53155].resample('D',how=np.std)\r\n    Out[15]:\r\n    2007-09-15 1.06066\r\n    Freq: D\r\n\r\n"""
3836,15356080,hayd,hayd,2013-06-10 16:21:32,2013-06-10 19:13:29,2013-06-10 19:13:29,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/3836,b'Strange results when slicing objects using ix',"b""Here's a weird one [from SO](http://stackoverflow.com/questions/17020763/pandas-0-10-1-to-0-11-0-is-method/). Bug in 0.11.0 (fixed in dev).\r\n```\r\naccurate_ICB = ['SA EQUITY CFD', 'SA EQUITY', 'SA SSF']\r\ndata = {'Classification': ['SA EQUITY CFD', 'bbb', 'SA EQUITY', 'SA SSF', 'aaa'],\r\n    'Random': [1,2,3,4,5],\r\n    'X': ['correct', 'wrong','correct', 'correct','wrong']}\r\ndf =pd.DataFrame(data)\r\nx = df[~df.Classification.isin(accurate_ICB)]\r\ndf.ix[x.index,'X'] = df['Classification']\r\nprint df\r\n```\r\n\r\nIn pandas 0.11.0 it produces:\r\n\r\n```\r\n  Classification  Random        X\r\n0  SA EQUITY CFD       1  correct\r\n1            bbb       2      bbb\r\n2      SA EQUITY       3  correct\r\n3         SA SSF       4  correct\r\n4            aaa       5      bbb\r\n```\r\n\r\nIn pandas 0.11.1dev it produces:\r\n\r\n```\r\n  Classification  Random        X\r\n0  SA EQUITY CFD       1  correct\r\n1            bbb       2      bbb\r\n2      SA EQUITY       3  correct\r\n3         SA SSF       4  correct\r\n4            aaa       5      aaa\r\n```"""
3794,15278529,jreback,jreback,2013-06-07 15:51:31,2014-03-15 14:33:32,2014-03-15 14:33:32,closed,,0.14.0,12,Bug;Enhancement;Groupby,https://api.github.com/repos/pydata/pandas/issues/3794,b'BUG/ENH: groupby with a list of customgroup and string should work',"b""see #3791, #2450\r\n\r\nThe following should work (but raises now)\r\n\r\n```\r\nimport datetime as DT\r\n\r\ndf = pd.DataFrame({\r\n'Branch' : 'A A A A A B'.split(),\r\n'Buyer': 'Carl Mark Carl Joe Joe Carl'.split(),\r\n'Quantity': [1,3,5,8,9,3],\r\n'Date' : [\r\nDT.datetime(2013,1,1,13,0),\r\nDT.datetime(2013,1,1,13,5),\r\nDT.datetime(2013,10,1,20,0),\r\nDT.datetime(2013,10,2,10,0),\r\nDT.datetime(2013,12,2,12,0),                                      \r\nDT.datetime(2013,12,2,14,0),\r\n]})\r\n       \r\ndf = df.set_index('Date', drop=False)\r\ndf.groupby([TimeGrouper('6M'),'Buyer']).sum()\r\n```"""
3791,15270623,aschilling,aschilling,2013-06-07 12:54:41,2013-06-25 18:38:40,2013-06-07 15:40:59,closed,,0.12,19,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3791,b'GroupBy using TimeGrouper does not work',"b'BUG: TimeGrouper not too friendly with other groups, e.g.\r\n\r\n``df.set_index(\'Date\').groupby([pd.TimeGrouper(\'6M\'),\'Branch\']).sum()``\r\nshould work\r\n\r\n------\r\n\r\nHi everybody,\r\n\r\nI found two issues with TimeGrouper:\r\n\r\n1) TimeGrouper does not work at all:\r\n\r\nLet\'s take the following example:\r\n\r\ndf = pd.DataFrame({\r\n    \'Branch\' : \'A A A A A B\'.split(),\r\n    \'Buyer\': \'Carl Mark Carl Joe Joe Carl\'.split(),\r\n    \'Quantity\': [1,3,5,8,9,3],\r\n    \'Date\' : [\r\n        DT.datetime(2013,1,1,13,0),\r\n        DT.datetime(2013,1,1,13,5),\r\n        DT.datetime(2013,10,1,20,0),\r\n        DT.datetime(2013,10,3,10,0),\r\n        DT.datetime(2013,12,2,12,0),                                      \r\n        DT.datetime(2013,12,2,14,0),\r\n        ]})\r\n\r\ngr = df.groupby(pd.TimeGrouper(freq=\'6M\'))\r\n\r\ndef testgr(df):\r\n    print df\r\n\r\ngr.apply(testgr)\r\n\r\n\r\nThis will raise the Exception: ""Exception: All objects passed were None""\r\n\r\n2) With previous Panda\'s version it was not possible to combine TimeGrouper with another criteria such as ""Branch"" in my case. \r\n\r\nThank you very much\r\n\r\nAndy\r\n\r\n'"
3781,15243194,danielballan,jreback,2013-06-06 20:21:42,2014-02-18 19:21:48,2014-02-18 19:21:48,closed,,0.14.0,3,API Design;Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/3781,b'BUG: Numpy Ufunc generates unintuitive AttributeError on object arrays',"b""Example:\r\n\r\n    In [15]: np.log(Series([1,2,3,4], dtype='object'))\r\n    ---------------------------------------------------------------------------\r\n    AttributeError                            Traceback (most recent call last)\r\n    <ipython-input-15-25deca6462b7> in <module>()\r\n    ----> 1 np.log(Series([1,2,3,4], dtype='object'))\r\n\r\n    AttributeError: log\r\n\r\nThis is really a [numpy issue](http://projects.scipy.org/numpy/ticket/1013) (opened by Wes back in 2009, as Andy Hayden pointed out). But since object arrays are especially common in database output, as was the case in this [SO question](http://stackoverflow.com/questions/16968433/error-when-trying-to-apply-log-method-to-pandas-data-frame-column-in-python/1696849), there some places in pandas where we can help avoid this error.\r\n\r\n"""
3776,15224172,jseabold,jreback,2013-06-06 14:05:35,2013-06-06 14:41:32,2013-06-06 14:38:58,closed,,0.12,5,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3776,b'concat no longer works on a generator',"b'I updated pandas and now my data loading scripts are broken.\r\n\r\nSomething like this works on 0.11.0\r\n\r\n```\r\npandas.concat(pandas.DataFrame(np.random.rand(5,5)) for _ in range(3)).reset_index()\r\n```\r\n\r\nDoes not work on recent master (6cd4adc).'"
3753,15123911,flo-rian,jreback,2013-06-04 15:10:42,2015-01-26 01:02:54,2015-01-26 01:02:54,closed,,0.16.0,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3753,b'BUG: rplot.GeomDensity2D flips axes',"b""The axes drawn by rplot.GeomDensity2D are flipped.\r\nline 565 reads:\r\n        `ax.contour(Z, extent=[x_min, x_max, y_min, y_max])`\r\nbut should read:\r\n        `ax.contour(Z.T, extent=[x_min, x_max, y_min, y_max])`\r\nThe following test code gives the results shown below:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport pandas.tools.rplot as rplot\r\n\r\nx = np.arange(1,30)\r\ny = x*x\r\nx = x + np.random.normal(size=x.shape)\r\ny = y + np.random.normal(size=y.shape)\r\ndata = data = pd.DataFrame({'x': x, 'y': y})\r\nplot = rplot.RPlot(data, x='x', y='y')\r\nplot.add(rplot.GeomScatter())\r\nplot.add(rplot.GeomDensity2D())\r\nplot.add(rplot.GeomPolyFit(degree=2))\r\nplot.render(plt.gcf())\r\nplt.show()\r\n```\r\n\r\n![density_as_is](https://f.cloud.github.com/assets/4612277/606348/ce6e5b74-cd28-11e2-9c2d-575eef4a8186.png)\r\n\r\n![density_fixed](https://f.cloud.github.com/assets/4612277/606349/ce706be4-cd28-11e2-8ac9-ea5a730c033c.png)"""
3750,15084931,jreback,jreback,2013-06-03 19:42:15,2013-06-13 15:38:49,2013-06-06 00:14:00,closed,,0.12,3,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/3750,b'BUG: HDFStore fails on py3 with pytables 3.0',"b""we are not testing this now on travis\r\n\r\nfails because pytables 3.0 reads back strings as bytes and \r\ninfer_dtype doesn't deal with this"""
3748,15081060,hmgaudecker,jreback,2013-06-03 18:25:22,2013-09-11 19:38:12,2013-06-03 21:06:35,closed,,0.12,16,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3748,b'Selecting columns from a HDFStore with a MultiIndex fails',"b""Using the example from the docs here: http://pandas.pydata.org/pandas-docs/dev/io.html#storing-multi-index-dataframes but specifying the 'columns' in `[1219]` keyword when selecting:\r\n\r\n    store.select('df_mi', columns=['A', 'B'])\r\n\r\nleads to the failure below. The original example works, as does selecting columns with a standard index. I'm on a fairly recent master (~1 week) and Python 3.3. Hope this is not expected and I did not miss an issue, I did search for a bit.\r\n\r\n```\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-67-678a9c77d998> in <module>()\r\n----> 1 store.select('df_mi', columns=['A', 'B'])\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/io/pytables.py in select(self, key, where, start, stop, columns, iterator, chunksize, **kwargs)\r\n    413             return TableIterator(func, nrows=s.nrows, start=start, stop=stop, chunksize=chunksize)\r\n    414 \r\n--> 415         return TableIterator(func, nrows=s.nrows, start=start, stop=stop).get_values()\r\n    416 \r\n    417     def select_as_coordinates(self, key, where=None, start=None, stop=None, **kwargs):\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/io/pytables.py in get_values(self)\r\n    931 \r\n    932     def get_values(self):\r\n--> 933         return self.func(self.start, self.stop)\r\n    934 \r\n    935 \r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/io/pytables.py in func(_start, _stop)\r\n    408         # what we are actually going to do for a chunk\r\n    409         def func(_start, _stop):\r\n--> 410             return s.read(where=where, start=_start, stop=_stop, columns=columns, **kwargs)\r\n    411 \r\n    412         if iterator or chunksize is not None:\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/io/pytables.py in read(self, *args, **kwargs)\r\n   3142     def read(self, *args, **kwargs):\r\n   3143         df = super(AppendableMultiFrameTable, self).read(*args, **kwargs)\r\n-> 3144         df.set_index(self.levels, inplace=True)\r\n   3145         return df\r\n   3146 \r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/core/frame.py in set_index(self, keys, drop, append, inplace, verify_integrity)\r\n   2795                 names.append(None)\r\n   2796             else:\r\n-> 2797                 level = frame[col].values\r\n   2798                 names.append(col)\r\n   2799                 if drop:\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/core/frame.py in __getitem__(self, key)\r\n   1992         else:\r\n   1993             # get column\r\n-> 1994             return self._get_item_cache(key)\r\n   1995 \r\n   1996     def _getitem_slice(self, key):\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/core/generic.py in _get_item_cache(self, item)\r\n    572             return cache[item]\r\n    573         except Exception:\r\n--> 574             values = self._data.get(item)\r\n    575             res = self._box_item_values(item, values)\r\n    576             cache[item] = res\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/core/internals.py in get(self, item)\r\n   1646 \r\n   1647     def get(self, item):\r\n-> 1648         _, block = self._find_block(item)\r\n   1649         return block.get(item)\r\n   1650 \r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/core/internals.py in _find_block(self, item)\r\n   1773 \r\n   1774     def _find_block(self, item):\r\n-> 1775         self._check_have(item)\r\n   1776         for i, block in enumerate(self.blocks):\r\n   1777             if item in block:\r\n\r\n/home/xxx/anaconda/envs/py33/lib/python3.3/site-packages/pandas-0.12.0.dev_2bd1cf8-py3.3-linux-x86_64.egg/pandas/core/internals.py in _check_have(self, item)\r\n   1780     def _check_have(self, item):\r\n   1781         if item not in self.items:\r\n-> 1782             raise KeyError('no item named %s' % com.pprint_thing(item))\r\n   1783 \r\n   1784     def reindex_axis(self, new_axis, method=None, axis=0, copy=True):\r\n\r\nKeyError: 'no item named foo'\r\n```"""
3745,15071566,jreback,jorisvandenbossche,2013-06-03 15:06:48,2014-09-20 19:01:26,2014-09-20 19:01:26,closed,,0.15.0,2,Bug;Data IO;IO SQL,https://api.github.com/repos/pydata/pandas/issues/3745,b'BUG: handle no rows returns from a sql query',b'related to #3523'
3742,15065998,thriveth,cpcloud,2013-06-03 13:12:28,2013-07-02 00:07:29,2013-06-27 19:11:37,closed,,0.13,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3742,b'MultiIndex names attribute broken when using set_value() to expand DataFrame()',"b""I have raised the issue in [this question](http://stackoverflow.com/questions/15621143/pandas-df-set-value-method-erases-resets-column-names-of-multiindex/15624426#15624426) on Stack Overflow, but I'm not sure it ever made it to the Pandas issue tracker.\r\n\r\nI have a MultiIndex'ed DataFrame which I want to expand by using ```set_value()```, but doing this destroys the ```names``` attribute of the index. This does not happen when setting the value of an already existing entry in the DataFrame. An easily reproducible example is to create the dataframe by:\r\n\r\n```python\r\nlev1 = ['hans', 'hans', 'hans', 'grethe', 'grethe', 'grethe']\r\nlev2 = ['1', '2', '3'] * 2\r\nidx = pd.MultiIndex.from_arrays(\r\n    [lev1, lev2], \r\n    names=['Name', 'Number'])\r\ndf = pd.DataFrame(\r\n    np.random.randn(6, 4),\r\n    columns=['one', 'two', 'three', 'four'],\r\n    index=idx)\r\ndf = df.sortlevel()\r\ndf \r\n```\r\nThis shows a neat and nice object, just as I expected, with proper naming of the index columns. If I now run:\r\n\r\n```python\r\ndf.set_value(('grethe', '3'), 'one', 99.34)\r\n```\r\n\r\nthe result is also as expected. But if I run:\r\n\r\n```python\r\ndf.set_value(('grethe', '4'), 'one', 99.34)\r\n```\r\nThe column names of the index are gone, and the ```names``` attribute has been set to ```[None, None]```."""
3740,15045045,fonnesbeck,jreback,2013-06-02 20:40:12,2013-06-03 21:44:31,2013-06-03 17:09:29,closed,,0.12,20,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/3740,b'Unexpected `transform` behavior on grouped dataset',"b""I have a simple longitudinal biomedical dataset that I am grouping according to the patient on which measurements are taken. Here are the first couple of groups:\r\n\r\n\r\n    1\r\n       patient  obs  week  site  id  treat  age sex  twstrs  treatment\r\n    0        1    1     0     1   1  5000U   65   F      32          1\r\n    1        1    2     2     1   1  5000U   65   F      30          1\r\n    2        1    3     4     1   1  5000U   65   F      24          1\r\n    3        1    4     8     1   1  5000U   65   F      37          1\r\n    4        1    5    12     1   1  5000U   65   F      39          1\r\n    5        1    6    16     1   1  5000U   65   F      36          1\r\n\r\n    2\r\n        patient  obs  week  site  id   treat  age sex  twstrs  treatment\r\n    6         2    1     0     1   2  10000U   70   F      60          2\r\n    7         2    2     2     1   2  10000U   70   F      26          2\r\n    8         2    3     4     1   2  10000U   70   F      27          2\r\n    9         2    4     8     1   2  10000U   70   F      41          2\r\n    10        2    5    12     1   2  10000U   70   F      65          2\r\n    11        2    6    16     1   2  10000U   70   F      67          2\r\n\r\nHowever, when I try to `transform` these data, say by normalization, I get nonsensical results:\r\n\r\n    normalize = lambda x: (x - x.mean())/x.std()\r\n    normed = cdystonia_grouped.transform(normalize)\r\n    normed.head(10)\r\n\r\n                   patient  obs  week                 site                   id  \\\r\n    0 -9223372036854775808   -1    -1 -9223372036854775808 -9223372036854775808   \r\n    1 -9223372036854775808    0     0 -9223372036854775808 -9223372036854775808   \r\n    2 -9223372036854775808    0     0 -9223372036854775808 -9223372036854775808   \r\n    3 -9223372036854775808    0     0 -9223372036854775808 -9223372036854775808   \r\n    4 -9223372036854775808    0     0 -9223372036854775808 -9223372036854775808   \r\n\r\n                       age  twstrs            treatment  \r\n    0 -9223372036854775808       0 -9223372036854775808  \r\n    1 -9223372036854775808       0 -9223372036854775808  \r\n    2 -9223372036854775808      -1 -9223372036854775808  \r\n    3 -9223372036854775808       0 -9223372036854775808  \r\n    4 -9223372036854775808       1 -9223372036854775808  \r\n\r\nThe `normalize` function is straightforward, and works fine when applied to manually subsetted data:\r\n\r\n    normalize(cdystonia.twstrs[cdystonia.patient==1])\r\n\r\n    0   -0.181369\r\n    1   -0.544107\r\n    2   -1.632322\r\n    3    0.725476\r\n    4    1.088214\r\n    5    0.544107\r\n    Name: twstrs, dtype: float64\r\n\r\nAny guidance here much appreciated. I'm hoping its something obvious."""
3738,15038844,floux,jreback,2013-06-02 10:27:23,2014-02-13 23:12:32,2014-02-13 23:12:32,closed,,0.14.0,2,Bug;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/3738,b'Assignment with MultiIndex replaces dataframe contents with NaNs',"b""Bug:\r\n  ``df.loc['bar'] = df['bar'] * 2``; assignment with rhs having a multi-index\r\n\r\n\r\nRelated to http://stackoverflow.com/questions/16833842/assign-new-values-to-slice-from-multiindex-dataframe\r\n\r\n```python\r\nIn [246]: arrays = [np.array(['bar', 'bar', 'baz', 'qux', 'qux', 'bar']),\r\n                  np.array(['one', 'two', 'one', 'one', 'two', 'one']),\r\n                  np.arange(0, 6, 1)]\r\n\r\nIn [247]: df = pd.DataFrame(randn(6, 3),\r\n   .....: index=arrays,\r\n   .....: columns=['A', 'B', 'C']).sort_index()\r\n\r\nIn [248]: df\r\nOut[248]:\r\n                  A         B         C\r\nbar one 0  2.186362 -1.749287 -0.776363\r\n        5  0.953196 -0.717784 -0.528991\r\n    two 1  0.692802 -0.166100  1.403452\r\nbaz one 2 -0.065485  0.642380 -1.154436\r\nqux one 3  1.515987  1.138678 -0.690564\r\n    two 4  0.261254 -0.066171 -1.352841\r\n\r\nIn [249]: df.loc['bar']\r\nOut[249]:\r\n              A         B         C\r\none 0  2.186362 -1.749287 -0.776363\r\n    5  0.953196 -0.717784 -0.528991\r\ntwo 1  0.692802 -0.166100  1.403452\r\n\r\nIn [250]: df.loc['bar'] = df.loc['bar'] * 2\r\n\r\nIn [251]: df\r\nOut[251]:\r\n                  A         B         C\r\nbar one 0       NaN       NaN       NaN\r\n        5       NaN       NaN       NaN\r\n    two 1       NaN       NaN       NaN\r\nbaz one 2 -0.065485  0.642380 -1.154436\r\nqux one 3  1.515987  1.138678 -0.690564\r\n    two 4  0.261254 -0.066171 -1.352841\r\n```\r\n\r\nThis however works:\r\n```python\r\nIn [270]: df\r\nOut[270]:\r\n                  A         B         C\r\nbar one 0  1.506060 -2.921602  1.057659\r\n        5 -1.264088  6.483417  3.259962\r\n    two 1  0.580368 -2.877875  2.916771\r\nbaz one 2 -0.003460 -1.779877  0.197661\r\nqux one 3  0.463015  0.193559 -0.705382\r\n    two 4  0.074625 -0.258451  0.389310\r\n\r\nIn [271]: idx = [x for x in df.index if x[0] == 'bar']\r\n\r\nIn [272]: df.loc[idx] = df.loc[idx] * 2\r\n\r\nIn [273]: df\r\nOut[273]:\r\n                  A          B         C\r\nbar one 0  3.012121  -5.843204  2.115318\r\n        5 -2.528176  12.966833  6.519925\r\n    two 1  1.160736  -5.755750  5.833541\r\nbaz one 2 -0.003460  -1.779877  0.197661\r\nqux one 3  0.463015   0.193559 -0.705382\r\n    two 4  0.074625  -0.258451  0.389310\r\n```\r\n\r\nI was also wondering: if I want to assign to a selection via MultiIndex, what is the best way to achieve that? Examples:\r\n```python\r\ndf.xs('blah', level=2) = df.xs('blah', level=2) * 2\r\ncrit = [x for x in df.index where x[2] == 'blah'] \r\ndf.ix[crit] = df.ix[crit] * 2\r\ndf.loc[crit] = df.loc[crit] * 2\r\ndf.reset_index(level=2); df[df.level_2 == 'blah'] = df[df.level_2 == 'blah'] * 2\r\n```"""
3714,14937652,gerigk,jreback,2013-05-30 12:21:08,2013-08-11 20:01:03,2013-08-11 20:01:03,closed,,0.13,14,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3714,b'Weird MultiIndex bug',"b'Bug: Assigning levels/labels to a multiindex (or really any fields in Index) should raise (if done externally)\r\n\r\n\r\nThis is (for me) one of the weirdest things I have found so far (0.10.1 still)\r\ntl;dr\r\nit seems like df.copy() creates a shallow copy of the MultiIndex w.r.t levels\r\nAlso, the setting of the index.levels does not seem to have effect on the index itself\r\n\r\nRunning the same code twice results in different results during the second time \r\nalthough I do not alter the original object in any place.\r\nalso note the line ""print data.index.levels[1]"" which changes after changing data2\r\n\r\nmy original goal (concatenating) always contains the original date in all rows\r\n\r\nCode\r\n\r\n```\r\nimport pandas as pd\r\nfrom datetime import date, timedelta\r\nimport datetime\r\nind = pd.MultiIndex.from_tuples([(pd.Timestamp(\'2013-04-30 00:00:00\'),\r\n   datetime.date(2013, 5, 30)), (pd.Timestamp(\'2013-05-01 00:00:00\'),datetime.date(2013, 5, 30))])\r\ndata = pd.DataFrame({\'orders_ga\': {0: 10.0,\r\n  1: 15.0},\r\n \'revenues_ga\': {0: 5.0,\r\n 1: 7.0}})\r\ndata.index = ind\r\n\r\n\r\ndata2 = data.copy()\r\nprint \'original index\'\r\nprint data.index\r\nprint \'copied index levels\'\r\nprint data2.index.levels[1]\r\ndata2.index.levels[1] = pd.Index([date.today() + timedelta(1)])\r\nprint \'index levels of original dataframe after assinging new index to the copied df\'\r\nprint data.index.levels[1]\r\nprint \'new dfs index levels after assigning new levels\'\r\nprint data2.index.levels[1]\r\n#print ##################\r\n#print data.index\r\n#data2[\'recorded_at\'] = date.today() + timedelta(1)\r\nprint \'##################################################  output of concat\'\r\nprint pd.concat([data, data2])\r\nprint \'##########################################################################\'\r\nprint \'one more time\'\r\ndata2 = data.copy()\r\nprint \'original index\'\r\nprint data.index\r\nprint \'copied index levels\'\r\nprint data2.index.levels[1]\r\ndata2.index.levels[1] = pd.Index([date.today() + timedelta(1)])\r\nprint \'index levels of original dataframe after assinging new index to the copied df\'\r\nprint data.index.levels[1]\r\nprint \'new dfs index levels after assigning new levels\'\r\nprint data2.index.levels[1]\r\n#print ##################\r\n#print data.index\r\n#data2[\'recorded_at\'] = date.today() + timedelta(1)\r\nprint \'##################################################  output of concat\'\r\nprint pd.concat([data, data2])\r\n```\r\nOutput:\r\n```\r\noriginal index\r\nMultiIndex\r\n[(2013-04-30 00:00:00, 2013-05-30), (2013-05-01 00:00:00, 2013-05-30)]\r\ncopied index levels\r\nIndex([2013-05-30], dtype=object)\r\nindex levels of original dataframe after assinging new index to the copied df\r\nIndex([2013-05-31], dtype=object)\r\nnew dfs index levels after assigning new levels\r\nIndex([2013-05-31], dtype=object)\r\n##################################################  output of concat\r\n                       orders_ga  revenues_ga\r\n2013-04-30 2013-05-30         10            5\r\n2013-05-01 2013-05-30         15            7\r\n2013-04-30 2013-05-30         10            5\r\n2013-05-01 2013-05-30         15            7\r\n##########################################################################\r\none more time\r\noriginal index\r\nMultiIndex\r\n[(2013-04-30 00:00:00, 2013-05-30), (2013-05-01 00:00:00, 2013-05-30)]\r\ncopied index levels\r\nIndex([2013-05-31], dtype=object)\r\nindex levels of original dataframe after assinging new index to the copied df\r\nIndex([2013-05-31], dtype=object)\r\nnew dfs index levels after assigning new levels\r\nIndex([2013-05-31], dtype=object)\r\n##################################################  output of concat\r\n                       orders_ga  revenues_ga\r\n2013-04-30 2013-05-30         10            5\r\n2013-05-01 2013-05-30         15            7\r\n2013-04-30 2013-05-30         10            5\r\n2013-05-01 2013-05-30         15            7\r\n```\r\n'"
3687,14668931,hayd,jreback,2013-05-23 11:10:38,2013-05-30 00:48:55,2013-05-30 00:48:55,closed,,0.12,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3687,b'Dataframe non unique column renaming',"b'If you try and change the column names of a DataFrame with non-unique column names, it seems to throw and error (11.0 and dev).\r\n\r\n```\r\nIn [1]: df = pd.DataFrame(np.random.randn(3, 2), columns=[\'A\', \'A\'])\r\n\r\nIn [2]: df.columns = range(2)\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-2-454b6d12f4bf> in <module>()\r\n----> 1 df.columns = range(2)\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc in __setattr__(self, name, value)\r\n   2016                 existing = getattr(self, name)\r\n   2017                 if isinstance(existing, Index):\r\n-> 2018                     super(DataFrame, self).__setattr__(name, value)\r\n   2019                 elif name in self.columns:\r\n   2020                     self[name] = value\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/lib.so in pandas.lib.AxisProperty.__set__ (pandas/lib.c:28448)()\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/generic.pyc in _set_axis(self, axis, labels)\r\n    557\r\n    558     def _set_axis(self, axis, labels):\r\n--> 559         self._data.set_axis(axis, labels)\r\n    560         self._clear_item_cache()\r\n    561\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/internals.pyc in set_axis(self, axis, value)\r\n    920         if axis == 0:\r\n    921             for block in self.blocks:\r\n--> 922                 block.set_ref_items(self.items, maybe_rename=True)\r\n    923\r\n    924     # make items read only for now\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/internals.pyc in set_ref_items(self, ref_items, maybe_rename)\r\n     72             raise AssertionError(\'block ref_items must be an Index\')\r\n     73         if maybe_rename:\r\n---> 74             self.items = ref_items.take(self.ref_locs)\r\n     75         self.ref_items = ref_items\r\n     76\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/internals.pyc in ref_locs(self)\r\n     57     def ref_locs(self):\r\n     58         if self._ref_locs is None:\r\n---> 59             indexer = self.ref_items.get_indexer(self.items)\r\n     60             indexer = com._ensure_platform_int(indexer)\r\n     61             if (indexer == -1).any():\r\n\r\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer(self, target, method, limit)\r\n    847\r\n    848         if not self.is_unique:\r\n--> 849             raise Exception(\'Reindexing only valid with uniquely valued Index \'\r\n    850                             \'objects\')\r\n    851\r\n\r\nException: Reindexing only valid with uniquely valued Index objects\r\n```\r\n\r\nThe similar behaviour (with index) doesn\'t.\r\n\r\n*From this [SO question](http://stackoverflow.com/questions/16711716/rename-pandas-columns-with-datetime-objects).*\r\n\r\n\r\nWhat makes this problem a little more annoying is that once you\'ve seen this error you can no longer use `df` (!):\r\n\r\n```\r\nIn [5]: df\r\nOut[5]: ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-7ed0097d7e9e> in <module>()\r\n----> 1 df\r\n\r\n/Library/Python/2.7/site-packages/IPython/core/displayhook.pyc in __call__(self, result)\r\n    236             self.start_displayhook()\r\n    237             self.write_output_prompt()\r\n--> 238             format_dict = self.compute_format_data(result)\r\n    239             self.write_format_data(format_dict)\r\n    240             self.update_user_ns(result)\r\n\r\n/Library/Python/2.7/site-packages/IPython/core/displayhook.pyc in compute_format_data(self, result)\r\n    148             MIME type representation of the object.\r\n    149         """"""\r\n--> 150         return self.shell.display_formatter.format(result)\r\n    151\r\n    152     def write_format_data(self, format_dict):\r\n\r\n/Library/Python/2.7/site-packages/IPython/core/formatters.pyc in format(self, obj, include, exclude)\r\n    124                     continue\r\n    125             try:\r\n--> 126                 data = formatter(obj)\r\n    127             except:\r\n    128                 # FIXME: log the exception\r\n\r\n/Library/Python/2.7/site-packages/IPython/core/formatters.pyc in __call__(self, obj)\r\n    445                 type_pprinters=self.type_printers,\r\n    446                 deferred_pprinters=self.deferred_printers)\r\n--> 447             printer.pretty(obj)\r\n    448             printer.flush()\r\n    449             return stream.getvalue()\r\n\r\n/Library/Python/2.7/site-packages/IPython/lib/pretty.pyc in pretty(self, obj)\r\n    358                             if callable(meth):\r\n    359                                 return meth(obj, self, cycle)\r\n--> 360             return _default_pprint(obj, self, cycle)\r\n    361         finally:\r\n    362             self.end_group()\r\n\r\n/Library/Python/2.7/site-packages/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)\r\n    478     if getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\r\n    479         # A user-provided repr.\r\n--> 480         p.text(repr(obj))\r\n    481         return\r\n    482     p.begin_group(1, \'<\')\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in __repr__(self)\r\n    725         Yields Bytestring in Py2, Unicode String in py3.\r\n    726         """"""\r\n--> 727         return str(self)\r\n    728\r\n    729     def _repr_html_(self):\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in __str__(self)\r\n    668         if py3compat.PY3:\r\n    669             return self.__unicode__()\r\n--> 670         return self.__bytes__()\r\n    671\r\n    672     def __bytes__(self):\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in __bytes__(self)\r\n    678         """"""\r\n    679         encoding = com.get_option(""display.encoding"")\r\n--> 680         return self.__unicode__().encode(encoding, \'replace\')\r\n    681\r\n    682     def __unicode__(self):\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in __unicode__(self)\r\n    693             # This needs to compute the entire repr\r\n    694             # so don\'t do it unless rownum is bounded\r\n--> 695             fits_horizontal = self._repr_fits_horizontal_()\r\n    696\r\n    697         if fits_vertical and fits_horizontal:\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in _repr_fits_horizontal_(self)\r\n    653             d=d.iloc[:min(max_rows, height,len(d))]\r\n    654\r\n--> 655         d.to_string(buf=buf)\r\n    656         value = buf.getvalue()\r\n    657         repr_width = max([len(l) for l in value.split(\'\\n\')])\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in to_string(self, buf, columns, col_space, colSpace, header, index, na_rep, formatters, float_format, sparsify, nanRep, index_names, justify, force_unicode, line_width)\r\n   1542                                            header=header, index=index,\r\n   1543                                            line_width=line_width)\r\n-> 1544         formatter.to_string()\r\n   1545\r\n   1546         if buf is None:\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/format.pyc in to_string(self, force_unicode)\r\n    292             text = info_line\r\n    293         else:\r\n--> 294             strcols = self._to_str_columns()\r\n    295             if self.line_width is None:\r\n    296                 text = adjoin(1, *strcols)\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/format.pyc in _to_str_columns(self)\r\n    245         for i, c in enumerate(self.columns):\r\n    246             if self.header:\r\n--> 247                 fmt_values = self._format_col(i)\r\n    248                 cheader = str_columns[i]\r\n    249\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/format.pyc in _format_col(self, i)\r\n    383     def _format_col(self, i):\r\n    384         formatter = self._get_formatter(i)\r\n--> 385         return format_array(self.frame.icol(i).values, formatter,\r\n    386                             float_format=self.float_format,\r\n    387                             na_rep=self.na_rep,\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in icol(self, i)\r\n   1911\r\n   1912     def icol(self, i):\r\n-> 1913         return self._ixs(i,axis=1)\r\n   1914\r\n   1915     def _ixs(self, i, axis=0, copy=False):\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/frame.pyc in _ixs(self, i, axis, copy)\r\n   1961                     return self.take(i, axis=1, convert=True)\r\n   1962\r\n-> 1963                 values = self._data.iget(i)\r\n   1964                 return self._col_klass.from_array(values, index=self.index,\r\n   1965                                                   name=label)\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/internals.pyc in iget(self, i)\r\n   1649         item = self.items[i]\r\n   1650         if self.items.is_unique:\r\n-> 1651             return self.get(item)\r\n   1652\r\n   1653         # compute the duplicative indexer if needed\r\n\r\n/Users/234BroadWalk/pandas/pandas/core/internals.pyc in get(self, item)\r\n   1643\r\n   1644     def get(self, item):\r\n-> 1645         _, block = self._find_block(item)\r\n   1646         return block.get(item)\r\n   1647\r\n\r\nTypeError: \'NoneType\' object is not iterable\r\n```'"
3682,14624363,burdanov,jreback,2013-05-22 14:02:09,2013-07-25 16:46:59,2013-05-30 00:46:35,closed,,0.12,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3682,"b""DataFrame.from_records doesn't get dtype from empty ndarray""","b""```python\r\na = np.array([(1, 2)], dtype=[('id', np.int64), ('value', np.int64)])\r\ndf = pd.DataFrame.from_records(a, index='id') # ok\r\n\r\nb = np.array([], dtype=[('id', np.int64), ('value', np.int64)])\r\ndf = pd.DataFrame.from_records(b, index='id') # KeyError: 'id'\r\n```\r\nThe cause is _to_arrays checks for empty data before understanding that ndarray was passed.\r\n\r\n"""
3679,14599479,jreback,jreback,2013-05-22 00:54:35,2013-05-30 00:48:23,2013-05-30 00:48:23,closed,,0.12,2,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/3679,b'BUG: cannot insert/delete to non-unique columns',"b""This currently breaks\r\n\r\n```\r\nIn [1]: df = DataFrame([[1,1,1,5],[1,1,2,5],[2,1,3,5]],columns=['foo','bar','foo','hello'])\r\n\r\nIn [2]: df['string'] = 'bar'\r\n\r\nIn [3]: df\r\nIndexError: ('index out of bounds', u'occurred at index hello')\r\n```"""
3678,14596149,jseabold,jreback,2013-05-21 22:59:34,2014-08-19 13:58:05,2014-08-19 13:58:05,closed,,0.15.0,8,API Design;Bug;Categorical;Missing-data,https://api.github.com/repos/pydata/pandas/issues/3678,b'Categoricals with NaNs',"b'Not sure how to handle this yet. It looks like NaNs do not become a level. Should they? Maybe so. Also describe fails if NaNs are present.\r\n\r\n```\r\npandas.Categorical([np.nan, np.nan, 1, 1, 1, 2, 3, 4, 5, 5, 4, 3, 3]).describe()\r\n```'"
3674,14586735,lodagro,jreback,2013-05-21 19:40:29,2013-06-13 18:43:02,2013-06-13 18:42:47,closed,,0.12,4,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3674,b'series.interpolate() corner cases',"b""from the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!topic/pydata/40-Yj9R0LJ4)\r\n\r\nI've been using Series.interpolate(), and have noticed that, while it\r\nleaves NaN entries alone if they precede a non-null value, it blows up if\r\nthere are no non-null entries. Wouldn't it make more sense just to leave\r\ncompletely null series alone (so the user doesn't have to manually check\r\nfor this case)? Here's an example:\r\n\r\n1) This one works:\r\n\r\n```python\r\nS1 = Series([np.nan, 2.0])\r\nprint S1.interpolate()\r\n\r\n-------\r\n\r\n0   NaN\r\n1     2\r\ndtype: float64\r\n```\r\n\r\n2) This one blows up:\r\n\r\n```python\r\nS2 = Series([np.nan, np.nan])\r\nprint S2.interpolate()\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-49-620b22122e43> in <module>()\r\n      3\r\n      4 S2 = Series([np.nan, np.nan])\r\n----> 5 print S2.interpolate()\r\n\r\n/RHS/packages/anaconda/pandas/pandas/core/series.pyc in interpolate(self,\r\nmethod)\r\n   3191         result = values.copy()\r\n   3192         result[firstIndex:][invalid] = np.interp(inds[invalid],\r\ninds[valid],\r\n-> 3193\r\nvalues[firstIndex:][valid])\r\n   3194\r\n   3195         return Series(result, index=self.index, name=self.name)\r\n\r\n/Users/stanton/anaconda/lib/python2.7/site-packages/numpy/lib/function_base\r\n.pyc in interp(x, xp, fp, left, right)\r\n   1067         return compiled_interp([x], xp, fp, left, right).item()\r\n   1068     else:\r\n-> 1069         return compiled_interp(x, xp, fp, left, right)\r\n   1070\r\n   1071\r\n\r\nValueError: array of sample points is empty\r\n```\r\n\r\n3) empty series (not mentioned in the original post)\r\n\r\n```python\r\nIn [38]: pd.Series([]).interpolate()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-38-012633a78523> in <module>()\r\n----> 1 pd.Series([]).interpolate()\r\n\r\n.../pandas/core/series.pyc in interpolate(self, method)\r\n   3184         valid = -invalid\r\n   3185 \r\n-> 3186         firstIndex = valid.argmax()\r\n   3187         valid = valid[firstIndex:]\r\n   3188         invalid = invalid[firstIndex:]\r\n\r\nValueError: attempt to get argmax/argmin of an empty sequence\r\n```"""
3669,14572012,hayd,hayd,2013-05-21 14:56:48,2013-07-08 09:27:57,2013-07-06 21:16:06,closed,,0.13,7,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3669,b'string to date format ignored on apply',"b""*From the [So question](http://stackoverflow.com/questions/16672237/specifying-date-format-when-converting-with-pandas-to-datetime).*\r\n\r\nI think apply be passing on the format keyword argument:\r\n\r\n```\r\nIn [1]: s = pd.Series(['12/1/2012', '30/01/2012'])\r\n\r\nIn [2]: s.apply(pd.to_datetime, format='%d/%m/%Y')\r\nOut[2]:\r\n0   2012-12-01 00:00:00\r\n1   2012-01-30 00:00:00\r\ndtype: datetime64[ns]\r\n\r\nIn [3]: pd.to_datetime(s, format='%d/%m/%Y')\r\nOut[3]:\r\n0   2012-01-12 00:00:00\r\n1   2012-01-30 00:00:00\r\ndtype: datetime64[ns]\r\n```"""
3668,14571914,jreback,jreback,2013-05-21 14:55:01,2013-06-12 17:55:10,2013-05-21 17:37:31,closed,,0.12,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3668,b'BUG: fancy setitem on a mixed-frame with a series fails',"b""```\r\nIn [35]: df = pandas.DataFrame({'x':range(10), 'y':range(10,20),'z' : 'bar'})\r\n\r\nIn [36]: df.ix[df.x % 2 == 0, 'y'] = df.ix[df.x % 2 == 0, 'y'].values * 100\r\n\r\nIn [37]: df\r\nOut[37]: \r\n   x     y    z\r\n0  0  1000  bar\r\n1  1    11  bar\r\n2  2  1000  bar\r\n3  3    13  bar\r\n4  4  1000  bar\r\n5  5    15  bar\r\n6  6  1000  bar\r\n7  7    17  bar\r\n8  8  1000  bar\r\n9  9    19  bar\r\n```"""
3659,14534295,jreback,jreback,2013-05-20 19:16:07,2013-05-20 23:34:40,2013-05-20 23:34:40,closed,jreback,0.12,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3659,b'BUG: non unique indexing via loc broken',"b""This throws a weird error\r\n```\r\ndf = DataFrame({'A' : [1,2,3,4], 'B' : [3, 4, 5, 6]}, index = [0,1,0,1]) \r\ndf.loc[1:]\r\n```\r\n\r\nhttps://groups.google.com/forum/?fromgroups#!topic/pydata/zTm2No0crYs"""
3654,14519524,jreback,jreback,2013-05-20 13:25:07,2013-05-21 13:43:38,2013-05-21 13:43:38,closed,,0.12,0,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/3654,b'TST: read_html tests failing when html5lib included on travis',"b""@cpcloud\r\n\r\nhttps://www.travis-ci.org/pydata/pandas/jobs/7320947\r\n\r\nI don't think travis was actually testing html5lib stuff....(I just added it in)\r\nits taken out right now\r\n\r\nadd in ci/install.sh (right after bs4)....and test"""
3634,14453917,yejunzhou,changhiskhan,2013-05-17 14:10:39,2013-06-07 17:05:41,2013-06-06 19:56:11,closed,,0.12,7,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3634,b'TextReader\xd6\xd0\xce\xc4\xc2\xb7\xbe\xb6\xce\xca\xcc\xe2 (Bug with Chinese characters in file path)',"b'Traceback (most recent call last):\r\n  File ""D:\\wiki\\pandas_test\\hello_pandas.py"", line 55, in <module>\r\n    tp = pd.read_csv(\'tmp\xb2\xe2\xca\xd4.csv\', iterator=True, chunksize=5000)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.py"", line 401, in parser_f\r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2013-01-01 00:00:00, ..., 2013-01-06 00:00:00]\r\nLength: 6, Freq: D, Timezone: None\r\n    return _read(filepath_or_buffer, kwds)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.py"", line 209, in _read\r\n    parser = TextFileReader(filepath_or_buffer, **kwds)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.py"", line 509, in __init__\r\n    self._make_engine(self.engine)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.py"", line 611, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.py"", line 893, in __init__\r\n    self._reader = _parser.TextReader(src, **kwds)\r\n  File ""parser.pyx"", line 312, in pandas._parser.TextReader.__cinit__ (pandas\\src\\parser.c:2781)\r\n  File ""parser.pyx"", line 512, in pandas._parser.TextReader._setup_parser_source (pandas\\src\\parser.c:4813)\r\nIOError: File tmp\xb2\xe2\xca\xd4.csv does not exist\r\n\r\ncode:\r\ntp = pd.read_csv(\'tmp\xb2\xe2\xca\xd4.csv\', iterator=True, chunksize=5000)\r\nfor i, chunk in enumerate(tp):\r\n    print chunk\r\nt1 = datetime.now()\r\n\r\n\r\n\xc8\xe7\xcf\xc2\xb4\xfa\xc2\xeb\xd4\xf2\xd5\xfd\xb3\xa3:  (the following code works)\r\ntp = pd.read_csv(\'tmp.csv\', iterator=True, chunksize=5000)\r\nfor i, chunk in enumerate(tp):\r\n    print chunk\r\nt1 = datetime.now()\r\n\r\n\xc8\xe7\xcf\xc2\xb4\xfa\xc2\xeb\xd2\xb2\xd5\xfd\xb3\xa3\xa3\xba (the following also works)\r\ntp = pd.read_csv(\'tmp\xb2\xe2\xca\xd4.csv\', iterator=True, skip_footer=1, chunksize=5000)\r\nfor i, chunk in enumerate(tp):\r\n    print chunk\r\nt1 = datetime.now()\r\n\r\n\xb5\xb1\xb2\xbb\xbc\xd3skip_footer\xca\xb1\xd3\xc3\xb5\xc4\xca\xc7engine=\'c\',\xd5\xe2\xca\xb1\xc8\xe7\xb9\xfb\xce\xc4\xbc\xfe\xc3\xfb\xd6\xd0\xd3\xd0\xd6\xd0\xce\xc4\xd4\xf2\xbb\xe1\xb1\xa8\xb3\xf6\xc8\xe7\xcf\xc2\xb4\xed\xce\xf3:IOError: File tmp\xb2\xe2\xca\xd4.csv does not exist\r\n\xd5\xe2\xd3\xa6\xb8\xc3\xcb\xe3\xd2\xbb\xb8\xf6bug,\xcf\xa3\xcd\xfb\xc4\xdc\xb0\xef\xc3\xa6\xbd\xe2\xbe\xf6\r\n\r\n(When not using the `skip_footer` parameter, the default engine is \'c\'. In that case if there are Chinese characters in the path an exception is raised ""IOError: File tmp\xb2\xe2\xca\xd4.csv does not exist"". This should be considered a bug and I hope you can help solve the issue).'"
3628,14428577,stonebig,jreback,2013-05-16 21:50:30,2013-07-13 00:18:20,2013-05-19 16:49:55,closed,,0.12,22,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3628,b'systematic bug in pandas/io/sql.py when trying to write a dataframe of one column of integers into sqlite',"b'Hello,\r\n\r\ntrying this code with pandasql, with ipython, I get a bogus result .\r\n\r\nimport pandas as pd\r\n\r\n%load https://raw.github.com/yhat/pandasql/master/pandasql/sqldf.py\r\n\r\ndf=pd.DataFrame([ 1, 2], columns=[\'f\'])\r\nsqldf(""select *  from df "", locals())\r\n\r\nthis gives the mysterious (binary ?) output: \r\n \tf\r\n0 \t(1, 0, 0, 0, 0, 0, 0, 0)\r\n1 \t(2, 0, 0, 0, 0, 0, 0, 0)\r\n\r\n\r\nDigging further, I found that the problem is in those lines generated by pandas/io/sql/py  :\r\n\r\nimport pandas as pd\r\nimport sqlite3\r\ndf=pd.DataFrame([ 1, 2], columns=[\'f\' ])\r\ncon = sqlite3.connect("":memory:"")\r\ncon.execute(""CREATE TABLE df ( [f] INTEGER );"")\r\ndata = [tuple(x) for x in df.values]\r\ncon.executemany(""INSERT INTO df ([f]) VALUES (?)"", data)\r\n \r\n# here is the way to see the bogus result\r\nfor row in con.execute(""select * from f""):\r\n    print (row) \r\n# this print : \r\n# (b\'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\',)\r\n# (b\'\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\',)\r\n# instead of expected : \r\n# (1,)\r\n# (2,) \r\n\r\nI can\'t guess further what is causing the issue, it seems there is a misunderstanding between pandas and sqlite when passing a ""data"" list of integers.\r\n\r\nproblem doesn\'t exist if we switch to non-integers like : \r\n df=pd.DataFrame([ 1., 2], columns=[\'f\' ])\r\n\r\nNota : I\'m under windows vista 32 bits, python 3.3, in a country where the decimal separator is a comma (,) , maybe it\'s related to the problem, maybe not.\r\n\r\n'"
3624,14415502,selasley,jreback,2013-05-16 16:50:31,2013-05-16 18:44:33,2013-05-16 17:38:59,closed,,0.12,7,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3624,"b'DataFrame.to_csv(f, index=False) fails after appending a column to the data frame'","b'In [7]: pd.__version__\r\nOut[7]: \'0.12.0.dev-a026561\'\r\nIn [8]: import pandas as pd\r\nIn [9]: df = pd.DataFrame({\'c1\':[1,2,3], \'c2\':[4,5,6]})\r\nIn [10]: df.to_csv(\'f1.txt\', index=False)\r\nIn [11]: df[\'c3\'] = [7,8,9]\r\nIn [12]: df.to_csv(\'f2.txt\', index=False)\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-12-b4861447b824> in <module>()\r\n----> 1 df.to_csv(\'f2.txt\', index=False)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas-0.12.0.dev_a026561-py3.3-macosx-10.6-intel.egg/pandas/core/frame.py in to_csv(self, path_or_buf, sep, na_rep, float_format, cols, header, index, index_label, mode, nanRep, encoding, quoting, line_terminator, chunksize, **kwds)\r\n   1445                                          index_label=index_label,mode=mode,\r\n   1446                                          chunksize=chunksize,engine=kwds.get(""engine"") )\r\n-> 1447             formatter.save()\r\n   1448 \r\n   1449     def to_excel(self, excel_writer, sheet_name=\'sheet1\', na_rep=\'\',\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas-0.12.0.dev_a026561-py3.3-macosx-10.6-intel.egg/pandas/core/format.py in save(self)\r\n    945 \r\n    946             else:\r\n--> 947                 self._save()\r\n    948 \r\n    949 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas-0.12.0.dev_a026561-py3.3-macosx-10.6-intel.egg/pandas/core/format.py in _save(self)\r\n   1017                 break\r\n   1018 \r\n-> 1019             self._save_chunk(start_i, end_i)\r\n   1020 \r\n   1021     def _save_chunk(self, start_i, end_i):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas-0.12.0.dev_a026561-py3.3-macosx-10.6-intel.egg/pandas/core/format.py in _save_chunk(self, start_i, end_i)\r\n   1031 \r\n   1032                 # self.data is a preallocated list\r\n-> 1033                 self.data[self.column_map[b][i]] = d[i]\r\n   1034 \r\n   1035         ix = data_index.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)\r\n\r\nKeyError: IntBlock: [c3], 1 x 3, dtype int64\r\n'"
3617,14382938,joeb1415,jreback,2013-05-15 22:39:10,2013-05-16 11:35:05,2013-05-16 11:35:05,closed,,0.12,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3617,b'Panel.iloc[] broken',"b""Panel.iloc[] doesn't index correctly given a mix of scalars and ranges\r\n\r\n    from pandas import Panel, date_range\r\n    from numpy.random import randn\r\n\r\n    p = Panel(randn(4, 4, 4))\r\n\r\n    print(p.iloc[:3, :3, :3].shape)\r\n    print(p.iloc[1, :3, :3].shape)\r\n    print(p.iloc[:3, 1, :3].shape)\r\n    print(p.iloc[:3, :3, 1].shape)\r\n    print(p.iloc[1, 1, :3].shape)\r\n    print(p.iloc[1, :3, 1].shape)\r\n    print(p.iloc[:3, 1, 1].shape)\r\n\r\nReturns\r\n\r\n    [3, 3, 3]\r\n    (4, 3)\r\n    (4, 3)\r\n    (3, 3)\r\n    (3L,)\r\n    (4L,)\r\n    (4L,)"""
3612,14371694,cpcloud,cpcloud,2013-05-15 18:13:32,2013-10-09 19:11:19,2013-10-09 19:10:20,closed,cpcloud,0.13,14,Bug;Dtypes;Frequency;Prio-medium;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3612,b'DatetimeIndexes with day frequency hang when they have more than 2 elements',"b""See #3609. Copied (more or less) from there\r\n\r\n```python\r\ndind = period_range('1/1/2001', '1/1/2002').to_timestamp()\r\ns = Series(randn(dind.size), dind)\r\ns.resample('T', kind='period')  # hangs here\r\n```\r\nDoesn't hang (throws an error) for the simple case of \r\n\r\n```python\r\ndind = period_range('1/1/2001', '1/2/2001').to_timestamp()\r\ns = Series(randn(dind.size), dind)\r\ns.resample('T', kind='period')\r\n```\r\nand starts to hang for `dind.size > 2`."""
3611,14365206,rhstanton,jreback,2013-05-15 15:47:37,2013-06-11 17:43:23,2013-06-11 17:43:23,closed,,0.12,34,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3611,b' Odd behavior from read_csv with na_values set to non-string values',"b""read_csv behaves oddly when na_values is set to non-string values. Sometimes\r\nit correctly replaces the assigned number with NaN, and sometimes it doesn't. Here are some examples. Note in particular the different behavior of the last two statements:\r\n \r\n# Create file\r\ndf = DataFrame({'A' : [-999, 2, 3], 'B' : [1.2, -999, 4.5]})\r\ndf.to_csv('test2.csv', sep=' ', index=False)\r\n \r\nprint read_csv('test2.csv', sep= ' ', header=0, na_values=[-999])\r\n\r\n---\r\nA B\r\n0 NaN 1.2\r\n1 2 -999.0\r\n2 3 4.5\r\n---\r\n \r\nprint read_csv('test2.csv', sep= ' ', header=0, na_values=[-999.0])\r\n \r\n---\r\nA B\r\n0 -999 1.2\r\n1 2 NaN\r\n2 3 4.5\r\n---\r\n \r\n \r\nprint read_csv('test2.csv', sep= ' ', header=0, na_values=[-999.0,-999])\r\n \r\n---\r\nA B\r\n0 -999 1.2\r\n1 2 NaN\r\n2 3 4.5\r\n---\r\n \r\nprint read_csv('test2.csv', sep= ' ', header=0, na_values=[-999,-999.0])\r\n \r\n---\r\nA B\r\n0 NaN 1.2\r\n1 2 -999.0\r\n2 3 4.5\r\n---\r\n"""
3610,14365066,ronenabr,jreback,2013-05-15 15:44:38,2013-05-15 19:28:17,2013-05-15 19:27:59,closed,,0.12,3,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/3610,"b'grouby( ... , as_index=False) convert ""float64"" to ""object"" type.'","b'```\'python\r\nIn [1]: import pandas as pd \r\n\r\nIn [2]: df = pd.DataFrame({""c1"" : [1,2,6,6,8]})\r\n\r\nIn [3]: df[""c2""] = df.c1/2.0\r\n\r\nIn [4]: df.groupby(""c2"").mean().reset_index().c2\r\nOut[4]: \r\n0    0.5\r\n1    1.0\r\n2    3.0\r\n3    4.0\r\nName: c2, dtype: float64\r\n\r\nIn [5]: df.groupby(""c2"", as_index=False).mean().c2\r\nOut[5]: \r\n0    0.5\r\n1      1\r\n2      3\r\n3      4\r\nName: c2, dtype: object\r\n\r\n```\r\n\r\nNote  the difference  in dtype between the two cases. \r\nI think the 2nd case shouldn\'t be different then the 1st case.\r\n'"
3609,14356528,JackKelly,jtratner,2013-05-15 12:44:42,2013-11-06 00:23:40,2013-11-06 00:23:40,closed,cpcloud,0.13,13,Bug;Enhancement;Groupby;Period,https://api.github.com/repos/pydata/pandas/issues/3609,"b'""AssertionError: Index length did not match values"" when resampling with kind=\'period\''","b""Should raise that ``kind='period'`` is not accepted for ``DatetimeIndex`` when resampling\r\nPossible issue with period index resampling hanging (see @cpcloud example below)\r\n\r\nversion = 0.12.0.dev-f61d7e3\r\n\r\nThis bug also exists in 0.11.\r\n\r\n## The bug\r\n\r\n```Python\r\nIn [20]: s.resample('T', kind='period')\r\n-----------------\r\nAssertionError  \r\nTraceback (most recent call last)\r\n<ipython-input-79-c290c0578332> in <module>()\r\n----> 1 s.resample('T', kind='period')\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/core/generic.py in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base)\r\n    255                               fill_method=fill_method, convention=convention,\r\n    256                               limit=limit, base=base)\r\n--> 257         return sampler.resample(self)\r\n    258 \r\n    259     def first(self, offset):\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/tseries/resample.py in resample(self, obj)\r\n     81 \r\n     82         if isinstance(axis, DatetimeIndex):\r\n---> 83             rs = self._resample_timestamps(obj)\r\n     84         elif isinstance(axis, PeriodIndex):\r\n     85             offset = to_offset(self.freq)\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/tseries/resample.py in _resample_timestamps(self, obj)\r\n    224             # Irregular data, have to use groupby\r\n    225             grouped = obj.groupby(grouper, axis=self.axis)\r\n--> 226             result = grouped.aggregate(self._agg_method)\r\n    227 \r\n    228             if self.fill_method is not None:\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/core/groupby.py in aggregate(self, func_or_funcs, *args, **kwargs)\r\n   1410         if isinstance(func_or_funcs, basestring):\r\n-> 1411             return getattr(self, func_or_funcs)(*args, **kwargs)\r\n   1412 \r\n   1413         if hasattr(func_or_funcs, '__iter__'):\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/core/groupby.py in mean(self)\r\n    356         except Exception:  # pragma: no cover\r\n    357             f = lambda x: x.mean(axis=self.axis)\r\n--> 358             return self._python_agg_general(f)\r\n    359 \r\n    360     def median(self):\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/core/groupby.py in _python_agg_general(self, func, *args, **kwargs)\r\n    498                 output[name] = self._try_cast(values[mask],result)\r\n    499 \r\n--> 500         return self._wrap_aggregated_output(output)\r\n    501 \r\n    502     def _wrap_applied_output(self, *args, **kwargs):\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/core/groupby.py in _wrap_aggregated_output(self, output, names)\r\n   1473             return DataFrame(output, index=index, columns=names)\r\n   1474         else:\r\n-> 1475             return Series(output, index=index, name=self.name)\r\n   1476 \r\n   1477     def _wrap_applied_output(self, keys, values, not_indexed_same=False):\r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/core/series.py in __new__(cls, data, index, dtype, name, copy)\r\n    494         else:\r\n    495             subarr = subarr.view(Series)\r\n--> 496         subarr.index = index\r\n    497         subarr.name = name\r\n    498 \r\n\r\n/home/dk3810/workspace/python/pda/scripts/src/pandas/pandas/lib.so in pandas.lib.SeriesIndex.__set__ (pandas/lib.c:29775)()\r\n\r\nAssertionError: Index length did not match values\r\n```\r\n\r\n## A workaround / expected behaviour\r\n\r\n```Python\r\nIn [81]: s.resample('T').to_period()\r\nOut[81]: \r\n2013-04-12 19:15    325.000000\r\n2013-04-12 19:16    326.899994\r\n...\r\n2013-04-12 22:58    305.600006\r\n2013-04-12 22:59    320.444458\r\nFreq: T, Length: 225, dtype: float32\r\n```\r\n\r\n## More information\r\n\r\n```Python\r\n\r\nIn [83]: s\r\nOut[83]: \r\n2013-04-12 19:15:25    323\r\n2013-04-12 19:15:28    NaN\r\n...\r\n2013-04-12 22:59:55    319\r\n2013-04-12 22:59:56    NaN\r\n2013-04-12 22:59:57    NaN\r\n2013-04-12 22:59:58    NaN\r\n2013-04-12 22:59:59    NaN\r\nName: aggregate, Length: 13034, dtype: float32\r\n\r\nIn [76]: s.index\r\nOut[76]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2013-04-12 19:15:25, ..., 2013-04-12 22:59:59]\r\nLength: 13034, Freq: None, Timezone: None\r\n\r\nIn [77]: s.head()\r\nOut[77]: \r\n2013-04-12 19:15:25    323\r\n2013-04-12 19:15:28    NaN\r\n2013-04-12 19:15:29    NaN\r\n2013-04-12 19:15:30    NaN\r\n2013-04-12 19:15:31    327\r\nName: aggregate, dtype: float32\r\n\r\nIn [78]: s.resample('T')\r\nOut[78]: \r\n2013-04-12 19:15:00    325.000000\r\n2013-04-12 19:16:00    326.899994\r\n...\r\n2013-04-12 22:58:00    305.600006\r\n2013-04-12 22:59:00    320.444458\r\nFreq: T, Length: 225, dtype: float32\r\n\r\nIn [80]: pd.__version__\r\nOut[80]: '0.12.0.dev-f61d7e3'\r\n\r\nIn [84]: type(s)\r\nOut[84]: pandas.core.series.TimeSeries\r\n```\r\n\r\n(Please let me know if you need more info!  I'm using Ubuntu 13.04.  It's entirely possible that this isn't a bug but instead I am doing something stupid.  Oh, and let me take this opportunity to thank the Pandas dev team!  Pandas is awesome!!!  THANK YOU!)"""
3608,14355427,ronenabr,y-p,2013-05-15 12:11:47,2013-05-23 14:03:36,2013-05-19 19:09:38,closed,,0.12,4,Bug;Duplicate,https://api.github.com/repos/pydata/pandas/issues/3608,b'Error in  html display of table with multicolumns',"b'version: 0.11.0\r\n\r\nwith small variation of python for data analysis book:\r\n\r\n```python\r\ntips = pd.read_csv(""https://github.com/pydata/pydata-book/raw/master/ch08/tips.csv"")\r\ngrouped = tips.groupby([\'sex\',\'smoker\'])\r\nres = grouped.agg({\'tip\': [\'min\',\'max\',\'mean\',\'std\'], \'size\':\'sum\'}) #By now, copied from the book, ~page 263-264\r\n\r\n#New stuff\r\nres[""foo""] = res[""tip"", ""max""] * res[""tip"", ""mean""] \r\n\r\n```\r\nThe result attached (with missing line. it seems like \'sum\' is part of \'foo\')\r\n![screenshot from 2013-05-15 15 10 15](https://f.cloud.github.com/assets/4340718/506658/970f92f4-bd58-11e2-9e82-7108d59c3912.png)\r\n'"
3606,14334979,timmie,jreback,2013-05-14 22:27:22,2013-08-06 23:30:09,2013-05-20 11:47:17,closed,,0.12,10,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3606,b'read_html: fails to parse column',"b""The second column of the table \r\nhttp://code.google.com/p/pythonxy/wiki/StandardPlugins#Python_packages\r\n\r\nis not parsed as shown with this code:\r\n\r\n```python\r\n\r\n# -*- coding: utf-8 -*-\r\n# <nbformat>3.0</nbformat>\r\n\r\n# <codecell>\r\n\r\nimport pandas as pd\r\n\r\n# <codecell>\r\n\r\nurl = 'http://code.google.com/p/pythonxy/wiki/StandardPlugins'\r\n\r\n# <codecell>\r\n\r\ndfs = pd.read_html(url, attrs={'class': 'wikitable'})\r\n\r\n# <codecell>\r\n\r\ndfs\r\n\r\n# <codecell>\r\n\r\ndfs = pd.read_html(url, flavor='lxml', attrs={'class': 'wikitable'})\r\n\r\n# <codecell>\r\n\r\ndfs\r\n\r\n# <codecell>\r\n\r\npython_core = dfs[0]\r\n\r\n# <codecell>\r\n\r\npython_core[:10]\r\n\r\n```"""
3605,14334094,jseabold,jreback,2013-05-14 22:05:35,2013-05-20 11:45:59,2013-05-20 11:45:59,closed,,0.12,6,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/3605,b'Importing bs4 in tests',"b'I just got a bunch of test errors because I don\'t have bs4. I do have lxml. Haven\'t looked at this at all to see why the test isn\'t conditional. E.g.,\r\n\r\n```\r\n======================================================================\r\nERROR: test_string (pandas.io.tests.test_html.TestBs4ReadHtml)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/tests/test_html.py"", line 221, in test_string\r\n    df1 = self.run_read_html(data, \'.*Water.*\', infer_types=False)\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/tests/test_html.py"", line 324, in run_read_html\r\n    return _run_read_html(*args, **kwargs)\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/tests/test_html.py"", line 31, in _run_read_html\r\n    return read_html(*args, **kwargs)\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/html.py"", line 701, in read_html\r\n    for data in p.parse_raw_data()]\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/html.py"", line 152, in parse_raw_data\r\n    for rows in self.parse_rows()]\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/html.py"", line 137, in parse_rows\r\n    tables = self._parse_tables(self._build_doc(), self.match, self.attrs)\r\n  File ""/home/skipper/src/pandas-skipper/pandas/io/html.py"", line 333, in _build_doc\r\n    from bs4 import BeautifulSoup, SoupStrainer\r\nImportError: No module named bs4\r\n```'"
3602,14315493,rhstanton,jreback,2013-05-14 15:28:02,2013-05-19 15:12:02,2013-05-19 15:12:02,closed,,0.12,20,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/3602,b'concat produces incorrect output',"b""Under certain circumstances, concat seems to produce erroneous results. I haven't worked out what causes the problems to arise, but here's an example:\r\n \r\ndf1 = DataFrame({'firmNo' : [0,0,0,0], 'stringvar' : ['rrr', 'rrr', 'rrr', 'rrr'], 'prc' : [6,6,6,6] })\r\ndf2 = DataFrame({'misc' : [1,2,3,4], 'prc' : [6,6,6,6], 'C' : [9,10,11,12]})\r\nconcat([df1,df2],axis=1)\r\n  \r\nproduces as output:\r\n  \r\n  firmNo  prc  stringvar   C  misc  prc\r\n0    rrr    0          6   9     1    6\r\n1    rrr    0          6  10     2    6\r\n2    rrr    0          6  11     3    6\r\n3    rrr    0          6  12     4    6\r\n"""
3601,14315331,rhstanton,jreback,2013-05-14 15:25:35,2013-05-19 17:30:26,2013-05-19 17:30:26,closed,,0.12,5,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3601,b'Plotting incorrectly reversed when using dates as x-axis',"b""Sometimes I have to read in spreadsheets containing data with the most\r\nrecent observations first. If I try to plot these without re-sorting\r\nfirst, the graph appears *backwards*. The dates on the x-axis are in\r\nascending order, but the data are plotted from left to right in the same\r\norder they appeared in the spreadsheet. Since there's no warning that the\r\nlabels on the axis don't correspond to the data being plotted, this can\r\ncause some embarrassment! This only happens when I use timestamps; dates\r\nas integers work fine.\r\n\r\nFor example, this works fine (dates reversed, stored as int)\r\n\r\ndf2 = DataFrame({'Date' : [1998, 1997, 1996], 'Data' : [9, 8,\r\n7]}).set_index('Date')\r\ndf2.plot()\r\n\r\nThis doesn't (same data set, but dates stored as timestamp instead):\r\n\r\nfrom dateutil import parser\r\ndf = DataFrame({'Date' : ['1998-01-01', '1997-01-01', '1996-01-01'],\r\n'Data' : [9, 8, 7]})\r\ndf.index = df.Date.map(parser.parse)\r\ndel df['Date']\r\ndf.plot()\r\n\r\nI can get around the problem by sorting the data set before plotting, but\r\nI don't always remember, and it definitely seems odd that the graph ends\r\nup displaying something that is not actually in the original data set..."""
3598,14297852,tonicebrian,jreback,2013-05-14 07:58:11,2013-07-28 05:43:36,2013-06-26 13:38:50,closed,,0.12,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3598,b'Make secondary_y not being ignored in bar charts',"b""If I want to plot 2 series ('A' and 'B') with different y-scales using a **different y-axis** for each one of them, I can use `secondary_y`:\r\n\r\n    df = pd.DataFrame(np.random.uniform(size=10).reshape(5,2),columns=['A','B'])\r\n    df['A'] = df['A'] * 100\r\n    df.plot(secondary_y=['A'])\r\n\r\nbut if I want to create bar graphs, the equivalent command **is ignored** (it doesn't put different scales on the y-axis), so the bars from 'A' are so big that the bars from 'B' cannot be distinguished:\r\n    \r\n    df.plot(kind='bar',secondary_y=['A'])\r\n\r\nThe last command should produce a graph with equally sized bars."""
3596,14286266,elliottwaldron,jreback,2013-05-13 23:00:24,2013-05-15 20:19:46,2013-05-15 20:19:46,closed,,0.12,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3596,b'DataFrame.groupby.count sometimes returns Series instead of DataFrame',"b""In version 0.11.0\r\n```python\r\nA = DataFrame([[1,1],[2,1]],columns=['X','Y']).groupby('X').count()\r\n```\r\nreturns a DataFrame, when there are 2+ grouping levels...\r\n\r\nbut,\r\n```python\r\nB = DataFrame([[1,1],[1,1]],columns=['X','Y']).groupby('X').count()\r\n```\r\nreturns a Series, when there is only 1 grouping level. \r\n\r\nIn version 0.10.0, B would also be a DataFrame. \r\n\r\nDon't know if this was by design, but it seems awkward to check the type before using B downstream. \r\n"""
3594,14282354,danmd,jreback,2013-05-13 21:17:30,2013-05-14 10:51:38,2013-05-14 10:51:38,closed,,0.12,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3594,"b'read_fwf(), read_table() are mangling columns'","b'version 0.11.0 has introduced a confusing behavior when importing data via read_fwf (and, I\'m pretty sure, read_table)\r\n\r\n\r\n<PRE>\r\nimport pandas as pd\r\nfrom cStringIO import StringIO\r\nfrom datetime import datetime\r\n\r\ntzlist = [1,10,20,30,60,80,100]\r\nntz = len(tzlist)\r\ntcolspecs = [16]+[8]*ntz\r\ntcolnames = [\'SST\'] + [""T%03d"" % z for z in tzlist[1:]]\r\ndata = \'\'\'  2009164202000   9.5403  9.4105  8.6571  7.8372  6.0612  5.8843  5.5192\r\n  2009164203000   9.5435  9.2010  8.6167  7.8176  6.0804  5.8728  5.4869\r\n  2009164204000   9.5873  9.1326  8.4694  7.5889  6.0422  5.8526  5.4657\r\n  2009164205000   9.5810  9.0896  8.4009  7.4652  6.0322  5.8189  5.4379\r\n  2009164210000   9.6034  9.0897  8.3822  7.4905  6.0908  5.7904  5.4039\'\'\'\r\ndftemp = pd.read_fwf(StringIO(data),\r\n                     index_col=0,\r\n                     header=None,\r\n                     names=tcolnames,\r\n                     widths=tcolspecs,\r\n                     parse_dates=True,\r\n                     date_parser=lambda s: datetime.strptime(s,\'%Y%j%H%M%S\'))\r\n</PRE>\r\n\r\nWith version 0.10.1\r\n---\r\n<PRE>\r\nIn [1]: pd.__version__\r\nOut[1]: \'0.10.1\'\r\n\r\nIn [2]: dftemp\r\nOut[2]: \r\n                        SST    T010    T020    T030    T060    T080    T100\r\n2009-06-13 20:20:00  9.5403  9.4105  8.6571  7.8372  6.0612  5.8843  5.5192\r\n2009-06-13 20:30:00  9.5435  9.2010  8.6167  7.8176  6.0804  5.8728  5.4869\r\n2009-06-13 20:40:00  9.5873  9.1326  8.4694  7.5889  6.0422  5.8526  5.4657\r\n2009-06-13 20:50:00  9.5810  9.0896  8.4009  7.4652  6.0322  5.8189  5.4379\r\n2009-06-13 21:00:00  9.6034  9.0897  8.3822  7.4905  6.0908  5.7904  5.4039\r\n\r\nIn [3]: dftemp.T030\r\nOut[3]: \r\n2009-06-13 20:20:00    7.8372\r\n2009-06-13 20:30:00    7.8176\r\n2009-06-13 20:40:00    7.5889\r\n2009-06-13 20:50:00    7.4652\r\n2009-06-13 21:00:00    7.4905\r\nName: T030, dtype: float64\r\n\r\nIn [4]: dftemp.T060\r\nOut[4]: \r\n2009-06-13 20:20:00    6.0612\r\n2009-06-13 20:30:00    6.0804\r\n2009-06-13 20:40:00    6.0422\r\n2009-06-13 20:50:00    6.0322\r\n2009-06-13 21:00:00    6.0908\r\nName: T060\r\n\r\nIn [5]: dftemp.T080\r\nOut[5]: \r\n2009-06-13 20:20:00    5.8843\r\n2009-06-13 20:30:00    5.8728\r\n2009-06-13 20:40:00    5.8526\r\n2009-06-13 20:50:00    5.8189\r\n2009-06-13 21:00:00    5.7904\r\nName: T080\r\n\r\nIn [6]: dftemp.T100\r\nOut[6]: \r\n2009-06-13 20:20:00    5.5192\r\n2009-06-13 20:30:00    5.4869\r\n2009-06-13 20:40:00    5.4657\r\n2009-06-13 20:50:00    5.4379\r\n2009-06-13 21:00:00    5.4039\r\nName: T100, dtype: float64\r\n</PRE>\r\n\r\nand, with version 0.11.0\r\n---\r\n<PRE>\r\nIn [1]: pd.__version__\r\nOut[1]: \'0.11.0\'\r\n\r\nIn [2]: dftemp\r\nOut[2]: \r\n                        SST    T010    T020    T030    T060    T080    T100\r\n2009-06-13 20:20:00  9.5403  9.4105  8.6571  7.8372  6.0612  5.8843  5.5192\r\n2009-06-13 20:30:00  9.5435  9.2010  8.6167  7.8176  6.0804  5.8728  5.4869\r\n2009-06-13 20:40:00  9.5873  9.1326  8.4694  7.5889  6.0422  5.8526  5.4657\r\n2009-06-13 20:50:00  9.5810  9.0896  8.4009  7.4652  6.0322  5.8189  5.4379\r\n2009-06-13 21:00:00  9.6034  9.0897  8.3822  7.4905  6.0908  5.7904  5.4039\r\n\r\nIn [3]: dftemp.T030\r\nOut[3]: \r\n2009-06-13 20:20:00    7.8372\r\n2009-06-13 20:30:00    7.8176\r\n2009-06-13 20:40:00    7.5889\r\n2009-06-13 20:50:00    7.4652\r\n2009-06-13 21:00:00    7.4905\r\nName: T030, dtype: float64\r\n\r\nIn [4]: dftemp.T060\r\nOut[4]: \r\nEmpty DataFrame\r\nColumns: [SST, T010, T020, T030, T060, T080, T100]\r\nIndex: []\r\n\r\nIn [5]: dftemp.T080\r\nOut[5]: \r\nEmpty DataFrame\r\nColumns: [SST, T010, T020, T030, T060, T080, T100]\r\nIndex: []\r\n\r\nIn [6]: dftemp.T100\r\nOut[6]: \r\n2009-06-13 20:20:00    5.5192\r\n2009-06-13 20:30:00    5.4869\r\n2009-06-13 20:40:00    5.4657\r\n2009-06-13 20:50:00    5.4379\r\n2009-06-13 21:00:00    5.4039\r\nName: T100, dtype: float64\r\n</PRE>\r\n\r\nNo matter how many columns I\'ve been importing (up to 32 in some cases), it seems like it is always the 5th and 6th columns getting hit.\r\n\r\n'"
3593,14278920,eike-welk,jreback,2013-05-13 20:03:25,2013-05-14 04:07:41,2013-05-13 22:50:08,closed,,0.12,5,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3593,b'Unwanted conversion of `Timestamp` to `long`',"b'Method `combine_first` performs an unwanted conversion of `Timestamp` to `long`. Pandas version is ""0.11.0"". Here is an IPython session demonstrating the behavior, which is IMHO a bug:\r\n\r\n    In [1]: import pandas as pd\r\n\r\n    In [2]: pd.__version__\r\n    Out[2]: \'0.11.0\'\r\n\r\n    In [4]: from datetime import datetime\r\n\r\n    In [5]: df0 = pd.DataFrame({""a"":[datetime(2000, 1, 1), datetime(2000, 1, 2), datetime(2000, 1, 3)]})\r\n\r\n    In [6]: df0\r\n    Out[6]: \r\n                        a\r\n    0 2000-01-01 00:00:00\r\n    1 2000-01-02 00:00:00\r\n    2 2000-01-03 00:00:00\r\n\r\n    In [7]: df1 = pd.DataFrame({""a"":[None, None, None]})\r\n\r\n    In [8]: df1\r\n    Out[8]: \r\n          a\r\n    0  None\r\n    1  None\r\n    2  None\r\n\r\n    In [9]: df2 = df1.combine_first(df0)\r\n\r\n    In [10]: df2\r\n    Out[10]: \r\n                        a\r\n    0  946684800000000000\r\n    1  946771200000000000\r\n    2  946857600000000000\r\n\r\n    In [11]: type(df2[""a""][0])\r\n    Out[11]: long\r\n\r\n    In [12]: type(df0[""a""][0])\r\n    Out[12]: pandas.tslib.Timestamp\r\n\r\nI think `df2` should contain time stamps like `df0`.'"
3590,14266259,tavistmorph,jreback,2013-05-13 15:32:17,2013-06-26 00:04:19,2013-05-14 21:48:10,closed,,0.12,4,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3590,b'Pandas mod 0 should not give 0',"b'In python:\r\n3 % 0 \r\nis undefined and returns ""ZeroDivisionError: integer division or modulo by zero""\r\n\r\nBut in pandas 0.11, that gives 0. I believe that is incorrect. It ought to give NaN or Inf\r\n\r\np = pandas.DataFrame({ \'first\' : [3,4,5,8], \'second\' : [0,0,0,3] })\r\np[\'mod\'] = p[\'first\'] % p[\'second\']\r\np\r\n\r\n'"
3588,14261449,seignour,jreback,2013-05-13 13:57:24,2014-06-17 11:23:40,2013-05-19 16:55:29,closed,,0.12,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3588,b'Pivot NaN Bug',"b'The \'pivot\' function does not properly handle NaNs. E.g.:\r\n\r\n```\r\nimport numpy as np, pandas as pd\r\ntest = pd.DataFrame({""a"":[\'R1\', \'R2\', \'R3\', \'R4\'], ""b"":[""C1"", ""C3"", np.nan , ""C4""], ""c"":[10, 15, np.nan , 20]})\r\n\r\ntest.head(2).pivot(\'a\', \'b\', \'c\')\r\nb   C1  C3\r\na\r\nR1  10 NaN\r\nR2 NaN  15\r\n```\r\n(No NaNs, works properly)\r\n\r\nBut:\r\n```\r\ntest.pivot(\'a\', \'b\', \'c\')\r\nb   C1  C3  C4\r\na\r\nR1 NaN NaN NaN\r\nR2 NaN  10  15\r\nR3 NaN NaN NaN\r\nR4 NaN NaN  20\r\n```\r\n(Incorrect results, does not crash)\r\n\r\nBug is present in versions 0.8.1 and 0.10.1'"
3586,14254961,cpbl,jreback,2013-05-13 10:51:23,2013-05-31 13:14:17,2013-05-13 17:35:52,closed,,0.12,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3586,b'Trouble with NaNs: set_index().reset_index() corrupts data',"b'In the following code, my data are corrupted by the sequence set_index().reset_index(). The value of ""QC"" is actually changed to 1 from NaN where it should be NaN.\r\n\r\nBtw, for symmetry I added the "".reset_index()"", but the data corruption is introduced by set_index.\r\n\r\nThis problem exists in both version \'0.10.1\' and \'0.11.0\'\r\n\r\n        bug=pd.DataFrame({\'PRuid\': {17: \'nonQC\', 18: \'nonQC\', 19: \'nonQC\', 20: \'10\', 21: \'11\', 22: \'12\', 23: \'13\', 24: \'24\', 25: \'35\', 26: \'46\', 27: \'47\', 28: \'48\', 29: \'59\', 30: \'10\'}, \'QC\': {17: 0.0, 18: 0.0, 19: 0.0, 20: nan, 21: nan, 22: nan, 23: nan, 24: 1.0, 25: nan, 26: nan, 27: nan, 28: nan, 29: nan, 30: nan}, \'data\': {17: 7.9544899999999998, 18: 8.0142609999999994, 19: 7.8591520000000008, 20: 0.86140349999999999, 21: 0.87853110000000001, 22: 0.8427041999999999, 23: 0.78587700000000005, 24: 0.73062459999999996, 25: 0.81668560000000001, 26: 0.81927080000000008, 27: 0.80705009999999999, 28: 0.81440240000000008, 29: 0.80140849999999997, 30: 0.81307740000000006}, \'year\': {17: 2006, 18: 2007, 19: 2008, 20: 1985, 21: 1985, 22: 1985, 23: 1985, 24: 1985, 25: 1985, 26: 1985, 27: 1985, 28: 1985, 29: 1985, 30: 1986}})\r\n        print bug\r\n        #print bug.set_index([\'year\',\'PRuid\',\'QC\'])\r\n        print bug.set_index([\'year\',\'PRuid\',\'QC\']).reset_index()\r\n'"
3579,14225885,goyodiaz,jreback,2013-05-11 19:27:40,2013-05-13 18:56:24,2013-05-13 18:56:24,closed,,0.12,4,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/3579,b'Iterating over groupby fails with period-indexed dataframe',"b'I have found this while working with period-indexed data frames on Windows XP 32 bits:\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    index = pd.period_range(start=\'1999-01\', periods=5, freq=\'M\')\r\n    s1 = pd.Series(np.random.rand(len(index)), index=index)\r\n    s2 = pd.Series(np.random.rand(len(index)), index=index)\r\n    series = [(\'s1\', s1), (\'s2\',s2)]\r\n    df = pd.DataFrame.from_items(series)\r\n    grouped = df.groupby(df.index.month)\r\n    list(grouped)\r\n    \r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py"", line 595, in get_iterator\r\n        for key, (i, group) in izip(keys, splitter):\r\n      File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py"", line 2214, in __iter__\r\n        sdata = self._get_sorted_data()\r\n      File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py"", line 2231, in _get_sorted_data\r\n        return self.data.take(self.sort_idx, axis=self.axis)\r\n      File ""D:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 2891, in take\r\n        new_index = self.index.take(indices)\r\n      File ""D:\\Python27\\lib\\site-packages\\pandas\\tseries\\period.py"", line 1110, in take\r\n        taken = self.values.take(indices, axis=axis)\r\n    TypeError: Cannot cast array data from dtype(\'int64\') to dtype(\'int32\') according to the rule \'safe\'\r\n\r\nThis happens with the last stable version 0.11.0. However iterating over grouped.s1 or grouped.s2 just works.'"
3578,14225835,ghl3,jreback,2013-05-11 19:23:47,2014-05-08 13:23:06,2014-05-08 13:23:06,closed,,0.14.0,3,Bug;Difficulty Novice;Visualization,https://api.github.com/repos/pydata/pandas/issues/3578,b'Issue with Pandas boxplot within a subplot',"b""related is #4636\r\n\r\nI'm having an issue drawing a Pandas boxplot within a subplot. Based on the two ways I'm trying, creating the boxplot either removes all the subplots that I've already created, or plots the boxplot after the subplot grid. But I can't seem to draw it within the subplot grid.\r\n\r\nimport matplotlib.pyplot as plt\r\nimport pandas\r\nfrom pandas import DataFrame, Series\r\n\r\ndata = {'day' : Series([1, 1, 1, 2, 2, 2, 3, 3, 3]),\r\n    'val' : Series([3, 4, 5, 6, 7, 8, 9, 10, 11])}\r\ndf = pandas.DataFrame(data)\r\n\r\nThe first thing I've tried is the following:\r\n\r\nplt.figure()\r\n\r\nplt.subplot(2, 2, 1)\r\nplt.plot([1, 2, 3])\r\n\r\nplt.subplot(2, 2, 4)\r\ndf.boxplot('val', 'day')\r\n\r\nBut this simply creates the plot outside of the subplots:\r\n\r\n![image](https://f.cloud.github.com/assets/1400021/491966/0cb03e8c-ba70-11e2-9aed-497ef76bea23.png)\r\n![image](https://f.cloud.github.com/assets/1400021/491967/1896efe8-ba70-11e2-8db5-782babdcb880.png)\r\n\r\nSo, I then tried supplying the axis by hand:\r\n\r\nplt.figure()\r\n\r\nplt.subplot(2, 2, 1)\r\nplt.plot([1, 2, 3])\r\n\r\nplt.subplot(2, 2, 4)\r\nax = plt.gca()\r\ndf.boxplot('val', 'day', ax=ax)\r\n\r\n![image](https://f.cloud.github.com/assets/1400021/491968/2783990c-ba70-11e2-9cfb-7c83bb881049.png)\r\n\r\n\r\nIt seems that an internal call to subplot is messing up my desired subplot structure, preventing me from getting my boxplot image to appear in the bottom right grid in the subplots (the one that's empty in the first set of images)?\r\n\r\nSee this question on StackOverflow:\r\nhttp://stackoverflow.com/questions/16500079/issue-with-pandas-boxplot-within-a-subplot"""
3573,14214776,wesm,wesm,2013-05-11 00:16:57,2013-09-20 03:01:36,2013-05-25 23:25:04,closed,,0.12,8,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3573,b'Unintuitive default behavior with wide DataFrames in the IPython notebook',"b'In the IPython notebook, HTML output it the default and whether summary view is displayed should not be governed by hypothetical line width. I ran into this problem in a demo recently and it took me a minute to figure out what was wrong, definitely a bad change in 0.11. '"
3562,14193395,y-p,jreback,2013-05-10 14:32:01,2013-05-10 16:29:24,2013-05-10 16:04:38,closed,jreback,0.12,3,Bug,https://api.github.com/repos/pydata/pandas/issues/3562,b'df.from_records should be consistent in handling empty `data` arg',"b""carried over from #3487\r\n```python\r\nDataFrame.from_records([], columns=['a','b','c'])\r\n\r\nEmpty DataFrame\r\nColumns: [a, b, c]\r\nIndex: []\r\n```\r\nvs.\r\n```python\r\nDataFrame.from_records([], columns=['a','b','b'])\r\n\r\nAssertionError: Number of manager items must equal union of block items\r\n```"""
3561,14180924,dalejung,jreback,2013-05-10 07:18:40,2013-05-14 21:44:57,2013-05-14 21:44:57,closed,,0.12,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3561,b'DataFrame.ix losing row ordering when index has duplicates',"b""```python\r\nimport pandas as pd\r\n\r\nind = ['A', 'A', 'B', 'C']i\r\ndf = pd.DataFrame({'test':range(len(ind))}, index=ind)\r\n\r\nrows = ['C', 'B']\r\nres = df.ix[rows]\r\nassert rows == list(res.index) # fails\r\n```\r\n\r\nThe problem is that the resulting DataFrame keeps the ordering of the `df.index` and not the `rows` key. You'll notice that the `rows` key doesn't reference a duplicate value. """
3556,14165007,etyurin,jreback,2013-05-09 20:42:22,2013-05-10 20:09:51,2013-05-10 20:09:51,closed,,0.12,6,API Design;Bug,https://api.github.com/repos/pydata/pandas/issues/3556,b'Panel arguments can no longer be named.',"b'This code used to work some time around version 0.10, but now generates a ValueError:\r\n\r\nIn [14]: y\r\nOut[14]: \r\n<class \'pandas.core.panel.Panel\'>\r\nDimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)\r\nItems axis: z1 to z2\r\nMajor_axis axis: 0 to 2\r\nMinor_axis axis: a to b\r\n\r\nIn [15]: y.transpose(\'minor\', \'major\', \'items\')\r\nOut[15]: \r\n<class \'pandas.core.panel.Panel\'>\r\nDimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)\r\nItems axis: a to b\r\nMajor_axis axis: 0 to 2\r\nMinor_axis axis: z1 to z2\r\n\r\nIn [16]: y.transpose( items=\'minor\', major=\'major\', minor=\'items\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-16-00ebdc16128e> in <module>()\r\n----> 1 y.transpose( items=\'minor\', major=\'major\', minor=\'items\')\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.12.0.dev_be25266-py2.7-linux-x86_64.egg/pandas/core/panel.pyc in transpose(self, *args, **kwargs)\r\n   1176                 except (IndexError):\r\n   1177                     raise ValueError(\r\n-> 1178                         ""not enough arguments specified to transpose!"")\r\n   1179 \r\n   1180         axes = [self._get_axis_number(kwargs[a]) for a in self._AXIS_ORDERS]\r\n\r\nValueError: not enough arguments specified to transpose!\r\n\r\n'"
3553,14154213,cpcloud,y-p,2013-05-09 16:39:42,2013-05-10 09:07:38,2013-05-10 09:07:15,closed,,0.12,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3553,b'to_html fails for MultiIndexes with empty strings and display.multi_sparse == False',"b""Piggybacking off of #3547.\r\n```python\r\nimport pandas as pd\r\nfrom nose.tools import assert_raises\r\npd.set_option('display.multi_sparse', False)\r\ndf = pd.DataFrame({'c1': ['a', 'b'], 'c2': ['a', ''], 'data': [1, 2]}).set_index(['c1', 'c2'])\r\nassert_raises(TypeError, df.to_html)  # True\r\n```"""
3552,14152654,wilsaj,jreback,2013-05-09 16:04:20,2014-06-19 15:13:37,2013-05-09 19:18:31,closed,,0.12,3,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3552,b'combine_first converts dtype from bool to object',"b'possibly related to #3041 but still present in 0.11 and current HEAD (23f6058bd6249c21b5db95530a703fa4bc4c2510)\r\n\r\n```python\r\n\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: df1 = pd.DataFrame([[np.nan, 3.,True], [-4.6, np.nan, True], [np.nan, 7., False]])\r\n\r\nIn [4]: df2 = pd.DataFrame([[-42.6, np.nan, True], [-5., 1.6, False]], index=[1, 2])\r\n\r\nIn [5]: df1[2]\r\nOut[5]:\r\n0     True\r\n1     True\r\n2    False\r\nName: 2, dtype: bool\r\n\r\nIn [6]: df2[2]\r\nOut[6]:\r\n1     True\r\n2    False\r\nName: 2, dtype: bool\r\n\r\nIn [7]: df1.combine_first(df2)[2]\r\nOut[7]:\r\n0     True\r\n1     True\r\n2    False\r\nName: 2, dtype: object\r\n```'"
3547,14112199,adeodatus,y-p,2013-05-08 17:19:50,2013-05-18 13:51:33,2013-05-10 09:25:32,closed,,0.12,4,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3547,b'to_html vertically expands multiindex cells if there are empty strings',"b'Notice in the html below the second index column\'s \'a\' field is given as `<th rowspan=""2"" valign=""top"">a</th>`, expanding over what should be an empty cell.\r\n\r\nI actually found this using a pivot table, `margins=True`, as this creates a row keyed like `(\'All\', \'\', \'\')`.\r\n\r\n    print df = pd.DataFrame({\'c1\': [\'a\', \'b\'], \'c2\': [\'a\', \'\'], \'data\': [1, 2]}).set_index([\'c1\', \'c2\'])\r\n           data\r\n    c1 c2      \r\n    a  a      1\r\n    b         2\r\n\r\n    print df.to_html()\r\n    <table border=""1"" class=""dataframe"">\r\n      <thead>\r\n        <tr style=""text-align: right;"">\r\n          <th></th>\r\n          <th></th>\r\n          <th>data</th>\r\n        </tr>\r\n        <tr>\r\n          <th>c1</th>\r\n          <th>c2</th>\r\n          <th></th>\r\n        </tr>\r\n      </thead>\r\n      <tbody>\r\n        <tr>\r\n          <th>a</th>\r\n          <th rowspan=""2"" valign=""top"">a</th>\r\n          <td> 1</td>\r\n        </tr>\r\n        <tr>\r\n          <th>b</th>\r\n          <td> 2</td>\r\n        </tr>\r\n      </tbody>\r\n    </table>'"
3546,14108300,goyodiaz,jreback,2013-05-08 16:01:40,2013-06-14 15:36:07,2013-05-08 18:00:28,closed,,0.12,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3546,b'Indexing TimeSeries by date string drops last day',"b""    >>> import pandas as pd\r\n    >>> pd.__version__\r\n    '0.12.0.dev'\r\n    >>> index = pd.date_range(start='2013-05-31 00:00', end='2013-05-31 23:00', freq='H')\r\n    >>> series = pd.Series(range(len(index)), index=index)\r\n    >>> print series.index.summary()\r\n    DatetimeIndex: 24 entries, 2013-05-31 00:00:00 to 2013-05-31 23:00:00\r\n    Freq: H\r\n    >>> print series['2013-05'].index.summary()\r\n    DatetimeIndex: 1 entries, 2013-05-31 00:00:00 to 2013-05-31 00:00:00\r\n    Freq: H\r\n\r\nI would expect the slice to be the same as the original series."""
3541,14073106,hayd,wesm,2013-05-07 21:54:16,2013-09-20 03:01:36,2013-05-25 23:10:58,closed,,0.12,18,Bug;Docs;Usage Question,https://api.github.com/repos/pydata/pandas/issues/3541,b'max_rows seems to have max value of 60',"b""Max_rows seems to have a maximum value of 60 (in 0.11 and dev, this works as expected in 0.10.1):\r\n\r\n```\r\nIn [1]: %paste\r\nfrom pandas import *\r\nn = 100\r\nfoo = DataFrame(index=range(n))\r\nfoo['floats'] = np.random.randn(n)\r\n\r\nset_option('display.max_rows', 101)\r\n\r\n## -- End pasted text --\r\n\r\nIn [2]: foo.head(100)\r\nOut[2]:\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 100 entries, 0 to 99\r\nColumns: 1 entries, floats to floats\r\ndtypes: float64(1)\r\n\r\nIn [3]: foo.head(61)\r\nOut[3]:\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 61 entries, 0 to 60\r\nData columns (total 1 columns):\r\nfloats    61  non-null values\r\ndtypes: float64(1)\r\n\r\nIn [4]: foo.head(60)\r\nOut[4]:\r\n      floats\r\n0  -0.631717\r\n1   0.231468\r\n2  -0.465971\r\n3  -1.427025\r\n...\r\n```\r\n\r\n```\r\nIn [5]: %paste\r\nset_option('display.max_rows', 10)\r\n\r\n## -- End pasted text --\r\n\r\nIn [6]: foo.head(10)\r\nOut[6]:\r\n     floats\r\n0 -0.194574\r\n1 -0.130100\r\n2  0.547474\r\n3  0.669904\r\n4  0.776946\r\n5 -2.384084\r\n6 -0.011147\r\n7 -0.272660\r\n8 -0.051326\r\n9  1.757020\r\n\r\nIn [7]: foo.head(11)\r\nOut[7]:\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 11 entries, 0 to 10\r\nData columns (total 1 columns):\r\nfloats    11  non-null values\r\ndtypes: float64(1)\r\n```\r\n```\r\nIn [8]: version.version # same true for 0.11\r\nOut[8]: '0.12.0.dev-d925966'\r\n```\r\n\r\nEssentially from http://stackoverflow.com/questions/16424493/pandas-setting-no-of-max-rows.\r\n\r\nRelated: #3395, #3426."""
3522,13942565,cpcloud,y-p,2013-05-03 16:47:52,2013-05-04 08:45:13,2013-05-04 08:45:13,closed,,0.12,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3522,b'qt repr bug',"b""HTML is repr'd, not the symbols `<` and `>`:\r\n\r\n![pandas-qt-bug](https://f.cloud.github.com/assets/417981/459984/2a6f775e-b411-11e2-97dd-1fdd5ce82c2f.png)\r\n"""
3518,13918878,lexual,wesm,2013-05-03 01:43:08,2014-01-17 13:37:10,2013-05-20 18:54:07,closed,,0.12,3,Bug;Docs,https://api.github.com/repos/pydata/pandas/issues/3518,"b'PDF for 0.11.0 is missing, points users to 0.10.1 version'","b'The link to the pdf version here: \r\nhttp://pandas.pydata.org/pandas-docs/stable/\r\n\r\npoint the user here:\r\nhttp://pandas.pydata.org/pandas-docs/stable/pandas.pdf\r\n\r\nWhich is the 0.10.1 version of the docs, not the 0.11.0 version of the docs.\r\n\r\nIt should be updated so that the 0.11.0 pdf versions of the docs are available for Pandas users.'"
3503,13849483,bmacauley,jreback,2013-05-01 14:09:44,2013-09-22 16:15:26,2013-09-22 16:15:26,closed,,0.13,4,Bug;Can't Repro;Data IO;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3503,b'to_csv does not quote fields with special characters',"b'I am using pandas 0.11.in python 2.7.3\r\n\r\nI am processing csv data with windows line terminators in some of the field data. The input data is properly quoted(with QUOTE_MINIMAL )\r\n\r\nwhen writing the data using to_csv, pandas does not quote the fields with windows line terminators ie not using QUOTE_MINIMAL ?\r\n\r\ntest3.csv\r\n\r\nId,Description,Field1,Field2\r\n1,""test data 1 ^M\\n"",2,3\r\n2,""test data 2 ^M\\n"",3,4\r\n\r\n```\r\nIn [93]: in_csv = \'test3.csv\'\r\n\r\nIn [94]: cols = [\'Id\', \'Description\', \'Field1\', \'Field2\']\r\n\r\nIn [95]: reader = pd.read_csv(in_csv, usecols=cols, dtype=object)\r\n\r\nIn [96]: print reader\r\n  Id       Description Field1 Field2\r\n0  1  test data 1 \\r\\n      2      3\r\n1  2  test data 2 \\r\\n      3      4\r\n\r\nIn [97]: reader.to_csv(out_csv, index=False, encoding=\'utf-8\')\r\n\r\nId,Description,Field1,Field2\r\n1,test data 1 ^M\\n,2,3\r\n2,test data 2 ^M\\n,3,4\r\n```\r\n\r\n\r\n'"
3501,13848974,bmacauley,jreback,2013-05-01 13:54:42,2016-02-11 19:36:56,2013-09-20 23:43:00,closed,,0.13,11,Bug;Can't Repro;Data IO;IO CSV,https://api.github.com/repos/pydata/pandas/issues/3501,b'BUG: read_csv does not  parse csv files with windows line terminator correctly',"b'I am using pandas 0.11.in python 2.7.3\r\n\r\nWhen I read csv data with a windows line terminator (\\r\\n), pandas creates extra rows in the dataframe ie it does not recognise the line terminator..i\r\n\r\ntest1.csv\r\n\r\nId,Description,Field1,Field2^M\r\n1,""test data 1 ^M\\n"",2,3^M\r\n2,""test data 2 ^M\\n"",3,4^M\r\n\r\nIn [80]: in_csv = \'test1.csv\'\r\n\r\nIn [81]: cols = [\'Id\', \'Description\', \'Field1\', \'Field2\']\r\n\r\nIn [82]: reader = pd.read_csv(in_csv, usecols=cols, dtype=object)\r\n\r\nIn [83]: print reader\r\n   Id       Description Field1 Field2\r\n0  \\r               NaN    NaN    NaN\r\n1   1  test data 1 \\r\\n      2      3\r\n2  \\r               NaN    NaN    NaN\r\n3   2  test data 2 \\r\\n      3      4\r\n4  \\r               NaN    NaN    NaN\r\n\r\nIn [86]: reader.to_csv(out_csv, index=False, encoding=\'utf-8\')\r\n\r\nId,Description,Field1,Field2\r\n^M,,,\r\n1,test data 1 ^M\\n,2,3\r\n^M,,,\r\n2,test data 2 ^M\\n,3,4\r\n^M,,,\r\n\r\n\r\n\r\n\r\n\r\nI cannot use...\r\n\r\n lineterminator : string (length 1), default None\r\n\r\nreader = pd.read_csv(in_csv, usecols=cols, dtype=object, lineterminator=\'\\r\\n\')\r\n\r\nValueError: Only length-1 line terminators supported\r\n\r\n'"
3495,13819974,jreback,jreback,2013-04-30 18:20:28,2013-05-02 14:54:00,2013-05-02 14:54:00,closed,,0.12,1,Bug;Data IO;Indexing,https://api.github.com/repos/pydata/pandas/issues/3495,b'BUG: fix to_csv to work with dup column indices',b'continuation of #3095 \r\n\r\nthis will work after #3483'
3493,13817509,jmellen,jreback,2013-04-30 17:29:01,2013-05-01 16:31:25,2013-05-01 16:31:25,closed,,0.12,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3493,b'Cannot append DataFrames with uint dtypes to HDFStore',"b""Trying to store DataFrames with unsigned integer dtypes in HDFStore fails due to a bug in the `get_atom_data` methods of the `DataCol` and `DataIndexableCol` classes in `pandas.io.pytables`.  These methods use Python's `capitalize` method to map dtypes to PyTables column types, and fail because PyTables' unsigned int classes start with two capital letters (e.g., UInt32Col).  This error occurs on 0.11 and the current master:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nuint8_series = pd.Series(np.random.random_integers(0,high=255,size=5), dtype=np.uint8)\r\nudf = pd.DataFrame({'u08': uint8_series}, index=np.arange(5))\r\n\r\nstore = pd.HDFStore('uint.h5')\r\n\r\n# this invocation will throw an error in pandas 0.11 and current master\r\nstore.append('uints', udf)\r\n```\r\n\r\nI have a commit + test on my fork that fixes this, just needed to submit the bug here first to have the right commit message on my fork, per contribution standards.  First time committer."""
3492,13807222,CRP,jreback,2013-04-30 13:51:51,2013-05-27 20:34:28,2013-05-07 17:34:46,closed,,0.12,8,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3492,b'assignment in mixed-type dataframes not working?',"b""    from pandas import DataFrame\r\n    df=DataFrame({'a':{1:'aaa',2:'bbb',3:'ccc'},'b':{1:111,2:222,3:333}})\r\n    #this works, new column is created correctly\r\n    df['test']=df['a'].apply(lambda x: '_' if x=='aaa' else x)\r\n    #this does not work, ie column test is not changed\r\n    idx=df['test']=='_'\r\n    temp=df.ix[idx,'a'].apply(lambda x: '-----' if x=='aaa' else x)\r\n    df.ix[idx,'test']=temp\r\n    #this works, column b is changed\r\n    temp=df.ix[idx,'a'].apply(lambda x: 99999 if x=='aaa' else x)\r\n    df.ix[idx,'b']=temp\r\n    #this does not work\r\n    temp=df.ix[idx,'a'].apply(lambda x: 'zzzzzz' if x=='aaa' else x)\r\n    df.ix[idx,'a']=temp\r\n\r\ncode works correctly on 0.9, not on 0.11"""
3490,13793396,bmu,jreback,2013-04-30 06:03:31,2014-07-01 23:59:35,2014-07-01 23:59:35,closed,,0.14.1,3,Bug;Frequency;Visualization,https://api.github.com/repos/pydata/pandas/issues/3490,b'xaxis limits change when plotting time series of different resolution as secondary_y',"b""```python\r\nIn [1]: N = 10000\r\n\r\nIn [2]: rng = pd.date_range('1/1/2000', periods=N, freq='min')\r\n\r\nIn [3]: ts = pd.Series(np.random.randn(N), index=rng)\r\n\r\nIn [4]: ax = ts.plot()\r\n\r\nIn [5]: ax.get_xlim()\r\nOut[5]: (15778080.0, 15788079.0)\r\n\r\nIn [6]: ts.resample('D').plot(style='ro', ax=ax)\r\nOut[6]: <matplotlib.axes.AxesSubplot at 0x4482c50>\r\n\r\nIn [7]: ax.get_xlim()\r\nOut[7]: (15778080.0, 15788079.0)\r\n\r\nIn [8]: ts.resample('D').plot(style='ro', secondary_y=True, ax=ax)\r\nOut[8]: <matplotlib.axes.AxesSubplot at 0x497e450>\r\n\r\nIn [9]: ax.get_xlim()\r\nOut[9]: (10957.0, 10963.0)\r\n```\r\n\r\nAs the xlim changes, the original data is not visible any more. \r\n\r\nCould be related to #2185  """
3489,13790554,jerryzhujian9,y-p,2013-04-30 03:01:04,2013-05-10 10:09:08,2013-05-10 10:09:08,closed,,0.12,4,Bug;Can't Repro,https://api.github.com/repos/pydata/pandas/issues/3489,b'version 0.11 issues with to_csv',"b""It looks like that some changes were made to the DataFrame.to_csv method, as mentioned in the what's new document. \r\n\r\nToday I found two issues with the updated to_csv:\r\n\r\n1) I have an int column, when I use floatformat parameter, the int are changed to float\r\n2) cols parameter simply changes the name of columns, it does not reorder the data in a whole column. \r\n\r\nThanks for your hard working. Hope we can make pandas less buggy and more powerful! """
3487,13774790,tavistmorph,y-p,2013-04-29 18:43:51,2013-05-10 16:38:13,2013-05-10 16:38:13,closed,,0.13,9,Bug,https://api.github.com/repos/pydata/pandas/issues/3487,"b""Can't handle duplicate column names in sql read""","b'Possibly related to #3468 \r\n\r\nUsing pandas 0.10.1.\r\nIt\'s legal for sql (at least Sql Server) to have queries that return two columns with the same name. And it\'s legal for pandas to have dataframes that have two columns with the same name. But pandas gives a very misleading error message when you try it with read_frame:\r\n\r\n#Query that has 2 columns with the same name:\r\nsql = ""select 1 as whatever, 2 as name, 3 as name where 1=0""\r\npos = pandas.io.sql.read_frame(sql, dbconn)\r\n#That gives very misleading error message'"
3484,13768532,dalito,jreback,2013-04-29 16:25:38,2015-01-26 00:57:32,2015-01-26 00:56:17,closed,,,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3484,b'Trellis plot: wrong encoding by shape (rplot.scaleshape)',"b'The trellis plot shown in the documentation (http://pandas.pydata.org/pandas-docs/stable/_images/rplot7_tips.png) that uses ScaleShape to encode the data column ""size"" by shape shows a bug:\r\n\r\nThe same shape is used for ""Sun, 5"" and ""Sun, 6"" although they should have different shapes. Also ""Thur, 4"" and ""Thur, 5"" have the same shape (not visible in linked image).\r\n\r\nThe image also shows that gridlines are not correcly drawn on all panels.'"
3480,13755393,gkoller,jreback,2013-04-29 11:18:59,2013-05-01 14:27:20,2013-05-01 14:27:20,closed,,0.12,4,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3480,"b"" An apply on a DataFrame along axis=1 breaks aggregations on groupby's of that DataFrame""","b""This seems to be a regression from 0.10.1 to 0.11.0\r\n\r\nThis works:\r\n\r\n```python\r\nfrom pandas import DataFrame\r\ndf = DataFrame({'foo1' : ['one', 'two', 'two', 'three', 'one', 'two'],\r\n                'foo2' : np.random.randn(6)})\r\ngrouped = df.groupby('foo1')\r\ngrouped.agg(['mean'])\r\n```\r\n\r\nThis does not (notice the apply):\r\n\r\n```python\r\nfrom pandas import DataFrame\r\ndf = DataFrame({'foo1' : ['one', 'two', 'two', 'three', 'one', 'two'],\r\n                'foo2' : np.random.randn(6)})\r\ndf = df.apply(lambda x: x, axis=1)\r\ngrouped = df.groupby('foo1')\r\ngrouped.agg(['mean'])\r\n```\r\n\r\nThe error raised is:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-47-4df234603be2> in <module>()\r\n----> 1 grouped.agg(['mean'])\r\n\r\n/Users/gkoller/.virtualenvs/abo/lib/python2.6/site-packages/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\r\n    336     @Appender(_agg_doc)\r\n    337     def agg(self, func, *args, **kwargs):\r\n--> 338         return self.aggregate(func, *args, **kwargs)\r\n    339 \r\n    340     def _iterate_slices(self):\r\n\r\n/Users/gkoller/.virtualenvs/abo/lib/python2.6/site-packages/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\r\n   1688                 result = DataFrame(result)\r\n   1689         elif isinstance(arg, list):\r\n-> 1690             return self._aggregate_multiple_funcs(arg)\r\n   1691         else:\r\n   1692             cyfunc = _intercept_cython(arg)\r\n\r\n/Users/gkoller/.virtualenvs/abo/lib/python2.6/site-packages/pandas/core/groupby.pyc in _aggregate_multiple_funcs(self, arg)\r\n   1736             except SpecificationError:\r\n   1737                 raise\r\n-> 1738         result = concat(results, keys=keys, axis=1)\r\n   1739 \r\n   1740         return result\r\n\r\n/Users/gkoller/.virtualenvs/abo/lib/python2.6/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity)\r\n    870                        ignore_index=ignore_index, join=join,\r\n    871                        keys=keys, levels=levels, names=names,\r\n--> 872                        verify_integrity=verify_integrity)\r\n    873     return op.get_result()\r\n    874 \r\n\r\n/Users/gkoller/.virtualenvs/abo/lib/python2.6/site-packages/pandas/tools/merge.pyc in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity)\r\n    911 \r\n    912         if len(objs) == 0:\r\n--> 913             raise Exception('All objects passed were None')\r\n    914 \r\n    915         # consolidate data\r\n\r\nException: All objects passed were None\r\n```\r\n"""
3474,13728027,y-p,y-p,2013-04-28 00:44:16,2013-08-23 13:08:18,2013-08-23 13:08:18,closed,,0.13,18,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3474,b'scatter_plot produces double plots',"b'following #3473:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom pandas.tools.plotting import scatter_plot \r\n\r\ndf = pd.DataFrame( np.random.randn(100,4))\r\nscatter_plot(df,x=1,y=2)\r\n\r\n**Plot**\r\n\r\n**Same Plot Again**\r\n```'"
3465,13701442,cpcloud,y-p,2013-04-26 18:11:36,2013-04-30 04:14:18,2013-04-26 18:52:57,closed,y-p,0.12,2,Bug,https://api.github.com/repos/pydata/pandas/issues/3465,b'Strange index/str repr behavior',"b'I love the idea of valid-python-code reprs, but this just seems plain wrong:\r\n\r\n![weird-repr](https://f.cloud.github.com/assets/417981/432123/aaae270a-ae9c-11e2-9ad6-3983ca576cf4.png)\r\n\r\nThis throws an exception if you try to eval it.\r\n'"
3461,13645472,rla3rd,jreback,2013-04-25 15:48:23,2013-04-30 04:14:24,2013-04-25 21:57:05,closed,,0.12,9,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/3461,b'Sorting on Timestamp broken version 0.11',"b""Issue: ``frame.sort_index`` uses ``argsort`` for a single sort column, but ``_lexsort_indexer`` for multi-columns\r\nneed to do a transform on datetimens[64] before passing to ``_lexsort_`indexer`\r\n\r\n\r\nIn [54]: a.columns\r\nOut[54]: Index([ticker, disclosuredate, txnid], dtype=object) \r\n\r\nIn [55]: a.values\r\nOut[55]: \r\narray([[A, 2010-03-09 00:00:00, 11110508],\r\n       [A, 2010-03-12 00:00:00, 11121853],\r\n       [A, 2011-02-15 00:00:00, 12488915],\r\n       [A, 2011-03-08 00:00:00, 12563380],\r\n       [A, 2011-04-22 00:00:00, 12653015],\r\n       [A, 2013-01-28 00:00:00, 15244694]], dtype=object)\r\n\r\nIn [56]: a.sort(columns=['disclosuredate']).values\r\nOut[56]: \r\narray([[A, 2010-03-09 00:00:00, 11110508],\r\n       [A, 2010-03-12 00:00:00, 11121853],\r\n       [A, 2011-04-22 00:00:00, 12653015],\r\n       [A, 2013-01-28 00:00:00, 15244694],\r\n       [A, 2011-03-08 00:00:00, 12563380],\r\n       [A, 2011-02-15 00:00:00, 12488915]], dtype=object)\r\n\r\nIn [57]: pd.__version__\r\nOut[57]: '0.11.0'\r\n\r\nIn [58]: import time\r\n\r\nIn [59]: a['epoch'] = a['disclosuredate'].map(lambda x: time.mktime(x.timetuple()))\r\n\r\nIn [60]: a.sort(['epoch']).values\r\nOut[60]: \r\narray([[A, 2010-03-09 00:00:00, 11110508, 1268110800.0],\r\n       [A, 2010-03-12 00:00:00, 11121853, 1268370000.0],\r\n       [A, 2011-02-15 00:00:00, 12488915, 1297746000.0],\r\n       [A, 2011-03-08 00:00:00, 12563380, 1299560400.0],\r\n       [A, 2011-04-22 00:00:00, 12653015, 1303444800.0],\r\n       [A, 2013-01-28 00:00:00, 15244694, 1359349200.0]], dtype=object)\r\n\r\n\r\n\r\n\r\n\r\n"""
3460,13644466,y-p,y-p,2013-04-25 15:36:06,2013-05-10 09:30:13,2013-05-10 09:30:13,closed,,0.12,2,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3460,b'tweak unicode() for PeriodIndex to yield valid python code',"b""```\r\n_p=PeriodIndex([u'2000-01-03'],freq='B')\r\nprint unicode(_p)\r\nPeriodIndex([u'2000-01-03'], dtype=int64)\r\n```\r\n\r\ncan't recreate object with result from `str(self)`.\r\n"""
3457,13635705,y-p,y-p,2013-04-25 12:29:18,2013-04-30 04:14:37,2013-04-25 15:25:49,closed,,0.12,7,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3457,"b""engine='python' df.to_csv() duplicate columns doesn't work""","b'example from 0.10.1, and carried on to `to_csv(engine=\'python\')` in 0.11\r\n\r\nnote the first columns duplicates the second columns, which shouldn\'t be.\r\n\r\nCorrolary #3454\r\n\r\n```\r\nfrom pandas.util.testing import makeCustomDataframe as mkdf\r\n\r\nN=10\r\ndf= mkdf(N, 3)\r\ndf.columns = [\'a\',\'a\',\'b\']\r\npath = ""/tmp/k.csv""\r\ndf.to_csv(""/tmp/k.csv"")\r\n!cat ""/tmp/k.csv""\r\nR0,a,a,b\r\nR_l0_g0,R0C1,R0C1,R0C2\r\nR_l0_g1,R1C1,R1C1,R1C2\r\nR_l0_g10,R2C1,R2C1,R2C2\r\nR_l0_g2,R3C1,R3C1,R3C2\r\nR_l0_g3,R4C1,R4C1,R4C2\r\nR_l0_g4,R5C1,R5C1,R5C2\r\nR_l0_g5,R6C1,R6C1,R6C2\r\nR_l0_g6,R7C1,R7C1,R7C2\r\nR_l0_g7,R8C1,R8C1,R8C2\r\nR_l0_g8,R9C1,R9C1,R9C2\r\n```'"
3455,13632598,y-p,jreback,2013-04-25 10:49:27,2013-05-03 17:15:39,2013-04-25 14:38:23,closed,,0.12,3,Bug,https://api.github.com/repos/pydata/pandas/issues/3455,b'fancy indexing with dupe columns yields unexpected ordering',"b""```\r\n\r\nIn [16]: from pandas.util.testing import makeCustomDataframe as mkdf\r\n    ...: df= mkdf(10, 3)\r\n    ...: df.columns = ['a','a','b']\r\n    ...: cols = ['b','a']\r\n    ...: df[['b','a']].columns\r\nOut[16]: Index([a, a, b], dtype=object)\r\n```\r\nexpected to see\r\n```\r\n['b','a','a']\r\n```\r\n\r\nOnce fixed, can relax format.py:CSVFormatter raising NotImplementedError()\r\non cols selection with dupe columns, for the new engine only. see #3457 .\r\n\r\n*edit:* #3509 removed that limitation from CSVFormatter"""
3454,13625052,abudis,y-p,2013-04-25 06:40:46,2013-12-04 00:57:21,2013-04-25 16:11:11,closed,,0.12,4,Bug;Data IO;Regression,https://api.github.com/repos/pydata/pandas/issues/3454,b'[BUG] to_csv() cols parameter not working properly.',"b'<b>pandas version:</b> 0.11\r\npython version: 2.7.4\r\nplatforms: linux & windows\r\n\r\nWhen using cols parameter of to_csv() method only the order of the header is changed, while actual values are not.\r\n```python\r\nimport pandas as pd\r\nfrom cStringIO import StringIO\r\n  \r\ndata = """"""\\\r\ndate,time,X1,X2\r\n2013-1-1,0030,0.1,0.4\r\n2013-1-1,0100,0.2,0.3\r\n2013-1-1,0130,0.3,0.2\r\n2013-1-1,0200,0.4,0.1\r\n""""""\r\n  \r\ndf = pd.read_csv(StringIO(data), parse_dates=[[\'date\', \'time\']], index_col=0)\r\ndf.to_csv(\'data.csv\', index_label=\'date_time\', cols=[\'X2\', \'X1\'])\r\n```\r\n<b>Expected output:</b>\r\ndate_time,X2,X1\r\n2013-01-01 00:30:00,0.4,0.1\r\n2013-01-01 01:00:00,0.3,0.2\r\n2013-01-01 01:30:00,0.2,0.3\r\n2013-01-01 02:00:00,0.1,0.4\r\n\r\n<b>Actual output:</b>\r\ndate_time,X2,X1\r\n2013-01-01 00:30:00,0.1,0.4\r\n2013-01-01 01:00:00,0.2,0.3\r\n2013-01-01 01:30:00,0.3,0.2\r\n2013-01-01 02:00:00,0.4,0.1\r\n\r\n```python\r\ndf.to_csv(\'data.csv\', index_label=\'date_time\', cols=[\'X2\'])\r\n```\r\ndate_time,X2\r\n2013-01-01 00:30:00,0.1\r\n2013-01-01 01:00:00,0.2\r\n2013-01-01 01:30:00,0.3\r\n2013-01-01 02:00:00,0.4\r\n\r\n```python\r\nprint df[\'X1\']\r\n```\r\ndate_time\r\n2013-01-01 00:30:00    0.1\r\n2013-01-01 01:00:00    0.2\r\n2013-01-01 01:30:00    0.3\r\n2013-01-01 02:00:00    0.4\r\nName: X1, dtype: float64\r\n\r\n```python\r\nprint df[\'X2\']\r\n```\r\ndate_time\r\n2013-01-01 00:30:00    0.4\r\n2013-01-01 01:00:00    0.3\r\n2013-01-01 01:30:00    0.2\r\n2013-01-01 02:00:00    0.1\r\nName: X2, dtype: float64'"
3449,13613364,kyleam,jreback,2013-04-24 22:07:11,2013-04-30 04:15:05,2013-04-25 01:56:02,closed,jreback,0.12,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3449,b'loc: label index does not raise KeyError with list of ints',"b""I didn't expect the last line of code to pass.\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    \r\n    pd.__version__\r\n    #[Out]# '0.12.0.dev-5c3ccdb'\r\n    \r\n    np.random.seed(33)\r\n    df = pd.DataFrame(np.random.random((3, 3)),\r\n                      index=['a', 'b', 'c'],\r\n                      columns=['e', 'f', 'g'])\r\n    \r\n    # raise a KeyError?\r\n    df.loc[[1, 2], [1, 2]]\r\n    #[Out]#           f         g\r\n    #[Out]# b  0.870396  0.185040\r\n    #[Out]# c  0.953252  0.680451\r\n\r\nBeing strictly label based, shouldn't `loc` fail when it doesn't find 1 or 2 as a label?"""
3448,13611161,jreback,jreback,2013-04-24 21:30:46,2013-04-30 04:14:54,2013-04-25 14:24:18,closed,,0.12,1,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/3448,b'BUG: selection from unordered time-series has incorrect / odd behavior',"b""original mods made < 0.11 in #3136 / #2437\r\n\r\nincorrect selections, depending on the start:end pos gettting weird values\r\n\r\nalso should be able to select a value that's bigger/smaller than the contained range and have it work (this is how ordered series works)\r\n\r\n```\r\n[21]: s = Series(randn(10),date_range('20130101',periods=10))\r\n\r\nIn [22]: x = s.index.values\r\n\r\nIn [23]: random.shuffle(x)\r\n\r\nIn [24]: s.index = x\r\n\r\nIn [25]: s\r\nOut[25]: \r\n2013-01-09    0.911882\r\n2013-01-06   -0.310166\r\n2013-01-10    1.090196\r\n2013-01-07   -0.846249\r\n2013-01-02   -0.205013\r\n2013-01-08   -1.284152\r\n2013-01-05    0.018700\r\n2013-01-04   -0.503527\r\n2013-01-01    0.073035\r\n2013-01-03    0.695904\r\ndtype: float64\r\n\r\nIn [26]: s['20130104':'20130105']\r\nOut[26]: Series([], dtype: float64)\r\n\r\nIn [27]: s['20130104':'20130107']\r\nOut[27]: Series([], dtype: float64)\r\n\r\nIn [28]: s['20130102':'20130107']\r\nOut[28]: Series([], dtype: float64)\r\n\r\nIn [29]: s['20130102':'20130104']\r\nOut[29]: \r\n2013-01-02   -0.205013\r\n2013-01-08   -1.284152\r\n2013-01-05    0.018700\r\n2013-01-04   -0.503527\r\ndtype: float64\r\n```"""
3446,13598617,danielballan,jreback,2013-04-24 17:01:42,2013-04-30 04:14:50,2013-04-25 14:25:43,closed,,0.12,8,API Design;Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3446,b'Should HDFStore raise an Exception or a Warning when I remove a nonexistent object?',"b""Here is a case where I where the HDFStore would complain:\r\n\r\n    In [60]: store\r\n    Out[60]: \r\n    <class 'pandas.io.pytables.HDFStore'>\r\n    File path: water/expected2.h5\r\n    /spelling            series       (shape->[2])\r\n\r\n    In [61]: store.remove('spellllling')\r\n\r\nIs there a reason to keep it silent?"""
3440,13558621,karmel,cpcloud,2013-04-23 21:33:55,2013-08-05 01:44:08,2013-08-05 01:44:08,closed,,0.13,3,Bug;Internals;Refactor,https://api.github.com/repos/pydata/pandas/issues/3440,"b""Panel attribute naming conflict if item is named 'a'""","b""Including an item named 'a' breaks the expected attribute-retrieval behavior for Panels\r\n\r\n    import numpy as np\r\n    from pandas import DataFrame, Panel\r\n    \r\n    df0 = DataFrame(np.zeros([3,4]))\r\n    df1 = DataFrame(np.ones([3,4]))\r\n    \r\n    p = Panel({'a':df0, 'b':df1})\r\n    \r\n    p.b\r\n    Out[6]: \r\n       0  1  2  3\r\n    0  1  1  1  1\r\n    1  1  1  1  1\r\n    2  1  1  1  1\r\n    \r\n    \r\n    p.a\r\n    Out[7]: 'minor_axis'\r\n\r\nThe expected behavior can be recovered by using anything other than 'a' itself:\r\n\r\n    p = Panel({'a_':df0, 'b':df1})\r\n    p.a_\r\n    Out[9]: \r\n       0  1  2  3\r\n    0  0  0  0  0\r\n    1  0  0  0  0\r\n    2  0  0  0  0\r\n\r\nI'm guessing this is related to the fact that many Panel methods use `a` in iterating through the axes:\r\n\r\n    passed_axes = [kwargs.get(a) for a in self._AXIS_ORDERS]\r\n\r\nThough it's not clear immediately which is the offending use, or if changing all of these is worth it for the recovery of the `a` namespace..."""
3437,13556160,jreback,y-p,2013-04-23 20:38:47,2013-04-30 04:15:36,2013-04-24 12:49:37,closed,y-p,0.12,23,Bug;Dtypes;Regression,https://api.github.com/repos/pydata/pandas/issues/3437,b'BUG: to_csv issue when multiple datetime64[ns] columns with some NaT',b'http://stackoverflow.com/questions/16175761/pandas-to-csv-drops-values/16178032?noredirect=1#comment23125823_16178032\r\n\r\n'
3435,13541616,ijmcf,jreback,2013-04-23 15:52:43,2014-04-27 13:56:33,2013-05-13 22:51:49,closed,,0.13,14,Bug,https://api.github.com/repos/pydata/pandas/issues/3435,b'fillna() does not work when value parameter is a list',"b""Should raise on a passed list to ``value``\r\n\r\nThe results from the fillna() method are very strange when the value parameter is given a list. \r\n\r\nFor example, using a simple example DataFrame:\r\n> df = pandas.DataFrame({'A': [numpy.nan, 1, 2], 'B': [10, numpy.nan, 12], 'C': [[20, 21, 22], [23, 24, 25], numpy.nan]})\r\n> df\r\n    A   B             C\r\n0 NaN  10  [20, 21, 22]\r\n1   1 NaN  [23, 24, 25]\r\n2   2  12           NaN\r\n\r\n> df.fillna(value=[100, 101, 102])\r\n     A    B             C\r\n0  100   10  [20, 21, 22]\r\n1    1  101  [23, 24, 25]\r\n2    2   12           102\r\n\r\nSo it appears the values in the list are used to fill the 'holes' in order, if the list has the same length as number of holes. But if the the list is shorter than the number of holes, the behavior changes to using only the first value in the list:\r\n>  df.fillna(value=[100, 101])\r\n     A    B             C\r\n0  100   10  [20, 21, 22]\r\n1    1  100  [23, 24, 25]\r\n2    2   12           100\r\n\r\nIf the list is longer than the number of holes, you get something even more odd:\r\n> df.fillna(value=[100, 101, 102, 103])\r\n     A    B             C\r\n0  100   10  [20, 21, 22]\r\n1    1  100  [23, 24, 25]\r\n2    2   12           102\r\n\r\nIf you specify provide a dict that specifies the fill values by column, the values from the list are used within that column only:\r\n> df.fillna(value={'C': [100, 101]})\r\n    A   B             C\r\n0 NaN  10  [20, 21, 22]\r\n1   1 NaN  [23, 24, 25]\r\n2   2  12           100\r\n\r\nSince it's not always practical to know the number of NaN values a priori, or to customize the length of the value list to match it, this is problematic. Furthermore, some desired values get over-interpreted and cannot be used:\r\n\r\nFor example, if you want to actually replace all NaN instances in a single column with the same list (either empty or non-empty), I can't figure out how to do it:\r\n> df.fillna(value={'C': [[100,101]]})\r\n    A   B             C\r\n0 NaN  10  [20, 21, 22]\r\n1   1 NaN  [23, 24, 25]\r\n2   2  12           100\r\n\r\nIndeed, if you specify the empty list nothing is filled:\r\n> df.fillna(value={'C': list()})\r\n    A   B             C\r\n0 NaN  10  [20, 21, 22]\r\n1   1 NaN  [23, 24, 25]\r\n2   2  12           NaN\r\n\r\nBut a dict works fine:\r\n> f.fillna(value={'C': {0: 1}})\r\n    A   B             C\r\n0 NaN  10  [20, 21, 22]\r\n1   1 NaN  [23, 24, 25]\r\n2   2  12        {0: 1}\r\n\r\n> df.fillna(value={'C': dict()})\r\n    A   B             C\r\n0 NaN  10  [20, 21, 22]\r\n1   1 NaN  [23, 24, 25]\r\n2   2  12            {}\r\n\r\nSo it appears the fillna() is making a lot of decisions about how the fill values should be applied, and certain desired outcomes can't be achieved because it's being too 'clever'.\r\n"""
3428,13525202,hemflit,jreback,2013-04-23 09:03:17,2013-09-25 16:23:43,2013-08-24 00:37:13,closed,jreback,0.13,4,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3428,b'Inconsistent comparison between datetime and series of datetime',"b""similar to #3416 (also failing in 0.11)\r\n\r\nUsing pandas 0.10.1 and numpy 1.6.1:\r\n\r\n    >>> import pandas; import numpy\r\n    >>> s = pandas.Series(pandas.date_range('2013-01-01', periods=3))\r\n    >>> d = numpy.datetime64('2013-01-02')\r\n    >>> s > d\r\n    0    False\r\n    1    False\r\n    2     True\r\n    >>> d < s\r\n    0    True\r\n    1    True\r\n    2    True\r\n    >>> s.dtype\r\n    dtype('datetime64[ns]')\r\n    >>> d.dtype\r\n    dtype('datetime64[us]')\r\n\r\nAll other comparison operators also give expected results with operands (s,d), and seem to compare them as unscaled integers when the order is (d,s)."""
3425,13514378,jreback,jreback,2013-04-23 00:46:29,2013-12-04 00:57:16,2013-05-08 21:51:55,closed,jreback,0.12,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3425,b'BUG: what to do with astype on datetime64',b'http://stackoverflow.com/questions/16158795/cant-convert-dates-to-datetime64\r\n\r\nprob should raise if u try to convert to something other than object or datetime64[ns]\r\notherwise can have weird things happen'
3419,13497132,y-p,y-p,2013-04-22 18:06:15,2013-04-30 04:15:23,2013-04-24 22:39:54,closed,,0.12,0,Bug,https://api.github.com/repos/pydata/pandas/issues/3419,b'investigate travis warning in py2.6',"b""```\r\nException RuntimeError: 'maximum recursion depth exceeded while calling a Python object' \r\nin <type 'exceptions.RuntimeError'> ignored\r\n```"""
3418,13494069,tavistmorph,jreback,2013-04-22 17:23:43,2013-09-30 13:07:28,2013-09-30 13:07:28,closed,,0.13,4,Bug;IO CSV;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3418,b'API: read_csv inconsistent with from_csv -- parses ints as dates',"b'Using pandas 0.10.1.\r\nI read the docs, but didn\'t see any explanation of why this would be true. pandas.read_csv() works exactly as you\'d expect, but pandas.DataFrame.from_csv() is different. Looks like the latter method assumes you\'re probably dealing with time series data, so it sets defaults parameters to automatically convert integers to dates. I disagree that this is desired, but even if it is, why would it be true for the later method but not the former? Why shouldn\'t both methods assume the same default assumptions?\r\n\r\nCreate a CSV like this:\r\na,b\r\n1,4\r\n2,3\r\n\r\nNow this does exactly what you\'d expect:\r\np =  pandas.read_csv(your_csv_file)\r\n\r\nBut this converts the first column into a data. Almost certainly not what you\'d expect:\r\np = pandas.DataFrame.from_csv(your_csv_file)\r\n\r\nThere is an optional parameter on the second method ""parse_dates"" which is default False. If you add that flag, then the second method works just like the first. But why the inconsistency? I\'d expect this method to default to acting just like the other one. '"
3417,13492481,floux,hayd,2013-04-22 16:52:27,2013-08-27 00:20:23,2013-08-27 00:20:23,closed,,0.13,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3417,b'DataFrame.groupby(as_index=False).apply() ignores option and sets wrong indices',"b""The following code starts from a 9 by 2 DataFrame with 3 groups, no index. When applying a function to each group that returns a 1 by 2 DataFrame, an index is introduced that doesn't seem to make sense, and the as_index=False option is ignored:\r\n\r\n    df = pd.DataFrame({\r\n                   'a' : [1,1,1,2,2,2,3,3,3],\r\n                   'b' : [1,2,3,4,5,6,7,8,9],\r\n    })\r\n\r\n    def f(x):\r\n        if x.a[:1] == 2:\r\n            # In the documentation there is nothing that indicates how one could\r\n            # return an 'empty group', ie nothing, from a function suitable for an apply.\r\n            # Is this solution adequate?\r\n            return pd.DataFrame({})\r\n        return pd.DataFrame({'mean' : [x.b.mean()], 'std' : [x.b.std()]})\r\n\r\n    grouped = df.groupby('a', as_index=False).apply(f)\r\n    grouped.index\r\n    MultiIndex\r\n    [(1, 0), (3, 0)]\r\n\r\nIn short: a MultiIndex is introduced, when ideally I would have expected no index at all, and in the worst case a non-hierarchical Index corresponding to the groups 1 and 3.\r\n    """
3416,13482171,jreback,jreback,2013-04-22 13:41:21,2014-03-30 09:25:39,2013-05-08 21:52:21,closed,jreback,0.12,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3416,b'BUG: conversion from np.datetime64 has odd numpy buglet',"b""see #3414\r\n\r\nThe resulting dtype is wrong (should be datetime64[ns]),\r\nand should either raise on dtype='datetime64' or deal with it in Series\r\n\r\n```\r\nIn [1]: dates = [\r\n   ...:     np.datetime64(datetime.date(2013, 1, 1)),\r\n   ...:     np.datetime64(datetime.date(2013, 1, 2)),\r\n   ...:     np.datetime64(datetime.date(2013, 1, 3)),\r\n   ...: ]\r\n\r\nIn [2]: s = pd.Series(dates, dtype='datetime64')\r\n\r\nIn [3]: s.ix[0] = np.nan\r\n\r\nIn [4]: print s\r\n0                   NaT\r\n1   2013-01-02 00:00:00\r\n2   2013-01-03 00:00:00\r\ndtype: datetime64[us]\r\n```\r\n\r\nThis should force conversion on the np.datetime64 (regardless of there dtype)\r\n```\r\nIn [22]: pd.Series(dates)\r\nOut[22]: \r\n0    2013-01-01 00:00:00\r\n1    2013-01-02 00:00:00\r\n2    2013-01-03 00:00:00\r\ndtype: object\r\n```\r\n\r\nClearly incorrect interpret of np.datetime64 dtypes as well (its interpreting them as M8[ns], not converting\r\nto)\r\n```\r\nIn [25]: pd.Series(dates,dtype='M8[ns]')\r\nOut[25]: \r\n0   1970-01-16 16:56:38.400000\r\n1   1970-01-16 16:58:04.800000\r\n2   1970-01-16 16:59:31.200000\r\ndtype: datetime64[ns]\r\n```\r\n\r\n"""
3415,13468466,mattias-lundell,jreback,2013-04-22 08:24:40,2013-09-25 15:10:49,2013-09-25 15:10:49,closed,,0.13,3,Bug;Numeric,https://api.github.com/repos/pydata/pandas/issues/3415,b'Possibility to include np.inf as end point in pd.cut bins.',"b""I don't know if it is desirable, but I think it would be nice to be able to include an infinity value as the endpoint to the bins in pandas cut function. \r\n\r\n```\r\npd.cut([10, 20], [1, 5, np.inf], right=False)\r\n```\r\n"""
3414,13463118,lexual,jreback,2013-04-22 04:52:37,2013-05-09 00:43:14,2013-05-09 00:43:14,closed,jreback,0.12,10,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3414,b'astype(object) with dtype datetime64 does wacky things when value is NaT',"b'Similar to #3416, failing in 0.11\r\n\r\nCalling astype(object) when the dtype is datetime64 and there are NaT values, sets them to 2262-04-10 00:12:43.145224\r\n\r\nI would have thought they should be set to None.\r\n\r\nSee attached test case.'"
3403,13417527,changhiskhan,jreback,2013-04-19 19:45:04,2013-09-22 00:34:41,2013-09-22 00:34:41,closed,,0.13.1,1,Bug;Duplicate;IO CSV;Unicode,https://api.github.com/repos/pydata/pandas/issues/3403,b'python parser does not handle encoding parameter correctly',"b'read_table with encoding=""ISO-8859-2"" and sep=""::"" does not convert movies.dat movielens dataset to unicode correctly.'"
3398,13390153,lodagro,wesm,2013-04-19 07:25:20,2013-04-23 00:17:26,2013-04-22 23:58:21,closed,,0.11,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3398,b'read_csv and missing data issue',"b'```python\r\nIn [69]: data = """"""\\\r\n   ....: A,B,C\r\n   ....: foo,,\r\n   ....: ,bar,\r\n   ....: ,,baz\r\n   ....: """"""\r\n\r\nIn [70]: pd.read_csv(StringIO(data))\r\n---------------------------------------------------------------------------\r\nCParserError                              Traceback (most recent call last)\r\n...\r\nCParserError: Error tokenizing data. C error: EOF inside string starting at line 2\r\n```\r\n.\r\n\r\n```python\r\nIn [79]: data = """"""\\\r\n   ....: A,B\r\n   ....: foo,\r\n   ....: ,bar\r\n   ....: """"""\r\n\r\nIn [80]: pd.read_csv(StringIO(data))\r\nOut[80]:\r\n         A   B\r\n0      foo NaN\r\n1  bar("""") NaN\r\n```\r\n\r\nThis is fine\r\n\r\n```python\r\nIn [67]: data = """"""\\\r\n   ....: A,B,C\r\n   ....: ,,foo\r\n   ....: ,bar,\r\n   ....: baz,,\r\n   ....: """"""\r\n\r\nIn [68]: pd.read_csv(StringIO(data))\r\nOut[68]:\r\n     A    B    C\r\n0  NaN  NaN  foo\r\n1  NaN  bar  NaN\r\n2  baz  NaN  NaN\r\n```'"
3387,13322408,cpcloud,y-p,2013-04-17 22:12:28,2013-04-22 14:38:51,2013-04-22 14:38:51,closed,lodagro,0.11,2,Bug;Output-Formatting;Regression,https://api.github.com/repos/pydata/pandas/issues/3387,b'DataFrame repr regression',"b""There's a call to `to_string` in `DataFrame._repr_fits_horizontal` that is **drastically** slowing down `repr`ing of large `DataFrame`s and incidentally makes the option `max_info_rows` useless since that call occurs before `max_info_rows` comes into play. \r\n\r\nIs there any way to compute the length of the column `repr` without using the entire frame as a string? Also, for large frames that will create tons of temporary strings that could cause a segfault if one doesn't have swap enabled (that's probably a bit alarmist; I'm trying to be thorough here. :)) At best it will raise a `MemoryError` exception. \r\n\r\nWhat's the thinking here? One could check `max_info_rows` in `DataFrame._repr_fits_horizontal`, but that's more of a stopgap solution than an actual one. Using `cStringIO` is not a viable solution: it only gives a ~29 ms speedup. Also, should there be a vbench for `repr`ing?"""
3386,13311794,jreback,jreback,2013-04-17 18:43:34,2013-08-16 19:32:47,2013-08-16 19:32:47,closed,,0.13,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3386,b'BUG: fillna upconversion on Frame',"b'Thought had fixed this.....but guess not\r\n\r\nhttp://stackoverflow.com/questions/16067144/in-pandas-how-to-use-fillna-to-fill-the-whole-columns-with-string-if-the-column/16067653#16067653\r\n```\r\nIn [2]: csv=u""""""a,a,,a\r\nb,b,,b\r\nc,c,,c\r\n""""""\r\n\r\nIn [3]: df = pd.read_csv(StringIO(csv), header=None)\r\n\r\nIn [4]: df\r\nOut[4]: \r\n   0  1   2  3\r\n0  a  a NaN  a\r\n1  b  b NaN  b\r\n2  c  c NaN  c\r\n\r\nIn [5]: df.dtypes\r\nOut[5]: \r\n0     object\r\n1     object\r\n2    float64\r\n3     object\r\ndtype: object\r\n\r\nIn [6]: df.fillna({ 2: \'foo\' })\r\n\r\nValueError: could not convert string to float: foo\r\n\r\n```'"
3381,13297406,sshell0611,y-p,2013-04-17 14:07:16,2013-05-10 10:57:28,2013-05-10 10:57:28,closed,,0.11,4,Bug,https://api.github.com/repos/pydata/pandas/issues/3381,b'Merge Issues',"b""I recently upgraded from pandas 0.90 to 0.10.1 and it seems a merge I was using is now broken.  For example, I have two dataframes that I am left merging on a Timestamp typed column.  \r\n\r\nThe first dataframe has three columns: Timestamp, float, Timestamp:\r\n\r\ndf1 = pd.DataFrame({'time':[pd.Timestamp('2013-01-01'),pd.Timestamp('2013-01-02')],'val':[15.6, 15.9], 'expiry':[pd.Timestamp('2013-01-16'), pd.Timestamp('2013-01-16')]})\r\n\r\nThe second DataFrame has three columns: Timestamp, Timestamp. float:\r\n\r\ndf2 = pd.DataFrame({'expiry':[pd.Timestamp('2013-01-16'), pd.Timestamp('2013-02-13')], 'badjfact':[.98, .78], 'volmat':[pd.Timestamp('2013-02-15'), pd.Timestamp('2013-03-15')]})\r\n\r\nMerging these works using the following command:\r\n\r\n>>> pd.merge(df1, df2, on='expiry', how='left')\r\n               expiry                time   val  badjfact              volmat\r\n0 2013-01-16 00:00:00 2013-01-01 00:00:00  15.6      0.98 2013-02-15 00:00:00\r\n1 2013-01-16 00:00:00 2013-01-02 00:00:00  15.9      0.98 2013-02-15 00:00:00\r\n\r\nHowever, when I change the 'val' column in DataFrame 1 to a string, (or if I add a new string column to DataFrame 1), the merge will then have NaN values from the second dataframe:\r\n\r\ndf1 = pd.DataFrame({'time':[pd.Timestamp('2013-01-01'),pd.Timestamp('2013-01-02')],'val':['test1', 'test2'], 'expiry':[pd.Timestamp('2013-01-16'), pd.Timestamp('2013-01-16')]})\r\n\r\nBroken output:\r\n\r\n>>> pd.merge(df1, df2, on='expiry', how='left')\r\n                expiry                 time    val  badjfact  volmat\r\n0  2013-01-16 00:00:00  2013-01-01 00:00:00  test1       NaN     NaT\r\n1  2013-01-16 00:00:00  2013-01-02 00:00:00  test2       NaN     NaT\r\n\r\n\r\n"""
3375,13258830,schwallie,jreback,2013-04-16 17:48:20,2015-01-26 00:52:19,2015-01-26 00:52:19,closed,,0.16.0,9,Bug;Dtypes;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3375,b'Nanoseconds being truncated in asof',"b""match_time = get_info.index.asof(trade_time)\r\nmatch_price = get_info[match_time]['Bid Price']\r\n\r\nThe actual index value:\r\n\\<Timestamp: 2013-04-10 10:22:01.696815975-0500 CDT, tz=US/Central>\r\n\r\nThe error returned:\r\nKeyError: u'no item named 2013-04-10 10:22:01.696815-05:00'\r\n\r\n\r\nIt seems to me that it is cutting off the last digits of the timestamp in this example.\r\n"""
3371,13217410,jreback,jreback,2013-04-15 20:07:32,2013-09-08 19:46:53,2013-09-08 19:46:53,closed,,0.13,0,Bug;Dtypes;Enhancement;Groupby;Timedelta,https://api.github.com/repos/pydata/pandas/issues/3371,b'BUG/ENH: timedelta fillna should work intuitively',"b""http://stackoverflow.com/questions/16023584/pandas-time-delta-from-grouped-neighbors\r\n\r\n\r\nsetup\r\n```\r\ntxt = '''ID,DATE\r\n002691c9cec109e64558848f1358ac16,2003-08-13 00:00:00\r\n002691c9cec109e64558848f1358ac16,2003-08-13 00:00:00\r\n0088f218a1f00e0fe1b94919dc68ec33,2006-05-07 00:00:00\r\n0088f218a1f00e0fe1b94919dc68ec33,2006-06-03 00:00:00\r\n00d34668025906d55ae2e529615f530a,2006-03-09 00:00:00\r\n00d34668025906d55ae2e529615f530a,2006-03-09 00:00:00\r\n0101d3286dfbd58642a7527ecbddb92e,2007-10-13 00:00:00\r\n0101d3286dfbd58642a7527ecbddb92e,2007-10-27 00:00:00\r\n0103bd73af66e5a44f7867c0bb2203cc,2001-02-01 00:00:00\r\n0103bd73af66e5a44f7867c0bb2203cc,2008-01-20 00:00:00\r\n'''\r\ndf = pandas.read_csv(StringIO.StringIO(txt))\r\ndf = df.sort('DATE')\r\ndf.DATE = pandas.to_datetime(df.DATE)\r\ngrouped = df.groupby('A')\r\n```\r\n1) this should probably work? (fill in a 0 timedelta)\r\n```grouped.apply(lambda g: g['DATE']-g['DATE'].shift()).fillna(0)```\r\n\r\n2) this is returning ``timedelta64[us]``, weird numpy error again\r\n```\r\nIn [34]: grouped.apply(lambda g: g['DATE']-g['DATE'].shift())\r\nOut[34]: \r\n8                   NaT\r\n0                   NaT\r\n1              00:00:00\r\n4                   NaT\r\n5              00:00:00\r\n2                   NaT\r\n3     27 days, 00:00:00\r\n6                   NaT\r\n7     14 days, 00:00:00\r\n9   2544 days, 00:00:00\r\nName: DATE, dtype: timedelta64[us]\r\n```\r\nThis DOES work\r\n```\r\nIn [57]: df['X_SEQUENCE_GAP'].sort_index().astype('timedelta64[ns]').fillna(0)\r\nOut[57]: \r\n0              00:00:00\r\n1              00:00:00\r\n2              00:00:00\r\n3     27 days, 00:00:00\r\n4              00:00:00\r\n5              00:00:00\r\n6              00:00:00\r\n7     14 days, 00:00:00\r\n8              00:00:00\r\n9   2544 days, 00:00:00\r\nName: X_SEQUENCE_GAP, dtype: timedelta64[ns]\r\n```\r\n\r\nThis DOES NOT WORK, (need to use a view of i8 first)\r\n```\r\nIn [58]: df['X_SEQUENCE_GAP'].sort_index().astype('timedelta64[ns]').ffill()\r\nValueError: Invalid dtype for pad_1d [timedelta64[ns]]\r\n```\r\n"""
3367,13209338,kiroh,jreback,2013-04-15 16:54:49,2014-07-23 21:28:50,2014-07-23 21:28:50,closed,,0.15.0,11,Bug;Period,https://api.github.com/repos/pydata/pandas/issues/3367,b'BUG: combine_first behaves strangely with period index',"b""If i do:\r\n```python\r\nfrom pandas import Series, PeriodIndex, Period\r\n\r\npi = PeriodIndex(start=Period('1950-1'), freq='M', end=Period('1950-7'))\r\na = Series([1,nan,nan,4,5,nan,7], pi)\r\nb = Series([9,9,9,9,9,9,9], pi)\r\n\r\ndisplay(a, b, a.combine_first(b))\r\n```\r\n\r\nThen the output is:\r\n```\r\n1950-01     1\r\n1950-02   NaN\r\n1950-03   NaN\r\n1950-04     4\r\n1950-05     5\r\n1950-06   NaN\r\n1950-07     7\r\nFreq: M\r\n\r\n1950-01    9\r\n1950-02    9\r\n1950-03    9\r\n1950-04    9\r\n1950-05    9\r\n1950-06    9\r\n1950-07    9\r\nFreq: M\r\n\r\n1930-01   NaN\r\n1930-03   NaN\r\n1930-05   NaN\r\n1930-07   NaN\r\n1930-09   NaN\r\n1930-11   NaN\r\n1931-01   NaN\r\nFreq: M\r\n```\r\n\r\nAs you can see the index is changed and the values are all gone.\r\nI think this is a bug. My Pandas version is `0.10.1`."""
3340,13157000,dengemann,jreback,2013-04-13 14:17:50,2013-09-22 00:32:10,2013-09-22 00:32:10,closed,,0.13,8,Bug;Enhancement;Indexing,https://api.github.com/repos/pydata/pandas/issues/3340,b'BUG? Indexing + assignment does not work if member selected after indexing',"b""Hi there,\r\n\r\nthis is one that has puzzled me for quite some time, but somehow it never asked about it. It seems whether an conditional assignment to a df column is performed or not depends on the order of indexing / member getting inside the left-hand expression of the assignment statement. See this snippet:\r\n\r\n``` Python\r\nimport pandas as pd\r\n\r\n# create some data\r\ndata = {'foo': list('abcde'),\r\n        'bar': range(5)}\r\ndf = pd.DataFrame(data)\r\n\r\n# create arbitrary index.\r\nmy_idx = df.bar > 2\r\nprint df[my_idx]\r\n\r\n# assign to a  member which was accessed after indexing.\r\ndf[my_idx].foo = 'X'\r\n\r\n# see there is no change\r\nprint df\r\n\r\n# assign to member which was accessed before indexing.\r\ndf.foo[my_idx] = 'X'\r\n\r\n# now it has worked\r\nprint df\r\n```\r\n\r\nIs there a deliberate reason for this behaviour? My feeling about this is that it may confuse new users and may be hard to track down, as this goes silently and looks like correct code. \r\n\r\n"""
3337,13151379,dengemann,y-p,2013-04-13 08:39:36,2013-04-22 14:38:23,2013-04-22 14:38:23,closed,lodagro,0.11,22,Bug,https://api.github.com/repos/pydata/pandas/issues/3337,b'FIX? df.__repr__ takes for ever with big data',"b""Hi all, \r\n\r\nafter having updated my local branch with the current dev master ( pd.__version__ == '0.11.0rc1' -- compiling under Mac OS 10.8 went smoothly) some of my scripts in which I handle large data frames (2.6 million rows, 10 columns) stopped working. \r\nIt turned out that accessing the df's repr method but also looking up docstrings seems to make ipython unresponsive and creates the impression as if it takes forever.\r\n\r\nThis little snippet reproduces the error with fake data ('tested' with EPD Python and IPython 7.2 (32-bit)):\r\n\r\n``` Python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndata  = np.random.random((3000000, 10))\r\ndf = pd.DataFrame(data)\r\n\r\nprint df  # does the same as `df.__repr__()` or `df?`\r\n```\r\n\r\nCan anyone confirm this?\r\nIt must have been some of the more recent changes as I did not have trouble with this earlier.\r\nAny pointers and comments are highly welcome.\r\n\r\nCheers,\r\nDenis"""
3335,13145555,wesm,jreback,2013-04-13 00:54:00,2015-11-15 16:46:44,2015-11-15 16:46:44,closed,,0.17.1,7,Bug;Difficulty Novice;Reshaping;Testing,https://api.github.com/repos/pydata/pandas/issues/3335,"b'Test/fix case where use of ""All"" causes problems in pivot_table with margins=True'",
3334,13145528,wesm,jreback,2013-04-13 00:52:37,2013-08-12 13:21:57,2013-08-12 13:21:57,closed,,0.13,3,Bug,https://api.github.com/repos/pydata/pandas/issues/3334,b'Pivot table margins brittleness',"b"">I'm trying to find the most efficient way to tabulate responses to a survey using pandas. Here's a sample survey:\r\n```\r\ndf = DataFrame({'Response' : ['Y', 'N' ,'N', 'Y', 'Y', 'N'],\r\n                'Type' : ['A', 'A', 'B', 'B', 'B', 'C']})\r\n```\r\n\r\n>I want to count the number of responses of each value (Y or N), with the different types across the top.\r\n\r\n>1) A simple way to do this is\r\n\r\n```\r\ndf.groupby('Type')['Response'].value_counts().unstack(level=0)\r\n\r\nType     A      B        C\r\nN       1       1       1\r\nY       1       2       NaN\r\n```\r\n>This is almost what I want, except I'd like row and column totals as well. Of course, I could calculate these manually, but that seems ugly. What I'd really like to do is create a pivot table with margins=True, but this isn't quite as easy as I thought it would be.\r\n\r\n>2) Again without totals, I can use\r\n\r\n        pivot_table(df, rows='Response',cols='Type',aggfunc=len)\r\n\r\n>This produces almost the same output as above, though now the index has a name:\r\n\r\n```\r\nType            A       B       C\r\nResponse\r\nN                       1       1        1\r\nY                       1       2       NaN\r\n```\r\n>So far so good, but adding margins=True results in an error.\r\n\r\n>3) To get exactly what I want, the following works, but it seems there should be a nicer way:\r\n\r\n```\r\ndf['Count'] = 1\r\npivot_table(df, rows='Response',cols='Type',values='Count',aggfunc=len,margins=True)     # Can also use sum\r\n```\r\n\r\n>I have three questions:\r\n\r\n>1) Why does adding 'margins=True' in #2 above result in an error, while it works in #3? I'm not sure I understand what's happening here.\r\n\r\n>2) If this behavior is intentional, is there a better way to get the output I'm looking for, without introducing an extra, unwanted column?\r\n\r\n>3) How do I remove the name of the index from a dataframe (to get the output from #2 to look the same as in #1)?\r\n"""
3321,13077741,y-p,jorisvandenbossche,2013-04-11 16:58:43,2016-07-04 10:50:34,2016-07-04 10:50:34,closed,,0.18.0,4,Bug;Resample;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3321,"b'ts.resample quietly does nothing when resampling with how=""ffill""'","b""```python\r\nix = pd.date_range('1/1/2013', periods=12, freq='M')\r\nts = Series(np.arange(12), index=ix)\r\nts\r\n2013-01-31     0\r\n2013-02-28     1\r\n2013-03-31     2\r\n2013-04-30     3\r\n2013-05-31     4\r\n2013-06-30     5\r\n2013-07-31     6\r\n2013-08-31     7\r\n2013-09-30     8\r\n2013-10-31     9\r\n2013-11-30    10\r\n2013-12-31    11\r\nFreq: M, dtype: int64\r\n\r\nts.resample('1D', how='ffill')\r\n2013-01-31     0\r\n2013-02-28     1\r\n2013-03-31     2\r\n2013-04-30     3\r\n2013-05-31     4\r\n2013-06-30     5\r\n2013-07-31     6\r\n2013-08-31     7\r\n2013-09-30     8\r\n2013-10-31     9\r\n2013-11-30    10\r\n2013-12-31    11\r\nFreq: M, dtype: int64\r\n```"""
3317,13050129,joeb1415,jreback,2013-04-11 02:36:41,2013-08-21 12:45:07,2013-08-21 12:45:07,closed,,0.13,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3317,b'BUG: reindex by both index and columns loses DatetimeIndex frequency',"b""Given:\r\n\r\n    from numpy import ones\r\n    from pandas import DataFrame, __version__\r\n    df = DataFrame(ones((3, 3)), index=[datetime(2012, 1, 1), datetime(2012, 1, 2), datetime(2012, 1, 3)], columns=['a', 'b', 'c'])\r\n\r\n    time_freq = date_range('2012-01-01', '2012-01-03', freq='d')\r\n    some_cols = ['a', 'b']\r\n\r\nWe get:\r\n\r\n    __version__\r\n> '0.10.1'\r\n\r\n    df.reindex(index=time_freq).index.freq\r\n> &lt;1 Day&gt;\r\n\r\n    df.reindex(index=time_freq, columns=some_cols).index.freq\r\n> None\r\n\r\n    df.reindex(index=time_freq).reindex(columns=some_cols).index.freq\r\n> &lt;1 Day&gt;"""
3315,13047707,JDavid4,hayd,2013-04-11 00:41:06,2013-07-29 06:41:19,2013-07-29 06:41:19,closed,,0.13,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3315,b'Data Frame division',b'I am seeing the following behavior when dividing dataframes A and B below.  It seems there is cross divisions between common indexes as in Out[268] below.  I am hoping for Out[269].\r\nIs the former the intended behavior?\r\n\r\n![image](https://f.cloud.github.com/assets/4120749/365315/002a11c6-a240-11e2-8411-4e7bac6a4d47.png)\r\n'
3311,13020867,jreback,jreback,2013-04-10 14:16:26,2013-08-11 02:11:03,2013-04-10 18:25:46,closed,,0.11,1,Bug;Dtypes;Indexing,https://api.github.com/repos/pydata/pandas/issues/3311,b'BUG: datetime selection in a DataFrame should work in the where',"b""I think something broke:\r\n\r\nsee: http://stackoverflow.com/questions/15927451/filter-on-pandas-dataframe-with-datetime-columns-raises-error/15927797?noredirect=1#comment22689922_15927797\r\n\r\nThis is wrong (should give rows > 0)\r\n```\r\nIn [3]: df = pd.DataFrame(dict(A = pd.date_range('20130102',periods=5), B = pd.date_range('20130104',periods=5)))\r\n\r\nIn [4]: df\r\nOut[4]: \r\n                    A                   B\r\n0 2013-01-02 00:00:00 2013-01-04 00:00:00\r\n1 2013-01-03 00:00:00 2013-01-05 00:00:00\r\n2 2013-01-04 00:00:00 2013-01-06 00:00:00\r\n3 2013-01-05 00:00:00 2013-01-07 00:00:00\r\n4 2013-01-06 00:00:00 2013-01-08 00:00:00\r\n\r\nIn [5]: df[df>pd.Timestamp('20130103')]\r\nOut[5]: \r\n                    A                   B\r\n0 2013-01-02 00:00:00 2013-01-04 00:00:00\r\n1 2013-01-03 00:00:00 2013-01-05 00:00:00\r\n2 2013-01-04 00:00:00 2013-01-06 00:00:00\r\n3 2013-01-05 00:00:00 2013-01-07 00:00:00\r\n4 2013-01-06 00:00:00 2013-01-08 00:00:00\r\n```"""
3309,13006328,wesm,y-p,2013-04-10 07:00:41,2013-04-10 08:00:48,2013-04-10 08:00:48,closed,,0.11,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3309,b'test_bar_log failure',"b'matplotlib v 1.1.1. maybe i should upgrade?\r\n\r\n```\r\n23:57 ~/code/pandas  (master)$ nosetests pandas/tests/test_graphics.py\r\n........F...............................\r\n======================================================================\r\nFAIL: test_bar_log (pandas.tests.test_graphics.TestDataFramePlots)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/wesm/code/pandas/pandas/tests/test_graphics.py"", line 414, in test_bar_log\r\n    self.assertEqual(ax.yaxis.get_ticklocs()[0],1.0)\r\nAssertionError: 0.10000000000000001 != 1.0\r\n\r\n----------------------------------------------------------------------\r\nRan 40 tests in 76.852s\r\n\r\nFAILED (failures=1)\r\n```'"
3308,13002034,TomAugspurger,wesm,2013-04-10 02:56:47,2013-04-12 04:44:35,2013-04-12 04:44:35,closed,wesm,0.11,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3308,b'segfault on set_index',"b""I probably shouldn't have been doing this in the first place, but I'm able to consistently generate a segfault with:\r\n\r\n```python\r\nimport pandas as pd\r\npd.__version__ # '0.11.0.dev-809b238'\r\n\r\nd = {'t1': [2, 2.5, 3], 't2': [4, 5, 6]}\r\ndf = pd.DataFrame(d)\r\ntuples = [(0, 1), (0, 2), (1, 2)]\r\ndf['tuples'] = tuples\r\n    \r\ndf.set_index(pd.MultiIndex.from_tuples(df['tuples']))\r\n```\r\n\r\nhttps://github.com/pydata/pandas/issues/3028 is probably relevant.  I have epsilon experience with Cython so I won't really be able to help out with this one.  Happy to do more debugging if you need any though."""
3291,12952868,wesm,y-p,2013-04-09 01:45:21,2013-04-09 02:17:35,2013-04-09 02:17:14,closed,,0.11,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3291,b'Improper singleton-tuple repr',"b'```\r\nIn [3]: (5,)\r\nOut[3]: (5,)\r\n\r\nIn [4]: Series([(4,), (5,), (6,)])\r\nOut[4]: \r\n0    (4)\r\n1    (5)\r\n2    (6)\r\ndtype: object\r\n```'"
3267,12898424,jseabold,wesm,2013-04-07 17:15:20,2013-06-03 01:16:15,2013-06-03 01:16:15,closed,,0.12,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3267,b'Bug in scatter_matrix',"b'If diagonal=\'kde\', then the keyword arguments are passed to both plot and scatter. They don\'t have the same call signatures, so you can\'t do this. I guess you need separate keyword arguments. I would vote for removing **kwds from the diagonal plots entirely and just fixing those axes ex-post if you need something changed.  For example,\r\n\r\n```\r\nimport numpy as np\r\nimport pandas\r\nimport matplotlib.pyplot as plt\r\n\r\nurl = (""http://archive.ics.uci.edu/ml/machine-learning-databases""\r\n       ""/pima-indians-diabetes/pima-indians-diabetes.data"")\r\n\r\nnames = [""num_pregnant"", ""plasma_glucose"", ""blood_press"",\r\n         ""skin_fold"", ""insulin"", ""bmi"", ""pedigree"", ""age"",\r\n         ""class""]\r\n\r\npima_data = pandas.read_csv(url, header=None, names=names)\r\noutcome = pima_data.pop(""class"")\r\n```\r\n\r\nSay I want to differentiate the features by color across the class. I can pass color keyword to scatter.\r\n\r\n```\r\nclass_color = list(outcome.apply(lambda x : ""red"" if x else ""blue""))\r\nfeatures = [names[1], names[4], names[5], names[7]]\r\nX = pima_data[features]\r\n\r\npandas.scatter_matrix(X, color=class_color)\r\n```\r\n\r\nbut as soon as you try to also use \'kde\' this breaks.\r\n\r\n```\r\npandas.scatter_matrix(X, color=class_color, diagonal=\'kde\')\r\n```'"
3260,12829166,jreback,jreback,2013-04-05 01:37:19,2013-10-02 21:04:31,2013-09-21 23:55:44,closed,,0.13,2,Bug;Duplicate;Indexing;Reshaping,https://api.github.com/repos/pydata/pandas/issues/3260,b'BUG: concat if datetime index with tz',b'http://stackoverflow.com/questions/15819050/pandas-dataframe-concat-vs-append/15822811?noredirect=1#comment22510428_15822811'
3257,12796754,jayzed82,jreback,2013-04-04 11:58:29,2013-04-07 14:28:48,2013-04-07 14:28:48,closed,,0.11,2,Bug,https://api.github.com/repos/pydata/pandas/issues/3257,b'diff(0) causes ValueError on Series and TimeSeries',"b'import pandas as pd\r\nimport scipy as sp\r\n\r\ndf = pd.DataFrame(sp.randn(5,2))\r\nts = df[0]\r\n\r\n### If you run: ```df.diff(0)```\r\n\r\nOut[43]:\r\n    0  1\r\n0  0  0\r\n1  0  0\r\n2  0  0\r\n3  0  0\r\n4  0  0\r\n\r\n### This works as expeted but if you run ```ts.diff(0)``` : \r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n\r\n/home/jzunigavu/workspace/hf_vol_data/predictor_curves/<ipython console> in <module>()\r\n\r\n/home/jzunigavu/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/series.pyc in diff(self, periods)\r\n   1736         diffed : Series\r\n   1737         """"""\r\n-> 1738         result = com.diff(self.values, periods)\r\n   1739         return Series(result, self.index, name=self.name)\r\n   1740\r\n\r\n/home/jzunigavu/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/common.pyc in diff(arr, n, axis)\r\n    489         lag_indexer = tuple(lag_indexer)\r\n    490\r\n--> 491         out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\r\n    492\r\n    493     return out_arr\r\n\r\nValueError: operands could not be broadcast together with shapes (5) (0)\r\n\r\n'"
3255,12779605,wesm,jreback,2013-04-03 23:39:07,2013-09-22 00:04:36,2013-09-22 00:04:36,closed,,0.13,3,Bug;Can't Repro;Data IO,https://api.github.com/repos/pydata/pandas/issues/3255,b'Cross-platform compatibility of pickle files',b'This one is a real doozy\r\n\r\nhttp://support.picloud.com/entries/23466073-Help-please-an-UnpicklingError-coming-from-pandas-index-with-freq-A-'
3247,12735020,gjx,gjx,2013-04-03 02:43:06,2013-04-03 20:04:18,2013-04-03 20:04:18,closed,,0.11,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3247,b'Log scale on bar chart hides bars',"b""When I try use log scales on a bar plot in pandas, the area of the bar disappears. Example in [this ipynb](http://nbviewer.ipython.org/urls/gist.github.com/gjx/5298007/raw/d1a0bd774a253fa1fc1f12a005dd70a697e8f360/pandas_bar_log)\r\n\r\nI think the answer is to pass `log=True` to `bar`, as in [this SO answer](http://stackoverflow.com/questions/14047068/python-pyplot-bar-plot-bars-disapear-when-using-log-scale), but when I add `log=True` as a kwarg to `Series.plot()`, it doesn't fix the problem."""
3243,12720124,y-p,jreback,2013-04-02 19:04:41,2013-04-03 14:35:52,2013-04-03 14:35:52,closed,,0.11,1,Bug;Prio-low,https://api.github.com/repos/pydata/pandas/issues/3243,b'Dataframe ctor dies when passed a list containing empty dataframe',"b'```python\r\npd.DataFrame([DataFrame([])])\r\n\r\n/home/user1/src/pandas/pandas/core/frame.pyc in __getattr__(self, name)\r\n   1965             return self[name]\r\n   1966         raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\r\n-> 1967                              (type(self).__name__, name))\r\n   1968 \r\n   1969     def __setattr__(self, name, value):\r\n\r\nAttributeError: \'DataFrame\' object has no attribute \'dtype\'\r\n```'"
3235,12699369,stefsmeets,jreback,2013-04-02 10:36:33,2013-04-02 15:39:58,2013-04-02 12:42:58,closed,,0.11,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3235,b'assigning list to series with boolean indexing',"b'Whenever a boolean array is used to assign some list or array to another series, it returns ""ValueError: Length of replacements must equal series length"". This was working fine in pandas 10.0, but broke with 10.1\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nx = pd.Series(np.arange(10))\r\nsel = x < 5\r\nlen(x[sel])\r\nx[sel] = range(5) # doesn\'t work anymore\r\nx[0:5] = range(5) # works\r\n```'"
3233,12688411,y-p,y-p,2013-04-02 02:57:15,2013-04-03 15:06:45,2013-04-03 15:05:25,closed,,0.11,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3233,"b""grid on/off isn't consistent when plotting multiple columns vs single""","b""```python\r\ndf=mkdf(100,2,data_gen_f=lambda r,c: r+ rand(1)*5,r_idx_type='i')\r\ndf.plot()\r\n# vs\r\ndf.C_l0_g0.plot()\r\n```"""
3230,12663994,y-p,jreback,2013-04-01 15:23:10,2013-05-02 14:54:40,2013-05-02 14:54:40,closed,,0.12,2,Bug,https://api.github.com/repos/pydata/pandas/issues/3230,b'Enable applymap for dataframes with duplicate columns',"b'#2786 , #3102, #3092\r\n\r\nIn 0.11, this raises a ValueError Exception, previously returned\r\nwrong data.'"
3217,12610017,jreback,jreback,2013-03-29 16:20:41,2013-08-16 19:27:53,2013-08-16 19:27:53,closed,,0.13,1,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3217,b'BUG: change dtype of Series from non-object to object inplace',"b'see also #3216 , #3386\r\n\r\ntest in tests/test_series.py (test_update)\r\n\r\nbut cannot be fixed until Series is not longer a ndarray sub-class\r\nas numpy cannot handle the float64 to object *in-place* dtype conversion\r\n(which is why the sub-class issue comes up), if we have a single block\r\nthen can just swap out the block rather than attempting numpy magic here\r\n\r\n```\r\nIn [18]: df = pd.DataFrame([{""a"": 1}, {""a"": 3, ""b"": 2}])\r\nIn [19]: df[\'c\'] = np.nan\r\nIn [20]: df[\'c\'].update(pd.Series([\'foo\'],index=[0]))\r\nTypeError: Cannot change data-type for object array.\r\n\r\n```'"
3216,12607047,jreback,jreback,2013-03-29 14:56:00,2013-03-31 00:12:53,2013-03-31 00:12:53,closed,,0.11,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3216,b'BUG: non-numeric asignment to numeric column not upcasting',"b'http://stackoverflow.com/questions/15704274/adding-new-column-to-pandas-dataframe-with-values-for-particular-items\r\n\r\n```\r\nIn [18]: df = pd.DataFrame([{""a"": 1}, {""a"": 3, ""b"": 2}])\r\n\r\nIn [19]: df[\'c\'] = np.nan\r\nIn [21]: df.ix[0,\'c\'] = \'foo\'\r\n---------------------------------------------------------------------------\r\nValueError: could not convert string to float: f\r\n\r\n```\r\n\r\n'"
3210,12592427,wesm,wesm,2013-03-29 01:33:40,2013-04-08 04:46:30,2013-04-08 04:46:30,closed,,0.11,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3210,b'DatetimeIndex bug when adding offset to tz-aware',"b""Migrated from pydata mailing list\r\n\r\n\r\nLet me begin by saying -- I have worked out a way around the problem using .shift(). But I still thought I would mention what I ran into and see if it's worthy of attention/fixing. \r\n\r\n\r\nI create a DatetimeIndex with a timezone:\r\n\r\n```\r\nIn [29]: dates = pd.DatetimeIndex(start='2012-11-01',periods = 10, freq = 'D', tz='US/Pacific')\r\nIn [30]: dates\r\nOut[30]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2012-11-01 00:00:00, ..., 2012-11-10 00:00:00]\r\nLength: 10, Freq: D, Timezone: US/Pacific\r\n```\r\n\r\nGreat! Now I want to have all of my timestamps be at 5am. So I try: \r\n\r\n```\r\ndates + pd.offsets.Hour(5)\r\n```\r\n\r\nAnd get: \r\n\r\n```\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2012-11-01 12:00:00, ..., 2012-11-10 13:00:00]\r\nLength: 10, Freq: None, Timezone: US/Pacific \r\n```\r\n\r\nHmm. Suddenly its 12 noon -- not 5am. It looks like pandas is trying to do some UTC to US/Pacific conversion -- even though I've already specified my timezone. \r\n\r\nThis just surprised me and has the feeling of a bug. \r\n\r\n Again, I solved it when I remembered I could do .shift()\r\n\r\n```\r\nIn [32]: dates.shift(5,freq='H')\r\nOut[32]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2012-11-01 05:00:00, ..., 2012-11-10 05:00:00]\r\nLength: 10, Freq: None, Timezone: US/Pacific\r\n```"""
3201,12565979,lodagro,lodagro,2013-03-28 14:12:53,2013-04-10 06:21:15,2013-04-10 06:21:15,closed,,0.11,2,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/3201,b'line_width failure',"b""Two examples where line_width display option requirement is not met:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.11.0.dev-250c8e0'\r\n\r\nIn [3]: len(repr(pd.DataFrame(123456, range(5), range(15))).split('\\n')[1])\r\nOut[3]: 92\r\n\r\nIn [4]: pd.options.display.max_columns = 100\r\n\r\nIn [5]: len(repr(pd.DataFrame(0, range(5), range(35))).split('\\n')[1])\r\nOut[5]: 104\r\n```"""
3192,12523431,tr11,jreback,2013-03-27 17:30:51,2013-07-30 23:10:05,2013-07-30 23:10:05,closed,,0.13,2,Bug;Data IO;Dtypes;IO CSV;Prio-high,https://api.github.com/repos/pydata/pandas/issues/3192,b'Issue with CSV parser when using usecols',"b""The following code:\r\n\r\n```python\r\nimport pandas\r\nfrom io import StringIO\r\n\r\ndata = u'1,2,3\\n4,5,6\\n7,8,9'\r\ndf = pandas.read_csv(StringIO(data), \r\n                     dtype={'B': int, 'C':float},\r\n                     header=None,\r\n                     names=['A', 'B', 'C'],\r\n                     converters={'A': str},\r\n)\r\nprint(df.dtypes)\r\nprint\r\ndf = pandas.read_csv(StringIO(data), usecols=[0,2],\r\n                     dtype={'B': int, 'C':float},\r\n                     header=None,\r\n                     names=['A', 'B', 'C'],\r\n                     converters={'A': str},\r\n)\r\nprint(df.dtypes)\r\nprint\r\ndf = pandas.read_csv(StringIO(data), usecols=[0,1,2],\r\n                     dtype={'B': int, 'C':float},\r\n                     header=None,\r\n                     names=['A', 'B', 'C'],\r\n                     converters={'A': str},\r\n)\r\nprint(df.dtypes)\r\n```\r\noutputs \r\n\r\n```python\r\nA     object\r\nB      int32\r\nC    float64\r\ndtype: object\r\n\r\nA     object\r\nC    float64\r\ndtype: object\r\n\r\nA    object\r\nB    object\r\nC    object\r\ndtype: object\r\n```\r\n\r\nThe last dtype should be the same as the first one. The issue arises when passing in a `converters` dictionary together with a `usecols` that uses all the available columns.\r\n\r\nThe commit https://github.com/tr11/pandas/commit/1ac3700e72a5861a7d8544a72d77a4d64c71f118 in my fork seems to fix this issue.\r\n"""
3189,12517625,cdeil,y-p,2013-03-27 15:38:30,2013-03-27 18:40:35,2013-03-27 18:40:09,closed,,0.11,0,Bug,https://api.github.com/repos/pydata/pandas/issues/3189,"b""DataFrame.to_records(index=True) doesn't work with MultiIndex""","b'If a DataFrame has a MultiIndex, the `to_records(index=True)` method gives incorrect results.\r\n\r\nA simple example test case is shown here:\r\nhttp://nbviewer.ipython.org/5255161'"
3179,12454894,wesm,wesm,2013-03-26 15:41:54,2013-03-28 05:08:22,2013-03-28 05:08:18,closed,,0.11,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/3179,"b'KeyError from PeriodIndex.get_loc reported as integer, not Period object'",
3178,12452423,wesm,changhiskhan,2013-03-26 14:59:39,2013-03-27 16:24:55,2013-03-27 16:24:55,closed,,0.11,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3178,b'PeriodIndex.tolist should box integers as Period objects with metadata',
3171,12409690,y-p,y-p,2013-03-25 17:04:25,2013-08-18 16:20:41,2013-08-18 16:20:41,closed,,0.13,8,API Design;Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3171,"b""API: Can't override type sniffing in df.from_csv()?""","b'```python\r\nIn [10]: ix = [-352.737091, 183.575577]\r\n    ...: df=pd.DataFrame([0,1],index=ix)\r\n    ...: df.to_csv(""/tmp/1.csv"")\r\n    ...: df2=pd.DataFrame.from_csv(""/tmp/1.csv"")\r\n    ...: print df\r\n    ...: print df2\r\n             0\r\n-352.737091  0\r\n 183.575577  1\r\n                            0\r\n2105-11-21 22:43:41.128654  0\r\n1936-11-21 22:43:41.128654  1\r\n```\r\nand if you try:\r\n```\r\n\r\nIn [13]: df2.index.astype(\'f\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-8d8276604fe4> in <module>()\r\n----> 1 df2.index.astype(\'f\')\r\n\r\n/home/user1/src/pandas/pandas/tseries/index.pyc in astype(self, dtype)\r\n    624             return self.asi8.copy()\r\n    625         else:  # pragma: no cover\r\n--> 626             raise ValueError(\'Cannot cast DatetimeIndex to dtype %s\' % dtype)\r\n    627 \r\n    628     @property\r\n\r\nValueError: Cannot cast DatetimeIndex to dtype float32\r\n```\r\nrelated [SO](http://stackoverflow.com/questions/10591000/specifying-data-type-in-pandas-csv-reader)\r\n'"
3170,12409572,hayd,hayd,2013-03-25 17:01:30,2013-03-31 15:07:42,2013-03-31 15:07:42,closed,,0.13,3,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/3170,b'Stacking with MultiIndex column DataFrame throws an error',"b'Stacking this particular MultiIndex column DataFrame throws an error:\r\n\r\n```\r\nimport pandas as pd\r\nfrom StringIO import StringIO\r\n\r\ncsv = StringIO(""""""ID,NAME,DATE,VAR1\r\n1,a,03-JAN-2013,69\r\n1,a,04-JAN-2013,77\r\n1,a,05-JAN-2013,75\r\n2,b,03-JAN-2013,69\r\n2,b,04-JAN-2013,75\r\n2,b,05-JAN-2013,72"""""")\r\n\r\ndf = pd.read_csv(csv, index_col=[\'DATE\', \'ID\'], parse_dates=[\'DATE\'])\r\ndf.columns.name = \'Params\'\r\n\r\nIn [11]: df.unstack(\'ID\').resample(\'W-THU\')\r\nOut[11]: \r\nParams      VAR1      \r\nID             1     2\r\nDATE                  \r\n2013-01-03    69  69.0\r\n2013-01-10    76  73.5\r\n```\r\n\r\nIt looks like you ought to be able to stack this over `\'ID\'`, but it throws an error:\r\n\r\n```\r\nIn [12]: df.unstack(\'ID\').resample(\'W-THU\').stack(\'ID\')\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-12-e61b475b7bed> in <module>()\r\n----> 1 df.unstack(\'ID\').resample(\'W-THU\').stack(\'ID\')\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/frame.pyc in stack(self, level, dropna)\r\n   3950             return result\r\n   3951         else:\r\n-> 3952             return stack(self, level, dropna=dropna)\r\n   3953 \r\n   3954     def unstack(self, level=-1):\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/reshape.pyc in stack(frame, level, dropna)\r\n    443 \r\n    444     if isinstance(frame.columns, MultiIndex):\r\n--> 445         return _stack_multi_columns(frame, level=level, dropna=True)\r\n    446     elif isinstance(frame.index, MultiIndex):\r\n    447         new_levels = list(frame.index.levels)\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/reshape.pyc in _stack_multi_columns(frame, level, dropna)\r\n    504         # can make more efficient?\r\n    505         if loc.stop - loc.start != levsize:\r\n--> 506             chunk = this.ix[:, this.columns[loc]]\r\n    507             chunk.columns = level_vals.take(chunk.columns.labels[-1])\r\n    508             value_slice = chunk.reindex(columns=level_vals).values\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/indexing.pyc in __getitem__(self, key)\r\n     32                 pass\r\n     33 \r\n---> 34             return self._getitem_tuple(key)\r\n     35         else:\r\n     36             return self._getitem_axis(key, axis=0)\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n    222                 continue\r\n    223 \r\n--> 224             retval = retval.ix._getitem_axis(key, axis=i)\r\n    225 \r\n    226         return retval\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/indexing.pyc in _getitem_axis(self, key, axis)\r\n    340                 raise ValueError(\'Cannot index with multidimensional key\')\r\n    341 \r\n--> 342             return self._getitem_iterable(key, axis=axis)\r\n    343         elif axis == 0:\r\n    344             is_int_index = _is_integer_index(labels)\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/indexing.pyc in _getitem_iterable(self, key, axis)\r\n    406             # this is not the most robust, but...\r\n    407             if (isinstance(labels, MultiIndex) and\r\n--> 408                     not isinstance(keyarr[0], tuple)):\r\n    409                 level = 0\r\n    410             else:\r\n\r\n/Library/Python/2.7/site-packages/pandas/core/index.pyc in __getitem__(self, key)\r\n   1745             retval = []\r\n   1746             for lev, lab in zip(self.levels, self.labels):\r\n-> 1747                 if lab[key] == -1:\r\n   1748                     retval.append(np.nan)\r\n   1749                 else:\r\n\r\nIndexError: index out of bounds\r\n```\r\n\r\n*Taken from this [SO question](http://stackoverflow.com/questions/15617496/resampling-a-multi-index-dataframe).*'"
3169,12408432,hayd,hayd,2013-03-25 16:35:57,2013-03-26 18:25:35,2013-03-25 19:47:25,closed,,0.13,7,Bug,https://api.github.com/repos/pydata/pandas/issues/3169,b'Drop gives TypeError message for too few arguments when that many are given',"b'This gives an incorrect error message: ""takes **at least 2** arguments ( **2** given)"".\r\n\r\n*Note: I may be using drop wrong here, but the error message is confusing.*\r\n\r\n```\r\nimport pandas as pd\r\nfrom StringIO import StringIO\r\n\r\ncsv = StringIO(""""""ID,NAME,DATE,VAR1\r\n1,a,03-JAN-2013,69\r\n1,a,04-JAN-2013,77\r\n1,a,05-JAN-2013,75\r\n2,b,03-JAN-2013,69\r\n2,b,04-JAN-2013,75\r\n2,b,05-JAN-2013,72"""""")\r\n\r\ndf = pd.read_csv(csv, index_col=[\'DATE\', \'ID\'], parse_dates=[\'DATE\'])\r\ndf.columns.name = \'Params\'\r\n\r\nIn [11]: a = df.unstack(\'ID\').resample(\'W-THU\').unstack()\r\n\r\nIn [12]: a.drop(level=0)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-93-6ee7c2529aa0> in <module>()\r\n----> 1 a.drop(level=0)\r\n\r\nTypeError: drop() takes at least 2 arguments (2 given)\r\n\r\nIn [13]: a.drop(axis=0, level=0)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-98-081a6501fa61> in <module>()\r\n----> 1 a.drop(axis=0, level=0)\r\n\r\nTypeError: drop() takes at least 2 arguments (3 given)\r\n```\r\n\r\n*Found when messing about with this [SO question](http://stackoverflow.com/questions/15617496/resampling-a-multi-index-dataframe), which I think may also be a bug...*'"
3167,12398326,jreback,jreback,2013-03-25 12:44:47,2014-07-24 09:41:21,2013-03-25 13:11:55,closed,,0.11,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3167,b'BUG/CLN: Exception in HDFStore are now ValueError or TypeError',b'A table will now raise if min_itemsize contains fields which are not queryables'
3165,12392617,hayd,hayd,2013-03-25 09:42:28,2013-06-23 17:26:13,2013-03-25 19:46:59,closed,,0.11,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3165,b'Rename in MultiIndex flattens index',"b""When trying to rename something in a MultiIndex it doesn't work and flattens the index:\r\n\r\n```\r\ndf = pd.DataFrame({'Data1': {0: 'present', 1: 'absent'}, 'Data2': {0: 'absent', 1: 'present'}, 'Location': {0: 'loc1', 1: 'loc2'}, 'Name': {0: 'Foo', 1: 'Foo2'}, 'Position': {0: 12345, 1: 67890}}).set_index(['Name', 'Location', 'Position'])\r\n\r\nIn [2]: df\r\nOut[2]: \r\n                          Data1    Data2\r\nName Location Position                  \r\nFoo  loc1     12345     present   absent\r\nFoo2 loc2     67890      absent  present\r\n\r\nIn [3]: df.rename({'Foo2': 'Bar'})\r\nOut[3]: \r\n                       Data1    Data2\r\n(Foo, loc1, 12345)   present   absent\r\n(Foo2, loc2, 67890)   absent  present\r\n\r\nIn [4]: df.rename_axis({'Foo2': 'Bar'}, axis=1)\r\nOut[4]: \r\n                       Data1    Data2\r\n(Foo, loc1, 12345)   present   absent\r\n(Foo2, loc2, 67890)   absent  present\r\n```\r\n\r\n*Taken from [this SO question](http://stackoverflow.com/questions/15610819/renaming-one-item-of-a-level-in-pandas-multiindex).*"""
3163,12384617,gdraps,jreback,2013-03-25 01:57:36,2014-01-24 14:35:14,2013-03-25 12:45:52,closed,,0.11,14,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3163,b'to_csv() fail on 0.11.dev',"b'Hit this after updating to \'0.11.0.dev-da54321\' from master.  Haven\'t had a chance to dig any deeper, other than isolate frame length as a factor.\r\n\r\n    df = pandas.util.testing.makeTimeDataFrame(25000)\r\n    df.to_csv(""save.csv"")  # works\r\n    df = pandas.util.testing.makeTimeDataFrame(25001)\r\n    df.to_csv(""save.csv"")  # throws exception below\r\n\r\n    ---------------------------------------------------------------------------\r\n    IndexError                                Traceback (most recent call last)\r\n    <ipython-input-83-12cc25e3eafd> in <module>()\r\n    ----> 1 df.to_csv(""save.csv"")\r\n\r\n    /usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_da54321-py2.7-linux-i686.egg/pandas/core/frame.pyc in to_csv(self, path_or_buf, sep, na_rep, float_format, cols, header, index, index_label, mode, nanRep, encoding, quoting, line_terminator, chunksize, **kwds)\r\n       1348                                          index_label=index_label,\r\n       1349                                          chunksize=chunksize,legacy=kwds.get(""legacy"",False) )\r\n    -> 1350             formatter.save()\r\n       1351 \r\n       1352     def to_excel(self, excel_writer, sheet_name=\'sheet1\', na_rep=\'\',\r\n\r\n    /usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_da54321-py2.7-linux-i686.egg/pandas/core/format.pyc in save(self)\r\n        936 \r\n        937             else:\r\n    --> 938                 self._save()\r\n        939 \r\n        940 \r\n\r\n    /usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_da54321-py2.7-linux-i686.egg/pandas/core/format.pyc in _save(self)\r\n       1008                 break\r\n       1009 \r\n    -> 1010             self._save_chunk(start_i, end_i)\r\n       1011 \r\n       1012     def _save_chunk(self, start_i, end_i):\r\n\r\n    /usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_da54321-py2.7-linux-i686.egg/pandas/core/format.pyc in _save_chunk(self, start_i, end_i)\r\n       1029         ix = data_index.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)\r\n       1030 \r\n    -> 1031         lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)\r\n       1032 \r\n       1033 # from collections import namedtuple\r\n\r\n    /usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_da54321-py2.7-linux-i686.egg/pandas/lib.so in pandas.lib.write_csv_rows (pandas/lib.c:13152)()\r\n\r\n    IndexError: list index out of range\r\n\r\n\r\nCurrent workaround:\r\n\r\n    df.to_csv(""save.csv"", legacy=True)\r\n'"
3155,12369793,iamlemec,y-p,2013-03-24 05:21:07,2013-03-26 11:01:55,2013-03-26 11:01:55,closed,,0.11,3,Bug,https://api.github.com/repos/pydata/pandas/issues/3155,"b""rolling_corr not in [-1,1] when using 'center' flag""","b""The function ```rolling_corr``` returns values with absolute value greater than one when using the 'center' flag. The following code will reproduce (with high probability):\r\n\r\n```\r\ndf = pandas.DataFrame(numpy.random.rand(30,2))\r\npandas.rolling_corr(df[0],df[1],5,center=True)\r\n```\r\n\r\nIt seems like passing center to ```rolling_count``` and ```rolling_mean``` in ```rolling_corr``` would be in order?\r\n\r\nAlso, unrelated to this, calling ```rolling_corr(5,6,10)``` will hit the recursion limit because ```_flex_binary_moment``` keeps switching the first two arguments around."""
3152,12365632,jreback,jreback,2013-03-23 22:09:03,2014-06-19 17:29:39,2013-03-24 03:07:37,closed,,0.11,0,Bug;Dtypes;Groupby,https://api.github.com/repos/pydata/pandas/issues/3152,b'BUG: GH2763 fixed downcasting of groupby results on SeriesGroupBy',b'closes #2763\r\ne.g. was returning float64 on int64 input when possible to preserve\r\n      in first/last\r\n\r\nmore general cases were fixed in prior PRs (e.g. #3041)'
3139,12330822,jreback,jreback,2013-03-22 18:09:16,2014-06-27 14:00:48,2013-03-22 18:53:50,closed,,0.11,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3139,b'BUG: GH2745 Fix issue with indexing a series with a boolean key a 1-len list on rhs',b'closes #2745'
3137,12321823,jreback,jreback,2013-03-22 14:44:36,2014-07-10 09:20:30,2013-03-22 16:20:31,closed,,0.11,0,Bug;Enhancement;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3137,"b'ENH: GH3070 allow string selection on a DataFrame with a datelike index, to have partial_string semantics (like Series)'",b'closes #3070'
3134,12319251,snth,jtratner,2013-03-22 13:42:55,2013-09-08 03:27:22,2013-09-08 03:27:22,closed,,0.13,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3134,b'groupby() syntactic sugar not working correctly in DataFrame form',"b'In Wes\' ""Python for Data Analysis"" book it says on the bottom of page 256 that:\r\n\r\n```python\r\n    df.groupby(\'key1\')[\'data1\']\r\n    df.groupby(\'key1\')[[\'data2\']]\r\n```\r\n\r\nare syntactic sugar for:\r\n\r\n```python\r\n    df[\'data1\'].groupby(df[\'key1\'])\r\n    df[[\'data2\']].groupby(df[\'key1\'])\r\n```\r\n\r\nI find that the first form works but the second one doesn\'t, in particular the\r\nrestriction on the data columns doesn\'t occurr in the second form. Here is some\r\nexample code:\r\n\r\n```python\r\nIn [33]: import pandas as pd\r\n\r\nIn [34]: df = pd.DataFrame(dict(key1=[0]*3+[1]*3, data1=range(6), data2=range(3)*2))\r\n\r\nIn [35]: print df\r\n   data1  data2  key1\r\n0      0      0     0\r\n1      1      1     0\r\n2      2      2     0\r\n3      3      0     1\r\n4      4      1     1\r\n5      5      2     1\r\n\r\nIn [36]: expected = df[\'data1\'].groupby(df[\'key1\']).count()\r\n\r\nIn [37]: print expected\r\nkey1\r\n0       3\r\n1       3\r\n\r\nIn [38]: actual = df.groupby(\'key1\')[\'data1\'].count()\r\n\r\nIn [39]: print actual\r\nkey1\r\n0       3\r\n1       3\r\n\r\nIn [40]: expected2 = df[[\'data2\']].groupby(df[\'key1\']).count()\r\n\r\nIn [41]: print expected2\r\n      data2\r\nkey1       \r\n0         3\r\n1         3\r\n\r\nIn [42]: actual2 = df.groupby(\'key1\')[[\'data2\']].count()\r\n\r\nIn [43]: print actual2\r\n      data1  data2  key1\r\nkey1                    \r\n0         3      3     3\r\n1         3      3     3\r\n\r\nIn [44]: print pd.__version__\r\n0.10.1\r\n```'"
3133,12302400,jreback,jreback,2013-03-22 00:51:56,2014-06-16 09:11:54,2013-03-22 14:33:05,closed,,0.11,6,Bug;Enhancement;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3133,b'ENH: extend selection semantics on ordered timeseries to unordered',b'closes #2437\r\ncloses #2787\r\ncloses #3070'
3127,12287459,jreback,jreback,2013-03-21 18:08:15,2014-06-20 13:45:44,2013-03-21 19:46:46,closed,,0.11,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3127,"b'BUG: GH2719, fixed reshape on a Series with invalid input'",b'fixes #2719'
3126,12286612,jreback,jreback,2013-03-21 17:50:03,2013-03-21 19:37:21,2013-03-21 19:37:21,closed,,0.11,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3126,"b'BUG: HDFStore bug when appending to a table, .typ not recreated on subsequent appends'","b'not correctctly recreating string columns so that could test the min_itemsize if its passed on subsequent appends, see:\r\n\r\nhttp://stackoverflow.com/questions/15488809/how-to-trouble-shoot-hdfstore-exception-cannot-find-the-correct-atom-type?noredirect=1#comment22032050_15488809'"
3125,12282741,jreback,jreback,2013-03-21 16:27:21,2014-06-30 09:18:29,2013-03-22 17:34:59,closed,,0.11,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3125,b'BUG: GH2903 implemented xs for axis=1 with a level specified',b'closes #2903'
3124,12280590,jreback,jreback,2013-03-21 15:45:44,2014-08-12 14:13:58,2013-03-22 16:33:55,closed,,0.11,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3124,b'BUG: GH2817 raise the correct KeyError that the multi-index is not sorted',b'closes #2817'
3122,12277107,lbeltrame,jreback,2013-03-21 14:32:52,2013-10-14 01:52:20,2013-10-14 01:52:20,closed,jtratner,0.13,19,Bug;Data IO;Dtypes;IO Excel,https://api.github.com/repos/pydata/pandas/issues/3122,b'ExcelWriter fails with columns with int32 dtype',"b'Test case\r\n\r\n````python\r\n\r\nIn [21]: df = pandas.DataFrame({""foo"": [1,2,3]})\r\n\r\nIn [22]: df[""foo""] = df[""foo""].astype(""int32"")\r\n\r\nIn [23]: df.to_excel(""foo.xls"")\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-23-a8d956752923> in <module>()\r\n----> 1 df.to_excel(""foo.xls"")\r\n\r\n/usr/lib64/python2.7/site-packages/pandas-0.11.0.dev_4e0465d-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in to_excel(self, excel_writer, sheet_name, na_rep, float_format, cols, header, index, index_label, startrow, startcol)\r\n   1406         formatted_cells = formatter.get_formatted_cells()\r\n   1407         excel_writer.write_cells(formatted_cells, sheet_name,\r\n-> 1408                                  startrow=startrow, startcol=startcol)\r\n   1409         if need_save:\r\n   1410             excel_writer.save()\r\n\r\n/usr/lib64/python2.7/site-packages/pandas-0.11.0.dev_4e0465d-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in write_cells(self, cells, sheet_name, startrow, startcol)\r\n   2194             self._writecells_xlsx(cells, sheet_name, startrow, startcol)\r\n   2195         else:\r\n-> 2196             self._writecells_xls(cells, sheet_name, startrow, startcol)\r\n   2197\r\n   2198     def _writecells_xlsx(self, cells, sheet_name, startrow, startcol):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas-0.11.0.dev_4e0465d-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in _writecells_xls(self, cells, sheet_name, startrow, startcol)\r\n   2270                 wks.write(startrow + cell.row,\r\n   2271                           startcol + cell.col,\r\n-> 2272                           val, style)\r\n\r\n/usr/lib/python2.7/site-packages/xlwt/Worksheet.pyc in write(self, r, c, label, style)\r\n   1030\r\n   1031     def write(self, r, c, label="""", style=Style.default_style):\r\n-> 1032         self.row(r).write(c, label, style)\r\n   1033\r\n   1034     def write_rich_text(self, r, c, rich_text_list, style=Style.default_style):\r\n\r\n/usr/lib/python2.7/site-packages/xlwt/Row.pyc in write(self, col, label, style)\r\n    257             self.__rich_text_helper(col, label, style, style_index)\r\n    258         else:\r\n--> 259             raise Exception(""Unexpected data type %r"" % type(label))\r\n    260\r\n    261     def set_cell_rich_text(self, col, rich_text_list, style=Style.default_style):\r\n\r\nException: Unexpected data type <type \'numpy.int32\'>\r\n\r\n````'"
3121,12276960,michael1e23,hayd,2013-03-21 14:29:40,2013-08-06 15:11:39,2013-08-06 15:11:39,closed,,0.13,2,Bug,https://api.github.com/repos/pydata/pandas/issues/3121,b'appending empty dataframe with columns fails',"b""I am trying to append an empty dataframe to another empty one. This fails if at least one of them has a column defined. Below is the code snippet to generate the error.\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    pd.__version__\r\n    #'0.10.1'\r\n    np.__version__\r\n    #'1.7.0'\r\n\r\n    a = pd.DataFrame()\r\n    b = pd.DataFrame()\r\n    a.append(b) # works\r\n\r\n    a = pd.DataFrame(columns=['asdf'])\r\n    b = pd.DataFrame(columns=['asdf'])\r\n    a.append(b) # fails\r\n\r\n    a = pd.DataFrame(columns=['asdf'])\r\n    b = pd.DataFrame()\r\n    a.append(b) # fails\r\n\r\n    a = pd.DataFrame()\r\n    b = pd.DataFrame(columns=['asdf'])\r\n    a.append(b) # fails\r\n\r\nException is: ValueError: need at least one array to concatenate\r\n\r\n"""
3110,12253792,jreback,jreback,2013-03-20 23:30:00,2014-06-22 12:52:31,2013-03-26 14:55:26,closed,,0.11,12,Bug;Enhancement;Indexing,https://api.github.com/repos/pydata/pandas/issues/3110,"b""BUG: GH3109 fixed issues where passing an axis of 'columns' would fail""","b""BUG: some operations were expecting an axis number, this fix allows passing the regular axis name as well, e.g. \r\n``df.sum(axis='columns')`` will now work\r\n\r\n*note: narrowed the scope of this PR to just a bug fix*\r\n\r\n```\r\nIn [3]: df = pd.DataFrame(np.random.rand(5,2),columns=['A','B'])\r\n\r\nIn [4]: df.sum(axis=1)\r\nOut[4]: \r\n0    1.150325\r\n1    0.789142\r\n2    0.581486\r\n3    0.864212\r\n4    0.731511\r\n\r\nIn [5]: df.sum(axis='columns')\r\n---------------------------------------------------------------------------\r\n\r\nException: Must have 0<= axis <= 1\r\n\r\n```"""
3109,12247761,jreback,jreback,2013-03-20 20:50:37,2013-03-21 15:14:26,2013-03-21 15:14:26,closed,,0.11,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3109,b'BUG/ENH: allow axis name aliasing',"b""http://stackoverflow.com/questions/15533093/pandas-access-axis-by-user-defined-name/15534409#15534409\r\n\r\nnot that this is user friendly, but\r\n\r\n```\r\npd.DataFrame._AXIS_ALIASES['myaxis'1] = 0\r\npd.DataFrame._AXIS_ALIASES['myaxis2'] = 1\r\n\r\ndf.sum(axis='myaxis1')\r\n```\r\n\r\nshould work but doesn't"""
3108,12244405,patricktokeeffe,wesm,2013-03-20 19:34:45,2013-04-12 04:45:25,2013-04-12 04:45:25,closed,,0.11,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/3108,b'Plotting Series/DF of dtype object fails intermittently',"b'There seems to be an intermittent issue which prevents plotting Series/DataFrames with dtype `object`. See traceback below. This manifests for me when reading data from files but not when manually creating dataframes from the command line. It can be worked around by explicitly casting to float64. The presence of NANs does not appear to have an effect.\r\n\r\n````python\r\n>>> df = pd.read_csv(\'data.csv\')  # CSV data file with some ""NAN"" entries\r\n>>> df[\'chan1\'].dtype\r\ndtype(\'object\')\r\n\r\n>>> df[\'chan1\'].plot()  # fails\r\n>>> df[\'chan1\'].dropna().plot()  # fails\r\n>>> df[\'chan1\'].apply(float).plot()  # works\r\n>>> df[\'chan1\'].dropna().apply(float).plot()  # works\r\n````\r\n\r\nThis works though:\r\n\r\n````python\r\ndf = pd.DataFrame([3, 1, 4, np.nan, 5, 9], dtype=object)  # works\r\ndf = pd.DataFrame([3, 1, 4, ""NAN"", 5, 9], dtype=object)  # works\r\ndf = pd.DataFrame([3, 1, 4, 1, 5, 9], dtype=object)  # works\r\n````\r\n\r\nHere is the traceback; it is the same as encountered in #478 and presumably in #1818. Note it doesn\'t include the error message added in 9186cbc\r\n\r\n````python\r\n>>> df = pd.read_csv(r\'C:\\test\\stats.dat\')\r\n>>> df[\'Ts_Avg\'].plot()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1534, in plot_series\r\n    plot_obj.generate()\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 735, in generate\r\n    self._make_plot()\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1106, in _make_plot\r\n    newline = plotf(*args, **kwds)[0]\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\matplotlib\\axes.py"", line 3996, in plot\r\n    for line in self._get_lines(*args, **kwargs):\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\matplotlib\\axes.py"", line 330, in _grab_next_args\r\n    for seg in self._plot_args(remaining, kwargs):\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\matplotlib\\axes.py"", line 289, in _plot_args\r\n    linestyle, marker, color = _process_plot_format(tup[-1])\r\n  File ""C:\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\matplotlib\\axes.py"", line 96, in _process_plot_format\r\n    if fmt.find(\'--\')>=0:\r\nAttributeError: \'numpy.ndarray\' object has no attribute \'find\'\r\n````\r\n'"
3106,12239972,jreback,jreback,2013-03-20 17:57:04,2013-03-20 18:27:10,2013-03-20 18:27:10,closed,jreback,0.13,0,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/3106,b'BUG: Multiple timedetal64[ns] operations causes changes in dtypes',"b""ok\r\n```\r\nIn [34]: df = pd.DataFrame({ 'time' : pd.date_range('20130102',periods=5), \r\n                                         'time2' : pd.date_range('20130105',periods=5) })\r\n\r\nIn [35]: df['off1'] = df['time2']-df['time']\r\n\r\nIn [36]: df.dtypes\r\nOut[36]: \r\ntime      datetime64[ns]\r\ntime2     datetime64[ns]\r\noff1     timedelta64[ns]\r\ndtype: object\r\n```\r\n\r\ndtypes of both existing are changed to ``us``\r\n```\r\nIn [37]: df['off2'] = df['time']-df['time2']\r\n\r\nIn [38]: df.dtypes\r\nOut[38]: \r\ntime      datetime64[ns]\r\ntime2     datetime64[ns]\r\noff1     timedelta64[us]\r\noff2     timedelta64[us]\r\ndtype: object\r\n```"""
3105,12235564,wesm,jreback,2013-03-20 16:26:39,2013-03-25 19:48:58,2013-03-25 19:48:58,closed,,0.11,5,Bug,https://api.github.com/repos/pydata/pandas/issues/3105,b'DataFrame constructor error unhelpful',"b'```\r\npd.DataFrame(np.arange(12).reshape((4, 3)), columns=[\'foo\', \'bar\', \'baz\'],\r\n             index=pd.date_range(\'2000-01-01\', periods=3))\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-80-77df75afb13d> in <module>()\r\n      1 pd.DataFrame(np.arange(12).reshape((4, 3)), columns=[\'foo\', \'bar\', \'baz\'],\r\n----> 2              index=pd.date_range(\'2000-01-01\', periods=3))\r\n\r\n/Users/wesm/code/pandas/pandas/core/frame.pyc in __init__(self, data, index, columns, dtype, copy)\r\n    408             else:\r\n    409                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n--> 410                                          copy=copy)\r\n    411         elif isinstance(data, list):\r\n    412             if len(data) > 0:\r\n\r\n/Users/wesm/code/pandas/pandas/core/frame.pyc in _init_ndarray(self, values, index, columns, dtype, copy)\r\n    555 \r\n    556         block = make_block(values.T, columns, columns)\r\n--> 557         return BlockManager([block], [columns, index])\r\n    558 \r\n    559     def _wrap_array(self, arr, axes, copy=False):\r\n\r\n/Users/wesm/code/pandas/pandas/core/internals.pyc in __init__(self, blocks, axes, do_integrity_check)\r\n    826 \r\n    827         if do_integrity_check:\r\n--> 828             self._verify_integrity()\r\n    829 \r\n    830         self._consolidate_check()\r\n\r\n/Users/wesm/code/pandas/pandas/core/internals.pyc in _verify_integrity(self)\r\n    916                                      ""items"")\r\n    917             if block.values.shape[1:] != mgr_shape[1:]:\r\n--> 918                 raise AssertionError(\'Block shape incompatible with manager\')\r\n    919         tot_items = sum(len(x.items) for x in self.blocks)\r\n    920         if len(self.items) != tot_items:\r\n\r\nAssertionError: Block shape incompatible with manager\r\n```'"
3102,12210717,y-p,y-p,2013-03-20 01:58:30,2014-06-15 04:34:20,2013-04-01 15:23:25,closed,,0.13,30,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/3102,"b'#2786: applymap fails with dupe columns, ObjectBlock convert() method bombs'","b'@jreback, does this look ok? travis is still running.\r\n\r\n#2786'"
3094,12196232,bluefir,jreback,2013-03-19 19:00:26,2013-03-23 17:38:05,2013-03-23 17:38:05,closed,,0.11,14,Bug,https://api.github.com/repos/pydata/pandas/issues/3094,b'0.11.0.dev-4b22372.win-amd64-py2.7: nosetests error',"b'ERROR: test_operators_timedelta64 (pandas.tests.test_series.TestSeries)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tests\\test_series.py"", line 1752, i\r\nn test_operators_timedelta64\r\n    result = resultb + df[\'A\']\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 144, in wrapp\r\ner\r\n    lvalues = lvalues.values\r\nAttributeError: \'numpy.ndarray\' object has no attribute \'values\'\r\n\r\n----------------------------------------------------------------------\r\nRan 3113 tests in 241.670s\r\n\r\nFAILED (SKIP=94, errors=1)'"
3089,12186173,jreback,jreback,2013-03-19 15:42:34,2013-03-21 21:32:55,2013-03-21 11:37:11,closed,,0.11,32,Bug;Performance,https://api.github.com/repos/pydata/pandas/issues/3089,b'PERF: regression from 0.10.1',b'f6ac8c091dfb2bbbbb02a34c13022c5427b290eb to 05a737dbb9f857e25a211739d90c42f3d9428b8e\r\n\r\nBad news:\r\n\r\n```\r\nResults:\r\n                                            t_head  t_baseline      ratio\r\nname                                                                     \r\nreindex_frame_level_reindex                 1.1343      0.7571     1.4982\r\nreindex_frame_level_align                   1.1690      0.7548     1.5488\r\n```\r\n\r\nGood news:\r\n\r\n```\r\nResults:\r\n                                            t_head  t_baseline      ratio\r\nname                                                                     \r\nframe_get_dtype_counts                      0.0968    208.6380     0.0005\r\nframe_wide_repr                             0.5321    207.0730     0.0026\r\ngroupby_first_float32                       3.0386    336.2439     0.0090\r\ngroupby_last_float32                        3.2206    335.2571     0.0096\r\nframe_to_csv2                             181.1550   2188.9260     0.0828\r\nindexing_dataframe_boolean                 12.9179    125.7350     0.1027\r\nwrite_csv_standard                         36.3702    230.9210     0.1575\r\nframe_reindex_axis0                         0.3122      1.0984     0.2842\r\nframe_to_csv_mixed                        340.1070   1091.9340     0.3115\r\nframe_to_csv                              106.5671    219.1830     0.4862\r\nframe_add                                  22.0693     34.1867     0.6456\r\nframe_mult                                 22.0064     33.9859     0.6475\r\nframe_reindex_upcast                       11.4179     17.3291     0.6589\r\nframe_fancy_lookup_all                     14.3931     20.5621     0.7000\r\nseries_string_vector_slice                148.3371    195.5922     0.7584\r\n```\r\n???\r\n```\r\nframe_reindex_axis1                         2.3612      2.3386     1.0097\r\n```\r\n\r\nFixed:\r\n```\r\nindexing_dataframe_boolean_rows             0.2180      0.1953     1.1164\r\n```'
3083,12148522,changhiskhan,jreback,2013-03-18 18:59:46,2015-01-26 00:45:21,2015-01-26 00:45:21,closed,,0.16.0,7,Bug;Docs;Indexing;MultiIndex,https://api.github.com/repos/pydata/pandas/issues/3083,b'Integer partial slicing on multiindex bug',"b'similar to one encountered in @wesm pydata 2013 tutorial\r\n\r\n```\r\nIn [9]: import pandas as pd\r\n\r\nIn [10]: idx = pd.MultiIndex.from_arrays([[2011,2011,2012,2012,2013], [1,2,1,2,1]])\r\n\r\nIn [11]: ser = pd.Series(randn(len(idx)), idx)\r\n\r\nIn [12]: ser\r\nOut[12]:\r\n2011  1   -0.485310\r\n      2   -0.679528\r\n2012  1    1.089995\r\n      2   -0.174895\r\n2013  1   -1.219255\r\ndtype: float64\r\n\r\nIn [13]: ser.ix[2011:2012]\r\nOut[13]:\r\n2011  1   -0.485310\r\n      2   -0.679528\r\n2012  1    1.089995\r\n      2   -0.174895\r\ndtype: float64\r\n\r\nIn [14]: ser[2011:2012]\r\nOut[14]: Series([], dtype: float64)\r\n```'"
3082,12141438,wesm,wesm,2013-03-18 16:26:32,2013-03-18 16:33:53,2013-03-18 16:33:53,closed,,,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3082,b'Daily data resampling default behavior',"b'from @dhirschfeld\r\n\r\nI just ran across the bug described in issue #3020 (https://github.com/pydata/pandas/issues/3020) as our unit-tests started failing after upgrading pandas.\r\n\r\nI did a git bisection and found the first bad commit to be 9deda9e6:\r\n\r\n```\r\n9deda9e687915017cc9db7698504ead1f89ead9e is the first bad commit\r\ncommit 9deda9e687915017cc9db7698504ead1f89ead9e\r\nAuthor: Chang She\r\nDate:   Sun Dec 2 02:21:10 2012 -0500\r\n\r\n    API: change resample to infer correct closed and label value by default #2363\r\n\r\n:040000 040000 3abca1d3eb70bcd6ade31581acc4684a3035b81e 4cdeae975685b996ef2f41c738ca70bfd41ca583 M  pandas\r\n```\r\n\r\nI think this is a fairly serious regression as (apart from causing our unit-tests to fail) it will silently give incorrect answers for an aggregation if the data has been resampled beforehand.\r\n\r\nThe following unit-test demonstrates the problem:\r\n\r\ndef test_resample_doesnt_truncate():\r\n    """"""Test for issue #3020""""""\r\n    import pandas as pd\r\n    dates = pd.date_range(\'01-Jan-2014\',\'05-Jan-2014\', freq=\'D\')\r\n    series = pd.TimeSeries(1, index=dates)\r\n    series = series.resample(\'D\')\r\n    assert series.index[0] == dates[0]\r\n#'"
3071,12103696,taavi,wesm,2013-03-17 02:11:47,2013-03-18 16:32:44,2013-03-17 03:11:33,closed,,0.11,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3071,b'read_csv(date_parser=x) fails because datetime.datetime has no dtype',"b""```python\r\nfrom cStringIO import StringIO\r\nlog_file = StringIO(\r\n    'posix_timestamp,elapsed,sys,user,queries,query_time,rows,'\r\n        'accountid,userid,contactid,level,silo,method\\n'\r\n    '1343103150,0.062353,0,4,6,0.01690,3,'\r\n        '12345,1,-1,3,invoice_InvoiceResource,search\\n'\r\n)\r\n\r\nimport pandas as pd\r\nfrom datetime import datetime\r\nimport gc\r\n\r\ndef posix_string_to_datetime(posix_string):\r\n    return datetime.utcfromtimestamp(int(posix_string))\r\n\r\n# This works on pandas 0.9.0, but not on 0.10.1 or github master\r\ndf = pd.io.parsers.read_csv(\r\n    log_file,\r\n    # index_col is the first column, our posix_timestamp\r\n    index_col=0,\r\n    # Interpret the index column as a date\r\n    parse_dates=0,\r\n    date_parser=posix_string_to_datetime)\r\n```\r\n\r\nThe crash looks like this (on 0.11.0.dev-6e7b37b, OSX 10.6 if it matters):\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-c3fa4840399b> in <module>()\r\n     17     # Interpret the index column as a date\r\n     18     parse_dates=0,\r\n---> 19     date_parser=posix_string_to_datetime)\r\n     20 gc.enable()\r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\r\n    398                     buffer_lines=buffer_lines)\r\n    399 \r\n--> 400         return _read(filepath_or_buffer, kwds)\r\n    401 \r\n    402     parser_f.__name__ = name\r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)\r\n    213         return parser\r\n    214 \r\n--> 215     return parser.read()\r\n    216 \r\n    217 _parser_defaults = {\r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in read(self, nrows)\r\n    630             #     self._engine.set_error_bad_lines(False)\r\n    631 \r\n--> 632         ret = self._engine.read(nrows)\r\n    633 \r\n    634         if self.options.get('as_recarray'):\r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in read(self, nrows)\r\n   1006 \r\n   1007             names, data = self._do_date_conversions(names, data)\r\n-> 1008             index = self._make_index(data, alldata, names)\r\n   1009 \r\n   1010         return index, names, data\r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in _make_index(self, data, alldata, columns)\r\n    706         elif not self._has_complex_date_col:\r\n    707             index = self._get_simple_index(alldata, columns)\r\n--> 708             index = self._agg_index(index)\r\n    709 \r\n    710         elif self._has_complex_date_col:\r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in _agg_index(self, index, try_parse_dates)\r\n    789                                                    self.na_values)\r\n    790 \r\n--> 791             arr, _ = self._convert_types(arr, col_na_values)\r\n    792             arrays.append(arr)\r\n    793 \r\n\r\n/Users/taavi/src/pandas/pandas/io/parsers.pyc in _convert_types(self, values, na_values, try_num_bool)\r\n    815     def _convert_types(self, values, na_values, try_num_bool=True):\r\n    816         na_count = 0\r\n--> 817         if issubclass(values.dtype.type, (np.number, np.bool_)):\r\n    818             mask = lib.ismember(values, na_values)\r\n    819             na_count = mask.sum()\r\n\r\nAttributeError: 'datetime.datetime' object has no attribute 'dtype'\r\n```\r\n\r\nThanks!"""
3047,12004240,simomo,simomo,2013-03-14 03:13:54,2013-03-20 12:40:35,2013-03-15 12:59:04,closed,,0.11,10,Bug,https://api.github.com/repos/pydata/pandas/issues/3047,"b""'datetime64[ns]' columns cause fillna function fail""","b"" * pd.version: 0.11.0.dev-156e03c\r\n * if dataframe had a column which had been recognized as 'datetime64[ns]' type, calling the dataframe's fillna function would cause an expection.\r\n\r\n```python\r\nIn [149]: df_t1 = DataFrame({'int': [1, 2, None], 'date_obj': [datetime.datetime(2012, 1, 1), datetime.datetime(2012, 1, 1), None], 'date_date':[datetime.datetime(2012, 1, 1), datetime.datetime(2012, 1, 1), datetime.datetime(2012, 1, 1)]})\r\n\r\nIn [150]: df_t1.dtypes\r\nOut[150]:\r\ndate_date    datetime64[ns]\r\ndate_obj             object\r\nint                 float64\r\ndtype: object\r\n\r\nIn [151]: df_t1.fillna(np.nan)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-151-782ffd0df53e> in <module>()\r\n----> 1 df_t1.fillna(np.nan)\r\n\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.11.0.dev_156e03c-py2.6-linux-x86_64.egg/pandas/core/frame.pyc in fillna(self, value, method, axis, inplace, limit)\r\n   3385                 return result\r\n   3386             else:\r\n-> 3387                 new_data = self._data.fillna(value, inplace=inplace)\r\n   3388 \r\n   3389         if inplace:\r\n\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.11.0.dev_156e03c-py2.6-linux-x86_64.egg/pandas/core/internals.pyc in fillna(self, *args, **kwargs)\r\n    944 \r\n    945     def fillna(self, *args, **kwargs):\r\n--> 946         return self.apply('fillna', *args, **kwargs)\r\n    947 \r\n    948     def astype(self, *args, **kwargs):\r\n\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.11.0.dev_156e03c-py2.6-linux-x86_64.egg/pandas/core/internals.pyc in apply(self, f, *args, **kwargs)\r\n    915                 applied = f(blk, *args, **kwargs)\r\n    916             else:\r\n--> 917                 applied = getattr(blk,f)(*args, **kwargs)\r\n    918 \r\n    919             if isinstance(applied,list):\r\n\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.11.0.dev_156e03c-py2.6-linux-x86_64.egg/pandas/core/internals.pyc in fillna(self, value, inplace)\r\n    212         new_values = self.values if inplace else self.values.copy()\r\n    213         mask = com.isnull(new_values)\r\n--> 214         np.putmask(new_values, mask, value)\r\n    215 \r\n    216         if inplace:\r\n\r\nValueError: Must be a datetime.date or datetime.datetime object\r\n\r\nIn [153]: del df_t1['date_date']\r\n\r\nIn [155]: df_t1.fillna(np.nan)\r\nOut[155]:\r\ndate_obj\tint\r\n0\t 2012-01-01 00:00:00\t 1\r\n1\t 2012-01-01 00:00:00\t 2\r\n2\t NaN\tNaN\r\n```\r\n * and i tried to convert the 'datetime64[ns]' column's type to object, but i failed\r\n\r\n```python\r\nIn [162]: df_t1.date_date = df_t1.date_date.astype(df_t1.date_obj.dtype)\r\nIn [164]: df_t1.dtypes\r\nOut[164]:\r\ndate_date    datetime64[ns]\r\ndate_obj             object\r\nint                 float64\r\ndtype: object\r\n```"""
3042,11998961,michaelaye,jreback,2013-03-13 23:39:00,2013-03-22 17:37:10,2013-03-22 17:37:10,closed,,0.11,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3042,b'Timestamp.fromordinal Exception AttributeError',"b'I have a datetime.datetime, convert it to ordinal and try to read it back with pd.Timestamp.fromordinal(). This produces the following error:\r\n\r\n````python\r\nIn [112]: dt\r\nOut[112]: datetime.datetime(2011, 4, 16, 0, 0)\r\n\r\nIn [113]: pd.Timestamp.fromordinal(dt.toordinal())\r\nException AttributeError: ""\'int\' object has no attribute \'utcoffset\'"" in \'pandas.tslib._localize_tso\' ignored\r\nOut[113]: <Timestamp: 1970-01-01 00:00:00.000002011>\r\n````\r\n\r\nAm I expecting something wrong by thinking this should work or is it a bug?\r\n'"
3035,11967877,dhirschfeld,y-p,2013-03-13 10:43:07,2013-03-17 15:33:15,2013-03-17 15:33:15,closed,,0.11,2,Bug,https://api.github.com/repos/pydata/pandas/issues/3035,b'BUG: groupby segfaults when passed a function of a timestamp which raises a TypeError',"b""It *is* a dumb thing to do, but pandas probably shouldn't segfault regardless:\r\n\r\n```python\r\nIn [1]: dates = pd.date_range('01-Jan-2013', periods=12, freq='MS')\r\n   ...: ts = pd.TimeSeries(randn(12), index=dates)\r\n\r\nIn [2]: ts.groupby(lambda key: key[0:4]).first()\r\nIt seems the kernel died unexpectedly. Use 'Restart kernel' to continue using this console.\r\n```\r\n\r\nTested with pandas 10.1 on  win32/win64,  Python 2.7 and pandas 0.11.0.dev-2f7b0e4 on win32, Python 2.7."""
3022,11928182,dhirschfeld,wesm,2013-03-12 15:02:43,2013-03-12 15:20:22,2013-03-12 15:20:18,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/3022,b'BUG: `concat` gives incorrect results for series with the same name',"b""As shown in the example below, when passing in two series with the same name to `concat` you end up with duplicates of the last series passed in. \r\n\r\n```python\r\nIn [42]: pd.__version__\r\nOut[42]: '0.10.1'\r\n\r\nIn [43]: dates = pd.date_range('01-Jan-2013','01-Dec-2013', freq='MS')\r\n    ...: L = pd.TimeSeries('L', dates, name='LeftRight')\r\n    ...: R = pd.TimeSeries('R', dates, name='LeftRight')\r\n    ...: \r\n\r\nIn [44]: L\r\nOut[44]: \r\n2013-01-01    L\r\n2013-02-01    L\r\n2013-03-01    L\r\n2013-04-01    L\r\n2013-05-01    L\r\n2013-06-01    L\r\n2013-07-01    L\r\n2013-08-01    L\r\n2013-09-01    L\r\n2013-10-01    L\r\n2013-11-01    L\r\n2013-12-01    L\r\nFreq: MS, Name: LeftRight\r\n\r\nIn [45]: R\r\nOut[45]: \r\n2013-01-01    R\r\n2013-02-01    R\r\n2013-03-01    R\r\n2013-04-01    R\r\n2013-05-01    R\r\n2013-06-01    R\r\n2013-07-01    R\r\n2013-08-01    R\r\n2013-09-01    R\r\n2013-10-01    R\r\n2013-11-01    R\r\n2013-12-01    R\r\nFreq: MS, Name: LeftRight\r\n\r\nIn [46]: pd.concat([L,R], axis=1)\r\nOut[46]: \r\n           LeftRight LeftRight\r\n2013-01-01         R         R\r\n2013-02-01         R         R\r\n2013-03-01         R         R\r\n2013-04-01         R         R\r\n2013-05-01         R         R\r\n2013-06-01         R         R\r\n2013-07-01         R         R\r\n2013-08-01         R         R\r\n2013-09-01         R         R\r\n2013-10-01         R         R\r\n2013-11-01         R         R\r\n2013-12-01         R         R\r\n\r\nIn [47]: pd.concat([L,R], axis=1, keys=['Left','Right'])\r\nOut[47]: \r\n           Left Right\r\n2013-01-01    L     R\r\n2013-02-01    L     R\r\n2013-03-01    L     R\r\n2013-04-01    L     R\r\n2013-05-01    L     R\r\n2013-06-01    L     R\r\n2013-07-01    L     R\r\n2013-08-01    L     R\r\n2013-09-01    L     R\r\n2013-10-01    L     R\r\n2013-11-01    L     R\r\n2013-12-01    L     R\r\n\r\n```"""
3020,11924688,dsimpson1980,wesm,2013-03-12 13:46:53,2014-03-29 00:53:55,2013-03-18 17:25:38,closed,wesm,0.11,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3020,b'resample up skips first day',"b""When resampling from a TimeSeries with freq='MS' to freq='D' the TimeSeries is shifted forward by one day, skipping the first element:\r\n```python\r\nIn [64]: dates = pd.date_range('01-Apr-13','01-May-13',freq='MS')\r\n\r\nIn [65]: ts = pd.TimeSeries([1,2],dates)\r\n\r\nIn [66]: ts\r\nOut[66]: \r\n2013-04-01    1\r\n2013-05-01    2\r\nFreq: MS\r\n\r\nIn [67]: ts.resample('d',closed='left',label='left')\r\nOut[67]: \r\n2013-04-02   NaN\r\n2013-04-03   NaN\r\n2013-04-04   NaN\r\n2013-04-05   NaN\r\n2013-04-06   NaN\r\n2013-04-07   NaN\r\n2013-04-08   NaN\r\n2013-04-09   NaN\r\n2013-04-10   NaN\r\n2013-04-11   NaN\r\n2013-04-12   NaN\r\n2013-04-13   NaN\r\n2013-04-14   NaN\r\n2013-04-15   NaN\r\n2013-04-16   NaN\r\n2013-04-17   NaN\r\n2013-04-18   NaN\r\n2013-04-19   NaN\r\n2013-04-20   NaN\r\n2013-04-21   NaN\r\n2013-04-22   NaN\r\n2013-04-23   NaN\r\n2013-04-24   NaN\r\n2013-04-25   NaN\r\n2013-04-26   NaN\r\n2013-04-27   NaN\r\n2013-04-28   NaN\r\n2013-04-29   NaN\r\n2013-04-30   NaN\r\n2013-05-01     2\r\n2013-05-02   NaN\r\nFreq: D\r\n```"""
3019,11923372,mgutman515,jreback,2013-03-12 13:12:42,2013-03-14 10:43:08,2013-03-14 10:43:08,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/3019,"b""shift doesn't maintain timestamp datatype""","b'So far so good...\r\npd.Series(pd.date_range(""2000-1-1"",""2000-1-10""))\r\n0   2000-01-01 00:00:00\r\n1   2000-01-02 00:00:00\r\n2   2000-01-03 00:00:00\r\n3   2000-01-04 00:00:00\r\n4   2000-01-05 00:00:00\r\n5   2000-01-06 00:00:00\r\n6   2000-01-07 00:00:00\r\n7   2000-01-08 00:00:00\r\n8   2000-01-09 00:00:00\r\n9   2000-01-10 00:00:00\r\n\r\npd.Series(pd.date_range(""2000-1-1"",""2000-1-10"")).shift(1)\r\n\r\n0             NaN\r\n1    9.466848e+17\r\n2    9.467712e+17\r\n3    9.468576e+17\r\n4    9.469440e+17\r\n5    9.470304e+17\r\n6    9.471168e+17\r\n7    9.472032e+17\r\n8    9.472896e+17\r\n9    9.473760e+17'"
3014,11890487,waitingkuo,y-p,2013-03-11 18:25:42,2014-07-01 11:00:16,2013-03-21 07:38:36,closed,,0.11,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/3014,"b'Bug: Fixed Issue 2993, Incorrect Timestamp Construction by datetime.date and tz'",b'`Timestamp` cannot be constructed correctly by given an `datetime.date` instance and `tz`.\r\nThe issued has been solved in this patch and the test case is add in `pandas/tseries/test/test_timezones.py`'
3011,11873378,debrouwere,y-p,2013-03-11 11:44:35,2013-03-16 23:53:25,2013-03-16 23:53:25,closed,,0.11,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/3011,b'segmentation fault on groupby',"b""```\r\nimport numpy as np\r\nimport pandas as pd\r\nseries = pd.Series([np.nan, np.nan, 1, 1, 2, 2, 3, 3, 4, 4])\r\nbins = pd.cut(series.dropna(), 4)\r\n# this works\r\nseries.dropna().groupby(bins).mean()\r\n# but this results in: \r\n# [1]    42448 segmentation fault  python\r\nseries.groupby(bins).mean()\r\n```\r\n\r\nSo, I can see why trying to group a series with bins based on a different (trimmed) series can't really work and don't consider that to be a bug, but it might be nice if `pandas` threw a proper error message instead of segfaulting."""
3010,11866851,zetyang,zetyang,2013-03-11 07:58:44,2013-03-13 03:23:33,2013-03-13 03:23:33,closed,,0.11,5,Bug,https://api.github.com/repos/pydata/pandas/issues/3010,b'DataFrame corrupted after improper column creation',"b'I want to add a column to DataFrame using a list of arrays, but I forgot to change the 2d array into list first, so  I got a ""Wrong number of items passed"" error.\r\n\r\nAfter that, the df seemed to be corrupted, whenever I want to print it, it kept raising a ""NoneType object is not iterable error"".\r\nWhile it could still be added new columns, the improperly assigned columns couldn\'t be deleted yet (also prompting the NoneType error).\r\n\r\nThis the code that could reproduce my problem:\r\n\r\n```python\r\ndf = DataFrame(np.ones((4,4)))\r\ndf[\'foo\'] = np.ones((4,2)).tolist()  # OK, and that\'s what I should type\r\ndf[\'test\'] = np.ones((4,2))  # the improper column creation\r\nAsssertionError: Wrong number of items passed \xa3\xa81 vs 2)\r\n\r\ndf\r\nTypeError: \'NoneType\' object is not iterable, u\'occured at index foo\'\r\n\r\ndel df[\'test\']\r\nTypeError: \'NoneType\' object is not iterable\r\n```\r\n\r\n\r\n'"
3002,11855114,justinvf,jreback,2013-03-10 17:41:32,2013-03-14 14:03:09,2013-03-14 14:03:09,closed,,0.11,6,Bug,https://api.github.com/repos/pydata/pandas/issues/3002,b'Weird Datetime Behaviour',"b'I am seeing odd behavior of dates being converted to `datetime64[ns]` times when that is inappropriate.\r\n```\r\n    In [272]: file = StringIO(""xxyyzz20100101PIE\\nxxyyzz20100101GUM\\nxxyyww20090101EGG\\nfoofoo20080909PIE"")\r\n    \r\n    In [273]: df = pd.read_fwf(file, widths=[6,8,3], names=[""person_id"", ""dt"", ""food""], parse_dates=[""dt""])\r\n    \r\n    In [274]: df\r\n    Out[274]: \r\n      person_id                  dt food\r\n    0    xxyyzz 2010-01-01 00:00:00  PIE\r\n    1    xxyyzz 2010-01-01 00:00:00  GUM\r\n    2    xxyyww 2009-01-01 00:00:00  EGG\r\n    3    foofoo 2008-09-09 00:00:00  PIE\r\n```\r\nEverything looks good. However:\r\n```\r\n    In [275]: df.dt.value_counts()\r\n    Out[275]: \r\n    1970-01-17 00:00:00    2\r\n    1970-01-18 00:00:00    1\r\n    1970-01-25 08:00:00    1\r\n    \r\n    In [276]: df.dt.dtype\r\n    Out[276]: dtype(\'datetime64[ns]\')\r\n```\r\nThe type is being stored as epoch nanoseconds internally, but usually displaying usually as the dates I want. But then everything is getting messed up when we drop down to numpy land for things like value_counts. What is going on? Is this a bug or am I making mistakes?\r\n\r\n```\r\nIn [280]: pd.__version__\r\nOut[280]: \'0.10.1\'\r\n\r\nIn [281]: np.__version__\r\nOut[281]: \'1.6.1\'\r\n```'"
3001,11849918,michaelaye,hayd,2013-03-10 08:47:39,2013-08-21 20:52:12,2013-08-21 20:52:12,closed,,Someday,5,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/3001,b'[io] comment option ignored in read_csv when sep provided.',"b'````python\r\nIn [8]: import pandas as pd\r\nIn [9]: from StringIO import StringIO\r\nIn [10]: s = """"""#        date,            utc,             jdate, orbit, sundist,   sunlat,    sunlon,             sclk,     sclat,     sclon,       scrad,       scalt,  el_cmd,  az_cmd,   af, orientlat, orientlon, c, det,    vlookx,    vlooky,    vlookz,   radiance,       tb,      clat,      clon,     cemis,   csunzen,   csunazi, cloctime, qca, qge, qmi""16-Apr-2011"", ""00:00:00.025"", 2455667.500000290,  8352, 1.00538,  1.33678,  27.55755, 0324604799.65011, -35.60774, 186.36107,  1798.15230,    60.70145, 180.000, 240.000,  110,   0.42378, 275.92764, 1,   1,  0.790839,  0.052956,  0.609729,     0.0077,    0.022, -35.53812, 186.44733,   2.91887, 140.57684,  10.78604, 22.59250, 000, 012, 000""""""\r\n\r\nIn [11]: s = StringIO(s)\r\nIn [12]: columns = [\'date\', \'utc\', \'jdate\', \'orbit\', \'sundist\', \'sunlat\', \'sunlon\', \'sclk\', \'sclat\', \'sclon\', \'scrad\', \'scalt\', \'el_cmd\', \'az_cmd\', \'af\', \'orientlat\', \'orientlon\', \'c\', \'det\', \'vlookx\', \'vlooky\', \'vlookz\', \'radiance\', \'tb\', \'clat\', \'clon\', \'cemis\', \'csunzen\', \'csunazi\', \'cloctime\', \'qca\', \'qge\', \'qmi\']\r\nIn [13]: pd.io.parsers.read_csv(s,sep=\'\\s+\',names=columns,comment=\'#\')\r\nOut[13]: <class \'pandas.core.frame.DataFrame\'>Int64Index: 2 entries, 0 to 1\r\nData columns:date         1  non-null valuesutc          1  non-null valuesjdate        1  non-null values\r\norbit        1  non-null values\r\nsundist      1  non-null values\r\nsunlat       1  non-null values\r\nsunlon       1  non-null values\r\nsclk         1  non-null values\r\nsclat        1  non-null values\r\nsclon        1  non-null values\r\nscrad        1  non-null values\r\nscalt        1  non-null values\r\nel_cmd       1  non-null values\r\naz_cmd       1  non-null values\r\naf           1  non-null values\r\norientlat    1  non-null values\r\norientlon    1  non-null values\r\nc            1  non-null values\r\ndet          1  non-null values\r\nvlookx       1  non-null values\r\nvlooky       1  non-null values\r\nvlookz       1  non-null values\r\nradiance     1  non-null values\r\ntb           1  non-null values\r\nclat         1  non-null values\r\nclon         1  non-null values\r\ncemis        1  non-null values\r\ncsunzen      1  non-null values\r\ncsunazi      1  non-null values\r\ncloctime     1  non-null values\r\nqca          1  non-null values\r\nqge          1  non-null values\r\nqmi          1  non-null values\r\ndtypes: float64(1), object(32)\r\n\r\nIn [14]: pd.__version__\r\nOut[14]: \'0.11.0.dev-b3e696c\'\r\n````\r\n\r\nexpected: 1 entry (comment line should have been ignored)\r\n'"
2998,11845992,huxley-source,jreback,2013-03-09 23:49:16,2013-03-14 10:45:52,2013-03-14 10:45:52,closed,,0.11,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2998,b'Series append function shifts index if timezone information exist',"b""Code to reproduce:\r\n\r\nrng1 = pd.date_range('1/1/2011 01:00', periods=10, freq='H')\r\nrng2 = pd.date_range('1/1/2011 02:00', periods=10, freq='H')\r\nts1 = pd.Series(np.random.randn(len(rng1)), index=rng1).tz_localize('CET')\r\nts2 = pd.Series(np.random.randn(len(rng2)), index=rng2).tz_localize('CET')\r\nts_result = ts1.append(ts2)\r\n\r\nResult in IPython:\r\n\r\nIn [208]:\r\n\r\nts1\r\nOut[208]:\r\n2011-01-01 01:00:00+01:00    0.164609\r\nFreq: H\r\nIn [209]:\r\n\r\nts2\r\nOut[209]:\r\n2011-01-01 02:00:00+01:00    2.138849\r\nFreq: H\r\nIn [210]:\r\n\r\nts_result\r\nOut[210]:\r\n2011-01-01 00:00:00+01:00    0.164609\r\n2011-01-01 01:00:00+01:00    2.138849"""
2994,11831024,wesm,jreback,2013-03-09 00:11:01,2013-03-21 12:31:46,2013-03-21 12:31:46,closed,,0.11,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2994,"b'DataFrame.replace(<scalar>, ...) is not handled'","b'I would expect like `df.replace(0, df.mean())` to work. And it does not'"
2983,11733232,rippedathlete,jreback,2013-03-06 21:19:44,2014-02-16 22:12:44,2014-02-16 22:12:44,closed,,0.14.0,1,Bug;Data IO;IO Google,https://api.github.com/repos/pydata/pandas/issues/2983,b'Google analytics error with eventLabel and totalEvents',"b'pandas 0.10.1, on OSX mountain lion, installed through pip.\r\n\r\nSo, this works fine for me,\r\n```python\r\nevents = ga.read_ga([\'visits\'],\r\n                dimensions=[""date""],\r\n                start_date=\'2013-01-15\',\r\n\r\n                secrets=secretsfile,\r\n                account_name=""******************"",\r\n                property_id=""***************"")\r\n```\r\n\r\nBut when I do the following,\r\n\r\n```python\r\nevents = ga.read_ga([\'totalEvents\'],\r\n                dimensions=[""eventLabel""],\r\n                start_date=\'2013-01-15\',\r\n\r\n                secrets=sec,\r\n                account_name=""Ripped Athlete"",\r\n                property_id=""UA-37875661-1"")\r\n```\r\n\r\nI get this traceback (abbreviated)\r\n\r\n```\r\n/usr/local/lib/python2.7/site-packages/pandas/io/ga.pyc in _parse_data(self, rows, col_info, index_col, parse_dates, keep_date_col, date_parser, dayfirst, na_values, converters, sort)\r\n    309                                   keep_date_col=keep_date_col,\r\n    310                                   converters=converters,\r\n--> 311                                   header=None, names=col_names))\r\n    312 \r\n    313         if isinstance(sort, bool) and sort:\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)\r\n    206 \r\n    207     # Create the parser.\r\n--> 208     parser = TextFileReader(filepath_or_buffer, **kwds)\r\n    209 \r\n    210     if nrows is not None:\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in __init__(self, f, engine, **kwds)\r\n    505             self.options[\'has_index_names\'] = kwds[\'has_index_names\']\r\n    506 \r\n--> 507         self._make_engine(self.engine)\r\n    508 \r\n    509     def _get_options_with_defaults(self, engine):\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in _make_engine(self, engine)\r\n    613             elif engine == \'python-fwf\':\r\n    614                 klass = FixedWidthFieldParser\r\n--> 615             self._engine = klass(self.f, **self.options)\r\n    616 \r\n    617     def _failover_to_python(self):\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in __init__(self, f, **kwds)\r\n   1157         else:\r\n   1158             self.data = f\r\n-> 1159         self.columns = self._infer_columns()\r\n   1160 \r\n   1161         # get popped off for index\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in _infer_columns(self)\r\n   1331                 line = self.buf[0]\r\n   1332             else:\r\n-> 1333                 line = self._next_line()\r\n   1334 \r\n   1335             ncols = len(line)\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in _next_line(self)\r\n   1352                 line = self.data[self.pos]\r\n   1353             except IndexError:\r\n-> 1354                 raise StopIteration\r\n   1355         else:\r\n   1356             while self.pos in self.skiprows:\r\n\r\nStopIteration: \r\n```\r\n\r\nBefore I dig in further, I noticed that [`PythonParser._infer_columns`](https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L1301) doesn\'t handle the `StopIteration` that can be raised by [`PythonParser._next_line`](https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L1351). Should we consider raising that exception to be normal operation and change `_infer_columns` to handle it, or should we figure out what strange mix of circumstances is causing `_next_iteration` to raise it?'"
2981,11729607,wesm,wesm,2013-03-06 19:53:49,2013-04-08 07:36:14,2013-04-08 07:36:14,closed,,0.11,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2981,b'Issues reading ragged CSV files',b'http://stackoverflow.com/questions/15242746/handling-variable-number-of-columns-with-pandas-python'
2978,11707022,cxrodgers,y-p,2013-03-06 10:21:36,2013-03-06 11:23:39,2013-03-06 11:23:39,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2978,b'BUG: Printing a DataFrame containing user-defined object causes infinite loop',"b'Version: pandas 0.10.0 , via `pip`\r\nOS: Ubuntu 12.04 x64\r\n\r\nDescription:\r\nI put a user-defined object (which defines only __getitem__) into a dataframe. Printing the dataframe causes the system to hang. I suspect that the construction of a text representation of the object is calling my object\'s __getitem__.\r\n\r\nA simple code snippet to fully reproduce the problem:\r\n\r\n    import pandas\r\n    class A:\r\n        def __getitem__(self, key):\r\n            return 3 # obviously simplified\r\n    df = pandas.DataFrame([A()])\r\n    print df\r\n\r\nDesired behavior: pandas should not be calling the __getitem__ method of objects in DataFrame.\r\n\r\nRunning this script causes my computer to hang. Ctrl+C dumps a long traceback. Here is the first bit of it:\r\n\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 6, in <module>\r\n    print df\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/frame.py"", line 636, in __str__\r\n    return self.__bytes__()\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/frame.py"", line 646, in __bytes__\r\n    return self.__unicode__().encode(encoding , \'replace\')\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/frame.py"", line 663, in __unicode__\r\n    self.to_string(buf=buf, line_width=line_width)\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/frame.py"", line 1503, in to_string\r\n    formatter.to_string()\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/format.py"", line 297, in to_string\r\n    strcols = self._to_str_columns()\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/format.py"", line 251, in _to_str_columns\r\n    fmt_values = self._format_col(i)\r\n  File ""/home/jack/.local/lib/python2.7/site-packages/pandas/core/format.py"", line 379, in _format_col\r\n    space=self.col_space)\r\n'"
2972,11692123,lesteve,dieterv77,2013-03-05 23:45:32,2013-03-16 02:31:33,2013-03-16 02:31:33,closed,,0.11,8,Bug,https://api.github.com/repos/pydata/pandas/issues/2972,b'pandas.concat bug with series of the same name and axis=1',"b""Kind of an edge case I would say, but still it would be nice to get an error or have an automatic renaming of the columns. If a simple error is acceptable as a first step I could definitely put together a PR.\r\n\r\nThe following snippet shows that when you concatenate a column of zeros and a column of ones the resulting dataframe has two columns of ones:\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nts0 = pd.Series(np.zeros(5))\r\nts1 = pd.Series(np.ones(5))\r\nts0.name = ts1.name = 'same name'\r\ndf = pd.concat([ts0, ts1], axis=1)\r\nprint df\r\n```\r\nand the output (pandas 0.10.1):\r\n```\r\n   same name  same name\r\n0          1          1\r\n1          1          1\r\n2          1          1\r\n3          1          1\r\n4          1          1\r\n```\r\n\r\nwhereas I would expect:\r\n```\r\n   same name  same name\r\n0          0          1\r\n1          0          1\r\n2          0          1\r\n3          0          1\r\n4          0          1\r\n```\r\n\r\nLooks like this is due to ```pandas.tools.merge._Concatenator._get_concat_axis``` which doesn't check for name uniqueness, here is the relevant snippet:\r\n```python\r\nnames = []\r\nfor x in self.objs:\r\n    if x.name is not None:\r\n        names.append(x.name)\r\n    else:\r\n        return Index(np.arange(len(self.objs))\r\nreturn Index(names)\r\n```"""
2971,11691598,dhstack,hayd,2013-03-05 23:29:10,2014-02-14 05:12:23,2014-02-14 05:12:23,closed,,0.14.0,5,Blocker;Bug;Data IO;IO SQL,https://api.github.com/repos/pydata/pandas/issues/2971,"b""BUG: io.sql.write_frame(if_exists='replace') not working as expected (with fix)""","b'If if_exists=\'replace\' and the table already exists, then the following error occurs:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\sql.py"", line 202, in write_frame\r\n    cur.execute(create)\r\nsqlite3.OperationalError: no such table: aggregatedData\r\n```\r\n\r\nThis is the code I ran with no issue:\r\n```psql.write_frame(outDf, \'aggregatedData\', conn)```\r\n\r\nThis is the code I ran to get the above error:\r\n```psql.write_frame(outDf, \'aggregatedData\', conn, if_exists=\'replace\')```\r\n\r\nIf the same code is re-run after the error it will work as expected because the table is not there anymore. Running the code a third time will cause the error; the error happens every other run.\r\n\r\nHere\'s the fix I implemented in pandas.io.sql.py (~line 191):\r\n```\r\n    # create or drop-recreate if necessary\r\n    create = None\r\n    if exists and if_exists == \'replace\':\r\n        create = ""DROP TABLE %s"" % name\r\n        cur = con.cursor()\r\n        cur.execute(create)\r\n        cur.close()\r\n        create = get_schema(frame, name, flavor)\r\n```\r\nBasically, I just dropped the existing table and set it to be recreated later in the code.'"
2967,11648367,jreback,jreback,2013-03-05 01:21:44,2013-03-06 12:41:02,2013-03-06 12:41:02,closed,,,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2967,b'BUG: Series.argsort() fails with datetime64[ns] with NaT / dtypes are odd',"b""pretty simple fix though, just need to make sure that the result array\r\nis typed float64 (to accomodate the nans) or int64 (if no nans)\r\n\r\n (rather than the same as input type)\r\n(which could give weird results in some cases), e.g. you wouldn't\r\nwant a float array back just because you fed it nans....\r\n\r\nheres the inspiration question\r\nhttp://stackoverflow.com/questions/15207279/return-sorted-indexes-skipping-nan-values-in-pandas\r\n```\r\nIn [29]: s = pd.Series([pd.Timestamp('201301%02d'% (i+1)) for i in range(5)])\r\n\r\nIn [30]: s\r\nOut[30]: \r\n0   2013-01-01 00:00:00\r\n1   2013-01-02 00:00:00\r\n2   2013-01-03 00:00:00\r\n3   2013-01-04 00:00:00\r\n4   2013-01-05 00:00:00\r\ndtype: datetime64[ns]\r\n\r\nIn [31]: s.argsort()\r\nOut[31]: \r\n0    0\r\n1    1\r\n2    2\r\n3    3\r\n4    4\r\ndtype: int64\r\n\r\nIn [32]: s.shift().argsort()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-32-61a586f08c06> in <module>()\r\n----> 1 s.shift().argsort()\r\n\r\n/mnt/home/jreback/pandas/pandas/core/series.pyc in argsort(self, axis, kind, order)\r\n   2119             result = values.copy()\r\n   2120             notmask = -mask\r\n-> 2121             result[notmask] = np.argsort(values[notmask], kind=kind)\r\n   2122             return Series(result, index=self.index, name=self.name)\r\n   2123         else:\r\n\r\nTypeError: array cannot be safely cast to required type\r\n```"""
2958,11564752,jreback,jreback,2013-03-01 22:20:11,2013-03-06 12:43:15,2013-03-06 12:43:15,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2958,b'BUG: fixed .abs on Series with a timedelta (partial fix for 2957)',
2957,11555945,jreback,jreback,2013-03-01 18:27:47,2013-12-04 00:57:33,2013-03-06 12:43:56,closed,,0.13,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2957,b'BUG: numpy bug in np.abs on timedelta64',"b""This is a very subtle bug in how np.abs operates on timedelta64[ns]\r\nIt converts them to us for some reason on numpy <= 1.6.2. (maybe numpy 1.7 is better)\r\nworkaround is to wrap in a Series and astype\r\n\r\ntricky to fix this because we really can't detect is except in finalize, but then\r\nhave to check all return dtypes from the operations.....\r\n\r\nexhibited also in #2948\r\n\r\n```\r\nIn [23]: s1 = pd.Series(pd.date_range('20120101',periods=3))\r\n\r\nIn [24]: s2 = pd.Series(pd.date_range('20120102',periods=3))\r\n\r\nIn [27]: s1-s2\r\nOut[27]: \r\n0   -1 days, 00:00:00\r\n1   -1 days, 00:00:00\r\n2   -1 days, 00:00:00\r\nDtype: timedelta64[ns]\r\n\r\n#### notice this is timedelta64[us] and not ns!\r\nIn [28]: np.abs(s1-s2)\r\nOut[28]: \r\n0   1 days, 00:00:00\r\n1   1 days, 00:00:00\r\n2   1 days, 00:00:00\r\nDtype: timedelta64[us]\r\n\r\n# workaround\r\nIn [30]: pd.Series(np.abs(s1-s2)).astype('timedelta64[ns]')\r\nOut[30]: \r\n0   1 days, 00:00:00\r\n1   1 days, 00:00:00\r\n2   1 days, 00:00:00\r\nDtype: timedelta64[ns]\r\n\r\n```"""
2954,11547371,jreback,jreback,2013-03-01 14:53:25,2013-09-21 01:29:24,2013-09-21 01:29:24,closed,,0.13,6,Bug;Ideas,https://api.github.com/repos/pydata/pandas/issues/2954,b'BUG: what should we do about illegal operations on mixed frames?',"b""so min across the columns is clearly non-sensical,\r\nwhat should we do about this when an op crosses dtype boundaries and is wrong ?\r\n(this is harder than it looks, because you can't always just get the numeric data,\r\na frame with only say timedelta64[ns] is ok) \r\n\r\nmaybe have a raise_on_error parameter to min? (default of True)?\r\n(if its mixed (and not all numeric)) types?\r\n\r\n```\r\n(Pdb) mixed\r\n                 A                 B    C  D  E\r\n0         00:05:05 -1 days, 00:00:00  foo  1  1\r\n1 1 days, 00:05:05 -1 days, 00:00:00  foo  1  1\r\n2 2 days, 00:05:05 -1 days, 00:00:00  foo  1  1\r\n(Pdb) mixed.min()\r\nA            0:05:05\r\nB    -1 day, 0:00:00\r\nC                foo\r\nD                  1\r\nE                  1\r\nDtype: object\r\n(Pdb) mixed.min(axis=1)\r\n0   -8.640000e+13\r\n1   -8.640000e+13\r\n2   -8.640000e+13\r\nDtype: float64\r\n(Pdb) mixed.dtypes\r\nA    timedelta64[ns]\r\nB    timedelta64[ns]\r\nC             object\r\nD              int64\r\nE            float64\r\nDtype: object\r\n```"""
2952,11530908,nmusolino,jreback,2013-03-01 03:49:03,2013-12-04 00:57:33,2013-03-14 10:42:46,closed,,0.11,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2952,b'shift(..) applied to datetime series',"b'I am reporting a bug in which a TimeSeries containing datetime values are converted to float64 values upon application of the shift(self, offset) function.  Version 0.10.1, python 2.7.\r\n\r\nAs an example below, I excerpted the first ten rows of end_time from the data frame listed.  The dtype of this timeseries is datetime64[ns].  When I apply shift(2), the values are converted to float64.  I tried applying the convert_type and astype functions to this timeseries, but neither would return the timeseries to its original datetime64[ns].\r\n\r\nExpected behavior: data type of time series should not change upon shifting.  \r\n\r\n```\r\nDatetimeIndex: 88449 entries, 2012-09-04 07:03:00 to 2013-02-25 20:00:00\r\nData columns:\r\ntime        88449  non-null values\r\nend_time    88448  non-null values\r\nmid         88449  non-null values\r\ndtypes: datetime64[ns](2), float64(1)\r\n```\r\n\r\n```\r\nIn [70]: end_times\r\nOut[70]: \r\ntime\r\n2012-09-04 07:05:00   2012-09-04 07:06:00\r\n2012-09-04 07:06:00   2012-09-04 07:07:00\r\n2012-09-04 07:07:00   2012-09-04 07:08:00\r\n2012-09-04 07:08:00   2012-09-04 07:09:00\r\n2012-09-04 07:09:00   2012-09-04 07:10:00\r\n2012-09-04 07:10:00   2012-09-04 07:11:00\r\n2012-09-04 07:11:00   2012-09-04 07:12:00\r\n2012-09-04 07:12:00   2012-09-04 07:13:00\r\n2012-09-04 07:13:00   2012-09-04 07:14:00\r\n\r\nIn [71]: end_times.shift(2)\r\nOut[71]: \r\ntime\r\n2012-09-04 07:05:00             NaN\r\n2012-09-04 07:06:00             NaN\r\n2012-09-04 07:07:00    1.346742e+18\r\n2012-09-04 07:08:00    1.346742e+18\r\n2012-09-04 07:09:00    1.346742e+18\r\n2012-09-04 07:10:00    1.346743e+18\r\n2012-09-04 07:11:00    1.346743e+18\r\n2012-09-04 07:12:00    1.346743e+18\r\n2012-09-04 07:13:00    1.346743e+18\r\nName: end_time\r\n```'"
2945,11477021,changhiskhan,changhiskhan,2013-02-27 22:00:01,2013-02-27 22:34:45,2013-02-27 22:34:45,closed,,0.11,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2945,"b""scalar subtraction doesn't work on Series with dtype datetime64""","b'In [34]: df\r\nOut[34]: \r\n          0         1                time\r\n0 -0.602181 -0.657444 2009-07-01 00:00:00\r\n1 -2.102987 -0.486064 2009-07-02 00:00:00\r\n2  0.819956 -0.800478 2009-07-03 00:00:00\r\n3  1.118629 -0.008019 2009-07-04 00:00:00\r\n4  0.404452  0.606187 2009-07-05 00:00:00\r\n\r\nIn [35]: df.time.dtype\r\nOut[35]: dtype(\'datetime64[ns]\')\r\n\r\nIn [36]: image_time\r\nOut[36]: datetime.datetime(2009, 7, 28, 13, 39, 2)\r\n\r\nIn [37]: df.time - image_time\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-37-32815ed7fa8b> in <module>()\r\n----> 1 df.time - image_time\r\n\r\n/Users/changshe/code/pandas/pandas/core/series.pyc in wrapper(self, other)\r\n     94                 dtype = \'timedelta64[ns]\'\r\n     95             else:\r\n---> 96                 raise ValueError(""cannot operate on a series with out a rhs of a series/ndarray of type datetime64[ns] or a timedelta"")\r\n     97 \r\n     98             lvalues = lvalues.view(\'i8\')\r\n\r\nValueError: cannot operate on a series with out a rhs of a series/ndarray of type datetime64[ns] or a timedelta'"
2944,11473538,jseabold,jseabold,2013-02-27 20:39:07,2013-02-27 22:58:12,2013-02-27 22:58:12,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2944,b'where broken after sort',"b'Glad I hunted this one down. Was seeing some very weird stuff. Using 0.10.1\r\n\r\n```\r\nimport pandas\r\nnames = [""a"",""b"",""c""]\r\ndf = pandas.DataFrame([[0,2,4],[2,0,7],[4,7,0]], index=names, columns=names)\r\ndf\r\n#   a  b  c\r\n#a  0  2  4\r\n#b  2  0  7\r\n#c  4  7  0\r\n\r\ndf2 = df.sort(""b"").T.sort(""c"").T\r\ndf.where(df <= 4, 0)\r\n#   a  b  c\r\n#a  0  2  4\r\n#b  2  0  0\r\n#c  4  0  0\r\n\r\ndf2.where(df <= 4, 0)\r\n#   c  a  b\r\n#b  7  2  0\r\n#a  4  0  0\r\n#c  0  0  7\r\n```'"
2942,11460762,jobingr,TomAugspurger,2013-02-27 15:47:43,2015-12-08 01:13:34,2015-12-08 01:13:34,closed,,Someday,3,Bug;IO LaTeX,https://api.github.com/repos/pydata/pandas/issues/2942,b'Multilevel indexing with .to_latex() method merges two index columns',b'The to_latex method produces wrong latex code for multi-indexed funds.   looks like it are missing a few blank {} and &.   Net result is that the current method merges the two indeces.    when used with output from pivot_table with major index giving categories and other one details this produces totally garbled results.\r\n\r\nThe formatters= parameter of to_latex method also creates exceptions for multi-indexed data'
2939,11440441,wesm,hayd,2013-02-27 03:26:13,2013-03-07 10:57:15,2013-03-07 10:57:15,closed,,0.11,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2939,b'DataFrame.drop loses index name',b'http://stackoverflow.com/questions/15102158/pandas-why-does-dataframe-droplabels-return-a-dataframe-with-removed-label-na'
2932,11388553,hayd,wesm,2013-02-26 00:55:57,2013-03-28 05:28:02,2013-03-28 05:28:02,closed,wesm,0.11,0,Bug;Data IO;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2932,b'BUG date_parser not accurate with Timestamp',"b'When reading in this DataFrame (with `parse_dates=[[0,1]]`) we see the datetimes have (29000) microseconds but when using `date_parser=pd.Timestamp` we lose this information:\r\n\r\n```\r\nfrom StringIO import StringIO\r\ndata = """"""05/31/2012,15:30:00.029,1306.25,1,E,0,,1306.25\r\n05/31/2012,15:30:00.029,1306.25,8,E,0,,1306.25""""""\r\n```\r\n```\r\nIn [3]: pd.read_csv(StringIO(data), sep=\',\', header=None, parse_dates=[[0,1]])\r\nOut[3]: \r\n                          0_1        2  3  4  5   6        7\r\n0  2012-05-31 15:30:00.029000  1306.25  1  E  0 NaN  1306.25\r\n1  2012-05-31 15:30:00.029000  1306.25  8  E  0 NaN  1306.25\r\n\r\nIn [4]: pd.read_csv(StringIO(data), sep=\',\', header=None, parse_dates=[[0,1]], date_parser=pd.Timestamp)\r\nOut[4]: \r\n                   0_1        2  3  4  5   6        7\r\n0  2012-05-31 00:00:00  1306.25  1  E  0 NaN  1306.25\r\n1  2012-05-31 00:00:00  1306.25  8  E  0 NaN  1306.25\r\n```\r\n\r\n*Note: This works accurately individually:*\r\n```\r\nIn [5]: pd.Timestamp(\'05/31/2012,15:30:00.029\')\r\nOut[5]: <Timestamp: 2012-05-31 15:30:00.029000>\r\n```\r\n\r\nMigrated from [StackOverflow](http://stackoverflow.com/questions/14446744/how-to-efficiently-handle-pandas-read-csv-with-datetime-index).'"
2928,11376318,wesm,wesm,2013-02-25 19:36:33,2013-03-10 17:44:04,2013-03-10 17:44:04,closed,,0.11,13,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2928,b'Care with dtype line in Series repr',"b""not good for long series. do we really need the dtype anyway?\r\n\r\n```\r\nyou          12\r\nthe           8\r\nand           8\r\na             7\r\nit            7\r\nfor           6\r\nin            6\r\ngcc-40        5\r\nto            5\r\ninstalled     5\r\nsome          4\r\npip           4\r\nif            4\r\nhave          4\r\nis            4\r\nDtype: int64\r\n...\r\n'-arch'       1\r\noff           1\r\nunless        1\r\ntools         1\r\nknow          1\r\nwell          1\r\ngeneral       1\r\nanother       1\r\nfink          1\r\nlaptop        1\r\ndon't         1\r\n/usr/bin      1\r\n-arch         1\r\nbeen          1\r\nre-install    1\r\nDtype: int64\r\nLength: 167, Dtype: int64\r\n```"""
2926,11369083,Moisan,wesm,2013-02-25 17:23:56,2013-03-28 05:35:58,2013-03-28 05:35:58,closed,,0.11,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2926,b'date_range with a given timezone fails if the start argument also has a timezone.',"b""I am using pandas version 0.11.0.dev-eb3134f\r\n\r\nThis code will raise [this assert](https://github.com/pydata/pandas/blob/master/pandas/tseries/index.py#L303-304)\r\n\r\nMy debugging shows that the code is testing the equality of a timezone object (inferred_tz) to a string (tz).\r\n\r\ndate_range si supposed to receive a string so I think that it is a legit use of date_range.\r\n\r\n```python\r\nfrom pandas import *\r\nstart = Timestamp('20130220 10:00', tz='US/Eastern')\r\ndr = date_range(start, tz='US/Eastern')\r\n```\r\n"""
2914,11270869,jreback,jreback,2013-02-22 01:53:33,2014-06-13 05:35:05,2013-02-23 16:43:51,closed,,,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2914,b'BUG: support data column indexers that have a large numbers of values',"b""This is conceptually (and implemented) like an ``isin``\r\n\r\nYou can give a list of values that you want for a particular column, rather\r\nthan a boolean expression. This was already speced in the API, just\r\nnot implemented correctly!\r\n\r\nSomething like this\r\n```\r\nstore.select('df',[pd.Term('users',[ 'a%03d' % i for i in xrange(60) ])])\r\n```\r\noriginally from the mailing list\r\nhttps://groups.google.com/forum/?fromgroups#!topic/pystatsmodels/oTjfOb0gazw"""
2912,11253587,abielr,y-p,2013-02-21 18:45:31,2013-03-20 01:22:50,2013-03-20 01:22:50,closed,,0.11,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2912,b'Groupby and transform on DataFrame produces FutureWarning',"b""Using pandas 0.10.1:\r\n\r\n    df1 = pd.DataFrame(np.random.randn(10))\r\n    df1.groupby(np.arange(len(df1))).transform(lambda x: x)\r\n\r\nproduces:\r\n\r\n    FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward.\r\n\r\nIn the release notes for 0.10.1 I saw the changes regarding functions with the `inplace` option. However, it didn't seem like the above code should naturally lead to such a warning. No warning is produced if the same operation is done on a Series."""
2910,11248911,jreback,jreback,2013-02-21 16:50:34,2014-06-22 07:36:52,2013-02-23 01:19:53,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2910,b'BUG: incorrect default in df.convert_objects was converting object types (#2909)',b'showing up in applymap (#2909)'
2906,11217478,xdong,jreback,2013-02-20 22:00:55,2013-03-13 23:53:50,2013-03-13 23:53:50,closed,,0.11,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2906,b'date_range bug?',"b""If 'start' is a timezone-aware datetime.datetime or Timestamp, then it will get shifted by several hours.\r\n```python\r\nIn [1]: from pandas import *\r\n\r\nIn [2]: start = Timestamp('20130220 10:00', tz='US/Eastern')\r\n\r\nIn [3]: start\r\nOut[3]: <Timestamp: 2013-02-20 10:00:00-0500 EST, tz=US/Eastern>\r\n\r\nIn [4]: dr = date_range(start, periods=2)\r\n\r\nIn [5]: dr\r\nOut[5]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2013-02-20 15:00:00, 2013-02-21 15:00:00]\r\nLength: 2, Freq: D, Timezone: US/Eastern\r\n\r\nIn [6]: dr[0]\r\nOut[6]: <Timestamp: 2013-02-20 15:00:00-0500 EST, tz=US/Eastern>\r\n```\r\n\r\nIt seems that start' time component was treated as if it's UTC by date_range.\r\n\r\n\r\n"""
2903,11187594,lodagro,jreback,2013-02-20 07:42:05,2013-04-17 00:13:10,2013-03-22 17:37:35,closed,,0.11,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2903,b'MultiIndex xs failure on axis=1',"b""```python\r\nIn [40]: columns = pd.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'hello'), ('b', 'world')], names=['lvl0', 'lvl1'])\r\n\r\nIn [41]: df = pd.DataFrame(np.random.randn(4, 4), columns=columns)\r\n\r\nIn [42]: df\r\nOut[42]:\r\nlvl0         a                   b\r\nlvl1       foo       bar     hello     world\r\n0     2.560515  1.579242  0.727753  2.207781\r\n1    -0.599632  0.254395  0.784745  0.340575\r\n2     1.262004  0.409085 -0.229616  1.096679\r\n3     1.637929 -0.694670 -0.356355  0.410580\r\n\r\nIn [43]: df.xs('a', level='lvl0', axis=1)\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n\r\n<...>\r\n\r\n<...>/lib/python2.7/site-packages/pandas-0.11.0.dev_14a04dd-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in set_axis(self, axis, value)\r\n    831         if len(value) != len(cur_axis):\r\n    832             raise Exception('Length mismatch (%d vs %d)'\r\n--> 833                             % (len(value), len(cur_axis)))\r\n    834         self.axes[axis] = value\r\n    835\r\n\r\nException: Length mismatch (2 vs 4)\r\n\r\nIn [44]: df.xs('a', axis=1)\r\nOut[44]:\r\nlvl1       foo       bar\r\n0     2.560515  1.579242\r\n1    -0.599632  0.254395\r\n2     1.262004  0.409085\r\n3     1.637929 -0.694670\r\n\r\nIn [45]: pd.__version__\r\nOut[45]: '0.11.0.dev-14a04dd'\r\n```\r\n\r\n```\r\n[1227][i] uname -a\r\nLinux xl-mec-03 2.6.18-308.11.1.el5 #1 SMP Fri Jun 15 15:41:53 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n"""
2902,11183346,theandygross,hayd,2013-02-20 03:22:35,2013-03-28 17:44:22,2013-03-28 17:44:22,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2902,b'MultiIndex on complex DataFrame.apply',"b'Seem to be converting MultiIndex into tuples when I do a complex apply:\r\n\r\n    import pandas as pd\r\n\r\n    s = pd.DataFrame([[1,2], [3,4], [5,6]])\r\n    s.index = pd.MultiIndex.from_arrays([[\'a\',\'a\',\'b\'], [\'c\',\'d\',\'d\']])\r\n    s.columns = [\'col1\',\'col2\']\r\n    s\r\n\r\n<table border=""1"" class=""dataframe"">\r\n  <thead>\r\n    <tr style=""text-align: right;"">\r\n      <th></th>\r\n      <th></th>\r\n      <th>col1</th>\r\n      <th>col2</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <td rowspan=""2"" valign=""top""><strong>a</strong></td>\r\n      <td><strong>c</strong></td>\r\n      <td> 1</td>\r\n      <td> 2</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>d</strong></td>\r\n      <td> 3</td>\r\n      <td> 4</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>b</strong></td>\r\n      <td><strong>d</strong></td>\r\n      <td> 5</td>\r\n      <td> 6</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\n    s_1 = s.apply(lambda s: pd.Series({\'min\': s.min(), \'max\': s.max()}), 1)\r\n\r\n<table border=""1"" class=""dataframe"">\r\n  <thead>\r\n    <tr style=""text-align: right;"">\r\n      <th></th>\r\n      <th>max</th>\r\n      <th>min</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <td><strong>(a, c)</strong></td>\r\n      <td> 2</td>\r\n      <td> 1</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>(a, d)</strong></td>\r\n      <td> 4</td>\r\n      <td> 3</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>(b, d)</strong></td>\r\n      <td> 6</td>\r\n      <td> 5</td>\r\n    </tr>\r\n  </tbody>\r\n</table>'"
2899,11156819,jreback,jreback,2013-02-19 15:11:51,2014-06-12 07:39:59,2013-02-23 20:30:30,closed,,,9,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2899,b'BUG: Series ops with a rhs of a Timestamp raising exception (#2898)',"b""- Series ops with a rhs of a Timestamp was throwing an exception (#2898)\r\n- fixed issue in _index.convert_scalar where the rhs was a non-scalar, lhs was dtype of M8[ns], and was trying\r\n  to convert to a scalar (but didn't need conversion)\r\n- added some utilities in tslib.pyx, inference.pyx to detect and convert timedelta/timedelta64\r\n- added timedelta64 to checknull routines, representing timedelta64 as NaT\r\n- aded setitem support to set NaT via np.nan (analagously to datetime64)\r\n- added ability for Series to detect and set a ``timedelta64[ns]`` dtype (if all passed objects are timedelta like)\r\n- added correct printing of timedelta64[ns] (looks like py3.3 changed the default for str(x), so rolled a new one)\r\n\r\n```\r\nIn [149]: from datetime import datetime, timedelta\r\n\r\nIn [150]: s  = Series(date_range('2012-1-1', periods=3, freq='D'))\r\n\r\nIn [151]: td = Series([ timedelta(days=i) for i in range(3) ])\r\n\r\nIn [152]: df = DataFrame(dict(A = s, B = td))\r\n\r\nIn [153]: df\r\nOut[153]: \r\n                    A                B\r\n0 2012-01-01 00:00:00          0:00:00\r\n1 2012-01-02 00:00:00   1 day, 0:00:00\r\n2 2012-01-03 00:00:00  2 days, 0:00:00\r\n\r\nIn [154]: df['C'] = df['A'] + df['B']\r\n\r\nIn [155]: df\r\nOut[155]: \r\n                    A                B                   C\r\n0 2012-01-01 00:00:00          0:00:00 2012-01-01 00:00:00\r\n1 2012-01-02 00:00:00   1 day, 0:00:00 2012-01-03 00:00:00\r\n2 2012-01-03 00:00:00  2 days, 0:00:00 2012-01-05 00:00:00\r\n\r\nIn [156]: df.dtypes\r\nOut[156]: \r\nA     datetime64[ns]\r\nB    timedelta64[ns]\r\nC     datetime64[ns]\r\nDtype: object\r\n\r\nIn [60]: s - s.max()\r\nOut[60]: \r\n0    -2 days, 0:00:00\r\n1     -1 day, 0:00:00\r\n2             0:00:00\r\nDtype: timedelta64[ns]\r\n\r\nIn [61]: s - datetime(2011,1,1,3,5)\r\nOut[61]: \r\n0    364 days, 20:55:00\r\n1    365 days, 20:55:00\r\n2    366 days, 20:55:00\r\nDtype: timedelta64[ns]\r\n\r\nIn [62]: s + timedelta(minutes=5)\r\nOut[62]: \r\n0   2012-01-01 00:05:00\r\n1   2012-01-02 00:05:00\r\n2   2012-01-03 00:05:00\r\nDtype: datetime64[ns]\r\n\r\nIn [160]: y = s - s.shift()\r\n\r\nIn [161]: y\r\nOut[161]: \r\n0              NaT\r\n1   1 day, 0:00:00\r\n2   1 day, 0:00:00\r\nDtype: timedelta64[ns]\r\n\r\nThe can be set to NaT using np.nan analagously to datetimes\r\n\r\nIn [162]: y[1] = np.nan\r\n\r\nIn [163]: y\r\nOut[163]: \r\n0              NaT\r\n1              NaT\r\n2   1 day, 0:00:00\r\nDtype: timedelta64[ns]\r\n\r\n# works on lhs too\r\nIn [64]: s.max() - s\r\nOut[64]: \r\n0    2 days, 0:00:00\r\n1     1 day, 0:00:00\r\n2            0:00:00\r\nDtype: timedelta64[ns]\r\n\r\nIn [65]: datetime(2011,1,1,3,5) - s\r\nOut[65]: \r\n0    -365 days, 3:05:00\r\n1    -366 days, 3:05:00\r\n2    -367 days, 3:05:00\r\nDtype: timedelta64[ns]\r\n\r\nIn [66]: timedelta(minutes=5) + s\r\nOut[66]: \r\n0   2012-01-01 00:05:00\r\n1   2012-01-02 00:05:00\r\n2   2012-01-03 00:05:00\r\nDtype: datetime64[ns]\r\n```"""
2897,11154720,pkaczynski,wesm,2013-02-19 14:18:16,2013-03-12 19:55:27,2013-03-12 19:55:27,closed,,0.11,7,Bug,https://api.github.com/repos/pydata/pandas/issues/2897,b'Wrong indexing with time zone support',"b""There is something really wrong with ix method while using time support. Simple example:\r\n\r\n```\r\ndef test_tz_error(self):\r\n        import pandas, pytz\r\n        from datetime import datetime\r\n        from datetime import timedelta as td\r\n        tz = pytz.timezone('Europe/Warsaw')\r\n        ts_from = tz.localize(datetime(2012, 1, 1, 0, 0))\r\n        ts_to = tz.localize(datetime(2012, 1, 1, 23, 0))\r\n        ttt = pandas.DataFrame(index=pandas.date_range(ts_from, ts_to, freq='H'), \r\n                               columns=['values'])\r\n        ttt['value'] = range(1,25)\r\n        ts = ts_from\r\n        while ts <= ts_to:\r\n            print ttt.ix[ts]['value']\r\n            ts += td(hours=1)\r\n```\r\n\r\nShould print out number 1,2,3,...,24. However i get:\r\n2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 24\r\n\r\nWhy is it from 2, not 1? And why the last value is repeated twice with value 24??\r\n\r\nI am using latest release 0.10.1"""
2896,11153956,p3trus,y-p,2013-02-19 13:55:24,2013-03-15 12:16:03,2013-03-15 12:16:03,closed,,,6,Bug,https://api.github.com/repos/pydata/pandas/issues/2896,b'Creating a MultiIndex with quantities fails.',"b""Using pandas MultiIndex with indices having quantities fails in some cases. An example is shown below.\r\n\r\n    import quantities as pq\r\n    import pandas as pd\r\n    \r\n    i = np.arange(10) * pq.J\r\n    j = np.array([1 for _ in xrange(10)]) * pq.K\r\n    \r\n    pd.MultiIndex.from_tuples(zip(i, j), names=['Energy', 'Temperature'])\r\n\r\nThis fails with the following traceback\r\n\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-86-c2d09517b80e> in <module>()\r\n          5 j = np.array([1 for _ in xrange(10)]) * pq.K\r\n          6 \r\n    ----> 7 pd.MultiIndex.from_tuples(zip(i, j), names=['Energy', 'Temperature'])\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in from_tuples(cls, tuples, sortorder, names)\r\n       1685 \r\n       1686         return MultiIndex.from_arrays(arrays, sortorder=sortorder,\r\n    -> 1687                                       names=names)\r\n       1688 \r\n       1689     @property\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in from_arrays(cls, arrays, sortorder, names)\r\n       1646             return Index(arrays[0], name=name)\r\n       1647 \r\n    -> 1648         cats = [Categorical.from_array(arr) for arr in arrays]\r\n       1649         levels = [c.levels for c in cats]\r\n       1650         labels = [c.labels for c in cats]\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\categorical.pyc in from_array(cls, data)\r\n         59 \r\n         60         return Categorical(labels, levels,\r\n    ---> 61                            name=getattr(data, 'name', None))\r\n         62 \r\n         63     _levels = None\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\categorical.pyc in __init__(self, labels, levels, name)\r\n         45     def __init__(self, labels, levels, name=None):\r\n         46         self.labels = labels\r\n    ---> 47         self.levels = levels\r\n         48         self.name = name\r\n         49 \r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\categorical.pyc in _set_levels(self, levels)\r\n         68         levels = _ensure_index(levels)\r\n         69         if not levels.is_unique:\r\n    ---> 70             raise ValueError('Categorical levels must be unique')\r\n         71         self._levels = levels\r\n         72 \r\n    \r\n    ValueError: Categorical levels must be unique\r\n\r\nIf I remove the units, it works just fine.\r\n\r\n    i = np.arange(10)\r\n    j = np.array([1 for _ in xrange(10)])\r\n    \r\n    pd.MultiIndex.from_tuples(zip(i, j), names=['Energy', 'Temperature'])\r\n\r\nIf I keep the units, but use a unique item for j, it works as well.\r\n\r\n    i = np.arange(10) * pq.J\r\n    j = np.arange(10) * pq.K\r\n    \r\n    pd.MultiIndex.from_tuples(zip(i, j), names=['Energy', 'Temperature'])\r\n\r\nThis is of course no option since the indices come from a measurement. I'd really like to keep the units, but since I'm not familiar with pandas internals I don't know how to fix this.\r\n\r\n## Versions\r\nI'm using pandas version 0.10.1 and quantities 0.10.1 in python 2.7.\r\n\r\n\r\n"""
2893,11127546,svendx4f,wesm,2013-02-18 19:14:58,2013-05-14 11:54:01,2013-04-08 16:41:15,closed,,0.11,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/2893,b'Inconsistent DataFrame.groupby returned type when grouped value is unique',"b'\r\n\r\nI am using groupby().apply() to compute new columns in a dataframe, for example like this: \r\n\r\n\r\n```python\r\ndf1 = DataFrame([{""val1"": 1, ""val2"" : 20}, {""val1"":1, ""val2"": 19}, {""val1"":2, ""val2"": 27}, {""val1"":2, ""val2"": 12}])\r\ndef func(dataf):\r\n\treturn dataf[""val2""]  - dataf[""val2""].mean()\r\nprint type(df1.groupby(""val1"").apply(func))\t\t# this is a Series\r\ndf1[""centered""] = df1.groupby(""val1"").apply(func)\r\nprint df1\r\n```\r\n\r\nHowever, if the set of values of the grouped by column (""val1"") is unique, the groupby above returns a dataframe as opposed to a Serie, in which case the assignment of the result to a column fails: \r\n\r\n```python\r\ndf2 = DataFrame([{""val1"": 1, ""val2"" : 20}, {""val1"":1, ""val2"": 19}, {""val1"":1, ""val2"": 27}, {""val1"":1, ""val2"": 12}])\r\ndef func(dataf):\r\n\treturn dataf[""val2""]  - dataf[""val2""].mean()\r\nprint type(df2.groupby(""val1"").apply(func))\t\t# this is a DataFrame\r\ndf2[""centered""] = df2.groupby(""val1"").apply(func)\t\t\t# this fails: cannot assign a DataFrame to a column\r\nprint df2\r\n```\r\n\r\nAs a result, my code is littered by check of uniqueness of grouped by parameter: \r\n\r\n```python\r\nif len(dataframe[""val1""].unique()) == 1:\r\n  df2[""centered""] = func(df2)\r\nelse:\r\n df2[""centered""] = df2.groupby(""val1"").apply(func)\r\n```\r\n\r\nAm I taking the wrong approach there? If not, would it be possible to have a consistent returned type?\r\n\r\nThanks!\r\n\r\n\r\n\r\n'"
2892,11124945,dsm054,jreback,2013-02-18 17:53:53,2013-03-13 00:22:32,2013-03-13 00:22:32,closed,,0.11,40,Bug,https://api.github.com/repos/pydata/pandas/issues/2892,b'intermittent segfault after take error',"b'Was playing around and managed to get a segfault, in several versions.  It\'s a little intermittent, and it may have to do with how things are left after an error is encountered.  The same code without the the faulty take line doesn\'t crash for me.\r\n\r\n```python\r\n>>> import pandas as pd, sys\r\n>>> sys.version\r\n\'2.7.3 (default, Aug  1 2012, 05:16:07) \\n[GCC 4.6.3]\'\r\n>>> pd.__version__\r\n\'0.11.0.dev-14a04dd\'\r\n>>> df = pd.DataFrame({\'A\': {1.0: 0.0, 2.0: 6.0, 3.0: 12.0}, \'C\': {1.0: 2.0, 2.0: 8.0, 3.0: 14.0}, \'B\': {1.0: 1.0, 2.0: 7.0, 3.0: 13.0}})\r\n>>> df\r\n    A   B   C\r\n1   0   1   2\r\n2   6   7   8\r\n3  12  13  14\r\n>>> \r\n>>> df.take(df.unstack())\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_14a04dd-py2.7-linux-i686.egg/pandas/core/frame.py"", line 2951, in take\r\n    new_index = self.index.take(indices)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_14a04dd-py2.7-linux-i686.egg/pandas/core/index.py"", line 416, in take\r\n    taken = self.view(np.ndarray).take(indexer)\r\nIndexError: index out of range for array\r\n>>> df.unstack()\r\nA  2     0\r\n   2     6\r\n   3    12\r\nB  2     1\r\n   2     7\r\n   3    13\r\nC  2     2\r\n   2     8\r\n   3    14\r\nDtype: float64\r\n>>> df.take(df.unstack())\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_14a04dd-py2.7-linux-i686.egg/pandas/core/frame.py"", line 2951, in take\r\n    new_index = self.index.take(indices)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pandas-0.11.0.dev_14a04dd-py2.7-linux-i686.egg/pandas/core/index.py"", line 416, in take\r\n    taken = self.view(np.ndarray).take(indexer)\r\nIndexError: index out of range for array\r\n>>> df.unstack()\r\nSegmentation fault (core dumped)\r\n```'"
2891,11122408,bjonen,y-p,2013-02-18 16:50:41,2013-03-30 21:06:16,2013-03-30 21:06:07,closed,,0.11,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2891,b'Cannot unpickle `DataFrame` created from period_range',"b'Running the following snippet on pandas version 0.10.1 leads the traceback at the bottom: \r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nfrom pandas import *\r\nprng = period_range(\'1/1/2011\', \'1/1/2012\', freq=\'M\')\r\nts = Series(np.random.randn(len(prng)), prng)\r\nts.save(\'pd_save\')\r\nnew_ts = pd.load(\'pd_save\')\r\nprint new_ts\r\n\r\nValueError                                Traceback (most recent call last)\r\nC:\\Python27\\lib\\site-packages\\IPython\\utils\\py3compat.pyc in execfile(fname, glob, loc)\r\n    169             else:\r\n    170                 filename = fname\r\n--> 171             exec compile(scripttext, filename, \'exec\') in glob, loc\r\n    172     else:\r\n    173         def execfile(fname, *where):\r\n\r\nC:\\development\\eclipse-workspace_23\\slquant-dev\\src\\pd_bug.py in <module>()\r\n      8 ts.save(\'pd_save\')\r\n      9 new_ts = pd.load(\'pd_save\')\r\n---> 10 print new_ts\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __str__(self)\r\n    968         if py3compat.PY3:\r\n    969             return self.__unicode__()\r\n--> 970         return self.__bytes__()\r\n    971\r\n    972     def __bytes__(self):\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __bytes__(self)\r\n    978         """"""\r\n    979         encoding = com.get_option(""display.encoding"")\r\n--> 980         return self.__unicode__().encode(encoding, \'replace\')\r\n    981\r\n    982     def __unicode__(self):\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __unicode__(self)\r\n    994             result = self._get_repr(print_header=True,\r\n    995                                     length=len(self) > 50,\r\n--> 996                                     name=True)\r\n    997         else:\r\n    998             result = com.pprint_thing(self)\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in _get_repr(self, name, print_header, length, na_rep, float_format)\r\n   1080                                         length=length, na_rep=na_rep,\r\n   1081                                         float_format=float_format)\r\n-> 1082         result = formatter.to_string()\r\n   1083         assert type(result) == unicode\r\n   1084         return result\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\format.pyc in to_string(self)\r\n    124             return u\'\'\r\n    125\r\n--> 126         fmt_index, have_header = self._get_formatted_index()\r\n    127         fmt_values = self._get_formatted_values()\r\n    128\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\format.pyc in _get_formatted_index(self)\r\n    110         else:\r\n    111             have_header = index.name is not None\r\n--> 112             fmt_index = index.format(name=True)\r\n    113         return fmt_index, have_header\r\n    114\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\tseries\\period.pyc in format(self, name, formatter)\r\n   1006             header.append(str(self.name) if self.name is not None else \'\')\r\n   1007\r\n-> 1008         return header + [\'%s\' % Period(x, freq=self.freq) for x in self]\r\n   1009\r\n   1010     def __array_finalize__(self, obj):\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\tseries\\period.pyc in __iter__(self)\r\n    683     def __iter__(self):\r\n    684         for val in self.values:\r\n--> 685             yield Period(ordinal=val, freq=self.freq)\r\n    686\r\n    687     @property\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\tseries\\period.pyc in __init__(self, value, freq, ordinal, year, month, quarter, day, hour, minute, second)\r\n     79                 raise ValueError(""Ordinal must be an integer"")\r\n     80             if freq is None:\r\n---> 81                 raise ValueError(\'Must supply freq for ordinal value\')\r\n     82             self.ordinal = ordinal\r\n     83\r\n\r\nValueError: Must supply freq for ordinal value\r\n```'"
2887,11082576,changhiskhan,hayd,2013-02-17 03:25:01,2014-05-29 06:08:18,2014-05-29 06:08:18,closed,,Someday,2,Bug;Prio-low,https://api.github.com/repos/pydata/pandas/issues/2887,b'ambiguous col/index name',b'This feels vaguely familiar but the DataFrame display is wrong after the stack/reset_index operations below (xref: http://stackoverflow.com/questions/14916358/reshaping-dataframes-in-pandas-based-on-column-labels/14917572#14917572):\r\n\r\n```\r\nIn [58]: df\r\nOut[58]: \r\nsample          s1                  s2          \r\nvariable         x         y         x         y\r\n0         0.298171 -1.336739  1.117346  2.291124\r\n1        -1.802796 -0.154719  0.532947 -0.338009\r\n2         0.086519  1.641511  0.001948 -0.575835\r\n3        -1.638318 -0.206623  0.238649 -0.145120\r\n4         0.368954  0.654914  1.529368  0.300910\r\n5         1.609236 -0.418993 -1.088934  0.912224\r\n6         0.437332  2.668343  0.751687  0.289443\r\n7         0.627601  0.057554  0.085994  2.038858\r\n8         0.785895  1.695244  0.085235  1.017542\r\n9         0.998866  1.001776 -0.662358  0.280511\r\n\r\nIn [59]: df.stack(0).reset_index(1)\r\nOut[59]: \r\nvariable sample         x         y\r\n0            s1  0.298171 -1.336739\r\n0            s2  1.117346  2.291124\r\n1            s1 -1.802796 -0.154719\r\n1            s2  0.532947 -0.338009\r\n2            s1  0.086519  1.641511\r\n2            s2  0.001948 -0.575835\r\n3            s1 -1.638318 -0.206623\r\n3            s2  0.238649 -0.145120\r\n4            s1  0.368954  0.654914\r\n4            s2  1.529368  0.300910\r\n5            s1  1.609236 -0.418993\r\n5            s2 -1.088934  0.912224\r\n6            s1  0.437332  2.668343\r\n6            s2  0.751687  0.289443\r\n7            s1  0.627601  0.057554\r\n7            s2  0.085994  2.038858\r\n8            s1  0.785895  1.695244\r\n8            s2  0.085235  1.017542\r\n9            s1  0.998866  1.001776\r\n9            s2 -0.662358  0.280511\r\n```'
2877,11049705,kdebrab,y-p,2013-02-15 19:55:15,2013-03-15 20:51:59,2013-03-15 20:51:59,closed,,0.11,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2877,"b""'Cannot compare tz-naive and tz-aware timestamps' when plotting series""","b'Pandas version 0.10.1:\r\n\r\n```\r\nimport pandas as pd\r\nindex = pd.date_range(\'1/1/2011\', periods=2, freq=\'H\', tz=\'Europe/Brussels\')\r\nts = pd.Series([188.5, 328.25], index=index)\r\nts.plot()\r\n```\r\nyields the following exception:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1534, in plot_series\r\n    plot_obj.generate()\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 735, in generate\r\n    self._make_plot()\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1076, in _make_plot\r\n    if not self.x_compat and self.use_index and self._use_dynamic_x():\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1057, in _use_dynamic_x\r\n    return (freq is not None) and self._is_dynamic_freq(freq)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1030, in _is_dynamic_freq\r\n    return freq is not None and self._no_base(freq)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\plotting.py"", line 1043, in _no_base\r\n    return Period(x[0], freq).to_timestamp() == x[0]\r\n  File ""tslib.pyx"", line 434, in pandas.tslib._Timestamp.__richcmp__ (pandas\\tslib.c:7930)\r\n  File ""tslib.pyx"", line 484, in pandas.tslib._Timestamp._assert_tzawareness_compat (pandas\\tslib.c:8528)\r\nException: Cannot compare tz-naive and tz-aware timestamps\r\n```'"
2876,11044846,mrjbq7,y-p,2013-02-15 17:35:14,2013-02-22 06:22:34,2013-02-22 06:22:34,closed,,0.11,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2876,b'Printing a DataFrame is slow',"b'For some reason, calling ``print df`` is really slow:\r\n\r\n```\r\nIn [1]: import pandas\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: df = pandas.DataFrame(np.random.randn(1000,100000))\r\n\r\nIn [4]: %timeit print df\r\n1 loops, best of 3: 1.93 s per loop\r\n\r\nIn [5]: %timeit print df.values\r\n1000 loops, best of 3: 1.25 ms per loop\r\n```\r\n\r\n'"
2874,11008812,alansaul,y-p,2013-02-14 18:56:59,2013-03-20 01:28:17,2013-03-20 01:28:17,closed,,0.11,2,Bug;Can't Repro,https://api.github.com/repos/pydata/pandas/issues/2874,b'Apply fails on empty Series',"b'I would expect the sum of [] to be giving me 0 as numpy would:\r\n\r\n    In [18]: import numpy as np\r\n\r\n    In [19]: np.sum([])\r\n    Out[19]: 0.0\r\n\r\nbut it appears apply cannot handle a lambda with a sum within if its logical selection evaluates to a empty series.\r\n\r\n    In [15]: a = pd.Series([])\r\n\r\n    In [16]: a.apply(lambda x: sum(a == x))\r\n    ---------------------------------------------------------------------------\r\n    IndexError                                Traceback (most recent call last)\r\n    <ipython-input-16-a2ddbb49d4e4> in <module>()\r\n    ----> 1 a.apply(lambda x: sum(a == x))\r\n\r\n    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-    packages/pandas/core/series.pyc in apply(self, func, convert_dtype, args, **kwds)\r\n       2295 \r\n       2296         mapped = lib.map_infer(values, f, convert=convert_dtype)\r\n    -> 2297         if isinstance(mapped[0], Series):\r\n       2298             from pandas.core.frame import DataFrame\r\n       2299             return DataFrame(mapped.tolist(), index=self.index)\r\n\r\n    IndexError: index 0 is out of bounds for axis 0 with size 0'"
2866,10956544,tavistmorph,y-p,2013-02-13 14:56:31,2013-03-20 01:08:39,2013-03-20 01:07:59,closed,,,7,Bug,https://api.github.com/repos/pydata/pandas/issues/2866,b'Multi-level .ix works on Series but not on DataFrame',"b""Using pandas 0.10.1.  The .ix on a multi-index Series seems very inconsistent with the .ix on a multi-index DataFrame. \r\n\r\n#From page 147 of Wes's book:\r\ndata = Series(np.random.randn(10),   index = [['a','a','a','b','b','b', 'c','c','d','d'],  [1,2,3,1,2,3,1,2,2,3]])\r\ndata.ix['a']  #works as you'd expect, pulls all the 'a'\r\ndata.ix[ : , 2 ]  #works as you'd expect, pulls all the '2'\r\n\r\n#Now the exact same thing except use DataFrame instead of Series.\r\ndata = DataFrame(np.random.randn(10),   index = [['a','a','a','b','b','b', 'c','c','d','d'],   [1,2,3,1,2,3,1,2,2,3]])\r\ndata.ix['a']  #works as you'd expect, pulls all the 'a'\r\ndata.ix[ : , 2 ]  #ERROR! But why? It works for series.\r\n\r\n#Also there is...\r\ndata.ix[ 'a' , 2 ] #works on both DataFrame and Series\r\ndata.ix[ 'a', : ]  #works on the DataFrame but ERROR on the series. (Though easy workaround is just .ix['a'])\r\n\r\nI'd sure expect the .ix[]  to work the same way on DataFrame and Series. Is there any reason they don't? Can we change it so the dataframe has the same functionality as series?"""
2852,10918844,tavistmorph,jreback,2013-02-12 18:29:20,2013-09-14 21:43:28,2013-05-08 21:48:02,closed,,0.12,19,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2852,"b""PyTables dates don't work when you switch to a different time zone""","b'Create a dataframe with a date (not datetime) column as the index. Save that dataframe to the HDFStore. Now open that same file on a computer in a time zone that\'s behind you. (IE, write the file in New York and then read the file from Dallas). Result: all the dates are off by one.\r\nHere is a code snippet to demonstrate the problem, and also a proposed fix to the pandas code. (But someone more qualified than me can review this fix).\r\n\r\nTo repro the problem:\r\n\r\n#write a file in New York time zone\r\nimport pandas\r\nimport datetime\r\n\r\ndef writeToFile(fileName) :\r\n    dates = [ datetime.date.today() , datetime.date.today(), datetime.date.today() ]\r\n    numbers = [ 1, 2, 3 ]\r\n    p = pandas.DataFrame({ ""date"" : dates, ""number"": numbers}, columns = [\'date\', \'number\'])\r\n    p.set_index(\'date\', inplace=True)\r\n    store = pandas.HDFStore(fileName)\r\n    store[\'obj1\'] = p\r\n    store.close()\r\n\r\n#Then go to a dallas time zone and read that file:  \r\ndef readFromFile(fileName) :\r\n    store = pandas.HDFStore(fileName)\r\n    return store[\'obj1\']\r\n\r\n#Notice that the dates on the dataframe now show YESTERDAY when they ought to show today.\r\n\r\nProposed fix in pandas.io.pytables.py:\r\n\r\ndef _convert_index(index):\r\n   ...\r\n   elif inferred_type == \'date\':\r\n        #OLD LINE: CHANGE THIS LINE\r\n        #converted = np.array([time.mktime(v.timetuple()) for v in values],\r\n        #                     dtype=np.int32)\r\n        #NEW LINE:\r\n        converted = np.array([v.toordinal() for v in values],  dtype=np.int32)        \r\n\r\n\r\nand then also change:\r\n\r\ndef _unconvert_index(data, kind):\r\n    ...\r\n    elif kind == \'date\':\r\n        # here we\'ll try from ordinal first.  If the date was saved with the old\r\n        # mktime mechanism it\'ll throw an exception as it\'ll be out of bounds.\r\n        # in those cases we\'ll convert using the old method (with the bug!)     \r\n        try: \r\n            index = np.array([date.fromordinal(v) for v in data], dtype=object)\r\n        except ValueError:\r\n            index = np.array([date.fromtimestamp(v) for v in data], dtype=object)'"
2850,10910900,hayd,y-p,2013-02-12 15:25:43,2013-12-17 07:31:45,2013-03-14 02:54:57,closed,,0.11,6,Bug,https://api.github.com/repos/pydata/pandas/issues/2850,b'BUG: Set_index fills in some NaN',"b""In this example when setting the index of a DataFrame to columns with NaN this values is filled:\r\n\r\n```\r\nIn [11]: df = pd.DataFrame({'id1': {0: '1a3', 1: '9h4'}, 'id2': {0: nan, 1: 'd67'},\r\n                            'id3': {0: '78d', 1: '79d'}, 'value': {0: 123, 1: 64}})\r\n\r\nIn [12]: df\r\nOut[12]: \r\n   id1  id2  id3  value\r\n0  1a3  NaN  78d    123\r\n1  9h4  d67  79d     64\r\n\r\nIn [13]: df.set_index(['id1', 'id2', 'id3'])\r\nOut[13]: \r\n             value\r\nid1 id2 id3       \r\n1a3 d67 78d    123\r\n9h4 d67 79d     64\r\n```\r\n\r\n*I see this on `'0.10.1.dev-f73128e'`, apologies if this has been mentioned before.*\r\nNote: This example is essentially from [this SO question](http://stackoverflow.com/questions/14835152/difference-between-two-pandas-dataframes-with-multiple-incomplete-indexes)."""
2844,10877573,jseabold,jseabold,2013-02-11 18:41:43,2013-04-08 07:18:45,2013-04-08 07:18:45,closed,,0.11,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2844,b'Problem with YearBegin frequency?',"b'Bug or user/expectation error? Been a while since I looked at this. Shouldn\'t this work and return days at the start of April?\r\n\r\n```\r\npandas.date_range(""1972-04-02"", ""2007-04-02"", freq=""AS-APR"")\r\n \r\n#<class \'pandas.tseries.index.DatetimeIndex\'>\r\n#[1973-01-01 00:00:00, ..., 2007-01-01 00:00:00]\r\n#Length: 35, Freq: AS-APR, Timezone: None\r\n\r\npandas.datetools.to_offset(""AS-APR"")\r\n#<1 YearBegin: kwds={\'month\': 4}, month=4>\r\n```\r\n\r\nThis does.\r\n\r\n```\r\npandas.date_range(""1972-04-02"", ""2007-04-02"", freq=""BAS-APR"")\r\n\r\n#<class \'pandas.tseries.index.DatetimeIndex\'>\r\n#[1972-04-03 00:00:00, ..., 2007-04-02 00:00:00]\r\n#Length: 36, Freq: BAS-APR, Timezone: None\r\n\r\npandas.date_range(""1972-04-02"", ""2007-04-02"", freq=""A-APR"")\r\n\r\n#<class \'pandas.tseries.index.DatetimeIndex\'>\r\n#[1972-04-30 00:00:00, ..., 2006-04-30 00:00:00]\r\n#Length: 35, Freq: A-APR, Timezone: None\r\n\r\npandas.version.version\r\n# \'0.10.1\'\r\n```'"
2843,10875976,rafaljozefowicz,hayd,2013-02-11 17:59:49,2013-03-07 10:58:16,2013-03-07 10:58:16,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2843,"b""left join on column doesn't work with missing values on right side""","b'```python\r\ndf1 = pd.DataFrame({""i1"" : [0, 1], ""i2"" : [0, 1]})\r\ndf2 = pd.DataFrame({""i1"" : [0], ""i3"" : [0]})\r\ndf1.join(df2, on=""i1"", rsuffix=""_"")\r\n```\r\n\r\nthrows:\r\n```\r\npandas-0.10.0-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in _maybe_add_join_keys(self, result, left_indexer, right_indexer)\r\n    224                         continue\r\n    225 \r\n--> 226                     left_na_indexer = left_indexer.take(na_indexer)\r\n    227                     key_col.put(na_indexer, com.take_1d(self.left_join_keys[i],\r\n    228                                                         left_na_indexer))\r\n\r\nAttributeError: \'NoneType\' object has no attribute \'take\'\r\n```\r\n\r\nwhile the following works as expected:\r\n```python\r\ndf1 = pd.DataFrame({""i1"" : [0, 1], ""i2"" : [0.5, 1.5]}).set_index(""i1"")\r\ndf2 = pd.DataFrame({""i1"" : [0], ""i3"" : [0.7]}).set_index(""i1"")\r\ndf1.join(df2, rsuffix=""_"")\r\n```'"
2839,10842973,OneWind,y-p,2013-02-11 04:03:15,2014-10-24 14:45:37,2013-07-29 05:07:06,closed,,Someday,6,Bug;Can't Repro;Data IO,https://api.github.com/repos/pydata/pandas/issues/2839,"b""Can't read in 1GB data""","b'I am trying to repeat the code in:\r\nhttp://wesmckinney.com/blog/?p=635\r\nBut I can\'t read in the data. I have more than 4GB memory available (with 8GB memory in total), but still has the following problem:\r\n\r\nIn [2]: data = pd.read_csv(""P00000001-ALL.csv"", index_col=False)\r\nPython(9081,0xacb01a28) malloc: *** mmap(size=24051712) failed (error code=12)\r\n*** error: can\'t allocate region\r\n*** set a breakpoint in malloc_error_break to debug'"
2830,10820515,changhiskhan,y-p,2013-02-10 09:39:48,2013-03-17 03:24:40,2013-03-17 03:24:40,closed,,0.11,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2830,b'possible version compat issue with MPL',b'http://stackoverflow.com/questions/14795787/python-matplotlib-typeerror-dont-know-how-to-convert-scalar-number-to-float'
2827,10814136,wesm,changhiskhan,2013-02-09 20:42:12,2013-04-08 15:06:06,2013-04-08 15:06:06,closed,,0.11,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2827,b'Bug with color cycles and multiple series plots',b'http://stackoverflow.com/questions/14287903/how-do-you-make-plotting-two-pandas-series-in-the-same-ipython-notebook-cell-use'
2817,10765502,femibyte,jreback,2013-02-08 02:11:43,2013-03-21 16:02:36,2013-03-21 16:02:15,closed,,0.11,10,Bug,https://api.github.com/repos/pydata/pandas/issues/2817,b'multilevel df.ix fails with IndexError on Multindex with duplicate entries',"b""The following example works:\r\n\r\ndf = DataFrame(np.arange(20).reshape(5,4))\r\ndf2 = df.set_index(keys=[0,1,2])\r\ndf2.ix[(4,5,6)]\r\n\r\nfor using a MultiIndex\r\n\r\nSo I created a file sample_data.csv that looks like this:\r\n\r\ncol1,col2,year,amount \r\n111111,3.5,2012,700 \r\n111112,3.5,2011,600 \r\n222221,4.0,2012,222 \r\n... \r\n\r\nI then ran the following:\r\n\r\nimport numpy as np \r\nimport pandas as pd \r\nsd=pd.read_csv('sample_data.csv') \r\nsd2=sd.set_index(keys=['col2','year']) \r\nsd2.ix[(4.0,2012)] \r\n\r\nBut this produces the following error: IndexError: index out of bounds\r\nCan anyone explain why the former example works, but the latter doesn't ?"""
2815,10762903,lesteve,changhiskhan,2013-02-08 00:11:57,2013-02-09 20:41:29,2013-02-09 20:41:29,closed,changhiskhan,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2815,b'pandas.Series.apply fails with IndexError with an empty series',"b'```python\r\nimport pandas as pd\r\ns = pd.Series()\r\ns.apply(lambda x: x)\r\n```\r\n```python\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-3-a33a9027c226> in <module>()\r\n----> 1 s.apply(lambda x: x)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc in apply(self, func, convert_dtype, args, **kwds)\r\n   2295 \r\n   2296         mapped = lib.map_infer(values, f, convert=convert_dtype)\r\n-> 2297         if isinstance(mapped[0], Series):\r\n   2298             from pandas.core.frame import DataFrame\r\n   2299             return DataFrame(mapped.tolist(), index=self.index)\r\n\r\nIndexError: index out of bounds\r\n```\r\nThis happens since 0.10.0 because of the new feature which upcasts the returned object to a dataframe if the function passed as the ```func``` argument returns a Series.'"
2814,10759616,wesm,y-p,2013-02-07 22:30:18,2013-03-18 05:43:20,2013-03-18 05:43:20,closed,,0.11,1,Bug;Can't Repro,https://api.github.com/repos/pydata/pandas/issues/2814,b'Regression? Using Series for string formatting',"b'from mailing list:\r\n\r\nRows used to be able to work as maps for string interpolation, like this:\r\n\r\nfrom pandas import *\r\n\r\nspeeds = read_csv(\'results.csv\')\r\nrow = speeds.ix[3]\r\nprint(""%(my_column_name)s"" % row)\r\n \r\nthat worked a few months ago, but doesn\'t seem to work in the latest Pandas. How can I turn a row into a map, or otherwise do simple & concise custom printing of rows?\r\n'"
2811,10753912,wesm,y-p,2013-02-07 20:21:56,2013-03-22 07:11:16,2013-03-22 07:11:16,closed,wesm,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2811,b'kind argument in Series.order has no effect',"b'Reported by user, always does mergesort'"
2809,10747976,dalejung,wesm,2013-02-07 17:47:21,2013-02-10 22:34:41,2013-02-10 22:34:41,closed,,0.11,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2809,b'Series does not upconvert datetimes to datetime64',"b'```python\r\nimport pandas as pd\r\nind = pd.date_range(start=""2000-01-01"", freq=""D"", periods=10)\r\ndatetimes = [ts.to_pydatetime() for ts in ind]\r\n\r\ndatetime_s = pd.Series(datetimes)\r\ndatetime_s.dtype\r\n# object\r\ndf = pd.DataFrame({\'datetime_s\':datetime_s})\r\nprint df.dtypes\r\n#datetime64\r\n```\r\n\r\nShouldn\'t Series upconvert to datetime64 since the DataFrame columns do?'"
2808,10738395,l736x,jreback,2013-02-07 14:29:32,2013-09-22 21:18:01,2013-09-21 23:58:52,closed,,0.13,7,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/2808,b'Groupby - apply concatenation of indexes depends on the order of the groups',"b'Sometimes I need to ""apply"" complicated functions that return differently shaped dataframes.\r\nIn these situations I often need to handle explicitly the case in which the result of the function is empty and return a properly shaped empty dataframe.\r\n\r\nFailing to return a proper df gives normally an exception, but I found a case where the exception is thrown depending on the order of the groups:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: \'0.10.1\'\r\n```\r\n\r\n```python\r\ndef f1(x):\r\n    y = x[(x.b % 2) == 1]**2\r\n    if y.empty:\r\n        multiindex = pd.MultiIndex(\r\n                levels = [[]]*2,\r\n                labels = [[]]*2,\r\n                names = [\'b\', \'c\']\r\n        )\r\n        res = pd.DataFrame(None,\r\n                           columns=[\'a\'],\r\n                           index=multiindex)\r\n        return res\r\n    else:\r\n        y = y.set_index([\'b\',\'c\'])\r\n        return y\r\n\r\ndef f2(x):\r\n    y = x[(x.b % 2) == 1]**2\r\n    if y.empty:\r\n        return pd.DataFrame()\r\n    else:\r\n        y = y.set_index([\'b\',\'c\'])\r\n        return y\r\n\r\ndef f3(x):\r\n    y = x[(x.b % 2) == 1]**2\r\n    if y.empty:\r\n        multiindex = pd.MultiIndex(\r\n                levels = [[]]*2,\r\n                labels = [[]]*2,\r\n                names = [\'foo\', \'bar\']\r\n        )\r\n        res = pd.DataFrame(None,\r\n                           columns=[\'a\',\'b\'],\r\n                           index=multiindex)\r\n        return res\r\n    else:\r\n        return y\r\n\r\ndf = pd.DataFrame({\'a\':[1,2,2,2],\r\n                   \'b\':range(4),\r\n                   \'c\':range(5,9)})\r\n\r\ndf2 = pd.DataFrame({\'a\':[3,2,2,2],\r\n                    \'b\':range(4),\r\n                    \'c\':range(5,9)})\r\n```\r\n\r\nf1 is the correct function and it works\r\nf2 is wrong because it returns an empty dataframe with a simple index and it fails (as it should)\r\nf3 is wrong but the exception is thrown only with df2\r\n\r\n```python\r\nIn [4]: df.groupby(\'a\').apply(f1)\r\nOut[4]:\r\n        a\r\na b c\r\n2 1 36  4\r\n  9 64  4\r\n\r\nIn [5]: df2.groupby(\'a\').apply(f1)\r\nOut[5]:\r\n        a\r\na b c\r\n2 1 36  4\r\n  9 64  4\r\n```\r\n\r\n```python\r\nIn [6]: df.groupby(\'a\').apply(f3)\r\nOut[6]:\r\n     a  b   c\r\na\r\n2 1  4  1  36\r\n  3  4  9  64\r\n\r\nIn [7]: df2.groupby(\'a\').apply(f3)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n/home/ldeleo/<ipython console> in <module>()\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in apply(self, func, *args, **kwargs)\r\n    320         func = _intercept_function(func)\r\n    321         f = lambda g: func(g, *args, **kwargs)\r\n--> 322         return self._python_apply_general(f)\r\n    323\r\n    324     def _python_apply_general(self, f):\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in _python_apply_general(self, f)\r\n    326\r\n    327         return self._wrap_applied_output(keys, values,\r\n--> 328                                          not_indexed_same=mutated)\r\n    329\r\n    330     def aggregate(self, func, *args, **kwargs):\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in _wrap_applied_output(self, keys, values, not_indexed_same)\r\n   1742         if isinstance(values[0], DataFrame):\r\n   1743             return self._concat_objects(keys, values,\r\n-> 1744                                         not_indexed_same=not_indexed_same)\r\n   1745         elif hasattr(self.grouper, \'groupings\'):\r\n   1746             if len(self.grouper.groupings) > 1:\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in _concat_objects(self, keys, values, not_indexed_same)\r\n    483             group_names = self.grouper.names\r\n    484             result = concat(values, axis=self.axis, keys=group_keys,\r\n--> 485                             levels=group_levels, names=group_names)\r\n    486         else:\r\n    487             result = concat(values, axis=self.axis)\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity)\r\n    892                        ignore_index=ignore_index, join=join,\r\n    893                        keys=keys, levels=levels, names=names,\r\n--> 894                        verify_integrity=verify_integrity)\r\n    895     return op.get_result()\r\n    896\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity)\r\n    962         self.verify_integrity = verify_integrity\r\n    963\r\n--> 964         self.new_axes = self._get_new_axes()\r\n    965\r\n    966     def get_result(self):\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in _get_new_axes(self)\r\n   1134             concat_axis = None\r\n   1135         else:\r\n-> 1136             concat_axis = self._get_concat_axis()\r\n   1137\r\n   1138         new_axes[self.axis] = concat_axis\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in _get_concat_axis(self)\r\n   1169         else:\r\n   1170             concat_axis = _make_concat_multiindex(indexes, self.keys,\r\n-> 1171                                                   self.levels, self.names)\r\n   1172\r\n   1173         self._maybe_check_integrity(concat_axis)\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in _make_concat_multiindex(indexes, keys, levels, names)\r\n   1243             names = names + _get_consensus_names(indexes)\r\n   1244\r\n-> 1245         return MultiIndex(levels=levels, labels=label_list, names=names)\r\n   1246\r\n   1247     new_index = indexes[0]\r\n\r\n/home/ldeleo/.local/lib/python2.7/site-packages/pandas-0.10.1-py2.7-linux-x86_64.egg/pandas/core/index.pyc in __new__(cls, levels, labels, sortorder, names)\r\n   1343             if len(names) != subarr.nlevels:\r\n   1344                 raise AssertionError((\'Length of names must be same as level \'\r\n-> 1345                                       \'(%d), got %d\') % (subarr.nlevels))\r\n   1346\r\n   1347             subarr.names = list(names)\r\n\r\nTypeError: not enough arguments for format string\r\n```\r\n\r\nThere are actually two bugs. The first is simply that instead of (subarr.nlevels) on line 1345 there should be (subarr.nlevels, len(names))\r\n\r\nThe second is more serious and goes back to _get_consensus_names (index.py:2671)\r\nI guess that the task of this function is, given a list of possibly different indexes, to give back the minimal ""shape"" that can contain all the indexes.\r\nOnly that what it does now is to give back the ""shape"" of the second type encountered along the list.\r\n\r\nFor this reason, when applied to df, it acts on\r\n\r\n[Multiindex[], Int64Index([1,3])]\r\n\r\ngiving back [[None]]\r\n\r\nWhen applied to df2 which differs only by the order of the groups \r\n\r\n[Int64Index([1,3]), Multiindex[]]\r\n\r\nit gives back [[None], [None]] and then fails.\r\n\r\nI agree that the case I showed is quite abstruse, but the non predictability of the behavior is quite dangerous in my opinion.'"
2807,10718752,cpcloud,y-p,2013-02-07 00:28:07,2014-01-16 15:24:08,2013-03-12 17:54:32,closed,,0.11,27,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2807,b'Slow printing of large data frames',b'I have DataFrames with about 14 million rows and 16 columns and I have to wait at least 3-4 seconds for it to `repr` in an IPython session. Is there anything that can be done about this?'
2806,10698256,hayd,y-p,2013-02-06 16:21:36,2013-03-14 03:40:33,2013-03-14 03:40:33,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2806,"b""string contains method doesn't respect na argument""","b""It seems that the argument `na` doesn't remove the NaN with `str.contains` (see [previous issue](https://github.com/pydata/pandas/issues/1689)).\r\n\r\nUsing similar to example in [test added for na functionality](https://github.com/pydata/pandas/commit/7a1ea0a3d189b1be7cdd23a7b98f79effd381430):\r\n\r\n```\r\nIn [1]: values = Series(['om', NA, 'foo_nom', 'nom', 'bar_foo', NA, 'foo'])\r\n\r\nIn [2]: values.str.contains('foo', na=False)\r\nOut[2]: \r\n0    False\r\n1      NaN\r\n2     True\r\n3    False\r\n4     True\r\n5      NaN\r\n6     True\r\n\r\nIn [3]: values.str.startswith('foo', na=False)\r\nOut[3]: \r\n0    False\r\n1    False\r\n2     True\r\n3    False\r\n4    False\r\n5    False\r\n6     True\r\n\r\n```\r\n\r\n*From [Stackoverflow question](http://stackoverflow.com/questions/14731714/value-error-when-slicing-in-pandas/14731958).*"""
2805,10680550,changhiskhan,jreback,2013-02-06 05:22:17,2014-02-16 22:16:17,2014-02-16 22:16:17,closed,changhiskhan,0.14.0,9,Bug;Data IO;IO Google,https://api.github.com/repos/pydata/pandas/issues/2805,b'Google Analytics chunksize issue',b'http://stackoverflow.com/questions/14696717/missing-records-using-chunksize-pandas-and-google-analytics-api-integration'
2803,10673030,benjello,y-p,2013-02-05 23:21:20,2013-12-04 00:40:16,2013-03-18 05:33:29,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2803,b'bug in density attribute computation for SparseSeries',"b""    ts = Series(randn(10))\r\n    ts[2:-2] = nan\r\n    sts = ts.to_sparse()\r\n    print sts\r\n    print sts.density\r\n\r\nresults in:\r\n\r\n  return float(len(self.sp_index)) / len(self.index)\r\nTypeError: object of type 'pandas._sparse.BlockIndex' has no len()\r\n"""
2800,10668017,adamsd5,jreback,2013-02-05 21:17:38,2013-09-13 00:05:49,2013-09-13 00:05:49,closed,jreback,0.13,3,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/2800,"b""Series with dtype='M8[us]' does not create NaT values""","b""Using Pandas 0.10.1, python 2.7, and Numpy 1.6.2:\r\n\r\n    >>> npa = numpy.ndarray(1, dtype=numpy.int64)\r\n    >>> npa[0] = -9223372036854775808\r\n    >>> type(pandas.Series(npa, dtype='M8[ns]')[0])\r\n    <class 'pandas.tslib.NaTType'>\r\n    >>> type(pandas.Series(npa, dtype='M8[us]')[0])\r\n    <class 'pandas.tslib.Timestamp'>\r\n\r\nThe latter should also have class 'pandas.tslib.NaTType', right?"""
2797,10620846,dhirschfeld,wesm,2013-02-04 19:06:25,2013-03-28 05:51:14,2013-03-28 05:51:14,closed,wesm,0.11,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2797,b'BUG: `concat` throwing TypeError when `ignore_index` is True',"b'```python\r\nIn [24]: dates = pd.date_range(\'01-Jan-2013\', \'01-Jan-2014\', freq=\'MS\')[0:-1]\r\n    ...: s1 = pd.Series(randn(len(dates)), index=dates, name=\'value\')    \r\n    ...: s2 = pd.Series(randn(len(dates)), index=dates, name=\'value\')\r\n\r\n\r\nIn [31]: df = pd.concat([s1, s2], axis=1, ignore_index=True)\r\n Traceback (most recent call last):\r\n\r\n   File ""<ipython-input-31-66b3acfe3258>"", line 1, in <module>\r\n     df = pd.concat([s1, s2], axis=1, ignore_index=True)\r\n\r\n   File ""c:\\dev\\code\\pandas\\pandas\\tools\\merge.py"", line 895, in concat\r\n     return op.get_result()\r\n\r\n   File ""c:\\dev\\code\\pandas\\pandas\\tools\\merge.py"", line 972, in get_result\r\n     data = dict(zip(self.new_axes[1], self.objs))\r\n\r\n TypeError: zip argument #1 must support iteration\r\n```'"
2789,10530965,stephenwlin,wesm,2013-02-01 07:16:32,2013-02-10 02:11:16,2013-02-10 02:11:16,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2789,b'BUG: Timestamp constructor not handling timezone conversions correctly',"b'`Timestamp` constructor raises an exception when given iso8601 string with UTC offset and a desired timezone. When given `datetime` object with timezone and a desired timezone, it gives a correct string but incorrect internal timestamp value.\r\n\r\n```python\r\nIn [1]: from pandas import Timestamp as ts, DataFrame as df\r\nIn [2]: from datetime import datetime as dt\r\nIn [3]: from pytz import timezone as tz\r\nIn [4]: UTC=\'UTC\'\r\nIn [5]: EST=\'US/Eastern\'\r\nIn [6]: CST=\'US/Central\'\r\nIn [7]: # all the following should have the same timestamp value\r\nIn [8]: # (commented lines throw exceptions!!)\r\nIn [9]: tss=[ts(""2012-01-02 01:00:00+00:00""),\r\n   ...: #    ts(""2012-01-02 01:00:00+00:00"", tz=EST),\r\n   ...:      ts(""2012-01-01 20:00:00"", tz=EST),\r\n   ...:      ts(dt(2012,1,1,20,0,0,tzinfo=tz(EST))),\r\n   ...:      ts(dt(2012,1,1,19,0,0,tzinfo=tz(CST))),\r\n   ...:      ts(dt(2012,1,1,20,0,0,tzinfo=tz(EST)), tz=EST),\r\n   ...:      ts(dt(2012,1,1,19,0,0,tzinfo=tz(CST)), tz=EST),\r\n   ...:      ts(""2012-01-01 20:00:00-05:00""),\r\n   ...:      ts(""2012-01-01 19:00:00-06:00""),\r\n   ...: #    ts(""2012-01-01 20:00:00-05:00"", tz=EST),\r\n   ...: #    ts(""2012-01-01 19:00:00-06:00"", tz=EST),\r\n   ...:      ts(1325466000000000000, tz=UTC),\r\n   ...:      ts(1325466000000000000, tz=EST)]\r\n\r\nIn [10]: strs = [str(ts) for ts in tss]\r\nIn [11]: vals = [(ts.value/1000000000000) for ts in tss]\r\nIn [12]: df({\'strs\' : strs, \'vals\' : vals})\r\nOut[12]: \r\n                        strs     vals\r\n0  2012-01-02 01:00:00+00:00  1325466\r\n1  2012-01-01 20:00:00-05:00  1325466\r\n2  2012-01-01 20:00:00-05:00  1325466\r\n3  2012-01-01 19:00:00-06:00  1325466\r\n4  2012-01-01 20:00:00-05:00  1325448 <-- bad value!!\r\n5  2012-01-01 20:00:00-05:00  1325448 <-- bad value!!\r\n6  2012-01-01 20:00:00-05:00  1325466\r\n7  2012-01-01 19:00:00-06:00  1325466\r\n8  2012-01-02 01:00:00+00:00  1325466\r\n9  2012-01-01 20:00:00-05:00  1325466\r\n```'"
2788,10527075,stephenwlin,wesm,2013-02-01 02:45:18,2013-02-10 02:11:09,2013-02-10 02:11:09,closed,,0.11,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2788,b'BUG: Slicing and setitem inconsistent with getitem using DateTimeIndex with timezone',"b'`__getitem__` with date string key interprets a string label in the index\'s timezone when none provided, but `__getitem__` with date string slices and `__setitem__` with string label/slices do not\r\n\r\n```python\r\nIn [1]: import pandas as p\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: index=p.date_range(""2012-01-01"", ""2012-01-02"",\r\n   ...:                    freq=\'60min\', tz=\'US/Eastern\')\r\n\r\nIn [4]: ts=p.Series(np.random.randn(len(index)), index)\r\n\r\nIn [5]: ts[""2012-01-01 15:00:00"":""2012-01-02 01:00:00""] \\\r\n   ...:     # off by five hours\r\nOut[5]: \r\n2012-01-01 10:00:00-05:00   -1.294343\r\n2012-01-01 11:00:00-05:00   -0.091218\r\n2012-01-01 12:00:00-05:00    0.049017\r\n2012-01-01 13:00:00-05:00   -1.068893\r\n2012-01-01 14:00:00-05:00   -0.470317\r\n2012-01-01 15:00:00-05:00    0.894762\r\n2012-01-01 16:00:00-05:00    0.223070\r\n2012-01-01 17:00:00-05:00   -0.414907\r\n2012-01-01 18:00:00-05:00   -0.925703\r\n2012-01-01 19:00:00-05:00    0.879205\r\n2012-01-01 20:00:00-05:00   -1.580123\r\nFreq: 60T\r\n\r\nIn [6]: ts[""2012-01-01 20:00:00""] # ok\r\nOut[6]: -1.5801231823927993\r\n\r\nIn [7]: ts[""2012-01-01 15:00:00""] # ok\r\nOut[7]: 0.89476211630990787\r\n\r\nIn [8]: ts[""2012-01-01 20:00:00""] = 1 # off by five hours\r\n\r\nIn [9]: ts[""2012-01-01 20:00:00""]\r\nOut[9]: -1.5801231823927993\r\n\r\nIn [10]: ts[""2012-01-01 15:00:00""]\r\nOut[10]: 1.0\r\n\r\nIn [11]: ts[""2012-01-01 20:00:00-05:00""] = 1 # ok\r\n\r\nIn [12]: ts[""2012-01-01 20:00:00""]\r\nOut[12]: 1.0\r\n```'"
2787,10524476,scottkidder,jreback,2013-02-01 00:45:30,2013-03-22 14:39:26,2013-03-22 14:39:26,closed,,0.11,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2787,b'DatetimeIndex.resolution throws NameError',"b""Attempting to retrieve the resolution attribute throws this exception:\r\n\r\n``` python\r\n34    @classmethod\r\n35    def get_str(cls, reso):\r\n36        return {RESO_US : 'microsecond',\r\n37                    RESO_SEC : 'second',\r\n38                    RESO_MIN: 'minute',\r\nNameError: global name 'RESO_US' is not defined\r\n```"""
2786,10520715,vincentarelbundock,y-p,2013-01-31 22:36:00,2013-05-03 17:21:26,2013-04-01 16:15:49,closed,,0.11,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2786,b'df.applymap duplicates data with frame has dupe columns',"b""Using pandas master/github. In this example, we get 4 times the same column after applying an element-wise operation: \r\n\r\n    In [1]: import numpy as np\r\n    In [2]: import pandas as pd\r\n    In [3]: df = pd.DataFrame(np.random.random((3,4)))\r\n    In [4]: cols = pd.Index(['a','a','a','a'])\r\n    In [5]: df.columns = cols\r\n    In [6]: df.applymap(str)\r\n    Out[6]: \r\n                    a               a               a               a\r\n    0  0.320051885413  0.320051885413  0.320051885413  0.320051885413\r\n    1  0.967238549103  0.967238549103  0.967238549103  0.967238549103\r\n    2  0.913201809648  0.913201809648  0.913201809648  0.913201809648"""
2784,10507695,jostheim,jostheim,2013-01-31 17:00:58,2013-02-18 23:07:47,2013-02-18 23:07:47,closed,,Someday,26,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2784,b'Another HDFStore error',"b'After a long run of extracting features for some random forest action I ran into this when serializing the features:\r\n\r\nTraceback (most recent call last):\r\n  File ""XXXXX.py"", line 1043, in <module>\r\n    write_dataframe(""features"", all_df, store)\r\n  File ""XXXXX.py"", line 55, in write_dataframe\r\n    store[name] = df\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 218, in __setitem__\r\n    self.put(key, value)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 458, in put\r\n    self._write_to_group(key, value, table=table, append=append, **kwargs)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 788, in _write_to_group\r\n    s.write(obj = value, append=append, complib=complib, **kwargs)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 1837, in write\r\n    self.write_array(\'block%d_values\' % i, blk.values)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 1639, in write_array\r\n    self.handle.createArray(self.group, key, value)\r\n  File ""/Library/Python/2.7/site-packages/tables-2.4.0-py2.7-macosx-10.8-intel.egg/tables/file.py"", line 780, in createArray\r\n    object=object, title=title, byteorder=byteorder)\r\n  File ""/Library/Python/2.7/site-packages/tables-2.4.0-py2.7-macosx-10.8-intel.egg/tables/array.py"", line 167, in __init__\r\n    byteorder, _log)\r\n  File ""/Library/Python/2.7/site-packages/tables-2.4.0-py2.7-macosx-10.8-intel.egg/tables/leaf.py"", line 263, in __init__\r\n    super(Leaf, self).__init__(parentNode, name, _log)\r\n  File ""/Library/Python/2.7/site-packages/tables-2.4.0-py2.7-macosx-10.8-intel.egg/tables/node.py"", line 250, in __init__\r\n    self._v_objectID = self._g_create()\r\n  File ""/Library/Python/2.7/site-packages/tables-2.4.0-py2.7-macosx-10.8-intel.egg/tables/array.py"", line 200, in _g_create\r\n    nparr, self._v_new_title, self.atom)\r\n  File ""hdf5Extension.pyx"", line 884, in tables.hdf5Extension.Array._createArray (tables/hdf5Extension.c:8498)\r\ntables.exceptions.HDF5ExtError: Problems creating the Array.\r\n\r\nThe error is pretty undefined, I know the table I was writing was big,  >17000 columns by > 20000 rows.  There are lots of np.nan\'s in the columns.\r\n\r\nSince I seem to be one of the few who are serializing massive sets, and I have a 64GB RAM machine sitting next to me, are there some test cases that I can run, or write that would help?  Thinking setting up large mixed dataframes etc...'"
2782,10500067,dsimpson1980,wesm,2013-01-31 13:27:26,2013-02-10 02:11:03,2013-02-10 02:11:03,closed,,0.11,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2782,b'Period slicing with period_range returns error',"b'Slicing a TimeSeries object with a PeriodIndex fails with an index out of bounds error\r\n\r\nExample\r\n\r\ndates = pd.period_range(start=\'01-Apr-11\',end=\'31-Mar-12 23:00\',freq=\'h\')\r\nts = pd.TimeSeries(0,dates)\r\nts[ts.index[4:10]]\r\n\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-2-c28eee792d28>"", line 3, in <module>\r\n    ts[ts.index[4:10]]\r\n\r\n  File ""C:\\dev\\bin\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 506, in __getitem__\r\n    return self._get_with(key)\r\n\r\n  File ""C:\\dev\\bin\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 544, in _get_with\r\n    return self._get_values(key)\r\n\r\n  File ""C:\\dev\\bin\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 575, in _get_values\r\n    return self.values[indexer]\r\n\r\nIndexError: index 361564 out of bounds 0<=index<8784\r\n\r\nIt appears the PeriodIndex is being inferred as \'integer\' in series._get_with() using lib.infer_dtype() whereas self.index.inferred_type returns \'period\'\r\n\r\n            if isinstance(key, Index):\r\n                key_type = lib.infer_dtype(key.values)\r\n            else:\r\n                key_type = lib.infer_dtype(key)\r\n\r\n            if key_type == \'integer\':\r\n                if self.index.inferred_type == \'integer\':\r\n                    return self.reindex(key)\r\n                else:\r\n                    return self._get_values(key)\r\n            elif key_type == \'boolean\':\r\n                return self._get_values(key)\r\n\r\n'"
2781,10496294,dhirschfeld,wesm,2013-01-31 11:06:47,2013-03-28 05:40:23,2013-03-28 05:40:23,closed,,0.11,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2781,"b""Periods don't sort correctly""","b""Unless I'm mistaken I think the following unit-test should pass:\r\n\r\n```python\r\ndef test_period_sorting():\r\n    periods = [pd.Period('2008-04', 'M'),\r\n               pd.Period('2008-02', 'M'),\r\n               pd.Period('2008-01', 'M'),\r\n               pd.Period('2008-05', 'M'),\r\n               pd.Period('2008-06', 'M'),\r\n               pd.Period('2008-03', 'M'),\r\n               pd.Period('2008-04', 'M'),\r\n               pd.Period('2008-02', 'M'),\r\n               pd.Period('2008-01', 'M'),\r\n               pd.Period('2008-05', 'M'),\r\n               pd.Period('2008-06', 'M'),\r\n               pd.Period('2008-03', 'M'),\r\n               pd.Period('2008-04', 'M'),\r\n               pd.Period('2008-02', 'M'),\r\n               pd.Period('2008-01', 'M'),\r\n               pd.Period('2008-05', 'M'),\r\n               pd.Period('2008-06', 'M'),\r\n               pd.Period('2008-03', 'M')]\r\n    assert sorted([p.to_timestamp() for p in periods]) \\\r\n        == [p.to_timestamp() for p in sorted(periods)]\r\n#\r\n```\r\n"""
2775,10460160,gtakacs,wesm,2013-01-30 14:25:05,2013-02-10 22:37:40,2013-02-10 22:37:40,closed,,0.11,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2775,b'segmentation fault in fillna',"b'The following code causes segmentation fault (tried with pandas 0.10.1):\r\npandas.DataFrame(columns=[""x""]).x.fillna(method=""pad"", inplace=1)\r\n'"
2773,10445571,jostheim,jostheim,2013-01-30 04:15:58,2015-06-19 20:32:33,2013-03-10 19:48:59,closed,,0.11,26,Bug,https://api.github.com/repos/pydata/pandas/issues/2773,b'Error in storing large dataframe to HDFStore',"b'HDFstore is working much better with the latest release, but I am encountering a new error I wanted to report:\r\n\r\ndf = get_joined_data(data_prefix, data_rev_prefix, date_prefix, store_filename)\r\n  File ""XXXXXX"", line 739, in get_joined_data\r\n    write_dataframe(""joined_{0}"".format(date_prefix), df, store)\r\n  File ""XXXXXXX"", line 55, in write_dataframe\r\n    store[name] = df\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 218, in __setitem__\r\n    self.put(key, value)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 458, in put\r\n    self._write_to_group(key, value, table=table, append=append, **kwargs)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 788, in _write_to_group\r\n    s.write(obj = value, append=append, complib=complib, **kwargs)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 1837, in write\r\n    self.write_array(\'block%d_values\' % i, blk.values)\r\n  File ""/Library/Python/2.7/site-packages/pandas/io/pytables.py"", line 1627, in write_array\r\n    vlarr.append(value)\r\n  File ""/Library/Python/2.7/site-packages/tables-2.4.0-py2.7-macosx-10.8-intel.egg/tables/vlarray.py"", line 480, in append\r\n    self._append(nparr, nobjects)\r\n  File ""hdf5Extension.pyx"", line 1499, in tables.hdf5Extension.VLArray._append (tables/hdf5Extension.c:13764)\r\nOverflowError: value too large to convert to int\r\n\r\nNot at all sure this is an actual pandas issue, but thought I would report it nonetheless.'"
2769,10424435,y-p,jreback,2013-01-29 16:58:07,2013-02-11 03:22:33,2013-02-11 03:22:33,closed,,0.11,9,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/2769,"b""TST: HDFStore tests don't clean up files after KeyboardInterrupt""","b'Cleanup code exists in the `tearDown` method, but a `KeyboardInterrupt`\r\neither doesn\'t run the `tearDown`, or maybe the file handle isn\'t closed so \r\nthe `os.remove` fails. \r\n\r\nOne example which left behind a file in the root of the source tree.\r\n\r\n```python\r\n\xa6\xcb nosetests pandas/io/tests/test_pytables.py\r\n.S......SSSS..^CTraceback (most recent call last):\r\n  File ""/usr/local/bin/nosetests"", line 9, in <module>\r\n    load_entry_point(\'nose==1.2.1\', \'console_scripts\', \'nosetests-2.7\')()\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/core.py"", line 118, in __init__\r\n    **extra_args)\r\n  File ""/usr/lib/python2.7/unittest/main.py"", line 95, in __init__\r\n    self.runTests()\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/core.py"", line 197, in runTests\r\n    result = self.testRunner.run(self.test)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/core.py"", line 61, in run\r\n    test(result)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/suite.py"", line 176, in __call__\r\n    return self.run(*arg, **kw)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/suite.py"", line 223, in run\r\n    test(orig)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/suite.py"", line 176, in __call__\r\n    return self.run(*arg, **kw)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/suite.py"", line 223, in run\r\n    test(orig)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/suite.py"", line 176, in __call__\r\n    return self.run(*arg, **kw)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/suite.py"", line 223, in run\r\n    test(orig)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 45, in __call__\r\n    return self.run(*arg, **kwarg)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 133, in run\r\n    self.runTest(result)\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 151, in runTest\r\n    test(result)\r\n  File ""/usr/lib/python2.7/unittest/case.py"", line 396, in __call__\r\n    return self.run(*args, **kwds)\r\n  File ""/usr/lib/python2.7/unittest/case.py"", line 332, in run\r\n    testMethod()\r\n  File ""/home/user1/src/pandas/pandas/io/tests/test_pytables.py"", line 1690, in test_coordinates\r\n    self.store.append(\'df1\', df1, data_columns=[\'A\', \'B\'])\r\n  File ""/home/user1/src/pandas/pandas/io/pytables.py"", line 532, in append\r\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\r\n  File ""/home/user1/src/pandas/pandas/io/pytables.py"", line 788, in _write_to_group\r\n    s.write(obj = value, append=append, complib=complib, **kwargs)\r\n  File ""/home/user1/src/pandas/pandas/io/pytables.py"", line 2502, in write\r\n    self.set_attrs()\r\n  File ""/home/user1/src/pandas/pandas/io/pytables.py"", line 1984, in set_attrs\r\n    self.attrs.table_type = self.table_type\r\n  File ""/usr/local/lib/python2.7/dist-packages/tables/attributeset.py"", line 444, in __setattr__\r\n    self._g__setattr(name, value)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tables/attributeset.py"", line 386, in _g__setattr\r\n    self._g_setAttr(self._v_node, name, stvalue)\r\n  File ""hdf5Extension.pyx"", line 419, in tables.hdf5Extension.AttributeSet._g_setAttr (tables/hdf5Extension.c:3928)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tables/atom.py"", line 368, in from_dtype\r\n    return class_.from_kind(\'string\', itemsize, dtype.shape, dflt)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tables/atom.py"", line 456, in from_kind\r\n    kwargs[\'itemsize\'] = itemsize\r\nKeyboardInterrupt\r\nClosing remaining open files: __26Weihln5J__.h5... done\r\n```\r\n'"
2768,10418350,dhirschfeld,wesm,2013-01-29 14:40:42,2013-04-08 07:47:26,2013-04-08 07:47:26,closed,,0.11,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2768,b'Incorrect calculation of weekofyear?',"b'The calculation of the week/weekofyear attribute of a Timestamp/DatetimeIndex differs from both python and mx.DateTime which both return `week = 1` for 31-Dec-2013.\r\n\r\n```python\r\nIn [25]: import mx.DateTime\r\n    ...: from datetime import datetime\r\n    ...: d = datetime(2013,12,31)\r\n    ...: \r\n\r\nIn [26]: d.isocalendar()\r\nOut[26]: (2014, 1, 2)\r\n\r\nIn [27]: mx.DateTime.DateTimeFrom(d).iso_week\r\nOut[27]: (2014, 1, 2)\r\n\r\nIn [28]: pd.Timestamp(d).week\r\nOut[28]: 53\r\n```'"
2767,10401479,wesm,wesm,2013-01-29 01:49:03,2013-01-29 02:53:21,2013-01-29 02:53:21,closed,,,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2767,b'Scatter matrix bug?',b'http://stackoverflow.com/questions/14511752/pandas-3x3-scatter-matrix-missing-labels'
2765,10393466,stephenwlin,wesm,2013-01-28 21:30:54,2013-02-10 22:38:03,2013-02-10 22:38:03,closed,,0.11,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2765,b'BUG: Various inconsistencies in DataFrame __getitem__ and __setitem__ behavior',"b'##### *\\_\\_setitem\\_\\_* fails with row slices supported by *\\_\\_getitem\\_\\_*\r\n```python\r\nIn [27]: df=p.DataFrame(\r\n   ....:              {""alpha"": [""a"", ""b"", ""c"", ""d"", ""e""],\r\n   ....:              ""beta"":[1,2,3,4,5]},\r\n   ....:              columns=[""alpha"",""beta""],\r\n   ....:              index=[1.0, 2.5, 3, 4.5, 5.0])\r\n\r\nIn [28]: df[0:1] # does work, slices on rows\r\nOut[28]: \r\n  alpha  beta\r\n1     a     1\r\n\r\nIn [29]: df[0:1] = 0 # fails\r\n(exception)\r\n```\r\n\r\n##### *\\_\\_setitem\\_\\_* fails on boolean indexing unless key is *ndarray*, while *\\_\\_getitem\\_\\_* casts appropriately\r\n```python\r\nIn [49]: df=p.DataFrame(\r\n   ....:              {""alpha"": [""a"", ""b"", ""c"", ""d"", ""e""],\r\n   ....:              ""beta"":[1,2,3,4,5]},\r\n   ....:              columns=[""alpha"",""beta""],\r\n   ....:              index=[1.0, 2.5, 3, 4.5, 5.0])\r\n\r\nIn [50]: df[np.array([True, False, False, False, True])]\r\nOut[50]: \r\n  alpha  beta\r\n1     a     1\r\n5     e     5\r\n\r\nIn [51]: df[[True, False, False, False, True]]\r\nOut[51]: \r\n  alpha  beta\r\n1     a     1\r\n5     e     5\r\n\r\nIn [52]: df[np.array([True, False, False, False, True])] = 4 # does work\r\n\r\nIn [53]: df[[True, False, False, False, True]] = 3 # does not work\r\n(exception)\r\n```\r\n\r\n##### *\\_\\_getitem\\_\\_* does not reindex boolean *Series* key but *\\_\\_setitem\\_\\_* does\r\n```python\r\nIn [54]: df=p.DataFrame(\r\n   ....:              {""alpha"": [""a"", ""b"", ""c"", ""d"", ""e""],\r\n   ....:              ""beta"":[1,2,3,4,5]},\r\n   ....:              columns=[""alpha"",""beta""],\r\n   ....:              index=[1.0, 2.5, 3, 4.5, 5.0])\r\n\r\nIn [55]: s=p.Series([True, True, False, False, False],\r\n   ....:              index=[5.0, 4.5, 3, 2.5, 1.0]) # row-reversed index\r\n\r\nIn [56]: df[s] # gets first two rows (no reindex)\r\nOut[56]: \r\n    alpha  beta\r\n1.0     a     1\r\n2.5     b     2\r\n\r\nIn [57]: df[s] = 1 # assigns last two rows (reindexed)\r\n\r\nIn [58]: df\r\nOut[58]: \r\n    alpha  beta\r\n1.0     a     1\r\n2.5     b     2\r\n3.0     c     3\r\n4.5     1     1\r\n5.0     1     1\r\n```\r\n\r\n##### *\\_\\_getitem\\_\\_* does not allow *DataFrame* key when columns are *MultiIndex*, but *\\_\\_setitem\\_\\_* does\r\n```python\r\nIn [60]: c2=p.MultiIndex(levels=[[\'foo\', \'bar\', \'baz\', \'qux\'],\r\n   ....:                        [\'one\', \'two\', \'three\']],\r\n   ....:                labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],\r\n   ....:                        [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\r\n   ....:                names=[\'first\', \'second\'])\r\n\r\nIn [61]: df2=p.DataFrame(np.random.randn(3, 10),\r\n   ....:              index=[\'A\',\'B\',\'C\'], columns=c2)\r\n\r\nIn [62]: df2[df2 > 0] # fails\r\n(exception)\r\n\r\nIn [63]: df2[df2 > 0] = 4 # ok\r\n```\r\n\r\n##### *\\_\\_setitem\\_\\_* and *\\_\\_getitem\\_\\_* reindex *DataFrame* keys when shapes differ, but do not reindex when shapes are the same and indexes/columns are different\r\n```python\r\nIn [68]: df3=p.DataFrame(np.random.randn(5, 2),\r\n   ....:              index=[1.0, 2.5, 3, 4.5, 5.0])\r\n\r\nIn [69]: key = (df3 > 0).reindex(df3.index[:-1]) # first four rows\r\n\r\nIn [70]: df3[key] = 5 # key is reindexed because shape does not match (ok)\r\n\r\nIn [71]: df3\r\nOut[71]: \r\n            0         1\r\n1.0 -1.599142  5.000000\r\n2.5  5.000000 -0.180330\r\n3.0 -0.568206  5.000000\r\n4.5 -0.340465 -0.070105\r\n5.0 -0.474955  1.928842\r\n\r\nIn [72]: key = key.reindex(key.index[::-1]) # reverse rows in key\r\n\r\nIn [73]: df3[key] = 4 # key is reindexed because shape does not match (ok)\r\n\r\nIn [74]: df3\r\nOut[74]: \r\n            0         1\r\n1.0 -1.599142  4.000000\r\n2.5  4.000000 -0.180330\r\n3.0 -0.568206  4.000000\r\n4.5 -0.340465 -0.070105\r\n5.0 -0.474955  1.928842\r\n\r\nIn [75]: df3=p.DataFrame(np.random.randn(5, 2),\r\n   ....:              index=[1.0, 2.5, 3, 4.5, 5.0])\r\n\r\nIn [76]: key = (df3 > 0) # all rows\r\n\r\nIn [77]: df3[key] = 5 # no reindex required\r\n\r\nIn [78]: df3\r\nOut[78]: \r\n            0         1\r\n1.0  5.000000 -0.233779\r\n2.5  5.000000 -0.141962\r\n3.0 -0.232551  5.000000\r\n4.5 -1.663034  5.000000\r\n5.0 -0.653200 -0.365681\r\n\r\nIn [79]: key = key.reindex(key.index[::-1]) # reverse rows in key\r\n\r\nIn [80]: df3[key] = 4 # key is not reindexed, even though rows are reversed\r\n\r\nIn [81]: df3\r\nOut[81]: \r\n            0         1\r\n1.0  5.000000 -0.233779\r\n2.5  5.000000  4.000000\r\n3.0 -0.232551  4.000000\r\n4.5  4.000000  5.000000\r\n5.0  4.000000 -0.365681\r\n```'"
2764,10391794,bmu,jreback,2013-01-28 20:49:06,2016-04-08 13:13:51,2016-04-08 13:13:51,closed,,Next Major Release,2,Bug;Difficulty Intermediate;Effort Medium;Enhancement;Resample;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2764,b'resample time series with 2 quarters',"b""see this question on so: http://stackoverflow.com/questions/14569223/timegrouper-pandas\r\n\r\n```python\r\nIn [213]: idx = pd.date_range('2008-07-01', '2010-12-01', freq='MS')\r\n\r\nIn [214]: s = pd.Series(np.random.randn(idx.size), index=idx)\r\n\r\nIn [215]: s.index[0] + pd.datetools.QuarterEnd(startingMonth=12) * 2\r\nOut[215]: <Timestamp: 2008-12-31 00:00:00>\r\n\r\nIn [216]: s.resample(pd.datetools.QuarterEnd(startingMonth=12) * 2)\r\nOut[216]: \r\n2008-09-30   -0.220156\r\n2009-03-31   -0.392138\r\n2009-09-30   -0.720243\r\n2010-03-31    0.261541\r\n2010-09-30    0.444490\r\n2011-03-31   -0.093383\r\nFreq: 2Q-DEC\r\n```\r\n\r\nSo adding the offset to the starting point of the time series leads to the time desired, while resampling starts one quarter before."""
2763,10385548,courosh,jreback,2013-01-28 18:06:38,2013-03-24 19:51:29,2013-03-24 19:50:25,closed,,0.13,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/2763,b'groupby first and last modify datatype',"b""Using `groupby(level=0).first()` or `groupby(level=0).last()` to eliminate duplicate index entries modifies integer data types to float.\r\n\r\n    import pandas as pd\r\n    idx = range(10)\r\n    idx.append(9)\r\n    df = pd.Series(data=range(11), index=idx, name='IntCol')\r\n    print df\r\n    print df.dtype\r\n\r\n    0     0\r\n    1     1\r\n    2     2\r\n    3     3\r\n    4     4\r\n    5     5\r\n    6     6\r\n    7     7\r\n    8     8\r\n    9     9\r\n    9    10\r\n    Name: IntCol\r\n    int64\r\n\r\n    df2 = df.groupby(level=0).first()\r\n    print df2\r\n    print df2.dtype\r\n\r\n    0    0\r\n    1    1\r\n    2    2\r\n    3    3\r\n    4    4\r\n    5    5\r\n    6    6\r\n    7    7\r\n    8    8\r\n    9    9\r\n    float64"""
2759,10352515,pprett,wesm,2013-01-27 10:17:34,2013-02-11 07:44:07,2013-02-11 02:17:32,closed,,0.11,11,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2759,b'Integer dtype is promoted to int64',"b""It seems that DataFrame (both constructor and ``astype``) promotes the dtypes np.int[8,16,32] to np.int64 ::\r\n\r\n    >>> pd.DataFrame(data=np.ones((10, 2), dtype=np.int32), dtype=np.int32).get_dtype_counts()\r\n    int64    2\r\n\r\nI'm not sure whether this is a but or on purpose (if so, a note in the docstring would be great).\r\nIs there a specific reason why this is the case (e.g. to accomodate NA values?). If there are no NA's in the frame, can the promotion be turned off?"""
2756,10325546,ronnydw,ronnydw,2013-01-25 20:40:53,2013-03-18 18:52:00,2013-03-18 18:26:02,closed,,0.11,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2756,b'plotting 3x3 scatter-matrix: missing labels',"b""I create a pandas scatter-matrix usng the following code:\r\n````python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\na = np.random.normal(1, 3, 100)\r\nb = np.random.normal(3, 1, 100)\r\nc = np.random.normal(2, 2, 100)\r\n\r\ndf = pd.DataFrame({'A':a,'B':b,'C':c})\r\npd.scatter_matrix(df, diagonal='kde')\r\n```\r\nThis result in the following scatter-matrix:\r\n![scatter-matrix-3](https://f.cloud.github.com/assets/1548156/98709/56df06f8-672f-11e2-8471-3011933d7cc1.png)\r\n\r\nThe first row has no ytick labels, the 3th column no xtick labels, the 3th item 'C' is not labeled.\r\n\r\nAny idea how to complete this plot with the missing labels ?"""
2754,10320951,jreback,jorisvandenbossche,2013-01-25 18:21:21,2014-09-13 21:48:56,2014-09-13 21:48:56,closed,,0.15.0,13,Bug;Data IO;Dtypes;IO SQL,https://api.github.com/repos/pydata/pandas/issues/2754,b'ENH: sql support for NaN/NaT conversions',"b'UPDATE from @jorisvandenbossche:\r\n\r\nOverview of current status writing nan values (see also tests added in #7100):\r\n\r\n* For MySQL using pymysql/MySQLdb nothing works: you get message `Unknown column \'nan\' in \'field list\'` (see also eg http://stackoverflow.com/questions/23353732/python-pandas-write-to-sql-with-nan-values)\r\n* Numeric columns:\r\n    * working for sqlite and postgres\r\n    * Only full NaN columns stay `None` in sqlite\r\n* Object columns (eg strings)\r\n    * for postgresql: NaN is converted to the string `u\'NaN\'`, which is not really what we want\r\n    * for sqlite it is returned as `None`\r\n* ``NaT``:\r\n    * postgresql: gives error on inserting ""0001-255-255T00:00:00""\r\n    * sqlite3: writing works, but reading it with query returns ``\'-001--1--1 -1:-1:-1.-00001\'``\r\n* MSSQL: not working with message ""The supplied value is not a valid instance of data type float"", see #8088 for more details\r\n\r\n---\r\n\r\nnot sure exactly what sql expects (Nones?) rather than np.nan (or NaT)\r\n\r\nhttps://groups.google.com/forum/?fromgroups#!topic/pydata/lxhnFtuzvWQ\r\n\r\nalso provide pandas ``datetime64[ns]``, instead of ``datetime/date`` types\r\nhttps://github.com/pydata/pandas/issues/3532\r\n'"
2753,10320750,hayd,hayd,2013-01-25 18:14:57,2013-04-01 10:30:45,2013-04-01 10:30:45,closed,changhiskhan,0.11,6,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2753,b'BUG resample business days/months',"b'The first resample starts at 2000-01-03 (as does `foo`) whilst the second resample starts at 2000-01-04, should it?\r\n\r\n```\r\nfoo = pd.Series(index=pd.bdate_range(datetime.date(2000,1,1),datetime.date(2001,1,1)))\r\nfoo.resample(""BMS"")\r\nfoo.resample(""BMS"").resample(""B"")\r\n```\r\n\r\n*Migrated from [StackOverflow](http://stackoverflow.com/questions/14525942/pandas-resampling-skips-first-date-of-timeseries).*'"
2747,10272815,dhirschfeld,jreback,2013-01-24 13:44:00,2013-09-03 08:31:48,2013-02-11 02:17:16,closed,,0.11,7,Bug,https://api.github.com/repos/pydata/pandas/issues/2747,b'BUG: DataFrame.clip sets all values to the lower_bound',"b'The following unit-test fails:\r\n\r\n```python\r\ndef test_dataframe_clip(lb=-1, ub=1):\r\n    df = pd.DataFrame(np.random.randn(1000,2))\r\n    lb_mask = df.values <= lb\r\n    ub_mask = df.values >= ub\r\n    mask = ~lb_mask & ~ub_mask\r\n    clipped_df = df.clip(lb, ub)\r\n    assert (clipped_df.values[lb_mask] == lb).all()\r\n    assert (clipped_df.values[ub_mask] == ub).all()\r\n    assert (clipped_df.valus[mask] == df.values[mask]).all()\r\n#\r\n```'"
2746,10272254,jreback,jreback,2013-01-24 13:22:45,2013-03-01 18:30:55,2013-03-01 18:30:55,closed,,0.11,7,Bug,https://api.github.com/repos/pydata/pandas/issues/2746,b'BUG: partial boolean indexing fails with dtype = int',"b""this fails because we are putmasking on a  dtype=int ndarray\r\nneed to cast if this is the case (in series.where)\r\n```\r\nimport pandas\r\ndf = pandas.DataFrame(index=[1,2])\r\ndf['test'] = [1,2]\r\ndf['test'][[True, False]] = pandas.Series([0],index=[1])\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-7738933c68bb> in <module>()\r\n----> 1 df['test'][[True,False]] = pandas.Series([0],index=[1])\r\n\r\n/mnt/home/jreback/pandas/pandas/core/series.pyc in __setitem__(self, key, value)\r\n    677         if _is_bool_indexer(key):\r\n    678             key = self._check_bool_indexer(key)\r\n--> 679             self.where(~key,value,inplace=True)\r\n    680         else:\r\n    681             self._set_with(key, value)\r\n\r\n/mnt/home/jreback/pandas/pandas/core/series.pyc in where(self, cond, other, inplace)\r\n    630             raise ValueError('Length of replacements must equal series length')\r\n    631 \r\n--> 632         np.putmask(ser, ~cond, other)\r\n    633 \r\n    634         return None if inplace else ser\r\n\r\nTypeError: array cannot be safely cast to required type\r\n\r\n```"""
2745,10267729,ruidc,jreback,2013-01-24 10:30:18,2013-09-05 15:00:16,2013-03-22 18:54:28,closed,wesm,0.11,19,Bug,https://api.github.com/repos/pydata/pandas/issues/2745,b'regression in 0.10.1 with boolean indexing?',"b""this used to work in 0.10 but now fails in 0.10.1:\r\n\r\n    import pandas\r\n    df = pandas.DataFrame(index=[1,2])\r\n    df['test'] = [1,2]\r\n    df['test'][[True, False]] = [0]\r\n\r\nNow gives:\r\nValueError: Length of replacements must equal series length\r\n\r\npossibly related to closed issue https://github.com/pydata/pandas/issues/2703"""
2743,10257433,jdhillon,jdhillon,2013-01-24 00:44:16,2013-03-03 20:15:58,2013-03-03 20:15:58,closed,,Someday,3,Bug;Community;Visualization,https://api.github.com/repos/pydata/pandas/issues/2743,b'tiny dataframe non-string colors bug',"b""Hi,\r\n\r\nIf we specify custom rgb 3-ples for rcParams['axes.color_cycle'], DataFrame.plot breaks because  _get_colors expects strings (the usual rcParams['axes.color_cycle'] = ['b','g'...] etc).\r\n\r\nTest case:\r\nrcParams['axes.color_cycle'] = [(1,1,1),(.1,.1,.1)]\r\nDataFrame({'a':[1,1], 'b':[2,2]}).plot()\r\n\r\nSeems like having _get_color return a list instead of taking the list and joining all the elements would fix it?\r\n\r\nThanks!"""
2742,10256386,aiko1895,changhiskhan,2013-01-24 00:01:57,2013-04-01 19:58:06,2013-04-01 19:58:06,closed,,0.11,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2742,b'Incorrect period end_time',"b""Used versions: Python 2.7.3 Pandas 0.10.1\r\n\r\nCode:\r\nfrom pandas import Period\r\nfrom datetime import datetime\r\n\r\nPeriod(datetime(2013, 1, 1, 0, 0), freq='W-SAT')\r\n\r\noutputs: Period('2012-12-30/2013-01-05', 'W-SAT') - correct\r\n\r\nPeriod(datetime(2013, 1, 1, 0, 0), freq='W-SAT').end_time\r\n\r\noutputs: Timestamp: 2013-01-11 23:59:59.999999999\r\n\r\nIs it expected behaviour ? Week end on 2013-01-11 not on 2013-01-05 ? """
2733,10227303,bmu,jreback,2013-01-23 10:30:39,2014-03-01 07:45:12,2014-03-01 07:45:12,closed,,Someday,10,Bug;Data IO;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2733,"b'read_csv: `usecols` doesn\'t work if separator is not "",""'","b'If I have data (without a header) separated by "","" usecols works, however if the data is separated by white space it doesn\'t seem to work:\r\n\r\n```python\r\nIn [30]: data = \'1,2,3\\n4,5,6\\n7,8,9\'\r\n\r\nIn [31]: pd.read_csv(StringIO(data), usecols=[0, 1], header=None)\r\nOut[31]: \r\n   0  1\r\n0  1  2\r\n1  4  5\r\n2  7  8\r\n\r\nIn [32]: data = \'1 2 3\\n4 5 6\\n7 8 9\'\r\n\r\nIn [33]: pd.read_csv(StringIO(data), sep=\'\\s+\', header=None)\r\nOut[33]: \r\n   0  1  2\r\n0  1  2  3\r\n1  4  5  6\r\n2  7  8  9\r\n\r\nIn [34]: pd.read_csv(StringIO(data), sep=\'\\s+\', usecols=[0, 1], header=None)\r\nOut[34]: \r\n   0  1  2\r\n0  1  2  3\r\n1  4  5  6\r\n2  7  8  9\r\n```'"
2727,10212086,stephenwlin,stephenwlin,2013-01-22 22:22:18,2013-02-26 22:24:32,2013-02-11 00:32:19,closed,,0.11,4,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2727,b'BUG: Mismatch between get and set behavior for slices of floating indices',"b'When *df.index.inferred_type* is *\'floating\'*, *df.ix[start:end]* uses only label-based indexing only when **both** *start* and *end* are floating on the *\\_\\_getitem\\_\\_* side but when **either** *start* or *end* are floating on the *\\_\\_setitem\\_\\_* side (where *start* and *end* are both either integers or floats within epsilon of integers)\r\n\r\n```python\r\nIn [2]: df=p.DataFrame([""a"", ""b"", ""c"", ""d""], index=[1.00, 2.00, 3.00, 4.00])\r\n\r\nIn [3]: df.ix[1.0:4] # uses positional indexing to get last three rows\r\nOut[3]: \r\n   0\r\n2  b\r\n3  c\r\n4  d\r\n\r\nIn [4]: df.ix[1.0:4.0] # uses label-based indexing to get all four rows\r\nOut[4]: \r\n   0\r\n1  a\r\n2  b\r\n3  c\r\n4  d\r\n\r\nIn [5]: df.ix[1.0:4] = ""y"" # uses label-based indexing to set all four rows\r\n\r\nIn [6]: df\r\nOut[6]:\r\n   0\r\n1  y\r\n2  y\r\n3  y\r\n4  y\r\n\r\nIn [7]: df.ix[1.0:4.0] = ""z"" # uses label-based indexing to set all four rows\r\n\r\nIn [8]: df\r\nOut[8]:\r\n   0\r\n1  z\r\n2  z\r\n3  z\r\n4  z\r\n```'"
2726,10209721,bluefir,changhiskhan,2013-01-22 21:18:10,2013-02-09 21:08:57,2013-02-09 21:08:57,closed,changhiskhan,0.11,4,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2726,b'0.10.1: DataFrame.plot() does not use integer index as ticks for x axis',"b""```\r\ndf = DataFrame(index=range(99, -1, -1))\r\ndf['y'] = range(100)\r\ndf.plot()\r\n```\r\n\r\nI observe a 45-degree line while what I expect to see is the -45-degree line (from top left corner to bottom right corner). Am I doing something wrong?"""
2725,10208471,bluefir,dieterv77,2013-01-22 20:42:23,2013-03-09 03:30:10,2013-03-09 03:30:10,closed,,0.11,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2725,"b""0.10.1: DataFrame.plot(x='x', y='y') does not correctly plot one column versus another""","b""```\r\ndf = DataFrame(index=range(100))\r\ndf['x'] = np.random.randn(100)\r\ndf['y'] = df['x']\r\ndf.plot(x='x', y='y', linestyle='None', marker='o')\r\n```\r\n\r\nI expected to see column 'y' plotted against column 'x' and, as such, to see scatter points along the 45-degree line. Instead, I get a strange scatter plot. Am I doing something wrong or is this a bug? If it is a bug, is there a workaround?"""
2722,10190905,simomo,jreback,2013-01-22 12:55:22,2013-09-21 14:21:39,2013-09-21 14:21:39,closed,,0.13,5,Bug;Dtypes,https://api.github.com/repos/pydata/pandas/issues/2722,b'Some dates cause error when calling dataframe.convert_objects',"b'* Example codes:\r\n<pre>\r\nli = [datetime.datetime(3800, 1, 1), ]\r\nli_df = DataFrame(li)\r\nli_df.convert_objects()\r\n</pre>\r\nthen raise the following exception:\r\n<pre>\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-112-1cb5a6d822e9> in <module>()\r\n----> 1 li_df.convert_objects()\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/core/frame.pyc in convert_objects(self, convert_dates)\r\n   1661         for col, s in self.iteritems():\r\n   1662             if s.dtype == np.object_:\r\n-> 1663                 new_data[col] = convert_f(s)\r\n   1664             else:\r\n   1665                 new_data[col] = s\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/core/frame.pyc in <lambda>(x)\r\n   1656         new_data = {}\r\n   1657         convert_f = lambda x: lib.maybe_convert_objects(\r\n-> 1658             x, convert_datetime=convert_dates)\r\n   1659 \r\n   1660         # TODO: could be more efficient taking advantage of the block\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/lib.so in pandas.lib.maybe_convert_objects (pandas/lib.c:33656)()\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/tslib.so in pandas.tslib.convert_to_tsobject (pandas/tslib.c:10796)()\r\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/tslib.so in pandas.tslib._check_dts_bounds (pandas/tslib.c:11827)()\r\nValueError: Out of bounds nanosecond timestamp: 3800-01-01 00:00:00\r\n</pre>'"
2719,10163149,szli,jreback,2013-01-21 16:41:18,2013-12-04 00:40:06,2013-03-21 18:10:03,closed,,0.11,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2719,b'BUG? Reshaping a Series',"b'It looks to me like a bug in pandas.Series.\r\n```\r\na = pd.Series([1,2,3,4])\r\nb = a.reshape(2,2)\r\nb\r\n```\r\nb has type Series but can not be displayed, the last statement gives exception, very lengthy, the last line is ""TypeError: %d format: a number is required, not numpy.ndarray"". b.shape returns (2,2), which contradicts its type Series. I am guessing perhaps pandas.Series does not implement reshape function and I am calling the version from np.array. I am at pandas 0.9.1. Some discussion on stackoverflow is here http://stackoverflow.com/questions/14390224/reshape-of-pandas-series\r\n\r\nIf it does not make sense to reshape a Series, should it throw an exception?\r\n'"
2718,10160730,jreback,jreback,2013-01-21 15:36:31,2013-01-22 02:22:16,2013-01-22 02:22:16,closed,,0.11,4,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2718,b'BUG: should astype only TRY to convert string columns?',"b""should astype TRY to convert string columns (e.g. if the value of the string column is like '5'), or skip if its an exception (and just return it), rather than raise an exception on the whole operation?\r\n\r\nI can fix for 0.10.2\r\n\r\n```\r\nIn [4]: df = pd.DataFrame({ 'a' : 'foo', 'b' : 1. },index=np.arange(10))\r\nIn [6]: df .dtypes\r\nOut[6]: \r\na     object\r\nb    float64\r\n\r\nIn [8]: df.astype('float64')\r\n/mnt/home/jreback/pandas/pandas/core/internals.pyc in astype(self, dtype)\r\n    613         new_blocks = []\r\n    614         for block in self.blocks:\r\n--> 615             newb = make_block(com._astype_nansafe(block.values, dtype),\r\n    616                               block.items, block.ref_items)\r\n    617             new_blocks.append(newb)\r\n\r\n/mnt/home/jreback/pandas/pandas/core/common.pyc in _astype_nansafe(arr, dtype)\r\n   1058         return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\r\n   1059 \r\n-> 1060     return arr.astype(dtype)\r\n   1061 \r\n   1062 \r\n\r\nValueError: could not convert string to float: foo\r\n```"""
2714,10141264,changhiskhan,jreback,2013-01-20 21:29:39,2013-09-16 15:07:10,2013-09-16 15:07:10,closed,,0.13,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2714,b'inserting new column with DataFrame.ix buggy',"b'```\r\nIn [22]: df\r\nOut[22]: \r\n          A         A         B         C\r\n0  0.148734 -0.041765  1.575866  1.422524\r\n1  0.188963  2.086219  1.595425  0.539364\r\n2  1.498709 -0.338857 -0.210994  0.984697\r\n3 -0.197789 -0.093690 -0.421010  0.710587\r\n\r\nIn [23]: df.ix[:, \'D\'] = [\'X\', \'Y\', \'Z\', \'V\']\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-23-e2003f3ca30c> in <module>()\r\n----> 1 df.ix[:, \'D\'] = [\'X\', \'Y\', \'Z\', \'V\']\r\n\r\n/Users/changshe/code/pandas/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n     69                 raise IndexingError(\'only tuples of length <= %d supported\',\r\n     70                                     self.ndim)\r\n---> 71             indexer = self._convert_tuple(key)\r\n     72         else:\r\n     73             indexer = self._convert_to_indexer(key)\r\n\r\n/Users/changshe/code/pandas/pandas/core/indexing.pyc in _convert_tuple(self, key)\r\n     78         keyidx = []\r\n     79         for i, k in enumerate(key):\r\n---> 80             idx = self._convert_to_indexer(k, axis=i)\r\n     81             keyidx.append(idx)\r\n     82         return tuple(keyidx)\r\n\r\n/Users/changshe/code/pandas/pandas/core/indexing.pyc in _convert_to_indexer(self, obj, axis)\r\n    535                 return indexer\r\n    536         else:\r\n--> 537             return labels.get_loc(obj)\r\n    538 \r\n    539     def _tuplify(self, loc):\r\n\r\n/Users/changshe/code/pandas/pandas/core/index.pyc in get_loc(self, key)\r\n    716         loc : int if unique index, possibly slice or mask if not\r\n    717         """"""\r\n--> 718         return self._engine.get_loc(key)\r\n    719 \r\n    720     def get_value(self, series, key):\r\n\r\n/Users/changshe/code/pandas/pandas/index.so in pandas.index.IndexEngine.get_loc (pandas/index.c:3222)()\r\n\r\n/Users/changshe/code/pandas/pandas/index.so in pandas.index.IndexEngine.get_loc (pandas/index.c:3053)()\r\n\r\n/Users/changshe/code/pandas/pandas/index.so in pandas.index.IndexEngine._get_loc_duplicates (pandas/index.c:3389)()\r\n\r\nKeyError: \'D\'\r\n```'"
2713,10133079,changhiskhan,changhiskhan,2013-01-20 05:18:32,2013-01-20 16:39:57,2013-01-20 16:39:57,closed,changhiskhan,0.10.1,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2713,b'Google Analytics does not do the right thing for segments',"b'I\'m getting an error with the segment option now.  When I go to the query explorer, my segments look like this: ""gaid::707967480""\r\nhttp://ga-dev-tools.appspot.com/explorer/\r\n\r\nSo the error I\'m getting is related to that string construction - is this a bug, or is this somehow my segment ids?\r\n\\---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-7b6938ee12f2> in <module>()\r\n----> 1 df = ga.read_ga([\'pagePath\', \'date\'], dimensions=[\'avgTimeonPage\'], start_date=\'2012-09-01\', segment=[\'742193199\'], secrets=\'./client_secrets.json\')\r\n      2 df\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas-0.10.0-py2.7-win32.egg\\pandas\\io\\ga.pyc in read_ga(metrics, dimensions, start_date, **kwargs)\r\n     94     reader = GAnalytics(**reader_kwds)\r\n     95     return reader.get_data(metrics=metrics, start_date=start_date,\r\n---> 96                            dimensions=dimensions, **kwargs)\r\n     97 \r\n     98 class OAuthDataReader(object):\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas-0.10.0-py2.7-win32.egg\\pandas\\io\\ga.pyc in get_data(self, metrics, start_date, end_date, dimensions, segment, filters, start_index, max_results, index_col, parse_dates, keep_date_col, date_parser, na_values, converters, sort, dayfirst, account_name, account_id, property_name, property_id, profile_name, profile_id, chunksize)\r\n    283 \r\n    284         if chunksize is None:\r\n--> 285             return _read(start_index, max_results)\r\n    286 \r\n    287         def iterator():\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas-0.10.0-py2.7-win32.egg\\pandas\\io\\ga.pyc in _read(start, result_size)\r\n    279             except HttpError, inst:\r\n    280                 raise ValueError(\'Google API error %s: %s\' % (inst.resp.status,\r\n--> 281                                  inst._get_reason()))\r\n    282 \r\n    283 \r\n\r\nValueError: Google API error 400: Invalid value \'ga:742193199\' for segment.'"
2707,10128617,wesm,changhiskhan,2013-01-19 19:36:56,2013-01-20 20:51:54,2013-01-20 20:17:12,closed,changhiskhan,0.10.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2707,b'Buggy .ix assignment',"b""from mailing list\r\n\r\n```\r\n1) With numeric values everywhere:\r\n\r\ndf['tt1'] = df['tt2'] = 0\r\ndf.ix[1, ['tt1', 'tt2']] = [1, 2]\r\nprint df.ix[0:2,['tt1', 'tt2']]\r\n\r\nOutput:\r\n\r\n   tt1  tt2\r\n0    0    0\r\n1    1    2\r\n2    0    0\r\n\r\n2) With string values everywhere:\r\n\r\ndf['tt1'] = df['tt2'] = '0'\r\ndf.ix[1, ['tt1', 'tt2']] = ['1', '2']\r\nprint df.ix[0:2,['tt1', 'tt2']]\r\n\r\nOutput:\r\n\r\n      tt1     tt2\r\n0       0       0\r\n1  [1, 2]  [1, 2]\r\n2       0       0\r\n```"""
2706,10120999,halleygithub,wesm,2013-01-19 03:39:00,2013-01-21 02:25:40,2013-01-20 21:59:52,closed,wesm,0.10.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2706,b'Bug related to multilevel index series ?',"b""I am upgrading Pandas from 0.8.1 to 0.10.1.dev-f7f7e13 . My environment is Window XP with below: Python: 2.7.3 \t Numpy: 1.6.2 \t MPL: 1.1.1 \t Pandas: 0.10.1.dev-f7f7e13.\r\n\r\nThen OK application on 0.8.1 now meets errors. I trace the root cause to filtering the duplicated index of Series. Detail in : http://stackoverflow.com/questions/14395678/how-to-drop-extra-copy-of-duplicate-index-of-pandas-series\r\n\r\nsimply put: below snippet has two issues :\r\n\r\nimport pandas as pd\r\nidx_tp = [('600809', '20061231'), ('600809', '20070331'), ('600809', '20070630'), ('600809', '20070331')]\r\ndt = ['demo','demo','demo','demo']\r\nidx = pd.MultiIndex.from_tuples(idx_tp,names = ['STK_ID','RPT_Date'])\r\ns = pd.Series(dt,index=idx)\r\n\r\n\\# Issue 1: s[s.index.unique()] works well on 0.8.1 but not 0.10.1\r\n\\# Issue 2: s.groupby(s.index).first() will crash on my machine"""
2705,10074417,daggre-gmu,hayd,2013-01-17 20:39:12,2014-05-29 05:01:18,2014-05-29 05:01:18,closed,,Someday,15,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2705,b'Loading of large pickled dataframes fails',"b""I tried pickling a very large dataframe (20GB or so) and that succeeded to write to disk, but when I try to read it, it fails with: ValueError: buffer size does not match array size\r\n\r\nNow I did a bit of research and found the following:\r\n\r\nhttp://stackoverflow.com/questions/12060932/unable-to-load-a-previously-dumped-pickle-file-of-large-size-in-python\r\n\r\nhttp://bugs.python.org/issue13555\r\n\r\nI am thinking this is a numpy/python issue, but it does cause me pretty big pain when I want to back up a dataframe that took a long time to join together, and I want all the dtypes stored (namely what columns are datetimes).  Perhaps a solution would be a csv file that keeps the dtypes somewhere (otherwise I'll have to figure out what columns are serialized dates).  Any workarounds would be appreciated."""
2702,9996620,jim22k,jreback,2013-01-15 21:43:41,2013-09-06 03:11:15,2013-09-06 03:11:15,closed,wesm,Someday,9,Bug,https://api.github.com/repos/pydata/pandas/issues/2702,b'Bug in Fancy/Boolean Indexing with nested lists',"b'Fancy or Boolean indexing on a Series has two strange behaviors.  My examples only show the behavior with Fancy indexing, but it\'s the same for Boolean indexing.\r\n\r\nLHS vs RHS length\r\n----------------------------\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> s[[0,1,2]] = range(27)\r\n        >>> list(s)\r\n        [0, 1, 2]\r\nI would have expected an error, similar to what I get with slice indexing\r\n\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> s[0:3] = range(27)\r\n        ValueError: cannot copy sequence with size 27 to array axis with dimension 3\r\nAn even odder behavior is when you have too few items in the RHS\r\n\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> s[[0,1,2]] = range(2)\r\n        >>> list(s)\r\n        [0, 1, 0]\r\nIt seems to be using something like itertools.cycle which seems very arbitrary to me\r\n\r\nNested RHS\r\n------------------\r\nThis may seem like a strange use of pandas, but I need to store Python lists\r\n\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> s[[0,1,2]] = [[100,200], [300,400], [500,600]]\r\n        >>> list(s)\r\n        [100, 200, 300]\r\nVery strange.  It\'s like it flattens the input first.\r\nBut this flattening only happens if the nested levels are all the same size.\r\n\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> s[[0,1,2]] = [[100,200], [300,400], [500,600, 601, 602]]\r\n        >>> list(s)\r\n        [[100,200], [300,400], [500,600, 601, 602]]\r\nI know in numpy the array constructor would make a distinction between these two inputs, so maybe that\'s the reason for the difference, but I still don\'t see why ndarrays are being flattened.\r\n\r\nI can work around the issue by converting the RHS to a 1-D array and passing that in.\r\n\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> rhs = np.empty(3).astype(\'object\')\r\n        >>> rhs[:] = [[100,200], [300,400], [500,600]]\r\n        >>> s[[0,1,2]] = rhs\r\n        >>> list(s)\r\n        [[100,200], [300,400], [500,600]]\r\nSlice indexing doesn\'t have this problem at all\r\n\r\n        >>> s = pd.Series(list(\'abc\'))\r\n        >>> s[0:3] = [[100,200], [300,400], [500,600]]\r\n        >>> list(s)\r\n        [[100,200], [300,400], [500,600]]\r\n\r\n**My Question**: Are these behaviors a *bug* or a ""feature""?  I think Fancy/Boolean indexing should operate the same as slice indexing -- i.e. check for matching lengths and don\'t auto-convert to numpy array.\r\n'"
2700,9986960,lselector,wesm,2013-01-15 17:24:16,2013-01-20 01:41:36,2013-01-20 01:41:22,closed,wesm,0.10.1,2,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/2700,b'groupby().max() operation removes string columns',"b""While upgrading pandas from 0.7.2 to 0.9.1 we have found that groupby().max() operation now removes non-numeric columns. This broke our code in several places. Workaround is to use  groupby().aggregate(np.max).\r\n\r\nHere is an example demonstrating the problem:\r\n\r\naa=DataFrame({'nn':[11,11,22,22],'ii':[1,2,3,4],'ss':4*['mama']})\r\naa.groupby('nn').max()\r\n\r\noutput on pandas 0.7.2\r\n   ii  nn    ss\r\nnn             \r\n11  2  11  mama\r\n22  4  22  mama\r\n\r\noutput on pandas 0.9.1\r\n    ii\r\nnn    \r\n11   2\r\n22   4\r\n\r\nAs you see, object column 'ss' is dropped in new version !!!\r\nThis was very un-intuitive."""
2699,9982280,wesm,wesm,2013-01-15 15:31:18,2013-01-19 23:45:48,2013-01-19 23:45:48,closed,wesm,0.10.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2699,b'pandas.to_datetime called on existing datetime64 Series results in bad data',"b'```\r\n\r\nIn [7]: df.date\r\nOut[7]: \r\n0    2007-10-29 00:00:00\r\n1    2007-10-30 00:00:00\r\n2    2007-10-31 00:00:00\r\n3    2007-11-01 00:00:00\r\n4    2007-11-02 00:00:00\r\n5    2007-11-05 00:00:00\r\n6    2007-11-06 00:00:00\r\n7    2007-11-07 00:00:00\r\n8    2007-11-08 00:00:00\r\n9    2007-11-09 00:00:00\r\n10   2007-11-12 00:00:00\r\n11   2007-11-13 00:00:00\r\n12   2007-11-14 00:00:00\r\n13   2007-11-15 00:00:00\r\n14   2007-11-16 00:00:00\r\n...\r\n985   2011-09-26 00:00:00\r\n986   2011-09-27 00:00:00\r\n987   2011-09-28 00:00:00\r\n988   2011-09-29 00:00:00\r\n989   2011-09-30 00:00:00\r\n990   2011-10-03 00:00:00\r\n991   2011-10-04 00:00:00\r\n992   2011-10-05 00:00:00\r\n993   2011-10-06 00:00:00\r\n994   2011-10-07 00:00:00\r\n995   2011-10-10 00:00:00\r\n996   2011-10-11 00:00:00\r\n997   2011-10-12 00:00:00\r\n998   2011-10-13 00:00:00\r\n999   2011-10-14 00:00:00\r\nName: date, Length: 1000\r\n\r\nIn [8]: pd.to_datetime(df.date)\r\nOut[8]: \r\n0    1970-01-18 08:00:00\r\n1    1970-01-19 08:00:00\r\n2    1970-01-20 08:00:00\r\n3    1970-01-21 08:00:00\r\n4    1970-01-22 08:00:00\r\n5    1970-01-14 16:00:00\r\n6    1970-01-15 16:00:00\r\n7    1970-01-16 16:00:00\r\n8    1970-01-17 16:00:00\r\n9    1970-01-18 16:00:00\r\n10   1970-01-21 16:00:00\r\n11   1970-01-22 16:00:00\r\n12   1970-01-23 16:00:00\r\n13   1970-01-14 00:00:00\r\n14   1970-01-15 00:00:00\r\n...\r\n985   1970-01-24 08:00:00\r\n986   1970-01-25 08:00:00\r\n987   1970-01-26 08:00:00\r\n988   1970-01-16 16:00:00\r\n989   1970-01-17 16:00:00\r\n990   1970-01-20 16:00:00\r\n991   1970-01-21 16:00:00\r\n992   1970-01-22 16:00:00\r\n993   1970-01-23 16:00:00\r\n994   1970-01-24 16:00:00\r\n995   1970-01-17 00:00:00\r\n996   1970-01-18 00:00:00\r\n997   1970-01-19 00:00:00\r\n998   1970-01-20 00:00:00\r\n999   1970-01-21 00:00:00\r\nName: date, Length: 1000\r\n```'"
2694,9957413,bsdfish,wesm,2013-01-14 21:01:15,2013-01-20 03:04:25,2013-01-20 03:04:25,closed,,0.10.1,5,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2694,b'HDFStore.__contains__ bug due to use of regex',"b'HDFStore.__contains__ takes the node name and uses it as a pattern to search for.   That crashes if the name of the node isn\'t a valid regex, for example ""node1(()""   and doesn\'t function correctly if the node name *is* a valid regex but doesn\'t necessarily match itself.'"
2692,9952924,lselector,wesm,2013-01-14 18:52:07,2013-12-04 00:40:09,2013-01-19 23:23:56,closed,wesm,0.10.1,1,Bug;Performance,https://api.github.com/repos/pydata/pandas/issues/2692,b'groupby().sum() very slow when applied to boolean columns',"b""While upgrading pandas from 0.7.2 to 0.9.1 we have bumped into slowness of certain groupby().sum() operations. Here is a simple example:\r\n\r\nN=10000\r\naa=DataFrame({'ii':range(N),'bb':[True for x in range(N)]})\r\ntimeit aa.sum()  # fast\r\ntimeit aa.groupby('bb').sum() #fast\r\ntimeit aa.groupby('ii').sum()  # very slow (~ 1000 times slower)"""
2690,9935697,gdraps,wesm,2013-01-14 09:32:28,2013-01-20 00:12:22,2013-01-19 23:39:12,closed,wesm,0.10.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2690,b'combinatorial explosion when merging dataframes',"b""Hi Wes, Not sure if this is real, but opening to follow up on a comment in http://stackoverflow.com/questions/14199168/combinatorial-explosion-when-merging-dataframes-in-pandas.  Here's a fabricated example to reproduce `MergeError: Combinatorial explosion! (boom)` in tools/merge.py:\r\n\r\n    df1 = pd.DataFrame(np.random.randn(1000, 7),\r\n                       columns=list('ABCDEF') + ['G1'])\r\n    df2 = pd.DataFrame(np.random.randn(1000, 7),\r\n                       columns=list('ABCDEF') + ['G2'])\r\n    df1.merge(df2) # Boom\r\n\r\nThe value of group_sizes at the time of exception is:\r\n\r\n    [2000, 2000, 2000, 2000, 2000, 2000]\r\n\r\nThe exception does not occur if the common columns are set as the index, though.\r\n\r\n    df1 = df1.set_index(list('ABCDEF'))\r\n    df2 = df2.set_index(list('ABCDEF'))\r\n    df1.merge(df2, left_index=True, right_index=True, how='outer') # OK"""
2689,9929304,NigelCleland,wesm,2013-01-14 00:59:48,2013-01-20 18:54:26,2013-01-20 18:54:26,closed,wesm,0.10.1,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2689,b'Apply datetime methods to timestamps results in numpy errors',"b""Hi,\r\n\r\nThis may or may not be a bug. I'm unsure of which functionality you were intending.\r\n\r\nI have a timestamp created by\r\n\r\n```\r\nrng = pd.date_range(start_date, periods = 100, freq='2H')\r\nA = DataFrame(rng) # We'll leave it as a generic index for now, not important.\r\n```\r\n\r\nI then have a function which is applied to that dataframe via A.apply(f)\r\n\r\n```\r\ndef f(x):\r\n    return (x.hour, x.day, x.month)\r\n```\r\n\r\nif I apply this functionality to the raw dataframe via .apply, this will result in a numpy.datetime64 error.\r\n\r\nHowever, if I apply it to individual points.\r\n\r\nE.g. f(A.ix[10]), then the correct functionality results.\r\n\r\nI have determined that this is due to a difference between the types of the two \r\n\r\ne.g. A.dtype = dtype('datetime64[n]s')\r\n\r\ntype(A.ix[10]) = pandas.tslib.Timestamp\r\n\r\nThus to get around the issue above you may simply add the following functionality to the function\r\n\r\ndef f(x):\r\n    x = pd.Timestamp(x)\r\n    return (x.hour, x.month, x.year)\r\n\r\nI am filing this as I'm unsure of what you're intended functionality is with this matter"""
2684,9877411,gerigk,wesm,2013-01-11 11:41:40,2013-12-04 00:40:11,2013-01-21 16:54:33,closed,wesm,0.10.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2684,b'MemoryError on df.sortlevel(k)',"b""see http://stackoverflow.com/questions/14265539/when-does-pandas-pandas-pydata-org-throw-a-memory-error-on-df-sortlevelk/14267566#14267566\r\nUnfortunately I can't provide the data set."""
2680,9852585,rhaskett,changhiskhan,2013-01-10 17:50:54,2014-01-09 08:00:54,2013-01-20 04:00:36,closed,,0.10.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2680,b'BDay() subtraction bug when the starting date is not on offset',"b""  Note the output skips the weekday 12/23/2011.\r\n\r\n    dt = datetime(2012, 1, 1)\r\n\r\n    for i in range(1,10):\r\n        print i, (dt - i * BDay()).strftime('%m/%d/%Y')\r\n\r\n1 12/30/2011\r\n2 12/29/2011\r\n3 12/28/2011\r\n4 12/27/2011\r\n5 12/26/2011\r\n6 12/22/2011\r\n7 12/21/2011\r\n8 12/20/2011\r\n9 12/19/2011\r\n\r\nhowever when the start date is onoffset it works fine\r\n\r\n    dt = datetime(2011, 12, 30)\r\n\r\n    for i in range(1, 10):\r\n        print i, (dt - i * BDay()).strftime('%m/%d/%Y')\r\n\r\n\r\n1 12/29/2011\r\n2 12/28/2011\r\n3 12/27/2011\r\n4 12/26/2011\r\n5 12/23/2011\r\n6 12/22/2011\r\n7 12/21/2011\r\n8 12/20/2011\r\n9 12/19/2011\r\n"""
2672,9825834,bluefir,wesm,2013-01-09 23:33:25,2013-01-20 02:25:18,2013-01-20 01:47:26,closed,wesm,0.10.1,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2672,b'version 0.10.0: read_table() mangles the first two lines of data if they contain fewer columns than the header',"b""I cannot figure out how to attach files to issues, so let's try this:\r\n\r\nCreate a file text.txt with tabs as delimiters:\r\n\r\nkey1\tkey2\tx1\tx2\tx3\r\n1\ta\t0\t100\r\n2\tb\t100\t0\r\n3\tc\t25\t75\r\n4\td\t35\t65\r\n5\te\t15\t85\r\n\r\nNotice that all data lines have 4 columns while the header has 5 columns.\r\n\r\nLoad this file into a DataFrame:\r\n\r\n```\r\nimport pandas as pd\r\ntest = pd.read_table('test.txt')\r\n```\r\n\r\nObserve the result:\r\n\r\n```\r\n>    test\r\n key1 key2   x1   x2  x3\r\n0     1    a    0  100   2\r\n1   NaN    b  100    0 NaN\r\n2     3    c   25   75 NaN\r\n3     4    d   35   65 NaN\r\n4     5    e   15   85 NaN\r\n```\r\n\r\nAs you can see, the last three data lines are loaded correctly, i.e. the missing column of data has NaN. However, the first two data lines are mangled; key1 value of the second line was loaded into x3 value of the first line. This was not the case in version 0.9.0."""
2671,9823618,bluefir,wesm,2013-01-09 22:23:21,2013-01-20 03:04:17,2013-01-20 03:04:17,closed,,0.10.1,3,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2671,b'nosetests: ERROR: test using ndim tables in new ways',"b'nosetests generated the following error:\r\n\r\n>======================================================================\r\nERROR: test using ndim tables in new ways\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\tests\\test_pytables.py"", line 34\r\n6, in test_ndim_indexables\r\n    tm.assert_panel4d_equal(self.store[\'p4d\'], p4d)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 215, in __get\r\nitem__\r\n    return self.get(key)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 337, in get\r\n    return self._read_group(group)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 850, in _read\r\n_group\r\n    v = handler(group, where, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 646, in _read\r\n_ndim_table\r\n    return t.read(where)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py"", line 1605, in read\r\n\r\n>    obj = obj.transpose(*tuple(Series(self.data_orientation).argsort()))\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\panel.py"", line 1153, in trans\r\npose\r\n    axes = [self._get_axis_number(kwargs[a]) for a in self._AXIS_ORDERS]\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.py"", line 45, in _get_\r\naxis_number\r\n    return cls._AXIS_NUMBERS[axis]\r\nKeyError: 1\r\n\r\n----------------------------------------------------------------------'"
2668,9808163,hayd,wesm,2013-01-09 15:25:55,2013-01-20 01:29:48,2013-01-20 01:29:42,closed,wesm,0.10.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2668,b'Odd behaviour in read_csv',"b""This is a strange example from [this StackOverflow question](http://stackoverflow.com/questions/14237749/pandas-read-csv-strange-behavior-for-empty-default-values):\r\n\r\n```\r\nDate,Currenncy,Symbol,Type,Units,UnitPrice,Cost,Tax\r\n2012-03-14,USD,AAPL,BUY,1000\r\n2012-05-12,USD,SBUX,SELL,500\r\n```\r\n\r\n```\r\nIn [1]: pd.read_csv('ttt.csv', index_col=0, parse_dates=True)\r\nOut[1]: \r\n           Currenncy Symbol  Type  Units   UnitPrice       Cost       Tax\r\nDate                                                                     \r\n2012-03-14       USD   AAPL   BUY   1000  2012-05-12  012-05-12  12-05-12\r\n2012-02-05       USD   SBUX  SELL    500         NaN        NaN       NaN\r\n```"""
2659,9783930,wesm,wesm,2013-01-08 21:33:27,2014-01-23 16:44:09,2013-03-28 05:09:16,closed,,0.11,5,Bug,https://api.github.com/repos/pydata/pandas/issues/2659,b'Cyclic GC issues',"b""A mystery to be debugged soon:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\narr = np.random.randn(100000, 5)\r\n\r\ndef leak():\r\n    for i in xrange(10000):\r\n        df = pd.DataFrame(arr.copy())\r\n        result = df.xs(1000)\r\n        # result = df.ix[5000]\r\n\r\nif __name__ == '__main__':\r\n    leak()\r\n```"""
2658,9782584,Bklyn,y-p,2013-01-08 20:54:08,2013-01-10 23:37:49,2013-01-10 23:37:49,closed,,0.10.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2658,"b""DateTime indexing: DateParseError: global name 'tz' is not defined""","b'I have a DataFrame with a DateTime index that I have read from a CSV file:\r\n\r\n    In [131]: d=pd.read_csv(\'data.csv\', parse_dates=[0], index_col=0)\r\n\r\nThe index values consist  of ISO-8601-formatted timestamps of the form ""2012-12-28T09:00:54.273000-0500"", which Pandas seems to handle perfectly:\r\n\r\n    In [132]: d.index\r\n    Out[132]: \r\n    <class \'pandas.tseries.index.DatetimeIndex\'>\r\n    [2012-12-28 09:00:54.272000, ..., 2012-12-28 09:59:46.518000]\r\n    Length: 1310, Freq: None, Timezone: tzoffset(None, -18000)\r\n\r\nBut if I try to access data using stringified times, I get this error (I\'ve left the last 2 stack fames for brevity):\r\n\r\n    In [133]: d[\'2012-12-28 09:30:00-05:00\':]\r\n    ---------------------------------------------------------------------------\r\n    DateParseError                            Traceback (most recent call last)\r\n    <ipython-input-133-4564ed4dbf71> in <module>()\r\n    ----> 1 d[\'2012-12-28 09:30:00-05:00\':]\r\n    \r\n    <...elided...>\r\n\r\n    /usr/local/lib/python2.7/dist-packages/pandas/tseries/index.pyc in _get_string_slice(self, key)\r\n       1126         freq = getattr(self, \'freqstr\',\r\n       1127                        getattr(self, \'inferred_freq\', None))\r\n    -> 1128         asdt, parsed, reso = parse_time_string(key, freq)\r\n       1129         key = asdt\r\n       1130         loc = self._partial_date_slice(reso, parsed)\r\n    \r\n    /usr/local/lib/python2.7/dist-packages/pandas/tseries/tools.pyc in parse_time_string(arg, freq, dayfirst, yearfirst)\r\n        234                                       yearfirst=yearfirst)\r\n        235     except Exception, e:\r\n    --> 236         raise DateParseError(e)\r\n        237 \r\n        238     if parsed is None:\r\n    \r\n    DateParseError: global name \'tz\' is not defined\r\n\r\nIf I specify the timestamp without the GMT offset, I must specify it in GMT (e.g. 9:30am my time is ""2012-12-28 14:30:00"") which is awkward.  I can use localized datetime objects instead of strings, but this is also a bit of a pain.\r\n\r\nIs there a fix for this ""tz"" error so I can use plain ol\' strings?  Thanks!\r\n\r\n'"
2654,9741515,floux,wesm,2013-01-07 17:39:54,2013-09-30 09:28:25,2013-01-21 19:07:46,closed,wesm,0.10.1,3,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2654,b'read_csv in combination with index_col and usecols',"b""Starting point:\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/io.html#index-columns-and-trailing-delimiters\r\n\r\nIf there is one more column of data than there are colum names, usecols exhibits some (at least for me) unintuitive behavior:\r\n\r\n```python\r\n>>> data = 'a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10'\r\n>>> pd.read_csv(StringIO(data))\r\n        a    b     c\r\n4   apple  bat   5.7\r\n8  orange  cow  10.0\r\n>>> pd.read_csv(StringIO(data), usecols=['a', 'b'])\r\n   a       b\r\n0  4   apple\r\n1  8  orange\r\n>>>\r\n```\r\n\r\nI was expecting it to be equal to\r\n```python\r\n>>> pd.read_csv(StringIO(data))[['a', 'b']]\r\n        a    b\r\n4   apple  bat\r\n8  orange  cow\r\n```\r\nI am not sure if my expectation is unfounded, though, and that this behavior is indeed intentional?"""
2651,9724341,lexual,lexual,2013-01-07 03:28:41,2013-01-24 05:41:58,2013-01-24 05:41:58,closed,wesm,0.11,7,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2651,b'Regression: pandas 0.10 fails to read an excel file that pandas 0.9 can read',"b'I have an excel file that reads fine in pandas 0.9, fails to read in pandas 0.10\r\n\r\n      fname = sys.argv[1]                                                            \r\n      xlsx = pd.ExcelFile(fname)                                                     \r\n      sheet = xlsx.sheet_names[0]                                                    \r\n                                                                                     \r\n      # this works in pandas 0.9, fails in pandas 0.10.0                             \r\n     df = xlsx.parse(sheet, index_col=[1, 2])                                                                                                                                                                                                                                                                              \r\n\r\nTriggers this error:\r\n\r\n\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    /home/vagrant/.virtualenvs/foo/local/lib/python2.7/site-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)\r\n        176             else:\r\n        177                 filename = fname\r\n    --> 178             __builtin__.execfile(filename, *where)\r\n    \r\n    /tmp/X/pandas_error_test.py in <module>()\r\n         15 \r\n         16 if __name__ == \'__main__\':\r\n    ---> 17     main()\r\n    \r\n    /tmp/X/pandas_error_test.py in main(files)\r\n         11 \r\n         12     # this works in pandas 0.9, fails in pandas 0.10.0\r\n    ---> 13     df = xlsx.parse(sheet, index_col=[1, 2])\r\n         14 \r\n         15 \r\n    \r\n    /home/vagrant/.virtualenvs/foo/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in parse(self, sheetname, header, skiprows, skip_footer, index_col, parse_cols, parse_dates, date_parser, na_values, thousands, chunksize, **kwds)\r\n       1866                                      thousands=thousands,\r\n       1867                                      chunksize=chunksize,\r\n    -> 1868                                      skip_footer=skip_footer)\r\n       1869 \r\n       1870     def _should_parse(self, i, parse_cols):\r\n    \r\n    /home/vagrant/.virtualenvs/foo/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in _parse_xlsx(self, sheetname, header, skiprows, skip_footer, index_col, has_index_names, parse_cols, parse_dates, date_parser, na_values, thousands, chunksize)\r\n       1934                             chunksize=chunksize)\r\n       1935 \r\n    -> 1936         return parser.read()\r\n       1937 \r\n       1938     def _parse_xls(self, sheetname, header=0, skiprows=None,\r\n    \r\n    /home/vagrant/.virtualenvs/foo/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in read(self, nrows)\r\n        622             #     self._engine.set_error_bad_lines(False)\r\n        623 \r\n    --> 624         ret = self._engine.read(nrows)\r\n        625 \r\n        626         if self.options.get(\'as_recarray\'):\r\n    \r\n    /home/vagrant/.virtualenvs/foo/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in read(self, rows)\r\n       1245             content = content[1:]\r\n       1246 \r\n    -> 1247         alldata = self._rows_to_cols(content)\r\n       1248         data = self._exclude_implicit_index(alldata)\r\n       1249 \r\n    \r\n    /home/vagrant/.virtualenvs/foo/local/lib/python2.7/site-packages/pandas/io/parsers.pyc in _rows_to_cols(self, content)\r\n       1461             msg = (\'Expected %d fields in line %d, saw %d\' %\r\n       1462                    (col_len, row_num + 1, zip_len))\r\n    -> 1463             raise ValueError(msg)\r\n       1464 \r\n       1465         return zipped_content\r\n    \r\n    ValueError: Expected 10 fields in line 2, saw 11\r\n\r\n\r\nI suspect a similar error happened when building the docs on the pandas site. See:\r\n\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/whatsnew.html#v0-10-0-december-17-2012\r\n\r\nAnd search for ""expected"", and you\'ll see a similar error is being triggered. See screenshot for details.\r\n![Screen Shot 2013-01-07 at 2 10 20 PM](https://f.cloud.github.com/assets/410907/46609/2c40e826-587a-11e2-9b28-397e01458e7b.png)\r\n\r\nI haven\'t attached the file, as it contains a client\'s data.'"
2640,9701887,pikeas,changhiskhan,2013-01-05 03:18:12,2013-01-19 23:13:44,2013-01-19 23:13:44,closed,changhiskhan,0.10.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2640,b'DataFrame.resample() fails on empty DataFrame',"b'If you resample() an empty DataFrame, you hit a TypeError in the else catch-all at line 98 of tseries/resample.py. This should either raise a different error, or return the passed empty DataFrame.'"
2633,9662040,pikeas,wesm,2013-01-04 01:28:46,2013-01-19 23:55:20,2013-01-19 23:55:20,closed,wesm,0.10.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2633,b'DataFrame.from_records() KeyError when passed empty list',"b'What it says on the tin. from_records() fails with a KeyError if passed an empty list. If the list is empty, an empty DataFrame should be created, conforming to the other arguments (columns, etc).'"
2629,9636144,asadovsky,wesm,2013-01-03 05:30:30,2013-01-20 02:33:16,2013-01-20 02:33:16,closed,changhiskhan,0.10.1,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2629,b'Subtracting Series of Timestamps gives wrong result',"b'Example:\r\n\r\nIn [11]: d0 = pd.to_datetime(pd.Series([""12/1/2011""]))\r\n\r\nIn [12]: d1 = pd.to_datetime(pd.Series([""12/3/2011""]))\r\n\r\nIn [13]: d1[0] - d0[0]\r\nOut[13]: datetime.timedelta(2)\r\n\r\nIn [14]: (d1 - d0)[0]\r\nOut[14]: 2000 days, 0:00:00\r\n'"
2627,9626958,dalejung,wesm,2013-01-02 20:47:31,2013-01-20 18:53:58,2013-01-20 18:51:53,closed,wesm,0.10.1,9,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2627,"b""apply on np.datetime col doesn't upconvert to Timestamp""","b'```python\r\nimport pandas as pd\r\n\r\ns = pd.Series(pd.date_range(start=""2000/1/1"", freq=""D"", periods=10))\r\ns.apply(lambda x: x.date())\r\n\r\n# AttributeError: \'numpy.datetime64\' object has no attribute \'date\'\r\n```\r\n\r\nThis is an error from some unit tests. Did something major change with how datetimes are handled? I recall that np.datetime64 was autoboxed into Timestamps. \r\n\r\nI\'ll update after poking around. '"
2626,9624465,asadovsky,changhiskhan,2013-01-02 19:18:52,2013-01-20 04:33:14,2013-01-20 04:33:14,closed,,0.10.1,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2626,b'Series.combine_first() misbehaves with Timestamp data',"b'Best illustrated by example:\r\n\r\nIn [957]: s0 = pd.Series([""2010"", np.NaN])\r\n\r\nIn [958]: s1 = pd.Series([np.NaN, ""2011""])\r\n\r\nIn [959]: s0.combine_first(s1)\r\nOut[959]: \r\n0    2010\r\n1    2011\r\n\r\nIn [960]: s0 = pd.to_datetime(pd.Series([""2010"", np.NaN]))\r\n\r\nIn [961]: s1 = pd.to_datetime(pd.Series([np.NaN, ""2011""]))\r\n\r\nIn [962]: s0.combine_first(s1)\r\nOut[962]: \r\n0   2221-02-23 04:49:47.750490112\r\n1   2051-05-17 05:40:40.331386880'"
2625,9620125,hayd,changhiskhan,2013-01-02 16:38:13,2013-01-05 14:18:01,2013-01-05 14:18:01,closed,,0.10.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2625,b'quantile throws error if not convertible to float',"b'If we try and `quantile` a DataFrame with string entries which are not convertible, there is a ValueError. Should this behave like mean (and ignore these entries)? (taken from [this StackOverflow question](http://stackoverflow.com/questions/14125428/how-to-apply-quantile-to-pandas-groupby-object)).\r\n\r\n```\r\nIn [1]: df = DataFrame({\'col1\':[\'A\',\'A\',\'B\',\'B\'], \'col2\':[1,2,3,4]})\r\n\r\nIn [2]: df\r\nOut[2]:\r\n  col1  col2\r\n0    A     1\r\n1    A     2\r\n2    B     3\r\n3    B     4\r\n\r\n\r\nIn [3]: g = df.groupby(\'col1\')\r\n\r\nIn [4]: g.mean()\r\nOut[4]: \r\n      col2\r\ncol1      \r\nA      1.5\r\nB      3.5\r\n\r\nIn [5]: g.quantile()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/andy/<ipython-input-70-8b0757805794> in <module>()\r\n----> 1 g.quantile()\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/groupby.pyc in wrapper(*args, **kwargs)\r\n    258                 return self.apply(curried_with_axis)\r\n    259             except Exception:\r\n--> 260                 return self.apply(curried)\r\n    261 \r\n    262         return wrapper\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/groupby.pyc in apply(self, func, *args, **kwargs)\r\n    319         func = _intercept_function(func)\r\n    320         f = lambda g: func(g, *args, **kwargs)\r\n--> 321         return self._python_apply_general(f)\r\n    322 \r\n    323     def _python_apply_general(self, f):\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/groupby.pyc in _python_apply_general(self, f)\r\n    322 \r\n    323     def _python_apply_general(self, f):\r\n--> 324         keys, values, mutated = self.grouper.apply(f, self.obj, self.axis)\r\n    325 \r\n    326         return self._wrap_applied_output(keys, values,\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/groupby.pyc in apply(self, f, data, axis, keep_internal)\r\n    594             # group might be modified\r\n\r\n    595             group_axes = _get_axes(group)\r\n--> 596             res = f(group)\r\n    597             if not _is_indexed_like(res, group_axes):\r\n    598                 mutated = True\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/groupby.pyc in <lambda>(g)\r\n    318         """"""\r\n    319         func = _intercept_function(func)\r\n--> 320         f = lambda g: func(g, *args, **kwargs)\r\n    321         return self._python_apply_general(f)\r\n    322 \r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/groupby.pyc in curried(x)\r\n    253 \r\n    254             def curried(x):\r\n--> 255                 return f(x, *args, **kwargs)\r\n    256 \r\n    257             try:\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/frame.pyc in quantile(self, q, axis)\r\n   4946                 return _quantile(arr, per)\r\n   4947 \r\n-> 4948         return self.apply(f, axis=axis)\r\n   4949 \r\n   4950     def clip(self, upper=None, lower=None):\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\r\n   4079                     return self._apply_raw(f, axis)\r\n   4080                 else:\r\n-> 4081                     return self._apply_standard(f, axis)\r\n   4082             else:\r\n   4083                 return self._apply_broadcast(f, axis)\r\n\r\n/usr/lib/pymodules/python2.7/pandas/core/frame.pyc in _apply_standard(self, func, axis, ignore_failures)\r\n   4154                     # no k defined yet\r\n\r\n   4155                     pass\r\n-> 4156                 raise e\r\n   4157 \r\n   4158         if len(results) > 0 and _is_sequence(results[0]):\r\n\r\nValueError: (\'could not convert string to float: A\', u\'occurred at index col1\')\r\n```'"
2624,9604028,dalejung,wesm,2013-01-01 19:18:45,2013-12-04 00:40:13,2013-01-20 19:51:00,closed,wesm,0.10.1,2,Bug;Enhancement;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2624,b'pd.concat merges object-datetime and np.datetime columns incorrectly.',"b'```python \r\nrows = []\r\nrows.append([pd.datetime(2010, 1, 1), 1])\r\nrows.append([pd.datetime(2010, 1, 2), \'hi\']) # test col upconverts to obj\r\n\r\ndf2_obj = pd.DataFrame.from_records(rows, columns=[\'date\', \'test\'])\r\n\r\nind = pd.date_range(start=""2000/1/1"", freq=""D"", periods=10)\r\ndf1 = pd.DataFrame({\'date\': ind, \'test\':range(10)})\r\n\r\ncdf = pd.concat([df1, df2_obj])\r\ncdf\r\n```\r\n\r\nThe rows where date was a np.datetime are converted to ints, while the object dates are left as datetimes. \r\n\r\nI\'m not sure if this is a bug. I would expect the np.datetime column to upconvert into Timestamp objects? \r\n\r\nI ran into this from a test failure. Thing is, it\'s a fairly old test and it used to pass. I checked it against 0.9.1 and 0.10.0 which failed as well. So I\'m not sure if I was on a previous commit that worked differently. it\'s weird. \r\n\r\nEither way, just wanted to get a check on what the expected behavior is. \r\n\r\nEDIT: http://nbviewer.ipython.org/4428511/'"
2623,9603957,dalejung,jreback,2013-01-01 19:07:33,2013-03-22 21:48:21,2013-03-22 21:48:21,closed,wesm,0.11,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2623,b'DataFrame.from_records incorrectly up-converts dtypes to object.',"b""```python\r\nrows = []\r\nrows.append([pd.datetime(2010, 1, 1), 1])\r\nrows.append([pd.datetime(2010, 1, 2), 'hi']) # test col upconverts to obj\r\n\r\ndf2_obj = pd.DataFrame.from_records(rows, columns=['date', 'test'])\r\n\r\nprint df2_obj.date.dtype # object\r\n```\r\nIf the `'hi'` is changed to an `int` then `df2_obj.date.dtype` is equal to `M8[ns]`\r\n"""
2621,9598733,rafaljozefowicz,wesm,2012-12-31 22:47:32,2013-01-20 02:45:41,2013-01-20 02:45:41,closed,wesm,0.10.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2621,b'DatetimeIndex.drop is losing timezone information',"b'```python\r\n\r\n    def test_index_tz(self):\r\n        ind = pd.date_range(""2012-12-01"", periods=10, tz=""utc"")\r\n        ind = ind.drop(ind[-1])\r\n        self.assertIsNone(ind.tz)\r\n```\r\n\r\nThat seems counter-intuitive.\r\nFreq information is also dropped, but that is IMHO less important\r\n\r\nThis is on pandas 0.10'"
2618,9587446,chadherman,y-p,2012-12-31 01:02:31,2013-03-28 05:54:41,2013-03-28 05:54:41,closed,,0.11,3,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2618,b'Possible date parsing bug in read_table',"b'\r\nI recently updated Pandas to 0.10.0 under Python 2.7.3. I have encountered problems with the read_table function. My program read in the following file:\r\n>ftp://ftp.ncdc.noaa.gov/pub/data/anomalies/monthly.land_ocean.90S.90N.df_1901-2000mean.dat\r\n\r\nHere\'s the head of the file:\r\n<pre>\r\n1880  1   -0.0760\r\n1880  2   -0.2099\r\n1880  3   -0.2170\r\n1880  4   -0.1180\r\n1880  5   -0.1680\r\n1880  6   -0.2055\r\n1880  7   -0.1863\r\n1880  8   -0.1128\r\n1880  9   -0.1192\r\n1880 10   -0.1951\r\n</pre>\r\n\r\nThe code to load the data:\r\n<pre>\r\nimport pandas as pd\r\nnoaa_file = ""monthly.land_ocean.90S.90N.df_1901-2000mean.dat""\r\n\r\nnoaa = pd.read_table(noaa_file, header=None, sep=r\'\\s*\', parse_dates=[[0,1]], index_col=0, squeeze=True, na_values=\'-999.0000\').to_period(freq=\'M\')\r\n</pre>\r\n\r\nThis throws an exception:\r\n<pre>AttributeError: \'Series\' object has no attribute \'to_period\'</pre>\r\n\r\nI invoke to_period because the dates that were parsed were appearing as YYYY-MM-DD. The data is monthly and there is no need for a day component. I dropped the to_period method and the error disappeared. But I noticed something strange about the index:\r\n<pre>             2\r\n0_1           \r\n1880 1 -0.0760\r\n1880 2 -0.2099\r\n1880 3 -0.2170\r\n1880 4 -0.1180\r\n1880 5 -0.1680\r\n</pre>\r\n\r\nThe index is a pandas.core.index.Index object. Under the previous version of the library, the index was a pandas.tseries.period.PeriodIndex object. It looks like the dates aren\'t be parsed at all. If I drop the to_period method and follow-up with\r\n<pre>noaa_temp = pd.Series(noaa_temp.values, pd.PeriodIndex(noaa_temp.index, freq=\'M\'))</pre>\r\n\r\nthen I get exactly what I need and what the original one-liner at the top produced under the previous version. The only way I can accomplish this in ""one"" line is to define a parser:\r\n\r\n<pre>\r\nfrom datetime import datetime\r\nparse = lambda x: datetime.strptime(x, \'%Y %m\')\r\n\r\nnoaa = pd.read_table(noaa_file, header=None, delim_whitespace=True, parse_dates=[[0,1]], index_col=0, squeeze=True, na_values=\'-999.0000\', date_parser=parse).to_period(freq=\'M\')\r\n</pre>\r\n\r\nNow everything works. I think this is a bug in the date parser.\r\n'"
2617,9585733,minrk,changhiskhan,2012-12-30 21:04:22,2013-01-20 01:46:13,2013-01-20 01:46:13,closed,changhiskhan,0.10.1,2,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2617,b'HTML reprs not properly escaped',"b'Object representations are not escaped for HTML when giving HTML repr to IPython Notebook,\r\nfor instance if the items themselves are types, whose reprs include `<`, and are interpreted as tags.\r\n\r\n[an example notebook](http://nbviewer.ipython.org/4415176)\r\n\r\nOriginally from [this SO question](http://stackoverflow.com/questions/14092798).\r\n'"
2616,9578496,gerigk,wesm,2012-12-29 23:32:29,2012-12-30 23:01:23,2012-12-30 16:11:04,closed,,0.10.1,4,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/2616,"b'Weird IndexError /segfault for df.groupby(level=""levelname"") if na in level'","b'I know that there are na issues for indices but I have never encountered this behavior before (pandas master):\r\n(fully functional code depending on the requests  and xlrd library and a berlin open data set)\r\n```\r\nimport pandas as pd\r\nimport requests\r\nthe_data = requests.get(\'http://www.berlin.de/imperia/md/content/senatsverwaltungen/finanzen/haushalt/ansatzn2013.xls?download.html\',\r\n    stream=True)\r\nxl_file = pd.io.parsers.ExcelFile(the_data.raw)  # only works with xlrd installed\r\ndf = xl_file.parse(\'Ansatz 2013\')\r\noriginal_data = df.copy()\r\ndf.columns = [\'bereich\', \'einzelplan\', \'kapitel\', \'titelart\', \'titel\', \'titelbezeichnung\', \'funktion\', \'betrag_tausend\']\r\ndf[\'bereich_id\'] = df.bereich.str.slice(start=1, stop=3).astype(int)\r\ndf[\'bereich\'] = df.bereich.str.slice(start=5)\r\ndf[\'einzelplan_id\'] = df.einzelplan.str.slice(start=1, stop=3).astype(int)\r\ndf[\'einzelplan\'] = df.einzelplan.str.slice(start=5)\r\ndf[\'kapitel_id\'] = df.kapitel.str.slice(start=1, stop=5).astype(int)\r\ndf[\'kapitel\'] = df.kapitel.str.slice(start=7)\r\ndf[\'funktion_id\'] = df.funktion.str.slice(start=1, stop=4).astype(float) # one missing value\r\ndf[\'funktion\'] = df.funktion.str.slice(start=6)\r\ndf[\'betrag\'] = df.betrag_tausend * 1000\r\ndel df[\'betrag_tausend\']\r\ndf = df[[\'bereich\', \'bereich_id\', \'einzelplan\', \'einzelplan_id\',\r\n    \'kapitel\', \'kapitel_id\', \'funktion\', \'funktion_id\',\r\n    \'titel\', \'titelbezeichnung\', \'titelart\', \'betrag\']]\r\ndf.set_index([\'bereich\', \'bereich_id\', \'einzelplan\', \'einzelplan_id\',\r\n    \'kapitel\', \'kapitel_id\', \'funktion\', \'funktion_id\',\r\n    \'titel\', \'titelbezeichnung\', \'titelart\'], inplace=True)\r\n```\r\nnow calling\r\n```\r\ndf.groupby(level=[\'bereich\']).sum()\r\n```\r\nworks nicely whereas \r\n\r\n```\r\ndf.groupby(level=[\'funktion_id\']).sum()\r\n```\r\nresults in\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-3-d8991eeaebb9> in <module>()\r\n----> 1 df.groupby(level=[\'funktion_id\']).sum()\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in groupby(self, by, axis, level, as_index, sort, group_keys)\r\n    132         from pandas.core.groupby import groupby\r\n    133         return groupby(self, by, axis=axis, level=level, as_index=as_index,\r\n--> 134                        sort=sort, group_keys=group_keys)\r\n    135 \r\n    136     def asfreq(self, freq, method=None, how=None, normalize=False):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in groupby(obj, by, **kwds)\r\n    498         raise TypeError(\'invalid type: %s\' % type(obj))\r\n    499 \r\n--> 500     return klass(obj, by, **kwds)\r\n    501 \r\n    502 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in __init__(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys)\r\n    189         if grouper is None:\r\n    190             grouper, exclusions = _get_grouper(obj, keys, axis=axis,\r\n--> 191                                                level=level, sort=sort)\r\n    192 \r\n    193         self.grouper = grouper\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in _get_grouper(obj, key, axis, level, sort)\r\n   1244             name = gpr\r\n   1245             gpr = obj[gpr]\r\n-> 1246         ping = Grouping(group_axis, gpr, name=name, level=level, sort=sort)\r\n   1247         groupings.append(ping)\r\n   1248 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in __init__(self, index, grouper, name, level, sort)\r\n   1115                 self._labels = labels\r\n   1116                 self._group_index = level_index\r\n-> 1117                 self.grouper = level_index.take(labels)\r\n   1118         else:\r\n   1119             if isinstance(self.grouper, (list, tuple)):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/index.pyc in take(self, indexer, axis)\r\n    415         """"""\r\n    416         indexer = com._ensure_platform_int(indexer)\r\n--> 417         taken = self.view(np.ndarray).take(indexer)\r\n    418         return self._constructor(taken, name=self.name)\r\n    419 \r\n\r\nIndexError: index 138 is out of bounds for axis 0 with size 138\r\n```\r\nand\r\n\r\n```\r\ndf.groupby(level=[\'funktion\']).sum()\r\n```\r\nsegfaults\r\n\r\n'"
2612,9573987,wesm,jreback,2012-12-29 14:50:05,2015-10-03 14:41:03,2015-10-03 14:41:03,closed,,0.17.0,9,Bug;Output-Formatting;Unicode,https://api.github.com/repos/pydata/pandas/issues/2612,b'Unicode column misalignment',"b""```\r\nIn [17]: open('/home/wesm/tmp/foo.csv', 'rb').read()\r\nOut[17]: '\\xe6\\xb8\\xac\\xe8\\xa9\\xa6\\xe4\\xb8\\x80,\\xe6\\xb8\\xac\\xe8\\xa9\\xa6\\xe4\\xb8\\x89\\r\\nabc@example.com,\\xe6\\xb8\\xac\\xe8\\xa9\\xa6\\xe4\\xb8\\x80\\r\\ndef@example.com,\\xe6\\xb8\\xac\\xe8\\xa9\\xa6\\xe4\\xba\\x8c\\r\\nghi@example.com,\\xe6\\xb8\\xac\\xe8\\xa9\\xa6\\xe4\\xb8\\x89\\r\\n'\r\n\r\nIn [18]: read_csv('/home/wesm/tmp/foo.csv', encoding='utf-8')\r\nOut[18]: \r\n               \x9cy\xd4\x87\xd2\xbb  \x9cy\xd4\x87\xc8\xfd\r\n0  abc@example.com  \x9cy\xd4\x87\xd2\xbb\r\n1  def@example.com  \x9cy\xd4\x87\xb6\xfe\r\n2  ghi@example.com  \x9cy\xd4\x87\xc8\xfd\r\n\r\nIn [24]: df\r\nOut[24]: \r\n               \x9cy\xd4\x87\xd2\xbb  \x9cy\xd4\x87\xc8\xfd\r\n0  abc@example.com  \x9cy\xd4\x87\xd2\xbb\r\n1  def@example.com  \x9cy\xd4\x87\xb6\xfe\r\n2  ghi@example.com  \x9cy\xd4\x87\xc8\xfd\r\n\r\nIn [25]: df.columns[0]\r\nOut[25]: u'\\u6e2c\\u8a66\\u4e00'\r\n\r\nIn [26]: df.columns[1]\r\nOut[26]: u'\\u6e2c\\u8a66\\u4e09'\r\n```"""
2610,9573237,janschulz,wesm,2012-12-29 13:09:34,2013-01-02 17:21:00,2013-01-02 17:21:00,closed,,0.10.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2610,b'describe() fails with stacktrace for an empty DataFrame',"b'This is from a first try to select all rows where an ID is in another dataset. The code does not work as intended (see last line), but I think that the `describe()` call shouldn\'t fail.\r\n\r\n[column names changed in output; output from last call shortened]\r\n \r\n```\r\nIn [47]: all\r\nOut[47]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nInt64Index: 974757 entries, 0 to 974756\r\nData columns:\r\neid                  974757  non-null values\r\nnumber               974757  non-null values\r\na                    972510  non-null values\r\nb                    974757  non-null values\r\nc                    929268  non-null values\r\nd                    922700  non-null values\r\ne                    974757  non-null values\r\ndtypes: int64(1), object(6)\r\nIn [48]: subset = all[all[""eid""].isin(other[""eid""])]\r\nIn [49]: subset\r\nOut[49]: \r\n\r\nInt64Index([], dtype=int64)\r\nEmpty DataFrame\r\n \r\n\r\nIn [50]: subset.describe()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-50-ff735ef04a17> in <module>()\r\n----> 1 business_authors.describe()\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in describe(self, percentile_width)\r\n   4539             series = self[column]\r\n   4540             destat.append([series.count(), series.mean(), series.std(),\r\n-> 4541                            series.min(), series.quantile(lb), series.median(),\r\n   4542                            series.quantile(ub), series.max()])\r\n   4543 \r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in min(self, axis, out, skipna, level)\r\n   1320         if level is not None:\r\n   1321             return self._agg_by_level(\'min\', level=level, skipna=skipna)\r\n-> 1322         return nanops.nanmin(self.values, skipna=skipna)\r\n   1323 \r\n   1324     @Substitution(name=\'maximum\', shortname=\'max\',\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\nanops.pyc in f(values, axis, skipna, **kwds)\r\n     46                 result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n     47         except Exception:\r\n---> 48             result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n     49 \r\n     50         return result\r\n\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\nanops.pyc in _nanmin(values, axis, skipna)\r\n    179              or values.size == 0):\r\n    180             result = values.sum(axis)\r\n--> 181             result.fill(np.nan)\r\n    182         else:\r\n    183             result = values.min(axis)\r\n\r\n\r\nValueError: cannot convert float NaN to integer\r\n\r\n\r\n\r\nIn[51]: all[""eid""].isin(other[""eid""])\r\nOut[51]: \r\n0     False\r\n1     False\r\n2     False\r\n3     False\r\n4     False\r\n...\r\n974752    False\r\n974753    False\r\n974754    False\r\n974755    False\r\n974756    False\r\nName: eid, Length: 974757\r\n```\r\n'"
2609,9566634,hayd,changhiskhan,2012-12-28 21:40:27,2013-01-20 05:00:55,2013-01-20 05:00:55,closed,,0.10.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2609,b'Plotting a dataframe with date unordered date index',"b'Migrated from [StackOverflow](http://stackoverflow.com/questions/14075326/pandas-runtimeerror-in-tseries-convertor-when-plotting). The following gives an exception (when trying to plot a DataFrame with date indices). I\'m using \'0.10.0b1\'.\r\n\r\n```\r\nIn [1]: df = DataFrame(randn(3,1),index=[date(2012,10,1),date(2012,9,1),date(2012,8,1)], columns=[\'test\'])\r\n\r\n#the same bug is present using datetime rather than date\r\nIn [2]: df\r\nOut[2]: \r\n                test\r\n2012-10-01 -0.315065\r\n2012-09-01  0.100678\r\n2012-08-01 -0.230353\r\n\r\nIn [4]: df.plot() \r\n#error\r\n```\r\n\r\nHere is the traceback:\r\n```\r\nException in Tkinter callback\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/lib-tk/Tkinter.py"", line 1413, in __call__\r\n    return self.func(*args)\r\n  File ""/usr/lib/python2.7/lib-tk/Tkinter.py"", line 498, in callit\r\n    func(*args)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.py"", line 254, in idle_draw\r\n    self.draw()\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.py"", line 239, in draw\r\n    FigureCanvasAgg.draw(self)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/backends/backend_agg.py"", line 421, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/figure.py"", line 898, in draw\r\n    func(*args)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/axes.py"", line 1997, in draw\r\n    a.draw(renderer)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/axis.py"", line 1041, in draw\r\n    ticks_to_draw = self._update_ticks(renderer)\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/axis.py"", line 931, in _update_ticks\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/axis.py"", line 878, in iter_ticks\r\n    majorLocs = self.major.locator()\r\n  File ""/usr/lib/pymodules/python2.7/matplotlib/dates.py"", line 750, in __call__\r\n    return self._locator()\r\n  File ""/usr/lib/pymodules/python2.7/pandas/tseries/converter.py"", line 317, in __call__\r\n    (estimate, dmin, dmax, self.MAXTICKS * 2))\r\nRuntimeError: MillisecondLocator estimated to generate 5270400 ticks from 2012-08-01 00:00:00+00:00 to 2012-10-01 00:00:00+00:00: exceeds Locator.MAXTICKS* 2 (2000) \r\n```\r\n\r\nNote that it plots ok, using `df.sort().plot()`.'"
2608,9565224,mkleehammer,wesm,2012-12-28 20:10:10,2013-01-21 19:30:16,2013-01-21 19:30:16,closed,wesm,0.10.1,0,Bug;Data IO;Multithreading,https://api.github.com/repos/pydata/pandas/issues/2608,b'read_csv crashes when run on multiple threads',"b'Running multiple threads each calling read_csv crashes on OS/X.  I\'ve seen two traps, which I\'ll put into an attachment.  Sometimes it says ""Fatal Python error: GC object already tracked\\nAbort trap: 6"""" and sometimes ""Segmentation fault: 11"".\r\n\r\nI\'ve put together a small example to reproduce it: https://gist.github.com/4401461\r\n\r\nI\'ve also added some of the OS/X crash report in case it isn\'t crashing on your install.\r\n\r\nI\'m using the built-in Python 2.7.2 on OS/X.  Pandas 0.10.0 was built locally using pip install -U pandas.  I don\'t know if this is new to 0.10.0 since I wasn\'t threading 0.9 yet.\r\n\r\nThis might be relevant for the GC already tracked: http://pyrit.wordpress.com/2010/02/18/385/\r\n'"
2605,9551516,tlmaloney,wesm,2012-12-28 05:01:32,2012-12-28 16:43:45,2012-12-28 13:48:05,closed,,0.10.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2605,b'AssertionError when using apply after GroupBy',"b'The following code raises an AssertionError with pandas 0.10.0, but works fine in 0.9.1. The error still exists in the latest dev version here. The code comes from Wes\' book, pages 33-36. The data files are from https://github.com/pydata/pydata-book\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nyears = range(1880, 2011)\r\npieces  = []\r\ncolumns = [\'name\', \'sex\', \'births\']\r\n\r\nfor year in years:\r\n    path = \'ch02/names/yob%d.txt\' % year\r\n    frame = pd.read_csv(path, names=columns)\r\n    frame[\'year\'] = year\r\n    pieces.append(frame)\r\n\r\nnames = pd.concat(pieces, ignore_index=True)\r\n\r\ndef get_top1000(group):\r\n    return group.sort_index(by=\'births\', ascending=False)[:1000]\r\n\r\ntop1000 = names.groupby([\'year\', \'sex\']).apply(get_top1000)\r\n```\r\nThe last line results in the following error trace:\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-8-569425503c6b> in <module>()\r\n----> 1 top1000 = names.groupby([\'year\', \'sex\']).apply(get_top1000)\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/groupby.pyc in apply(self, func, *args, **kwargs)\r\n    319         func = _intercept_function(func)\r\n    320         f = lambda g: func(g, *args, **kwargs)\r\n--> 321         return self._python_apply_general(f)\r\n    322 \r\n    323     def _python_apply_general(self, f):\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/groupby.pyc in _python_apply_general(self, f)\r\n    322 \r\n    323     def _python_apply_general(self, f):\r\n--> 324         keys, values, mutated = self.grouper.apply(f, self.obj, self.axis)\r\n    325 \r\n    326         return self._wrap_applied_output(keys, values,\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/groupby.pyc in apply(self, f, data, axis, keep_internal)\r\n    583         if hasattr(splitter, \'fast_apply\') and axis == 0:\r\n    584             try:\r\n--> 585                 values, mutated = splitter.fast_apply(f, group_keys)\r\n    586                 return group_keys, values, mutated\r\n    587             except lib.InvalidApply:\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/groupby.pyc in fast_apply(self, f, names)\r\n   2125 \r\n   2126         sdata = self._get_sorted_data()\r\n-> 2127         results, mutated = lib.apply_frame_axis0(sdata, f, names, starts, ends)\r\n   2128 \r\n   2129         return results, mutated\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/lib.so in pandas.lib.apply_frame_axis0 (pandas/lib.c:24934)()\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/frame.pyc in __setattr__(self, name, value)\r\n   2026                     super(DataFrame, self).__setattr__(name, value)\r\n   2027                 elif name in self.columns:\r\n-> 2028                     self[name] = value\r\n   2029                 else:\r\n   2030                     object.__setattr__(self, name, value)\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/frame.pyc in __setitem__(self, key, value)\r\n   2041         else:\r\n   2042             # set column\r\n-> 2043             self._set_item(key, value)\r\n   2044 \r\n   2045     def _boolean_set(self, key, value):\r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/frame.pyc in _set_item(self, key, value)\r\n   2076         ensure homogeneity.\r\n   2077         """"""\r\n-> 2078         value = self._sanitize_column(key, value)\r\n   2079         NDFrame._set_item(self, key, value)\r\n   2080 \r\n\r\n/home/tlmaloney/vedev/ve-pydata-book/local/lib/python2.7/site-packages/pandas/core/frame.pyc in _sanitize_column(self, key, value)\r\n   2110             else:\r\n   2111                 if len(value) != len(self.index):\r\n-> 2112                     raise AssertionError(\'Length of values does not match \'\r\n   2113                                          \'length of index\')\r\n   2114 \r\n\r\nAssertionError: Length of values does not match length of index\r\n```'"
2604,9548742,hauptmech,wesm,2012-12-28 00:03:16,2012-12-28 18:42:39,2012-12-28 18:42:39,closed,changhiskhan,0.10.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2604,b'Indexes created from datetimes that are tz aware but tz=None (pytz.FixedOffset()) lose tz awareness',b'The index created from datetimes with a FixedOffset tz get converted to the correct UTC time but lose their timezone awareness so that further operations with tz aware datetimes fail.\r\n\r\npytz.FixedOffset() timezones have tzname() return None\r\n\r\n\r\n'
2603,9547308,wesm,wesm,2012-12-27 22:22:48,2012-12-28 15:41:11,2012-12-28 15:41:11,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2603,b'Concat series with axis=1 does not preserve names when reindexing is necessary',
2601,9545135,dsm054,wesm,2012-12-27 20:13:30,2012-12-28 14:35:37,2012-12-28 14:35:37,closed,,0.10.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2601,b'read_table/csv unexpected type dependence on delimiter',"b'While answering a question on SO I came across something which puzzled me:\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n\'0.10.0b1\'\r\n>>> repr(open(\'cusip.txt\').read())\r\n""\'65248E10 11\\\\n55555E55 22\\\\n\'""\r\n>>> !cat cusip.txt\r\n65248E10 11\r\n55555E55 22\r\n>>> df = pd.read_table(""cusip.txt"", header=None, sep="" "")\r\n>>> df\r\n              0   1\r\n0  6.524800e+14  11\r\n1  5.555500e+59  22\r\n>>> type(df[0][0])\r\n<type \'numpy.float64\'>\r\n>>> df = pd.read_table(""cusip.txt"", header=None, sep=r""\\s+"")\r\n>>> df\r\n                     0   1\r\n0      652480000000000  11\r\n1 -9223372036854775808  22\r\n>>> type(df[0][0])\r\n<type \'numpy.int64\'>\r\n```\r\n\r\nChanging the delimiter from `"" ""` to `r""\\s+""` somehow triggered the interpretation of the first column as integers instead of floats.'"
2600,9529284,wesm,jreback,2012-12-27 00:12:12,2016-04-08 13:02:12,2016-04-08 13:02:12,closed,,No action,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2600,b'Buggy negative slicing with ix',"b'http://stackoverflow.com/questions/14035817/slicing-pandas-dataframe-with-negative-index-with-ix-method\r\n\r\n```\r\n    In [1]: df = pd.DataFrame(np.random.randn(10, 4))\r\n    \r\n    In [2]: df\r\n    Out[2]: \r\n              0         1         2         3\r\n    0 -3.100926 -0.580586 -1.216032  0.425951\r\n    1 -0.264271 -1.091915 -0.602675  0.099971\r\n    2 -0.846290  1.363663 -0.382874  0.065783\r\n    3 -0.099879 -0.679027 -0.708940  0.138728\r\n    4 -0.302597  0.753350 -0.112674 -1.253316\r\n    5 -0.213237 -0.467802  0.037350  0.369167\r\n    6  0.754915 -0.569134 -0.297824 -0.600527\r\n    7  0.644742  0.038862  0.216869  0.294149\r\n    8  0.101684  0.784329  0.218221  0.965897\r\n    9 -1.482837 -1.325625  1.008795 -0.150439\r\n    \r\n    In [3]: df.ix[-2:]\r\n    Out[3]: \r\n              0         1         2         3\r\n    0 -3.100926 -0.580586 -1.216032  0.425951\r\n    1 -0.264271 -1.091915 -0.602675  0.099971\r\n    2 -0.846290  1.363663 -0.382874  0.065783\r\n    3 -0.099879 -0.679027 -0.708940  0.138728\r\n    4 -0.302597  0.753350 -0.112674 -1.253316\r\n    5 -0.213237 -0.467802  0.037350  0.369167\r\n    6  0.754915 -0.569134 -0.297824 -0.600527\r\n    7  0.644742  0.038862  0.216869  0.294149\r\n    8  0.101684  0.784329  0.218221  0.965897\r\n    9 -1.482837 -1.325625  1.008795 -0.150439\r\n```'"
2599,9516209,sanand0,wesm,2012-12-26 04:47:38,2013-01-05 21:22:21,2013-01-05 21:22:21,closed,,0.10.1,13,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2599,b'read_csv treats zeroes as nan if column contains any nan',"b""If `data.csv` contains the following (column B has a zero in the first row, and is empty in the second)\r\n\r\n    A,B\r\n    0,0\r\n    0,\r\n\r\n... pandas 0.10.0 incorrectly reads it as:\r\n\r\n    In [7]: pd.read_csv('data.csv')\r\n    Out[9]:\r\n       A   B\r\n    0  0 NaN\r\n    1  0 NaN\r\n\r\n... whereas pandas 0.9.0 reads it right:\r\n\r\n    In [5]: pd.read_csv('data.csv')\r\n    Out[6]:\r\n       A   B\r\n    0  0   0\r\n    1  0 NaN\r\n"""
2596,9512906,wesm,wesm,2012-12-25 18:59:22,2013-01-20 02:40:45,2013-01-20 02:40:45,closed,changhiskhan,0.10.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2596,b'Hierarchical columns indexing issue',b'http://stackoverflow.com/questions/14025879/assertion-error-in-columns-in-dataframe-with-hierarchical-indexing'
2594,9503725,wesm,hayd,2012-12-24 17:30:22,2013-08-26 19:00:12,2013-08-26 19:00:12,closed,,0.13,3,Bug;Data IO;Enhancement;IO CSV,https://api.github.com/repos/pydata/pandas/issues/2594,b'Support parsing thousands separators in floating point data',"b'xref #584 \r\n\r\n```\r\nIt seems that the decimal format works ok for the decimal sign or for the thousands but not combined.\r\nReopen the issue?\r\n\r\nExample\r\n\r\nimport pandas as pd\r\nfrom StringIO import StringIO\r\ndata = """"""A;B;C\r\n0;0,11;0,11\r\n1.000;1000,11;1.000,11\r\n20.000;20000,22;20.000,22\r\n300.000;300000,33;300.000,33\r\n4.000.000;4000000,44;4.000.000,44\r\n5.000.000.000;5000000000,55;5.000.000.000,55""""""\r\n\r\ndf = pd.read_csv(StringIO(data), sep=\';\', thousands=\'.\', decimal =\',\')\r\nprint df.dtypes\r\nprint df\r\n\r\nResults in\r\n\r\nA int64\r\nB float64\r\nC object\r\nA B C\r\n0 0 1.100000e-01 0,11\r\n1 1000 1.000110e+03 1.000,11\r\n2 20000 2.000022e+04 20.000,22\r\n3 300000 3.000003e+05 300.000,33\r\n4 4000000 4.000000e+06 4.000.000,44\r\n5 5000000000 5.000000e+09 5.000.000.000,55\r\n```'"
2593,9503230,vishnu2kmohan,wesm,2012-12-24 16:40:28,2012-12-28 14:48:30,2012-12-28 14:47:55,closed,,0.10.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2593,"b'read_csv, compression, CParser'","b'The CParser doesn\'t appear to handle the compression flag in 0.10.0\r\n\r\n```\r\nvishnu@grsectoo ~/python/pandas $ cat compression.csv\r\n""Time"",""A"",""B""\r\n""12/24/2012 12:00:00"",""1.00"",""2.00""\r\nvishnu@grsectoo ~/python/pandas $ zcat compression.csv.gz \r\n""Time"",""A"",""B""\r\n""12/24/2012 12:00:00"",""1.00"",""2.00"",\r\nvishnu@grsectoo ~/python/pandas $ bzcat compression.csv.bz2 \r\n""Time"",""A"",""B""\r\n""12/24/2012 12:00:00"",""1.00"",""2.00""\r\n```\r\n\r\nI\'ve shown my results with Python 3.2.3 and IPython 0.13.1 but it also manifests in Python 2.7.3\r\n```\r\nvishnu@grsectoo ~/python/pandas $ ipython\r\nPython 3.2.3 (default, Dec 17 2012, 23:03:08) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\nIPython 0.13.1 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\nIn [1]: import pandas as pd\r\nIn [2]: import gzip\r\nIn [3]: import bz2\r\n```\r\n\r\nThe raw csv file reads just fine:\r\n```\r\nIn [4]: with open(\'compression.csv\') as fh:                                  \r\n    pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0])\r\n   ...:     \r\n```\r\n\r\nAttempting to read the compressed version, while passing compression=\'gzip\' fails:\r\n```\r\nIn [5]: with open(\'compression.csv.gz\') as fh:\r\n    pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0], compression=\'gzip\')\r\n   ...:     \r\n---------------------------------------------------------------------------\r\nCParserError                              Traceback (most recent call last)\r\n<ipython-input-5-3e61cc453d63> in <module>()\r\n      1 with open(\'compression.csv.gz\') as fh:\r\n----> 2     pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0], compression=\'gzip\')\r\n      3 \r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\r\n    390                     buffer_lines=buffer_lines)\r\n    391 \r\n--> 392         return _read(filepath_or_buffer, kwds)\r\n    393 \r\n    394     parser_f.__name__ = name\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)\r\n    199 \r\n    200     # Create the parser.\r\n--> 201     parser = TextFileReader(filepath_or_buffer, **kwds)\r\n    202 \r\n    203     if nrows is not None:\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)\r\n    499             self.options[\'has_index_names\'] = kwds[\'has_index_names\']\r\n    500 \r\n--> 501         self._make_engine(self.engine)\r\n    502 \r\n    503     def _get_options_with_defaults(self, engine):\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in _make_engine(self, engine)\r\n    601     def _make_engine(self, engine=\'c\'):\r\n    602         if engine == \'c\':\r\n--> 603             self._engine = CParserWrapper(self.f, **self.options)\r\n    604         else:\r\n    605             if engine == \'python\':\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds)\r\n    880         # #2442\r\n    881         kwds[\'allow_leading_cols\'] = self.index_col is not False\r\n--> 882         self._reader = _parser.TextReader(src, **kwds)\r\n    883 \r\n    884         # XXX\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3915)()\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4956)()\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:6531)()\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.raise_parser_error (pandas/src/parser.c:16903)()\r\nCParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine=\'python\'.\r\n```\r\n\r\nHowever if I pass a GzipFile handle instead, we are able to read the csv;\r\n```\r\nIn [6]: with gzip.GzipFile(\'compression.csv.gz\') as fh:                                  \r\n    pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0], compression=\'gzip\')\r\n   ...:     \r\n```\r\n\r\nAlso, opening a bzip2 version while passing in compression=\'bz2\' fails:\r\n```\r\nIn [7]: with open(\'compression.csv.bz2\') as fh:\r\n    pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0], compression=\'bz2\')\r\n   ...:     \r\n---------------------------------------------------------------------------\r\nCParserError                              Traceback (most recent call last)\r\n<ipython-input-7-2736c68aa21d> in <module>()\r\n      1 with open(\'compression.csv.bz2\') as fh:\r\n----> 2     pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0], compression=\'bz2\')\r\n      3 \r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\r\n    390                     buffer_lines=buffer_lines)\r\n    391 \r\n--> 392         return _read(filepath_or_buffer, kwds)\r\n    393 \r\n    394     parser_f.__name__ = name\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)\r\n    199 \r\n    200     # Create the parser.\r\n--> 201     parser = TextFileReader(filepath_or_buffer, **kwds)\r\n    202 \r\n    203     if nrows is not None:\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)\r\n    499             self.options[\'has_index_names\'] = kwds[\'has_index_names\']\r\n    500 \r\n--> 501         self._make_engine(self.engine)\r\n    502 \r\n    503     def _get_options_with_defaults(self, engine):\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in _make_engine(self, engine)\r\n    601     def _make_engine(self, engine=\'c\'):\r\n    602         if engine == \'c\':\r\n--> 603             self._engine = CParserWrapper(self.f, **self.options)\r\n    604         else:\r\n    605             if engine == \'python\':\r\n/usr/lib64/python3.2/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds)\r\n    880         # #2442\r\n    881         kwds[\'allow_leading_cols\'] = self.index_col is not False\r\n--> 882         self._reader = _parser.TextReader(src, **kwds)\r\n    883 \r\n    884         # XXX\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3915)()\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4956)()\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:6531)()\r\n/usr/lib64/python3.2/site-packages/pandas/_parser.cpython-32.so in pandas._parser.raise_parser_error (pandas/src/parser.c:16903)()\r\nCParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine=\'python\'.\r\n```\r\n\r\nPassing a BZ2File handle allows the parser to read the csv:\r\n```\r\nIn [9]: with bz2.BZ2File(\'compression.csv.bz2\') as fh:   \r\n    pd.read_csv(fh, sep=\',\', parse_dates=[0], index_col=[0], compression=\'bz2\')\r\n   ...:   \r\n```\r\nVersion:\r\n```\r\nIn [10]: pd.version.version\r\nOut[10]: \'0.10.0\'\r\n```\r\n\r\nThis issue also manifests in the current git master.\r\n\r\nThanks,\r\nVishnu'"
2588,9488241,brendam,wesm,2012-12-23 06:38:47,2013-01-19 22:14:52,2013-01-19 22:14:50,closed,,0.10.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2588,b'pandas plot giving value error in IPython/zmq/pylab/backend_inline.pyc',"b'Using ipython 0.13.1 and pandas 0.10.0, having both the \'color\' and \'style\' parameter to pandas.Series.plot() gives a value error in pylab. Not sure if I should report this here or in ipython?\r\n\r\n```python\r\nfrom pandas import *\r\nts = Series(randn(1000), index=date_range(\'1/1/2000\', periods=1000))\r\nts = ts.cumsum()\r\n# having color and style causes Value error crash\r\nts.plot(color=\'#000099\', style=\'b-\')\r\n```\r\n\r\n```python\r\nValueError: to_rgba: Invalid rgba arg ""[\'#000099\']""\r\nneed more than 1 value to unpack\r\n```\r\n'"
2587,9488101,zhangruoyu,wesm,2012-12-23 05:58:27,2012-12-28 15:42:29,2012-12-28 15:42:29,closed,,0.10.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2587,"b""Series.add can't do with scalar value""","b's = pd.Series([1,2,3])\r\ns.add(3)\r\n\r\nraise  AssertionError: Other operand must be Series.'"
2585,9486416,wesm,wesm,2012-12-22 20:39:56,2012-12-29 04:30:44,2012-12-28 18:44:30,closed,,0.10.1,8,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2585,b'Wide repr formatting problem',"b'Bummer. This happened in IPython on git master, `display.line_width` set to 80\r\n\r\n```\r\nIn [18]: df.columns\r\nOut[18]: Index([Symbol, Description, MTM MTD, MTM YTD, Realized ST MTD, Realized ST YTD, Realized LT MTD, Realized LT YTD, Underlying, Kind], dtype=object)\r\n\r\nIn [19]: df\r\nOut[19]: \r\n  Symbol Description  MTM MTD  MTM YTD  Realized ST MTD  Realized ST YTD  Realized LT MTD  Realized LT YTD Underlying Kind\r\n0    NaN         NaN      NaN      NaN              NaN              NaN              NaN              NaN        NaN  NaN\r\n1    NaN         NaN      NaN      NaN              NaN              NaN              NaN              NaN        NaN  NaN\r\n2    NaN         NaN      NaN      NaN              NaN              NaN              NaN              NaN        NaN  NaN\r\n3    NaN         NaN      NaN      NaN              NaN              NaN              NaN              NaN        NaN  NaN\r\n4    NaN         NaN      NaN      NaN              NaN              NaN              NaN              NaN        NaN  NaN\r\n```'"
2578,9471792,jreback,jreback,2012-12-21 18:11:09,2014-09-04 18:11:29,2013-09-05 17:23:51,closed,,0.13,0,Bug;Enhancement;Indexing,https://api.github.com/repos/pydata/pandas/issues/2578,b'BUG/ENH: should .ix allow partial setting?',"b'related to GH #2033\r\n\r\n```\r\nIn [27]: df = pd.DataFrame(np.random.rand(4,2),index=pd.date_range(\'2001/1/12\',periods=4),columns=[\'A\',\'B\'])\r\n\r\nIn [28]: df\r\nOut[28]: \r\n                   A         B\r\n2001-01-12  0.861201  0.530351\r\n2001-01-13  0.434518  0.692022\r\n2001-01-14  0.794237  0.912073\r\n2001-01-15  0.421282  0.930005\r\n\r\nIn [29]: df.ix[:,\'B\'] = df.ix[:,\'A\'].copy()\r\n\r\nIn [30]: df.ix[:,\'C\'] = df.ix[:,\'A\'].copy()\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-30-2107107d0bd8> in <module>()\r\n----> 1 df.ix[:,\'C\'] = df.ix[:,\'A\'].copy()\r\n\r\n\r\n/usr/local/lib/python2.7/site-packages/pandas-0.10.0-py2.7-linux-x86_64.egg/pandas/core/index.pyc in get_loc(self, key)\r\n    714         loc : int if unique index, possibly slice or mask if not\r\n    715         """"""\r\n--> 716         return self._engine.get_loc(key)\r\n    717 \r\n    718     def get_value(self, series, key):\r\n\r\nKeyError: \'C\'\r\n```\r\nof course in this case ```df[\'C\'] = df.ix[:,\'A\'].copy()``` accomplishes this\r\n\r\nPanel example: have to transpose, setitem, then transpose back to accomplish this\r\n\r\n```\r\nIn [46]: p = pd.Panel(np.random.rand(2,4,2),items=[\'Item1\',\'Item2\'],major_axis=pd.date_range(\'2001/1/12\',periods=4),minor_axis=[\'A\',\'B\'])\r\n\r\nIn [47]: p\r\nOut[47]: \r\n<class \'pandas.core.panel.Panel\'>\r\nDimensions: 2 (items) x 4 (major_axis) x 2 (minor_axis)\r\nItems axis: Item1 to Item2\r\nMajor_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00\r\nMinor_axis axis: A to B\r\n\r\nIn [48]: pt = p.transpose(2,1,0)\r\n\r\nIn [49]: pt\r\nOut[49]: \r\n<class \'pandas.core.panel.Panel\'>\r\nDimensions: 2 (items) x 4 (major_axis) x 2 (minor_axis)\r\nItems axis: A to B\r\nMajor_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00\r\nMinor_axis axis: Item1 to Item2\r\n\r\nIn [50]: pt[\'C\'] = p.ix[:,:,\'B\'].copy()\r\n\r\nIn [51]: pt.transpose(2,1,0)\r\nOut[51]: \r\n<class \'pandas.core.panel.Panel\'>\r\nDimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)\r\nItems axis: Item1 to Item2\r\nMajor_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00\r\nMinor_axis axis: A to C\r\n\r\n```\r\n\r\n'"
2576,9468832,invisibleroads,wesm,2012-12-21 16:09:50,2013-12-04 00:40:22,2012-12-28 15:49:38,closed,,0.10.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2576,b'UnicodeDecodeError in DataFrame.__repr__() for non-ascii column names',"b'Here is the traceback using Python 2.7.3 and Pandas trunk.\r\n\r\n    /pandas/core/frame.pyc in __repr__(self)\r\n        678         Yields Bytestring in Py2, Unicode String in py3.\r\n        679         """"""\r\n    --> 680         return str(self)\r\n        681 \r\n        682     def _repr_html_(self):\r\n\r\n    /pandas/core/frame.pyc in __str__(self)\r\n        634         if py3compat.PY3:\r\n        635             return self.__unicode__()\r\n    --> 636         return self.__bytes__()\r\n        637 \r\n        638     def __bytes__(self):\r\n\r\n    /pandas/core/frame.pyc in __bytes__(self)\r\n        644         """"""\r\n        645         encoding = com.get_option(""display.encoding"")\r\n    --> 646         return self.__unicode__().encode(encoding , \'replace\')\r\n        647 \r\n        648     def __unicode__(self):\r\n\r\n    /pandas/core/frame.pyc in __unicode__(self)\r\n        655         buf = StringIO(u"""")\r\n        656         if self._need_info_repr_():\r\n    --> 657             self.info(buf=buf, verbose=self._verbose_info)\r\n        658         else:\r\n        659             is_wide = self._need_wide_repr()\r\n\r\n    /pandas/core/frame.pyc in info(self, verbose, buf, max_cols)\r\n    1624         dtypes = [\'%s(%d)\' % k for k in sorted(counts.iteritems())]\r\n    1625         lines.append(\'dtypes: %s\' % \', \'.join(dtypes))\r\n    -> 1626         _put_lines(buf, lines)\r\n    1627 \r\n    1628     @property\r\n\r\n    /pandas/core/format.pyc in _put_lines(buf, lines)\r\n    1443 def _put_lines(buf, lines):\r\n    1444     if any(isinstance(x, unicode) for x in lines):\r\n    -> 1445         lines = [unicode(x) for x in lines]\r\n    1446     buf.write(\'\\n\'.join(lines))\r\n    1447 \r\n\r\n    UnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xd5 in position 8: ordinal not in range(128)\r\n\r\n'"
2575,9464914,timtroendle,wesm,2012-12-21 13:41:03,2013-01-20 02:42:11,2013-01-20 02:42:08,closed,changhiskhan,0.10.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2575,b'Indexing/Slicing tz-aware TimeSeries fails when using datetime',"b""The following code should clarify the problem:\r\n```python\r\nimport pandas\r\nimport pytz\r\nindex = pandas.date_range(start='2012-12-24 16:00', end='2012-12-24 18:00', freq='H', tz='Europe/Berlin')\r\nts = pandas.TimeSeries(index=index, data=index.hour)\r\ntime_pandas = pandas.Timestamp('2012-12-24 17:00', tz='Europe/Berlin')\r\ntime_datetime = datetime.datetime(2012,12,24,17,00, tzinfo=pytz.timezone('Europe/Berlin'))\r\ntime_pandas == time_datetime\r\nts[time_pandas] == ts[time_datetime]\r\n```\r\nWhile the two time stamps are equal, using them as an index on the TimeSeries results in different values. The same is true for slicing.\r\n\r\nWhen using a tz-aware datetime for indexing/slicing, pandas seems to remove the timezone and interprete the time stamp as UTC."""
2574,9462372,dalejung,wesm,2012-12-21 11:27:51,2012-12-28 15:42:03,2012-12-28 15:42:03,closed,,0.10.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2574,b'sub/mul/div for Series does not accept scalar',"b""```python\r\nimport panda as pd\r\n\r\ns = pd.Series(range(10))\r\ns.sub(1)\r\n\r\n# AssertionError: Other operand must be Series\r\n\r\ndf = pd.DataFrame({'test':s})\r\ndf.sub(1) # works\r\n```\r\n\r\nAm I misunderstanding or should scalars work with sub/mul/div/etc?"""
2571,9430404,bmu,changhiskhan,2012-12-20 10:46:53,2013-01-21 17:32:00,2013-01-21 17:32:00,closed,changhiskhan,0.10.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2571,b'Timestamp when plotting hourly time series with base != 0',"b""If I plot an hourly time series with a different base (don't know if this is correct englisch I mean if it is not 12:00, 13:00, ... but something like 12:30, 13:30, ...) the values are plotted as full hours at the xaxis. If I convert to a resolution of '60min' it works.  \r\n\r\n```python\r\nIn [250]: idx = pd.date_range('2012-12-20', periods=24, freq='H') + datetime.timedelta(minutes=30)\r\n\r\nIn [251]: idx\r\nOut[251]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2012-12-20 00:30:00, ..., 2012-12-20 23:30:00]\r\nLength: 24, Freq: H, Timezone: None\r\n\r\nIn [252]: df = pd.DataFrame(np.arange(24), index=idx)\r\n\r\nIn [253]: df.plot(style='r.')\r\nOut[253]: <matplotlib.axes.AxesSubplot at 0xd34920d0>\r\n\r\nIn [254]: df2 = df.resample('60min', base=30, closed='right')\r\n\r\nIn [255]: df2.index\r\nOut[255]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2012-12-20 00:30:00, ..., 2012-12-20 23:30:00]\r\nLength: 24, Freq: 60T, Timezone: None\r\n\r\nIn [256]: df2.plot(style='r.')\r\nOut[256]: <matplotlib.axes.AxesSubplot at 0xd43fde90>\r\n```"""
2569,9415249,leonbaum,wesm,2012-12-19 21:20:55,2012-12-28 16:41:12,2012-12-28 16:41:12,closed,,0.10.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2569,b'Grouping over PeriodIndex breaks its format',"b""With the master branch (f014b01af7bd2d03266697e672c2d41daded3fca), grouping over a PeriodIndex is breaking the format of the index:\r\n\r\n```\r\nIn [100]: df = pandas.DataFrame({'x': [1, 2, 3], 'y': ['2012-12-18', '2012-12-19', '2013-01-04']})\r\n\r\nIn [101]: df['y'] = df.y.astype('datetime64[D]')\r\n\r\nIn [102]: df = df.set_index('y').to_period('M')\r\n\r\nIn [103]: df\r\nOut[103]: \r\n         x\r\n2012-12  1\r\n2012-12  2\r\n2013-01  3\r\n\r\nIn [104]: df.groupby(level=0).sum()\r\nOut[104]: \r\n     x\r\n515  3\r\n516  3\r\n```\r\n\r\nI would expect to still see the dates in the index."""
2553,9331715,hauptmech,changhiskhan,2012-12-17 12:41:19,2012-12-17 15:19:33,2012-12-17 15:19:33,closed,,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2553,b'Slicing a DataFrame where index has tzinfo produces a DataFrame with index having no tzinfo',"b""When I slice a DataFrame with an index of datetimes that contains time zone info, the resulting DataFrame index is missing the timezone info.\r\n\r\n>>> numpy.__version__\r\n'1.7.0b2'\r\n>>> pandas.__version__\r\n'0.9.0'"""
2549,9318048,jreback,wesm,2012-12-16 18:17:49,2012-12-17 15:15:50,2012-12-17 15:15:50,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2549,b'BUG: string output on PeriodIndex fails',"b""used to work in 0.9.1.\r\n\r\nthis is on current master\r\n\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: index = pd.PeriodIndex(['2011-1', '2011-2', '2011-3'], freq='M')\r\n\r\nIn [4]: frame = pd.DataFrame(np.random.randn(3,4),index=index)\r\n\r\nIn [5]: frame.to_string()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-b6219037419a> in <module>()\r\n----> 1 frame.to_string()\r\n\r\n/mnt/home/jreback/pandas/pandas/core/frame.pyc in to_string(self, buf, columns, col_space, colSpace, header, index, na_rep, formatters, float_format, sparsify, nanRep, index_names, justify, force_unicode, line_width)\r\n   1501                                            header=header, index=index,\r\n   1502                                            line_width=line_width)\r\n-> 1503         formatter.to_string()\r\n   1504 \r\n   1505         if buf is None:\r\n\r\n/mnt/home/jreback/pandas/pandas/core/format.pyc in to_string(self, force_unicode)\r\n    295             text = info_line\r\n    296         else:\r\n--> 297             strcols = self._to_str_columns()\r\n    298             if self.line_width is None:\r\n    299                 text = adjoin(1, *strcols)\r\n\r\n/mnt/home/jreback/pandas/pandas/core/format.pyc in _to_str_columns(self)\r\n    240 \r\n    241         # may include levels names also\r\n--> 242         str_index = self._get_formatted_index()\r\n    243         str_columns = self._get_formatted_column_labels()\r\n    244 \r\n\r\n/mnt/home/jreback/pandas/pandas/core/format.pyc in _get_formatted_index(self)\r\n    444                                      formatter=fmt)\r\n    445         else:\r\n--> 446             fmt_index = [index.format(name=show_index_names, formatter=fmt)]\r\n    447 \r\n    448         adjoined = adjoin(1, *fmt_index).split('\\n')\r\n\r\nTypeError: format() got an unexpected keyword argument 'formatter'\r\n\r\n```"""
2538,9295011,hugadams,wesm,2012-12-14 19:31:37,2012-12-14 23:21:37,2012-12-14 23:21:36,closed,wesm,0.10,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2538,b'date_range() alert user when using periods and end args?',"b""Hi all,\r\n\r\nI am using date_range() and noticed that  if the arguments start, end and period are all passed, the *period* argument essentially overrides *end*. \r\n\r\nFor example, consider the following index created using start and end:\r\n\r\n    start = datetime(2011, 1, 1, 5, 3, 40)\r\n    end = datetime(2011, 1, 1, 8, 9, 40)\r\n\r\n    tmanual=date_range(start, end, freq='s')\r\n\r\n    >>>[2011-01-01 05:03:40, ..., 2011-01-01 08:09:40]\r\n\r\nAn alternative is to provide a start time,  and then a number of periods.\r\n\r\n    tperiods=date_range(start, freq='s', periods=10)\r\n\r\n    >>> [2011-01-01 05:03:40, ..., 2011-01-01 05:03:49]\r\n\r\nThe weird part is when I pass all three arguments (start, end and periods).  It actually defers to the output of tperiods.\r\n\r\n    tweird=date_range(start, end, freq='s', period=10)\r\n\r\n    >>>[2011-01-01 05:03:40, ..., 2011-01-01 05:03:49]\r\n\r\nThus, there is no difference between the following calls:\r\n\r\n    date_range(start, freq='s', periods=10)\r\n    date_range(start, end, freq='s', period=10)\r\n\r\nIMO, I think this should at least yield an error or a flag.  My preferred behavior would actually yield a DatetimeIndex that runs from start to end, sampling 10 times over the entire array of second-spaced data.   One would be effectively defining a step size for sampling of length (start-end)/period, where start-end is an array spaced at second intervals.\r\n\r\nWhat do you guys think?  Here's an example of how the function could work:\r\n\r\n    def date_range(...):\r\n    ........\r\n    if start and end and period:\r\n        tindex=date_range(start, end, freq=freq)\r\n   \r\n    return tindex[0 :: period]\r\n   \r\n\r\n\r\n\r\n\r\n\r\n"""
2537,9292632,dalejung,wesm,2012-12-14 18:01:02,2012-12-16 04:43:19,2012-12-16 04:43:19,closed,,0.10,1,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/2537,b'TimeGrouper error on PanelGroupBy.agg with custom functions.',"b'http://nbviewer.ipython.org/4287202/\r\n\r\nWhen using a regular groupby, custom agg functions will receive a Panel as expected. If you groupby with TimeGrouper, the agg function will receive a Series. The regular cython aggregates likes mean, sum work fine. '"
2535,9287458,TomAugspurger,wesm,2012-12-14 15:15:16,2012-12-23 21:44:50,2012-12-14 16:41:14,closed,,0.10,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2535,"b'read_csv, compression, and bad column names.'","b""Hi.  I've got a data set that I'm working with from [here](http://epp.eurostat.ec.europa.eu/NavTree_prod/everybody/BulkDownloadListing?dir=data&sort=1&sort=2&start=b), the ``bop_its_det.tsv.gz`` file.  [Direct Link](http://epp.eurostat.ec.europa.eu/NavTree_prod/everybody/BulkDownloadListing?sort=1&file=data%2Fbop_its_det.tsv.gz). It's about 80 MB unzipped.  I tried to create a csv of just the first 10 rows [here](https://www.dropbox.com/s/u1pz5gvbwwaupjn/test.csv) but I think I messed up some of the newlines.  [Here's](https://gist.github.com/4286107) a gist of the plain text.\r\n\r\nI want to say up front that I'm using pandas version `0.10.0.dev-16774b3`, which I think is from a couple days before the official 10.0 beta was released, so maybe this isn't an issue anymore.\r\n\r\nSomeone in the EU's stats department decided it would be a good idea to separate some column names by columns and others by tabs so the first line looks like\r\n\r\n    In [332]: f = open('BulkDownloadListing.tsv', 'r')\r\n    \r\n    In [333]: f.readline()\r\n    Out[333]: 'currency,post,flow,partner,geo\\\\time\\t2011 \\t2010 \\t2009 \\t2008 \\t2007 \\t2006 \\t2005 \\t2004 \\n'\r\n\r\nread_csv can handle this fine with:\r\n\r\n    df = pd.read_csv('BulkDownloadListing.tsv', sep=',|s*\\t', na_values=[':', ' :', ': '], nrows=10)\r\n\r\nwhen I try with the compressed file ``BulkDownloadListing.gz``:\r\n\r\n    df = pd.read_csv('BulkDownloadListing.gz', compression='gzip', sep=',|s*\\t', na_values=[':', ' :', ': '], nrows=10)\r\n\r\nI get \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-337-07c5e7dff532> in <module>()\r\n----> 1 df = pd.read_csv('BulkDownloadListing.gz', compression='gzip', sep=',|s*\\t', na_values=[':', ' :', ': '], nrows=10)\r\n/usr/local/Cellar/python/2.7.3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.10.0.dev_16774b3-py2.7-macosx-10.8-x86_64.egg/pandas/io/parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\r\n    392                     buffer_lines=buffer_lines)\r\n    393 \r\n--> 394         return _read(filepath_or_buffer, kwds)\r\n    395 \r\n    396     parser_f.__name__ = name\r\n/usr/local/Cellar/python/2.7.3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.10.0.dev_16774b3-py2.7-macosx-10.8-x86_64.egg/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)\r\n    204 \r\n    205     if nrows is not None:\r\n--> 206         return parser.read(nrows)\r\n    207     elif chunksize or iterator:\r\n    208         return parser\r\n/usr/local/Cellar/python/2.7.3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.10.0.dev_16774b3-py2.7-macosx-10.8-x86_64.egg/pandas/io/parsers.pyc in read(self, nrows)\r\n    622             #     self._engine.set_error_bad_lines(False)\r\n    623 \r\n--> 624         ret = self._engine.read(nrows)\r\n    625 \r\n    626         if self.options.get('as_recarray'):\r\n/usr/local/Cellar/python/2.7.3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.10.0.dev_16774b3-py2.7-macosx-10.8-x86_64.egg/pandas/io/parsers.pyc in read(self, rows)\r\n   1238             content = content[1:]\r\n   1239 \r\n-> 1240         alldata = self._rows_to_cols(content)\r\n   1241         data = self._exclude_implicit_index(alldata)\r\n   1242 \r\n/usr/local/Cellar/python/2.7.3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.10.0.dev_16774b3-py2.7-macosx-10.8-x86_64.egg/pandas/io/parsers.pyc in _rows_to_cols(self, content)\r\n   1454             msg = ('Expected %d fields in line %d, saw %d' %\r\n   1455                    (col_len, row_num + 1, zip_len))\r\n-> 1456             raise ValueError(msg)\r\n   1457 \r\n   1458         return zipped_content\r\nValueError: Expected 5 fields in line 3, saw 11\r\n```"""
2534,9276553,changhiskhan,changhiskhan,2012-12-14 05:33:19,2012-12-14 05:38:50,2012-12-14 05:38:50,closed,,0.10,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2534,b'Parser should strip whitespace around tokens',"b'spaces between column names for example:\r\n\r\na,b,c\r\n\r\nvs\r\n\r\na, b,c'"
2527,9267832,craustin,wesm,2012-12-13 21:46:27,2012-12-13 22:21:57,2012-12-13 22:21:16,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2527,b'rolling_mean returns slightly-negative value on Series with only non-negative values',"b""Seeing this in a few cases.  This is the simplest repro I can find:\r\n```python\r\nfrom pandas import date_range, rolling_mean, Series\r\ns = Series(index=date_range('1999-02-03','1999-04-05'))\r\ns['1999-02-03'] = 0.00012456\r\ns['1999-02-04'] = 0.0003\r\ns['1999-04-05'] = 0\r\nrolling_mean(s, 1)['1999-04-05']\r\n```\r\nReturns 0 in 0.9.0.\r\nReturns -5E-20 in 0.10.0.\r\n\r\nWe found this issue because we do a sqrt on the result and expected 0s when the input was a 0."""
2526,9264938,louist87,louist87,2012-12-13 20:19:43,2012-12-14 01:09:17,2012-12-14 01:09:17,closed,,0.10,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2526,b'Segfault in read_table',"b""Hello,\r\n\r\nI get a segfault in pandas version `0.10.0.dev` when attempting to import a tab-delimited file with `pandas.read_table`.\r\n\r\nHere is the code that produces the bug:\r\n\r\n```python\r\n# note:  tf is a string containing the path to the file\r\npd.read_table(tf, names=range(1, 129) + ['ref'], header=None)\r\n```\r\n\r\nI tried loading the data into a list and then passing it to the `DataFrame` constructor and was successful.  Here's what I did:\r\n\r\n```python\r\nwith open(filename, 'rt') as f:\r\n    # there is only one line because this file was encoded using '\\r' instead of '\\n'\r\n    data = f.readlines()[0] \r\n\r\n# remove the empty strings that occasionally pop up\r\ndata = [[s for s in line.split('\\t') if s] for line in data.split('\\r') if line] \r\n\r\npd.DataFrame(data, columns=range(1, 129) + ['ref'])  # works\r\n```\r\n\r\n\r\n\r\n[Here](https://dl.dropbox.com/u/6160029/data.txt) is some example data that you may use."""
2524,9261322,wesm,changhiskhan,2012-12-13 18:29:10,2012-12-14 03:55:56,2012-12-14 03:55:56,closed,changhiskhan,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2524,"b""Don't make it impossible to get full DataFrame.info summary when lots of columns""",b'Max is hard-coded to 100'
2523,9261005,craustin,changhiskhan,2012-12-13 18:20:20,2012-12-14 03:19:46,2012-12-14 03:19:46,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2523,"b""Exception using float as 'diff' periods""","b'from pandas import DataFrame\r\ndf = DataFrame({\'US\': [1, 2]}, index=[datetime(2012,1,1), datetime(2012,1,2)])\r\ndf.diff(1.)\r\n\r\nThis succeeded in 0.9.0. Now it raises:\r\n\r\npandas\\core\\frame.pyc in diff(self, periods)\r\n   3937         diffed : DataFrame\r\n   3938         """"""\r\n-> 3939         new_blocks = [b.diff(periods) for b in self._data.blocks]\r\n   3940         new_data = BlockManager(new_blocks, [self.columns, self.index])\r\n   3941         return self._constructor(new_data)\r\n\r\npandas\\core\\internals.pyc in diff(self, n)\r\n    292\r\n    293     def diff(self, n):\r\n--> 294         new_values = com.diff(self.values, n, axis=1)\r\n    295         return make_block(new_values, self.items, self.ref_items)\r\n    296\r\n\r\npandas\\core\\common.pyc in diff(arr, n, axis)\r\n    470     if arr.ndim == 2 and arr.dtype.name in _diff_special:\r\n    471         f = _diff_special[arr.dtype.name]\r\n--> 472         f(arr, out_arr, n, axis)\r\n    473     else:\r\n    474         res_indexer = [slice(None)] * arr.ndim\r\n\r\npandas\\algos.pyd in pandas.algos.diff_2d_int64 (pandas\\algos.c:14030)()\r\n\r\nTypeError: \'float\' object cannot be interpreted as an index\r\n\r\nThis might very well be a desired change - but it\'s something we\'ll have to work around. I\'m just reporting it for completeness :-)'"
2517,9247131,akloster,wesm,2012-12-13 10:01:57,2012-12-14 02:29:47,2012-12-14 02:29:40,closed,,0.10,6,Bug,https://api.github.com/repos/pydata/pandas/issues/2517,b'DataFrame.from_dict should respect OrderedDict',"b'Hi, I\'m repeatedly running into an issue where I want a specific ordering of columns and rows. Often that won\'t work. I usually have to get around the restriction by converting this to lists etc., which makes everything a little messier than using dictionaries for indexing.\r\n\r\nI more or less figured out why. In DataFrame.from_dict my OrderedDict is used to populate a OrderedDict(dict) class.\r\n\r\nDo you think it would be a good idea to change this to ""OrderedDict""?'"
2513,9241781,gdraps,changhiskhan,2012-12-13 03:58:53,2012-12-13 16:47:43,2012-12-13 16:47:43,closed,changhiskhan,0.10,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2513,"b'Series.str.split() behavior on multi-character patterns (pandas 0.9.1, py2.7)'","b'Not sure if this is an issue in my setup or the intended behavior, but str.split behavior with multi-character patterns changed between 0.9.0 and 0.9.1, most likely due to #2119.  Here is the original behavior going back to 0.8.1 as best I can tell.\r\n\r\n    In [1]: pd.__version__\r\n    Out[1]: \'0.9.1.dev-c252129\'\r\n\r\n    In [2]: s = pd.Series([""D0->D2""])\r\n\r\n    In [3]: s.str.split(""->"")\r\n    Out[3]: 0    [D0, D2]\r\n\r\nIn 0.9.1:\r\n\r\n    In [1]: pd.__version__\r\n    Out[1]: \'0.9.1\'\r\n\r\n    In [2]: s = pd.Series([""D0->D2""])\r\n\r\n    In [3]: s.str.split(""->"")\r\n    Out[3]: 0    [D0->D2]\r\n\r\nSetting n=0 restores the behavior, on my Python install at least.\r\n\r\n    In [5]: s.str.split(""->"", n=0)\r\n    Out[5]: 0    [D0, D2]\r\n\r\nReproducible in 0.10.0b1.\r\n\r\n    In [1]: pd.__version__\r\n    Out[1]: \'0.10.0b1\'\r\n\r\n    In [2]: s = pd.Series([""D0->D2""])\r\n\r\n    In [3]: s.str.split(""->"")\r\n    Out[3]: 0    [D0->D2]\r\n\r\nPython 2.7.2+ (default, Jul 20 2012, 22:12:53) \r\n[GCC 4.6.1] on linux2\r\n\r\nAny thoughts on changing the default n for str.split back to 0?\r\n\r\nMany thanks!'"
2503,9212401,hayd,cpcloud,2012-12-12 10:59:05,2014-06-09 02:24:52,2014-06-09 02:24:52,closed,,0.16.0,7,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2503,b'Ginput giving wrong date for datetimeindex plot',"b""I rewrote this [StackOverflow question](http://stackoverflow.com/questions/13832938/how-use-ginput-method-get-the-correct-points) to include an example. I think this looks like a bug (possibly in `num2date` or `date2num`)?\r\n\r\n-------\r\n\r\nI read-in a file and plot it with pandas `DataFrame`. The index is DatetimeIndex, and then I use `ginput(1)` method to get one point, however the coordinate which I get is wrong.\r\n\r\nThe code is as follows:\r\n\r\n    import pandas as pd\r\n    from matplotlib.dates import num2date, date2num\r\n    ts = pd.date_range('2012-04-12,16:13:09', '2012-04-14,00:13:09', freq='H')\r\n    df = pd.DataFrame(index=ts)\r\n    df[0] = 20.6\r\n\r\nI then plot and click on the graph using [ginput](http://glowingpython.blogspot.co.uk/2011/08/how-to-use-ginput.html):\r\n\r\n    df.plot()\r\n    t = pylab.ginput(n=1) #click somewhere near 13-APR-2012\r\n\r\nHowever, the first item appears to be a float\r\n\r\n    In [8]: x = t[0][0] # ~ 370631.67741935479\r\n\r\n    In [9]: num2date(x)\r\n    Out[9]: datetime.datetime(1015, 10, 3, 16, 15, 29, 32253, tzinfo=<matplotlib.dates._UTC object at 0x104196550>)\r\n    # this is way out!\r\n\r\nThe docs suggest that it should be using these floats (from [`datetonum`](http://matplotlib.org/api/dates_api.html#matplotlib.dates.date2num)):\r\n\r\n    In [10]: dt = pd.to_datetime('13-4-2012', dayfirst=True)\r\n\r\n    In [11]: date2num(dt)\r\n    Out[11]: 734606.0\r\n\r\nWhat is this float, and how can I convert it to a datetime?\r\n\r\n*Note: If I remove one of the rows from the dataframe this works correctly:*\r\n\r\n    df1 = df.drop(ts[1], axis=0)\r\n    ..."""
2496,9203192,hugadams,changhiskhan,2012-12-12 01:34:45,2012-12-14 03:42:57,2012-12-14 03:42:57,closed,changhiskhan,0.10,5,Bug,https://api.github.com/repos/pydata/pandas/issues/2496,b'DataFrame.from_dict (orient broken?)',"b'I tried creating using the from_dict() method to create a dataframe recently and noticed that orient=\'index\' failed, even with trivial data.\r\n\r\n>>> a={\'hi\': [32, 3, 3], \'there\': [3, 5, 3]}\r\n\r\n>>> df=DataFrame.from_dict(a)\r\n\r\n>>> df2=DataFrame.from_dict(a, orient=\'index\')\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/EPD7.3/lib/python2.7/site-packages/pandas/core/frame.py"", line 756, in from_dict\r\n    for col, v in s.iteritems():\r\nAttributeError: \'list\' object has no attribute \'iteritems\'\r\n\r\nIt also failed with tuple values in the dictionary.\r\n\r\nMy current workaround is just to take the dataframe and perform a transpose, no big deal really.\r\n'"
2489,9175211,gbakalian,wesm,2012-12-11 10:35:11,2012-12-14 04:27:26,2012-12-14 04:27:26,closed,wesm,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2489,"b'Loosing series names, when concatenating into DataFrame'","b""\r\nIn the version 9.1, when we are concatenating pandas.Series, we would expect the columns name of the created DataFrame to be the ones of the Series which were concatenated together. Nevertheless the series' names are lost, and the columns are simply 0, 1, 2 ...\r\n\r\n\r\nError shown below:\r\n\r\n```python\r\npd.__version__\r\nOut[14]: '0.9.1rc1'\r\n\r\ns1 = pd.TimeSeries(randn(5), index=pd.date_range('01-Jan-2013', periods=5, freq='D'), name='A')\r\n\r\ns2 = pd.TimeSeries(randn(5), index=pd.date_range('01-Jan-2013', periods=5, freq='D'), name='B')\r\n\r\nseries = [s1, s2]\r\n\r\npd.concat(series, axis=1)\r\n\r\n \r\n\r\n_32.columns = map(lambda s: s.name, series)\r\n\r\n_32\r\n\r\n \r\n\r\n\r\n```"""
2488,9174606,gerigk,wesm,2012-12-11 10:11:16,2012-12-11 21:00:41,2012-12-11 21:00:41,closed,,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2488,b'df.sort(col) fails if col is not unique.',"b""one more trouble case with duplicate column names\r\nthe exception could be nicer (sort ambiguous because of duplicate column or similar) or it could sort by the first? column with this name (although this would not be nice in case the column is not duplicate but has in fact different content with the same name and the user not being aware of this).\r\n\r\n```\r\nimport pandas as pd\r\ndf = pd.DataFrame([(1,2), (3,4)], columns = ['a', 'a'])\r\ndf.sort('a')\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-251abbb5c77e> in <module>()\r\n      1 import pandas as pd\r\n      2 df = pd.DataFrame([(1,2), (3,4)], columns = ['a', 'a'])\r\n----> 3 df.sort('a')\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.0.dev_74ab638-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in sort(self, columns, column, axis, ascending, inplace)\r\n   3062             columns = column\r\n   3063         return self.sort_index(by=columns, axis=axis, ascending=ascending,\r\n-> 3064                                inplace=inplace)\r\n   3065 \r\n   3066     def sort_index(self, axis=0, by=None, ascending=True, inplace=False):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.0.dev_74ab638-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in sort_index(self, axis, by, ascending, inplace)\r\n   3125             self._clear_item_cache()\r\n   3126         else:\r\n-> 3127             return self.take(indexer, axis=axis)\r\n   3128 \r\n   3129     def sortlevel(self, level=0, axis=0, ascending=True, inplace=False):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.0.dev_74ab638-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in take(self, indices, axis)\r\n   2853             new_values = com.take_2d(self.values,\r\n   2854                                      com._ensure_int64(indices),\r\n-> 2855                                      axis=axis)\r\n   2856             if axis == 0:\r\n   2857                 new_columns = self.columns\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.0.dev_74ab638-py2.7-linux-x86_64.egg/pandas/core/common.pyc in take_2d(arr, indexer, out, mask, needs_masking, axis, fill_value)\r\n    414                 out = np.empty(out_shape, dtype=arr.dtype)\r\n    415             take_f = _get_take2d_function(dtype_str, axis=axis)\r\n--> 416             take_f(arr, _ensure_int64(indexer), out=out, fill_value=fill_value)\r\n    417             return out\r\n    418     elif dtype_str in ('float64', 'object', 'datetime64[ns]'):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.0.dev_74ab638-py2.7-linux-x86_64.egg/pandas/algos.so in pandas.algos.take_2d_axis0_int64 (pandas/algos.c:73957)()\r\n\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n```"""
2476,9149373,hayd,changhiskhan,2012-12-10 16:37:37,2013-07-17 15:31:35,2012-12-10 17:08:23,closed,,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2476,b'Apply on an empty dataframe exception',"b'Apply to a DataFrame using lambda with a index/key (although that is in the DataFrame\'s index throws an error (a KeyError or IndexError respectively), perhaps it should exit more gracefully?\n\n```\nIn [1]: x = pd.DataFrame(index=[""a""])\n\nIn [2]: x\nOut[2]: \nEmpty DataFrame\nColumns: array([], dtype=object)\nIndex: array([a], dtype=object)\n\nIn [3]: x.apply(lambda x: x[""a""], axis=1)\n```\n\n```---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-3-80540f6c1d18> in <module>()\n----> 1 x.apply(lambda x: x[""a""], axis=1)\n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/core/frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   3750             if not broadcast:\n   3751                 if not all(self.shape):\n-> 3752                     is_reduction = not isinstance(f(_EMPTY_SERIES),\n   3753                                                   np.ndarray)\n   3754                     if is_reduction:\n\n<ipython-input-447-80540f6c1d18> in <lambda>(x)\n----> 1 x.apply(lambda x: x[""a""], axis=1)\n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/core/series.pyc in __getitem__(self, key)\n    462     def __getitem__(self, key):\n    463         try:\n--> 464             return self.index.get_value(self, key)\n    465         except InvalidIndexError:\n    466             pass\n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/core/index.pyc in get_value(self, series, key)\n    689                     raise InvalidIndexError(key)\n    690                 else:\n--> 691                     raise e1\n    692             except Exception:  # pragma: no cover\n    693                 raise e1\n\nKeyError: \'a\'\n```\n\n*I am trying to wrangle data from a huge number of Excel sheets (converting each sheet into a DataFrame but need to clean them before they are merged into something more sensible... I wanted to apply functions across these without worrying which are empty...*\n\n(same error in 0.9.1.)'"
2474,9146880,wesm,wesm,2012-12-10 15:52:51,2012-12-10 19:33:20,2012-12-10 19:33:20,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2474,b'Unrecognized compression mode causes segfault in read_csv',
2471,9146144,wesm,changhiskhan,2012-12-10 15:39:06,2012-12-10 17:35:36,2012-12-10 17:35:36,closed,changhiskhan,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2471,b'Timestamp.to_datetime fails with tzoffset objects',"b""example from #2262\r\n\r\n```\r\nIn [8]: df2['ts'][0].to_datetime() \r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-0663d65dd92c> in <module>()\r\n----> 1 df2['ts'][0].to_datetime()\r\n\r\n/home/wesm/code/pandas/pandas/tslib.so in pandas.tslib._Timestamp.to_datetime (pandas/tslib.c:8590)()\r\n\r\n/home/wesm/code/pandas/pandas/tslib.so in pandas.tslib._Timestamp.to_datetime (pandas/tslib.c:8454)()\r\n\r\n/home/wesm/code/pandas/pandas/tslib.so in pandas.tslib.convert_to_tsobject (pandas/tslib.c:10284)()\r\n\r\nAttributeError: 'tzoffset' object has no attribute 'normalize'\r\n\r\nIn [9]: df2['ts'][0]\r\nOut[9]: <Timestamp: 2012-11-15 10:38:04-0500, tz=tzoffset(None, -18000)>\r\n```"""
2465,9121457,wesm,wesm,2012-12-09 16:22:39,2012-12-09 21:27:05,2012-12-09 21:27:05,closed,wesm,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2465,"b'Usecols fails when actual column names are used, but passed in names argument'","b""e.g. usecols=['b', 'd', 'e'] vs. usecols=[1, 2, 4]"""
2464,9120896,aflaxman,wesm,2012-12-09 15:09:15,2012-12-12 15:56:04,2012-12-12 15:56:04,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2464,b'filter with like keyword fails when column names contain integers',"b""Here is a small example:\n\n<pre>\nIn [58]:\ndf = pd.DataFrame(0., index=[0,1,2], columns=[0,1,'A','B'])\ndf.filter(like='A')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-58-123fe8a23e8b> in <module>()\n      1 # BUG in pandas: cannot filter with like keyword if columns contain integers and string\n      2 df = pd.DataFrame(0., index=[0,1,2], columns=[0,1,'A','B'])\n----> 3 df.filter(like='A')\n\n/snfs2/HOME/abie/ENV/lib/python2.7/site-packages/pandas-0.10.0.dev_85b4b11-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in filter(self, items, like, regex)\n   2864             return self.reindex(columns=[r for r in items if r in self])\n   2865         elif like:\n-> 2866             return self.select(lambda x: like in x, axis=1)\n   2867         elif regex:\n   2868             matcher = re.compile(regex)\n\n/snfs2/HOME/abie/ENV/lib/python2.7/site-packages/pandas-0.10.0.dev_85b4b11-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in select(self, crit, axis)\n    318 \n    319         if len(axis) > 0:\n--> 320             new_axis = axis[np.asarray([crit(label) for label in axis])]\n    321         else:\n    322             new_axis = axis\n\n/snfs2/HOME/abie/ENV/lib/python2.7/site-packages/pandas-0.10.0.dev_85b4b11-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in <lambda>(x)\n   2864             return self.reindex(columns=[r for r in items if r in self])\n   2865         elif like:\n-> 2866             return self.select(lambda x: like in x, axis=1)\n   2867         elif regex:\n   2868             matcher = re.compile(regex)\n\nTypeError: argument of type 'int' is not iterable\n</pre>\n\nThis may have a simple solution, by making all of the labels into strings on line 320 of generic.py, I will investigate."""
2461,9118938,y-p,y-p,2012-12-09 09:24:40,2014-01-10 12:24:30,2014-01-10 12:24:30,closed,jtratner,Someday,10,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2461,b'Index is not hashable',"b""```python\nhash(df.index)\n> /home/user1/src/pandas/pandas/core/index.py(346)__hash__()\n    345     def __hash__(self):\n--> 346         return hash(self.view(np.ndarray))\n    347 \n\n/home/user1/src/pandas/pandas/core/index.pyc in __hash__(self)\n    344 \n    345     def __hash__(self):\n--> 346         return hash(self.view(np.ndarray))\n    347 \n    348     def __setitem__(self, key, value):\n\nTypeError: unhashable type: 'numpy.ndarray'\n```"""
2459,9116992,kieranholland,wesm,2012-12-09 02:21:15,2012-12-09 18:04:08,2012-12-09 18:04:08,closed,,0.10,8,Bug;Docs,https://api.github.com/repos/pydata/pandas/issues/2459,b'read_csv with names arg not implicitly setting header=None',"b""    >>> pandas.read_csv(StringIO('1,2'), names=['a', 'b'])\n    Empty DataFrame\n    Columns: [a, b]\n    Index: []\n    >>> pandas.read_csv(StringIO('1,2'), names=['a', 'b'], header=None)\n       a  b\n    0  1  2"""
2458,9116470,janschulz,y-p,2012-12-09 00:44:31,2014-07-20 08:52:27,2012-12-09 14:34:57,closed,,0.10,11,Bug,https://api.github.com/repos/pydata/pandas/issues/2458,b'umlauts in dataframe cannot be displayed in ipython notebook',"b'When I put this into a ipython notebook cell, an exception is thrown. \n\n```python\nimport pandas\ndata2 = [u""test"", u"""", u"""", u""\xa8\xa2""]\ndf2 = pandas.DataFrame({""a"":data2})\nprint(df2[""a""][1])\ndf2\n```\n\n```\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-5-0bb7d6f7d515> in <module>()\n      2 df2 = pandas.DataFrame({""a"":data2})\n      3 print(df2[""a""][1])\n----> 4 df2\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\core\\displayhook.pyc in __call__(self, result)\n    244             self.update_user_ns(result)\n    245             self.log_output(format_dict)\n--> 246             self.finish_displayhook()\n    247 \n    248     def flush(self):\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\zmq\\displayhook.pyc in finish_displayhook(self)\n     59         sys.stdout.flush()\n     60         sys.stderr.flush()\n---> 61         self.session.send(self.pub_socket, self.msg, ident=self.topic)\n     62         self.msg = None\n     63 \n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\zmq\\session.pyc in send(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\n    576 \n    577         buffers = [] if buffers is None else buffers\n--> 578         to_send = self.serialize(msg, ident)\n    579         to_send.extend(buffers)\n    580         longest = max([ len(s) for s in to_send ])\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\zmq\\session.pyc in serialize(self, msg, ident)\n    484             content = self.none\n    485         elif isinstance(content, dict):\n--> 486             content = self.pack(content)\n    487         elif isinstance(content, bytes):\n    488             # content is already packed, as in a relayed message\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\zmq\\session.pyc in <lambda>(obj)\n     78 _version_info_list = list(IPython.version_info)\n     79 # ISO8601-ify datetime objects\n---> 80 json_packer = lambda obj: jsonapi.dumps(obj, default=date_default)\n     81 json_unpacker = lambda s: extract_dates(jsonapi.loads(s))\n     82 \n\nC:\\portabel\\Python27\\lib\\site-packages\\zmq\\utils\\jsonapi.pyc in dumps(o, **kwargs)\n     70         kwargs[\'separators\'] = (\',\', \':\')\n     71 \n---> 72     return _squash_unicode(jsonmod.dumps(o, **kwargs))\n     73 \n     74 def loads(s, **kwargs):\n\nC:\\portabel\\Python27\\lib\\site-packages\\simplejson\\__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, use_decimal, namedtuple_as_object, tuple_as_array, bigint_as_string, sort_keys, item_sort_key, **kw)\n    332         sort_keys=sort_keys,\n    333         item_sort_key=item_sort_key,\n--> 334         **kw).encode(obj)\n    335 \n    336 \n\nC:\\portabel\\Python27\\lib\\site-packages\\simplejson\\encoder.pyc in encode(self, o)\n    235         # exceptions aren\'t as detailed.  The list call should be roughly\n    236         # equivalent to the PySequence_Fast that \'\'.join() would do.\n--> 237         chunks = self.iterencode(o, _one_shot=True)\n    238         if not isinstance(chunks, (list, tuple)):\n    239             chunks = list(chunks)\n\nC:\\portabel\\Python27\\lib\\site-packages\\simplejson\\encoder.pyc in iterencode(self, o, _one_shot)\n    309                 Decimal=Decimal)\n    310         try:\n--> 311             return _iterencode(o, 0)\n    312         finally:\n    313             key_memo.clear()\n\nUnicodeDecodeError: \'utf8\' codec can\'t decode byte 0xdf in position 22: invalid continuation byte\n\n```\nThis also happens when I have umlauts in csv files and specify an encoding:\n```\ndf3 = pandas.read_csv(""file_with_unlauts.csv"", encoding=""iso-8859-15"")\nprint(df3.head(10)[""name""][8]) # prints the right chars\ndf3.head(10) # throws the above error\n```'"
2453,9104947,rafaljozefowicz,wesm,2012-12-07 23:37:59,2012-12-08 00:16:32,2012-12-08 00:16:32,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2453,b'Data corruption in DataFrame when creating new columns while iterating over the index',"b""That's the shortest example I could find. Data gets somehow corrupted when creating new columns while iterating over the index. Pre-generating initial values for columns solved my problem (commented out part)\nI am using pandas 0.9.0\n\n```python\ndf = pd.DataFrame(index=[0,1])\ndf[0] = nan\nwasCol = {}\n# uncommenting these makes the results match\n#for col in xrange(100, 200):\n#    wasCol[col] = 1\n#    df[col] = nan\n\nfor i, dt in enumerate(df.index):\n    for col in xrange(100, 200):\n        if not col in wasCol:\n            wasCol[col] = 1\n            df[col] = nan\n        df[col][dt] = i\n\nmyid = 100\nprint(len(df.ix[isnan(df[myid]), [myid]]))\nprint(len(df.ix[isnan(df[myid]), [myid]]))\n```\n\nOutputs:\n```\n0\n1\n```"""
2449,9095664,aldanor,wesm,2012-12-07 17:46:50,2012-12-07 21:05:26,2012-12-07 21:05:26,closed,wesm,0.10,2,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2449,b'get_level_values() method for MultiIndex containing dates',"b""I was wondering if it was possible to elegantly extract the values of a MultiIndex which contains dates as datetime (and not an ndarray of datetime64[ns] which screws things up)?\n\n```\n>>> print df\n\n                  numtrades\ndate       id              \n2012-01-03 11891          1\n           14259          2\n2012-01-05 14259          1\n2012-01-06 11891          7\n2012-01-09 11891          2\n2012-01-10 11891         12\n           14259          4\n           10304          2\n2012-01-11 11891         11\n           14259          8\n\n>>>  df.index[0]\n\n(<Timestamp: 2012-01-03 00:00:00>, 11891)\n\n>>> df.index.get_level_values('date')\n\narray([1970-01-16 16:00:00, 1970-01-16 16:00:00, 1970-01-16 64:00:00,\n       1970-01-16 88:00:00, 1970-01-16 160:00:00, 1970-01-16 184:00:00,\n       1970-01-16 184:00:00, 1970-01-16 184:00:00, 1970-01-16 208:00:00,\n       1970-01-16 208:00:00], dtype=datetime64[ns])\n```\n\nOf course, we can always use `to_datetime` and then `to_pydatetime`, is that the preferred method? Am I missing something?"""
2448,9090272,rhkarls,changhiskhan,2012-12-07 14:55:11,2012-12-09 17:55:59,2012-12-09 17:55:59,closed,,0.10,2,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/2448,b'TimeSeries shift in plot when plotting different frequencies on same axis',"b""using pandas.TimeSeries.plot() causes a shift in the plotted timeseries when plotting two different TimeSeries objects on the same axis. When plotting in subplots or plotting using pylab/matplotlib directly it behaves as expected. Issue remains if objects are DataFrames instead of Series.\n\nVersion 0.9.1\n\nSimple code to reproduce:\n\n\nimport pandas as pd\nimport pylab as pl\n\nts_ind=pd.date_range('2012-01-01 13:00', '2012-01-02', freq='H')\nts_data=pl.random(12)\n\n#hourly timeseries\nts=pd.TimeSeries(ts_data, index=ts_ind)\n#minute frequency timeseries\nts2=ts.asfreq('T').interpolate()\n\n#using TimeSeries.plot()\npl.figure()\nts.plot()\nts2.plot(style='r')\n\n#using pylab.plot()\npl.figure()\npl.plot(ts.index, ts.values)\npl.plot(ts2.index, ts2.values, '-r')\n\n#using TimeSeries.plot() on different axes\npl.figure()\npl.subplot(211)\nts.plot()\npl.subplot(212)\nts2.plot(style='r')\n\n#the two timeseries objects are correct, problem lies with the plotting:\nts.index[ts==ts.max()]\nts2.index[ts2==ts2.max()]\n"""
2447,9074111,wesm,changhiskhan,2012-12-07 00:33:19,2012-12-07 16:58:58,2012-12-07 16:36:55,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2447,b'Override Series.tolist to control datetime64 -> PyObject conversion',
2443,9071272,wesm,wesm,2012-12-06 22:37:14,2012-12-07 16:53:27,2012-12-07 16:53:27,closed,wesm,0.10,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2443,b'Timestamp repr error with tzoffset tz',"b""e.g.\n\n`result = Timestamp('2002-06-28T01:00:00.000000000+0100')`\n\nfrom http://stackoverflow.com/questions/13703720/converting-between-datetime-timestamp-and-datetime64"""
2441,9067687,rafaljozefowicz,wesm,2012-12-06 21:00:21,2012-12-07 22:34:25,2012-12-07 22:34:25,closed,wesm,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2441,b'Corrupted values in Panel when index is not unique',"b'import pandas as pd\nfrom StringIO import StringIO\ntxt = """"""time,id,value\n2012-10-20,1,1.5\n2012-10-20,1,2.5\n2012-10-21,1,1.5\n""""""\nxxx = pd.read_csv(StringIO(txt), parse_dates=0)\nprint(xxx.set_index([""time""])[xxx.id == 1])\nprint(xxx.set_index([""time"", ""id""]).to_panel().value)\n\nIt prints out:\n            id  value\ntime                 \n2012-10-20   1    1.5\n2012-10-20   1    2.5\n2012-10-21   1    1.5\n\nid            1\ntime           \n2012-10-20  1.5\n2012-10-21  2.5\n\nThe value for panel on 2012-10-21 is incorrect.\n\n\nI am using pandas 0.9.0'"
2437,9051292,hayd,jreback,2012-12-06 12:09:25,2014-11-20 16:45:49,2013-03-22 14:58:02,closed,,0.11,5,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2437,b'Partial indexing only valid for ordered time series',"b""This was mentioned on [StackOverflow](http://stackoverflow.com/questions/13737992/indexing-timeseries-by-date-string), I thought I ought to post it here. I'm not sure whether or not this is a bug:\n\nYou can select by a date string in an ordered tseries, but not in an ordered one:\n\n```\nimport pandas as pd\nfrom numpy.random import randn\nfrom random import shuffle\nrng = pd.date_range(start='2011-01-01', end='2011-12-31')\nrng2 = list(rng)\nshuffle(rng2)\n```\n```\nts = pd.Series(randn(len(rng)), index=rng)\nts2 = pd.Series(randn(len(rng)), index=rng2)\n```\n```\nts.index\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2011-01-01 00:00:00, ..., 2011-12-31 00:00:00]\nLength: 365, Freq: D, Timezone: None\n\nts['2011-01-01']\n# -1.1454418070543406\n```\n```\nts2.index\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2011-04-16 00:00:00, ..., 2011-03-10 00:00:00]\nLength: 365, Freq: None, Timezone: None\n\nts2['2011-01-01'] # same for ts2.ix['2011-01-01']\n```\n```    \nTimeSeriesError                           Traceback (most recent call last)\n<ipython-input-112-051e81424a0d> in <module>()\n----> 1 ts2['2011-01-01']\n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/core/series.pyc in __getitem__(self, key)\n    462     def __getitem__(self, key):\n    463         try:\n--> 464             return self.index.get_value(self, key)\n    465         except InvalidIndexError:\n    466             pass\n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/tseries/index.pyc in get_value(self, series, key)\n   1015 \n   1016             try:\n-> 1017                 loc = self._get_string_slice(key)\n   1018                 return series[loc]\n   1019             except (TypeError, ValueError, KeyError):\n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/tseries/index.pyc in _get_string_slice(self, key)\n   1062         asdt, parsed, reso = parse_time_string(key, freq)\n   1063         key = asdt\n-> 1064         loc = self._partial_date_slice(reso, parsed)\n   1065         return loc\n   1066 \n\n/Library/Python/2.7/site-packages/pandas-0.9.0-py2.7-macosx-10.7-intel.egg/pandas/tseries/index.pyc in _partial_date_slice(self, reso, parsed)\n    977     def _partial_date_slice(self, reso, parsed):\n    978         if not self.is_monotonic:\n--> 979             raise TimeSeriesError('Partial indexing only valid for ordered time'\n    980                                   ' series')\n    981 \n\nTimeSeriesError: Partial indexing only valid for ordered time series\n```"""
2435,9034386,rbakhru,wesm,2012-12-05 21:27:50,2012-12-07 17:13:13,2012-12-07 17:13:13,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2435,b'index get_loc getting called on construction / major slowdown',"b""Using pandas 0.9\n\nget_loc in tseries/index.py is getting called a lot more than necessary, and its fairly slow given the attempts at converting strings to dates in the exception handler.  if you breakpoint get_loc and try things like truncate with a datetime range, calling as_matrix, or even just pulling slices, you'll notice it getting called with a key value for all types of magic methods and object variables at frame construction and slicing time.  \n\nAs an example, when constructing a dataframe, the superclass calls:\nself._item_cache = {}\n\nThat in turns calls __setattr_ which is overriden and it ends up calling get_loc off the index with a key of _item_cache (only to get back a KeyError).  \n\nBased off some cprofile results on the same set of code, my reindex calls using the repository codebase (for 0.9) results in 238534 calls to reindex that take 155.451 seconds.  If I modify get_loc with the statement below, it drops to 50.54 seconds.  This is closer to older versions of pandas that I had used.  Similar improvements are found in many major operations (transpose, as_matrix, etc).\n\nHack code I added to test speedup - add this at beginning of get_loc:\n\n        if isinstance(key, str) and key[0] == '_':\n            raise KeyError(key)\n\nThere's probably a better way to solve this directly in the __setattr__ though.\n\n\n"""
2431,9004763,y-p,y-p,2012-12-05 00:50:14,2012-12-05 01:53:42,2012-12-05 01:53:42,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2431,b'pd.save/load does not recreate some dataframe attributes',"b'Ran into this while trying some dropna and merge type operations\non a pickled data frame I loaded back i, It died with an `AttributeError`\non `self._known_consolidated`.\n\n```python\ndf = pd.DataFrame( [[1,2]])\ndf.save(""/tmp/1.pickle"")\nprint df._data._known_consolidated\ndf2=df.load(""/tmp/1.pickle"")\nprint df2._data._known_consolidated\n```\n\nlooking at `internals.BLockManager.__setstate__` and `__getstate__` [Here](https://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L547), those\nattributes are not saved off. \na quick fix would be to just init to false on load, But is there a legacy reason\nwhy getstate drops everything beyond the 3rd arg or can that be extended?\nis there a versioning mechanism?\n\n\n\n'"
2418,8964420,changhiskhan,wesm,2012-12-03 21:25:10,2012-12-06 18:35:47,2012-12-06 18:35:47,closed,,0.10,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2418,b'More broken UTF-16 support',"b""http://stackoverflow.com/questions/13690122/pandas-read-csv-and-utf-16\r\n\r\n```\r\nIn [3]: url = 'http://brianckeegan.com/data/candidates-spanish.txt'\r\n\r\nIn [4]: pd.read_table(url, sep='\\t', encoding='utf-16')\r\nSegmentation fault: 11 (core dumped)\r\n```"""
2411,8956108,wesm,wesm,2012-12-03 17:00:09,2012-12-07 16:28:59,2012-12-07 16:28:59,closed,,0.10,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2411,b'Enable passing arguments to str.l/r/strip functions',b're: http://stackoverflow.com/questions/13682044/pandas-dataframe-remove-unwanted-parts-from-strings-in-a-column?utm_source=twitterfeed:python:newest&utm_medium=twitter'
2404,8908881,y-p,wesm,2012-12-02 02:06:36,2012-12-02 03:23:16,2012-12-02 03:23:16,closed,wesm,0.10,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2404,b'failing vbenchmarks',b'https://travis-ci.org/y-p/pandas/jobs/3456390'
2396,8850635,lbeltrame,y-p,2012-11-30 10:08:17,2014-02-07 14:35:24,2012-12-10 18:55:03,closed,,0.10,39,Bug,https://api.github.com/repos/pydata/pandas/issues/2396,b'New Excel changes cause an extra line to be generated in the Excel file',"b'An example:\n\n````python\n\nIn [9]: data = pandas.ExcelFile(""sample.xls"") # generated with df.to_excel()\n\nIn [10]: data.parse(data.sheet_names[0], index_col=0)\nOut[10]: \n         Value\nID         NaN\nID1  38.625700\nID2  44.691054\n\n````\nTo be more clear, ``ID`` is the original index name, but it is shifted down one row when saving to Excel, and then it is picked up as a regular row by the Excel parser.\n\nThis kinds of complicates parsing if I\'m expecting to read something generated by to_excel()...'"
2386,8812502,shuras,wesm,2012-11-29 14:39:56,2012-12-02 16:38:52,2012-12-02 16:38:52,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2386,b'integer overflow in pandas.DataFrame.unstack()',"b'Pandas version 0.9.1\r\n\r\nThere is an integer overflow problem in pandas.DataFrame.unstack()\r\nWhen unstack() is called on a relatively large data set (1 million rows) with four-column MultiIndex an exception is raised: ""ValueError: negative dimensions are not allowed"".\r\nStack trace is below,\r\n\r\nThe actual problem is in method pandas.core.reshape._Unstacker._make_sorted_values_labels(self)\r\n\r\nline 91:        max_groups = np.prod(sizes)\r\n\r\nEach of the sizes fits in int32, but their product doesn\'t.\r\nSo max_groups becomes negative.\r\n\r\n```\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in unstack(self, level)\r\n   3712         """"""\r\n   3713         from pandas.core.reshape import unstack\r\n-> 3714         return unstack(self, level)\r\n   3715 \r\n   3716     #----------------------------------------------------------------------\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\reshape.pyc in unstack(obj, level)\r\n    361     if isinstance(obj, DataFrame):\r\n    362         if isinstance(obj.index, MultiIndex):\r\n--> 363             return _unstack_frame(obj, level)\r\n    364         else:\r\n    365             return obj.T.stack(dropna=False)\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\reshape.pyc in _unstack_frame(obj, level)\r\n    399     else:\r\n    400         unstacker = _Unstacker(obj.values, obj.index, level=level,\r\n--> 401                                value_columns=obj.columns)\r\n    402         return unstacker.get_result()\r\n    403 \r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\reshape.pyc in __init__(self, values, index, level, value_columns)\r\n     77         self.full_shape = np.prod(lshape[:v] + lshape[v + 1:]), lshape[v]\r\n     78 \r\n---> 79         self._make_sorted_values_labels()\r\n     80         self._make_selectors()\r\n     81 \r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\reshape.pyc in _make_sorted_values_labels(self)\r\n     96             comp_index, ngroups = group_index, max_groups\r\n     97 \r\n---> 98         indexer = lib.groupsort_indexer(comp_index, ngroups)[0]\r\n     99         indexer = _ensure_platform_int(indexer)\r\n    100 \r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.groupsort_indexer (pandas\\src\\tseries.c:51018)()\r\n\r\nValueError: negative dimensions are not allowed\r\n```'"
2374,8767706,craustin,wesm,2012-11-28 14:55:22,2012-12-07 21:41:26,2012-12-07 21:32:06,closed,wesm,0.10,5,Bug,https://api.github.com/repos/pydata/pandas/issues/2374,b'DataFrame.applymap with identity function changes dtype',"b""from datetime import datetime\nfrom pandas import DataFrame\ndf = DataFrame({'x1': [datetime(1996,1,1)]})\ndf2 = df.applymap(lambda x: x)\n\nIssue:\ntype(df['x1'][0]) == datetime.datetime\ntype(df2['x1'][0]) == pandas.lib.Timestamp\n\nThis repros in the 11/27/2012 cut of 0.10.0 - but not in 0.9.0 release."""
2367,8726942,wesm,wesm,2012-11-27 16:33:03,2012-11-28 03:36:24,2012-11-28 03:36:24,closed,wesm,0.10,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2367,b'Setting timezone in DatetimeIndex.union looks fishy',b'Noted during code review'
2361,8693862,etyurin,wesm,2012-11-26 21:02:40,2012-11-26 22:31:02,2012-11-26 22:31:02,closed,wesm,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2361,b'Stdout/stderr not handled properly in pandas.read_table()',"b'pandas.read_table() does not handle stderr consistently.\n\nerror_bad_lines=True writes to stderr (which is correct), but warn_bad_lines=True writes to stdout (which is wrong).\n\nThis is with commit ae1b8a888149f6474c13141f8194da322dccc466\n'"
2355,8653636,wesm,wesm,2012-11-25 18:16:16,2012-11-28 02:47:33,2012-11-28 02:47:33,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2355,b'Raise exception from overflowing uint64 data',b'ref: http://stackoverflow.com/questions/13550940/python-pandas-insert-long-integer'
2350,8643231,wesm,wesm,2012-11-25 04:28:10,2012-11-25 05:19:30,2012-11-25 05:19:30,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2350,b'Debug sparse memory usage',"b'I would expect this to use almost no memory. Instead, > 2GB on my box. \r\n\r\n`SparseDataFrame(columns=np.arange(100), index=np.arange(1e6))`'"
2348,8636922,jseabold,wesm,2012-11-24 21:48:01,2012-11-26 16:36:21,2012-11-25 03:38:08,closed,,0.10,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2348,b'read_csv segfault',"b'Works fine on v0.9.0. With master\n\n```\nrossi = pandas.read_csv(""http://www.ams.jhu.edu/~dan/550.400/datasets/recidivism/Rossi%20data.csv"")\n*** glibc detected *** /usr/bin/python: malloc(): memory corruption: 0x0000000003c26680 ***\n======= Backtrace: =========\n/lib/x86_64-linux-gnu/libc.so.6(+0x7ba9a)[0x7f498b804a9a]\n/lib/x86_64-linux-gnu/libc.so.6(+0x7ce32)[0x7f498b805e32]\n/lib/x86_64-linux-gnu/libc.so.6(realloc+0xf9)[0x7f498b807839]\n/usr/bin/python(PyList_Append+0xd6)[0x46e336]\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x99763)[0x7f497f8cf763]\n/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x998d9)[0x7f497f8cf8d9]\n/usr/bin/python[0x47ac88]\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_f64ccf0-py2.7-linux-x86_64.egg/pandas/_parser.so(+0xb955)[0x7f4976c5a955]\n/usr/bin/python(PyObject_Call+0x44)[0x45fed4]\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_f64ccf0-py2.7-linux-x86_64.egg/pandas/_parser.so(+0x2418d)[0x7f4976c7318d]\n/usr/bin/python(PyObject_Call+0x44)[0x45fed4]\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_f64ccf0-py2.7-linux-x86_64.egg/pandas/_parser.so(+0x17ae1)[0x7f4976c66ae1]\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_f64ccf0-py2.7-linux-x86_64.egg/pandas/_parser.so(+0x142f3)[0x7f4976c632f3]\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_f64ccf0-py2.7-linux-x86_64.egg/pandas/_parser.so(+0xf5bd)[0x7f4976c5e5bd]\n/usr/bin/python(PyEval_EvalFrameEx+0x361)[0x499061]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalFrameEx+0xb2f)[0x49982f]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalCode+0x32)[0x4eeb92]\n/usr/bin/python(PyEval_EvalFrameEx+0x37d5)[0x49c4d5]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalFrameEx+0x802)[0x499502]\n/usr/bin/python(PyEval_EvalCodeEx+0x145)[0x49fd55]\n/usr/bin/python(PyEval_EvalCode+0x32)[0x4eeb92]\n/usr/bin/python[0x4ff7f4]\n/usr/bin/python(PyRun_FileExFlags+0x90)[0x42cdd0]\n/usr/bin/python(PyRun_SimpleFileExFlags+0x2dd)[0x42d798]\n/usr/bin/python(Py_Main+0xad3)[0x418db5]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xff)[0x7f498b7a7eff]\n/usr/bin/python[0x4c91b1]\n======= Memory map: ========\n00400000-0062f000 r-xp 00000000 08:05 12064965                           /usr/bin/python2.7\n0082e000-0082f000 r--p 0022e000 08:05 12064965                           /usr/bin/python2.7\n0082f000-00897000 rw-p 0022f000 08:05 12064965                           /usr/bin/python2.7\n00897000-008a9000 rw-p 00000000 00:00 0 \n00c80000-03e97000 rw-p 00000000 00:00 0                                  [heap]\n7f496b8fd000-7f496b914000 r-xp 00000000 08:05 13107724                   /lib/x86_64-linux-gnu/libresolv-2.13.so\n7f496b914000-7f496bb14000 ---p 00017000 08:05 13107724                   /lib/x86_64-linux-gnu/libresolv-2.13.so\n7f496bb14000-7f496bb15000 r--p 00017000 08:05 13107724                   /lib/x86_64-linux-gnu/libresolv-2.13.so\n7f496bb15000-7f496bb16000 rw-p 00018000 08:05 13107724                   /lib/x86_64-linux-gnu/libresolv-2.13.so\n7f496bb16000-7f496bb18000 rw-p 00000000 00:00 0 \n7f496bb18000-7f496bb1f000 r-xp 00000000 08:05 13107879                   /lib/x86_64-linux-gnu/libnss_dns-2.13.so\n7f496bb1f000-7f496bd1e000 ---p 00007000 08:05 13107879                   /lib/x86_64-linux-gnu/libnss_dns-2.13.so\n7f496bd1e000-7f496bd1f000 r--p 00006000 08:05 13107879                   /lib/x86_64-linux-gnu/libnss_dns-2.13.so\n7f496bd1f000-7f496bd20000 rw-p 00007000 08:05 13107879                   /lib/x86_64-linux-gnu/libnss_dns-2.13.so\n7f496bd20000-7f496bd22000 r-xp 00000000 08:05 13107270                   /lib/libnss_mdns4_minimal.so.2\n7f496bd22000-7f496bf21000 ---p 00002000 08:05 13107270                   /lib/libnss_mdns4_minimal.so.2\n7f496bf21000-7f496bf22000 r--p 00001000 08:05 13107270                   /lib/libnss_mdns4_minimal.so.2\n7f496bf22000-7f496bf23000 rw-p 00002000 08:05 13107270                   /lib/libnss_mdns4_minimal.so.2\n7f496bf23000-7f496bf2f000 r-xp 00000000 08:05 13107883                   /lib/x86_64-linux-gnu/libnss_files-2.13.so\n7f496bf2f000-7f496c12e000 ---p 0000c000 08:05 13107883                   /lib/x86_64-linux-gnu/libnss_files-2.13.so\n7f496c12e000-7f496c12f000 r--p 0000b000 08:05 13107883                   /lib/x86_64-linux-gnu/libnss_files-2.13.so\n7f496c12f000-7f496c130000 rw-p 0000c000 08:05 13107883                   /lib/x86_64-linux-gnu/libnss_files-2.13.so\n7f496c130000-7f496c136000 r-xp 00000000 08:05 12194961                   /usr/lib/python2.7/lib-dynload/_multiprocessing.so\n7f496c136000-7f496c335000 ---p 00006000 08:05 12194961                   /usr/lib/python2.7/lib-dynload/_multiprocessing.so\n7f496c335000-7f496c336000 r--p 00005000 08:05 12194961                   /usr/lib/python2.7/lib-dynload/_multiprocessing.so\n7f496c336000-7f496c337000 rw-p 00006000 08:05 12194961                   /usr/lib/python2.7/lib-dynload/_multiprocessing.so\n7f496c337000-7f496c347000 r-xp 00000000 08:05 5117216                    /usr/local/lib/python2.7/dist-packages/sklearn/utils/murmurhash.so\n7f496c347000-7f496c546000 ---p 00010000 08:05 5117216                    /usr/local/lib/python2.7/dist-packages/sklearn/utils/murmurhash.so\n7f496c546000-7f496c547000 r--p 0000f000 08:05 5117216                    /usr/local/lib/python2.7/dist-packages/sklearn/utils/murmurhash.so\n7f496c547000-7f496c549000 rw-p 00010000 08:05 5117216                    /usr/local/lib/python2.7/dist-packages/sklearn/utils/murmurhash.so\n7f496c549000-7f496c54b000 r-xp 00000000 08:05 5117217                    /usr/local/lib/python2.7/dist-packages/sklearn/__check_build/_check_build.so\n7f496c54b000-7f496c74a000 ---p 00002000 08:05 5117217                    /usr/local/lib/python2.7/dist-packages/sklearn/__check_build/_check_build.so\n7f496c74a000-7f496c74b000 r--p 00001000 08:05 5117217                    /usr/local/lib/python2.7/dist-packages/sklearn/__check_build/_check_build.soAborted\n```\n\n'"
2347,8636816,y-p,wesm,2012-11-24 21:41:06,2012-11-25 19:17:43,2012-11-25 19:17:43,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2347,b'del df[k] fails for a non-unique key',"b'``` python\nimport pandas as pd\ndf=pd.DataFrame({0:[0,1],1:[0,1],2:[4,5]})\ndf.columns=[""a"",""b"",""a""]\nres=df.pop(""a"")\n...\n\n/home/user1/src/pandas/pandas/core/internals.py in _delete_from_block(self, i, item)\n    971         """"""\n    972         block = self.blocks.pop(i)\n--> 973         new_left, new_right = block.split_block_at(item)\n    974 \n    975         if new_left is not None:\n\n/home/user1/src/pandas/pandas/core/internals.py in split_block_at(self, item)\n    195             return None, None\n    196 \n--> 197         if loc == 0:\n    198             # at front\n    199             left_block = None\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n```\n'"
2341,8627854,bad,wesm,2012-11-24 11:51:39,2012-11-28 13:59:56,2012-11-25 05:29:09,closed,,0.10,6,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/2341,b'integer constant overflow in generated tseries.c',"b'The generated tseries.c included in the 0.9.0 and 0.9.1 distributions contains two integer constants that are to large for 32bit values and need an LL suffix.  Note that the LL suffix is present in the cython source.\n\n\n```\n--- pandas/src/tseries.c.orig   2012-11-15 00:20:20.000000000 +0000\n+++ pandas/src/tseries.c        2012-11-20 22:58:37.000000000 +0000\n@@ -151203,7 +151203,7 @@\n  * cdef int64_t _NS_UPPER_BOUND = -9223372036854775807LL\n  * \n  */\n-  __pyx_v_6pandas_3lib__NS_LOWER_BOUND = -9223285636854775809;\n+  __pyx_v_6pandas_3lib__NS_LOWER_BOUND = -9223285636854775809LL;\n \n   /* ""/home/wesm/code/pandas/pandas/src/datetime.pyx"":665\n  * \n@@ -151212,7 +151212,7 @@\n  * \n  * cdef inline _check_dts_bounds(int64_t value, pandas_datetimestruct *dts):\n  */\n-  __pyx_v_6pandas_3lib__NS_UPPER_BOUND = -9223372036854775807;\n+  __pyx_v_6pandas_3lib__NS_UPPER_BOUND = -9223372036854775807LL;\n \n   /* ""/home/wesm/code/pandas/pandas/src/datetime.pyx"":745\n  *         raise ValueError(\'Unable to parse %s\' % str(val))\n```'"
2338,8620615,wesm,changhiskhan,2012-11-24 00:56:00,2012-11-25 19:18:35,2012-11-24 07:03:32,closed,,0.10,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2338,"b""DatetimeIndex.normalize isn't tz-aware-friendly""",
2331,8586291,juliantaylor,y-p,2012-11-22 19:56:44,2012-12-02 13:05:12,2012-12-02 13:05:12,closed,y-p,0.10,22,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/2331,b'python3 test failures',"b'on ubuntu 13.04 several tests fail with 3.3 while they suceed with python2\n13.04 has python 3.3 and numpy 1.7\ncurrent pandas head 61766ec5f898c89\n\n```\n======================================================================\nFAIL: test_quoting (pandas.io.tests.test_parsers.TestParsers)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/jtaylor/tmp/pandas-0.9.1/build/lib.linux-x86_64-3.3/pandas/io/tests/test_parsers.py"", line 528, in test_quoting\n    sep=\'\\t\')\nAssertionError: Exception not raised by read_table\n\n======================================================================\nFAIL: test_cant_compare_tz_naive_w_aware (pandas.tseries.tests.test_timeseries.TestTimestamp)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/jtaylor/tmp/pandas-0.9.1/build/lib.linux-x86_64-3.3/pandas/tseries/tests/test_timeseries.py"", line 2349, in test_cant_compare_tz_naive_w_aware\n    self.assertRaises(Exception, a.__eq__, b.to_pydatetime())\nAssertionError: Exception not raised by __eq__\n\n----------------------------------------------------------------------\n\n======================================================================\nFAIL: test_more_flexible_frame_multi_function (__main__.TestGroupBy)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/jtaylor/tmp/pandas-0.9.1/build/lib.linux-x86_64-3.3/pandas/tests/test_groupby.py"", line 1909, in test_more_flexible_frame_multi_function\n    assert_frame_equal(result, expected)\n  File ""/home/jtaylor/tmp/pandas-0.9.1/build/lib.linux-x86_64-3.3/pandas/util/testing.py"", line 167, in assert_frame_equal\n    assert(left.columns.equals(right.columns))\nAssertionError\n\n----------------------------------------------------------------------\n\n======================================================================\nERROR: test_yahoo (pandas.io.tests.test_yahoo.TestYahoo)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/tests/test_yahoo.py"", line 25, in test_yahoo\n    pd.DataReader(""F"", \'yahoo\', start, end)[\'Close\'][-1],\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/data.py"", line 58, in DataReader\n    retry_count=retry_count, pause=pause)\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/data.py"", line 149, in get_data_yahoo\n    parse_dates=True)[::-1]\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/parsers.py"", line 364, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/parsers.py"", line 195, in _read\n    return parser.read()\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/parsers.py"", line 592, in read\n    ret = self._engine.read(nrows)\n  File ""/tmp/pandas/build/lib.linux-x86_64-3.3/pandas/io/parsers.py"", line 844, in read\n    data = self._reader.read(nrows)\n  File ""parser.pyx"", line 597, in pandas._parser.TextReader.read (pandas/src/parser.c:5342)\n  File ""parser.pyx"", line 619, in pandas._parser.TextReader._read_low_memory (pandas/src/parser.c:5562)\n  File ""parser.pyx"", line 668, in pandas._parser.TextReader._read_rows (pandas/src/parser.c:6143)\n  File ""parser.pyx"", line 655, in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:6027)\n  File ""parser.pyx"", line 1385, in pandas._parser.raise_parser_error (pandas/src/parser.c:14807)\npandas._parser.CParserError: Error tokenizing data. C error: Expected 7 fields in line 106, saw 3\n\n\n\n```\n\n\n\n```\ntest_more_flexible_frame_multi_function (__main__.TestGroupBy) ... > /home/jtaylor/tmp/pandas-0.9.1/build/lib.linux-x86_64-3.3/pandas/util/testing.py(167)assert_frame_equal()\n-> assert(left.columns.equals(right.columns))\n(Pdb) p left.columns\n\n[(D, mean), (D, std), (C, mean), (C, std)]\n(Pdb) p right.columns\nMultiIndex\n[(C, mean), (C, std), (D, mean), (D, std)]\n```\n\nthis one is random probably related to hash randomization\n\n\nthe yahoo one might be due to missing internet connection?\n\n'"
2329,8584744,cottrell,cottrell,2012-11-22 18:24:36,2012-11-23 20:01:48,2012-11-23 20:01:48,closed,,0.10,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2329,b'datetime to index issue',"b""I'm not sure if this is related to the datetime64 issues listed I've seen posted here. Still haven't really figured out exactly what is going but the date gets mangled when it moved to an index.\r\n\r\n```\r\nIn [93]: import datetime, pandas\r\nIn [94]: df = pandas.DataFrame([[datetime.datetime.today(), 12.1]], columns=['Date', 'Value'])\r\n\r\nIn [95]: df = df.set_index('Date')\r\n\r\nIn [96]: df\r\nOut[96]: \r\n                            Value\r\nDate                             \r\n2012-11-22 12:12:40.905739   12.1    \r\n\r\nIn [97]: df.index\r\nOut[97]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2190-12-31 02:18:44.941732032]\r\nLength: 1, Freq: None, Timezone: None\r\n\r\nIn [98]: df = df.reset_index()\r\n\r\nIn [99]: df\r\nOut[99]: \r\n                           Date  Value\r\n0 2190-12-31 02:18:44.941732032   12.1\r\n```"""
2319,8559841,wesm,wesm,2012-11-21 21:52:29,2012-11-27 03:07:33,2012-11-23 04:55:50,closed,,0.10,7,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2319,b'Index.__repr__ no longer summarizing sufficiently large arrays',"b'e.g.\r\n\r\n```\r\nIndex(np.arange(100000))\r\n```\r\n\r\nThis might have happened in the `pprint_thing` refactoring, will have to take a look'"
2318,8557473,juliantaylor,wesm,2012-11-21 20:33:19,2012-11-26 22:59:06,2012-11-26 22:59:06,closed,,0.10,5,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/2318,b'0.9.1 test failures on big endian machines',"b'see\nhttps://launchpadlibrarian.net/123637321/buildlog_ubuntu-raring-powerpc.pandas_0.9.1-1ubuntu1_FAILEDTOBUILD.txt.gz\n\nhttps://buildd.debian.org/status/package.php?p=pandas&suite=experimental\n\n```\n======================================================================\nERROR: test_fperr_robustness (pandas.stats.tests.test_moments.TestMoments)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/build/buildd/pandas-0.9.1/debian/python-pandas/usr/lib/python2.7/dist-packages/pandas/stats/tests/test_moments.py"", line 175, in test_fperr_robustness\n    result = mom.rolling_sum(arr, 2)\n  File ""/build/buildd/pandas-0.9.1/debian/python-pandas/usr/lib/python2.7/dist-packages/pandas/stats/moments.py"", line 458, in f\n    freq=freq, time_rule=time_rule, **kwargs)\n  File ""/build/buildd/pandas-0.9.1/debian/python-pandas/usr/lib/python2.7/dist-packages/pandas/stats/moments.py"", line 261, in _rolling_moment\n    result = np.apply_along_axis(calc, axis, values)\n  File ""/usr/lib/python2.7/dist-packages/numpy/lib/shape_base.py"", line 80, in apply_along_axis\n    res = func1d(arr[tuple(i.tolist())],*args)\n  File ""/build/buildd/pandas-0.9.1/debian/python-pandas/usr/lib/python2.7/dist-packages/pandas/stats/moments.py"", line 258, in <lambda>\n    calc = lambda x: func(x, window, minp=minp, **kwargs)\n  File ""/build/buildd/pandas-0.9.1/debian/python-pandas/usr/lib/python2.7/dist-packages/pandas/stats/moments.py"", line 456, in call_cython\n    return func(arg, window, minp, **kwds)\n  File ""moments.pyx"", line 156, in pandas.lib.roll_sum (pandas/src/tseries.c:77996)\nValueError: Little-endian buffer not supported on big-endian compiler\n\n======================================================================\nFAIL: test_from_M8_structured (pandas.tseries.tests.test_timeseries.TestLegacySupport)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/build/buildd/pandas-0.9.1/debian/python-pandas/usr/lib/python2.7/dist-packages/pandas/tseries/tests/test_timeseries.py"", line 1917, in test_from_M8_structured\n    self.assertEqual(df[\'Date\'][0], dates[0][0])\nAssertionError: <Timestamp: 1976-02-12 15:25:34.986240> != datetime.datetime(2012, 9, 9, 0, 0)\n```\n\n'"
2317,8556834,bball7210,wesm,2012-11-21 20:11:38,2012-11-24 02:24:44,2012-11-24 00:50:56,closed,wesm,0.10,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2317,b'Error in Timezone handling of DataFrames when joined',"b'When joining two DataFrames with localized indexes, the resulting DataFrame has erroneous Timestamps. (It appears that at some point the timestamps are interpreted at UTC without a correct timezone correction?)\n\nSample code:\n\n```\nIn [94]: test1 = pandas.DataFrame(numpy.zeros((6,3)),index=pandas.tseries.index.date_range(""2012-11-15 00:00:00"", periods=6, freq=""100L"", tz=""US/Central""))\n\nIn [95]: test2 = pandas.DataFrame(numpy.zeros((3,3)),index=pandas.tseries.index.date_range(""2012-11-15 00:00:00"", periods=3, freq=""250L"", tz=""US/Central""), columns=range(3,6))\n\nIn [96]: test1\nOut[96]:\n                                  0  1  2\n2012-11-15 00:00:00-06:00         0  0  0\n2012-11-15 00:00:00.100000-06:00  0  0  0\n2012-11-15 00:00:00.200000-06:00  0  0  0\n2012-11-15 00:00:00.300000-06:00  0  0  0\n2012-11-15 00:00:00.400000-06:00  0  0  0\n2012-11-15 00:00:00.500000-06:00  0  0  0\n\nIn [97]: test2\nOut[97]:\n                                  3  4  5\n2012-11-15 00:00:00-06:00         0  0  0\n2012-11-15 00:00:00.250000-06:00  0  0  0\n2012-11-15 00:00:00.500000-06:00  0  0  0\n\nIn [98]: test1.join(test2, how=""outer"")\nOut[98]:\n                                   0   1   2   3   4   5\n2012-11-15 06:00:00-06:00          0   0   0   0   0   0\n2012-11-15 06:00:00.100000-06:00   0   0   0 NaN NaN NaN\n2012-11-15 06:00:00.200000-06:00   0   0   0 NaN NaN NaN\n2012-11-15 06:00:00.250000-06:00 NaN NaN NaN   0   0   0\n2012-11-15 06:00:00.300000-06:00   0   0   0 NaN NaN NaN\n2012-11-15 06:00:00.400000-06:00   0   0   0 NaN NaN NaN\n2012-11-15 06:00:00.500000-06:00   0   0   0   0   0   0\n```\n'"
2307,8529024,pikeas,wesm,2012-11-21 02:39:29,2012-11-24 00:24:55,2012-11-24 00:24:55,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2307,b'DataFrame.combine_first() breaks for DataFrame with indices but no columns',"b""Title says it all. core/frame.py, combine() method at line 3448:\n\n        if other.empty:\n            return self.copy()\n\nIf other is a DataFrame which has indices but no columns, a combine_first() should add the indices with every column set to NaN.\n\nSorry for not submitting a patch myself - I'm very new to the pandas codebase, so I'm not sure whether this should be resolved in combine() for all combination attempts, or only in combine_first().\n\n...Maybe empty should check whether the DataFrame has indices? Though that could silently break a lot of old code..."""
2306,8521361,pikeas,wesm,2012-11-20 21:36:29,2012-12-02 17:12:13,2012-12-02 17:12:13,closed,,0.10,10,Bug,https://api.github.com/repos/pydata/pandas/issues/2306,b'Bug/inconsistent result when indexing DataFrame by YY-MM-DD',"b""    rng = date_range('2012-01-01', periods=1000, freq='12H')\n    vals = randn(1000)\n    df = DataFrame(vals, index=rng)\n\n`df.ix['2012']` and `df['2012-05']` (selection by year or year and month) work as expected. However:\n\n    In [314]: df.ix['2012-01-01']\n    Out[314]: \n    0    0.107819\n    Name: 2012-01-01 00:00:00\n\nThe output from indexing by year/month/day is unexpected - we should receive either all values matching the day or an error (if indexing by day has not been implemented), instead we get only the first match."""
2300,8505571,dalejung,wesm,2012-11-20 14:09:25,2012-12-07 20:48:03,2012-12-07 20:48:03,closed,wesm,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2300,b'BinGrouper AttributeError when returning a Series from DataFrame.apply',"b'http://nbviewer.ipython.org/4118124/\n\n```python\nimport pandas as pd\nfrom pandas.tseries.resample import TimeGrouper\n\nN = 1000\nind = pd.date_range(start=""2000-01-01"", freq=""D"", periods=N)\ndf = pd.DataFrame({\'open\':1, \'close\':2}, index=ind)\ntg = TimeGrouper(\'M\')\ngrouper = tg.get_grouper(df)\n# Errors\ndf.groupby(grouper).apply(lambda df: df[\'close\'] / df[\'open\'])\n# AttributeError: \'BinGrouper\' object has no attribute \'groupings\'\n```\n\nError does not occur for regular Grouper. '"
2298,8497822,gerigk,wesm,2012-11-20 09:27:11,2012-12-06 18:37:11,2012-12-06 18:37:11,closed,,0.10,10,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2298,b'read_csv fails for UTF-16 with BOM (maybe also other encodings with BOM) and skiprows',"b""Unfortunately I can't send the file but from the output of head filename -n 7\n```\nName\tAd performance report\t\t\t\t\t\t\t\nType\tAd\t\t\t\t\t\t\t\t\t\nFrequency\tOne time\t\t\t\t\t\t\t\nDate range\tCustom date range\t\t\t\t\t\t\nDates\tSep 19, 2012-Nov 19, 2012\t\t\t\t\t\t\nAccount\tDay\tCampaign\tAd group\tAd ID\tClient name\tDestination URL\tImpressions\tClicks\tCost\tAvg. position\tStatus\tConv. (1-per-click)\nCategories 2\t15.11.2012\tsomething: \x11\x04;\x04C\x047\x04:\x048\x04\t[somethinglse]{test}: \x11\x04;\x04C\x047\x04:\x048\x04\t16902484818\tCategories 2\thttp://www.someurl?ad=291012\t333\t2\t4.7\t5.5\tapproved\t0\n```\nI guess that the beginning of the file is the BOM and that this causes problems when skipping the rows. Without skiprows everything gets read into one row with the first column containing the BOM.\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 0 entries\nData columns:\nName\\tAd performance report\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n...\n```\n\nThe error raised is:\n```\npd.read_csv('/home/arthur/Desktop/client 139 - ads report/test_pandas.csv', sep='\\t', skiprows=5)\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, header, index_col, names, skiprows, skipfooter, skip_footer, na_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\n    361                     buffer_lines=buffer_lines)\n    362 \n--> 363         return _read(filepath_or_buffer, kwds)\n    364 \n    365     parser_f.__name__ = name\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)\n    185 \n    186     # Create the parser.\n--> 187     parser = TextFileReader(filepath_or_buffer, **kwds)\n    188 \n    189     if nrows is not None:\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in __init__(self, f, engine, **kwds)\n    465         self.options, self.engine = self._clean_options(options, engine)\n    466 \n--> 467         self._make_engine(self.engine)\n    468 \n    469     def _get_options_with_defaults(self, engine):\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in _make_engine(self, engine)\n    567     def _make_engine(self, engine='c'):\n    568         if engine == 'c':\n--> 569             self._engine = CParserWrapper(self.f, **self.options)\n    570         else:\n    571             if engine == 'python':\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/io/parsers.pyc in __init__(self, src, **kwds)\n    787         ParserBase.__init__(self, kwds)\n    788 \n--> 789         self._reader = _parser.TextReader(src, **kwds)\n    790 \n    791         # XXX\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/_parser.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3579)()\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_b8dae94-py2.7-linux-x86_64.egg/pandas/_parser.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4590)()\n\nCParserError: Passed header=0 but only 0 lines in file\n```\n"""
2296,8486974,cossatot,wesm,2012-11-19 23:17:27,2012-11-20 04:54:46,2012-11-20 04:53:42,closed,,0.10,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2296,b'bug in csv_reader?',"b""Some previously-functioning code of mine broke this morning after upgrading to the newest Ubuntu package of Pandas v. 0.9.2.  It seems to be a bug in the upgraded read_csv parser.  Beyond that, the error message (below) is fairly unhelpful to the uninitiated.  Maybe this is a bug, or maybe the new version uses different syntax (although some of the .csv files are able to be imported...).\n\nI am sending a copy of the offending file to Wes, as I've got no quick place to put it.\n\n\nThanks,\nRichard\n\naHe_df = pd.readcsv('aHe_aliquots.csv')\n\n---------------------------------------------------------------------------\nCParserError                              Traceback (most recent call last)\n/home/itchy/ecopetrol/ec-working/data/<ipython-input-29-9ec771184960> in <module>()\n----> 1 aHe_df =pd.read_csv('aHe_aliquots.csv')\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, header, index_col, names, skiprows, skipfooter, skip_footer, na_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\n    361                     buffer_lines=buffer_lines)\n    362 \n--> 363         return _read(filepath_or_buffer, kwds)\n    364 \n    365     parser_f.__name__ = name\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)\n    185 \n    186     # Create the parser.\n\n--> 187     parser = TextFileReader(filepath_or_buffer, **kwds)\n    188 \n    189     if nrows is not None:\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in __init__(self, f, engine, **kwds)\n    465         self.options, self.engine = self._clean_options(options, engine)\n    466 \n--> 467         self._make_engine(self.engine)\n    468 \n    469     def _get_options_with_defaults(self, engine):\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in _make_engine(self, engine)\n    567     def _make_engine(self, engine='c'):\n    568         if engine == 'c':\n--> 569             self._engine = CParserWrapper(self.f, **self.options)\n    570         else:\n    571             if engine == 'python':\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in __init__(self, src, **kwds)\n    787         ParserBase.__init__(self, kwds)\n    788 \n--> 789         self._reader = _parser.TextReader(src, **kwds)\n    790 \n    791         # XXX\n\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3357)()\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4283)()\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:5731)()\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.raise_parser_error (pandas/src/parser.c:13774)()\n\nCParserError: Error tokenizing data. C error: no error message set"""
2292,8481713,jseabold,wesm,2012-11-19 20:25:46,2012-11-27 02:37:27,2012-11-27 02:37:27,closed,,0.10,5,Bug,https://api.github.com/repos/pydata/pandas/issues/2292,b'sum over objects',"b'Was surprised by this. I don\'t recall seeing this behavior before. I guess there are some object types that you do want to try to add together? Still seems like this could lead to problems\n\n```\n[~/statsmodels/statsmodels-skipper/statsmodels/sandbox/examples/]\n[20]: import statsmodels.api as sm\n\n[~/statsmodels/statsmodels-skipper/statsmodels/sandbox/examples/]\n[21]: aml = sm.datasets.get_rdataset(""leukemia"", ""survival"")                    \n\n[~/statsmodels/statsmodels-skipper/statsmodels/sandbox/examples/]\n\\[22\\]: aml.data.head(2)                                                          \n[22]: \n   time  status           x\n0     9       1  Maintained\n1    13       1  Maintained\n\n[~/statsmodels/statsmodels-skipper/statsmodels/sandbox/examples/]\n\\[23\\]: aml.data.sum()\n[23]: \ntime                                                    678\nstatus                                                   18\nx         MaintainedMaintainedMaintainedMaintainedMainta...\n```'"
2288,8459442,wesm,wesm,2012-11-19 04:56:16,2012-11-20 19:36:07,2012-11-20 19:36:07,closed,,0.10,0,Bug;Community;Testing,https://api.github.com/repos/pydata/pandas/issues/2288,b'Replace all usages of assert keyword/function with AssertionErrors',b'This is probably long overdue. Brought up by #2057. If you help with this please make the new error message informative!'
2281,8452787,wesm,wesm,2012-11-18 19:02:11,2012-11-23 05:43:27,2012-11-23 05:43:27,closed,,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2281,b'Assigning PeriodIndex to DataFrame columns should box period objects',b'http://stackoverflow.com/questions/13432213/pandas-time-period-data-type-prints-as-numbers'
2280,8452058,juliantaylor,wesm,2012-11-18 17:35:32,2012-11-19 20:30:49,2012-11-19 01:41:28,closed,,0.10,4,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/2280,b'test_transform_broadcast failure on ubuntu 13.04',"b""on ubuntu 13.04 test_transform_broadcast fails due to rounding issues.\nI'm not sure if this is a bug in the test or if some underlying component may have changed to cause rounding errors when there were non before.\na possible reason might be numpy 1.7 because the test works fine in ubuntu 12.10 with numpy 1.6\n\nsee also https://launchpadlibrarian.net/122195559/buildlog_ubuntu-raring-i386.pandas_0.8.1-1_FAILEDTOBUILD.txt.gz\nit affects git HEAD too.\n\n```\ntest_transform_broadcast (__main__.TestGroupBy) ... > /usr/lib/python2.7/unittest/case.py(425)assertTrue()\n-> raise self.failureException(msg)\n(Pdb) bt\n  /usr/lib/python2.7/unittest/case.py(332)run()\n-> testMethod()\n  /tmp/pandas/pandas/tests/test_groupby.py(404)test_transform_broadcast()\n-> self.assert_((res[col] == agged[col]).all())\n> /usr/lib/python2.7/unittest/case.py(425)assertTrue()\n-> raise self.failureException(msg)\n(Pdb) up\n> /tmp/pandas/pandas/tests/test_groupby.py(404)test_transform_broadcast()\n-> self.assert_((res[col] == agged[col]).all())\n\n(Pdb) p res[col][0]\n0.14593691347699592\n(Pdb) p agged[col]   \n0.14593691347699594\n```"""
2278,8437651,wesm,wesm,2012-11-17 02:32:14,2012-11-22 20:34:04,2012-11-22 20:34:04,closed,wesm,0.10,0,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/2278,b'Unstack is not careful about potential memory use problems',"b""Cartesian product problem; unstacks relative to hypothetical possibilities instead of observed combinations. I'm claiming this unless someone else wants to look inside the reshape code\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Generate Long File & Test Pivot\r\nNUM_ROWS = 1000000\r\n\r\ndf = pd.DataFrame({'A' : np.random.randint(100, size=NUM_ROWS), \r\n                                'B' : np.random.randint(300, size=NUM_ROWS), \r\n                                'C' : np.random.randint(-7, 7, size=NUM_ROWS), \r\n                                'D' : np.random.randint(-19,19, size=NUM_ROWS),\r\n                                'E' : np.random.randint(3000, size=NUM_ROWS),\r\n                                'F' : np.random.randn(NUM_ROWS)})\r\n\r\ndf_pivoted = df.pivot_table(rows=['A', 'B', 'C'], cols='E', values='F')\r\ndf_pivoted\r\n```"""
2277,8437124,damirv,wesm,2012-11-17 01:31:18,2012-11-23 05:53:42,2012-11-23 05:53:31,closed,,0.10,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2277,"b'Series.reset_index not working with ""inplace=True""'","b'When I call reset_index on a Series object with arguments inplace=True, it does not work. See the code example below. Am I missing something here? (I am using ""0.9.0"", checked with pandas.__version__)\n\n```python\nts_a = pandas.Series([1]*3+[2]*3+[3]*4, range(10))\nts_b = pandas.Series([2]*3+[3]*3+[1]*4, range(10))\nts_c = pandas.Series(numpy.random.randn(10), range(10))\ndf = pandas.DataFrame({""A"": ts_a, ""B"": ts_b, ""C"":ts_c})\ndf = df.set_index([\'A\', \'B\'])[\'C\'] # I select column C here to have a Series object\n# print df\n# A  B\n# 1  2    1.032\n#    2    0.139\n#    2   -0.708\n# 2  3   -1.268\n#    3    0.138\n#    3    0.497\n# 3  1    0.973\n#    1   -0.634\n#    1   -0.112\n#    1   -0.997\n\n# now let\'s reset index \'B\': inplace does not work?\ndf.reset_index(\'B\', inplace=True)\n# print df\n# A  B\n# 1  2    1.032\n#    2    0.139\n#    2   -0.708\n# 2  3   -1.268\n#    3    0.138\n#    3    0.497\n# 3  1    0.973\n#    1   -0.634\n#    1   -0.112\n#    1   -0.997\n\n# when copied, it does work\ndf = df.reset_index(\'B\')\n# print df\n# Name: C\n#    B      C\n# A          \n# 1  2  1.032\n# 1  2  0.139\n# 1  2 -0.708\n# 2  3 -1.268\n# 2  3  0.138\n# 2  3  0.497\n# 3  1  0.973\n# 3  1 -0.634\n# 3  1 -0.112\n# 3  1 -0.997\n```'"
2275,8433239,csakhil,wesm,2012-11-16 21:57:13,2013-04-19 14:51:49,2012-11-30 23:30:45,closed,,0.10,12,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2275,b'qtconsole + _repr_html_ bug',"b'Printing a `pandas.DataFrame` in IPython\'s qtconsole tries to use `_repr_html_`.  This method returns `None` if the terminal window that started the IPython session is too small.  \n\nI\'m not sure this behaviour is right.  When you\'re in the qtconsole, `_repr_html_` should check the size of the qtconsole window and not the the size of the terminal that started the session.\n\nHere are some steps to reproduce the issue:\n\n1.  Create a very small terminal window and run `ipython qtconsole`.\n2.  Create a `pandas.DataFrame` called `df`.  \n3.  Run `df.head()` in IPython.  You should notice that the HTML representation of the table is not used. \n4.  Maximize the small terminal window from step 1.\n5.  Run `df.head()` in IPython.  The HTML representation is now used.\n\nI would guess the easiest way to fix this would be to add a qtconsole ""os"" in `get_terminal_size` in `util/terminal.py` and use Qt hooks to figure out the dimensions of the qtconsole.'"
2273,8429456,bluefir,wesm,2012-11-16 19:32:08,2012-11-17 02:53:26,2012-11-17 02:53:26,closed,,0.10,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2273,b'Significant performance degradation in 0.9.1 for SparseDataFrame methods like to_dense() and save() and for arithmetic operations',"b""This is what I have in version 0.9.0:\n\n    import pandas as pd\n    pd.__version__\n>'0.9.0'\n\n    barra_industry_exposures\n>\\<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 253738 entries, (20061229, '00036110') to (20120928, 'Y8564W10')\nData columns:\nMINING_METALS                  253738  non-null values\nGOLD                           253738  non-null values\nFORESTRY_PAPER                 253738  non-null values\nCHEMICAL                       253738  non-null values\nENERGY_RESERVES                253738  non-null values\nOIL_REFINING                   253738  non-null values\nOIL_SERVICES                   253738  non-null values\nFOOD_BEVERAGES                 253738  non-null values\nALCOHOL                        253738  non-null values\nTOBACCO                        253738  non-null values\nHOME_PRODUCTS                  253738  non-null values\nGROCERY_STORES                 253738  non-null values\nCONSUMER_DURABLES              253738  non-null values\nMOTOR_VEHICLES                 253738  non-null values\nAPPAREL_TEXTILES               253738  non-null values\nCLOTHING_STORES                253738  non-null values\nSPECIALTY_RETAIL               253738  non-null values\nDEPARTMENT_STORES              253738  non-null values\nCONSTRUCTION                   253738  non-null values\nPUBLISHING                     253738  non-null values\nMEDIA                          253738  non-null values\nHOTELS                         253738  non-null values\nRESTAURANTS                    253738  non-null values\nENTERTAINMENT                  253738  non-null values\nLEISURE                        253738  non-null values\nENVIRONMENTAL_SERVICES         253738  non-null values\nHEAVY_ELECTRICAL_EQUIPMENT     253738  non-null values\nHEAVY_MACHINERY                253738  non-null values\nINDUSTRIAL_PARTS               253738  non-null values\nELECTRICAL_UTILITY             253738  non-null values\nGAS_WATER_UTILITY              253738  non-null values\nRAILROADS                      253738  non-null values\nAIRLINES                       253738  non-null values\nFREIGHT                        253738  non-null values\nMEDICAL_SERVICES               253738  non-null values\nMEDICAL_PRODUCTS               253738  non-null values\nDRUGS                          253738  non-null values\nELECTRONIC_EQUIPMENT           253738  non-null values\nSEMICONDUCTORS                 253738  non-null values\nCOMPUTER_HARDWARE              253738  non-null values\nCOMPUTER_SOFTWARE              253738  non-null values\nDEFENCE_AEROSPACE              253738  non-null values\nTELEPHONE                      253738  non-null values\nWIRELESS                       253738  non-null values\nINFORMATION_SERVICES           253738  non-null values\nINDUSTRIAL_SERVICES            253738  non-null values\nLIFE_HEALTH_INSURANCE          253738  non-null values\nPROPERTY_CASUALTY_INSURANCE    253738  non-null values\nBANKS                          253738  non-null values\nTHRIFTS                        253738  non-null values\nASSET_MANAGEMENT               253738  non-null values\nFINANCIAL_SERVICES             253738  non-null values\nINTERNET                       253738  non-null values\nREITS                          253738  non-null values\nBIOTECH                        253738  non-null values\ndtypes: int64(55)\n\n    sparse = barra_industry_exposures.to_sparse(fill_value=0)\n    sparse\n>\\<class 'pandas.sparse.frame.SparseDataFrame'>\nMultiIndex: 253738 entries, (20061229, '00036110') to (20120928, 'Y8564W10')\nColumns: 55 entries, AIRLINES to WIRELESS\ndtypes: float64(55)\n\n    %timeit sparse / 100.\n>100 loops, best of 3: 6.64 ms per loop\n\n    %timeit sparse.to_dense()\n>10 loops, best of 3: 127 ms per loop\n\n    %timeit sparse.save('test.pkl')\n>1 loops, best of 3: 16.9 ms per loop\n\n\nNow this is what I get in 0.9.1:\n\n    import pandas as pd\n    pd.__version__\n>'0.9.1'\n\n    %timeit sparse / 100.\n>1 loops, best of 3: 92.2 s per loop\n\n    %timeit sparse.to_dense()\n>1 loops, best of 3: 99.8 s per loop\n\n    %timeit sparse.save('test.pkl')\n>1 loops, best of 3: 100 s per loop\n\nSo, in the new version SparseDataFrame methods that used to run in less than 7-130 ms now run in more than 90 s. Ouch! What happened?"""
2272,8428085,wesm,wesm,2012-11-16 18:41:09,2012-11-18 22:04:18,2012-11-18 22:04:18,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2272,b'datetime64 ns representation error in Series repr',b'Will post example'
2270,8425490,wesm,wesm,2012-11-16 17:04:34,2012-11-18 18:53:34,2012-11-18 18:53:34,closed,,0.10,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2270,b'usecols conflicts with leading columns handling in read_csv',"b'E.g. 3-column file, `usecols=[1, 2]`'"
2269,8425419,wesm,wesm,2012-11-16 17:02:12,2012-11-27 02:38:42,2012-11-27 02:38:42,closed,,0.10,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2269,"b'In read_csv, header=0 and passed names should discard first row'",b'Currently does not'
2266,8402394,etyurin,wesm,2012-11-15 21:23:40,2012-11-17 02:57:35,2012-11-17 02:57:28,closed,changhiskhan,0.10,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2266,b'diff() behavior causes memory problems',"b'diff(n) with a negative n used to be a recipe for obtaining a forward-looking diff. Now it causes memory problems. \n\nThe change was introduced in commit 06f74e5d06c4766ec59564374640c4758a72409d and ""improved"" in commit 0d6fb1b381ef3882ceb766fecb0e28a5d5c2bd2bw . The first commit produces unexpected results silently. The second commit causes an outright coredump with \n\n*** glibc detected *** /usr/bin/python: double free or corruption (out): 0x000000000591db00 ***\n\nThis is a simple script to trigger this bug:\n\nimport pandas\nimport numpy as np\nx = np.sin(np.arange(24))\nx = x.reshape((4,6))\ndf = pandas.DataFrame(x,columns=[\'a\',\'b\',\'c\',\'d\',\'e\',\'f\'])\nprint df\ndf2 = -df.diff(-1)\nprint df2\n\nThe results used to be mathematically correct:\n\n          a         b         c         d         e         f\n0  0.000000  0.841471  0.909297  0.141120 -0.756802 -0.958924\n1 -0.279415  0.656987  0.989358  0.412118 -0.544021 -0.999990\n2 -0.536573  0.420167  0.990607  0.650288 -0.287903 -0.961397\n3 -0.750987  0.149877  0.912945  0.836656 -0.008851 -0.846220\n          a         b         c         d         e         f\n0 -0.279415 -0.184484  0.080061  0.270998  0.212781 -0.041066\n1 -0.257157 -0.236820  0.001249  0.238169  0.256118  0.038593\n2 -0.214414 -0.270290 -0.077662  0.186368  0.279052  0.115177\n3       NaN       NaN       NaN       NaN       NaN       NaN\n\nNow they are not even consistent column-by-column:\n\n          a         b         c         d         e         f\n0  0.000000  0.841471  0.909297  0.141120 -0.756802 -0.958924\n1 -0.279415  0.656987  0.989358  0.412118 -0.544021 -0.999990\n2 -0.536573  0.420167  0.990607  0.650288 -0.287903 -0.961397\n3 -0.750987  0.149877  0.912945  0.836656 -0.008851 -0.846220\n          a         b         c         d         e         f\n0       NaN       NaN       NaN       NaN       NaN       NaN\n1       NaN       NaN       NaN       NaN       NaN       NaN\n2       NaN       NaN       NaN       NaN       NaN       NaN\n3  0.750987  0.691594 -0.003648 -0.695536 -0.747951 -0.112704\n\n'"
2262,8398112,jim22k,wesm,2012-11-15 19:11:09,2012-12-10 15:38:43,2012-12-10 15:38:43,closed,,0.10,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2262,b'DataFrame.reset_index() loses timezone',"b""When timezone-aware datetime.datetime objects are used as the index of a DataFrame via set_index(), undoing the operation with reset_index() loses the timezone.  \n\nAfter the round-trip, the datetime.datetime column becomes a datetime64[ns] column, which can be considered a feature, not a bug.  The times are correctly converted to UTC (which is good if you're going to lose the timezone), but the loss of timezone information seems like a bug.\n\nThis has been tested against version 0.9.1 on Python 2.7 (Windows 32-bit)\n\nCode to recreate\n----------------------\n    import datetime, dateutil, pandas as pd\n    tzoffset = dateutil.tz.tzoffset(None, -5*60*60)\n    ts = datetime.datetime(2012,11,15,10,38,4,tzinfo=tzoffset)\n    print ts # this has a timezone\n    df = pd.DataFrame([[ts, 15]], columns=['ts', 'val'])\n    dfi = df.set_index('ts')\n    print dfi.index # this has a timezone\n    df2 = dfi.reset_index() # <-- This is where the timezone gets lost\n    print df2['ts'][0] # this doesn't have a timezone\n    print df2['ts'][0].to_datetime() # neither does this\n"""
2260,8396373,dirkdevriendt,wesm,2012-11-15 18:34:08,2012-12-10 15:33:03,2012-12-10 15:33:03,closed,,0.10,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2260,b'Series append function drops timezone information',"b""Except when one of the two series is empty; in that case a tz-aware Series is returned\n\n\nCode to reproduce:\n\nimport numpy as np\nimport pandas as pd\nrng1 = pd.date_range('1/1/2011 01:00', periods=1, freq='H')\nrng2 = pd.date_range('1/1/2011 02:00', periods=1, freq='H')\nts1 = pd.Series(np.random.randn(len(rng1)), index=rng1).tz_localize('UTC')\nts2 = pd.Series(np.random.randn(len(rng2)), index=rng2).tz_localize('UTC')\nts_result = ts1.append(ts2)\n\n\nResult in IPython:\n\nIn [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: rng1 = pd.date_range('1/1/2011 01:00', periods=1, freq='H')\n\nIn [4]: rng2 = pd.date_range('1/1/2011 02:00', periods=1, freq='H')\n\nIn [5]: ts1 = pd.Series(np.random.randn(len(rng1)), index=rng1).tz_localize('UTC')\n\nIn [6]: ts1\nOut[6]: \n2011-01-01 01:00:00+00:00    1.870471\nFreq: H\n\nIn [7]: ts2 = pd.Series(np.random.randn(len(rng2)), index=rng2).tz_localize('UTC')\n\nIn [8]: ts2\nOut[8]: \n2011-01-01 02:00:00+00:00    1.266536\nFreq: H\n\nIn [9]: ts_result = ts1.append(ts2)\n\nIn [10]: ts_result\nOut[10]: \n2011-01-01 01:00:00    1.870471\n2011-01-01 02:00:00    1.266536"""
2259,8395131,y-p,wesm,2012-11-15 18:02:29,2012-12-14 02:41:34,2012-12-14 02:41:34,closed,,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2259,"b'icol([0]) fails with non-unique, integer columns'","b'```python\r\nIn [14]: df=pd.DataFrame([[1,2,3],[4,5,6]],columns=[1,1,2])\r\n\r\nIn [15]: df.icol([0])\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-15-bb76389fbd62> in <module>()\r\n----> 1 df.icol([0])\r\n\r\n/home/user1/src/pandas/pandas/core/frame.pyc in icol(self, i)\r\n   1877             return self[name]\r\n   1878         raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\r\n-> 1879                              (type(self).__name__, name))\r\n   1880 \r\n   1881     def __setattr__(self, name, value):\r\n\r\n/home/user1/src/pandas/pandas/core/frame.pyc in reindex(self, index, columns, method, level, fill_value, limit, copy)\r\n   2503             necessary. Setting to False will improve the performance of this\r\n   2504             method\r\n-> 2505 \r\n   2506         Examples\r\n   2507         --------\r\n\r\n/home/user1/src/pandas/pandas/core/frame.pyc in _reindex_columns(self, new_columns, copy, level, fill_value, limit)\r\n   2590             levels are named. If None then the index name is repeated.\r\n   2591 \r\n-> 2592         Returns\r\n   2593         -------\r\n   2594         resetted : DataFrame\r\n\r\n/home/user1/src/pandas/pandas/core/index.pyc in reindex(self, target, method, level, limit)\r\n    870         if self._join_precedence < other._join_precedence:\r\n    871             how = {\'right\': \'left\', \'left\': \'right\'}.get(how, how)\r\n--> 872             result = other.join(self, how=how, level=level,\r\n    873                                 return_indexers=return_indexers)\r\n    874             if return_indexers:\r\n\r\n/home/user1/src/pandas/pandas/core/index.pyc in get_indexer(self, target, method, limit)\r\n    789         is_contained : ndarray (boolean dtype)\r\n    790         """"""\r\n--> 791         value_set = set(values)\r\n    792         return lib.ismember(self._array_values(), value_set)\r\n    793 \r\n\r\nException: Reindexing only valid with uniquely valued Index objects\r\n```\r\n'"
2257,8388541,jreback,wesm,2012-11-15 14:40:30,2012-11-19 05:14:07,2012-11-19 05:14:07,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2257,b'Regression issue with concat of panels in 0.9.1',"b'concat of panels with the same items and minor_axis (but different major_axis) on axis=2 worked in 0.8.1, but fails in 0.9.1\n\n```\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: pandas.__version__\nOut[3]: \'0.9.1\'\n\nIn [4]: def make_panel():\n   ...:         index = 5\n   ...:         cols  = 3\n   ...:         return pandas.Panel(dict([ (""Item%s"" % x,pandas.DataFrame(np.random.randn(index,cols),index = [ ""I%s"" % i for i in range(index) ], columns = [ ""C%s"" % i for i in range(cols) ])) for x in [\'A\',\'B\',\'C\'] ]))\n   ...: \n\nIn [5]: panel1 = make_panel()\n\nIn [6]: panel1\nOut[6]: \n<class \'pandas.core.panel.Panel\'>\nDimensions: 3 (items) x 5 (major) x 3 (minor)\nItems: ItemA to ItemC\nMajor axis: I0 to I4\nMinor axis: C0 to C2\n\nIn [7]: panel2 = make_panel()\n\nIn [8]: panel2 = panel2.rename_axis(dict([ (x,""%s_1"" % x) for x in panel2.major_axis ]), axis=1)\n\nIn [9]: panel2\nOut[9]: \n<class \'pandas.core.panel.Panel\'>\nDimensions: 3 (items) x 5 (major) x 3 (minor)\nItems: ItemA to ItemC\nMajor axis: I0_1 to I4_1\nMinor axis: C0 to C2\n\nIn [10]: panel3 = make_panel()\n\nIn [11]: panel3 = panel2.rename_axis(dict([ (x,""%s_1"" % x) for x in panel2.major_axis ]), axis=1).rename_axis(dict([ (x,""%s_1"" % x) for x in panel2.minor_axis ]), axis=2)\n\nIn [12]: panel3\nOut[12]: \n<class \'pandas.core.panel.Panel\'>\nDimensions: 3 (items) x 5 (major) x 3 (minor)\nItems: ItemA to ItemC\nMajor axis: I0_1_1 to I4_1_1\nMinor axis: C0_1 to C2_1\n```\nthis works in 0.8.1 & 0.9.1\n\n```\nIn [14]: pandas.concat([ panel1, panel2 ], axis = 1, verify_integrity = True)\nOut[14]: \n<class \'pandas.core.panel.Panel\'>\nDimensions: 3 (items) x 10 (major) x 3 (minor)\nItems: ItemA to ItemC\nMajor axis: I0 to I4_1\nMinor axis: C0 to C2\n```\n\nfails under 0.9.1 (worked in 0.8.1)\n```\nIn [16]: pandas.concat([ panel1, panel3 ], axis = 1, verify_integrity = True)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-16-72a0267c3f61> in <module>()\n----> 1 pandas.concat([ panel1, panel3 ], axis = 1, verify_integrity = True)\n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity)\n    877                        keys=keys, levels=levels, names=names,\n    878                        verify_integrity=verify_integrity)\n--> 879     return op.get_result()\n    880 \n    881 \n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in get_result(self)\n    959         else:\n    960             new_data = self._get_concatenated_data()\n--> 961             return self.objs[0]._from_axes(new_data, self.new_axes)\n    962 \n    963     def _get_fresh_axis(self):\n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/core/panel.pyc in _from_axes(cls, data, axes)\n    241             items, major, minor = axes\n    242             return cls(data, items=items, major_axis=major,\n--> 243                        minor_axis=minor, copy=False)\n    244 \n    245     def _init_dict(self, data, axes, dtype=None):\n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/core/panel.pyc in __init__(self, data, items, major_axis, minor_axis, copy, dtype)\n    221             mgr = data\n    222         elif isinstance(data, dict):\n--> 223             mgr = self._init_dict(data, passed_axes, dtype=dtype)\n    224             copy = False\n    225             dtype = None\n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/core/panel.pyc in _init_dict(self, data, axes, dtype)\n    280             arrays.append(values)\n    281 \n--> 282         return self._init_arrays(arrays, items,  axes)\n    283 \n    284     def _init_arrays(self, arrays, arr_names, axes):\n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/core/panel.pyc in _init_arrays(self, arrays, arr_names, axes)\n    285         # segregates dtypes and forms blocks matching to columns\n    286         blocks = form_blocks(arrays, arr_names, axes)\n--> 287         mgr = BlockManager(blocks, axes).consolidate()\n    288         return mgr\n    289 \n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    484 \n    485         if do_integrity_check:\n--> 486             self._verify_integrity()\n    487 \n    488     @classmethod\n\n/usr/local/lib/python2.7/site-packages/pandas-0.9.1-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in _verify_integrity(self)\n    566         for block in self.blocks:\n    567             assert(block.ref_items is self.items)\n--> 568             assert(block.values.shape[1:] == mgr_shape[1:])\n    569         tot_items = sum(len(x.items) for x in self.blocks)\n    570         assert(len(self.items) == tot_items)\n\nAssertionError: \n```'"
2252,8372735,bball7210,changhiskhan,2012-11-14 23:24:59,2012-11-20 15:28:41,2012-11-20 15:28:41,closed,,0.10,7,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2252,b'Losing Nanosecond precision upon conversion to DatetimeIndex',"b'All,\r\n\r\nI am experiencing a problem when converting from a TimeSeries to a DatetimeIndex. In the code below, I create a TimeSeries of nanosecond precision TimeStamps. I then convert it to a DatetimeIndex and lose the nanosecond precision (but maintain the microseconds). This appears to be a bug as the resulting elements of DatetimeIndex are of time TimeStamp and can handle nanosecond precision. Please see the snippet below.\r\n\r\nAny help/resolution would be greatly appreciated.\r\n\r\nThanks,\r\nJoe\r\n\r\n```\r\nIn [233]: t1 = pandas.Timestamp((1352934390*1000000000)+1000000+1000+1)\r\n\r\nIn [234]: t2 = pandas.Timestamp((1352934390*1000000000)+1000000+1000+5)\r\n\r\nIn [235]: timeseries = pandas.TimeSeries([t1,t2])\r\n\r\nIn [236]: timeseries[0]\r\nOut[236]: <Timestamp: 2012-11-14 23:06:30.001001001>\r\n\r\nIn [237]: datetimeindex = pandas.DatetimeIndex(timeseries)\r\n\r\nIn [238]: datetimeindex[0]\r\nOut[238]: <Timestamp: 2012-11-14 23:06:30.001001>\r\n\r\nIn [236]: timeseries[0]\r\nOut[236]: <Timestamp: 2012-11-14 23:06:30.001001001>\r\n```'"
2251,8367855,y-p,wesm,2012-11-14 20:48:59,2012-11-15 00:35:25,2012-11-15 00:35:25,closed,,0.9.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2251,"b""BUG: icol() doesn't preserve index type on sparse DataFrames""","b""```python\r\nfrom pandas.sparse.api import SparseSeries,SparseDataFrame\r\nfrom pandas import Series, DataFrame, bdate_range, Panel\r\ndata = {'A' : [0,1 ]}\r\niframe = SparseDataFrame(data, default_kind='integer')\r\nprint type(iframe['A'].sp_index)\r\nprint type(iframe.icol(0).sp_index)\r\n\r\n<type 'pandas._sparse.IntIndex'>\r\n<type 'pandas._sparse.BlockIndex'>\r\n```\r\n"""
2249,8367748,y-p,wesm,2012-11-14 20:45:31,2012-11-15 00:37:59,2012-11-15 00:37:59,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2249,b'icol() should propegate fill_value for sparse data frames',"b""```python\r\nfrom pandas.sparse.api import SparseSeries,SparseDataFrame\r\nfrom pandas import Series, DataFrame, bdate_range, Panel\r\ndata = {'A' : [0,1 ]}\r\niframe = SparseDataFrame(data, default_kind='integer')\r\nprint type(iframe['A'].sp_index)\r\nprint type(iframe.icol(0).sp_index)\r\n```"""
2248,8362209,craustin,wesm,2012-11-14 18:01:54,2012-11-14 20:22:25,2012-11-14 20:22:25,closed,wesm,0.9.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2248,b'Exception during tz_localize w/ empty Series',"b'from pandas import Series\r\ns = Series()\r\ns.tz_localize(\'UTC\')\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\core\\series.pyc in tz_localize(self, tz, copy)\r\n   2730         localized : TimeSeries\r\n   2731         """"""\r\n-> 2732         new_index = self.index.tz_localize(tz)\r\n   2733\r\n   2734         new_values = self.values\r\n\r\nAttributeError: \'Index\' object has no attribute \'tz_localize\''"
2245,8356255,craustin,wesm,2012-11-14 15:03:51,2014-10-22 10:46:09,2012-11-14 16:34:16,closed,wesm,0.9.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2245,b'Exception during resample w/ tz',"b""from pandas import date_range, Series\r\nidx = date_range('2001-09-20 15:59','2001-09-20 16:00', freq='T', tz='Australia/Sydney')\r\ns = Series([1,2], index=idx)\r\ns.resample('D')\r\n\r\nIn [4]: s.resample('D')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-5ce76d689c21> in <module>()\r\n----> 1 s.resample('D')\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\core\\generic.pyc in resample(self, rule, how, axis, fill_method, closed,\r\nlabel, convention, kind, loffset, limit, base)\r\n    187                               fill_method=fill_method, convention=convention,\r\n    188                               limit=limit, base=base)\r\n--> 189         return sampler.resample(self)\r\n    190\r\n    191     def first(self, offset):\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in resample(self, obj)\r\n     66\r\n     67         if isinstance(axis, DatetimeIndex):\r\n---> 68             rs = self._resample_timestamps(obj)\r\n     69         elif isinstance(axis, PeriodIndex):\r\n     70             offset = to_offset(self.freq)\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in _resample_timestamps(self, obj)\r\n    180         axlabels = obj._get_axis(self.axis)\r\n    181\r\n--> 182         binner, grouper = self._get_time_grouper(obj)\r\n    183\r\n    184         # Determine if we're downsampling\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in _get_time_grouper(self, obj)\r\n     95\r\n     96         if self.kind is None or self.kind == 'timestamp':\r\n---> 97             binner, bins, binlabels = self._get_time_bins(axis)\r\n     98         else:\r\n     99             binner, bins, binlabels = self._get_time_period_bins(axis)\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in _get_time_bins(self, axis)\r\n    125\r\n    126         # general version, knowing nothing about relative frequencies\r\n--> 127         bins = lib.generate_bins_dt64(ax_values, bin_edges, self.closed)\r\n    128\r\n    129         if self.closed == 'right':\r\n\r\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\lib.pyd in pandas.lib.generate_bins_dt64 (pandas\\src\\tseries.c:61444)()\r\n\r\nValueError: Values falls after last bin"""
2243,8345424,changhiskhan,wesm,2012-11-14 05:16:56,2012-11-23 05:43:17,2012-11-23 05:43:17,closed,,0.10,0,Bug;Indexing;Reshaping;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2243,b'reset_index on DataFrame with PeriodIndex changes to ordinal representation',b'Should be array of Period objects probably\r\n\r\nNote this is due to PeriodIndex.values converting to array of ordinals'
2240,8340063,simonm3,wesm,2012-11-13 23:30:13,2013-12-04 00:57:52,2012-11-14 16:55:47,closed,changhiskhan,0.9.1,1,Bug;Data IO;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2240,"b""cannot write resampled frame to excel TypeError: 'Period' object does not support indexing""","b'Excel_to function is not working with some dataframes with period index\r\n\r\n\\# this works for a dataframe with timeseries index\r\nclose.to_excel(""temp.xlsx"")\r\n\r\n\\# this does not work. fails with error shown above\r\nmonthly=close.resample(\'M\', kind=\'period\')\r\nmonthly.to_excel(""temp.xlsx"")'"
2237,8306210,khughitt,wesm,2012-11-12 22:43:41,2012-11-14 19:36:35,2012-11-14 19:36:35,closed,changhiskhan,0.9.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2237,b'parallel_coordinates incorrect auto legend',"b'Just tried the latest version of the parallel coordinates plot, but the default legend appears to be incorrect: instead of just choosing the four expected groups, one legend entry is included per row, many of them set to ""None"".\r\n\r\nTo reproduce, follow steps in the [pandas guide]( http://pandas.pydata.org/pandas-docs/stable/visualization.html#parallel-coordinates).\r\n\r\nSystem info:\r\n----------------\r\n Arch Linux 64-bit\r\n\r\n pandas 0.9.1rc1 (git)\r\n numpy 1.7.0b2\r\n matplotlib 1.2.0\r\n\r\nScreenshot\r\n---------------\r\n![pandas screenshot](http://i49.tinypic.com/33u7pjt.png)'"
2236,8302533,jpindi,wesm,2012-11-12 20:34:27,2014-06-13 02:57:06,2012-11-13 21:54:20,closed,,0.9.1,5,Bug,https://api.github.com/repos/pydata/pandas/issues/2236,b'Error converting DataFrame with duplicate columns to ndarray',"b'Installed latest version of pandas 0.9.0 in case this was an error\r\nTrying to read Excel file. That part seems ok.\r\nOriginally, I was trying iteritems() for each row of the pandas dataframe, as the id_company had to be verified against a mysql database (code not included). Same/similar error message to putting it into a tuple (code is below). Error message follows.\r\n\r\nNote there is a .reindex() but it didn\'t work before, either. The reindex() was kind of a hail-mary.\r\n\r\nAs a work-around, I\'m probably going to simply import from my target sql and do a join. I\'m concerned because of the size of the datasets.\r\n\r\n```\r\nimport pandas as pd\r\ndef runNow():\r\n    #identify sheet\r\n    source = \'C:\\Users\\jlalonde\\Desktop\\startup_geno\\startupgenome_w_id_xl_20121109.xlsx\'\r\n    xls_file = pd.ExcelFile(source)\r\n    sd = xls_file.parse(\'Sheet1\')\r\n    source_u = sd.drop_duplicates(cols = \'id_company\', take_last=False)\r\n    source_r = source_u[[\'id_company\',\'id_good\',\'description\', \'website\',\'keyword\', \'company_name\',\'founded_month\', \'founded_year\', \'description\']]\r\n    source_i = source_r.reindex() #hail mary\r\n    tup_r = [tuple(x) for x in source_i.values]\r\n\r\nTraceback (most recent call last):\r\n  File ""<pyshell#10>"", line 1, in <module>\r\n    sg_sql_2.runNow()\r\n  File ""sg_sql_2.py"", line 31, in runNow\r\n    tup_r = [tuple(x) for x in source_r.values]\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 1443, in as_matrix\r\n    return self._data.as_matrix(columns).T\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 723, in as_matrix\r\n    mat = self._interleave(self.items)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 743, in _interleave\r\n    indexer = items.get_indexer(block.items)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\index.py"", line 748, in get_indexer\r\n    raise Exception(\'Reindexing only valid with uniquely valued Index \'\r\nException: Reindexing only valid with uniquely valued Index objects\r\n```'"
2234,8289643,grsr,wesm,2012-11-12 13:56:11,2012-11-14 17:44:28,2012-11-14 17:44:28,closed,,0.9.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2234,"b""Can't create a DataFrame from an empty Series""","b'For a project I am working on it would be convenient if you could create a DataFrame from an empty Series. I have some library code that will create a Series object constructed from a dict, most of the time this dict will have some entries, but occasionally it does not. Currently you can create an empty DataFrame by not supplying anything to the constructor, and you can create a DataFrame or a Series from an empty dict, but if you try to create a DataFrame from an Series constructed from an empty dict pandas throws an AssertionError. I can code around this easily enough, but it would be preferable just to return an empty DataFrame. Example code below:\r\n\r\n```python\r\nIn [73]: pandas.__version__\r\nOut[73]: \'0.9.0\'\r\n\r\nIn [74]: df = DataFrame()\r\n\r\nIn [75]: df = DataFrame({})\r\n\r\nIn [76]: s = Series({}, name=""foo"")\r\n\r\nIn [77]: df = DataFrame(s)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-77-4c4337b321e1> in <module>()\r\n----> 1 df = DataFrame(s)\r\n\r\n/nfs/users/nfs_g/gr5/software/epd-7.2-2-rh5-x86_64/lib/python2.7/site-packages/pandas-0.9.0-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in __init__(self, data, index, columns, dtype, copy)\r\n    392             else:\r\n    393                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n--> 394                                          copy=copy)\r\n    395         elif isinstance(data, list):\r\n    396             if len(data) > 0:\r\n\r\n/nfs/users/nfs_g/gr5/software/epd-7.2-2-rh5-x86_64/lib/python2.7/site-packages/pandas-0.9.0-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in _init_ndarray(self, values, index, columns, dtype, copy)\r\n    504             columns = _ensure_index(columns)\r\n    505 \r\n--> 506         block = make_block(values.T, columns, columns)\r\n    507         return BlockManager([block], [columns, index])\r\n    508 \r\n\r\n/nfs/users/nfs_g/gr5/software/epd-7.2-2-rh5-x86_64/lib/python2.7/site-packages/pandas-0.9.0-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in make_block(values, items, ref_items, do_integrity_check)\r\n    459 \r\n    460     return klass(values, items, ref_items, ndim=values.ndim,\r\n--> 461                  do_integrity_check=do_integrity_check)\r\n    462 \r\n    463 # TODO: flexible with index=None and/or items=None\r\n\r\n/nfs/users/nfs_g/gr5/software/epd-7.2-2-rh5-x86_64/lib/python2.7/site-packages/pandas-0.9.0-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in __init__(self, values, items, ref_items, ndim, do_integrity_check)\r\n     24 \r\n     25         assert(values.ndim == ndim)\r\n---> 26         assert(len(items) == len(values))\r\n     27 \r\n     28         self.values = values\r\n\r\nAssertionError: \r\n```\r\n\r\nCheers,\r\n\r\nGraham'"
2229,8275247,y-p,wesm,2012-11-11 20:09:20,2012-11-14 16:50:46,2012-11-14 16:50:46,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2229,b'SparseSeries.from_array returns Series rather then SparseSeries',"b'```python\r\nIn [16]: s=SparseSeries.from_array(np.array([1,2,3]),index=Index(range(3)))\r\n    ...: type(s)\r\n    ...: \r\nOut[16]: pandas.core.series.Series\r\n```'"
2228,8275085,y-p,wesm,2012-11-11 19:52:49,2012-11-14 00:10:35,2012-11-14 00:10:35,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2228,b'df.icol([0]) fails when df has non-unique column labels',"b""```python\r\ndf=DataFrame([[1,2,3],[4,5,6]],columns=['a','b','b'])\r\ndf.icol([0])\r\n\r\npandas/pandas/core/index.pyc in get_indexer(self, target, method, limit)\r\n    747 \r\n    748         if not self.is_unique:\r\n--> 749             raise Exception('Reindexing only valid with uniquely valued Index '\r\n    750                             'objects')\r\n    751 \r\n\r\nException: Reindexing only valid with uniquely valued Index objects\r\n```"""
2227,8275001,y-p,wesm,2012-11-11 19:45:04,2012-11-14 16:50:29,2012-11-14 16:50:29,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2227,b'SparseDataFrame.icol(j) returns Series rather then SparseSeries',"b""```python\r\nIn [38]: df=SparseDataFrame([[1,2],[4,5]],columns=['a','b'])\r\n    ...: type(df.icol(0))\r\nOut[38]: pandas.core.series.Series\r\n```"""
2225,8274267,y-p,wesm,2012-11-11 18:33:40,2014-04-05 19:32:40,2012-11-27 15:33:55,closed,,0.10,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2225,b'BUG: series tidy_repr UnicodeDecodeError',"b'```python\r\na=pd.Series([u""\\u05d0""]*1000)\r\na.name= \'title1\'\r\nrepr(a)\r\n\r\npandas/pandas/core/series.py in _tidy_repr(self, max_vals)\r\n    869                                                   name=False)\r\n    870         result = head + \'\\n...\\n\' + tail\r\n--> 871         return \'%s\\n%s\' % (result, self._repr_footer())\r\n    872 \r\n    873     def _repr_footer(self):\r\n\r\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xd7 in position 6: ordinal not in range(128)\r\n```\r\n'"
2220,8271155,y-p,wesm,2012-11-11 12:26:32,2012-11-14 07:51:36,2012-11-13 23:51:13,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2220,b'1 ** Sparse df with nans makes sp_values() produce strange results under df[] vs df.take([])',"b""```python\r\ndf = SparseDataFrame({    'A' : [nan, 0, 1]    }) # note : no error without nan\r\ndf = 1 ** df # note that 2 ** df works fine, also df ** 1\r\nr1=df.take([0],1)['A']\r\nr2=df['A']\r\nassert len(r2.sp_values) == len(r1.sp_values)\r\n```\r\n\r\n- I'm not familiar with the block management code so I can't figure this out\r\nwith reasonable effort right now.\r\n\r\n- as a side issue, it looks like 1 ** nan == 1 in this context, seems suspect.\r\n"""
2219,8271120,y-p,wesm,2012-11-11 12:21:29,2012-11-13 23:26:18,2012-11-13 23:26:18,closed,,0.9.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2219,b' df.iteritems() should yield Series even with non-unique column labels',"b""```python\r\ndf=DataFrame([[1,2,3],[4,5,6]],columns=['a','a','b'])\r\nfor k,v in df.iteritems():\r\n    self.assertEqual(type(v),Series)\r\n```"""
2218,8271102,y-p,wesm,2012-11-11 12:18:35,2012-11-13 23:27:13,2012-11-13 23:27:13,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2218,b'df with dupe cols should raise KeyError on accessing non-existent col via list',"b'```python\r\nfrom pandas import DataFrame\r\ndf=DataFrame([[1,2,3],[4,5,6]],columns=[\'a\',\'a\',\'b\'])\r\ntry:\r\n    df[[\'baf\']]\r\nexcept KeyError:\r\n    pass\r\nelse:\r\n    self.fail(""Dataframe failed to raise KeyError"")\r\n```\r\n'"
2207,8249302,TomAugspurger,TomAugspurger,2012-11-09 19:24:44,2013-01-20 19:51:45,2013-01-20 19:51:45,closed,,0.11,5,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2207,b'MultiIndex with Heterogenous dtype',"b'I\'m not sure if this is supposed to work or not.\r\n\r\nI\'m reading in a csv file:\r\n\r\n`df = pd.read_csv(\'nc201052.dat\', index_col=[\'FLOW\', \'PERIOD\', \'DECLARANT\', \'PARTNER\', \'PRODUCT_NC\'], nrows=1000)`\r\n\r\n```\r\ndf.head(5)\r\n\r\n                                          STAT_REGIME  VALUE_1000ECU  QUANTITY_TON  SUP_QUANTITY\r\nFLOW PERIOD DECLARANT PARTNER PRODUCT_NC                                                        \r\n1    201052 001       3       01                    4       41818.21       13419.4             0\r\n                      4       01                    4       17667.97        3939.6             0\r\n                      5       01                    4        6956.63        1181.9             0\r\n                      6       01                    4       44011.98        1031.2             0\r\n                      7       01                    4        7141.68         559.0             0\r\n\r\n```\r\n\r\nThe \'DECLARANT\'  columns index type is \'object\' (they coded one item as \'EU\'). \r\n\r\n`df.index[0]` gives `(1, 201052, \'001\', 3, \'01\')`, but `df.ix[1, 201052, \'001\']` raises [this](https://gist.github.com/b735fad192ce878dc353) error.\r\n\r\nWhen I only import the first 10 rows, the parser infers the ""DECLARANT"" column as an integer, and the slice works.  That\'s what is leading me to guess that it\'s the type of the index that\'s messing things up.  \r\n\r\nSorry if this isn\'t actually a bug.  Still very new to python.  Thanks!'"
2205,8244722,changhiskhan,changhiskhan,2012-11-09 16:35:34,2012-11-12 04:39:39,2012-11-12 04:39:39,closed,,0.9.1,0,Bug;Enhancement;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/2205,b'Add option to disable pandas-style tick locator and formatter for plotting',b'so no conversion to Period (for compatibility with irregular timeseries on twinx etc)'
2200,8222491,y-p,wesm,2012-11-08 23:13:36,2012-11-09 17:40:06,2012-11-09 17:40:06,closed,,0.9.1,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2200,b'Breakage when setting MultiIndex with a mutable sequence',"b'```python\r\nimport pandas as pd\r\ndf=pd.DataFrame({""a"":[1,2,3],""b"":[4,5,6],""c"":[7,8,9]}).set_index([""a"",""b""])\r\nl=list(df.index)\r\nl[0]=(""faz"",""boo"")\r\ndf.index=l\r\nprint df\r\n# all good\r\nl[0]=[""faz"",""boo""]\r\ndf.index=l\r\nprint df\r\n# badness\r\n```\r\n\r\nIf this acceptable usage and index entries must be immutable, failing early would be good.\r\n\r\n**Note:** If you get an `UnboundLocalError` Exception, That\'s an un-related issue, PR fix as soon\r\nas travis is green.'"
2199,8221534,y-p,wesm,2012-11-08 22:38:32,2012-11-09 18:18:21,2012-11-09 17:00:35,closed,,0.9.1,6,Bug,https://api.github.com/repos/pydata/pandas/issues/2199,b'Curious: df.ix[False] returns the first row',"b'```python\r\nIn [21]: df=pd.DataFrame({""a"":[1,2,3]})\r\n    ...: df.ix[False]\r\nOut[21]: \r\na    1\r\nName: 0\r\n```\r\n\r\nDon\'t know if this is a bug or intentiional.'"
2196,8201289,mattijohn,wesm,2012-11-08 11:26:40,2012-11-09 17:58:02,2012-11-09 17:58:02,closed,,0.9.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2196,b'index.union does not sort monotonic indexes',"b'from [StackOverflow](http://stackoverflow.com/questions/13279690/unexpected-behavior-when-combining-two-dataframes-in-pandas):\r\n\r\nWhile index.union is not guaranteed to sort results, it does not try to sort the result for monotonic indexes, but does try to sort non-monotonic indexes.'"
2191,8184156,wesm,changhiskhan,2012-11-07 19:58:11,2012-11-09 03:22:03,2012-11-09 03:21:53,closed,,0.9.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2191,b'DataFrame.add with fill_value bug',"b""Very, very surprised this bug isn't caught by the test suite. \r\n\r\ndf1 = pandas.DataFrame({'a':{True:1,False:5},'b':{True:7,False:8}})\r\ndf2 = pandas.DataFrame({'a':{True:1,False:5},'c':{True:7,False:8}})\r\n\r\n```\r\nIn [15]: df1.add(df2, fill_value=0)\r\nOut[15]: \r\n        a   b   c\r\nFalse  10 NaN NaN\r\nTrue    2 NaN NaN\r\n```"""
2189,8165939,saroele,wesm,2012-11-07 10:32:38,2012-11-09 17:18:15,2012-11-09 17:18:15,closed,,0.9.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2189,b'bug in dataframe.join()',"b""Hi all,\r\n\r\nI have a strange issue with pandas 0.9, I think it's a bug.  I'm trying to use dataframe.join() and it works well on a random dataframe, but not on a dataframe created from my simulation result.  The code below shows that join() on the second dataframe blows up the index and the result is completely wrong.  \r\n\r\nTo run this code you need this file: https://dl.dropbox.com/u/6200325/mydf.dataframe in your work folder.  The script below can also be downloaded here: https://dl.dropbox.com/u/6200325/BugJoin.py\r\n\r\nThis is the result I get:\r\n\r\nIn [17]: run -i 'C:\\Workspace\\Python\\Tests\\BugJoin.py'\r\n\r\nBefore:\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 100000 entries, 2012-01-01 00:00:00 to 2023-05-29 15:00:00\r\nFreq: H\r\nEmpty DataFrame \r\n\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 100000 entries, 2012-01-01 00:00:00 to 2023-05-29 15:00:00\r\nFreq: H\r\nData columns:\r\n0    100000  non-null values\r\ndtypes: float64(1) \r\n\r\nAfter:\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 100000 entries, 2012-01-01 00:00:00 to 2023-05-29 15:00:00\r\nFreq: H\r\nData columns:\r\n0    100000  non-null values\r\ndtypes: float64(1)\r\n\r\nBefore:\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 108355 entries, 2010-01-01 00:00:00 to 2011-01-01 00:00:00\r\nEmpty DataFrame \r\n\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 108355 entries, 2010-01-01 00:00:00 to 2011-01-01 00:00:00\r\nData columns:\r\nSID0000    108355  non-null values\r\ndtypes: float64(1) \r\n\r\nAfter:\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 4054807 entries, 2010-01-09 16:00:00 to 2010-05-17 15:55:42\r\nData columns:\r\nSID0000    4054807  non-null values\r\ndtypes: float64(1)\r\n\r\n\r\nThis is the code from the script:\r\n\r\n    import numpy as np\r\n    import pandas as pd\r\n    from scipy.integrate import cumtrapz\r\n\r\n    df1=pd.DataFrame(np.random.rand(1e5), \r\n         index=pd.date_range('2012-01-01', freq='H', periods=1e5))\r\n\r\n    df2=pd.load('mydf.dataframe')\r\n\r\n    for dataframe in [df1, df2]:\r\n    \r\n        cum = pd.DataFrame(index=dataframe.index)\r\n        for c in dataframe.columns:\r\n            # we need to remove the empty values for the cumtrapz function to work\r\n            ts = dataframe[c].dropna()\r\n        \r\n            tscum = pd.DataFrame(data=cumtrapz(ts.values, ts.index.asi8/1e9, initial=0),\r\n                             index=ts.index, \r\n                             columns=[c])\r\n            print '\\nBefore:'\r\n            print cum, '\\n'\r\n            print tscum, '\\n'\r\n        \r\n            cum=cum.join(tscum, how='left')\r\n\r\n            print 'After:'\r\n            print cum\r\n"""
2187,8157783,wesm,wesm,2012-11-07 00:38:20,2012-11-07 01:03:56,2012-11-07 01:03:56,closed,,0.9.1,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2187,b'Handling of newline at end of file in read_csv',"b'\r\n\r\n```\r\nIn [1314]: data = """"""\\\r\n   ......: A,B,C\r\n   ......: 00001,001,5\r\n   ......: 00002,002,6\r\n   ......: """"""\r\n   ......:\r\n\r\nIn [1315]: from cStringIO import StringIO\r\n\r\nIn [1316]: read_csv(StringIO(data), converters={\'A\' : lambda x: x.strip()})\r\nOut[1316]: \r\n        A   B   C\r\n0   00001   1   5\r\n1   00002   2   6\r\n2         NaN NaN\r\n```'"
2182,8119737,saroele,saroele,2012-11-05 21:27:28,2012-11-07 15:06:38,2012-11-05 22:28:08,closed,,0.9.1,9,Bug,https://api.github.com/repos/pydata/pandas/issues/2182,b'python crash without exception',"b""I'm using pandas 0.9.0.  I have a function that returns a DataFrame.  The resulting DataFrame is 'unstable': many operations I try on it, interactively in IPython, lead to a complete python.exe crash.  I get no exception or indication on what's going wrong which is very frustrating.\r\n\r\nSome operations do work however, like df.index or df.values.\r\n\r\nIn the screenshot below, r is a custom-built class with method to_dataframe() that generates the dataframe.  \r\n\r\nIn [6]: df=r.to_dataframe()\r\n\r\nIn [7]: df.index\r\nOut[7]:\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\n[2010-01-01 00:00:00, ..., 2011-01-01 00:00:00]\r\nLength: 108355, Freq: None, Timezone: None\r\n\r\n\r\nIn [9]: df.values\r\nOut[9]:\r\narray([[-0.],\r\n       [-0.],\r\n       [-0.],\r\n       ...,\r\n       [-0.],\r\n       [-0.],\r\n       [-0.]], dtype=float32)\r\n\r\nIn [10]: df\r\nOut[10]:\r\nHERE IS THE CRASH\r\n\r\nI also get the crash when doing print df, df.resample('D'), etc. \r\nWhat can I do to get more information, or solve this bug?\r\n\r\n\r\n\r\n\r\n"""
2179,8103932,y-p,wesm,2012-11-05 12:58:16,2012-11-24 18:49:32,2012-11-24 18:49:32,closed,wesm,0.10,9,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/2179,b'BUG: df.from_records() is all kinds of broken',"b'- the `columns` argument gets clobbered:\r\nIn [13]: pd.DataFrame.from_records({1:[""foo""],2:[""bar""]},columns=[\'a\',\'b\']).columns\r\nOut[13]: Index([1, 2], dtype=int64)\r\n\r\n- if `index` is specified, and `result_index` computed, it will get clobbered\r\nlater on.\r\n \r\n- if `index` is specified as a list of labels, with the first few matching columns names\r\nand the others not, then sdict will get mutilated, because the removal of columns\r\noccurs within the try clause rather then after success.\r\n**EDIT** - ignore, I misread the code.\r\n\r\n- There\'s duplication against the main Dataframe ctor which also accepts dicts and arrays, \r\nand The exclusion  and float coercsion which are unique to `from_records()` would be useful \r\nto have generally available in the main ctor anyway.\r\n\r\n- It\'s unclear if the `columns` argument should be specified relative to the original\r\ndata, or relative to the data modulo excluded columns\r\n\r\n- Doesn\'t support duplicate column names (Although that\'s checked with\r\na warning)\r\n\r\n- the docstring specifies the datatypes for `data` , which do not include a dict.\r\nBut the code specifically checks and handles dict input, This is a minor thing, but\r\nusing `columns` along with dict is not well defined because of key (non-)ordering,\r\nso the original docstring spec seems more sane.\r\n\r\n\r\n'"
2172,8091742,changhiskhan,jreback,2012-11-04 20:17:59,2013-09-21 13:24:41,2013-09-21 13:24:35,closed,,0.13,3,Bug;Duplicate;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/2172,b'Use index option for bar plot',b'help with bar and line plot types'
2167,8076546,guyrt,wesm,2012-11-03 03:56:34,2012-11-03 19:23:06,2012-11-03 19:23:06,closed,,0.9.1,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2167,b'Strange output in DataFrame representation of datetime64',"b""It looks like printing a DataFrame's representation gives a strange result when one column is datetime64. Details in this SO post:\r\n\r\nhttp://stackoverflow.com/questions/13202326/pandas-to-datetime-produces-confusing-result"""
2164,8061161,wesm,wesm,2012-11-02 15:10:01,2012-11-02 22:47:26,2012-11-02 22:47:26,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2164,b'Panel.shift with negative numbers broken',
2163,8061113,wesm,wesm,2012-11-02 15:08:16,2012-11-02 22:03:26,2012-11-02 22:03:26,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2163,b'DataFrame.to_panel discards minor index name',
2161,8060294,wesm,wesm,2012-11-02 14:39:03,2012-11-02 21:35:26,2012-11-02 21:35:26,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2161,b'Use index name (if any) in to_records',b'xref http://stackoverflow.com/questions/13187778/pandas-dataframe-to-numpy-array-include-index'
2157,8041971,lodagro,changhiskhan,2012-11-01 20:22:14,2012-11-04 02:35:25,2012-11-04 02:35:25,closed,,0.9.1,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2157,b'off-center grid in case of stacked bar plot',"b""from [mailing list](https://groups.google.com/forum/#!searchin/pydata/Spacing$20error$20in$20bar$20graphs/pydata/KkIz8evL4As/ft8Puy8UwCwJ)\r\n\r\n```python\r\ndf = DataFrame({'A' : [3] * 5, 'B' : range(5)}, index = range(5))\r\ndf.plot(kind='bar', stacked='True', grid=True)\r\n```\r\n\r\n![example image](http://i.imgur.com/c1AMA.png)"""
2155,8040593,wesm,wesm,2012-11-01 19:30:39,2012-11-02 21:29:46,2012-11-02 21:29:46,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2155,b'Timestamp indexing bug',"b""```\r\nfrom datetime import datetime\r\nimport numpy as np\r\nfrom pandas import DataFrame, Timestamp, DatetimeIndex\r\nimport pandas.core.datetools as datetools\r\n \r\ncolumns = DatetimeIndex(start='1/1/2012', end='2/1/2012', freq=datetools.bday)\r\nindex = range(10)\r\ndata = DataFrame(columns=columns, index=index)\r\nt = datetime(2012, 11, 1)\r\nts = Timestamp(t)\r\n \r\n \r\n \r\nThis raises an exception:\r\n \r\ndata[ts] = np.nan\r\n \r\nwhile this works:\r\n \r\ndata[t] = np.nan\r\n```"""
2150,8033300,changhiskhan,changhiskhan,2012-11-01 15:22:20,2012-11-02 16:23:05,2012-11-02 16:22:54,closed,,0.9.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2150,b'Inconsistencies in dateoffsets',b'In [26]: pd.offsets.Minute().isAnchored()\r\nOut[26]: True\r\n\r\nIn [27]: pd.offsets.Day().isAnchored()\r\nOut[27]: False'
2148,8019725,wesm,wesm,2012-11-01 00:09:21,2012-11-02 21:27:55,2012-11-02 21:27:55,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2148,"b""Series.iget / DataFrame.iget_value doesn't box Timestamps""",b'Unclear how important this is'
2146,7965976,pag,wesm,2012-10-30 12:17:49,2013-12-04 00:57:52,2012-11-02 18:20:02,closed,,0.9.1,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2146,b'Printing a Series with dtype timedelta64',"b'```\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> print pd.Series(np.array([1100, 20], dtype=\'timedelta64[s]\'))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\series.py"", line 911, in __str__\r\n    return repr(self)\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\series.py"", line 847, in __repr__\r\n    name=True)\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\series.py"", line 908, in _get_repr\r\n    return formatter.to_string()\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\format.py"", line 126, in to_string\r\n    fmt_values = self._get_formatted_values()\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\format.py"", line 117, in _get_formatted_values\r\n    na_rep=self.na_rep)\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\format.py"", line 691, in format_array\r\n    return fmt_obj.get_result()\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\format.py"", line 814, in get_result\r\n    fmt_values = [formatter(x) for x in self.values]\r\n  File ""C:\\Python27_64\\lib\\site-packages\\pandas\\core\\format.py"", line 812, in <lambda>\r\n    formatter = lambda x: \'% d\' % x\r\nTypeError: %d format: a number is required, not numpy.timedelta64\r\n```\r\n\r\nThis is because timedelta64 is an int type (see format.format_array) but can\'t be formatted like that.'"
2144,7942264,wesm,wesm,2012-10-29 16:16:42,2012-11-01 00:56:44,2012-11-01 00:56:44,closed,,0.9.1,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2144,b'__repr__ display of dict elements misleading',"b""```\r\nIn [14]: DataFrame([data])['name']\r\nOut[14]: \r\n0    [full, last, ascii_first, ascii_last, first]\r\nName: name\r\n\r\nIn [15]: DataFrame([data])['name'][0]\r\nOut[15]: \r\n{u'ascii_first': u'Paul',\r\n u'ascii_last': u'Millsap',\r\n u'first': u'Paul',\r\n u'full': u'Paul Millsap',\r\n u'last': u'Millsap'}\r\n```"""
2140,7911819,wesm,wesm,2012-10-27 15:31:13,2012-11-03 16:19:38,2012-11-03 16:19:38,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2140,b'Time rule inference regression with WOM- frequencies',b'This needs to be implemented (hopefully) in the frequency inference code'
2133,7895719,gerigk,changhiskhan,2012-10-26 15:56:21,2012-11-24 23:06:35,2012-11-24 23:06:35,closed,,0.10,6,Bug,https://api.github.com/repos/pydata/pandas/issues/2133,"b'groupby.first() casts datetime64[ns] series to \'object\' with \'long"" elements'","b""```\r\nimport numpy as np\r\nimport pandas as pd\r\ndf = pd.DataFrame([(1, 1351036800000000000), (2, 1351036800000000000)])\r\ndf[1] = df[1].view('M8[ns]')\r\nprint df[1].dtype\r\nprint df.groupby(0).first()[1].dtype\r\nprint type(df.groupby(0).first()[1][1])\r\n\r\ndatetime64[ns]\r\nobject\r\n<type 'long'>\r\n```"""
2129,7880737,Lamarth,wesm,2012-10-26 02:24:21,2012-11-08 07:50:47,2012-11-02 16:39:22,closed,,0.9.1,3,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2129,"b'intersection on DatetimeIndex fails for empty indices of ""quality""'","b'index = pandas.DatetimeIndex(start=\'2000-01-01\', periods=4, freq=\'T\')\r\nindex[0:0].intersection(index)\r\n\r\nTraceback (most recent call last):\r\n  File ""<console>"", line 1, in <module>\r\n  File ""C:\\dev\\Python27\\Lib\\site-packages\\pandas\\tseries\\index.py"", line 845, in intersection\r\n    if self[0] <= other[0]:\r\n  File ""C:\\dev\\Python27\\Lib\\site-packages\\pandas\\tseries\\index.py"", line 976, in __getitem__\r\n    val = arr_idx[key]\r\nIndexError: index out of bounds'"
2128,7880721,Lamarth,wesm,2012-10-26 02:22:14,2012-11-04 22:47:06,2012-11-04 22:47:05,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2128,b'ewma adjust incorrectly ticked for NaNs',"b""series = pandas.Series([1., numpy.NaN, numpy.NaN, 1.], index=pandas.DatetimeIndex(start='2000-01-01', periods=4, freq='T'))\r\npandas.ewma(series, com=5)\r\n\r\n2000-01-01 00:00:00    1.000000\r\n2000-01-01 00:01:00    0.545455\r\n2000-01-01 00:02:00    0.545455\r\n2000-01-01 00:03:00    1.000000\r\n\r\nAlmost right - but clearly the adjust factor is ticked in the wrong spot."""
2127,7877129,darragjm,wesm,2012-10-25 22:25:14,2012-11-04 21:24:45,2012-11-04 21:24:45,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2127,b'DataFrame.align() does not retain DatetimeIndex timezone awareness',"b'Aligning two DataFrames with timezone-aware DatetimeIndex indexes returns a tuple (DataFrame, DataFrame) in which both DataFrames are timezone naive.'"
2125,7873070,ijmcf,wesm,2012-10-25 20:03:33,2012-11-04 21:18:58,2012-11-04 21:18:58,closed,changhiskhan,0.9.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2125,b'Period.end_time is incorrect when period is W or longer',"b""For example:\r\n>>> p = pandas.Period('2012', freq='M')\r\n>>> p.end_time\r\n<Timestamp: 2012-01-31 00:00:00>\r\n\r\nThis is the *start* of the last day of the month.  So if one were using this in code, you could erroneously find that any datetime on January 31st is after the end of the month of January, for example:\r\n\r\n>>> datetime.datetime(2012, 1, 31, 12) > p.end_time\r\nTrue\r\n"""
2124,7873043,ijmcf,wesm,2012-10-25 20:02:35,2012-11-04 21:18:36,2012-11-04 21:18:36,closed,changhiskhan,0.9.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2124,"b'Period.end_time is same as start_time when period is D, B, H, T, or S'","b""For example:\r\n>>> p = pandas.Period('2012-10-23', 'D')\r\n>>> p.start_time\r\n<Timestamp: 2012-10-23 00:00:00>\r\n>>> p.end_time\r\n<Timestamp: 2012-10-23 00:00:00>\r\n\r\nThere are tests, but they fail to detect the problem because they are comparing the end_time to a constant which is the start_time (so they pass)."""
2120,7861957,nicktp,lodagro,2012-10-25 14:00:58,2012-10-31 20:23:16,2012-10-31 20:23:03,closed,,0.9.1,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2120,"b""Series 'print' output truncation""","b""When printing a Series (I haven't checked if the same applies to a DataFrame), the output is truncated in such a way as to change the meaning.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nvals = [2.08430917305e+10, 3.52205017305e+10, 2.30674817305e+10, \\\r\n        2.03954217305e+10, 5.59897817305e+10]\r\n\r\ns = pd.Series(vals)\r\nprint s.values == vals # True, ie the numbers stored are unaffected\r\nprint s # Should be 'e+10' rather than 'e+1'\r\n```\r\n\r\nOddly, changing a single power to 9 rather than 10 makes the whole thing display properly:\r\n\r\n```python\r\ns[0] = 2.08430917305e+9\r\nprint s # Now shows the correct exponent values.\r\n```"""
2119,7861633,wesm,wesm,2012-10-25 13:49:27,2012-11-01 00:42:14,2012-11-01 00:42:14,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2119,"b""str.split('|') failure""","b""Look at Series with strings like 'A|B|C'"""
2117,7860021,lodagro,wesm,2012-10-25 13:06:57,2012-11-03 19:11:45,2012-11-03 19:11:45,closed,,0.9.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2117,b'Case where xs(copy=False) does not return view.',"b""from [stackoverflow](http://stackoverflow.com/questions/13068551/pandas-slice-on-hierarchical-index-without-a-copy/)\r\n\r\n\r\n\r\n```python\r\n\r\nIn [25]: cpaste\r\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\r\n:>>> index = pd.MultiIndex.from_arrays([['a','a', 'b', 'b'], [1,2,1,2]], \r\n:...                                   names=['first', 'second'])\r\n:>>> data = pd.DataFrame(np.random.rand(len(index)), index=index, columns=['A'])\r\n:>>> print data\r\n:--\r\n                     A\r\nfirst second          \r\na     1       0.026915\r\n      2       0.777457\r\nb     1       0.445726\r\n      2       0.243053\r\n\r\nIn [26]: selected = data.xs(2, level='second', copy=False)\r\n\r\nIn [27]: selected\r\nOut[27]: \r\n              A\r\nfirst          \r\na      0.777457\r\nb      0.243053\r\n\r\nIn [28]: selected['A'][0] = 100\r\n\r\nIn [29]: selected\r\nOut[29]: \r\n                A\r\nfirst            \r\na      100.000000\r\nb        0.243053\r\n\r\nIn [30]: data\r\nOut[30]: \r\n                     A\r\nfirst second          \r\na     1       0.026915\r\n      2       0.777457\r\nb     1       0.445726\r\n      2       0.243053\r\n```"""
2114,7837232,mrjbq7,wesm,2012-10-24 18:53:20,2012-11-04 22:04:13,2012-11-04 22:04:13,closed,wesm,0.9.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2114,b'rolling_mean and rolling_sum produce negative output from positive input',"b""I have an array of non-negative numbers, that when used with ``rolling_sum`` or ``rolling_mean`` produce an output array that has a small negative number in it.\r\n\r\nThe test looks like this: \r\n\r\n```python\r\nimport numpy as np\r\nimport pandas\r\n\r\ndata = np.load('data.npy')\r\nassert all(data >= 0)\r\n\r\nsums = pandas.rolling_sum(data, 2, min_periods=1)\r\nzero = np.where(sums < 0)[0]\r\nassert len(zero) == 0, zero\r\n\r\nmean = pandas.rolling_mean(data, 2, min_periods=1)\r\nzero = np.where(mean < 0)[0]\r\nassert len(zero) == 0, zero\r\n```\r\n\r\nIt requires a small binary array to reproduce, because of the floating point numbers (so I created a gist: https://gist.github.com/3948013).\r\n\r\nYou can run the test case:\r\n\r\n```\r\n$ git clone git://gist.github.com/3948013.git\r\n$ cd 3948013\r\n$ python test.py\r\n```\r\n\r\nI made sure this bug affects the most current version of Pandas:\r\n\r\n```python\r\n>>> import pandas\r\n>>> pandas.__version__\r\n'0.9.1.dev-8cd93d3'\r\n\r\n>>> import numpy\r\n>>> numpy.__version__\r\n'1.7.0b2'\r\n```\r\n"""
2113,7835024,leonbaum,wesm,2012-10-24 17:41:03,2012-10-25 12:21:58,2012-10-24 18:16:29,closed,,0.9.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2113,b'Random seg fault with groupby',"b'I came across this seg fault when I was trying to figure out the syntax to get column means.  The below example triggers the seg fault, but strangely only about 30% of the time.  About 10% of the time I get a reasonable error.  The randomness of the result is very strange to me because the same command was run every time!\r\n\r\nI am using the latest pandas and numpy from git.  I have ECC memory and have never seen any hardware triggered issues.\r\n\r\n```\r\n$ python\r\nPython 3.2.2 (default, Mar  9 2012, 14:43:22)\r\n[GCC 4.4.4 20100726 (Red Hat 4.4.4-13)] on linux2\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas\r\n>>> pandas.DataFrame({\'x\': [1,2,3], \'y\': [3,4,5]}).groupby(level=0, axis=\'columns\').mean()\r\nSegmentation fault\r\n\r\n$ python\r\nPython 3.2.2 (default, Mar  9 2012, 14:43:22)\r\n[GCC 4.4.4 20100726 (Red Hat 4.4.4-13)] on linux2\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas\r\n>>> pandas.DataFrame({\'x\': [1,2,3], \'y\': [3,4,5]}).groupby(level=0, axis=\'columns\').mean()\r\n   0  1  2\r\nx  1  2  3\r\ny  3  4  5\r\n>>> pandas.DataFrame({\'x\': [1,2,3], \'y\': [3,4,5]}).groupby(level=0, axis=\'columns\').mean()\r\n   0  1  2\r\nx  1  2  3\r\ny  3  4  5\r\n>>> pandas.DataFrame({\'x\': [1,2,3], \'y\': [3,4,5]}).groupby(level=0, axis=\'columns\').mean()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.2/site-packages/pandas-0.9.1.dev_ebcbd33-py3.2-linux-x86_64.egg/pandas/core/generic.py"", line 134, in groupby\r\n    sort=sort, group_keys=group_keys)\r\n  File ""/usr/local/lib/python3.2/site-packages/pandas-0.9.1.dev_ebcbd33-py3.2-linux-x86_64.egg/pandas/core/groupby.py"", line 481, in groupby\r\n    return klass(obj, by, **kwds)\r\n  File ""/usr/local/lib/python3.2/site-packages/pandas-0.9.1.dev_ebcbd33-py3.2-linux-x86_64.egg/pandas/core/groupby.py"", line 146, in __init__\r\n    grouper, exclusions = _get_grouper(obj, keys, axis=axis,\r\nIndexError: tuple index out of range\r\n>>> pandas.DataFrame({\'x\': [1,2,3], \'y\': [3,4,5]}).groupby(level=0, axis=\'columns\').mean()\r\nSegmentation fault\r\n```'"
2108,7810916,glencarl,wesm,2012-10-23 21:22:32,2012-11-07 13:41:28,2012-10-24 15:49:36,closed,,0.9.1,5,Bug,https://api.github.com/repos/pydata/pandas/issues/2108,b'AttributeError np._asarray_tublesafe in tseries/converter',"b'Discovered matplotlib/axis convert_units raised AttributeError\r\n\r\npandas/tseries/converter.py"", line 157, in convert\r\n    values = np._asarray_tuplesafe(values)\r\nAttributeError: \'module\' object has no attribute \'_asarray_tuplesafe\'\r\n\r\nIt can be easily fixed by adding:\r\nfrom pandas.core.common import _asarray_tuple\r\nand change line 157 to\r\nvalues =  _asarray_tuplesafe(values)'"
2107,7808002,bluefir,wesm,2012-10-23 19:41:30,2012-11-03 15:23:50,2012-11-03 15:23:50,closed,changhiskhan,0.9.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2107,b'data_frame.xs() with MultiIndex throws exception when explicitly setting level= to the first level',"b""I have the following DataFrame:\r\n\r\n    market_caps\r\n\r\n>\\<class 'pandas.core.frame.DataFrame'>\r\nMultiIndex: 409938 entries, (20111230, '00036110') to (20121019, 'Y8564W10')\r\nData columns:\r\nmarket_cap    409938  non-null values\r\ndtypes: float64(1)\r\n\r\n    market_caps.index.names\r\n\r\n>['date', 'security_id']\r\n\r\nI want to get a cross-section by date. The following works just fine, implicitly slicing on the first level of the MultiIndex:\r\n\r\n    market_caps.xs(20111230)\r\n\r\n>\\<class 'pandas.core.frame.DataFrame'>\r\nIndex: 1993 entries, 00036110 to Y8564W10\r\nData columns:\r\nmarket_cap    1993  non-null values\r\ndtypes: float64(1)\r\n\r\nHowever, if I try to explicitly declare the first level, I get an exception:\r\n\r\n    market_caps.xs(20111230, level='date')\r\n\r\n>---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-60-c2c77fa1057f> in <module>()\r\n----> 1 market_caps.xs(20111230, level='date')\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in xs(self, key, axis, level, copy)\r\n   1983 \r\n   1984             result = self.ix[indexer]\r\n-> 1985             setattr(result, result._get_axis_name(axis), new_ax)\r\n   1986             return result\r\n   1987 \r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __setattr__(self, name, value)\r\n   1780                 existing = getattr(self, name)\r\n   1781                 if isinstance(existing, Index):\r\n-> 1782                     super(DataFrame, self).__setattr__(name, value)\r\n   1783                 elif name in self.columns:\r\n   1784                     self[name] = value\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.AxisProperty.__set__ (pandas\\src\\tseries.c:97451)()\r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in _set_axis(self, axis, labels)\r\n    461 \r\n    462     def _set_axis(self, axis, labels):\r\n--> 463         self._data.set_axis(axis, labels)\r\n    464         self._clear_item_cache()\r\n    465 \r\n>\r\n>C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in set_axis(self, axis, value)\r\n    516         if len(value) != len(cur_axis):\r\n    517             raise Exception('Length mismatch (%d vs %d)'\r\n--> 518                             % (len(value), len(cur_axis)))\r\n    519         self.axes[axis] = _ensure_index(value)\r\n    520 \r\n>\r\n>Exception: Length mismatch (1993 vs 0)\r\n\r\nSlicing on the second level works just fine:\r\n\r\n    market_caps.xs('00036110', level='security_id')\r\n\r\n>\\<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 204 entries, 20111230 to 20121019\r\nData columns:\r\nmarket_cap    204  non-null values\r\ndtypes: float64(1)\r\n\r\nIs it supposed to be like this?"""
2103,7769554,jseabold,wesm,2012-10-22 15:24:38,2012-11-02 16:40:07,2012-11-02 16:40:07,closed,,0.9.1,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2103,b'Setting a MultiIndex after concat does not work properly',"b'Consider this. It seems to work properly after sorting but not otherwise?\r\n\r\n```\r\ndf = pandas.DataFrame([[""A"",""a"", 1.2],[""A"",""b"", 2.2],[""B"", ""a"", 1.1],[""C"", ""a"", 1.2]], columns=[""var1"",""var2"", ""var3""])\r\n\r\ndf2 = pandas.DataFrame([[""A"",""c"", 1.2],[""B"", ""c"", 4.1],[""C"", ""c"", 2.2]], columns=[""var1"",""var2"", ""var3""])\r\n\r\ndf3 = pandas.concat((df, df2))\r\n\r\n\r\ndf3.set_index([""var1"",""var2""])\r\n                                                                        \r\ndf3.sort(""var1"").set_index([""var1"",""var2""])                                \r\n```\r\n'"
2101,7765604,jseabold,wesm,2012-10-22 13:14:30,2012-11-03 00:23:14,2012-11-03 00:22:54,closed,wesm,0.9.1,7,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2101,b'Bug in set_index or drop',"b'I\'m not sure if the bug is in the drop or that it shouldn\'t have let me set this MultiIndex since it\'s non-unique. var1 is just a combination of var2 and var3. In any event the result given back by this is garbage. I wanted to drop all the rows where the count of var1 == 1. Since there\'s not to my knowledge a way to drop variables without setting them to an index, I tried to do this without thinking\r\n\r\n```\r\ndf = pandas.DataFrame([[""x-a"", ""x"", ""a"", 1.5],[""x-a"", ""x"", ""a"", 1.2],\r\n                        [""z-c"", ""z"", ""c"", 3.1], [""x-a"", ""x"", ""a"", 4.1],\r\n                       [""x-b"", ""x"", ""b"", 5.1],[""x-b"", ""x"", ""b"", 4.1],\r\n                       [""x-b"", ""x"", ""b"", 2.2],\r\n                       [""y-a"", ""y"", ""a"", 1.2],[""z-b"", ""z"", ""b"", 2.1]],\r\n                       columns=[""var1"", ""var2"", ""var3"", ""var4""])\r\n\r\ngrp_size = df.groupby(""var1"").size()\r\ndrop_idx = grp_size.ix[grp_size == 1]\r\n\r\ndf.set_index([""var1"", ""var2"", ""var3""]).drop(drop_idx.index, level=0).reset_index()\r\n```'"
2100,7754338,wesm,wesm,2012-10-22 00:32:03,2012-11-01 00:20:38,2012-11-01 00:20:38,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2100,b'Odd unstack failure',"b'Set day,time, smoker as index, unstack(2)\r\n\r\n```\r\nday,time,smoker,sum,len\r\nFri,Dinner,No,8.25,3\r\nFri,Dinner,Yes,27.03,9\r\nFri,Lunch,No,3.0,1\r\nFri,Lunch,Yes,13.68,6\r\nSat,Dinner,No,139.63,45\r\nSat,Dinner,Yes,120.77,42\r\nSun,Dinner,No,180.57,57\r\nSun,Dinner,Yes,66.82,19\r\nThur,Dinner,No,3.0,1\r\nThur,Lunch,No,117.32,44\r\nThur,Lunch,Yes,51.51,17\r\n```'"
2098,7753680,jseabold,jreback,2012-10-21 23:09:13,2013-09-21 13:03:39,2013-09-21 13:03:39,closed,wesm,0.13,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2098,b'merging on dates ignores sort keyword',"b'```\r\nimport pandas\r\nimport numpy as np\r\n\r\nfrom datetime import datetime\r\n\r\nd = {""var1"" : np.random.randint(0, 10, size=10), ""var2"" : np.random.randint(0, 10, size=10), ""var3"" : [datetime(2012, 1, 12), datetime(2011, 2, 4), datetime(2010, 2, 3), datetime(2012, 1, 12), datetime(2011, 2, 4), datetime(2012, 4, 3), datetime(2012, 3, 4), datetime(2008, 5, 1), datetime(2010, 2, 3), datetime(2012, 2, 3)]}\r\ndf = pandas.DataFrame.from_dict(d)\r\ndf.ix[:7]\r\nvar3 = df.var3.unique()\r\nvar3.sort()\r\nnew = pandas.DataFrame.from_dict({""var3"" : var3, ""var8"" : np.random.random(7)})\r\ndf.merge(new, on=""var3"", sort=False).ix[:7]\r\n```'"
2096,7741010,wesm,wesm,2012-10-20 20:23:37,2012-11-02 23:06:06,2012-11-02 23:06:06,closed,wesm,0.9.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2096,b'Bug with boolean indexing on empty DataFrame',"b""xref http://stackoverflow.com/questions/12700251/pandas-edge-case-on-boolean-index-into-empty-dataframe\r\n\r\n```\r\nblah = pandas.DataFrame(numpy.empty([0, 1]), columns=['A'], index=pandas.DatetimeIndex([]))\r\nblah[numpy.array([], bool)]\r\n```\r\n\r\nEmpty DataFrame\r\n\r\n```\r\nblah[numpy.array([], bool)] = 0.0\r\n```"""
2095,7740987,wesm,wesm,2012-10-20 20:19:49,2012-12-06 23:39:11,2012-11-01 14:00:10,closed,,0.9.1,8,Bug,https://api.github.com/repos/pydata/pandas/issues/2095,b'Incorrect handling of datetime64 values in structured arrays',"b""see:\r\n\r\n```\r\narr = np.array([ (datetime.datetime(2012, 9, 9, 0, 0), datetime.datetime(2012, 9, 8, 15, 10))],\r\ndtype=[('Date', '<M8[us]'), ('Forecasting', '<M8[us]')])\r\nDataFrame(arr)\r\n```\r\n\r\ncf http://stackoverflow.com/questions/12369546/pandas-parsing-datetime-column-from-sqlite-database"""
2087,7737265,ktii,wesm,2012-10-20 12:33:17,2012-11-04 23:03:17,2012-11-04 23:03:17,closed,changhiskhan,0.9.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/2087,b'Series.diff() not exact for huge numbers: approximation or mistake?',"b'Series.diff() is not exact for huge numbers. This might be due to some quick approximation or just a bug/mistake.\r\n\r\nIn [40]: a = 10000000000000000\r\n\r\nIn [41]: log10(a)\r\nOut[41]: 16.0\r\n\r\nIn [42]: b = a + 1\r\n\r\nIn [43]: s = Series([a,b])\r\n\r\nIn [44]: s.diff()\r\nOut[44]: \r\n0   NaN\r\n1     0\r\n\r\n\r\nnumpy.diff() is not to blame:\r\n\r\nIn [45]: v = s.values\r\n\r\nIn [46]: v\r\nOut[46]: array([10000000000000000, 10000000000000001], dtype=int64)\r\n\r\nIn [47]: diff(v)\r\nOut[47]: array([1], dtype=int64)\r\n\r\n\r\nOne less digit is fine:\r\n\r\nIn [48]: a = 1000000000000000\r\n\r\nIn [48]: log10(a)\r\nOut[48]: 15.0\r\n\r\nIn [49]: b = a + 1\r\n\r\nIn [50]: s = Series([a,b])\r\n\r\nIn [51]: s.diff()\r\nOut[51]: \r\n0   NaN\r\n1     1\r\n\r\nWhy do I need these huge numbers? Certain timestamps (in my case VMS) have this many digits (tenth of microseconds since the year 1858 if I remember correctly).'"
2083,7717799,wesm,wesm,2012-10-19 14:45:21,2012-11-01 00:10:29,2012-11-01 00:10:29,closed,wesm,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2083,b'min/max on datetime64-dtype Series should yield Timestamp',b'xref: http://stackoverflow.com/questions/12974404/unexpected-results-of-min-and-max-methods-of-pandas-series-made-of-timestamp'
2082,7715448,changhiskhan,wesm,2012-10-19 13:22:18,2012-11-03 16:23:01,2012-11-03 16:23:01,closed,,0.9.1,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2082,b'bar plot color cycle',b'for line graph `style` correctly sets color cycle\r\nfor bar/barh `color` does. \r\nNeed to make both work for both'
2078,7639412,cpcloud,wesm,2012-10-16 21:15:38,2012-11-02 21:55:14,2012-11-02 21:55:14,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2078,"b'""greater than"" comparison method of doesn\'t perform as expected on Micro objects'","b""Using IPython:\r\n```python\r\nIn [1]: import pandas\r\n\r\nIn [2]: Micro = pandas.datetools.Micro\r\n\r\nIn [3]: 3 * Micro() > 2 * Micro()\r\nOut[3]: False\r\n\r\nIn [4]: 2 * Micro() > 3 * Micro()\r\nOut[4]: False\r\n```\r\nThey can't both be `False`. Giving the arguments to `>` to `cmp` works as expected. Not sure where the issue lies since neither `Micro` nor any of its superclasses implements any of the comparison methods except `__eq__` and `__ne__`."""
2074,7604527,jseabold,hayd,2012-10-15 20:21:49,2014-05-29 07:42:03,2014-05-29 07:42:03,closed,,Someday,9,Bug;Strings,https://api.github.com/repos/pydata/pandas/issues/2074,b'Problem with Series.str.match',"b'Not sure yet what\'s going on here. Doesn\'t appear to be a unicode issue\r\n\r\n```\r\ndf = pandas.DataFrame([[u\'A Confe\\xdfion of the most auncient and true christen catholike olde belefe accordyng to the ordre of the .XII. Articles of our co[m]mon crede, set furthe in Englishe to the glory of almightye God, and to the confirmacion of Christes people in Christes catholike olde faith.\']], columns=[""title""])\r\n\r\ndf.title.str.match("".*[A|a]lmight"")\r\n# returns \r\n#0    ()\r\n#Name: title\r\n\r\nre.match("".*[A|a]lmight"", df.title.ix[0])\r\n# expected output\r\n#<_sre.SRE_Match at 0x4fa8100>\r\n```'"
2071,7587169,lbeltrame,wesm,2012-10-15 09:23:44,2012-11-15 16:07:15,2012-11-15 15:46:12,closed,,0.10,8,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/2071,b'c-parser branch: Iteration over an open file handle makes the parser fail',"b'An example is better than words:\r\n\r\n````python\r\n\r\nIn [16]: cat test.txt\r\nAAA\r\nBBB\r\nCCC\r\nDDD\r\nEEE\r\nFFF\r\nGGG\r\n\r\nIn [17]: with open(""test.txt"") as handle:                      \r\n    iterator = itertools.islice(handle, 3)\r\n    print list(iterator)\r\n    res = pandas.read_table(handle, squeeze=True, header=None)\r\n   ....:     \r\n[\'AAA\\n\', \'BBB\\n\', \'CCC\\n\']\r\n\r\nIn [18]: res\r\nOut[18]: \r\nEmpty DataFrame\r\nColumns: array([], dtype=object)\r\nIndex: array([], dtype=object)\r\n````\r\n\r\nThis works with the python engine. Notice that the handle is not really iterated through: when debugging I noticed that after iterator usage, the handle keeps on staying at the same file line (IOW the parser is not iterating on it at all).'"
2070,7583186,abielr,wesm,2012-10-15 04:23:59,2013-01-21 17:17:58,2013-01-21 17:17:58,closed,wesm,0.10.1,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2070,b'Cannot aggregate by mean when using PeriodIndex and high-frequency series does cross between bins',"b'When using a Series indexed by a PeriodIndex and downsampling, resampling fails when using how=\'mean\' and where the series to be resampled does not span multiple lower-frequency bins. \r\n\r\nFor example:\r\n\r\n    ix = period_range(start=""2012-01-01"", end=""2012-12-31"", freq=""M"")\r\n    s = Series(np.random.randn(len(ix)), index=ix)\r\n    s.resample(""A"", how=\'mean\')\r\n\r\nFails because the period range is entirely contained within a single year. I\'ve been able to replicate this going from quarterly to annual, or monthly to quarterly, etc. As of 0.9.1-dev, crashes Python without an exception as Cython function group_mean_bin() attempts to index into an empty bins array.\r\n\r\n    if bins[len(bins) - 1] == len(values): # Crash\r\n\r\nI don\'t know how fine-grained pandas is right now when aggregating partially-filled periods, but it could be nice to have an option to return a NaN when the higher-frequency window is only partially filled. For example, suppose we sum daily to monthly and take a percent change across months, and either the recording started partway through the first month or data is only available partway through the last month. Then the first or last period percent change will possibly show a dramatic swing, and the user may not realize its simply an artifact of the data availability, as opposed to a truly interesting move in the underlying process. When running alot of automated aggregations the user may wish to not aggregate any partially filled periods in order to protect themselves from reaching a false conclusion about the time-series trend at the beginning or end of the series.'"
2068,7576861,wesm,wesm,2012-10-14 19:08:42,2012-10-31 23:10:18,2012-10-31 23:10:18,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2068,b'Better prevent UnboundLocalError in DataFrame.apply',b'cf. http://stackoverflow.com/questions/12608718/python-pandas-unbound-local-error-while-calling-a-function-df-apply'
2065,7558899,wesm,wesm,2012-10-13 03:19:12,2012-11-01 00:23:39,2012-11-01 00:23:39,closed,,0.9.1,1,Bug;Stats,https://api.github.com/repos/pydata/pandas/issues/2065,b'Update pandas WLS centered_tss to match statsmodels',
2060,7524203,wesm,wesm,2012-10-11 19:39:09,2012-11-02 16:46:21,2012-11-02 16:46:21,closed,,0.9.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/2060,b'Timestamp.astimezone returns datetime.datetime',
2057,7520097,bluefir,wesm,2012-10-11 17:20:52,2012-11-19 04:59:35,2012-11-19 04:59:35,closed,,0.10,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2057,b'transform in groupby throws TypeError when run with python -O option',"b'I have a script that works fine when run without any options but generates the following traceback when run with \'python -O\':\r\n\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py"", line 1745, in transform\r\n    return self._transform_item_by_item(obj, wrapper)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py"", line 1777, in _transform_item_by_item\r\n    raise TypeError(\'Transform function invalid for data types\')\r\nTypeError: Transform function invalid for data types\r\n\r\nI have Python 2.7.3 and pandas 0.9.0:\r\n\r\nPython 2.7.3 (default, Apr 10 2012, 23:24:47) [MSC v.1500 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pandas\r\n>>> pandas.__version__\r\n\'0.9.0\''"
2055,7513067,breisfeld,jreback,2012-10-11 13:39:21,2013-09-21 12:16:59,2013-09-21 12:16:59,closed,,0.13,5,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/2055,b'tests hang on Windows 7',"b""OS: Windows 7\r\n\r\n```\r\n$: git describe\r\nv0.7.0rc1-1778-g9d4f557\r\n```\r\n\r\n```\r\n$: python\r\nEnthought Python Distribution -- www.enthought.com\r\nVersion: 7.1-1 (32-bit)\r\n\r\n>>> import tables\r\n>>> tables.__version__\r\n'2.4.0'\r\n>>>\r\n```\r\n\r\n```\r\n$: nosetests -v pandas\r\n...\r\ntest_verbose_import (pandas.io.tests.test_parsers.TestParsers) ... ok\r\ntest_xlsx_table (pandas.io.tests.test_parsers.TestParsers) ... ok\r\ntest_append (pandas.io.tests.test_pytables.TestHDFStore) ...\r\n```\r\n\r\nThe tests then hang and the python process quits. \r\n"""
2051,7474110,lodagro,wesm,2012-10-10 07:46:56,2012-10-12 23:38:13,2012-10-12 23:36:01,closed,,0.9.1,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/2051,b'Series repr failure when name is tuple holding non-string type.',"b'```python\r\nIn [140]: s = pd.Series(list(\'abcd\'), name=(\'foo\', 1))\r\n\r\nIn [141]: s\r\nOut[141]: ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n...\r\n\r\n.../pandas/core/format.pyc in _get_footer(self)\r\n     86                     series_name = self.series.name\r\n     87                 elif isinstance(self.series.name, tuple):\r\n---> 88                     series_name = ""(\'%s\')"" % ""\', \'"".join(self.series.name)\r\n     89                 else:\r\n     90                     series_name = str(self.series.name)\r\nTypeError: sequence item 1: expected string, int found\r\n```\r\n\r\nFound the issue when indexing a MultiIndexed frame.\r\n\r\n```python\r\n\r\nIn [142]: mi = pd.MultiIndex.from_tuples([(\'foo\', 1), (\'foo\', 2)])\r\n\r\nIn [143]: df = pd.DataFrame(np.random.randn(2,2), index=mi)\r\n\r\nIn [144]: df.ix[(\'foo\', 1)]\r\nOut[144]: ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n```'"
2047,7445990,changhiskhan,wesm,2012-10-09 12:36:43,2012-11-18 18:54:52,2012-11-18 18:54:52,closed,changhiskhan,0.9.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/2047,b'DataFrame.iteritems bug for nonunique columns',b'have two choices:\r\n\r\n1. iterate over unique columns and returns self[col]\r\n2. iterate over each column entry and return self.icol(i)\r\n\r\n\\#1 is better for a lot of current cases in using iteritems to reconstruct a dict. #2 seems more natural though. \r\n\r\n@wesm @lodagro any opinions?'
2045,7434454,changhiskhan,changhiskhan,2012-10-08 23:36:03,2012-11-19 01:45:18,2012-11-19 00:20:51,closed,,0.10,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2045,b'ordered_merge output out of order',"b'```python\r\nIn [31]: df1\r\nOut[31]: \r\n                 date       val\r\n0 2012-01-01 00:00:00 -0.199984\r\n1 2012-01-04 00:00:00  1.175552\r\n2 2012-01-07 00:00:00  0.190748\r\n3 2012-01-10 00:00:00  0.993547\r\n4 2012-01-13 00:00:00  0.582836\r\n\r\nIn [32]: df2\r\nOut[32]: \r\n                 date       val\r\n0 2012-01-02 00:00:00  0.701435\r\n1 2012-01-05 00:00:00  0.825832\r\n2 2012-01-08 00:00:00  0.298673\r\n3 2012-01-11 00:00:00  1.554882\r\n4 2012-01-14 00:00:00  0.990432\r\n\r\nIn [33]: pd.ordered_merge(df1, df2)\r\nOut[33]: \r\n                 date       val\r\n0 2012-01-13 00:00:00  0.582836\r\n1 2012-01-14 00:00:00  0.990432\r\n2 2012-01-04 00:00:00  1.175552\r\n3 2012-01-05 00:00:00  0.825832\r\n4 2012-01-07 00:00:00  0.190748\r\n5 2012-01-08 00:00:00  0.298673\r\n6 2012-01-10 00:00:00  0.993547\r\n7 2012-01-11 00:00:00  1.554882\r\n8 2012-01-01 00:00:00 -0.199984\r\n9 2012-01-02 00:00:00  0.701435\r\n```'"
2042,7432285,ludaavics,wesm,2012-10-08 21:41:52,2012-10-31 21:02:21,2012-10-31 21:02:21,closed,,0.9.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2042,"b""Dot doesn't accept arrays""","b'    In [93]: a = rand(1,5)\r\n\r\n    In [94]: b = rand(5,1)\r\n\r\n    In [95]: A = pd.DataFrame(a)\r\n\r\n    In [96]: B = pd.DataFrame(b)\r\n\r\n    In [97]: a.dot(b)\r\n    Out[97]: array([[ 1.50425267]])\r\n\r\n    In [98]: A.dot(B)\r\n    Out[98]: \r\n\r\n\r\n    0\r\n    0\r\n    1.504253\r\n     \r\n\r\n    In [99]: a.dot(B)\r\n    Out[99]: array([[ 1.50425267]])\r\n\r\n    In [100]: A.dot(b)\r\n    ---------------------------------------------------------------------------\r\n    AttributeError                            Traceback (most recent call last)\r\n    D:\\Projects\\moc\\<ipython-input-100-2a3d67ed3e0f> in <module>()\r\n    ----> 1 A.dot(b)\r\n\r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in dot(self, other)\r\n        722         """"""\r\n        723         lvals = self.values\r\n    --> 724         rvals = other.values\r\n        725         result = np.dot(lvals, rvals)\r\n        726         return DataFrame(result, index=self.index, columns=other.columns)\r\n\r\n    AttributeError: \'numpy.ndarray\' object has no attribute \'values\'\r\n\r\nI would guess replace `rvals = other.values` with\r\n\r\n    try:\r\n        rvals = other.values\r\n    except AttributeError:\r\n        pass\r\n\r\nunless you specifically intended for this to fail?\r\n\r\nThanks,\r\nLudovic'"
2041,7428804,lodagro,wesm,2012-10-08 19:19:54,2012-11-01 14:31:52,2012-11-01 14:10:01,closed,,0.9.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2041,b'read_csv treats both inf and -inf as very large negative number',"b'from [mailing list](https://groups.google.com/forum/#!topic/pydata/Wy3eBEnoDrM), same discussion led to #2026\r\n\r\n```python\r\n\r\nIn [42]: data = """"""\\\r\n   ....: ,A\r\n   ....: a,inf\r\n   ....: b,-inf\r\n   ....: """"""\r\n\r\nIn [43]: df = pd.read_csv(StringIO(data), index_col=0)\r\n\r\nIn [44]: df\r\nOut[44]: \r\n                     A\r\na -9223372036854775808\r\nb -9223372036854775808\r\n\r\nIn [45]: df.dtypes\r\nOut[45]: A    int64\r\n```'"
2039,7422701,CerebralMastication,CerebralMastication,2012-10-08 15:08:41,2012-10-09 14:09:21,2012-10-09 14:09:21,closed,,,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2039,b'kde plots produce unexpected behavior with integers',"b""If we feed a series of integers into the kde plot the results are a horizontal line:\r\n\r\n    newy = pandas.Series([10,26,36,64,69,73,76,79,80,81,81,82,83,86,89,93,98,126,136,152])\r\n    newy.plot(kind='kde')\r\n\r\nHowever, if the series is floats, instead of ints, things work as expected:\r\n\r\n    newy = pandas.Series([10.0,26,36,64,69,73,76,79,80,81,81,82,83,86,89,93,98,126,136,152])\r\n    newy.plot(kind='kde')\r\n\r\nI originally noticed the behavior using `scatter_matrix(..., diagonal='kde')` which uses the same plotting engine, I presume. """
2033,7404224,janschulz,jreback,2012-10-07 11:40:41,2014-07-08 11:43:03,2013-12-18 20:09:34,closed,,0.13.1,5,API Design;Bug;Indexing;Prio-high,https://api.github.com/repos/pydata/pandas/issues/2033,b'.ix does not warn when selecting a none-existing column',"b'```\r\ndf3 = DataFrame({""a"":[1,2,3,4], ""b"":[1,2,3,4]})\r\ndf3.ix[:,[""b"",""c""]]\r\nOut[1]: \r\n   b   c\r\n0  1 NaN\r\n1  2 NaN\r\n2  3 NaN\r\n3  4 NaN\r\n```\r\nI would have (had...) expected that the `df3.ix[:,[""b"",""c""]]` will throw an error :-('"
2031,7400645,janschulz,wesm,2012-10-06 21:33:59,2013-01-20 00:13:43,2013-01-20 00:13:43,closed,,0.10.1,3,Bug;Stats,https://api.github.com/repos/pydata/pandas/issues/2031,b'pearson corr can throw IndexError',"b'I just got this from a ipython parallel run (so unfortunately no reproduceable case):\r\n\r\n```\r\nIndexError                               Traceback (most recent call last)<string>in <module>()\r\nC:\\data\\phd\\montecarlo-paper\\functions.py in simulate(inputs)\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in corr(self, method)\r\n   3355                valid=mask[i]&mask[j]\r\n   3356                if not valid.all():\r\n-> 3357                     c=corrf(ac[valid],bc[valid])\r\n   3358                else:\r\n   3359                    c=corrf(ac,bc)\r\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\nanops.pyc in _pearson(a, b)\r\n    328\r\n    329    def _pearson(a,b):\r\n--> 330         return np.corrcoef(a,b)[0,1]\r\n    331    def _kendall(a,b):\r\n    332        return kendalltau(a,b)[0]\r\nIndexError: invalid index\r\nIndexError(invalid index)\r\n```\r\n(i had to do some S&R to get the stacktrace into this form, so hopefully I didn\'t reformat/remove anything)\r\n\r\nI\'m not sure under which circumstance np.corrcoef does not return an array, but if so pandas should return ""nan"" in that case.'"
2025,7377047,snth,wesm,2012-10-05 12:43:36,2012-10-06 17:49:10,2012-10-06 17:49:04,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2025,b'datetime64[ns] index values output incorrect in DataFrame.to_html()',"b'Please see the example below:\r\n\r\n```python\r\nIn [39]: df = pd.DataFrame({\'a\': np.arange(3)}, index=pd.date_range(\'2010-01-01\', periods=3, freq=\'B\'))\r\n\r\nIn [40]: print df\r\n            a\r\n2010-01-01  0\r\n2010-01-04  1\r\n2010-01-05  2\r\n\r\nIn [41]: print df.to_html()\r\n<table border=""1"" class=""dataframe"">\r\n  <thead>\r\n    <tr style=""text-align: right;"">\r\n      <th></th>\r\n      <th>a</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <td><strong>1970-01-15 48:00:00</strong></td>\r\n      <td> 0</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>1970-01-15 120:00:00</strong></td>\r\n      <td> 1</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>1970-01-15 144:00:00</strong></td>\r\n      <td> 2</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\nIn [42]: print pd.version.version\r\n0.9.0.dev-a884949\r\n\r\nIn [43]: print np.version.version\r\n1.6.1\r\n\r\nIn [44]: df.index.dtype\r\nOut[44]: dtype(\'datetime64[ns]\')\r\n```\r\n'"
2024,7373596,gerigk,wesm,2012-10-05 09:33:52,2012-11-28 03:10:37,2012-11-28 03:07:50,closed,,0.10,2,Bug,https://api.github.com/repos/pydata/pandas/issues/2024,b'pd.merge fails if columns of only one side are hierarchical (even if index is equal)',"b'```\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame([(1,2,3), (4,5,6)], columns = [\'a\',\'b\',\'c\'])\r\nnew_df = df.groupby([\'a\']).agg({\'b\': [np.mean, np.sum]})\r\nother_df = df = pd.DataFrame([(1,2,3), (7,10,6)], columns = [\'a\',\'b\',\'d\'])\r\nother_df.set_index(\'a\', inplace=True)\r\nprint new_df\r\nprint other_df\r\npd.merge(new_df, other_df, left_index=True, right_index=True)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-11-4c9da13e85ff> in <module>()\r\n      7 print new_df\r\n      8 print other_df\r\n----> 9 pd.merge(new_df, other_df, left_index=True, right_index=True)\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy)\r\n     31                          right_index=right_index, sort=sort, suffixes=suffixes,\r\n     32                          copy=copy)\r\n---> 33     return op.get_result()\r\n     34 if __debug__: merge.__doc__ = _merge_doc % \'\\nleft : DataFrame\'\r\n     35 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in get_result(self)\r\n    181 \r\n    182         # this is a bit kludgy\r\n--> 183         ldata, rdata = self._get_merge_data()\r\n    184 \r\n    185         # TODO: more efficiently handle group keys to avoid extra consolidation!\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/tools/merge.pyc in _get_merge_data(self)\r\n    271         lsuf, rsuf = self.suffixes\r\n    272         ldata, rdata = ldata._maybe_rename_join(rdata, lsuf, rsuf,\r\n--> 273                                                 copydata=False)\r\n    274         return ldata, rdata\r\n    275 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in _maybe_rename_join(self, other, lsuffix, rsuffix, copydata)\r\n   1115 \r\n   1116     def _maybe_rename_join(self, other, lsuffix, rsuffix, copydata=True):\r\n-> 1117         to_rename = self.items.intersection(other.items)\r\n   1118         if len(to_rename) > 0:\r\n   1119             if not lsuffix and not rsuffix:\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/index.pyc in intersection(self, other)\r\n   2270         Index\r\n   2271         """"""\r\n-> 2272         self._assert_can_do_setop(other)\r\n   2273 \r\n   2274         if self.equals(other):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/index.pyc in _assert_can_do_setop(self, other)\r\n   2319             if len(other) == 0:\r\n   2320                 return True\r\n-> 2321             raise TypeError(\'can only call with other hierarchical \'\r\n   2322                             \'index objects\')\r\n   2323 \r\n\r\nTypeError: can only call with other hierarchical index objects\r\n\r\n      b     \r\n   mean  sum\r\na           \r\n1     2    2\r\n4     5    5\r\n    b  d\r\na       \r\n1   2  3\r\n7  10  6\r\n```\r\nalso this fails\r\n```\r\npd.merge(new_df.reset_index(), other_df.reset_index(), left_index=False, right_index=False, left_on =(\'a\', \'\'), right_on =\'a\' )\r\n```\r\n\r\nand this\r\n\r\n```\r\npd.merge(new_df.reset_index(), other_df.reset_index(), left_index=False, right_index=False, left_on =[(\'a\', \'\')], right_on =[\'a\'] )\r\n```'"
2021,7365109,eheisman,wesm,2012-10-04 22:40:03,2012-10-05 01:53:14,2012-10-05 01:53:14,closed,,0.9,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2021,b'Timestamp.dayofyear method returns day of month.',"b""I noticed this while doing some work where I'm filtering values based on day of year:  The dayofyear method of the Timestamp class returns the day of the month, not the day of the year.  I realize there are a number of ways to compute this, none of which is very fast, but it could be changed to return the difference, in days, between it's own value and 0000 01 January of the current year.\r\n\r\nIn pandas/src/datetime.pyx, the problematic code is, starting on line 147/148, in the definition of the function."""
2018,7360911,gerigk,changhiskhan,2012-10-04 19:44:28,2014-08-07 10:32:22,2012-11-03 19:21:36,closed,,0.9.1,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/2018,b'Confusing pd.to_datetime behavior',"b""The docstring says\r\n\r\nParameters\r\n----------\r\narg : string, datetime, array of strings (with possible NAs)\r\n\r\nWithout any date format option.\r\n\r\nI do get some weird output though:\r\n\r\n```\r\nIn [4]:\r\n\r\npd.to_datetime('121001')\r\nOut[4]:\r\ndatetime.datetime(2012, 10, 1, 0, 0)\r\n\r\nIn [5]:\r\n\r\npd.to_datetime('121001', dayfirst=True)\r\nOut[5]:\r\ndatetime.datetime(2012, 10, 1, 0, 0)\r\n\r\nIn [6]:\r\n\r\npd.to_datetime(['121001'])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-e09fc721e662> in <module>()\r\n----> 1 pd.to_datetime(['121001'])\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/tseries/tools.pyc in to_datetime(arg, errors, dayfirst, utc, box)\r\n    106 \r\n    107         try:\r\n--> 108             return _convert_f(arg)\r\n    109         except ValueError:\r\n    110             raise\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/tseries/tools.pyc in _convert_f(arg)\r\n     80                 return DatetimeIndex._simple_new(values, None, tz=tz)\r\n     81             except (ValueError, TypeError):\r\n---> 82                 raise e\r\n     83 \r\n     84     if arg is None:\r\n\r\nValueError: Out of bounds nanosecond timestamp: 121001-01-01 00:00:00\r\n```\r\n"""
2017,7360796,gerigk,wesm,2012-10-04 19:39:27,2012-10-05 07:14:46,2012-10-05 00:41:37,closed,,0.9,4,Bug,https://api.github.com/repos/pydata/pandas/issues/2017,b'reset_index fails with MultiIndex in columns',"b'```\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame([(1,2,3), (4,5,6)], columns = [\'a\',\'b\',\'c\'])\r\ndf.groupby([\'a\']).agg({\'b\': [np.mean, np.sum]}).reset_index()\r\n\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-1-a362e367701d> in <module>()\r\n      2 import numpy as np\r\n      3 df = pd.DataFrame([(1,2,3), (4,5,6)], columns = [\'a\',\'b\',\'c\'])\r\n----> 4 df.groupby([\'a\']).agg({\'b\': [np.mean, np.sum]}).reset_index()\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in reset_index(self, level, drop, inplace)\r\n   2522             if name is None or name == \'index\':\r\n   2523                 name = \'index\' if \'index\' not in self else \'level_0\'\r\n-> 2524             new_obj.insert(0, name, _maybe_cast(self.index.values))\r\n   2525 \r\n   2526         new_obj.index = new_index\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in insert(self, loc, column, value)\r\n   1857         """"""\r\n   1858         value = self._sanitize_column(column, value)\r\n-> 1859         self._data.insert(loc, column, value)\r\n   1860 \r\n   1861     def _sanitize_column(self, key, value):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/internals.pyc in insert(self, loc, item, value)\r\n    899             raise Exception(\'cannot insert %s, already exists\' % item)\r\n    900 \r\n--> 901         new_items = self.items.insert(loc, item)\r\n    902         self.set_items_norename(new_items)\r\n    903 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0rc2-py2.7-linux-x86_64.egg/pandas/core/index.pyc in insert(self, loc, item)\r\n   2340         if not isinstance(item, tuple) or len(item) != self.nlevels:\r\n   2341             raise Exception(""%s cannot be inserted in this MultiIndex""\r\n-> 2342                             % str(item))\r\n   2343 \r\n   2344         new_levels = []\r\n\r\nException: a cannot be inserted in this MultiIndex\r\n```\r\n\r\n'"
2011,7306610,jseabold,wesm,2012-10-02 21:35:48,2012-10-05 00:55:29,2012-10-05 00:55:19,closed,,0.9,1,Bug;Docs,https://api.github.com/repos/pydata/pandas/issues/2011,b'io.data._sanitize_dates behavior vs. docs',"b'Docs say default start for DataReader should be 2010-1-1, but it returns 1 year ago from today. I prefer the docs. Thoughts?\r\n\r\n```\r\n>>> from pandas.io.data import _sanitize_dates\r\n>>> _sanitize_dates(None, None)                                                               \r\n(datetime.datetime(2011, 10, 3, 17, 32, 52, 969119),\r\n datetime.datetime(2012, 10, 2, 17, 32, 52, 969163))\r\n```'"
2010,7302421,lodagro,jreback,2012-10-02 19:09:13,2015-11-23 08:23:51,2014-03-09 15:21:07,closed,,0.14.0,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/2010,b'Setting the x margin in a plot of a data frame with a DatetimeIndex is ignored.',"b""from [mailing list](https://groups.google.com/forum/?fromgroups=#!searchin/pystatsmodels/pandas$20plot$20with$20DatetimeIndex$20ignore$20x$20axis$20margin/pystatsmodels/9ZQkzoSibSE/jvU35qVeasUJ)\r\n\r\nHello,\r\nI noticed that setting the x margin in a plot of a data frame with a DatetimeIndex is ignored.\r\n\r\nExample plot: \r\n\r\n![Example plot](http://i.imgur.com/ZOqmJ.png)\r\n\r\n```python\r\n#!/usr/bin/python\r\n\r\nfrom matplotlib.pyplot import *\r\nfrom pandas import *\r\nfrom pylab import show\r\n\r\nfig, axes = subplots(2,1)\r\n\r\ndf1 = DataFrame([0.5,1,1.5,2,2.5,3,3.5])\r\ndf1.plot(ax=axes[0],drawstyle='steps',style='g')\r\n\r\n# this x axis margin is honored\r\naxes[0].margins(0.05,0.05)\r\n\r\n\r\ndf2 = DataFrame([0.5,1.5,2.5,3.5],index=date_range('1-1-2000',periods=4))\r\ndf2.plot(ax=axes[1],drawstyle='steps',style='r')\r\n\r\n# this x axis margin is ignored\r\naxes[1].margins(0.05,0.05)\r\n\r\nshow()\r\n```"""
2004,7273343,jseabold,wesm,2012-10-01 19:26:36,2012-11-03 18:08:58,2012-11-03 18:08:58,closed,,,3,Bug,https://api.github.com/repos/pydata/pandas/issues/2004,b'Series slicing with None returns array?',"b'Is this a bug? Definitely was unexpected.\r\n\r\n```\r\ny = pandas.Series(np.random.random(10))\r\ntype(y[:,None])\r\n```\r\n'"
1993,7244635,ldkphd,wesm,2012-09-30 10:47:35,2012-10-01 01:20:22,2012-10-01 01:19:11,closed,,0.9,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1993,b'date_range export to csv weird behavior',"b'Hello,\r\n\r\nWorking with 0.8.1\r\n\r\ndate_range works nicely, I am using the technique to reduce sparse GPS measurements to a fixed time step.\r\n\r\nprocedure:\r\nstep 1: create date_range (""10S""), all datetime within a single day (input from gpx)\r\nstep 2: outer join with original data\r\nstep 3: interpolate the locations\r\n\r\nEverything works fine, .to_string() results in results as expected. \r\n\r\nWhen I export the result to_csv(), the date_range object is exported with the correct datetime, but the date part is reduced to to january 1970. How is this possible?\r\n\r\nCan somebody get me on the road again?\r\n\r\nLuc\r\n\r\n\r\n'"
1990,7239081,jseabold,wesm,2012-09-29 17:31:42,2012-09-29 21:04:45,2012-09-29 21:03:14,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1990,b'NaN comparison in merge',"b'Parking this here so I don\'t forget about it. I assume that the NaNs here are not comparing equal, so we get ""duplicate"" rows on a merge, which I think is a bug.\r\n\r\n```\r\ndata = [[1950, ""A"", 1.5],\r\n        [1950, ""B"", 1.5],\r\n        [1955, ""B"", 1.5],\r\n        [1960, ""B"", np.nan],\r\n        [1970, ""B"", 4.],\r\n        [1950, ""C"", 4.],\r\n        [1960, ""C"", np.nan],\r\n        [1965, ""C"", 3.],\r\n        [1970, ""C"", 4.],\r\n        ]\r\nframe = pandas.DataFrame(data, columns=[""year"", ""panel"", ""data""])\r\n\r\nother_data = [[1960, \'A\', np.nan],\r\n [1970, \'A\', np.nan],\r\n [1955, \'A\', np.nan],\r\n [1965, \'A\', np.nan],\r\n [1965, \'B\', np.nan],\r\n [1955, \'C\', np.nan]]\r\nother = pandas.DataFrame(other_data, columns=[\'year\', \'panel\', \'data\'])\r\n\r\ntogether = frame.merge(other, how=""outer"")\r\n```\r\n\r\nWorked around this by filling in the NaNs before the merge, but then I have to convert them back to NaNs later.'"
1987,7218346,lbeltrame,wesm,2012-09-28 15:29:03,2012-09-29 21:33:39,2012-09-29 21:33:39,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1987,"b'When a series contains all strings, converting to dtype ""int"" generates bogus int values'","b'This works:\n\n```python\n\nIn [3]: series = pandas.Series([""car"", ""house"", ""tree"", 1])\n\nIn [4]: series\nOut[4]: \n0      car\n1    house\n2     tree\n3        1\n\nIn [5]: series.astype(""int"")\nValueError: invalid literal for long() with base 10: \'tree\'\n````\n\nThis doesn\'t:\n\n````python\nIn [7]: series = pandas.Series([""car"", ""house"", ""tree"",""1""])\n\nIn [8]: series.astype(""int"")\nOut[8]: \n0                  0\n1           44735616\n2    139883221510192\n3                  1\n````\nThis with RC2 state from git.'"
1982,7187954,y-p,wesm,2012-09-27 15:22:38,2012-10-01 01:53:53,2012-10-01 01:53:53,closed,,0.9,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/1982,b'UnicodeEncodeError in index.summary()',"b"">  idx=pd.Index([u'\\u05d3\\u05e4\\u05e0\\u05d4',u'\\u05d3\\u05e4\\u05e0\\u05d4'])\n> idx.summary()\n\n---------------------------------------------------------------------------\nUnicodeEncodeError                        Traceback (most recent call last)\n<ipython-input-135-26bf122d0c51> in <module>()\n      1 idx=pd.Index([u'\\u05d3\\u05e4\\u05e0\\u05d4',u'\\u05d3\\u05e4\\u05e0\\u05d4'])\n----> 2 idx.summary()\n      3 \n\n/usr/local/lib/python2.7/dist-packages/pandas-0.9.0.dev_37f4ded-py2.7-linux-x86_64.egg/pandas/core/index.pyc in summary(self, name)\n    197     def summary(self, name=None):\n    198         if len(self) > 0:\n--> 199             index_summary = ', %s to %s' % (str(self[0]), str(self[-1]))\n    200         else:\n    201             index_summary = ''\n\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-3: ordinal not in range(128)"""
1979,7164420,sunfaquir,wesm,2012-09-26 21:59:04,2012-11-03 17:08:52,2012-11-03 17:08:52,closed,wesm,0.9.1,6,Bug,https://api.github.com/repos/pydata/pandas/issues/1979,b'qcut bin formatting bugs',"b""I'm reading CSV tables and then use qcut to bin the continuous numbers. Given the number of bins, most of the time the qcut works fine. However, some times, it doesn't function correctly. The following are some weird cases I got in the result levels index. Sorry I can't give the experimental data  now because I'm running it on a very large data collections and currently I'm not allowed to public the data\r\n\r\n(1) When dealing with negative numbers, sometimes it gives [-117.-1,  1], This happens at the first bin index in my test case.\r\n(2) when bin# is two, it gives results like [1, 2005], (2005, 2005]\r\n(3)a bin result like this:  please note the value 0.993, and value 0.1\r\narray([[0.6, 0.991], (0.991, 0.992], (0.992, 0.993], (0.993, 0.993],\r\n       (0.993, 0.994], (0.994, 0.995], (0.995, 0.996], (0.996, 0.997],\r\n       (0.997, 0.998], (0.998, 0.1], (0.1, 1.2], (1.2, 1.5], (1.5, 2.1],\r\n       (2.1, 3.5], (3.5, 5.2], (5.2, 7.1], (7.1, 8.5], (8.5, 11],\r\n       (11, 13.7], (13.7, 65.8]], dtype=object)\r\n\r\n(2) and (3) might be caused by the same reason\r\n\r\nI'm using pandas 0.8.1 thanks.\r\n"""
1978,7163953,wesm,wesm,2012-09-26 21:41:17,2013-03-28 05:20:38,2013-03-28 05:19:19,closed,wesm,0.11,5,Bug,https://api.github.com/repos/pydata/pandas/issues/1978,b'qcut user-reported failure',"b""from pystatsmodels mailing list\n\n```\nqcut in pandas 0.8.1 is failing for some quantile lists but not others (see below). Sorry if I'm missing something...\n\ntype(F.g)\n\nOut[3]:\npandas.core.series.TimeSeries\n\nchg = (F.g[20:]-F.g[20:].shift(1))\nfac = qcut(chg, [0, .5, 1])\nfac\n\nOut[4]:\nCategorical: g\narray([nan, [-1005094.81, 0], [-1005094.81, 0], ..., (0, 1478547.3],\n       (0, 1478547.3], (0, 1478547.3]], dtype=object)\nLevels (2): Index([[-1005094.81, 0], (0, 1478547.3]], dtype=object)\n\nchg = (F.g[20:]-F.g[20:].shift(1))\nfac = qcut(chg, [0, .5, 1])\nfac\n\nOut[5]:\nCategorical: g\narray([nan, [-1005094.81, 0], [-1005094.81, 0], ..., (0, 1478547.3],\n       (0, 1478547.3], (0, 1478547.3]], dtype=object)\nLevels (2): Index([[-1005094.81, 0], (0, 1478547.3]], dtype=object)\n\nfac = qcut(chg, [0, .5, .75, 1])\nfac\n\nOut[6]:\nCategorical: g\narray([nan, [-1005094.81, 0], [-1005094.81, 0], ..., (0, 1478547.3],\n       (0, 1478547.3], (0, 1478547.3]], dtype=object)\nLevels (3): Index([[-1005094.81, 0], (0, 0], (0, 1478547.3]], dtype=object)\n\nfac = qcut(chg, [0, .25, .5, .75, 1])\nfac\n\nOut[7]:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/birone/<ipython-input-9-3896feb36dcd> in <module>()\n----> 1 fac = qcut(chg, [0, .25, .5, .75, 1])\n      2 fac\n\n/usr/lib/pymodules/python2.7/pandas/tools/tile.pyc in qcut(x, q, labels, retbins, precision)\n    139     bins = algos.quantile(x, quantiles)\n    140     return _bins_to_cuts(x, bins, labels=labels, retbins=retbins,\n--> 141                          precision=precision, include_lowest=True)\n    142 \n    143 \n\n/usr/lib/pymodules/python2.7/pandas/tools/tile.pyc in _bins_to_cuts(x, bins, right, labels, retbins, precision, name, include_lowest)\n    177         levels = np.asarray(levels, dtype=object)\n    178         np.putmask(ids, na_mask, 0)\n--> 179         fac = Categorical(ids - 1, levels, name=name)\n    180     else:\n    181         fac = ids - 1\n\n/usr/lib/pymodules/python2.7/pandas/core/categorical.pyc in __init__(self, labels, levels, name)\n     43     def __init__(self, labels, levels, name=None):\n     44         self.labels = labels\n---> 45         self.levels = levels\n     46         self.name = name\n     47 \n\n/usr/lib/pymodules/python2.7/pandas/core/categorical.pyc in _set_levels(self, levels)\n     62         levels = _ensure_index(levels)\n     63         if not levels.is_unique:\n---> 64             raise ValueError('Categorical levels must be unique')\n     65         self._levels = levels\n     66 \n\nValueError: Categorical levels must be unique\n\nfac = qcut(chg, n) fails with the same error for n>3\n```"""
1977,7163740,ijmcf,wesm,2012-09-26 21:32:15,2012-09-26 23:08:28,2012-09-26 23:08:28,closed,,0.9,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1977,b'PeriodIndex slicing by datetime fails when either end out-of-bounds',"b""pi = pandas.PeriodIndex(start='2012-01-01', periods=10, freq='W-MON')\nts = pandas.TimeSeries(range(len(pi)), index=pi)\n\ndt1 = datetime.datetime(2011, 10, 2)\ndt2 = datetime.datetime(2012, 1, 2)\ndt3 = datetime.datetime(2012, 2, 10)\ndt4 = datetime.datetime(2012, 4, 20)\n\nts[dt2: dt3]\nts[dt1: dt3]\nts[dt2: dt4]\n\nThe first slice works, the second and third do not. It looks like the code that handles 'out-of-bounds' slices is failing in this case."""
1976,7155535,elliottwaldron,wesm,2012-09-26 16:53:22,2013-06-03 13:14:02,2012-11-02 22:30:04,closed,wesm,0.9.1,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1976,b'regex filter in DataFrame.filter',"b'Hi pandas folks - \n\nBack to using pandas after having to take a hiatus in MATLAB.  One minor problem I ran into was the use of re.match instead of re.search (as is documented) in the DataFrame.filter method.   \n\nI assumed there was a good reason for the change, but for most of my use cases the re.search way was less verbose.   \n\nWe should update the method documentation anyway, but could you see supporting both re.match and re.search?\n\nExcellent package and am using in on a daily basis for both research and production code.\n\nBest,\n\nElliott   '"
1975,7151568,jseabold,changhiskhan,2012-09-26 14:37:15,2012-09-26 21:50:52,2012-09-26 21:40:33,closed,,0.9,3,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1975,b'read_table and encoding',"b'What in the world is going on here? Shouldn\'t these both give unicode?\n\n```python\nimport pandas\nfrom StringIO import StringIO\nfin = StringIO(""aski, Jan;1"")\ndf1 = pandas.read_table(fin, sep="";"", encoding=""utf-8"", header=None)\nfin.seek(0)\ndf2 = pandas.read_csv(fin, sep="";"", encoding=""utf-8"", header=None)\n\ntype(df1[""X.1""].values[0])\ntype(df2[""X.1""].values[0])\n```'"
1971,7145605,hayd,wesm,2012-09-26 09:52:21,2012-09-27 14:40:12,2012-09-27 00:40:07,closed,,0.9,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1971,b'set_index breaks with multi keys but one empty',"b'Migrated from StackOverflow: http://stackoverflow.com/questions/12598520/set-index-on-multiple-columns-with-one-empty-column\n\n```\ndf = DataFrame([\n    dict(a=1, p=0), \n    dict(a=2, m=10), \n    dict(a=3, m=11, p=20), \n    dict(a=4, m=12, p=21)\n], columns=(\'a\', \'m\', \'p\', \'x\'))\n```\n\n```\n     a     m    p     x\n  0  1   NaN    0   NaN\n  1  2    10  NaN   NaN\n  2  3    11   20   NaN\n  3  4    12   21   NaN\n```\n```\n# single column index on an empty column works\ndf.set_index([\'x\'])\n# two-columns index on non-empty columns works\ndf.set_index([\'a\', \'m\'])\ndf.set_index([\'a\', \'p\'])\ndf.set_index([\'m\', \'p\'])\n\n# but two-columns index including an empty column fails\ndf.set_index([\'a\', \'x\'])\n```\n\n```\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Library/Python/2.7/site-packages/pandas-0.8.2.dev_f5a74d4_20120725-py2.7-macosx-10.8-x86_64.egg/pandas/core/frame.py"", line 2328, in set_index\n    if verify_integrity and not index.is_unique:\n  File ""properties.pyx"", line 27, in pandas.lib.cache_readonly.__get__ (pandas/src/tseries.c:95395)\n  File ""/Library/Python/2.7/site-packages/pandas-0.8.2.dev_f5a74d4_20120725-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.py"", line 227, in is_unique\n    return self._engine.is_unique\n  File ""engines.pyx"", line 186, in pandas.lib.IndexEngine.is_unique.__get__ (pandas/src/tseries.c:115047)\n  File ""engines.pyx"", line 215, in pandas.lib.IndexEngine._do_unique_check (pandas/src/tseries.c:115456)\n  File ""engines.pyx"", line 228, in pandas.lib.IndexEngine._ensure_mapping_populated (pandas/src/tseries.c:115629)\n  File ""engines.pyx"", line 231, in pandas.lib.IndexEngine.initialize (pandas/src/tseries.c:115678)\n  File ""engines.pyx"", line 212, in pandas.lib.IndexEngine._get_index_values (pandas/src/tseries.c:115414)\n  File ""/Library/Python/2.7/site-packages/pandas-0.8.2.dev_f5a74d4_20120725-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.py"", line 247, in <lambda>\n    return self._engine_type(lambda: self.values, len(self))\n  File ""/Library/Python/2.7/site-packages/pandas-0.8.2.dev_f5a74d4_20120725-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.py"", line 1363, in values\n    for lev, lab in zip(self.levels, self.labels)]\n  File ""/Library/Python/2.7/site-packages/pandas-0.8.2.dev_f5a74d4_20120725-py2.7-macosx-10.8-x86_64.egg/pandas/core/common.py"", line 348, in ndtake\n    return arr.take(_ensure_platform_int(indexer), axis=axis, out=out)\nIndexError: index -1 is out of bounds for axis 0 with size 0\n```'"
1970,7139788,changhiskhan,changhiskhan,2012-09-26 02:17:22,2012-09-26 21:04:39,2012-09-26 21:04:39,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1970,b'DataFrame fail if columns is non-unique MultiIndex',"b'In [33]: cols2 = pd.MultiIndex.from_tuples(list(itertools.product([\'A\', \'B\', \'C\'], [\'X\', \'Y\', \'Z\']))*2)\n\nIn [34]: df2 = DataFrame(np.random.randn(3, len(cols2)), columns=cols2)\n\nIn [35]: df2\nOut[35]: ---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-35-d4a59695a920> in <module>()\n----> 1 df2\n\n/Users/changshe/code/ipython/IPython/core/displayhook.pyc in __call__(self, result)\n    236             self.start_displayhook()\n    237             self.write_output_prompt()\n--> 238             format_dict = self.compute_format_data(result)\n    239             self.write_format_data(format_dict)\n    240             self.update_user_ns(result)\n\n...\n\n/Users/changshe/code/pandas/pandas/core/format.pyc in _get_formatted_column_labels(self)\n    341             fmt_columns = self.columns.format(sparsify=False, adjoin=False)\n    342             fmt_columns = zip(*fmt_columns)\n--> 343             dtypes = self.frame.dtypes.values\n    344             need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))\n    345             str_columns = zip(*[[\' \' + y\n\n/Users/changshe/code/pandas/pandas/core/frame.pyc in __getattr__(self, name)\n   1764             return self[name]\n   1765         raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\n-> 1766                              (type(self).__name__, name))\n   1767 \n   1768     def __setattr__(self, name, value):\n\nAttributeError: \'DataFrame\' object has no attribute \'dtypes\'\n'"
1968,7135790,jseabold,wesm,2012-09-25 22:09:08,2013-01-18 04:44:27,2013-01-18 04:44:27,closed,,,4,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/1968,"b'Unicode, unique, and ndarrays'","b'More unicode fun. I seem to recall seeing this before, but I can\'t find anything about it. Is this numpy, what pandas hands off to numpy, or user error?\n\n```python\ndf = pandas.DataFrame([u""\\xe9""])\ndf[0].unique()\n<snip>\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.py"", line 1409, in array_repr\n    \', \', ""array("")\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/arrayprint.py"", line 445, in array2string\n    separator, prefix, formatter=formatter)\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/arrayprint.py"", line 320, in _array2string\n    _summaryEdgeItems, summary_insert)[:-1]\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/arrayprint.py"", line 491, in _formatArray\n    word = format_function(a[-1])\nUnicodeEncodeError: \'ascii\' codec can\'t encode character u\'\\xe9\' in position 0: ordinal not in range(128)\n```\n```\npandas.version.version\n\'0.9.0rc1\'\n\nnp.version.full_version\n\'1.8.0.dev-578a419\'\n```\n'"
1957,7078191,changhiskhan,changhiskhan,2012-09-24 03:56:04,2012-09-24 05:32:23,2012-09-24 05:32:23,closed,,0.9,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1957,b'reset_index fails if level and drop are both specified',"b""If `drop` is specified then `level` looks like it's ignored and all levels are reset"""
1951,7073809,timmie,jtratner,2012-09-23 18:56:36,2013-09-16 03:51:38,2013-09-16 03:51:38,closed,,Someday,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1951,b'pivot_annual returns null values',"b""The data info\n\n```\nts_monthly.dtype\nOut[41]: dtype('float64')\n\n ts_monthly.shape\nOut[43]: (216,)\n\n ts_monthly.index\nOut[44]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[1994-01-31 00:00:00, ..., 2011-12-31 00:00:00]\nLength: 216, Freq: M, Timezone: None\n\n```\nThe data returns when pivoted to annual null values:\n\n```\nfrom pandas.tseries.util import pivot_annual\n\nts_monthly_ann = pivot_annual(ts_monthly)\n\nts_monthly_ann\nOut[40]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 18 entries, 1994 to 2011\nData columns:\n1     18  non-null values\n2     18  non-null values\n3     18  non-null values\n4     18  non-null values\n5     18  non-null values\n6     18  non-null values\n7     18  non-null values\n8     18  non-null values\n9     18  non-null values\n10    18  non-null values\n11    18  non-null values\n12    18  non-null values\ndtypes: float64(12)\n```\n\nWhat could be wrong?"""
1948,7061661,gerigk,changhiskhan,2012-09-22 17:06:40,2014-04-27 18:51:58,2012-09-26 02:07:43,closed,,0.9,7,Bug,https://api.github.com/repos/pydata/pandas/issues/1948,"b""Release notes don't warn/inform about skip_footer changes""","b'This change ( before, skipping the last row was skip_footer=1, now this is accomplished by skip_footer=-1, otherwise you will only receive one row) should be announced clearly in the release notes imho.'"
1946,7055057,adamgreenhall,wesm,2012-09-21 22:58:18,2012-09-23 17:37:16,2012-09-23 17:37:09,closed,,0.9,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1946,b'hours are wrong for DatetimeIndex with timezone set',"b""Using  ```hours``` on a basic hourly ```DatetimeIndex``` with a time zone gives the incorrect hours. \n\n```\nimport pandas\nfrom pandas.io.parsers import read_csv\ntimezone = 'America/Atikokan'\nfilename = 'forecast.csv'\ndata = read_csv(filename, index_col=0, header=None, squeeze=True)\ndata.index = pandas.DatetimeIndex(data.index).tz_localize(timezone)\nprint data.index[:10]\nprint [str(t) for t in data.index[:10]]\nprint data.index.hour[:10]\n```\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2011-10-02 00:00:00, ..., 2011-10-02 09:00:00]\nLength: 10, Freq: None, Timezone: America/Atikokan\n\n['2011-10-02 00:00:00-05:00', '2011-10-02 01:00:00-05:00', '2011-10-02 02:00:00-05:00', '2011-10-02 03:00:00-05:00', '2011-10-02 04:00:00-05:00', '2011-10-02 05:00:00-05:00', '2011-10-02 06:00:00-05:00', '2011-10-02 07:00:00-05:00', '2011-10-02 08:00:00-05:00', '2011-10-02 09:00:00-05:00']\n\n[ 9  9 19 19 23  0  1  2  3  4]"""
1943,7013605,lodagro,jreback,2012-09-20 14:30:31,2013-04-30 17:49:54,2013-04-30 17:49:54,closed,changhiskhan,0.13,6,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1943,b'Selecting multiple columns from DataFrame with duplicate column labels failure.',"b""```python\nIn [25]: df = pandas.DataFrame(np.random.randn(4,4), columns=list('AABC'))\n\nIn [26]: df\nOut[26]: \n          A         A         B         C\n0 -0.174905  0.332522  1.134984 -0.201270\n1  1.730445  0.382556 -0.607761  1.221815\n2  0.513049  0.196231 -1.746732 -0.252282\n3 -0.297577 -1.000121 -0.090442 -2.129467\n\nIn [27]: df.ix[:,['A', 'B']]\nOut[27]: \n          A         A         B\n0 -0.174905  0.332522  1.134984\n1  1.730445  0.382556 -0.607761\n2  0.513049  0.196231 -1.746732\n3 -0.297577 -1.000121 -0.090442\n\nIn [28]: df[['A', 'B']]\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n...\nException: Reindexing only valid with uniquely valued Index objects\n\nIn [29]: df[['B', 'C']]\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n...\nException: Reindexing only valid with uniquely valued Index objects\n\n```\n"""
1942,7010839,CRP,wesm,2012-09-20 12:56:09,2012-09-20 19:15:21,2012-09-20 19:15:21,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1942,b'assignment with index broken',"b""```python\na=DataFrame(randn(20,2),index=[chr(x+65) for x in range(20)])\n\na.ix[-1]=a.ix[-2]\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-49-79d0c89159f2> in <module>()\n----> 1 a.ix[-1]=a.ix[-2]\n\n/usr/local/lib/python2.7/site-packages/pandas/core/indexing.pyc in __setitem__(self, key, value)\n     80             indexer = self._convert_to_indexer(key)\n     81 \n---> 82         self._setitem_with_indexer(indexer, value)\n     83 \n     84     def _convert_tuple(self, key):\n\n/usr/local/lib/python2.7/site-packages/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\n    134                 value = self._align_frame(indexer, value)\n    135 \n--> 136             self.obj.values[indexer] = value\n    137 \n    138     def _align_series(self, indexer, ser):\n\nValueError: operands could not be broadcast together with shapes (2) (20) \n\n pandas.__version__\nOut[51]: '0.9.0.dev-ba93669'\n```"""
1941,7004759,gerigk,lodagro,2012-09-20 08:32:31,2012-09-20 16:12:36,2012-09-20 09:02:29,closed,,0.9,0,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/1941,"b'nosetest errors when ""xlrd"" isn\'t installed'","b'Maybe this is because I have openpyxl but not xlrd installed.\n\nAnyway, I guess the tests should be skipped instead of throwing an error.\n\n```\n======================================================================\nERROR: test_parse_cols_int (pandas.io.tests.test_parsers.TestParsers)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/arthur/python-packages/pandas/pandas/io/tests/test_parsers.py"", line 795, in test_parse_cols_int\n    xls = ExcelFile(pth)\n  File ""/home/arthur/python-packages/pandas/pandas/io/parsers.py"", line 1257, in __init__\n    import xlrd\nImportError: No module named xlrd\n\n======================================================================\nERROR: test_parse_cols_list (pandas.io.tests.test_parsers.TestParsers)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/arthur/python-packages/pandas/pandas/io/tests/test_parsers.py"", line 813, in test_parse_cols_list\n    xlsx = ExcelFile(pth)\n  File ""/home/arthur/python-packages/pandas/pandas/io/parsers.py"", line 1257, in __init__\n    import xlrd\nImportError: No module named xlrd\n\n======================================================================\nERROR: test_to_excel_float_format (pandas.tests.test_frame.TestDataFrame)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/arthur/python-packages/pandas/pandas/tests/test_frame.py"", line 3663, in test_to_excel_float_format\n    reader = ExcelFile(filename)\n  File ""/home/arthur/python-packages/pandas/pandas/io/parsers.py"", line 1257, in __init__\n    import xlrd\nImportError: No module named xlrd\n\n======================================================================\nERROR: test_to_excel_unicode_filename (pandas.tests.test_frame.TestDataFrame)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/arthur/python-packages/pandas/pandas/tests/test_frame.py"", line 3693, in test_to_excel_unicode_filename\n    reader = ExcelFile(filename)\n  File ""/home/arthur/python-packages/pandas/pandas/io/parsers.py"", line 1257, in __init__\n    import xlrd\nImportError: No module named xlrd\n\n----------------------------------------------------------------------\nRan 2372 tests in 140.055s\n\nFAILED (SKIP=18, errors=4)\n```'"
1939,7001578,chrisjbillington,chrisjbillington,2012-09-20 04:09:18,2012-09-21 03:48:04,2012-09-21 03:48:04,closed,,0.10,4,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1939,b'Repeated unpickling of DataFrame results in different data each time',"b""I have a MultiIndexed DataFrame which, when pickled and unpickled, results in some of the data changing.\n\nSpecifically, some of its columns contain data which is a tuple of integers, and these integers are different each unpickling. Here's a DataFrame which shows this problem:\n\nhttp://bec.physics.monash.edu/docs/dataframe.pickle\n\nIt was pickled with pandas 0.9.0.dev-667220a (current git-master) and Python 2.7.3 64 bit.\n\nTry the following to reproduce:\n```python\nimport pickle\ndf = pickle.load(open('dataframe.pickle'))\nprint df['top','roi0']\n```\nRepeating this gives me different data each time. \n\nWhich integers I get seems slightly platform dependent. The tuples contain four integers, and on Linux most rows (but not all) in the DataFrame end up getting three zeros and one nonzero value, and all the nonzero values are somewhat similar, and in the tens of millions. On Windows the data seems to be random integers that are less regular, and without most of them being zeros. The data that was pickled should have four different values, between 0 and 2048 or so (these are definitions of region of interest rects on a CCD camera).\n\nI am unable to reproduce the problem with non-herarchical index DataFrames.\n\nAny clue as to why this might be, or any suggested workarounds? We use different serialisation formats for on-disk persistence, but we've been using pickle for slinging dataframes across networks.\n\n"""
1928,6941792,bf0,wesm,2012-09-18 05:37:22,2012-09-19 02:19:05,2012-09-19 02:19:05,closed,changhiskhan,0.9,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1928,"b'One more  ""\'tzoffset\' object has no attribute \'zone\' ""'","b'**Version:** 0.9.0.dev_de31cbe  (py2.7-macosx-10.6-x86_64)\n\nRelated to #1922 (closed)\n\nI could be the only dummy with this kind of data so I might be able to find a few more of these. \nAlthough I think these are typical Postgres tz-aware timestamps.\n\n\n**Repro**\n````\nfrom StringIO import StringIO\ntable=""""""ts, d1\n2012-05-23 10:00:01.123000-07:00,  3.1459265\n2012-05-23 10:00:02.456000-07:00,  2.7182818\n""""""\ndf = pd.read_table(StringIO(table), parse_dates=True, index_col=\'ts\', sep=\',\')\ndf.resample(\'t\')\n````\n\n**Traceback**\n````\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-90-a0ce8072eb5a> in <module>()\n----> 1 df.resample(\'t\')\n\n   pandas/core/generic.pyc in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base)\n    187                               fill_method=fill_method, convention=convention,\n    188                               limit=limit, base=base)\n--> 189         return sampler.resample(self)\n    190 \n    191     def first(self, offset):\n\n   pandas/tseries/resample.pyc in resample(self, obj)\n     66 \n     67         if isinstance(axis, DatetimeIndex):\n---> 68             rs = self._resample_timestamps(obj)\n     69         elif isinstance(axis, PeriodIndex):\n     70             offset = to_offset(self.freq)\n\n   pandas/tseries/resample.pyc in _resample_timestamps(self, obj)\n    180         axlabels = obj._get_axis(self.axis)\n    181 \n--> 182         binner, grouper = self._get_time_grouper(obj)\n    183 \n    184         # Determine if we\'re downsampling\n\n   pandas/tseries/resample.pyc in _get_time_grouper(self, obj)\n     95 \n     96         if self.kind is None or self.kind == \'timestamp\':\n---> 97             binner, bins, binlabels = self._get_time_bins(axis)\n     98         else:\n     99             binner, bins, binlabels = self._get_time_period_bins(axis)\n\n   pandas/tseries/resample.pyc in _get_time_bins(self, axis)\n    111         first, last = _get_range_edges(axis, self.freq, closed=self.closed,\n    112                                        base=self.base)\n--> 113         binner = labels = DatetimeIndex(freq=self.freq, start=first, end=last)\n    114 \n    115         # a little hack\n\n   pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, **kwds)\n    175         if data is None:\n    176             return cls._generate(start, end, periods, name, offset,\n--> 177                                  tz=tz, normalize=normalize)\n    178 \n    179         if not isinstance(data, np.ndarray):\n\n   pandas/tseries/index.pyc in _generate(cls, start, end, periods, name, offset, tz, normalize)\n    282             end = Timestamp(end)\n    283 \n--> 284         inferred_tz = tools._infer_tzinfo(start, end)\n    285 \n    286         if tz is not None and inferred_tz is not None:\n\n   pandas/tseries/tools.pyc in _infer_tzinfo(start, end)\n     31     tz = None\n     32     if start is not None:\n---> 33         tz = _infer(start, end)\n     34     elif end is not None:\n     35         tz = _infer(end, start)\n\n   pandas/tseries/tools.pyc in _infer(a, b)\n     27         tz = a.tzinfo\n     28         if b and b.tzinfo:\n---> 29             assert(tz.zone == b.tzinfo.zone)\n     30         return tz\n     31     tz = None\n\nAttributeError: \'tzoffset\' object has no attribute \'zone\'\n````'"
1922,6909829,bf0,changhiskhan,2012-09-17 04:00:11,2013-12-04 00:57:52,2012-09-17 23:36:27,closed,,0.9,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1922,b'Handling for FixedOffsetTimezone in tseries/tools.py',"b'**Version:** 0.9.0.dev e98a568-py2.7-macosx-10.6-x86_64\n\nWhen passing a string column with timestamps like `""2012-04-23 10:00:23.123000-07:00""` (fixed offset timezone) to DataFrame.set_index() ,  getting exception:\n\n**AttributeError: \'FixedOffsetTimezone\' object has no attribute \'zone\'**\n\n````\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-4-d8465aa13e80> in <module>()\n----> 1 df.set_index(\'ts\')\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/core/frame.pyc in set_index(self, keys, drop, append, inplace, verify_integrity)\n   2422             arrays.append(level)\n   2423 \n-> 2424         index = MultiIndex.from_arrays(arrays, names=names)\n   2425 \n   2426         if verify_integrity and not index.is_unique:\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/core/index.pyc in from_arrays(cls, arrays, sortorder, names)\n   1551         if len(arrays) == 1:\n   1552             name = None if names is None else names[0]\n-> 1553             return Index(arrays[0], name=name)\n   1554 \n   1555         cats = [Categorical.from_array(arr) for arr in arrays]\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/core/index.pyc in __new__(cls, data, dtype, copy, name)\n    109             if _shouldbe_timestamp(subarr):\n    110                 from pandas.tseries.index import DatetimeIndex\n--> 111                 return DatetimeIndex(subarr, copy=copy, name=name)\n    112 \n    113             if lib.is_period_array(subarr):\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, **kwds)\n    228         else:\n    229             try:\n--> 230                 subarr = tools.to_datetime(data)\n    231             except ValueError:\n    232                 # tz aware\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/tseries/tools.pyc in to_datetime(arg, errors, dayfirst, utc, box)\n     90         if isinstance(arg, list):\n     91             arg = np.array(arg, dtype=\'O\')\n---> 92         result = _convert_f(arg)\n     93         return result\n     94     try:\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/tseries/tools.pyc in _convert_f(arg)\n     75         except ValueError, e:\n     76             try:\n---> 77                 values, tz = lib.datetime_to_datetime64(arg)\n     78                 return DatetimeIndex._simple_new(values, None, tz=tz)\n     79             except (ValueError, TypeError):\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/lib.so in pandas.lib.datetime_to_datetime64 (pandas/src/tseries.c:38359)()\n\n/Users/redacted/.pythonbrew/venvs/Python-2.7.3/redacted/lib/python2.7/site-packages/pandas-0.9.0.dev_e98a568-py2.7-macosx-10.6-x86_64.egg/pandas/lib.so in pandas.lib._get_zone (pandas/src/tseries.c:37262)()\n\nAttributeError: \'FixedOffsetTimezone\' object has no attribute \'zone\'\n\n````\n\n\n\n\nAs an aside, I believe this is a common tz-aware timestamp for Postgres, so encountering this with:\n\n````df = pd.io.sql.frame_query(QRY, con=_engine.raw_connection(), index_col=\'ts\')````\n````\n'"
1917,6904990,jseabold,wesm,2012-09-16 18:28:47,2012-09-17 20:13:31,2012-09-17 16:09:22,closed,wesm,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1917,b'drop_duplicates regression with variable name',"b""I just updated to master and now this doesn't work.\n\n```\ndf = pandas.DataFrame([1,1,2,2,3,3,4,5], columns=['variable'])\ndf.drop_duplicates('variable')\n```\n\nI believe cfe674e is the offending commit. A string is iterable but we don't want to iterate over it."""
1913,6893724,lbeltrame,wesm,2012-09-15 08:18:33,2012-09-17 17:52:57,2012-09-17 17:52:37,closed,,0.9,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1913,b'Constructing a Series with a set returns a set and not a Series',"b'With latest master:\n\n```python\n\nIn [1]: import pandas\n\nIn [2]: data = set([1,2,3,4])\n\nIn [3]: pandas.Series(data)\nOut[3]: set([1, 2, 3, 4])\n````\n\n'"
1911,6866545,louist87,wesm,2012-09-14 02:35:46,2012-09-17 23:30:44,2012-09-17 23:30:28,closed,,0.9,2,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/1911,b'Output for small decimal numbers is misleading',"b'I think that [this](http://stackoverflow.com/questions/12417129/how-do-i-convert-a-column-from-a-pandas-dataframe-from-str-scientific-notation) question on stackoverflow sums up the problem fairly concisely.\n\nIn a nutshell, the `0` and `-0` output for very small decimal numbers is misleading and looks a lot like an error.  It might be better just to show scientific notation directly.'"
1906,6836468,wesm,wesm,2012-09-13 01:09:29,2012-09-13 03:04:20,2012-09-13 03:04:20,closed,,0.9,0,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/1906,b'DataFrame formatting column name truncation problem',"b'```\nIn [43]: read_csv(StringIO(data2))\nOut[43]: \n        Id                                            StringC\n0  7117434  Is it possible to modify drop plot code so that t\n\nIn [44]: read_csv(StringIO(data2)).columns\nOut[44]: Index([Id, StringCol], dtype=object)\n```'"
1905,6830289,changhiskhan,wesm,2012-09-12 20:11:43,2012-10-05 01:54:51,2012-10-05 01:54:51,closed,changhiskhan,0.9,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1905,b'read_csv regression',"b""parses dates after type conversions now so can't parse dates like '010508'"""
1900,6806493,erg,wesm,2012-09-12 00:38:07,2012-09-17 19:14:36,2012-09-17 19:14:26,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1900,"b""pandas.ewma doesn't handle empty inputs correctly""","b'```\nIn [34]: pandas.ewma(np.array([]), 3)\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-34-dd4e4b00c97b> in <module>()\n----> 1 pandas.ewma(np.array([]), 3)\n\n/usr/lib/python2.7/site-packages/pandas-0.9.0.dev_c32cc6e-py2.7-linux-x86_64.egg/pandas/stats/moments.pyc in ewma(arg, com, span, min_periods, freq, time_rule, adjust)\n    307 \n    308     return_hook, values = _process_data_structure(arg)\n--> 309     output = np.apply_along_axis(_ewma, 0, values)\n    310     return return_hook(output)\n    311 \n\n/usr/lib/python2.7/site-packages/numpy/lib/shape_base.pyc in apply_along_axis(func1d, axis, arr, *args)\n     78     outshape = asarray(arr.shape).take(indlist)\n     79     i.put(indlist, ind)\n---> 80     res = func1d(arr[tuple(i.tolist())],*args)\n     81     #  if res is a number, then we have a smaller output array\n     82     if isscalar(res):\n\n/usr/lib/python2.7/site-packages/pandas-0.9.0.dev_c32cc6e-py2.7-linux-x86_64.egg/pandas/stats/moments.pyc in _ewma(v)\n    301 \n    302     def _ewma(v):\n--> 303         result = lib.ewma(v, com, int(adjust))\n    304         first_index = _first_valid_index(v)\n    305         result[first_index : first_index + min_periods] = NaN\n\n/usr/lib/python2.7/site-packages/pandas-0.9.0.dev_c32cc6e-py2.7-linux-x86_64.egg/pandas/lib.so in pandas.lib.ewma (pandas/src/tseries.c:74301)()\n\nIndexError: Out of bounds on buffer access (axis 0)\n```\n\nI think it should output an empty array instead, like ``rolling_sum``.\n\n```\nIn [43]: pandas.rolling_sum(np.array([]), 3)\nOut[43]: array([], dtype=float64)\n```'"
1898,6801252,changhiskhan,wesm,2012-09-11 20:40:50,2012-09-15 18:49:38,2012-09-15 18:49:38,closed,,0.9,5,Bug,https://api.github.com/repos/pydata/pandas/issues/1898,b'algorithms.unique kills python interpreter',"b'32-bit Python 2.7.3 in 64-bit Windows 7\n\npandas master, numpy 1.6.1\n\nPython 2.7.3 |EPD 7.3-2 (32-bit)| (default, Apr 12 2012, 14:30:37) [MSC v.1500 32 bit (Intel)]\nType ""copyright"", ""credits"" or ""license"" for more information.\n\nIPython 0.14.dev -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython\'s features.\n%quickref -> Quick reference.\nhelp      -> Python\'s own help system.\nobject?   -> Details about \'object\', use \'object??\' for extra details.\n\nIn [1]: lst = [\'A\', \'B\', \'C\', \'D\', \'E\']\n\nIn [2]: import pandas.core.algorithms as algos\n\nIn [3]: algos.unique(lst)\nOut[3]: array([A, B, C, D, E], dtype=object)\n\nIn [4]: len(algos.unique(lst)) # --> triggers ""Python has stopped working...""\n\n\nOddly enough this actually works:\n\nIn [3]: rs = algos.unique(lst)\n\nIn [4]: len(rs)\nOut[4]: 5\n'"
1897,6799268,erg,wesm,2012-09-11 19:23:24,2012-09-12 14:53:40,2012-09-12 14:53:40,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1897,b'pandas.rolling_min/max fail if window size is larger than input array',"b'This is a regression since July 12.\n\n```\nimport pandas\nimport numpy as np\npandas.rolling_max(np.arange(3), 100, min_periods=1)\npandas.rolling_min(np.arange(3), 100, min_periods=1)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-4-782732058ee9> in <module>()\n----> 1 pandas.rolling_max(np.arange(3), 100, min_periods=1)\n\n/Library/Python/2.7/site-packages/pandas-0.9.0.dev_1e1c922-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in f(arg, window, min_periods, freq, time_rule, **kwargs)\n   436             return func(arg, window, minp, **kwds)\n   437         return _rolling_moment(arg, window, call_cython, min_periods,\n--> 438                                freq=freq, time_rule=time_rule, **kwargs)\n   439 \n   440     return f\n\n/Library/Python/2.7/site-packages/pandas-0.9.0.dev_1e1c922-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in _rolling_moment(arg, window, func, minp, axis, freq, time_rule, **kwargs)\n   252     return_hook, values = _process_data_structure(arg)\n   253     # actually calculate the moment. Faster way to do this?\n--> 254     result = np.apply_along_axis(calc, axis, values)\n   255 \n   256     return return_hook(result)\n\n/Library/Python/2.7/site-packages/numpy-1.6.2-py2.7-macosx-10.8-intel.egg/numpy/lib/shape_base.pyc in apply_along_axis(func1d, axis, arr, *args)\n    78     outshape = asarray(arr.shape).take(indlist)\n    79     i.put(indlist, ind)\n---> 80     res = func1d(arr[tuple(i.tolist())],*args)\n    81     #  if res is a number, then we have a smaller output array\n    82     if isscalar(res):\n\n/Library/Python/2.7/site-packages/pandas-0.9.0.dev_1e1c922-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in <lambda>(x)\n   249     """"""\n   250     arg = _conv_timerule(arg, freq, time_rule)\n--> 251     calc = lambda x: func(x, window, minp=minp, **kwargs)\n   252     return_hook, values = _process_data_structure(arg)\n   253     # actually calculate the moment. Faster way to do this?\n\n/Library/Python/2.7/site-packages/pandas-0.9.0.dev_1e1c922-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in call_cython(arg, window, minp, **kwds)\n   434         def call_cython(arg, window, minp, **kwds):\n   435             minp = check_minp(minp, window)\n--> 436             return func(arg, window, minp, **kwds)\n   437         return _rolling_moment(arg, window, call_cython, min_periods,\n   438                                freq=freq, time_rule=time_rule, **kwargs)\n\n/Library/Python/2.7/site-packages/pandas-0.9.0.dev_1e1c922-py2.7-macosx-10.8-intel.egg/pandas/lib.so in pandas.lib.roll_max2 (pandas/src/tseries.c:78735)()\n\nValueError: Invalid window size 100 for len 3 array\n```'"
1896,6795666,changhiskhan,wesm,2012-09-11 17:17:03,2012-09-13 01:47:19,2012-09-13 01:47:19,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1896,b'Mixed type DataFrame.diff converts to object',"b""```\nIn [68]: df = DataFrame(np.random.randn(5, 3))\n\nIn [69]: df\nOut[69]:\n          0         1         2\n0 -1.102224  0.395433 -0.112327\n1 -0.025758  2.810779 -0.823856\n2  1.100939  0.741825  1.016203\n3  0.452166 -1.914473  1.439475\n4  0.030840 -0.690666 -0.346612\n\nIn [70]: df.diff().dtypes\nOut[70]:\n0    float64\n1    float64\n2    float64\n\nIn [71]: df['A'] = np.array([1, 2, 3, 4, 5], dtype=object)\n\nIn [72]: df.dtypes\nOut[72]:\n0    float64\n1    float64\n2    float64\nA     object\n\nIn [73]: df.diff().dtypes\nOut[73]:\n0    object\n1    object\n2    object\nA    object\n```"""
1895,6795089,changhiskhan,wesm,2012-09-11 16:57:31,2012-09-13 02:12:23,2012-09-13 02:12:23,closed,wesm,0.9,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1895,b'DatetimeIndex min failure',"b""```\nIn [14]: rng = pd.date_range('2012-8-1', freq='T', periods=100)\n\nIn [15]: m = rng.min()\n\nIn [16]: m\nOut[16]: ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n...\n\nc:\\code\\pandas\\pandas\\tseries\\index.py in __repr__(self)\n    472\n    473         summary = str(self.__class__)\n--> 474         if len(self) == 1:\n    475             first = _format_datetime64(values[0], tz=self.tz)\n    476             summary += '\\n[%s]' % first\n\nTypeError: len() of unsized object\n\nIn [17]: type(m)\nOut[17]: pandas.tseries.index.DatetimeIndex\n\nIn [20]: rng.asobject.min()\nOut[20]: Index(<Timestamp: 2012-08-01 00:00:00>, dtype=object)\n```"""
1890,6788535,andreas-h,wesm,2012-09-11 11:49:09,2012-10-31 20:21:18,2012-10-31 20:21:18,closed,,0.9.1,6,Bug,https://api.github.com/repos/pydata/pandas/issues/1890,b'`TimeSeries.plot` ignores `color` kwarg',"b""I know this looks like a duplicate of #1636, but even with pandas 0.8.1, I don't get the expected plotting behaviour:\n\n    In [1]: from numpy import arange\n    \n    In [2]: import matplotlib.pyplot as plt\n    \n    In [3]: import pandas\n    \n    In [4]: pandas.__version__\n    Out[4]: '0.8.1'\n    \n    In [5]: plt.figure()\n    Out[5]: <matplotlib.figure.Figure at 0x33afd50>\n    \n    In [6]: plt.plot(arange(12), arange(12), color='green')     # this line is green\n    Out[6]: [<matplotlib.lines.Line2D at 0x337c150>]\n    \n    In [7]: pandas.Series(arange(12) + 1).plot(color='green')   # this line is blue\n    Out[7]: <matplotlib.axes.AxesSubplot at 0x33c80d0>\n"""
1888,6774916,jreback,wesm,2012-09-10 21:22:38,2012-09-17 20:23:33,2012-09-17 20:23:06,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1888,b'panel ix change in 0.8?',"b""seems p.ix[:,[-1],:] throws an exception (from take)\np.ix[:,-1:,:] is the workaround\nmaybe add to the docs?\n\nipython session follows:\n9/10/12 panel test ix change\n\n```\n1/2\nIn [7]:\nIn [2]:\nOut[2]: <class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 8 (major) x 4 (minor)\nItems: Item1 to Item2\nMajor axis: 2000-01-01 00:00:00 to 2000-01-08 00:00:00\nMinor axis: A to D\nIn [6]:\n\\---------------------------------------------------------------------------\nException Traceback (most recent call last)\n/mnt/home/jreback/<ipython-input-6-1531bef93beb> in <module>()\n1 # this worked prior to 0.8\n----> 2 p.ix[:,[-1],:]\n/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linuxx86_\n64.egg/pandas/core/indexing.pyc in __getitem__(self, key)\n31 pass\n32\n---> 33 return self._getitem_tuple(key)\n34 else:\n35 return self._getitem_axis(key, axis=0)\n/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linuxx86_\n64.egg/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\n137 continue\n138\n--> 139 retval = retval.ix._getitem_axis(key, axis=i)\n140\n141 return retval\n/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linuxx86_\n64.egg/pandas/core/indexing.pyc in _getitem_axis(self, key, axis)\n247 raise ValueError('Cannot index with multidimensional key')\n248\n--> 249 return self._getitem_iterable(key, axis=axis)\n250 elif axis == 0:\n251 is_int_index = _is_integer_index(labels)\n/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linuxx86_\n64.egg/pandas/core/indexing.pyc in _getitem_iterable(self, key, axis)\n295\n296 if _is_integer_dtype(keyarr) and not _is_integer_index(labels):\n--> 297 return self.obj.take(keyarr, axis=axis)\n298\n299 # this is not the most robust, but...\n/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linuxx86_\n64.egg/pandas/core/generic.pyc in take(self, indices, axis)\n857 new_data = self._data.reindex_axis(new_items, axis=0)\n858 else:\nimport pandas\nindex = pandas.date_range('1/1/2000', periods=8,)\np = pandas.Panel(np.random.randn(2, len(index), 4), items=['Item1', 'Item2'], major_axis\np\n\n\\# this worked prior to 0.8\np.ix[:,[-1],:]\n9/10/12 panel test ix change\n2/2\n--> 859 new_data = self._data.take(indices, axis=axis)\n860 return self._constructor(new_data)\n861\n/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linuxx86_\n64.egg/pandas/core/internals.pyc in take(self, indexer, axis)\n1075 n = len(self.axes[axis])\n1076 if ((indexer == -1) | (indexer >= n)).any():\n-> 1077 raise Exception('Indices must be nonzero and less than '\n1078 'the axis length')\n1079\nException: Indices must be nonzero and less than the axis length\nIn [5]:\nOut[5]: <class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 1 (major) x 4 (minor)\nItems: Item1 to Item2\nMajor axis: 2000-01-08 00:00:00 to 2000-01-08 00:00:00\nMinor axis: A to D\nIn [ ]:\n\n\\# this works ok\np.ix[:,-1:,:]\n```"""
1887,6773972,lodagro,wesm,2012-09-10 20:42:21,2012-09-10 22:09:42,2012-09-10 22:09:37,closed,,0.9,1,Bug;Output-Formatting,https://api.github.com/repos/pydata/pandas/issues/1887,b'Series repr issue (numpy.int64)',"b""```python\nIn [18]: df = pandas.DataFrame(np.random.randn(4,4))\n\nIn [19]: s = df.ix[3]\n\nIn [20]: type(s.name)\nOut[20]: numpy.int64\n\nIn [21]: s\nOut[21]: ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n...\nTypeError: 'numpy.int64' object is not iterable\n```"""
1885,6772425,lodagro,changhiskhan,2012-09-10 19:43:48,2012-09-20 16:49:01,2012-09-18 02:17:42,closed,,0.9,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1885,b'df.set_index() fails for MultiIndex columns',"b""```python\nIn [20]: columns = pandas.MultiIndex.from_tuples([('foo', 1), ('foo', 2), ('bar', 1)])\n\nIn [21]: df = pandas.DataFrame(np.random.randint(0, 10, (3,3)), columns=columns)\n\nIn [22]: df\nOut[22]: \n   foo     bar\n     1  2    1\n0    0  5    0\n1    5  6    6\n2    5  1    6\n\nIn [23]: df.set_index(df.columns[0])\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n\n```"""
1884,6769949,erg,wesm,2012-09-10 18:34:27,2012-09-13 01:23:16,2012-09-13 01:22:46,closed,,0.9,5,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1884,b'pandas.rolling_std() first value is nan',"b""The window is 3, but we want a std at ``min_periods=1``.  The one-period standard deviation is trivially 0.\n\n```\nIn [28]: pandas.rolling_std(np.array([1,2,3,4,5], dtype='double'), 3, min_periods=1)\nOut[28]: array([        nan,  0.70710678,  1.        ,  1.        ,  1.        ])\n```\n\nThe pathological case:\n\n```\nIn [29]: pandas.rolling_std(np.array([1,2,3,4,5], dtype='double'), 1, min_periods=1)\nOut[29]: array([ nan,  nan,  nan,  nan,  nan])\n```\n\nMaybe it's because pandas is taking the unbiased std for ``N-1`` where ``N = 1``, so it's dividing by zero?"""
1883,6766350,ijmcf,wesm,2012-09-10 16:33:44,2012-09-17 19:16:30,2012-09-17 19:16:30,closed,changhiskhan,0.9,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1883,b'Series or DataFrame asof() fails when the index is a PeriodIndex and the value is a datetime or Timestamp',"b""For example:\n>>> pi = pandas.period_range('2012/09/10', periods=10, freq='D')\n>>> s = pandas.TimeSeries(data=range(len(pi)), index=pi)\n>>> dt = pandas.Timestamp('2012/09/13')\n\n>>> s[dt]\n3\n\n>> s.asof(dt)\nTypeError: Cannot compare Timestamp with 25-May-2012\n"""
1881,6761793,jreback,wesm,2012-09-10 13:27:05,2012-09-17 18:03:53,2012-09-17 18:03:39,closed,,0.9,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1881,b'Backwards incompatibility in io/pytables with existing data prior to 0.8 causes data corruption',"b'trying to append to a table that was created before 0.8 (e.g. before the kind of datetime64 existed) causes corrupted data (essentially the index is converted to datetime64, but since its in nanoseconds, existing datetimes are not evaluated correctly). easiest to convert existing data (read in and just write it out), so this patch is simply an exception which is raised if incompatble kinds are detected.\n\n```\n---\n pandas/io/pytables.py |    6 ++++++\n 1 files changed, 6 insertions(+), 0 deletions(-)\n\ndiff --git a/pandas/io/pytables.py b/pandas/io/pytables.py\nindex 0f386c7..e3b84fb 100755\n--- a/pandas/io/pytables.py\n+++ b/pandas/io/pytables.py\n@@ -825,6 +825,12 @@ class HDFStore(object):\n             # the table must already exist\n             table = getattr(group, \'table\', None)\n \n+            # check for backwards incompatibility\n+            if append:\n+                existing_kind = table._v_attrs.index_kind\n+                if existing_kind != index_kind:\n+                    raise Exception(""incompatible kind in index [%s - %s]"" % (existing_kind,index_kind))\n+\n         # add kinds\n         table._v_attrs.index_kind = index_kind\n         table._v_attrs.columns_kind = cols_kind\n-- \n1.7.2.5\n```'"
1878,6751942,theandygross,wesm,2012-09-10 01:52:52,2012-09-18 20:21:41,2012-09-18 20:21:41,closed,changhiskhan,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1878,b'Apply Across Non-Unique Indices.',"b""Applying a function across a data-frame does not seem to work for non-unique indices.\n\n    df = pandas.DataFrame([[1,1,1], [2,2,2], [3,3,3]], index=['a','a','c'])\n    df.apply(lambda s: s[0], axis=1)\n\n    a    2\n    a    2\n    c    3\n\nIt seems to be just returning the value for the first instance of the index."""
1866,6739082,wesm,changhiskhan,2012-09-09 00:40:21,2012-09-10 02:39:13,2012-09-10 02:39:13,closed,,0.9,0,Bug;Docs,https://api.github.com/repos/pydata/pandas/issues/1866,b'Doc build bug',"b'@changhiskhan there\'s some bug showing up in the docs but not in the main test suite. Can you have a look?\n\n```\nbuilding [html]: targets for 22 source files that are out of date\nupdating environment: 218 added, 0 changed, 0 removed\n---------------------------------------------------------------------------    \nTypeError                                 Traceback (most recent call last)\n<ipython-input-767-cc6f0e841b62> in <module>()\n      1 df = read_csv(\'tmp.csv\', header=None, parse_dates=date_spec,\n----> 2                date_parser=conv.parse_date_time)\n\n/home/wesm/code/pandas/pandas/io/parsers.pyc in read_csv(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, keep_default_na, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding, squeeze)\n    239         kwds[\'delimiter\'] = sep\n    240 \n--> 241     return _read(TextParser, filepath_or_buffer, kwds)\n    242 \n    243 @Appender(_read_table_doc)\n\n/home/wesm/code/pandas/pandas/io/parsers.pyc in _read(cls, filepath_or_buffer, kwds)\n    192         return parser\n    193 \n--> 194     return parser.get_chunk()\n    195 \n    196 @Appender(_read_csv_doc)\n\n/home/wesm/code/pandas/pandas/io/parsers.pyc in get_chunk(self, rows)\n    812         columns = list(self.columns)\n    813         if self.parse_dates is not None:\n--> 814             data, columns = self._process_date_conversion(data)\n    815 \n    816         df = DataFrame(data=data, columns=columns, index=index)\n\n/home/wesm/code/pandas/pandas/io/parsers.pyc in _process_date_conversion(self, data_dict)\n    989 \n    990                 _, col, old_names = _try_convert_dates(\n--> 991                     self._conv_date, colspec, data_dict, self.orig_columns)\n    992 \n    993                 new_data[new_name] = col\n\n/home/wesm/code/pandas/pandas/io/parsers.pyc in _try_convert_dates(parser, colspec, data_dict, columns)\n   1120     to_parse = [data_dict[c] for c in colspec if c in data_dict]\n   1121     try:\n-> 1122         new_col = parser(*to_parse)\n   1123     except DateConversionError:\n   1124         new_col = parser(_concat_date_cols(to_parse))\n\n/home/wesm/code/pandas/pandas/io/parsers.pyc in _conv_date(self, *date_cols)\n    951                     return lib.try_parse_dates(_concat_date_cols(date_cols),\n    952                                                parser=self.date_parser,\n--> 953                                                dayfirst=self.dayfirst)\n    954 \n    955     def _process_date_conversion(self, data_dict):\n\n/home/wesm/code/pandas/pandas/lib.so in pandas.lib.try_parse_dates (pandas/src/tseries.c:98928)()\n\n/home/wesm/code/pandas/pandas/lib.so in pandas.lib.try_parse_dates (pandas/src/tseries.c:98870)()\n\nTypeError: parse_date_time() takes exactly 2 arguments (1 given)\nreading sources... [100%] whatsnew                                             \n/home/wesm/code/pandas/doc/source/faq.rst:74: ERROR: Unknown interpreted text role ""issue"".\nsource/v0.8.0.txt:151: WARNING: Inline literal start-st\n```'"
1863,6738456,wesm,jreback,2012-09-08 22:47:44,2015-01-26 00:34:27,2015-01-26 00:34:27,closed,,0.16.0,1,Bug;MultiIndex;Stats;Won't Fix,https://api.github.com/repos/pydata/pandas/issues/1863,b'OLS issues interacting with MultiIndex',b'x-linking:\n\nhttp://stackoverflow.com/questions/11586068/collapse-a-pandas-multiindex-or-run-ols-regression-on-a-multiindexed-dataframe'
1861,6731102,jreback,wesm,2012-09-08 13:04:34,2012-09-09 01:22:33,2012-09-09 01:22:33,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1861,b'deprecated from pandas.stats import misc fails in 0.8.1',"b'I believe this is a deprecated import (I used it to access bucket_series)\nexists currently in master 0.9.dev as well\n\nPython 2.7.3 (default, Jun 21 2012, 07:50:29) \n>>> import pandas\n>>> print pandas.__version__\n0.8.1\n>>> from pandas.stats import misc\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linux-x86_64.egg/pandas/stats/misc.py"", line 7, in <module>\n    from pandas.tools.tile import quantileTS\nImportError: cannot import name quantileTS\n>>> '"
1859,6725882,wesm,wesm,2012-09-07 22:23:57,2012-09-08 02:02:01,2012-09-08 02:02:01,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1859,"b""Series.str.split doesn't work with no arguments""",b'Inconsistent with builtin string API'
1857,6723984,mlew,changhiskhan,2012-09-07 20:46:20,2012-09-10 21:57:01,2012-09-10 21:57:01,closed,changhiskhan,0.9,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1857,b'Period.start_time returns the same value as Period.end_time',"b""If you create a Period object with a non-Day frequency, the start_time property incorrectly returns the end_time value.\n\nex:\n```python\nIn [1]: from pandas import *\n\nIn [2]: w = period_range('2012-8-20', '2012-9-10', freq='W-MON')\n\nIn [3]: w[0]\nOut[3]: Period('14-Aug-2012/20-Aug-2012', 'W-MON')\n\nIn [4]: w[0].start_time\nOut[4]: <Timestamp: 2012-08-20 00:00:00>\n\nIn [5]: w[0].end_time\nOut[5]: <Timestamp: 2012-08-20 00:00:00>\n\nIn [6]: w[0].freq\nOut[6]: 'W-MON'\n```\n\nThe expected behavior would return a ```<Timestamp: 2012-08-14 00:00:00>``` object for the Period.start_time property."""
1855,6723376,wesm,jreback,2012-09-07 20:19:52,2013-09-21 13:51:39,2013-09-21 13:51:39,closed,,0.13,1,Bug;Data IO;IO CSV,https://api.github.com/repos/pydata/pandas/issues/1855,b'Problems converting a large file to a panel using read_csv',b'Bug?\n\nx-reference from SO\n\nhttp://stackoverflow.com/questions/12085393/problems-converting-a-large-file-to-a-panel-using-read-csv-into-python-pandas'
1850,6702258,mrjbq7,wesm,2012-09-06 23:02:13,2012-09-08 02:34:55,2012-09-08 02:34:47,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1850,"b'pandas.rolling_apply: ""Out of bounds on buffer access""'","b'Looks like ``pandas.rolling_apply`` doesn\'t like a window larger than the length of the array:\n\n```\nIn [3]: import pandas\n\nIn [4]: import numpy as np\n\nIn [5]: a = np.array(range(4))\n\nIn [6]: pandas.rolling_sum(a, 10)\nOut[6]: array([ nan,  nan,  nan,  nan])\n\nIn [7]: pandas.rolling_apply(a, 10, np.sum)\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-7-1ff0414da8bd> in <module>()\n----> 1 pandas.rolling_apply(a, 10, np.sum)\n\n/Library/Python/2.7/site-packages/pandas-0.8.1-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in rolling_apply(arg, window, func, min_periods, freq, time_rule)\n    465         return lib.roll_generic(arg, window, minp, func)\n    466     return _rolling_moment(arg, window, call_cython, min_periods,\n--> 467                            freq=freq, time_rule=time_rule)\n\n/Library/Python/2.7/site-packages/pandas-0.8.1-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in _rolling_moment(arg, window, func, minp, axis, freq, time_rule, **kwargs)\n    227     return_hook, values = _process_data_structure(arg)\n    228     # actually calculate the moment. Faster way to do this?\n--> 229     result = np.apply_along_axis(calc, axis, values)\n    230 \n    231     return return_hook(result)\n\n/Library/Python/2.7/site-packages/numpy-1.6.2-py2.7-macosx-10.8-intel.egg/numpy/lib/shape_base.pyc in apply_along_axis(func1d, axis, arr, *args)\n     78     outshape = asarray(arr.shape).take(indlist)\n     79     i.put(indlist, ind)\n---> 80     res = func1d(arr[tuple(i.tolist())],*args)\n     81     #  if res is a number, then we have a smaller output array\n     82     if isscalar(res):\n\n/Library/Python/2.7/site-packages/pandas-0.8.1-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in <lambda>(x)\n    224     """"""\n    225     arg = _conv_timerule(arg, freq, time_rule)\n--> 226     calc = lambda x: func(x, window, minp=minp, **kwargs)\n    227     return_hook, values = _process_data_structure(arg)\n    228     # actually calculate the moment. Faster way to do this?\n\n/Library/Python/2.7/site-packages/pandas-0.8.1-py2.7-macosx-10.8-intel.egg/pandas/stats/moments.pyc in call_cython(arg, window, minp)\n    463     def call_cython(arg, window, minp):\n    464         minp = _use_window(minp, window)\n--> 465         return lib.roll_generic(arg, window, minp, func)\n    466     return _rolling_moment(arg, window, call_cython, min_periods,\n    467                            freq=freq, time_rule=time_rule)\n\n/Library/Python/2.7/site-packages/pandas-0.8.1-py2.7-macosx-10.8-intel.egg/pandas/lib.so in pandas.lib.roll_generic (pandas/src/tseries.c:74581)()\n\nIndexError: Out of bounds on buffer access (axis 0)\n```'"
1849,6699133,aheilbut,wesm,2012-09-06 20:47:11,2012-09-10 22:22:29,2012-09-10 22:22:29,closed,changhiskhan,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1849,b'merge or join fails if the left input dataframe contains a column of float32',"b""A merge or join involving a left DataFrame that has single precision floats will fail.  Seems to be OK if the right dataframe has float32; these get cast to float64 (which they probably shouldn't be)\n\n```python\nimport numpy as np\nimport pandas\na = np.random.randint(0, 5, 100)\ndf = pandas.DataFrame({'a': a})\ns = pandas.DataFrame( pandas.Series(np.random.random(5).astype('f'), name='md') )\ndf.merge(s, left_on='a', right_index=True) # this is OK\ndf['b'] = np.random.randint(0, 5, 100)\ndf.merge(s, left_on='a', right_index=True) # this is OK\ndf['c'] = np.random.random(100).astype('Float64')\ndf.merge(s, left_on='a', right_index=True) # this is OK\ndf['d'] = np.random.random(100).astype('Float32')\nprint df.dtypes\ndf.merge(s, left_on='a', right_index=True) # this fails\n```"""
1848,6689788,John-Colvin,wesm,2012-09-06 14:47:53,2014-03-05 15:18:23,2012-09-09 03:17:31,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1848,b'lib.infer_dtype_list(MultiIndex) crashes python',"b'If lib.infer_dtype_list is called with a MultiIndex, it crashes python (on OS X 10.7 at least)\n\nThis crash occurs when writing a dataframe with a multiindex to a pytables table (_write_table)'"
1843,6655302,lodagro,changhiskhan,2012-09-05 08:30:11,2012-09-25 18:40:55,2012-09-18 15:10:45,closed,,0.9,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1843,b'ExcelFile(...).parse(skiprows=...) fails for rows at the end of the sheet',"b""from [mailinglist](https://groups.google.com/forum/?fromgroups=#!searchin/pydata/richard$20stanton/pydata/z1BNtywmc80/qzgG2pUrsk8J) (tried it myself on master also)\n\n\nI've been using the ExcelFile function to read in a dataset that has some lines of text above and below the numbers I'm interested in.\n\nThe option skiprows=[0,1,3,4,5,6,7] works fine to ignore the text above the data (apart from line 2, which contains the labels).\n\nHowever, if I add values 151, 152, etc., to try to ignore the text lines after the end of the data, this seems to have no effect. Is this behavior deliberate?*\n\nI can get rid of those extra lines at the end by using dropna(), but this seems to have the effect of making the first series of type object rather than float, which is (a little) inconvenient."""
1842,6654868,lodagro,lodagro,2012-09-05 08:00:07,2012-09-10 19:52:55,2012-09-10 19:52:55,closed,lodagro,0.9,6,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1842,"b'df.plot(x=..., y=...) fails for mixed-integer column names'","b""```python\nIn [187]: df = pandas.DataFrame(np.random.randn(100,3),columns=['A', 'B', 0])\n\nIn [188]: df.head()\nOut[188]: \n          A         B         0\n0  0.699932 -1.090587  1.014325\n1 -0.148856 -0.994177 -1.107741\n2  1.384710  0.107488  1.108757\n3 -0.795969 -1.297598 -0.268348\n4 -0.247695 -0.627048  0.010415\n\nIn [189]: df.plot(x='A', y=0)\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n...\nKeyError: 'no item named A'\n\nIn [190]: \n```"""
1840,6642447,erg,wesm,2012-09-04 18:35:53,2012-09-07 18:47:49,2012-09-07 18:47:41,closed,,0.9,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1840,"b""pandas.rolling_std() gives nans where it probably shouldn't""","b'The output differs from bottleneck, which also gives nans, but fewer.\n\nThe Factor version never outputs nans and otherwise matches the pandas version.\n\nhttps://gist.github.com/3624548'"
1839,6636306,ijmcf,changhiskhan,2012-09-04 14:38:57,2012-11-28 18:39:21,2012-11-28 18:39:21,closed,changhiskhan,0.10,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1839,"b""Weekly anchored frequency offsets don't behave as expected""","b'For example:\n>>> period_index = pandas.period_range(\'2012/6/3\', periods=10, freq=\'W-FRI\')\n<class \'pandas.tseries.period.PeriodIndex\'>\nfreq: W-FRI\n[02-Jun-2012/08-Jun-2012, ..., 04-Aug-2012/10-Aug-2012]\nlength: 10\n\nAlthough the frequency is ""W-FRI"", the start dates listed (02-Jun-2012, ..., 04-Aug-2012) are Saturdays.\n\nConverting the frequency of the PeriodIndex to daily using ""start"", you get Saturdays:\n>>> period_index.asfreq(freq=\'D\', how=\'start\')\n<class \'pandas.tseries.period.PeriodIndex\'>\nfreq: D\n[02-Jun-2012, ..., 04-Aug-2012]\nlength: 10\n\nAnd if you convert the PeriodIndex to a DatetimeIndex using to_timestamp(), you get a frequency that is explicitly ""W-SAT"":\n>>> period_index.to_timestamp(freq=\'D\', how=\'s\')\n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2012-06-02 00:00:00, ..., 2012-08-04 00:00:00]\nLength: 10, Freq: W-SAT, Timezone: None\n'"
1838,6636196,ijmcf,wesm,2012-09-04 14:34:49,2012-09-19 02:18:53,2012-09-19 02:18:53,closed,changhiskhan,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1838,b'Adding an offset to a timezone-aware DatetimeIndex shifts the times',"b""For example:\n>>> dr = pandas.date_range('03/06/2012 00:00', periods=25, freq='W-FRI', tz='US/Eastern')\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-09 00:00:00, ..., 2012-08-24 00:00:00]\nLength: 25, Freq: W-FRI, Timezone: US/Eastern\n\n>>> dr + pandas.datetools.BDay()\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-12 05:00:00, ..., 2012-08-27 04:00:00]\nLength: 25, Freq: None, Timezone: US/Eastern\n\nContrast with a timezone-naive DatetimeIndex:\n>>> dr = pandas.date_range('03/06/2012 00:00', periods=25, freq='W-FRI')\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-09 00:00:00, ..., 2012-08-24 00:00:00]\nLength: 25, Freq: W-FRI, Timezone: None\n\n>>> dr + pandas.datetools.BDay()\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-12 00:00:00, ..., 2012-08-27 00:00:00]\nLength: 25, Freq: W-MON, Timezone: None\n"""
1835,6621496,lodagro,wesm,2012-09-03 19:39:24,2012-09-10 22:00:10,2012-09-10 22:00:10,closed,changhiskhan,0.9,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1835,b'read_csv fails when both converts and index_col are used on the same column',"b'```python\nIn [473]: d = """"""\\\nA;B\n1;2\n3;4\n""""""\n\nIn [474]: pandas.read_csv(StringIO(d), sep=\';\', converters={\'A\': lambda x: x}, index_col=\'A\')\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-474-6bc08702ea91> in <module>()\n...\nKeyError: \'A\'\n\nIn [475]: pandas.read_csv(StringIO(d), sep=\';\', converters={\'A\': lambda x: x}).set_index(\'A\')\nOut[475]: \n   B\nA   \n1  2\n3  4\n'"
1833,6613588,snth,wesm,2012-09-03 11:07:26,2013-12-04 00:47:26,2012-09-09 01:33:37,closed,,0.9,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1833,b'Bug in DataFrame.duplicated() when dealing with datetime64',"b""The following looks like a bug to me as DataFrame.duplicated() gives different results on what should be identical inputs. To me it looks like the problem is with the datetime64 values because if you look at the output of `dates.values` it's clear that the last 4 values are duplicates.\n\nPlease see the code below that reproduces the problem:\n\n```python\nIn [701]: dates = date_range('2010-07-01', end='2010-08-05')\n\nIn [702]: tst = DataFrame({'symbol': 'AAA', 'date': dates})\n\nIn [703]: tst.tail()\nOut[703]: \n                  date symbol\n31 2010-08-01 00:00:00    AAA\n32 2010-08-02 00:00:00    AAA\n33 2010-08-03 00:00:00    AAA\n34 2010-08-04 00:00:00    AAA\n35 2010-08-05 00:00:00    AAA\n\nIn [704]: tst.duplicated().tail()\nOut[704]: \n31    False\n32    False\n33    False\n34    False\n35    False\n\nIn [705]: tst.duplicated(['date', 'symbol']).tail()\nOut[705]: \n31    False\n32     True\n33     True\n34     True\n35     True\n\nIn [706]: dates.values\nOut[706]: \narray([1970-01-15 40:00:00, 1970-01-15 64:00:00, 1970-01-15 88:00:00,\n       1970-01-15 112:00:00, 1970-01-15 136:00:00, 1970-01-15 160:00:00,\n       1970-01-15 184:00:00, 1970-01-15 208:00:00, 1970-01-15 232:00:00,\n       1970-01-15 00:00:00, 1970-01-15 24:00:00, 1970-01-15 48:00:00,\n       1970-01-15 72:00:00, 1970-01-15 96:00:00, 1970-01-15 120:00:00,\n       1970-01-15 144:00:00, 1970-01-15 168:00:00, 1970-01-15 192:00:00,\n       1970-01-15 216:00:00, 1970-01-15 240:00:00, 1970-01-15 08:00:00,\n       1970-01-15 32:00:00, 1970-01-15 56:00:00, 1970-01-15 80:00:00,\n       1970-01-15 104:00:00, 1970-01-15 128:00:00, 1970-01-15 152:00:00,\n       1970-01-15 176:00:00, 1970-01-15 200:00:00, 1970-01-15 224:00:00,\n       1970-01-15 248:00:00, 1970-01-15 16:00:00, 1970-01-15 40:00:00,\n       1970-01-15 64:00:00, 1970-01-15 88:00:00, 1970-01-15 112:00:00], dtype=datetime64[ns])\n\nIn [707]: dates\nOut[707]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-07-01 00:00:00, ..., 2010-08-05 00:00:00]\nLength: 36, Freq: D, Timezone: None\n\nIn [708]: dates.dtype\nOut[708]: dtype('datetime64[ns]')\n\nIn [709]: dates.values.dtype\nOut[709]: dtype('datetime64[ns]')\n\nIn [710]: sys.version\nOut[710]: '2.7.3 (default, Aug  1 2012, 05:14:39) \\n[GCC 4.6.3]'\n\nIn [711]: np.version.version\nOut[711]: '1.6.1'\n\nIn [713]: pd.version.version\nOut[713]: '0.8.1'\n\nIn [714]: \n```\n\nI remember reading somewhere that there are problems with datetime64 in numpy 1.6 but I don't understand what coercions are taking place behind the scenes. Also, if someone could please explain to me why the dates in `dates.values` above are wrong and how to avoid this, I would appreciate it.\n\n"""
1831,6582357,snth,changhiskhan,2012-08-31 15:52:45,2012-09-10 17:48:43,2012-09-10 17:48:43,closed,,0.9,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1831,b'set_index drops data in the presence of duplicates when inplace=True and verify_integrity=True',"b""When calling set_index on an index with duplicates, the verify_integrity=True option correctly identifies the duplicates but this check appears to take place after the original columns have already been dropped when inplace=True is also passed. This results in data being lost.\n\nI believe it would be better if the original DataFrame object was only modified in the case that the set_index operation is successful.\n\nCode to reproduce the problem:\n\n```python\nIn [189]: df = DataFrame({'one':[1, 1, 2], 'two':[1,2,3]})\n\nIn [190]: df\nOut[190]: \n   one  two\n0    1    1\n1    1    2\n2    2    3\n\nIn [191]: df.set_index(['one'], inplace=True, verify_integrity=True)\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n/mnt/hgfs/fastdata/<ipython-input-191-e1c0e8c92f6c> in <module>()\n----> 1 df.set_index(['one'], inplace=True, verify_integrity=True)\n\n/home/tobias/code/envs/mac/local/lib/python2.7/site-packages/pandas/core/frame.pyc in set_index(self, keys, drop, append, inplace, verify_integrity)\n   2328         if verify_integrity and not index.is_unique:\n   2329             duplicates = index.get_duplicates()\n-> 2330             raise Exception('Index has duplicate keys: %s' % duplicates)\n   2331 \n   2332         # clear up memory usage\n\n\nException: Index has duplicate keys: [1]\n\nIn [192]: df\nOut[192]: \n   two\n0    1\n1    2\n2    3\n\nIn [202]: print sys.version\n2.7.3 (default, Aug  1 2012, 05:14:39) \n[GCC 4.6.3]\n\nIn [203]: print pd.version.version\n0.8.1\n\nIn [204]: \n```"""
1830,6571499,changhiskhan,changhiskhan,2012-08-31 05:32:59,2012-09-10 12:49:00,2012-09-10 12:49:00,closed,,0.9,1,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1830,b'Period Indexing Bug',b'From mailing list:\n\n\nIn [82]: df\nOut[82]:\n      data\n1971     2\n1972     3\n1973     4\n\nIn [83]: df.ix[Period(1971)]\nOut[83]:\ndata    3\nName: 1972\n'
1828,6561447,lbeltrame,wesm,2012-08-30 18:44:07,2012-09-13 01:50:47,2012-09-13 01:50:47,closed,changhiskhan,0.9,5,Bug;Data IO;Unicode,https://api.github.com/repos/pydata/pandas/issues/1828,b'to_excel() fails if unicode sheet name is used',"b'Like the subject says, the check in DataFrame.to_excel() checks for an instance of str, rather than basestring, hence unicode names skip the if check and are treated like files, which causes an exception later on in the code. Changing isistance() to check for basestring rather than str fixes the issue.'"
1825,6542007,wesm,wesm,2012-08-30 00:02:04,2012-09-10 22:36:39,2012-09-10 22:36:39,closed,wesm,0.9,0,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/1825,b'Test datetime64 conversion in from_records error',b'http://stackoverflow.com/questions/11945897/python-pandas-0-81-dataframe-from-records-convert-django-datetime-to-datetime'
1824,6540709,wesm,wesm,2012-08-29 22:37:44,2012-09-09 01:04:55,2012-09-09 01:04:55,closed,,0.9,1,Bug;Performance,https://api.github.com/repos/pydata/pandas/issues/1824,b'HDFStore format of very long DataFrames',b'http://stackoverflow.com/questions/12053003/pandas-pytables-warnings-and-slow-performance'
1823,6538933,wesm,wesm,2012-08-29 21:13:23,2012-09-09 03:07:58,2012-09-09 03:07:58,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1823,b'Panel truncate failure',"b'user reported this error (may be for all panels)\n\n```\npanel.truncate(before=None, after=None,\naxis=\xa1\xafitems\xa1\xaf).fillna(value=0.0) will fail with an AssertionError\n\nThe failure path is pretty narrow.  If \xa1\xaebefore\xa1\xaf or \xa1\xaeafter\xa1\xaf is\nbinding, it won\xa1\xaft fail.  If the axis is \xa1\xaemajor\xa1\xaf or \xa1\xaeminor\xa1\xaf it\nwon\xa1\xaft fail.  If fillna method is \xa1\xaepad\xa1\xaf, it won\xa1\xaft fail.  So, it\nseems to be just this pretty narrow set of parameter values that\ncauses a problem (i.e. reindexing the items of a Panel that\nresults in no change to the Index).\n ```'"
1822,6537203,wesm,jreback,2012-08-29 20:03:30,2016-02-23 00:57:55,2016-02-23 00:57:55,closed,,Someday,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1822,b'Long variable names table formatting in ols',"b'```\nRunning pd.ols from pandas 0.8.1, I get misaligned output when some of my variable names are long. For example:\n\n[...]\n\n-----------------------Summary of Estimated Coefficients------------------------\n      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%\n--------------------------------------------------------------------------------\nprovision_alll_ratio     0.8377     0.0030     281.62     0.0000     0.8319     0.8436\nlead_1_provision_alll_ratio     0.0322     0.0033       9.70     0.0000     0.0257     0.0387\nlead_2_provision_alll_ratio     0.0332     0.0033      10.02     0.0000     0.0267     0.0397\nlead_3_provision_alll_ratio     0.0245     0.0033       7.39     0.0000     0.0180     0.0310\nlead_4_provision_alll_ratio     0.0119     0.0030       4.00     0.0001     0.0061     0.0177\n--------------------------------------------------------------------------------\n     intercept    -0.0005     0.0000     -28.57     0.0000    -0.0005    -0.0004\n---------------------------------End of Summary---------------------------------\n\nIs there a way to make the first output field long enough to fit the longest variable name?\n\nThanks,\n\nRichard Stanton\n```'"
1821,6509487,lodagro,wesm,2012-08-28 20:28:40,2012-09-18 21:09:31,2012-09-18 21:09:23,closed,wesm,0.9,1,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1821,"b'DatetimeIndex, df.ix[date] and df.ix[[date]] failure'","b'from [stackoverflow](http://stackoverflow.com/questions/11991627/selecting-a-subset-of-a-pandas-dataframe-indexed-by-datetimeindex-with-a-list-of)\n\nThe original frame can not be shared, but following code reproduces the issue.\n\n```python\nimport pandas\nimport numpy as np\nimport datetime\n\n# create large list of non periodic datetime\ndates = []\nsec = datetime.timedelta(seconds=1)\nhalf_sec = sec / 2\nd = datetime.datetime(2011, 12, 5, 20, 30)\nn = 100000\nfor i in range(n):\n    dates.append(d)\n    dates.append(d + sec)\n    dates.append(d + sec + half_sec)\n    dates.append(d + sec + sec + half_sec)\n    d += 3 * sec\n\n# duplicate some values in the list\nduplicate_positions = np.random.randint(0, len(dates) - 1, 20)\nfor p in duplicate_positions:\n    dates[p + 1] = dates[p]\nprint ""duplicated values at %s"" % str(duplicate_positions)\n\ndf = pandas.DataFrame(np.random.randn(len(dates), 4),\n                      index=dates,\n                      columns=list(\'ABCD\'))\n\npos = n * 3\ntimestamp = df.index[pos]\nprint ""timestamp = df.index[%d] = %s"" % (pos, str(timestamp))\nprint ""timestamp in index? %s"" % str(timestamp in df.index)\nprint ""df.ix[timestamp] = \\n%s"" % str(df.ix[timestamp])\nprint ""df.ix[[timestamp]] = \\n%s"" % str(df.ix[[timestamp]])\n```\n\nrunning this script outputs:\n\n```\nduplicated values at [143779 365504 338898 192427  63827 333772 366805 378282 375136  77760\n  59843 215934 173395 185449 310734 184004  48594 221298 348967  86615]\ntimestamp = df.index[300000] = 2011-12-08 11:00:00\ntimestamp in index? True\ndf.ix[timestamp] = \nA    0.959639\nB    0.651874\nC    1.059355\nD   -1.296393\nName: 2011-12-08 11:00:00\ndf.ix[[timestamp]] = \nEmpty DataFrame\nColumns: array([A, B, C, D], dtype=object)\nIndex: <class \'pandas.tseries.index.DatetimeIndex\'>\nLength: 0, Freq: None, Timezone: None\n```\n\nfor n = 1000000 it gets worse\n\n```python\nduplicated values at [ 686452 3862593 3747433   63099 2422495 2191536  486238 1632442 2373460\n 3266942 3127937 2538658 1405505  739509 1519644 2817907 1005119  755410\n 1784244   86211]\ntimestamp = df.index[3000000] = 2011-12-31 21:30:00\ntimestamp in index? False\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n...\nTypeError: Cannot compare Timestamp with 1326116999500000000\n```\n\n'"
1820,6503483,warrenwjackson,changhiskhan,2012-08-28 16:25:00,2012-09-10 19:59:55,2012-09-10 19:59:55,closed,,0.9,1,Bug;Reshaping,https://api.github.com/repos/pydata/pandas/issues/1820,"b""Unstacking boolean DataFrame fills 'True' for missing values""","b""Is this a bug?  If this is intentional, it is pretty dangerous to fill missing values with ```True```. \n```\n>>> df = pd.DataFrame([False, False], index=pd.MultiIndex.from_arrays([['a', 'b'], ['c', 'l']]), columns=['col'])\n>>> df\n       col\na c  False\nb l  False\n>>> df.unstack()\n     col\n       c      l\na  False   True\nb   True  False\n```"""
1815,6465425,jmloser,wesm,2012-08-27 02:57:38,2012-09-10 23:06:39,2012-09-10 23:06:27,closed,,0.9,1,Bug;Reshaping;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1815,b'concat() drops index when used on series with a PeriodIndex',"b""New to this, sorry if I'm just missing something...\n\n```python\nimport numpy as np\nimport pandas as pd\n\nd1 = pd.date_range('12/31/1990', '12/31/1999', freq='A-DEC')\nd2 = pd.date_range('12/31/2000', '12/31/2009', freq='A-DEC')\n\ns1 = pd.Series(np.random.randn(10), d1)\ns2 = pd.Series(np.random.randn(10), d2)\n\n# works\npd.concat([s1,s2])\n\ns1 = s1.to_period()\ns2 = s2.to_period()\n\n# drops index\npd.concat([s1,s2])\n\n# workaround\npd.concat([s1.to_timestamp(), s2.to_timestamp()]).to_period('A-DEC')\n```"""
1814,6459430,dalejung,changhiskhan,2012-08-26 15:27:20,2012-09-10 20:27:30,2012-09-10 20:27:30,closed,,0.9,0,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/1814,b'bool dataframe: df.shift fill rows with True instead of NaN like series.shift',"b""```\nimport pandas as pd\n\ndf = pd.DataFrame({'high':[True, False], 'low':[False, False]})\ndf.shift(1) \n\n#   high    low\n# 0  True   True\n# 1  True  False\n```\n\nseries.shift works as expected. """
1810,6453879,changhiskhan,changhiskhan,2012-08-25 19:15:35,2012-09-18 02:46:30,2012-09-18 02:46:30,closed,,0.9,1,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1810,b'Index.asof returns different types',"b'depending on whether the date exists or not:\n\nIn [42]: idx.asof(datetime(2008, 5, 2))\nOut[42]: datetime.datetime(2008, 5, 2, 0, 0)\n\nIn [43]: idx.asof(datetime(2008, 5, 3))\nOut[43]: \\<Timestamp: 2008-05-02 00:00:00\\>\n'"
1809,6453816,gtakacs,wesm,2012-08-25 19:06:50,2012-09-20 03:12:48,2012-09-20 03:12:48,closed,changhiskhan,0.9,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1809,b'groupby().last() drops columns',"b'DataFrame.groupby().last() gives incorrect result in the following case (pandas version: 0.8.1):\n\n```\ndf1 = DataFrame({""A"": [1, 1], ""B"": [1, 1], ""C"": [""x"", ""y""]})\nprint df1.groupby(""A"").last()\n#    B\n# A   \n# 1  1\n```\n\nThe integer column B is handled correctly, but the string column C is dropped.\nThe result is correct if column B is not present:\n\n```\ndf2 = DataFrame({""A"": [1, 1], ""C"": [""x"", ""y""]})\nprint df2.groupby(""A"").last()\n#    C\n# A   \n# 1  y\n```\n\n'"
1808,6450484,wesm,wesm,2012-08-25 11:51:17,2012-09-09 18:25:19,2012-09-09 18:25:19,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1808,b'Handling of lambda functions in list for resample',b'DataFrame bad column name reported by user'
1807,6418582,jseabold,wesm,2012-08-23 20:25:06,2012-09-04 02:17:27,2012-09-04 02:16:37,closed,,,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1807,b'Sort broken after unique -> Series with object array',"b'I have no idea why this fails.\n\n```\npandas.Series(pandas.Series([""a"",""c"",""b""]).unique()).sort()\n```\n\nbut this doesn\'t\n\n```\npandas.Series(np.array([""a"", ""c"", ""b""], dtype=object)).sort()\n```'"
1803,6379768,sadruddin,wesm,2012-08-22 15:12:03,2012-12-02 02:13:09,2012-12-02 02:13:09,closed,,0.10,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1803,"b""Can't assign to subsets of MultiIndex columns DataFrames using partial labels""","b""Assigning a value (scalar or Series) to the subset of a dataframe with MultiIndex columns addressed by a partial label doesn't work.\n\n    df = pandas.DataFrame(index=[1, 3, 5], columns=pandas.MultiIndex.from_tuples([('A', '1'), ('A', '2'), ('B', '1')]))\n    df['A'] = 0.0 # Doesn't work\n    df['A'] = df['B', '1'] # Works, but adds a column instead of updating the two existing ones\n\n    df2 = df.T.copy()\n    df2.ix['A'] = 0.0 # Works\n    df2.ix['A'] = df2['B', '1'] # Works\n\nThe same thing in the rows dimension works as expected."""
1800,6375548,ohadle,cpcloud,2012-08-22 12:16:11,2013-09-21 13:36:28,2013-09-21 13:36:28,closed,cpcloud,0.13,5,Bug;Prio-high;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1800,b'plot() fails with a combined bar and line graph',"b'When I plot one column of a df as a bar, run twinx(), and plot another as a line, I get the following error:\n(using pandas 0.7.0 and numpy 1.5.1)\n\nIn [94]: Traceback (most recent call last):\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/artist.py"", line 55, in draw_wrapper\n    draw(artist, renderer, *args, **kwargs)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/figure.py"", line 886, in draw\n    func(*args)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/artist.py"", line 55, in draw_wrapper\n    draw(artist, renderer, *args, **kwargs)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/axes.py"", line 1983, in draw\n    a.draw(renderer)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/artist.py"", line 55, in draw_wrapper\n    draw(artist, renderer, *args, **kwargs)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/axis.py"", line 1041, in draw\n    ticks_to_draw = self._update_ticks(renderer)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/axis.py"", line 931, in _update_ticks\n    tick_tups = [ t for t in self.iter_ticks()]\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/axis.py"", line 878, in iter_ticks\n    majorLocs = self.major.locator()\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/dates.py"", line 752, in __call__\n    self.refresh()\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/dates.py"", line 761, in refresh\n    dmin, dmax = self.viewlim_to_dt()\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/dates.py"", line 533, in viewlim_to_dt\n    return num2date(vmin, self.tz), num2date(vmax, self.tz)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/dates.py"", line 292, in num2date\n    if not cbook.iterable(x): return _from_ordinalf(x, tz)\n  File ""/intucell/IntuRMS/intustall/deps/osx/lib/python2.7/site-packages/matplotlib/dates.py"", line 206, in _from_ordinalf\n    dt = datetime.datetime.fromordinal(ix)\nValueError: ordinal must be >= 1\n\nIf you suspect this is an IPython bug, please report it at:\n    https://github.com/ipython/ipython/issues\nor send an email to the mailing list at ipython-dev@scipy.org\n\nYou can print a more detailed traceback right now with ""%tb"", or use ""%debug""\nto interactively debug it.\n\nExtra-detailed tracebacks for bug-reporting purposes can be enabled via:\n    %config Application.verbose_crash=True'"
1799,6375476,lodagro,wesm,2012-08-22 12:11:18,2012-09-11 02:40:30,2012-09-11 02:40:30,closed,wesm,0.9,3,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1799,b'.ix() failure for mixed-integer index',"b""from [mailinglist](https://groups.google.com/forum/?fromgroups=#!topic/pystatsmodels/6n0wj9iLKTI)\ncc @ruidc \n\n```\nWith the following DataFrame:\n\ndf = pandas.DataFrame(index=[1, 10, 'C', 'E'], columns=[1, 2, 3])\n\ndf.ix[df.index[:-1]] works\ndf.ix[:-1] doesn't\n\ndf.ix[[1, 10]] does not work\ndf.ix[pandas.Index([1, 10], dtype=df.index.dtype)] works\n```\n\n\n```\ndf.ix[10] does not work\n```\n"""
1796,6361410,lodagro,wesm,2012-08-21 20:21:47,2012-09-18 21:30:14,2012-09-18 21:29:51,closed,wesm,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1796,b'df.xs() level argument behavior changed.',"b""When using xs in combination with a MultiIndex, before it was not needed to specify the *level* argument if *key* started from level 0 and referred to consecutive levels (see also the examples described in #1684), now this no longer works and *level* needs to be defined in case *key* is a list.\n\n```python\nIn [74]: df\nOut[74]: \n                       A         B         C         D\nlvl0 lvl1 lvl2                                        \nfoo  one  -1   -0.945315  0.355323 -1.292034 -0.241153\n           1   -0.798705 -0.155502  0.269164  2.659962\n     two  -1    1.059727 -1.624291 -0.199047 -1.080560\n           1    0.937473 -0.726302  0.848377  0.450377\nbar  one  -1    0.473692  0.135078  0.066102 -0.922182\n           1   -0.059887  0.306716  1.933680 -1.314217\n     two  -1   -0.182526 -0.074111 -1.305636 -0.404485\n           1   -0.913486  1.758551  0.575856  0.560052\n\nIn [75]: df.xs(['foo', 'one'])\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-75-deeb03cdd75e> in <module>()\n----> 1 df.xs(['foo', 'one'])\n...\nTypeError: \n\nIn [76]: df.xs(['foo', 'one'], level=[0, 1])\nOut[76]: \n             A         B         C         D\nlvl2                                        \n-1   -0.945315  0.355323 -1.292034 -0.241153\n 1   -0.798705 -0.155502  0.269164  2.659962\n```"""
1791,6332063,jseabold,wesm,2012-08-20 18:17:04,2012-09-09 17:16:52,2012-09-09 17:16:52,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1791,b'lib.isnullobj fails on list of strings input',"b""Was playing around with the new string functions and noticed this. Don't really know if it's a bug or expected\n\n```\nnp.version.full_version                                                     \n#'1.6.1'\ndf = pandas.DataFrame(['jack and jill','jesse and frank'])\nsplit_df = df[0].str.split(r'\\s+and\\s+')\nsplit_df.to_csv('test')\n#<snip>\n#    75         shape = values.shape\n#     76         result = np.empty(shape, dtype=bool)\n#---> 77         vec = lib.isnullobj(values.ravel())\n#     78         result[:] = vec.reshape(shape)\n#     79 \n#\n#/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_3942eb0-py2.7-linux-x86_64.egg/pandas/lib.so in pandas.lib.isnullobj (pandas/src/tseries.c:5355)()            \n#\n#ValueError: Does not understand character buffer dtype format string ('s')\n\nfrom pandas import lib\nlib.isnullobj(np.array(split_df.ix[0]))                                      \n#<snip>\n#\n#ValueError: Does not understand character buffer dtype format string ('s')\n```"""
1790,6331816,ijmcf,wesm,2012-08-20 18:06:19,2012-09-04 01:45:05,2012-09-04 01:45:05,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1790,b'Creating DatetimeIndex from timezone-aware datetimes shifts the times',"b""Creating a DatetimeIndex from a list of timezone-aware datetimes shift the times according to the offset of the timezone.\n\nUsing an easy way to get a list of datetimes by creating a DatetimeIndex and casting it to a list:\ndr = pandas.date_range('2012-06-02', periods=10, tz=pytz.timezone('US/Eastern'))\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-06-02 00:00:00, ..., 2012-06-11 00:00:00]\nLength: 10, Freq: D, Timezone: US/Eastern\n\npandas.DatetimeIndex(list(a))\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-06-02 04:00:00, ..., 2012-06-11 04:00:00]\nLength: 10, Freq: None, Timezone: US/Eastern\n"""
1789,6329122,jseabold,changhiskhan,2012-08-20 16:04:24,2013-12-04 00:43:04,2012-09-10 18:40:25,closed,,0.9,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1789,b'infer_freq end assumption',"b""I'm not sure whether this is something that should be fixed/changed or not, but right now infer_freq takes the first value in a DatetimeIndex and uses that to impose end of period assumptions. E.g., if I have \n\n```\ndates = sm.tsa.datetools.dates_from_range('1959Q2', '2009Q3')\npandas.Index(dates).inferred_freq\n```\n\nI get 'Q-JUN' just because of where the series starts. Would it be better to just use the default 'Q' -> 'Q-DEC'?"""
1788,6326693,lodagro,lodagro,2012-08-20 14:31:12,2012-08-22 18:38:29,2012-08-22 18:38:29,closed,,,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1788,b'pct_change() fails on series holding int64',"b""```python\nIn [132]: df\nOut[132]: \n    A   B\na  10   8\nb  12  16\nc  12  24\n\nIn [133]: df.dtypes\nOut[133]: \nA    int64\nB    int64\n\nIn [134]: df.pct_change()\nOut[134]: \n     A    B\na  NaN  NaN\nb  0.2  1.0\nc  0.0  0.5\n\nIn [135]: df['A'].pct_change()\n---------------------------------------------------------------------------\n...\nValueError: Invalid dtype for padding\n```"""
1786,6317707,zachcp,changhiskhan,2012-08-20 03:34:54,2012-09-18 20:13:17,2012-09-18 20:13:17,closed,changhiskhan,0.9,2,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1786,b'inconsistency in text handling of pandas read_table() function',"b""I've been trying to get around an inconsistency in how pandas read_table() function handles two very similar tables. A few people have given some suggestions about how to handle this problem on [StackOverflow] (http://stackoverflow.com/questions/12031532/possible-inconsistency-in-text-handling-of-pandas-read-table-function#comment16059573_12031532) and can replicate the error but nobody had a solution. So even though I don't have a detailed Idea of whats going on I wanted to post this here.\n\nI have two files,[AY907538][2] and [AY942707][3] , that I try to load with read_table(). One works fine and the other gives an issue.\n\n\n    # filename:AY942707\n    # this will load with no problem\n    data = read_table('AY942707.hmmdomtblout', header=None, skiprows=3, sep=r'\\s*')\n    \n    ## filename: AY907538\n    data = read_table('AY907538.hmmdomtblout', header=None, skiprows=3, sep=r'\\s*')\n\nThe error report is here:\n\n    ValueError                                Traceback (most recent call last)\n    <ipython-input-17-e0dbd8eee3ac> in <module>()\n     2 \n     3 #temp = get_dataset('AY907538.hmmdomtblout')\n    4 data = read_table('AY907538.hmmdomtblout', header=None, skiprows=3, sep=r'\\s*')\n      5 #data = read_table('test.txt', header=None, skiprows=3, sep=r'\\s*')\n      6 #data = read_table('test2.txt', header=None, skiprows=3, sep=r'\\s*')\n\n    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc in       read_table(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, thousands, comment,     parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter,    encoding, squeeze)\n    282     kwds['encoding'] = None\n    283 \n    284     return _read(TextParser, filepath_or_buffer, kwds)\n    285 \n    286 @Appender(_read_fwf_doc)\n\n    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc in _read(cls, filepath_or_buffer, kwds)\n    189         return parser\n    190 \n    191     return parser.get_chunk()\n    192 \n    193 @Appender(_read_csv_doc)\n\n    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc in get_chunk(self, rows)\n    779             msg = ('Expecting %d columns, got %d in row %d' %\n    780                    (col_len, zip_len, row_num))\n    781             raise ValueError(msg)\n    782 \n    783         data = dict((k, v) for k, v in izip(self.columns, zipped_content))\n\n    ValueError: Expecting 26 columns, got 28 in row 6\n\n\n[2]: https://dl.dropbox.com/u/1803062/AY907538.hmmdomtblout\n  [3]: https://dl.dropbox.com/u/1803062/AY942707.hmmdomtblout"""
1783,6310014,dalejung,wesm,2012-08-19 04:16:28,2012-09-18 17:25:12,2012-09-18 17:21:48,closed,,0.9,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1783,b'Default empty DataFrame to dtype=object',"b""I ran into the following :\n\n```python\nimport pandas as pd\n\ndata = {}\ncolumns = ['symbol', 'price']\ndf = pd.DataFrame(data, columns=columns)\n\ndf.symbol == 'AAPL'\n# TypeError: Could not compare <type 'str'> type with Series\n```\n\nThe problem being that an empty dataframe's dtype defaults to float. I've adjusted for it by checking the len of data and setting the dtype to object when at 0. But I feel like an empty DataFrame should act like an empty list, valid for all list operations. Which in lieu of #549, means casting the widest net possible with dtype=object. """
1778,6294762,ijmcf,wesm,2012-08-17 17:41:15,2012-09-04 01:36:38,2012-09-04 01:36:38,closed,,0.9,5,Bug,https://api.github.com/repos/pydata/pandas/issues/1778,"b'In a DatetimeIndex using a non-static timezone over a time range that crosses the DST line, the time shifts around'","b""For example:\n>>> dr = pandas.date_range('03/06/2012 00:00', periods=25, freq='W-FRI', tz='US/Eastern')\n>>> print dr\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-09 00:00:00, ..., 2012-08-24 01:00:00]\nLength: 25, Freq: W-FRI, Timezone: US/Eastern\n\nThis is midnight at the start, but 1am at the end. Shouldn't it be midnight all along, regardless of DST?\n"""
1777,6294721,ijmcf,wesm,2012-08-17 17:39:32,2012-08-18 01:57:02,2012-08-18 01:56:03,closed,,0.9,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1777,b'DatetimeIndex from timezone-aware datetimes fails',"b'If you try to create a DatetimeIndex from a list of timezone aware datetimes, you get the exception: ""Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True""\n\nFor example:\n>>> import pandas, datetime, pytz\n>>> d = [datetime.datetime(2012, 8, 19, tzinfo=pytz.timezone(\'US/Eastern\'))]\n>>> pandas.DatetimeIndex(d)\n'"
1774,6285287,elypma,wesm,2012-08-17 09:15:45,2012-09-09 02:55:31,2012-09-09 02:55:31,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1774,b'Error when using py2exe & pandas 0.8.1',"b""When compiling pandas 2 errors prevent the compiled installer from running:\n\nIn pandas.core.format (line 155) an unitintialised doc string is used:\n\n__doc__ += docstring_to_string\n\nEasy solution:\n\nif __doc__:\n    __doc__ += docstring_to_string\nelse:\n    __doc__ = docstring_to_string\n\n\nIn pandas.utils.decorators.py line 98:\n\ndocitems = [func.__doc__ if func.__doc__ else '', self.addendum]\n        func.__doc__ = ''.join(docitems)\n        return func\n\nbecomes:\n\ndef __call__(self, func):       \n        docitems = [func.__doc__ if func.__doc__ else '', self.addendum if self.addendum else '']\n        func.__doc__ = ''.join(docitems)\n        return func"""
1773,6263767,spearsem,lodagro,2012-08-16 13:00:45,2013-12-04 00:57:58,2012-09-04 20:28:13,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1773,"b'DataFrame.drop_duplicates works literally only with list of column names, but fails when used on output of DataFrame.columns'","b'`DataFrame.drop_duplicates()` does not properly handle array objects returned by `DataFrame.columns` (whether or not you use `DataFrame.columns.values` to get a NumPy array). If you compute \n\n    list(DataFrame.columns.values)\n\nthen it works, but this is needless overkill, especially when dealing with a large number of columns. Below is an example from IPython.\n\n```python\nIn [71]: dfrm = pandas.DataFrame({""A"":[1,2,1,2,1,2], ""B"":[3,4,3,4,3,4], ""C"":[1,2,1,2,1,3]})\n\nIn [72]: dfrm\nOut[72]:\n   A  B  C\n0  1  3  1\n1  2  4  2\n2  1  3  1\n3  2  4  2\n4  1  3  1\n5  2  4  3\n\nIn [73]: dfrm.drop_duplicates(dfrm.columns)\nERROR: An unexpected error occurred while tokenizing input\nThe following traceback may be corrupted or invalid\nThe error message is: (\'EOF in multi-line statement\', (882, 0))\nERROR: An unexpected error occurred while tokenizing input\nThe following traceback may be corrupted or invalid\nThe error message is: (\'EOF in multi-line statement\', (6442, 0))\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/home/espears/<ipython-input-73-bee9ee352073> in <module>()\n----> 1 dfrm.drop_duplicates(dfrm.columns)\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/core/frame.pyc in drop_duplicates(self, cols, take_last)\n   2254         deduplicated : DataFrame\n   2255         """"""\n-> 2256         duplicated = self.duplicated(cols, take_last=take_last)\n   2257         return self[-duplicated]\n   2258\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/core/frame.pyc in duplicated(self, cols, take_last)\n   2283\n   2284         duplicated = lib.duplicated(keys, take_last=take_last)\n-> 2285         return Series(duplicated, index=self.index)\n   2286\n   2287     #----------------------------------------------------------------------\n\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/core/series.pyc in __new__(cls, data, index, dtype, name, copy)\n    286         else:\n    287             subarr = subarr.view(Series)\n--> 288         subarr.index = index\n    289         subarr.name = name\n    290\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/_tseries.so in pandas._tseries.SeriesIndex.__set__ (pandas/src/tseries.c:73097)()\n\nAssertionError: Index length did not match values\n\nIn [74]: dfrm.drop_duplicates(dfrm.columns.values)\nERROR: An unexpected error occurred while tokenizing input\nThe following traceback may be corrupted or invalid\nThe error message is: (\'EOF in multi-line statement\', (882, 0))\nERROR: An unexpected error occurred while tokenizing input\nThe following traceback may be corrupted or invalid\nThe error message is: (\'EOF in multi-line statement\', (6442, 0))\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/home/espears/<ipython-input-74-cb96df701a9b> in <module>()\n----> 1 dfrm.drop_duplicates(dfrm.columns.values)\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/core/frame.pyc in drop_duplicates(self, cols, take_last)\n   2254         deduplicated : DataFrame\n   2255         """"""\n-> 2256         duplicated = self.duplicated(cols, take_last=take_last)\n   2257         return self[-duplicated]\n   2258\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/core/frame.pyc in duplicated(self, cols, take_last)\n   2283\n   2284         duplicated = lib.duplicated(keys, take_last=take_last)\n-> 2285         return Series(duplicated, index=self.index)\n   2286\n   2287     #----------------------------------------------------------------------\n\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/core/series.pyc in __new__(cls, data, index, dtype, name, copy)\n    286         else:\n    287             subarr = subarr.view(Series)\n--> 288         subarr.index = index\n    289         subarr.name = name\n    290\n\n/opt/epd/7.2-1/lib/python2.7/site-packages/pandas/_tseries.so in pandas._tseries.SeriesIndex.__set__ (pandas/src/tseries.c:73097)()\n\nAssertionError: Index length did not match values\n\nIn [75]: dfrm.columns.values\nOut[75]: array([A, B, C], dtype=object)\n\nIn [76]: list(dfrm.columns.values)\nOut[76]: [\'A\', \'B\', \'C\']\n\nIn [77]: dfrm.drop_duplicates(list(dfrm.columns.values))\nOut[77]:\n   A  B  C\n0  1  3  1\n1  2  4  2\n5  2  4  3\n```\n\nFWIW:\n\n    In [91]: pandas.__version__\n    Out[91]: \'0.7.3\'\n'"
1772,6256204,dalejung,wesm,2012-08-16 03:01:34,2012-09-11 01:13:59,2012-09-11 01:11:58,closed,wesm,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1772,b'Error with downsampling intraday data where end.time() < start.time()',"b'Simple Example\n```python\nimport pandas as pd\nstart = datetime.datetime(1999, 3, 1, 5)\n# end hour is less than start\nend = datetime.datetime(2012, 7, 31, 4)\nbad_ind = pd.date_range(start, end, freq=""30min"")\ndf = pd.DataFrame({\'close\':1}, index=bad_ind)\ntry:\n\tdf.resample(\'AS\', \'sum\')\nexcept ValueError as e:\n    print e\n```\nLong example:\nhttp://nbviewer.maxdrawdown.com/3344040/intraday%20binning%20error.ipynb\n\nTracking it down, it appears that the problem is that `_get_range_edges` carries the time over when downsampling intraday data. So when `generate_range` is called during the `DatetimeIndex` creation, the final bin doesn\'t pass the `while cur <= end` check. \n\nThinking about it,  there are two issues. \n\n1) `generate_range` should never output an index that doesn\'t include end. Maybe something\n```python\n    while True:                                                                          \n        yield cur                                                                        \n                                                                                         \n        # last                                                                           \n        if cur >= end:                                                                   \n            break \n```\n\n2) `_generate_range_edges` should generate a range that is perfectly divisible by the freq. For the downsampling, we\'d have to change the time by adjusting the end time or just zeroing both out. I don\'t know how many rely on this behavior though. \n\n'"
1771,6241640,RutgerK,wesm,2012-08-15 14:35:02,2012-08-16 09:24:04,2012-08-15 19:44:38,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1771,"b'Resampling with custom freq length: ""Values falls before first bin""'","b""    print pandas.__version__\n0.8.1\n\n    dtrange = pandas.date_range(start='1-1-2009', periods=21, freq='D')\n\n    raw = pandas.Series(np.arange(len(dtrange)), index=dtrange)\n    test1 = raw.resample('7D', how='count', closed='right')\n\n    print test1\n\n2009-01-01    1\n2009-01-08    7\n2009-01-15    7\n2009-01-22    6\n\n    test2 = raw.resample('7D', how='count', closed='left', label='left')\n    print test2\n\nValueError: Values falls before first bin\n\nWhen trying to resample 21 days to three weeks i ran into the issue above. Closing to the right puts the 21 days in four bins, closing to the left (which i think does what i want) fails with the error above.\n\nChanging the frequency from '7D' to 'W' makes the code run fine so it might have something to do with the 'custom' frequency length."""
1768,6236413,y-p,y-p,2012-08-15 08:58:26,2012-09-08 21:11:58,2012-08-18 21:18:29,closed,,0.9,0,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/1768,b'console_encode() fails on unicode when using ipython-notebook',b'converted to pull request #1782'
1764,6214168,dhirschfeld,wesm,2012-08-14 11:58:32,2012-11-04 21:21:59,2012-11-04 21:21:59,closed,changhiskhan,0.9.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1764,b'How argument ignored in to_timestamp',"b""As discussed in https://groups.google.com/group/pydata/browse_thread/thread/180249bd46ed1944 the following fails:\n\n```python\ndef test_to_timstamp_how():\n    months = pd.period_range('01-Aug-2012', periods=1, freq='M')\n    assert months[0].asfreq('S','S').to_timestamp() == months.to_timestamp(how='start')[0]\n#\n```"""
1763,6190428,lesteve,wesm,2012-08-13 13:43:47,2012-08-17 18:37:41,2012-08-17 18:37:41,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1763,b'pandas.DatetimeIndex.isin always returns an array of False',"b""pandas version: 0.8.2.dev-742d7fb\n\n```python\nimport pandas\nimport pandas.util.testing as put\nindex = put.makeDateIndex(4)\nindex.isin(index) # returns array([False, False, False, False], dtype=bool)\n```\n\nIt works fine with pandas 0.7.3, i.e. it returns an array of True instead. This works fine as well with a pandas.Int64Index. ```index.isin(index)``` just calls ```pandas.lib.ismember(index._array_values(), set(index))``` but I am afraid I lack the skills to understand what goes wrong in the cython code.\n\nJust for some background, the original problem I bumped into:\n\n```python\nimport numpy as np\nimport pandas\nts = pandas.TimeSeries(data=[1,2,3], index=[ np.datetime64(x) for x in ['2000', '2000', '2002'] ])\nts.ix[ts.index] # returns an empty timeseries whereas ts[ts.index] works fine\n```\n\nI tracked it down to ```ts.ix._get_item_iterable``` which has a special case for when the index has duplicates which use pandas.TimeSeries.isin.\n\nAs an aside it looks like pandas.core.common._asarray_tuplesafe(index) is wrong when using numpy 1.6 so that ts.ix[ts.index] still would return an empty array if this bug was fixed, but I guess that's a numpy 1.6 datetime64 bug which I have seen mentioned in a few other places.\n"""
1761,6188768,janschulz,wesm,2012-08-13 12:19:34,2012-08-13 21:50:47,2012-08-13 21:50:47,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1761,b'DataFrame.corr() throws error on simple data',"b'```\ndf3 = DataFrame({""a"":[1,2,3,4], ""b"":[1,2,3,4]})\ndf3\n   a  b\n0  1  1\n1  2  2\n2  3  3\n3  4  4\ndf3.corr()\nTraceback (most recent call last):\n  File ""<console>"", line 1, in <module>\n  File ""C:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 3867, in corr\n    correl = lib.nancorr(mat)\n  File ""moments.pyx"", line 284, in pandas.lib.nancorr (pandas\\src\\tseries.c:67792)\nValueError: Buffer dtype mismatch, expected \'float64_t\' but got \'long long\'\n```'"
1760,6186204,janschulz,wesm,2012-08-13 09:34:02,2012-09-10 09:45:20,2012-09-10 03:27:18,closed,,0.9,5,Bug;Docs,https://api.github.com/repos/pydata/pandas/issues/1760,b'google search returns old documentation',"b'Searching google for ""pandas groupby"" returns links to the 0.7.0 or older documentation on sourceforge.net in the first few places and only then the pydata links. It would be nice if sourceforge could be cleaned up, so that google returns links to the current docs :-)\n\nThanks!'"
1759,6179563,wesm,wesm,2012-08-12 20:03:52,2012-09-18 20:22:29,2012-09-18 20:22:29,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1759,b'pandas.unique needs to be more careful on index types',"b'e.g.\n\n```\nIn [2]: mindex = MultiIndex.from_arrays([np.arange(5).repeat(5), np.tile(np.arange(5), 5)])\n\nIn [3]: pd.unique(mindex)\nOut[3]: array([], dtype=object)\n```'"
1757,6165207,njsmith,wesm,2012-08-10 22:23:29,2012-08-13 09:24:39,2012-08-13 03:20:13,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1757,b'MultiIndexes containing >=1000000 elements do not work',"b'With current master, any attempt to index into a Series (or whatever) with a MultiIndex and >=1000000 (= _SIZE_CUTOFF) elements simply raises an error:\n\n```\nIn [5]: n = 1000000 - 1; pandas.Series(np.arange(n), pandas.MultiIndex.from_arrays(([""a""] * n, np.arange(n))))[(""a"", 5)]\nOut[5]: 5\n\nIn [6]: n = 1000000; pandas.Series(np.arange(n), pandas.MultiIndex.from_arrays(([""a""] * n, np.arange(n))))[(""a"", 5)]\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-6-a83549b6c664> in <module>()\n----> 1 n = 1000000; pandas.Series(np.arange(n), pandas.MultiIndex.from_arrays(([""a""] * n, np.arange(n))))[(""a"", 5)]\n\n/home/njs/src/pandas/pandas/core/series.pyc in __getitem__(self, key)\n    453             key = np.asarray(key, dtype=bool)\n    454 \n--> 455         return self._get_with(key)\n    456 \n    457     def _get_with(self, key):\n\n/home/njs/src/pandas/pandas/core/series.pyc in _get_with(self, key)\n    471             if isinstance(key, tuple):\n    472                 try:\n--> 473                     return self._get_values_tuple(key)\n    474                 except:\n    475                     if len(key) == 1:\n\n/home/njs/src/pandas/pandas/core/series.pyc in _get_values_tuple(self, key)\n    514 \n    515         # If key is contained, would have returned by now\n--> 516         indexer, new_index = self.index.get_loc_level(key)\n    517         return Series(self.values[indexer], index=new_index, name=self.name)\n    518 \n\n/home/njs/src/pandas/pandas/core/index.pyc in get_loc_level(self, key, level)\n   2069             if not any(isinstance(k, slice) for k in key):\n   2070                 if len(key) == self.nlevels:\n-> 2071                     return self._engine.get_loc(key), None\n   2072                 else:\n   2073                     # partial selection\n\n/home/njs/src/pandas/pandas/lib.so in pandas.lib.IndexEngine.get_loc (pandas/src/tseries.c:107666)()\n\n/home/njs/src/pandas/pandas/lib.so in pandas.lib.IndexEngine.get_loc (pandas/src/tseries.c:107423)()\n\n/home/njs/src/pandas/pandas/lib.so in util.get_value_at (pandas/src/tseries.c:114800)()\n\nTypeError: only integer arrays with one element can be converted to an index\n```\n'"
1756,6162186,wesm,wesm,2012-08-10 21:08:32,2012-08-10 22:05:19,2012-08-10 22:05:19,closed,,0.9,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1756,b'Time zone conversion routines assume monotonic timestamps',
1755,6159276,jseabold,wesm,2012-08-10 18:42:45,2012-08-10 22:10:22,2012-08-10 22:10:21,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1755,b'isnull returns a boolean for array-like inputs',"b'In numpy\n\n```python\n>>> np.isnan([[False]])\narray([[False]], dtype=bool)\n```\n\nIn pandas\n\n```python\n>>> pandas.isnull([[False]])                                                  \nFalse\n\n>>> pandas.isnull([[1],[2]])\nFalse\n```\n\nI would expect these to return arrays for array-like input. Is there any reason not to?'"
1753,6154126,lodagro,wesm,2012-08-10 14:44:18,2012-08-10 15:31:47,2012-08-10 15:31:41,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1753,b'Failure when constructing a DataFrame from multiple time series.',"b""From [stackoverflow](http://stackoverflow.com/questions/11848578/pandas-messing-up-data-frame).\n\nConstructing a DataFrame from multiple time series fails, all values from the resulting frame are NaN (which is not the case in the individual series).\n\nCode to reproduce: http://pastebin.com/WTsCkKrL\n\nI could reproduce it.\n\n```python\nIn [101]: np.__version__\nOut[101]: '1.6.1'\n\nIn [102]: pandas.__version__\nOut[102]: '0.8.2.dev-8cde377'\n\nIn [103]: df\nOut[103]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 386 entries, 1992-06-05 15:50:11.527680 to 1774-08-13 02:00:15.237103\nData columns:\n770000000006    0  non-null values\n770000000009    0  non-null values\n770000000010    0  non-null values\n770000000011    0  non-null values\n770000000012    0  non-null values\n770000000013    0  non-null values\n770000000018    0  non-null values\n770000000020    0  non-null values\n770000000021    0  non-null values\n770000000022    0  non-null values\n770000000024    0  non-null values\n770000000029    0  non-null values\n770000000030    0  non-null values\n770000000032    0  non-null values\n770000000034    0  non-null values\n770000000049    0  non-null values\ndtypes: float64(16)\n```"""
1750,6140363,manuteleco,wesm,2012-08-09 23:37:20,2012-08-13 02:45:11,2012-08-13 02:45:01,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1750,b'Assignment to DataFrame through .ix with non-unique MultiIndex',"b'Hi,\n\nI think there might be a bug in the assignment to a DataFrame object through .ix when non-unique MultiIndexes are involved. The next sample code describes this issue:\n\n```\nIn [1]: from pandas import DataFrame, MultiIndex\n\nIn [2]: \n\nIn [2]: df = DataFrame([[1, 1, ""x"", ""X""], [1, 1, ""y"", ""Y""], [1, 2, ""z"", ""Z""]],\n   ...:                columns=list(""ABCD""))\n\nIn [3]: df = df.set_index(list(""AB""))\n\nIn [4]: df\nOut[4]: \n     C  D\nA B      \n1 1  x  X\n  1  y  Y\n  2  z  Z\n\nIn [5]: \n\nIn [5]: ix = MultiIndex.from_tuples([(1, 1)])\n\nIn [6]: df.ix[ix, ""C""]  # Shows both values with index (1, 1)\nOut[6]: \nA  B\n1  1    x\n   1    y\nName: C\n\nIn [7]: \n\nIn [7]: df.ix[ix, ""C""] = ""_""  # Only sets one of the two values with index (1, 1): BUG?\n\nIn [8]: df\nOut[8]: \n     C  D\nA B      \n1 1  x  X\n  1  _  Y\n  2  z  Z\n\n\n```\n\nThanks and regards.\nManu.'"
1749,6125953,lodagro,wesm,2012-08-09 12:50:40,2012-08-12 22:16:08,2012-08-12 22:16:08,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1749,b'Series __repr__ Exception when series hold DataFrames',"b""from [stackoverflow](http://stackoverflow.com/questions/11870058/pandas-stacking-dataframes-generated-by-apply)\n\n```python\nIn [67]: s = pandas.Series([pandas.DataFrame(np.random.randn(2,2)) for i in range(5)])\n\nIn [68]: s\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nException ValueError: ValueError('Cannot call bool() on DataFrame.',) in 'util._checknull' ignored\nOut[68]:\n0              0         1\n0  1.821894 -1.268104\n1  0.\n1              0         1\n0 -1.361116  0.809527\n1 -0.\n2              0         1\n0  0.603496  0.034081\n1 -0.\n3              0         1\n0 -0.274753 -0.544456\n1  0.\n4              0         1\n0 -0.699664  0.366191\n1 -0.\n```"""
1745,6100949,beaumartinez,wesm,2012-08-08 11:55:36,2012-08-10 16:06:24,2012-08-10 15:23:45,closed,,,5,Bug,https://api.github.com/repos/pydata/pandas/issues/1745,b'Creating a DataFrame from Series with unsorted indeces gives completely bad data',"b""Creating a DataFrame from Series with unsorted indeces gives completely bad data.\n\nI've [written a module](https://gist.github.com/3294509/e777009080fbcbe3c6c91baeb3a71fbb1a092cdc) that reproduces the bug\xa1\xaaunless this is by design. Give it a try.\n\n(I'm running `pandas==0.8.1`, Python 2.7.2, and OS X Mountain Lion.)"""
1741,6089468,danxshap,wesm,2012-08-07 21:29:23,2012-09-08 21:11:23,2012-09-08 21:11:23,closed,,0.9,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1741,"b""Errors in 0.8.1 when run with Django/uWSGI, but not with Django's dev runserver""","b'I\'m using pandas in a Django application running on Ubuntu 12.04 and after upgrading from pandas 0.6.1 to 0.8.1 I\'m getting some errors that completely prevent the application from running.\n\nIf I run Django\'s development server (manage.py runserver), everything works fine, but if instead I run uWSGI I see the following 2 tracebacks in my uwsgi log:\n```\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py"", line 101, in get_response\n    request.path_info)\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py"", line 298, in resolve\n    for pattern in self.url_patterns:\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py"", line 328, in url_patterns\n    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py"", line 323, in urlconf_module\n    self._urlconf_module = import_module(self.urlconf_name)\n  File ""/usr/local/lib/python2.7/dist-packages/django/utils/importlib.py"", line 35, in import_module\n    __import__(name)\n  File ""/sites/ycharts/urls.py"", line 5, in <module>\n    from apps.companies.sitemaps import CompanySitemap\n  File ""/sites/ycharts/apps/companies/sitemaps.py"", line 6, in <module>\n    from apps.companies.models import Company\n  File ""/sites/ycharts/apps/companies/models.py"", line 19, in <module>\n    from apps.main.utils import googlesearch, wikipedia, date_utils, data_utils, \\\n  File ""/sites/ycharts/apps/main/utils/data_utils.py"", line 3, in <module>\n    import pandas\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/__init__.py"", line 14, in <module>\n    import pandas.lib as lib\nAttributeError: \'module\' object has no attribute \'lib\'\n```\n\nand\n```\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py"", line 101, in get_response\n    request.path_info)\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py"", line 298, in resolve\n    for pattern in self.url_patterns:\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py"", line 328, in url_patterns\n    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)\n  File ""/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py"", line 323, in urlconf_module\n    self._urlconf_module = import_module(self.urlconf_name)\n  File ""/usr/local/lib/python2.7/dist-packages/django/utils/importlib.py"", line 35, in import_module\n    __import__(name)\n  File ""/sites/ycharts/urls.py"", line 5, in <module>\n    from apps.companies.sitemaps import CompanySitemap\n  File ""/sites/ycharts/apps/companies/sitemaps.py"", line 6, in <module>\n    from apps.companies.models import Company\n  File ""/sites/ycharts/apps/companies/models.py"", line 19, in <module>\n    from apps.main.utils import googlesearch, wikipedia, date_utils, data_utils, \\\n  File ""/sites/ycharts/apps/main/utils/data_utils.py"", line 3, in <module>\n    import pandas\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/__init__.py"", line 28, in <module>\n    from pandas.core.api import *\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/api.py"", line 10, in <module>\n    from pandas.core.format import (set_printoptions, reset_printoptions,\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/format.py"", line 147, in <module>\n    class DataFrameFormatter(object):\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/format.py"", line 156, in DataFrameFormatter\n    __doc__ += docstring_to_string\nTypeError: unsupported operand type(s) for +=: \'NoneType\' and \'str\n```\n\nAfter reading in #284, I thought it may be a path issue so I tried overwriting `sys.path` in my uwsgi configuration file so that it\'s the exact same path that the dev server sets up for me, but that didn\'t help.\n\nAny tips / debugging ideas / help on the above would be greatly appreciated.\n\nThank you!\n\n\n\n'"
1738,6089141,wesm,y-p,2012-08-07 21:16:32,2013-03-23 01:53:06,2013-03-23 01:53:06,closed,y-p,0.11,2,Bug;Can't Repro;Groupby,https://api.github.com/repos/pydata/pandas/issues/1738,b'Returning Series of dicts from DataFrame.apply',"b""from the mailing list cc @lodagro \n\n```\nThat is indeed odd.\nThere also seems to be a dependency on the dtype. Below an example for all ints and all floats.\n\nIn [121]: df\nOut[121]: \n   A  B\na  7  0\nb  7 -2\n\nIn [122]: df.apply(foo, 1)\nOut[122]: \n    A   B\na NaN NaN\nb NaN NaN\n\nAll NaN???\nI would expect to see this:\n\nIn [123]: s = pandas.Series([foo(row[1]) for row in df.iterrows()], df.index)\n\nIn [124]: s\nOut[124]: \na     {'properties': {'A': 7, 'B': 0}}\nb    {'properties': {'A': 7, 'B': -2}}\n\nSeems to work fine for all floats.\n\nIn [125]: df = pandas.DataFrame(np.random.randn(2, 2), columns=list('AB'), index=list('ab'))\n\nIn [126]: df\nOut[126]: \n          A         B\na -0.407883  0.018206\nb -1.081038  0.492944\n\nIn [127]: df.apply(foo, 1)\nOut[127]: \na    {'properties': {'A': -0.407882576359619, 'B': 0.0\nb    {'properties': {'A': -1.081038117264707, 'B': 0.4\n```"""
1737,6080561,jgarcke,jgarcke,2012-08-07 15:23:26,2012-08-13 12:44:42,2012-08-13 08:02:16,closed,changhiskhan,0.9,5,Bug,https://api.github.com/repos/pydata/pandas/issues/1737,b'plot of Series fails',"b""The example here fails with 0.8.1 (worked with 0.8.0)\nhttp://pandas.pydata.org/pandas-docs/dev/computation.html?highlight=outlier#moving-rolling-statistics-moments\n\nIn [1]: from pandas import *\n\nIn [2]: from numpy.random import randn\n\nIn [3]: ts = Series(randn(1000), index=date_range('1/1/2000', periods=1000))\n\nIn [4]: ts = ts.cumsum()\n\nIn [5]: ts.plot(style='k--')\n/usr/lib64/python2.6/site-packages/matplotlib/backends/backend_gtk.py:621: DeprecationWarning: Use the new widget gtk.Tooltip\n  self.tooltips = gtk.Tooltips()\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\n/home/jgarcke/code/python_fx_tools/<ipython console> in <module>()\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.1-py2.6-linux-x86_64.egg/pandas/tools/plotting.pyc in plot_series(series, label, kind, use_index, rot, xticks, yticks, xlim, ylim, ax, style, grid, logy, secondary_y, **kwds)\n   1292                      secondary_y=secondary_y, **kwds)\n   1293 \n-> 1294     plot_obj.generate()\n   1295     plot_obj.draw()\n   1296 \n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.1-py2.6-linux-x86_64.egg/pandas/tools/plotting.pyc in generate(self)\n    573         self._compute_plot_data()\n    574         self._setup_subplots()\n--> 575         self._make_plot()\n    576         self._post_plot_logic()\n    577         self._adorn_subplots()\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.1-py2.6-linux-x86_64.egg/pandas/tools/plotting.pyc in _make_plot(self)\n    858         if self.use_index and self._use_dynamic_x():\n    859             data = self._maybe_convert_index(self.data)\n--> 860             self._make_ts_plot(data, **self.kwds)\n    861         else:\n    862             import matplotlib.pyplot as plt\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.1-py2.6-linux-x86_64.egg/pandas/tools/plotting.pyc in _make_ts_plot(self, data, **kwargs)\n    918 \n    919             newlines = tsplot(data, plotf, ax=ax, label=label, style=self.style,\n--> 920                              **kwargs)\n    921             ax.grid(self.grid)\n    922             lines.append(newlines[0])\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.1-py2.6-linux-x86_64.egg/pandas/tseries/plotting.pyc in tsplot(series, plotf, **kwargs)\n     80     # set date formatter, locators and rescale limits\n\n     81     format_dateaxis(ax, ax.freq)\n---> 82     left, right = _get_xlim(ax.get_lines())\n     83     ax.set_xlim(left, right)\n     84 \n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.1-py2.6-linux-x86_64.egg/pandas/tseries/plotting.pyc in _get_xlim(lines)\n    210     for l in lines:\n    211         x = l.get_xdata()\n--> 212         left = min(x[0].ordinal, left)\n    213         right = max(x[-1].ordinal, right)\n    214     return left, right\n\nAttributeError: 'numpy.int64' object has no attribute 'ordinal'\n"""
1736,6061045,lodagro,wesm,2012-08-06 20:07:04,2012-08-12 20:37:34,2012-08-12 20:37:34,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1736,b'Series/DataFrame repr fails for MultiIndex level names with unicode',"b""```python\nIn [34]: tuples = [t for t in product(range(2), range(2))]\n\nIn [35]: index = pandas.MultiIndex.from_tuples(tuples, names=[u'\\u0394', 'i1'])\n\nIn [36]: df = pandas.DataFrame(np.random.randn(4,4), index=index)\n\nIn [37]: s = pandas.Series(range(4), index=index)\n\nIn[38]: df\n...\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u0394' in position 0: ordinal not in range(128)\n\nIn[38]: s\n...\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u0394' in position 0: ordinal not in range(128)\n\n```"""
1734,6033453,wesm,wesm,2012-08-04 23:00:11,2012-08-10 16:49:40,2012-08-10 16:49:40,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1734,b'Fix statsmodels import in pandas.stats.var or remove module',b'cc @blais '
1732,6033377,kdebrab,changhiskhan,2012-08-04 22:42:38,2012-09-08 22:13:09,2012-08-20 05:36:24,closed,changhiskhan,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1732,b'Error when trying to plot time series with different frequencies',"b""Pandas 0.8.1:\n\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    rng_a = pd.date_range('1/1/2012', periods=4, freq='2H'')\n    rng_b = pd.date_range('1/1/2012', periods=4, freq='H')\n    rng_c = pd.date_range('1/1/2012', periods=4, freq='30T')\n    a = pd.Series([0, 1, 3, 6], rng_a)\n    b = pd.Series([0, 1, 3, 6], rng_b)\n    c = pd.Series([0, 1, 3, 6], rng_c)\n    a.plot()\n    b.plot() # works fine\n    c.plot() # ValueError: Incompatible frequency conversion\n    plt.show() # only a and b are plotted\n\nb.plot() works fine, but c.plot() doesn't and returns the following error:\n\n    ValueError: Incompatible frequency conversion\n\nWhen building the timeseries without specifying their frequency, no error is reported when trying to plot them, but there seems to be another issue occuring:\n\n    a = pd.Series([0, 1, 2, 3], ['1/1/2012 00:00:00', '1/1/2012 02:00:00', '1/1/2012 04:00:00', '1/1/2012 06:00:00'])\n    b = pd.Series([0, 1, 2, 3], ['1/1/2012 00:00:00', '1/1/2012 01:00:00', '1/1/2012 02:00:00', '1/1/2012 03:00:00'])\n    c = pd.Series([0, 1, 2, 3], ['1/1/2012 00:00:00', '1/1/2012 00:30:00', '1/1/2012 01:00:00', '1/1/2012 01:30:00'])\n    a.plot()\n    b.plot() # no error\n    c.plot() # no error\n    plt.show() # only c is plotted\n\nNo errors are reported, but only series c is plotted."""
1728,6016411,CRP,lodagro,2012-08-03 14:54:44,2012-08-05 19:48:41,2012-08-05 19:45:44,closed,lodagro,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1728,b'to_html issues',"b'- doc string mentions a ""justify"" parameter that apparently does not exist and provokes an errore if used\n- setting index=False produces no visible effect'"
1727,6011789,grsr,wesm,2012-08-03 10:13:51,2012-08-12 21:35:09,2012-08-12 21:34:54,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1727,b'inconsistent results building a DataFrame from a dict of Series with MultiIndexes',"b""I am trying to build a DataFrame from a dict of Series objects which have (not necessarily exactly matching) MultiIndex indices, sometimes I don't get any results for some particular data point and so I create an empty Series object and add that to the dict, as I need there to be a column present even without any data (later I convert all NAs to 0). This seems to work sometimes, but other times I get an error message that implies all the Series need hierarchical indices, it seems to depend on the order in which the Series are added to the dict. See example session below, the first time I create the DataFrame it behaves just as I'd like, giving me a df index that is the union of all the indices in the populated Series and supplying NaNs wherever there is no data, but the second time it blows up. Perhaps I shouldn't be relying on this behaviour but it seems that the results should at least be consistent.\n\nAny tips on how to solve this in a cleaner way also very welcome. Thanks.\n\n```python\nIn [292]: pandas.__version__\nOut[292]: '0.8.1'\n\nIn [293]: s1 = Series([1,2,3,4], index=MultiIndex.from_tuples([(1,2),(1,3),(2,2),(2,4)]))\n\nIn [294]: s2 = Series([1,2,3,4], index=MultiIndex.from_tuples([(1,2),(1,3),(3,2),(3,4)]))\n\nIn [295]: s3 = Series()\n\nIn [296]: df = DataFrame.from_dict({'foo':s1, 'bar':s2, 'baz':s3})\n\nIn [297]: df\nOut[297]: \n     bar  baz  foo\n1 2    1  NaN    1\n  3    2  NaN    2\n2 2  NaN  NaN    3\n  4  NaN  NaN    4\n3 2    3  NaN  NaN\n  4    4  NaN  NaN\n\nIn [298]: df = DataFrame.from_dict({'foo':s1, 'baz':s3, 'bar':s2})\n\n... stacktrace\n\nTypeError: can only call with other hierarchical index objects\n```\n"""
1726,6007572,dalejung,wesm,2012-08-03 03:49:52,2013-11-24 13:44:10,2012-08-13 01:08:30,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1726,"b""Error with closed='left' and _adjust_bin_edges""","b'```python\nimport pandas as pd\n\n# 8/6/12 is a Monday\nind = pd.DatetimeIndex(start=""8/6/2012"", end=""8/26/2012"", freq=""D"")\nn = len(ind)\ndata = [[x] * 5 for x in range(n)]\ndf = pd.DataFrame(data, columns=[\'open\', \'high\', \'low\', \'close\', \'vol\'], index=ind)\n\ndf.resample(\'W-MON\', how=\'first\', closed=\'left\', label=\'left\')\n```\n\nWill result in an error. I tracked it down to \n\n```python\n      def _adjust_bin_edges(self, binner, ax_values):\n        # Some hacks for > daily data, see #1471, #1458, #1483\n\n        bin_edges = binner.asi8\n\n        if self.freq != \'D\' and is_superperiod(self.freq, \'D\'):\n            day_nanos = _delta_to_nanoseconds(timedelta(1))\n            if self.closed == \'right\':\n                bin_edges = bin_edges + day_nanos - 1\n            else:\n                bin_edges = bin_edges + day_nanos\n```\n\nI\'m not sure why the day is added when closed=\'left\'. Even on datasets that don\'t start on a Monday, and thus don\'t error, it\'ll cause the intervals to start on Tuesday.'"
1721,5987693,tschm,wesm,2012-08-02 09:00:24,2012-08-09 02:24:57,2012-08-09 02:24:57,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1721,b'np.fix crashes with pandas time series',"b'        import numpy as np\n        import pandas\n\n        x = pandas.TimeSeries(data = np.array([-0.54548407,  2.81105692, np.nan, -1.61954275, 1.3670269,  -0.73211055, -0.22832786,  0.06384124, -1.7113508,  -2.42429978]), index = np.arange(0,10))\n       \n`print np.fix(x)`\n\n\ncrashes with:\n\n```\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\lib\\ufunclike.py"", line 48, in fix\n    y[...] = nx.where(x >= 0, y1, y2)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 540, in __setitem__\n    raise KeyError(\'%s not in this series!\' % str(key))\nKeyError: \'Ellipsis not in this series!\'\n```\n\nI assume this should work?\nThomas\n'"
1720,5983047,wesm,wesm,2012-08-02 01:27:47,2012-08-12 21:57:29,2012-08-12 21:57:29,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1720,b'DataFrame.to_records failure with DatetimeIndex',"b'http://stackoverflow.com/questions/11754334/dataframe-to-records-gives-an-error-when-typeindex-is-datetimeindex\n\n```\nIn [20]: df = read_clipboard(sep=\'\\s+\', parse_dates=True)\n\nIn [21]: df\nOut[21]: \n                   a         b         c\n2012-08-01  2.355928 -2.465061  0.240094\n2012-08-02 -0.952323  0.746623 -0.384021\n2012-08-03  1.460156  0.292560 -0.494793\n2012-08-04 -0.989584 -1.630384  1.373587\n2012-08-05  0.014760 -0.789603 -0.622780\n\nIn [22]: df.to_records()\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-22-6d3142e97d2d> in <module>()\n----> 1 df.to_records()\n\n/Users/wesm/code/pandas/pandas/core/frame.pyc in to_records(self, index)\n    889             names = list(map(str, self.columns))\n    890 \n--> 891         return np.rec.fromarrays(arrays, names=names)\n    892 \n    893     @classmethod\n\n/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/site-packages/numpy/core/records.pyc in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\n    546     # Determine shape from data-type.\n    547     if len(descr) != len(arrayList):\n--> 548         raise ValueError, ""mismatch between the number of fields ""\\\n    549               ""and the number of arrays""\n    550 \n\nValueError: mismatch between the number of fields and the number of arrays\n```'"
1719,5982960,wesm,wesm,2012-08-02 01:25:10,2012-08-09 02:26:52,2012-08-09 02:26:52,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1719,"b""concat with axis=1, join='outer' not working correctly""","b""http://stackoverflow.com/questions/11761884/pandas-concatouter-not-doing-union\n\n```\n\nIt looks pandas.concat is doing 'left outer' join instead of just union the indexes. Seems a bug to me but maybe I'm missing something obvious.\n\n    import pandas\n    import pandas.util.testing as put\n    ts1 = put.makeTimeSeries()\n    ts2 = put.makeTimeSeries()[::2]\n    ts3 = put.makeTimeSeries()[::3]\n    ts4 = put.makeTimeSeries()[::4]\n\n    ## to join with union\n    ## these two are of different length!\n    pandas.concat([ts1,ts2], join='outer', axis = 1) \n    pandas.concat([ts2,ts1], join='outer', axis = 1)\nAny idea how can I get the full union (as they do claim by using join='outer' on the pandas document)\n\nThanks.\n```"""
1717,5965308,gerigk,wesm,2012-08-01 10:52:31,2012-08-09 03:14:33,2012-08-09 03:11:50,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1717,b'groupby.first() fails for np.datetime64 columns',"b'numpy 1.7 dev and pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64\n```\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([(3,np.datetime64(\'2012-07-03\')),(3,np.datetime64(\'2012-07-04\'))], columns = [\'a\', \'date\'])\ndf.groupby(\'a\').first()\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-10-56fc44df2d9e> in <module>()\n      2 import numpy as np\n      3 df = pd.DataFrame([(3,np.datetime64(\'2012-07-03\')),(3,np.datetime64(\'2012-07-04\'))], columns = [\'a\', \'date\'])\n----> 4 df.groupby(\'a\').first()\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in f(self)\n     25             return self._cython_agg_general(alias)\n     26         except Exception:\n---> 27             return self.aggregate(lambda x: npfunc(x, axis=self.axis))\n     28 \n     29     f.__doc__ = ""Compute %s of group values"" % name\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\n   1501                 return self._python_agg_general(arg, *args, **kwargs)\n   1502             else:\n-> 1503                 result = self._aggregate_generic(arg, *args, **kwargs)\n   1504 \n   1505         if not self.as_index:\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in _aggregate_generic(self, func, *args, **kwargs)\n   1564                     result[name] = data.apply(wrapper, axis=axis)\n   1565 \n-> 1566         return self._wrap_generic_output(result, obj)\n   1567 \n   1568     def _wrap_aggregated_output(self, output, names=None):\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/groupby.pyc in _wrap_generic_output(self, result, obj)\n   1763             if self.axis == 0:\n   1764                 result = DataFrame(result, index=obj.columns,\n-> 1765                                    columns=result_index).T\n   1766             else:\n   1767                 result = DataFrame(result, index=obj.index,\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    371             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    372         elif isinstance(data, dict):\n--> 373             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    374         elif isinstance(data, ma.MaskedArray):\n    375             mask = ma.getmaskarray(data)\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in _init_dict(self, data, index, columns, dtype)\n    459 \n    460         # don\'t force copy because getting jammed in an ndarray anyway\n--> 461         homogenized = _homogenize(data, index, columns, dtype)\n    462 \n    463         # from BlockManager perspective\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in _homogenize(data, index, columns, dtype)\n   4879 \n   4880             v = _sanitize_array(v, index, dtype=dtype, copy=False,\n-> 4881                                 raise_cast_failure=False)\n   4882 \n   4883         homogenized[k] = v\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64.egg/pandas/core/series.pyc in _sanitize_array(data, index, dtype, copy, raise_cast_failure)\n   2724             else:\n   2725                 subarr = np.empty(len(index), dtype=dtype)\n-> 2726             subarr.fill(value)\n   2727         else:\n   2728             return subarr.item()\n\nValueError: Cannot convert from specific units to generic units in NumPy datetimes or timedeltas\n```'"
1711,5942018,mcobzarenco,wesm,2012-07-31 12:32:20,2012-08-12 18:48:09,2012-08-12 18:48:09,closed,,0.9,3,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1711,b'pandas.Series.plot ignores color in **kwarg',"b""E.g.:\n\n```python\npandas.Series(randn(10)).plot(color='red')\n```\n\nWhose output is:\n\n![Series Plot](http://i.imgur.com/dYVV2.png)\n\n\nThis used to work correctly in pandas 0.7.2 """
1708,5928761,kdebrab,wesm,2012-07-30 20:33:54,2012-08-10 16:55:17,2012-08-10 16:55:00,closed,,0.9,1,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1708,"b'Unexpected result with union, intersection and diff on Index objects'","b""Pandas 0.8.1:\n\n    import pandas as pd\n    import datetime as dt\n    index_1 = pd.DatetimeIndex([dt.datetime(2012,1,1,0), dt.datetime(2012,1,1,12),\n        dt.datetime(2012,1,2,0), dt.datetime(2012,1,2,12)])\n    index_2 = index_1 + pd.DateOffset(hours=1)\n    index_1 & index_2\n\ncorrectly returns:\n\n    <class 'pandas.tseries.index.DatetimeIndex'>\n    Length: 0, Freq: None, Timezone: None\n\nBut when building the same Index objects by specifying their frequency:\n\n    index_1 = pd.date_range('1/1/2012', periods=4, freq='12H')\n    index_2 = index_1 + pd.DateOffset(hours=1)\n    index_1 & index_2\n\nunexpectedly results in:\n\n    <class 'pandas.tseries.index.DatetimeIndex'>\n    [2012-01-01 12:00:00, ..., 2012-01-02 12:00:00]\n    Length: 3, Freq: 12H, Timezone: None\n\nThe same issue occurs when directly calling the intersection() method and similar unexpected results occur for union (|, +) and diff (-) operators.\n\nFor information, combining append() and order() methods as an alternative for the union operators does give the correct result, independently how index_1 and index_2 are built:\n\n    index_1 = pd.date_range('1/1/2012', periods=4, freq='12H')\n    index_2 = index_1 + pd.DateOffset(hours=1)\n    index_1.append(index_2).order()\n\ncorrectly results in:\n\n    [2012-01-01 00:00:00, ..., 2012-01-02 13:00:00]\n    Length: 8, Freq: None, Timezone: None"""
1707,5927631,eriknw,wesm,2012-07-30 19:55:06,2012-08-12 01:31:59,2012-08-12 01:31:39,closed,,0.9,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1707,"b""Can't save empty Series or DataFrame to hdf5 with HDFStore""","b'With pandas 0.8.1 (and pytables 2.3.1), trying to save an empty Series or DataFrame when using HDFStore results in an exception after some (but not all) data has been written to the hdf5 file.\n\n```python\n\nfrom pandas import DataFrame, Series, HDFStore\n\n# These are all empty\ns0 = Series()\ns1 = Series(name=\'myseries\')\ndf0 = DataFrame()\ndf1 = DataFrame(index=[\'a\', \'b\', \'c\'])\ndf2 = DataFrame(columns=[\'d\', \'e\', \'f\'])\nstore = HDFStore(\'myfile.h5\')\n\n# These all fail\ntry:\n    store[\'s0\'] = s0\nexcept ValueError:\n    print \'Failed to write s0\'\n\ntry:\n    store[\'s1\'] = s1\nexcept ValueError:\n    print \'Failed to write s1\'\n\ntry:\n    store[\'df0\'] = df0\nexcept ValueError:\n    print \'Failed to write df0\'\n\ntry:\n    store[\'df1\'] = df1\nexcept ValueError:\n    print \'Failed to write df1\'\n\ntry:\n    store[\'df2\'] = df2\nexcept ValueError:\n    print \'Failed to write df2\'\n```\n\nHere is the traceback:\n```python\nValueError                                Traceback (most recent call last)\n/usr/lib/python2.7/dist-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)\n    176             else:\n    177                 filename = fname\n--> 178             __builtin__.execfile(filename, *where)\n\n/home/erikw/pandas_hdf5_fail.py in <module>()\n     30 \n     31 try:\n---> 32     store[\'df2\'] = df2\n     33 except ValueError:\n     34     print \'Failed to write df2\'\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in __setitem__(self, key, value)\n    184 \n    185     def __setitem__(self, key, value):\n--> 186         self.put(key, value)\n    187 \n    188     def __contains__(self, key):\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in put(self, key, value, table, append, compression)\n    341         self._write_to_group(key, value, table=table, append=append,\n--> 342                              comp=compression)\n    343 \n    344     def _get_handler(self, op, kind):\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in _write_to_group(self, key, value, table, append, comp)\n    408             wrapper = lambda value: handler(group, value)\n    409 \n--> 410         wrapper(value)\n    411         group._v_attrs.pandas_type = kind\n    412 \n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in <lambda>(value)\n    406 \n    407             handler = self._get_handler(op=\'write\', kind=kind)\n--> 408             wrapper = lambda value: handler(group, value)\n    409 \n    410         wrapper(value)\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in _write_frame(self, group, df)\n    489 \n    490     def _write_frame(self, group, df):\n--> 491         self._write_block_manager(group, df._data)\n    492 \n    493     def _read_frame(self, group, where=None):\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in _write_block_manager(self, group, data)\n    500         group._v_attrs.ndim = data.ndim\n    501         for i, ax in enumerate(data.axes):\n--> 502             self._write_index(group, \'axis%d\' % i, ax)\n    503 \n    504         # Supporting mixed-type DataFrame objects...nontrivial\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in _write_index(self, group, key, index)\n    571         else:\n    572             if len(index) == 0:\n--> 573                 raise ValueError(\'Can not write empty structure, \'\n    574                                  \'axis length was 0\')\n    575 \n\nValueError: Can not write empty structure, axis length was 0\n```\n\nAnd here I show an issue that arises from only some of the data being written:\n```python\n\nIn [6]: store.keys()\nOut[6]: [\'s1\', \'s0\', \'df1\', \'df0\', \'df2\']\n\nIn [7]: store[\'df0\']\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-7-b5d10da56de7> in <module>()\n----> 1 store[\'df0\']\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in __getitem__(self, key)\n    181 \n    182     def __getitem__(self, key):\n--> 183         return self.get(key)\n    184 \n    185     def __setitem__(self, key, value):\n\n/usr/lib/pymodules/python2.7/pandas/io/pytables.pyc in get(self, key)\n    283             return self._read_group(group)\n    284         except (exc_type, AttributeError):\n--> 285             raise KeyError(\'No object named %s in the file\' % key)\n    286 \n    287     def select(self, key, where=None):\n\nKeyError: \'No object named df0 in the file\'\n```\n\nOh, and just for the record, all tests in ""io/tests/test_pytables.py"" succeed for me.'"
1705,5914645,kdebrab,wesm,2012-07-30 10:29:53,2012-08-12 20:18:39,2012-08-12 20:18:39,closed,,0.9,0,Bug;Indexing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1705,b'Incorrect behaviour when using periods in a hierarchical index',"b""Pandas 0.8.1:\n\n    import pandas as pd\n    index = pd.date_range('1/1/2012',periods=4,freq='12H')\n    index_as_arrays = [index.to_period(freq='D'), index.hour]\n    pd.Series([0, 1, 2, 3], index_as_arrays)\n\nincorrectly returns:\n\n    15340  0     0\n           12    1\n    15341  0     2\n           12    3\n\nThe correct result is obtained though when explicitly building the MultiIndex:\n\n    index_as_multi_index = pd.MultiIndex.from_tuples(zip(*index_as_arrays))\n    pd.Series([0, 1, 2, 3], index_as_multi_index)\n\ncorrectly returns:\n\n    01-Jan-2012  0     0\n                 12    1\n    02-Jan-2012  0     2\n                 12    3\n\n"""
1703,5914440,kdebrab,wesm,2012-07-30 10:15:27,2012-08-12 20:53:07,2012-08-12 20:53:07,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1703,b'Incorrect result when using to_datetime() on PeriodIndex',"b""Pandas 0.8.1:\n\n    import pandas as pd\n    index = pd.period_range('1/1/2012',periods=4,freq='D')\n    index.to_datetime()\n\nreturns incorrectly:\n\n    <class 'pandas.tseries.index.DatetimeIndex'>\n    [1970-01-01 00:00:00.000015340, ..., 1970-01-01 00:00:00.000015343]\n    Length: 4, Freq: None, Timezone: None\n\nThe correct result is obtained though with to_timestamp() instead of to_datetime():\n\n    index.to_timestamp()\n\nreturns correctly:\n\n    <class 'pandas.tseries.index.DatetimeIndex'>\n    [2012-01-01 00:00:00, ..., 2012-01-04 00:00:00]\n    Length: 4, Freq: D, Timezone: None\n"""
1702,5909978,mnbbrown,wesm,2012-07-30 03:01:24,2012-08-10 16:03:00,2012-08-10 16:03:00,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1702,b'concat DataFrame along timeseries indexes',"b""Please see: http://stackoverflow.com/questions/11714768/concat-pandas-dataframe-along-timeseries-indexes \n\nNumPy v1.6.3\nPandas v0.8.1\n\nI have two largish (snippets provided) pandas DateFrames with unequal dates as indexes that I wish to concat into one:\n\n```\n           NAB.AX                                  CBA.AX\n            Close    Volume                         Close    Volume\nDate                                    Date\n2009-06-05  36.51   4962900             2009-06-08  21.95         0\n2009-06-04  36.79   5528800             2009-06-05  21.95   8917000\n2009-06-03  36.80   5116500             2009-06-04  22.21  18723600\n2009-06-02  36.33   5303700             2009-06-03  23.11  11643800\n2009-06-01  36.16   5625500             2009-06-02  22.80  14249900\n2009-05-29  35.14  13038600   --AND--   2009-06-01  22.52  11687200\n2009-05-28  33.95   7917600             2009-05-29  22.02  22350700\n2009-05-27  35.13   4701100             2009-05-28  21.63   9679800\n2009-05-26  35.45   4572700             2009-05-27  21.74   9338200\n2009-05-25  34.80   3652500             2009-05-26  21.64   8502900\n```\n\nProblem is, if I run this:\n```\nkeys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \n```\nthe following DateFrame is produced:\n```\n                                 CBA.AX          NAB.AX        \n                              Close  Volume   Close  Volume\nDate                                                      \n2200-08-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2203-05-13 04:24:21.460041     NaN     NaN     NaN     NaN\n2206-02-06 04:24:21.460041     NaN     NaN     NaN     NaN\n2208-11-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2211-07-30 04:24:21.460041     NaN     NaN     NaN     NaN\n2219-10-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2222-07-12 04:24:21.460041     NaN     NaN     NaN     NaN\n2225-04-07 04:24:21.460041     NaN     NaN     NaN     NaN\n2228-01-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2230-09-28 04:24:21.460041     NaN     NaN     NaN     NaN\n2238-12-15 04:24:21.460041     NaN     NaN     NaN     NaN\n```\n\nDoes anybody have any idea why this might be the case?\n\nOn another point: is there any python libraries around that pull data from yahoo and normalise it?\n\n```\nFor reference:\n\ndata = {\n'CBA.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2313 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2313  non-null values\n        Volume    2313  non-null values\n    dtypes: float64(1), int64(1),\n\n 'NAB.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2329 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2329  non-null values\n        Volume    2329  non-null values\n    dtypes: float64(1), int64(1)\n}\n```"""
1701,5909069,wesm,wesm,2012-07-30 00:38:49,2012-07-30 00:48:21,2012-07-30 00:48:21,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1701,"b""GroupBy.apply doesn't pass group key name onto index in all cases""",
1700,5905328,changhiskhan,wesm,2012-07-29 14:07:20,2012-08-10 17:26:23,2012-08-10 17:26:23,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1700,b'DataFrame repr fails for nonunique columns',"b'```\nIn [14]: df.columns\nOut[14]: \nIndex([Ticker, Company Name, Position, $ Value, Position, $ Value,\n       % of Equity, Current Price $, Closing Price $, Current Price,\n       Closing Price, Px Change, Daily P&L $, Daily Perf., Total P/L 2008], dtype=object)\n\nIn [15]: repr(df)\n\\---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-15-d04570933a95> in <module>()\n----> 1 repr(df)\n\n/Users/changshe/code/pandas/pandas/core/frame.pyc in __repr__(self)\n    579         buf = StringIO()\n    580         if self._need_info_repr_():\n--> 581             self.info(buf=buf, verbose=self._verbose_info)\n    582         else:\n    583             self.to_string(buf=buf)\n\n/Users/changshe/code/pandas/pandas/core/frame.pyc in info(self, verbose, buf)\n   1310             lines.append(self.columns.summary(name=\'Columns\'))\n   1311 \n-> 1312         counts = self.get_dtype_counts()\n   1313         dtypes = [\'%s(%d)\' % k for k in sorted(counts.iteritems())]\n   1314         lines.append(\'dtypes: %s\' % \', \'.join(dtypes))\n\n/Users/changshe/code/pandas/pandas/core/frame.pyc in get_dtype_counts(self)\n   1342         for _, series in self.iterkv():\n   1343             # endianness can cause dtypes to look different\n-> 1344             dtype_str = str(series.dtype)\n   1345             if dtype_str in counts:\n   1346                 counts[dtype_str] += 1\n\n/Users/changshe/code/pandas/pandas/core/frame.pyc in __getattr__(self, name)\n   1697             return self[name]\n   1698         raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\n-> 1699                              (type(self).__name__, name))\n   1700 \n   1701     def __setattr__(self, name, value):\n\nAttributeError: \'DataFrame\' object has no attribute \'dtype\'\n```'"
1698,5895782,kdebrab,wesm,2012-07-28 19:57:26,2012-08-12 03:49:05,2012-08-12 03:48:53,closed,,0.9,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1698,"b""Interpolating with method='time' incorrect for resolution higher than daily""","b""The following code (pandas 0.8.0b2):\n\n    import pandas as pd\n    index = pd.date_range('1/1/2012', periods=4, freq='12D')\n    ts = pd.Series([0, 12, 24, 36], index)\n    new_index = index.append(index + pd.DateOffset(days=1)).order()\n    ts.reindex(new_index).interpolate(method='time')\n\ncorrectly returns:\n2012-01-01     0\n2012-01-02     1\n2012-01-13    12\n2012-01-14    13\n2012-01-25    24\n2012-01-26    25\n2012-02-06    36\n2012-02-07    36\n\nBut when exchanging days for hours:\n\n    index = pd.date_range('1/1/2012', periods=4, freq='12H')\n    ts = pd.Series([0, 12, 24, 36], index)\n    new_index = index.append(index + pd.DateOffset(hours=1)).order()\n    ts.reindex(new_index).interpolate(method='time')\n\nthe result is not correct:\n2011-01-01 00:00:00     0\n2011-01-01 01:00:00    12\n2011-01-01 12:00:00    12\n2011-01-01 13:00:00    12\n2011-01-02 00:00:00    24\n2011-01-02 01:00:00    36\n2011-01-02 12:00:00    36\n2011-01-02 13:00:00    36\n"""
1697,5885707,ichipper,wesm,2012-07-27 21:03:25,2012-08-12 00:53:10,2012-08-12 00:53:09,closed,,0.9,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1697,b'Bug/unexpected behaviour when using groupby and aggregation functions with DataFrame',"b""Here is the bug to reproduce the bug/unexpected behavior:\n\n\n```python\nfrom pandas import DataFrame\nfrom pandas import MultiIndex\n\nmidx = MultiIndex.from_tuples([('f1', 's1'),('f1','s2'),('f2', 's1'),('f2', 's2'),('f3', 's1'),('f3','s2')])\ndf = DataFrame([[1,2,3,4,5,6],[7,8,9,10,11,12]], columns= midx)\ndf1 = df.select(lambda u: u[0] in ['f2', 'f3'], axis=1)\ndf1_group = df1.groupby(axis=1, level=0)\nprint df1_group.groups\nprint df1_group.sum()\n```\n\n============================\nWhen running the code, we can see that df1 is:\n\n```\n   f1          f2         f3    \n   s1  s2  s1  s2  s1  s2\n0   1   2   3    4    5     6\n1   7   8   9   10  11   12\n```\n\nAnd df1 is selected from subblocks of df:\n\n```\n   f2        f3    \n   s1  s2  s1  s2\n0   3   4   5   6\n1   9  10  11  12\n```\n\nAfter grouping df1 by the first level of multiindex of the columns,\nwe can see df1_group.groups is:\n\n`{'f2': [('f2', 's1'), ('f2', 's2')], 'f3': [('f3', 's1'), ('f3', 's2')]}`\n\nHowever, when apply a sum function to aggregate the columns inside each group, as in the example code,\ndf1_group.sum() results in:\n\n```\n   f1       f2  f3\n0 NaN   7  11\n1 NaN  19  23\n```\n\nIt seems it tries to do the aggregation using the columns of df instead of df1 so the columns of the resulting dataframe \ninclude the label 'f1', which doesn't exist in df1.\n\n\n\n\n"""
1695,5879885,wesm,wesm,2012-07-27 16:16:11,2012-08-11 22:24:04,2012-08-11 22:24:04,closed,wesm,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1695,"b""New rolling_max/min impl's do not handle min_periods option""",
1694,5878765,manuteleco,wesm,2012-07-27 15:27:56,2012-08-10 19:05:55,2012-08-10 19:05:54,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1694,b'Unexpected behavior on Series.irow (Series.iget) with non-unique index',"b'Hi,\n\n`Series.irow(i)` (and also `Series.iget(i)`), with `i` being an integer, returns more than one value on Series with non-unique indexes if the index at location `i` has duplicates. This is not the behavior I expected and it is also different from the behavior shown by `DataFrame.irow(i)`, since it only returns the row located at position `i` regardless of any existing index duplicates. Also the documentation for `Series.irow`/`Series.iget` specifies that an ""int"" parameter should return a ""scalar"" value, so I guess this might be a bug.\n\nHere is some sample code I\'ve run using pandas \'0.8.2.dev-f5a74d4\':\n\n```\nIn [52]: s = Series([0, 1, 2], index=[0, 1, 0])\n\nIn [53]: s.irow(0)  # here I was expecting 0\nOut[53]: \n0    0\n0    2\n\nIn [54]: s.irow(1)\nOut[54]: 1\n\nIn [55]: s.irow(2)  # here I was expecting 2\nOut[55]: \n0    0\n0    2\n\n```\n\nand its DataFrame counterpart:\n```\nIn [56]: df = DataFrame([[0], [1], [2]], index=[0, 1, 0])\n\nIn [57]: df.irow(0)\nOut[57]: \n0    0\nName: 0\n\nIn [58]: df.irow(1)\nOut[58]: \n0    1\nName: 1\n\nIn [59]: df.irow(2)\nOut[59]: \n0    2\nName: 0\n```\n\nThanks and regards.'"
1693,5877335,khughitt,wesm,2012-07-27 14:25:01,2012-08-10 18:35:52,2012-08-10 18:35:52,closed,,0.9,0,Bug;Data IO;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1693,b'read_csv: Tz-aware datetime.datetime cannot be converted to datetime64',"b'This may be related to some of the other TimeZone issues.\n\nThis worked in 0.8, but now raises an error in 0.8.1:\n\n    import pandas\n    import StringIO\n\n    data = StringIO.StringIO(""Date,x\\n2012-06-13T01:39:00Z,0.5"")\n    pandas.read_csv(data,index_col=0, parse_dates=True)\n\nError:\n\n    ValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True\n'"
1692,5877149,mcobzarenco,wesm,2012-07-27 14:15:52,2012-08-12 21:24:43,2012-08-12 21:23:56,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1692,b'Timezone Conversion Bug',"b""It seems that it is not possible to use static timezones with pandas:\nhttp://nullege.com/codes/search/pytz.tzinfo.StaticTzInfo\n\n```python\npandas.DatetimeIndex([dt(2012, 1, 1)], tz=pytz.timezone('EST'))\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-483-37a722361cf0> in <module>()\n----> 1 pandas.DatetimeIndex([dt(2012, 1, 1)], tz=pytz.timezone('EST'))\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, **kwds)\n    224             ints = subarr.view('i8')\n    225 \n--> 226             subarr = lib.tz_localize_to_utc(ints, tz)\n    227             subarr = subarr.view(_NS_DTYPE)\n    228 \n\n/usr/lib64/python2.7/site-packages/pandas/lib.so in pandas.lib.tz_localize_to_utc (pandas/src/tseries.c:38322)()\n\n/usr/lib64/python2.7/site-packages/pandas/lib.so in pandas.lib._get_transitions (pandas/src/tseries.c:37001)()\n\nAttributeError: 'EST' object has no attribute '_utc_transition_times'\n```"""
1688,5868979,taavi,wesm,2012-07-27 02:59:56,2013-10-04 18:30:44,2012-08-11 22:48:10,closed,,0.9,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1688,"b'AssertionError with df.resample(how=""median"")'","b'I\'ve reproduced *something* using `how=""median""`, perhaps #1648. It seems to hit when there are discontinuities in the resampling (i.e. minutes with no records when downsampling).\n\nBoth pandas 0.8.1 and 0.8.2.dev-f5a74d4 don\'t like it:\n```python\nimport pandas as pd\nfrom datetime import datetime\ndf = pd.DataFrame([1, 2], index=[datetime(2012,1,1,0,0,0), datetime(2012,1,1,0,5,0)])\ndf.resample(""T"", how=""median"")\n# Throws AssertionError\n```'"
1686,5866065,taavi,wesm,2012-07-26 22:38:57,2012-08-12 19:55:41,2012-08-12 19:55:27,closed,,0.9,5,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1686,b'Performance of DataFrame.resample with a lot of non-unique datetime index values',"b'I\'ve noticed that the first time I resample a DataFrame with a datetime index, it\'s REALLY slow compared to doing exactly the same thing in pure Python.\n\nMy use case is analyzing logs from a web application server. The index datetimes indicate when an event happened, and there are many per second (so non-unique), and millions of rows per day (I\'m totally fine with having separate days in separate dataframes). Once I\'ve done a resample to a particular frequency (`.resample(""T"")`), I can do another resample with a different `how=` and it\'s nice and snappy, so I suspect there\'s an inefficiency somewhere in the code that figures out the resampled row groupings, and that those groupings are cached.\n\nFrom iPython (for %time):\n```python\nfrom pandas import DataFrame\nfrom datetime import datetime\nfrom itertools import groupby\n\ndatelist = sum([[datetime(2012,7,26,0,x)]*1000 for x in range(10)], [])\na = DataFrame([1]*len(datelist), index=datelist)\n\n# This is slower than it should be\n%time a.resample(""T"", how=len)\n# CPU times: user 1.49 s, sys: 0.01 s, total: 1.50 s\n# Wall time: 1.50 s\n# Out[6]: \n#                         0\n# 2012-07-26 00:00:00  1000\n# 2012-07-26 00:01:00  1000\n# 2012-07-26 00:02:00  1000\n# 2012-07-26 00:03:00  1000\n# 2012-07-26 00:04:00  1000\n# 2012-07-26 00:05:00  1000\n# 2012-07-26 00:06:00  1000\n# 2012-07-26 00:07:00  1000\n# 2012-07-26 00:08:00  1000\n# 2012-07-26 00:09:00  1000\n\n# This is unexpectedly fast (guess: caching)\n%time a.resample(""T"", how=len)\n# CPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s\n# Wall time: 0.00 s\n# Out[7]: \n#                         0\n# 2012-07-26 00:00:00  1000\n# 2012-07-26 00:01:00  1000\n# 2012-07-26 00:02:00  1000\n# 2012-07-26 00:03:00  1000\n# 2012-07-26 00:04:00  1000\n# 2012-07-26 00:05:00  1000\n# 2012-07-26 00:06:00  1000\n# 2012-07-26 00:07:00  1000\n# 2012-07-26 00:08:00  1000\n# 2012-07-26 00:09:00  1000\n\ndef by_minute(dt):\n    return (dt.year, dt.month, dt.day, dt.hour, dt.minute)\ndef how((key, values)):\n    return datetime(*key), len(list(values))\n\n# This is how fast I think .resample() SHOULD be. :)\n%time map(how, groupby(datelist, key=by_minute))\n# CPU times: user 0.01 s, sys: 0.00 s, total: 0.01 s\n# Wall time: 0.01 s\n# Out[9]: \n# [(datetime.datetime(2012, 7, 26, 0, 0), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 1), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 2), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 3), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 4), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 5), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 6), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 7), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 8), 1000),\n#  (datetime.datetime(2012, 7, 26, 0, 9), 1000)]\n```\n\nThanks!'"
1685,5863295,JoeGermuska,lodagro,2012-07-26 20:27:10,2012-08-06 19:57:43,2012-08-06 19:57:43,closed,,0.9,2,Bug;Community;Unicode;Visualization,https://api.github.com/repos/pydata/pandas/issues/1685,b'plotting._stringify not unicode safe',"b""/Users/germuska/.virtualenvs/data/lib/python2.6/site-packages/pandas/tools/plotting.pyc in _stringify(x)\n   1073         return '|'.join(str(y) for y in x)\n   1074     else:\n-> 1075         return str(x)\n   1076 \n   1077 \n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u2019' in position 1: ordinal not in range(128)\n\nIf labels can't be unicode, perhaps can a clearer error be thrown?"""
1683,5856174,wesm,wesm,2012-07-26 15:17:33,2012-08-12 01:05:09,2012-08-12 01:05:09,closed,wesm,0.9,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1683,b'Tz-aware DatetimeIndex shift issue',"b""```\ndr = pandas.date_range('2011/1/1', '2012/1/1', freq='W-FRI')\ndr_tz = dr.tz_localize('US/Eastern')\ndr_tz.shift(1, '10T')\n```"""
1682,5856129,wesm,wesm,2012-07-26 15:15:52,2012-08-10 19:14:17,2012-08-10 19:14:17,closed,wesm,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1682,b'DatetimeIndex with time zone loses tzinfo in DataFrame constructor',"b""```\ndr = pandas.date_range('2011/1/1', '2012/1/1', freq='W-FRI')\ndr_tz = dr.tz_localize('US/Eastern')\ne = pandas.DataFrame({'A': 'foo', 'B': dr_tz}, index=dr)\n```"""
1681,5856085,wesm,wesm,2012-07-26 15:14:08,2012-08-12 17:50:07,2012-08-12 17:50:07,closed,wesm,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1681,b'Concat datetime64 bug',"b""```\ndr = pandas.date_range('2011/1/1', '2012/1/1', freq='W-FRI')\na = pandas.DataFrame()\nc = pandas.DataFrame({'A': 'foo', 'B': dr}, index=dr)\na.append(c)\n```"""
1680,5855974,wesm,wesm,2012-07-26 15:10:18,2012-08-12 03:57:56,2012-08-12 03:57:56,closed,wesm,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1680,b'Series with datetime64 dtype causes failure in dict passed to DataFrame',"b""works:\n\n```\nIn [9]: d = pandas.DataFrame({'A': 'foo', 'B': ts.values}, index=dr)\n\nIn [10]: d.head()\nOut[10]: \n              A                   B\n2011-01-07  foo 2011-01-07 00:00:00\n2011-01-14  foo 2011-01-14 00:00:00\n2011-01-21  foo 2011-01-21 00:00:00\n2011-01-28  foo 2011-01-28 00:00:00\n2011-02-04  foo 2011-02-04 00:00:00\n```\n\ndoes not work:\n\n```\nd = pandas.DataFrame({'A': 'foo', 'B': ts.values}, index=dr)\n```"""
1678,5841731,wesm,wesm,2012-07-25 22:03:04,2012-08-13 01:54:03,2012-08-13 01:54:02,closed,,0.9,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1678,b'.ix with MultiIndex indexing buglet',"b""from mailing list:\n\n```\nIn [42]: frame\nOut[42]: \n      Ohio       Colorado\n     Green  Red     Green\na 1      0    1         2\n  2      3    4         5\nb 1      6    7         8\n  2      9   10        11\n\nIn [43]: frame.ix[:,0]\nOut[43]: \na  1    0\n   2    3\nb  1    6\n   2    9\nName: ('Ohio', 'Green')\n\nIn [44]: frame.icol(0)\nOut[44]: \na  1    0\n   2    3\nb  1    6\n   2    9\nName: ('Ohio', 'Green')\n\nIn [45]: frame.ix[:,1]\nOut[45]: \n    Ohio       Colorado\n   Green  Red     Green\na      0    1         2\nb      6    7         8\n\nIn [46]: frame.icol(1)\nOut[46]: \na  1     1\n   2     4\nb  1     7\n   2    10\nName: ('Ohio', 'Red')\n```"""
1677,5838742,wesm,wesm,2012-07-25 19:51:50,2012-08-10 15:58:20,2012-08-10 15:58:00,closed,wesm,0.9,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1677,b'GroupBy bug with time series',"b""from mailing list\n\n```\na = pd.DataFrame({'dt':pd.datetime(2012,4,25,9,30,0,393000),'size':50,'price':87.67},index=[pd.datetime(2012,4,25,9,30,0,393000)])\ndt = a.index[0]\nminutely = pd.date_range(dt,dt,freq='5T')\ngrouped = a.groupby(minutely.asof)\ngrouped['size'].last()\n\n# error: len() of unsized object```"""
1676,5830674,kdebrab,wesm,2012-07-25 14:02:58,2012-09-18 16:38:42,2012-09-18 16:38:42,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1676,b'timezone aware DatetimeIndex from datetime.datetime (regression)',"b'The following code worked in pandas 0.8.0b1, but not longer in 0.8.1:\n\n```\nimport pandas as pnd\nfrom datetime import datetime\nfrom dateutil.tz import tzoffset\nvalues = [188.5, 328.25]\nindex = [datetime(2012, 5, 11, 11, tzinfo=tzoffset(None, 7200)), datetime(2012, 5, 11, 12, tzinfo=tzoffset(None, 7200))]\nseries = pnd.Series(data = values, index = index)\n```\n\nWhat would you propose as a workaround?\n\nPython 2.7.2, Numpy 1.6.1'"
1675,5826307,ruidc,wesm,2012-07-25 09:30:27,2012-09-18 21:10:47,2012-09-18 21:10:47,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1675,b'concatenated DataFrames with duplicated columns cannot be printed',"b""In the following, the concatenation happens, columns c and d are duplicated, but printing the dataframe raises error:\n\n    import pandas\n    df1 = pandas.DataFrame([[1,2], [3,4]], columns=['a','b'], index=[1,2])\n    df2 = pandas.DataFrame([[1,2], [3,4]], columns=['c','d'], index=[1,2])\n    df_all = pandas.concat([df1, df2], axis=1)\n    print(df_all)\n    df4 = pandas.DataFrame([[1,2], [3,4]], columns=['a','b'], index=[3,4])\n    df_all = pandas.concat([df_all, df4], axis=0)\n    print(df_all)\n    df5 = pandas.DataFrame([[1,2], [3,4]], columns=['c','d'], index=[3,4])\n    df_all = pandas.concat([df_all, df5], axis=1)\n    print(df_all)\n"""
1674,5823802,killinc,wesm,2012-07-25 06:20:39,2012-08-12 03:24:09,2012-08-12 03:24:09,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1674,b'issue with changing frequencies when timezones are involved',"b""the following code fails:\n\n    dr = date_range('2011-12-01','2012-07-20',freq = 'D', tz = 'US/Eastern')\n    dr.asfreq('T')\n\nit appears that the asfreq code checks whether the timezone of the start and end date are the same. In this particular example, the daylight savings offsets are different at the beginning and end so the test for equality fails. Is this intended behaviour or a bug?"""
1673,5822131,killinc,wesm,2012-07-25 02:54:17,2012-09-26 21:27:29,2012-09-26 21:13:17,closed,,0.9,10,Bug,https://api.github.com/repos/pydata/pandas/issues/1673,b'bug with time fields when using timezones',"b""    # the following code fails\n    dr = date_range('2012-01-01','2012-01-10',freq = 'D', tz = 'Hongkong')\n    dr.hour\n    # whereas this doesn't\n    dr = date_range('2012-01-01','2012-01-10',freq = 'D')\n    dr.hour\n\nThe reason that this is an important use case is that often you have data stored in UTC but want to sample at a particular time of day in another region.\n\nFor some dataframe y whose index is timezone aware, I would like to be able to say x = y[y.index.hour == 1]\n\nInstead I need to write:\n\n    hours = [dt.hour for dt in y.index]\n    x = y[hours == 1]\n\nthis is ok but the list comprehension step takes ages because it is created a new timestamp for each underlying numpy datetime...\n\n"""
1672,5818693,wesm,wesm,2012-07-24 22:24:09,2012-07-24 22:31:02,2012-07-24 22:31:02,closed,wesm,0.9,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1672,b'Strings like 5-2001 broken in parser',"b""It's my fault"""
1671,5805723,wesm,wesm,2012-07-24 15:36:53,2012-08-13 17:31:21,2012-08-11 21:47:43,closed,wesm,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1671,b'Improper handling of Series in Series constructor',"b""This looks like a regression that did not have a unit test (pretty sure this used to do the right thing)\n\n```\nindex1 = ['d', 'b', 'a', 'c']\nindex2 = sorted(index1)\ns1 = Series([4, 7, -5, 3], index=index1)\ns2 = Series(s1, index=index2)\n```\n\ncc @hughdbrown"""
1669,5804349,changhiskhan,changhiskhan,2012-07-24 14:44:02,2012-07-24 21:58:54,2012-07-24 21:58:54,closed,changhiskhan,0.9,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1669,b'Inferred freq bug',"b""In [13]: tsWithGaps.index[3]\nOut[13]: <Timestamp: 2001-01-06 00:00:00>\n\nIn [14]: tsWithGaps.index[3].dayofweek\nOut[14]: 5\n\nIn [15]: tsWithGaps.index.inferred_freq\nOut[15]: 'B'"""
1664,5793519,wesm,jreback,2012-07-24 01:35:24,2013-09-21 12:53:40,2013-09-21 12:53:40,closed,,0.13,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1664,b'Cut/qcut with labels=False should probably return Categorical',b'Otherwise reindexing may be necessary in some cases'
1657,5756277,BrenBarn,changhiskhan,2012-07-21 21:04:46,2016-03-24 19:32:26,2012-08-20 06:25:19,closed,changhiskhan,0.9,12,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1657,b'More consistent na_values handling in read_csv',"b'The current handling of the `na_values` argument to `read_csv` is strangely different depending on what kind of value you pass to `na_values`.  If you pass None, the default NA values are used.  If you pass a dict mapping column names to values, then those values will be used for those columns, totally overriding the default NA values, while for columns not in the dict, the default values will be used.  If you pass some other kind of iterable, it uses the *union* of the passed values *and* the default values as the NA values.\n\nThis behavior is confusing because sometimes the passed values override the defaults, but other times they just add to the defaults.  It\'s also contrary to the documentation at http://pandas.pydata.org/pandas-docs/stable/io.html#csv-text-files, which says: ""If you pass an empty list or an empty list for a particular column, no values (including empty strings) will be considered NA.""  But passing an empty list doesn\'t result in no values being considered NA.  In fact, passing an empty list does nothing, since the empty list is unioned with the default NA values, so the default NA values are just used anyway.\n\nCurrently there is no easy way to pass a list of NA values which overrides the default for all columns.  You can pass a dict, but then you have to specify the defaults per column.  If you pass a list, you\'re not overriding the defaults, you\'re adding to them.  This makes for confusing behavior when reading CSV files with string data in which strings like ""na"" and ""nan"" are valid data and should be read as their literal string values.\n\nThere should be a way to pass an all-column set of NA values that overrides the defaults.  One possibility would be to have two arguments, something like `all_na_values` and `more_na_values`, to specify overriding and additional values, respectively.  Another possibility would be to expose the default (currently the module-level `_NA_VALUES` in parsers.py), and allow users to add to it it they want to add more NA values (e.g., `read_csv(na_values=set([\'newNA\']) | pandas.default_nas)`.'"
1649,5724551,tkf,wesm,2012-07-19 20:59:23,2012-07-20 15:38:11,2012-07-20 15:38:11,closed,wesm,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1649,b'A bug in pandas.concat when the given dict contains None',"b'```python\nIn [2]:\nimport pandas\n\nIn [3]:\ndf0 = pandas.DataFrame([[10, 20, 30], [10, 20, 30], [10, 20, 30]])\ndf0\nOut [3]:\n    0   1   2\n0  10  20  30\n1  10  20  30\n2  10  20  30\n\nIn [4]:\npandas.concat(dict(a=None, b=df0, c=df0[:2], d=df0[:1], e=df0))\nOut [4]:\n      0   1   2\na 0  10  20  30\n  1  10  20  30\n  2  10  20  30\nb 0  10  20  30\n  1  10  20  30\nc 0  10  20  30\nd 0  10  20  30\n  1  10  20  30\n  2  10  20  30\n```\n\nChecked with the current master e1129b11b1fe0016748d3d279000a2a71db8dee5 and 0.7.3'"
1648,5723551,wesm,wesm,2012-07-19 20:14:12,2012-08-11 22:48:43,2012-08-11 22:48:43,closed,,0.9,2,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1648,"b""Resample with how='median' broken in some cases""",
1647,5723543,wesm,wesm,2012-07-19 20:13:44,2012-07-19 21:49:13,2012-07-19 21:49:13,closed,,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1647,"b""String timestamp aliases don't work with tz-aware data""",
1646,5723533,wesm,wesm,2012-07-19 20:13:12,2012-07-21 14:01:59,2012-07-21 14:01:59,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1646,"b""Interpolate with method='values' and DatetimeIndex fails""",
1645,5723509,wesm,wesm,2012-07-19 20:11:57,2012-07-19 21:21:46,2012-07-19 21:21:46,closed,,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1645,b'Business month start (BMS) bug',"b""```\nIn [10]: date_range('1/1/2000', periods=10, freq='BMS')\nOut[10]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-02-01 00:00:00, ..., 2000-11-01 00:00:00]\nLength: 10, Freq: BMS, Timezone: None\n```"""
1644,5723460,wesm,wesm,2012-07-19 20:09:52,2013-12-04 00:57:58,2012-07-19 21:38:47,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1644,"b""Unconverted 'name' of Series object from DataFrame of time series row""","b""```\nIn [6]: import pandas.util.testing as tm\n\nIn [7]: df = tm.makeTimeDataFrame()\n\nIn [8]: df.ix['2000-02-02'].name\nOut[8]: '2000-02-02'\n```"""
1636,5684452,andreas-h,wesm,2012-07-18 09:32:20,2012-07-19 19:38:58,2012-07-19 19:38:58,closed,,0.8.1,2,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1636,b'`kwargs` are ignored in `Series.plot`',"b'It seems that some `kwargs` (like `mew` and `ms`) given to the `Series.plot` method are being ignored.\n\nI\'m running pandas.0.8.0 and matplotlib 1.1.0 on Archlinux, Python 2.7.3 with ipython 0.12.1, and starting ipython via `ipython2 --pylab=qt`.\n\nThe following code produces the erroneous behaviour. The mpl plotting works just fine, whereas the pandas plotting ignores `ms` and `mew` arguments.\n\n    import matplotlib as mpl\n    from numpy import random\n    import pandas\n    print pandas.__version__\n    \n    test = pandas.Series(random.randn(120),\n                         index=pandas.date_range(start=""2000-01-01"",\n                                                 end=""2009-12-31"",\n                                                 freq=pandas.datetools.MonthBegin()))\n    \n    mpl.pyplot.figure()\n    # this produces the correct markers\n    mpl.pyplot.plot(test, \'.\', ms=12.)\n    \n    mpl.pyplot.figure()\n    # this ignores the ms kwarg\n    test.plot(style=\'.\', ms=12.)\n\nIt would be okay to use matplotlib\'s plotting function, but I\'m dealing with time series, and I don\'t think matplotlib\'s support for dates is too convenient.\n\nIs this a bug, or am I just doing it wrong?'"
1635,5674169,changhiskhan,jreback,2012-07-17 20:39:33,2014-07-13 09:29:02,2014-03-09 15:20:46,closed,,0.14.0,5,Bug;Period;Resample,https://api.github.com/repos/pydata/pandas/issues/1635,b'Resample with PeriodIndex should allow third option',"b""Suppose I have:\n\nprng = period_range('1/1/1999', freq='M', periods=3)\ns = Series(np.random.randn(len(prng)), prng)\n\ns.resample('D') should by default go from 1/1/1999 to 3/31/1999\nas opposed to 1/31/1999 - 3/31/1999 ('e') or 1/1/1999-3/1/1999 ('s')\n\n"""
1631,5655860,wesm,wesm,2012-07-17 04:28:40,2012-07-18 14:35:00,2012-07-18 14:35:00,closed,,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1631,b'Resample loses index name',
1630,5639012,CRP,wesm,2012-07-16 13:31:42,2012-09-20 03:00:44,2012-09-20 03:00:44,closed,changhiskhan,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1630,b'Panel slice assigning does not reindex correctly?',"b'Suppose I have\n\na=Panel(randn(3,10,2))\nb=Series(randn(10))\nb.sort()\n\nnow the index of b has a different order, yet same values as the major_index of a\n\nif I do\n\na.ix[0,:,0]=b\n\nthe slice of a gets assigned the values of b without reindexing first, but somehow the index is till kept track of:\n\n```\nb\nOut[220]: \n4   -2.465417\n8   -2.202345\n3   -1.173703\n5   -0.802436\n7   -0.307515\n0   -0.045943\n1    0.155771\n6    0.367755\n9    0.458332\n2    1.629641\n\na.ix[0,:,0]=b\n\na.ix[0,:,0]\nOut[223]: \n0   -2.465417\n1   -2.202345\n2   -1.173703\n3   -0.802436\n4   -0.307515\n5   -0.045943\n6    0.155771\n7    0.367755\n8    0.458332\n9    1.629641\n\na.ix[0,:,0]==b\nOut[224]: \n0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n6    True\n7    True\n8    True\n9    True\n```'"
1629,5638760,ruidc,ruidc,2012-07-16 13:19:03,2013-10-03 03:30:36,2013-09-05 06:55:20,closed,,0.13,8,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1629,b'reading xslx returns incorrect data due to bug in openpyxl load_workbook being called with use_iterators=True',"b'The situation is described in https://bitbucket.org/ericgazoni/openpyxl/issue/124/rawcellis_date-returns-false-positive\nand results in datetime being returned instead of float causing incorrect data.\n\nCould this method argument be exposed to the ExcelFile __init__ with a default of True? \n\nusing False will work around the issue, presumably at the cost of performance.\nIteration code would then be needed in pandas ExcelFile._parse_xlsx\n\n... As this bug has existed for months in openpyxl without comment and code is still described as ""very raw"" in openpyxl\n\nhttps://bitbucket.org/ericgazoni/openpyxl/src/0082a961cf8b/openpyxl/reader/iter_worksheet.py#cl-27\n\nAlternatively, if xlrd 0.8.0 is released with xlsx support, pandas could use that instead.'"
1628,5633524,idoko,wesm,2012-07-16 07:12:22,2013-03-07 18:18:14,2012-07-21 14:01:59,closed,,0.8.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1628,b'merge with empty DataFrame IndexError',"b""```\nIn [1]: from pandas import *\n\nIn [2]: a = DataFrame(columns=['column1'], data=[1])\n\nIn [3]: b = DataFrame(columns=['column1'])\n\nIn [4]: a.merge(b, on=['column1'], how='left')\n\n\nIndexError                                Traceback (most recent call last)\nIndexError: index out of range for array\n```\n"""
1625,5629991,wesm,wesm,2012-07-15 22:04:46,2012-07-19 19:07:56,2012-07-19 19:07:56,closed,wesm,0.8.1,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1625,"b""scatter_matrix doesn't label the axes right on a DataFrame of time series""",b'http://imgur.com/BGUgT'
1624,5629587,wesm,wesm,2012-07-15 20:58:38,2012-07-15 23:08:12,2012-07-15 23:08:12,closed,wesm,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1624,b'DatetimeIndex unnecessarily copies int64 array',
1622,5623573,wesm,wesm,2012-07-14 22:57:43,2012-07-15 23:24:51,2012-07-15 23:24:51,closed,,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1622,b'Bug resampling quarterly period data to annual',"b""```\nIn [10]: rng = period_range('2000Q1', periods=10, freq='Q-DEC')\n\nIn [11]: rng\nOut[11]: \n<class 'pandas.tseries.period.PeriodIndex'>\nfreq: Q-DEC\n[2000Q1, ..., 2002Q2]\nlength: 10\n\nIn [12]: ts = Series(np.arange(10), index=rng)\n\nIn [13]: ts\nOut[13]: \n2000Q1    0\n2000Q2    1\n2000Q3    2\n2000Q4    3\n2001Q1    4\n2001Q2    5\n2001Q3    6\n2001Q4    7\n2002Q1    8\n2002Q2    9\nFreq: Q-DEC\n\nIn [14]: ts.resample('A')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/wesm/Dropbox/talks/scipy2012/tutorial/<ipython-input-14-d20eb06885e9> in <module>()\n----> 1 ts.resample('A')\n\n/home/wesm/code/pandas/pandas/core/generic.pyc in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base)\n    187                               fill_method=fill_method, convention=convention,\n    188                               limit=limit, base=base)\n--> 189         return sampler.resample(self)\n    190 \n    191     def first(self, offset):\n\n/home/wesm/code/pandas/pandas/tseries/resample.pyc in resample(self, obj)\n     72 \n     73             if self.kind is None or self.kind == 'period':\n---> 74                 return self._resample_periods(obj)\n     75             else:\n     76                 obj = obj.to_timestamp(how=self.convention)\n\n/home/wesm/code/pandas/pandas/tseries/resample.pyc in _resample_periods(self, obj)\n    233         else:\n    234             raise ValueError('Frequency %s cannot be resampled to %s'\n--> 235                              % (axlabels.freq, self.freq))\n    236 \n    237 \n\nValueError: Frequency Q-DEC cannot be resampled to <1 YearEnd: kwds={'month': 12}, month=12>\n```"""
1620,5619342,manuteleco,wesm,2012-07-14 10:04:40,2012-08-22 17:15:14,2012-07-21 16:07:53,closed,changhiskhan,0.8.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1620,b'__repr__ wrong column alignment with non-ascii characters',"b'Hi,\n\nit seems that when DataFrame, Series and maybe other objects contain non-ascii characters inside non-unicode strings the `__repr__` method is not able to give the correct column alignment to its values. However, we see that this issue does not affect unicode strings. I\'m using pandas \'0.8.1.dev-70c3deb\' in a Linux box.\n\nSample code:\n```python\n# -*- coding: utf-8 -*-\n\nfrom pandas import Series, DataFrame\n\ndf1 = DataFrame([[""aaaa"", 1], [""bbbb"", 2]])\ndf2 = DataFrame([[""aa"", 1], [""bbbb"", 2]])\ndf3 = DataFrame([[u""aa"", 1], [""bbbb"", 2]])\n\n# Comparison between ""similar dataframes""\nprint df1\nprint\nprint df2\nprint\nprint df3\nprint\n\n# Other cases:\ns1 = Series(["""", ""bbbb"", """"])\nprint\nprint s1\n```\n\nThis results in:\n```\n      0  1\n0  aaaa  1\n1  bbbb  2\n\n        0  1\n0  aa  1\n1    bbbb  2\n\n      0  1\n0  aa  1\n1  bbbb  2\n\n\n0      \n1    bbbb\n2    \n```\n\nThanks and regards.'"
1615,5582437,jseabold,wesm,2012-07-12 21:21:14,2012-08-10 16:11:23,2012-08-10 16:11:15,closed,,0.9,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1615,b'Trouble with factors when there are nans',"b""```python\nfrom pandas.rpy.common import load_data\nprestige = load_data('Prestige', 'car')\n```\n"""
1609,5555024,changhiskhan,changhiskhan,2012-07-11 18:28:03,2012-07-12 02:37:20,2012-07-12 02:37:20,closed,,0.8.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1609,b'Nonunique integer index bug',"b'related to #1586\n\n\nIn [8]: s = Series(range(5), index=[2, 0, 0, 1, 1])\n\nIn [9]: s\nOut[9]: \n2    0\n0    1\n0    2\n1    3\n1    4\n\nIn [10]: s[2]\n...\nKeyError: 0\n'"
1606,5550414,wesm,wesm,2012-07-11 15:09:47,2012-07-11 20:40:25,2012-07-11 20:40:25,closed,wesm,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1606,b'Sparsify console output issue',"b""Should not have that AAA hole\n\n```\nIn [31]: result\nOut[31]: \n                            sp1  sp2  sp3\nmethod replicate site year               \nEDGE   1         AAA  2006    2    4    6\n                 BBB  2006    2    4    6\n       2         AAA  2006    2    4    6\nRIFFLE 1              2006    2    4    6\n                 BBB  2006    2    4    6\n       2         AAA  2006    2    4    6\n\nIn [32]: result.index\nOut[32]: \nMultiIndex\n[('EDGE', 1, 'AAA', 2006) ('EDGE', 1, 'BBB', 2006)\n ('EDGE', 2, 'AAA', 2006) ('RIFFLE', 1, 'AAA', 2006)\n ('RIFFLE', 1, 'BBB', 2006) ('RIFFLE', 2, 'AAA', 2006)]\n```"""
1603,5532674,wesm,wesm,2012-07-10 20:12:13,2012-07-11 17:57:19,2012-07-11 17:57:19,closed,wesm,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1603,b'Panel .ix indexing bug',"b'Major axis is DatetimeIndex\n\n```\npdata.ix[:, -1, :]\n```'"
1602,5532380,wesm,changhiskhan,2012-07-10 19:58:53,2012-07-11 16:46:22,2012-07-11 16:46:22,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1602,b'Series repr incorrect if name==0',"b'```\nIn [13]: Series(range(5), name=0)\nOut[13]: \n0    0\n1    1\n2    2\n3    3\n4    4\n\nIn [14]: Series(range(5), name=1)\nOut[14]: \n0    0\n1    1\n2    2\n3    3\n4    4\nName: 1\n```'"
1601,5531549,wesm,wesm,2012-07-10 19:22:58,2012-07-11 18:57:09,2012-07-11 18:57:09,closed,wesm,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1601,"b""Year partial string indexing doesn't work right with freq='M' PeriodIndex""",
1600,5530307,changhiskhan,changhiskhan,2012-07-10 18:30:48,2012-07-11 22:26:36,2012-07-11 22:26:36,closed,changhiskhan,0.8.1,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1600,b'period alias lookup error',"b""In [18]: period_range('1/1/2012', periods=500, freq='U')\nOut[18]: \n<class 'pandas.tseries.period.PeriodIndex'>\nfreq: D\n[01-Jan-2012, ..., 14-May-2013]\nlength: 500\n"""
1599,5530114,changhiskhan,wesm,2012-07-10 18:21:26,2015-02-18 15:41:44,2012-07-13 03:11:47,closed,changhiskhan,0.8.1,1,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1599,b'Plotting of millisecond frequency broken',"b'from stackoverflow:\n\nI am trying to create a plot waith an x range of e.g. 500 milliseconds.\n\nrng = date_range(s,periods=500,freq=""U"")\ndf = DataFrame(randn(500),index=rng,columns=[""A""])\n\nto plot column A:\n\ndf[""A""].plot()\nThe whole plot will be squeezed into a single spike because the x range is defined from Jan-2011 until Jul-2014.\n\nIs there a way to change this?\n'"
1595,5526647,jianpan,wesm,2012-07-10 15:47:56,2012-07-11 15:45:59,2012-07-11 15:45:59,closed,,0.8.1,1,Bug;Stats,https://api.github.com/repos/pydata/pandas/issues/1595,b'Pandas DataFrame.corr() raises exception if two columns have no common values',b'For example:\n     A    B\n0   1    NaN\n1   1    NaN\n2   1    NaN\n3   NaN 1\n4   NaN 1\n5   NaN 1\n\nIn DataFrame.corr() it only checks for valid.all(). It needs to check for valid.any() also.\n\n\n\n'
1594,5521377,gorkypl,wesm,2012-07-10 12:05:52,2013-12-04 00:43:09,2012-07-19 19:10:01,closed,,0.8.1,12,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1594,"b""period_range fails on python 2.7 (works on python 3.2) - 'str' object has no attribute 'read'""","b'I am doing an exemple from http://pandas.pydata.org/pandas-docs/stable/timeseries.html\n\nrng = pandas.period_range(\'1/1/2011\', periods=500, freq=\'M\')\n\n---------------------------------------------------------------------------\nDateParseError                            Traceback (most recent call last)\n/home/users/pawel/AGH/mgr/dane/gazowe/<ipython-input-4-a54f4f8b58b5> in <module>()\n----> 1 rng = pandas.period_range(\'1/1/2011\', periods=500, freq=\'M\')\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/period.pyc in period_range(start, end, periods, freq)\n   1064     """"""\n   1065     return PeriodIndex(start=start, end=end, periods=periods,\n-> 1066                        freq=freq)\n   1067 \n   1068 def _period_rule_to_timestamp_rule(freq, how=\'end\'):\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/period.pyc in __new__(cls, data, ordinal, freq, start, end, periods, copy, name, year, month, quarter, day, hour, minute, second)\n    533                 fields = [year, month, quarter, day, hour, minute, second]\n    534                 data, freq = cls._generate_range(start, end, periods,\n--> 535                                                     freq, fields)\n    536         else:\n    537             ordinal, freq = cls._from_arraylike(data, freq)\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/period.pyc in _generate_range(cls, start, end, periods, freq, fields)\n    551                 raise ValueError(\'Can either instantiate from fields \'\n    552                                  \'or endpoints, but not both\')\n--> 553             subarr, freq = _get_ordinal_range(start, end, periods, freq)\n    554         elif field_count > 0:\n    555             y, mth, q, d, h, minute, s = fields\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/period.pyc in _get_ordinal_range(start, end, periods, freq)\n    925 \n    926     if start is not None:\n--> 927         start = Period(start, freq)\n    928     if end is not None:\n    929         end = Period(end, freq)\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/period.pyc in __init__(self, value, freq, ordinal, year, month, quarter, day, hour, minute, second)\n     98                 value = str(value)\n     99 \n--> 100             dt, freq = _get_date_and_freq(value, freq)\n    101 \n    102         elif isinstance(value, datetime):\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/period.pyc in _get_date_and_freq(value, freq)\n    382 def _get_date_and_freq(value, freq):\n    383     value = value.upper()\n--> 384     dt, _, reso = parse_time_string(value, freq)\n    385 \n    386     if freq is None:\n\n/usr/lib64/python2.7/site-packages/pandas/tseries/tools.pyc in parse_time_string(arg, freq)\n    217         return ret, parsed, reso  # datetime, resolution\n    218     except Exception, e:\n--> 219         raise DateParseError(e)\n    220 \n    221 def _try_parse_monthly(arg):\n\nDateParseError: \'str\' object has no attribute \'read\'\n\nWorks fine with python-3.2.\npython-2.7.3, numpy-1.6.2, scipy-0.10.1, pandas 0.8.0'"
1593,5514623,leonbaum,leonbaum,2012-07-10 02:46:45,2012-07-10 14:16:48,2012-07-10 14:16:43,closed,,0.8.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1593,b'drop() issue with MultiIndex after append()',"b""Minimal example:\n\n```\nIn [10]: df1 = pandas.DataFrame({'x': ['a', 'b'], 'y': ['A', 'A'], 'z': [1,1]}).set_index(['x', 'y'])\n\nIn [11]: df2 = pandas.DataFrame({'x': ['a'], 'y': ['B'], 'z': [1]}).set_index(['x', 'y'])\n\nIn [12]: df3 = df1.append(df2)\n\nIn [13]: df3\nOut[13]: \n     z\nx y   \na A  1\nb A  1\na B  1\n\nIn [14]: df3.drop('a')\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/home/me/<ipython-input-14-d69e3ae69a9d> in <module>()\n----> 1 df3.drop('a')\n\n/home/me/.local/lib64/python3.2/site-packages/pandas-0.8.1.dev_e1ff90c-py3.2-linux-x86_64.egg/pandas/core/generic.py in drop(self, labels, axis, level)\n    298             new_axis = axis.drop(labels, level=level)\n    299         else:\n--> 300             new_axis = axis.drop(labels)\n    301 \n    302         return self.reindex(**{axis_name : new_axis})\n\n/home/me/.local/lib64/python3.2/site-packages/pandas-0.8.1.dev_e1ff90c-py3.2-linux-x86_64.egg/pandas/core/index.py in drop(self, labels, level)\n   1671                 inds.append(loc)\n   1672             else:\n-> 1673                 inds.extend(list(range(loc.start, loc.stop)))\n   1674 \n   1675         return self.delete(inds)\n\nAttributeError: 'numpy.ndarray' object has no attribute 'start'\n```\n\nThere isn't a duplicate index, so I think it should work."""
1591,5512106,petergx,wesm,2012-07-09 23:29:55,2012-07-11 21:12:50,2012-07-11 21:12:40,closed,wesm,0.8.1,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1591,"b""Cannot resample with 'D' and tz aware timeseries""","b""```python\nfrom pandas import *\ndr = date_range(start='2012-5-1', end='2012-6-1')\nts = Series(range(len(dr)), dr)\nts_utc = ts.tz_localize('UTC')\nts_local = ts_utc.tz_convert('America/Los_Angeles')\nts_local.resample('D')\n```\n\nyields..\n\n```traceback\n/env/lib/python2.7/site-packages/pandas/tseries/resample.pyc in _get_time_bins(self, axis)\n     97         # a little hack\n     98         trimmed = False\n---> 99         if len(binner) > 2 and binner[-2] == axis[-1] and self.closed == 'right':\n    100             binner = binner[:-1]\n    101             trimmed = True\n\n/env/lib/python2.7/site-packages/pandas/lib.so in pandas.lib._Timestamp.__richcmp__ (pandas/src/tseries.c:33751)()\n\nException: Cannot compare tz-naive and tz-aware timestamps\n```"""
1590,5509831,aleyan,wesm,2012-07-09 22:09:39,2013-12-04 00:43:09,2012-07-11 17:09:52,closed,,0.8.1,2,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1590,"b""set_index doesn't like non zero-starting initial indeces""","b'It seems unless a DataFrame is completely fresh and it\'s index hasn\'t been manipulated in anyway ( such as being a result of a group by ) then set_index() doesn\'t quite work.\n\n\tIn [1]: import pandas\n\n\tIn [2]: pandas.__version__\n\tOut[2]: \'0.8.0\'\n\n\tIn [3]: df = pandas.DataFrame([ {\'val\':0,\'key\':\'a\'},{\'val\':1,\'key\':\'b\'},{\'val\':2,\'key\':\'c\'} ] )\n\n\tIn [4]: df\n\tOut[4]:\n\t  key  val\n\t0   a    0\n\t1   b    1\n\t2   c    2\n\n\tIn [5]: df.set_index(\'key\')\n\tOut[5]:\n\t     val\n\tkey\n\ta      0\n\tb      1\n\tc      2\n\n\tIn [6]: df2 = df.select(lambda indx:indx>=1)\n\n\tIn [8]: df2\n\tOut[8]:\n\t  key  val\n\t1   b    1\n\t2   c    2\n\nso far so good\n\n\tIn [9]: df2.set_index(\'key\')\n\t---------------------------------------------------------------------------\n\tKeyError                                  Traceback (most recent call last)\n\t<ipython-input-9-366937e9485c> in <module>()\n\t----> 1 df2.set_index(\'key\')\n\n\tc:\\python27\\lib\\site-packages\\pandas\\core\\frame.pyc in set_index(self, keys, dro\n\tp, inplace, verify_integrity)\n\t   2306             arrays.append(level)\n\t   2307\n\t-> 2308         index = MultiIndex.from_arrays(arrays, names=keys)\n\t   2309\n\t   2310         if verify_integrity and not index.is_unique:\n\n\tc:\\python27\\lib\\site-packages\\pandas\\core\\index.pyc in from_arrays(cls, arrays,\n\tsortorder, names)\n\t   1504         if len(arrays) == 1:\n\t   1505             name = None if names is None else names[0]\n\t-> 1506             return Index(arrays[0], name=name)\n\t   1507\n\t   1508         cats = [Categorical.from_array(arr) for arr in arrays]\n\n\tc:\\python27\\lib\\site-packages\\pandas\\core\\index.pyc in __new__(cls, data, dtype,\n\t copy, name)\n\t    102         if dtype is None:\n\t    103             if (lib.is_datetime_array(subarr)\n\t--> 104                 or lib.is_datetime64_array(subarr)\n\t    105                 or lib.is_timestamp_array(subarr)):\n\t    106                 from pandas.tseries.index import DatetimeIndex\n\n\tc:\\python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.is_datetime64_array (\n\tpandas\\src\\tseries.c:90487)()\n\n\tc:\\python27\\lib\\site-packages\\pandas\\core\\series.pyc in __getitem__(self, key)\n\t    427     def __getitem__(self, key):\n\t    428         try:\n\t--> 429             return self.index.get_value(self, key)\n\t    430         except InvalidIndexError:\n\t    431             pass\n\n\tc:\\python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_value(self, series, k\n\tey)\n\t    639         """"""\n\t    640         try:\n\t--> 641             return self._engine.get_value(series, key)\n\t    642         except KeyError, e1:\n\t    643             if len(self) > 0 and self.inferred_type == \'integer\':\n\n\tc:\\python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.IndexEngine.get_value\n\t (pandas\\src\\tseries.c:104257)()\n\n\tc:\\python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.IndexEngine.get_value\n\t (pandas\\src\\tseries.c:104085)()\n\n\tc:\\python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.IndexEngine.get_loc (\n\tpandas\\src\\tseries.c:104794)()\n\n\tc:\\python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.Int64HashTable.get_it\n\tem (pandas\\src\\tseries.c:15561)()\n\n\tc:\\python27\\lib\\site-packages\\pandas\\lib.pyd in pandas.lib.Int64HashTable.get_it\n\tem (pandas\\src\\tseries.c:15515)()\n\n\tKeyError: 0L\t'"
1589,5508912,wesm,wesm,2012-07-09 21:26:43,2012-07-11 21:50:46,2012-07-11 21:50:46,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1589,b'Series/DataFrame.rank broken on all integer data',"b'int64 should alias to generic, or perhaps generate a specialized int64 version'"
1588,5506904,wesm,wesm,2012-07-09 19:47:42,2012-07-09 20:19:52,2012-07-09 20:19:52,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1588,b'Resampling bug',"b""This blows up inside the Cython binning method:\n\n```\ndates = date_range('4/16/2012 20:00', periods=5000000, freq='s')\nts = Series(randn(len(dates)), index=dates)\nts.resample('d')\n```"""
1587,5499714,jgarcke,changhiskhan,2012-07-09 14:24:45,2012-07-13 16:49:01,2012-07-13 16:49:01,closed,changhiskhan,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1587,b'slicing with multiple timestamps',"b""I have a problem for time series with multiple time stamps.\nThis is the data file:\n06/01/04 10:11:46 1.2735 1.2737\n06/01/04 10:11:47 1.2735 1.2737\n06/01/04 10:11:48 1.2736 1.2738\n06/01/04 10:11:50 1.2736 1.2738\n06/01/04 10:11:50 1.2735 1.2737\n\nraw_data = read_csv('bug_data',header=None, parse_dates=[[0,1]], sep=' ', index_col=0, dayfirst=True)\noldest=raw_data.index[-1]\n\nthen:\nraw_data[:oldest]\nfails with TypeError: unsupported operand type(s) for +: 'slice' and 'int'\nas does\nraw_data.ix[:oldest]\n\nraw_data[oldest:]\ndoes work, whereas\nraw_data.ix[oldest:]\nfails with IndexError: invalid slice\n\nCutting out the last entry things work again, e.g.\nraw_data[:-1].ix[oldest:]\nraw_data[:-1].ix[:oldest]\nor\nraw_data[:-1][oldest:]\nraw_data[:-1][:oldest]\n\na (or the ?) get_slice-routines return a slice in these cases.\n\nget_loc gives back a slice when there are non-unique indexes, not sure if\nthat routine(s) should be fixed or slice_locs \n\nmy workaround (?) is catching this in slice_loc but I guess the get_loc's should be fixed instead\n--- /home/jgarcke/Prog/Packages/pandas-git/pandas/core/index.py    \n2012-06-26 14:10:12.107724998 +0200\n+++ index.py    2012-06-26 16:47:19.917653332 +0200\n@@ -1039,7 +1039,11 @@\n             beg_slice = 0\n         else:\n             try:\n-                beg_slice = self.get_loc(start)\n+                if isinstance(self.get_loc(start), slice):\n+                    beg_slice = self.get_loc(start).start\n+                else:\n+                    beg_slice = self.get_loc(start)\n             except KeyError:\n                 if self.is_monotonic:\n                     beg_slice = self.searchsorted(start, side='left')\n@@ -1050,7 +1054,11 @@\n             end_slice = len(self)\n         else:\n             try:\n-                end_slice = self.get_loc(end) + 1\n+                if isinstance(self.get_loc(end), slice):\n+                    end_slice = self.get_loc(end).stop + 1\n+                else:\n+                    end_slice = self.get_loc(end) + 1\n             except KeyError:\n                 if self.is_monotonic:\n                     end_slice = self.searchsorted(end, side='right') \n"""
1586,5490062,wesm,wesm,2012-07-08 23:41:04,2012-07-11 20:59:59,2012-07-11 20:59:59,closed,,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1586,"b""Unordered duplicate index doesn't yield scalar value""","b""```\nIn [13]: obj = Series(range(5), index=['c', 'a', 'a', 'b', 'b'])\n\nIn [14]: obj['c']\nOut[14]: c    0\n```"""
1585,5489719,mcobzarenco,wesm,2012-07-08 22:35:49,2012-07-11 22:25:00,2012-07-11 22:25:00,closed,wesm,0.8.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1585,"b""Cannot slice a SparseDataFrame's columns.""","b""Example:\n\n```python\nsdf = pandas.SparseDataFrame(index=[0, 1, 2], columns=['a', 'b','c'])\n\nsdf[['a', 'b']]\n\n/usr/lib64/python2.7/site-packages/pandas/sparse/frame.pyc in __getitem__(self, item)\n    321         try:\n    322             # unsure about how kludgy this is\n--> 323             s = self._series[item]\n    324             s.name = item\n    325             return s\n\nTypeError: unhashable type: 'list'\n\n```"""
1582,5487627,wesm,wesm,2012-07-08 16:44:45,2012-07-11 20:52:56,2012-07-11 20:52:56,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1582,b'to_panel issues with integers',"b""Case 1 (i'll look for the other)\n\nhttp://stackoverflow.com/questions/11329611/python-pandas-to-panel-from-dataframe-returns-weird-numbers-for-binary-variab\n\npm10 = pds.read_csv('pm10.csv', index_col = [0,1], parse_dates=True)\npanel_exog = pm10.to_panel()['pass_ind']"""
1581,5487595,wesm,wesm,2012-07-08 16:38:23,2012-07-21 14:01:59,2012-07-21 14:01:59,closed,,0.10,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1581,b'No datetime64 DataFrame column conversion of datetime.datetime with tzinfo',b'inspired by: http://stackoverflow.com/questions/11337437/convert-object-to-daterange/11384801#11384801'
1580,5482746,danse,danse,2012-07-07 20:22:59,2012-07-12 22:34:57,2012-07-12 22:34:57,closed,,0.8.1,7,Bug;Groupby;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1580,b'Weird results grouping data by day',"b""\nHi I was feeding pandas (0.8.0rc2) with dates and found some errors. The\namounts from following csv file are grouped by date, but the sums for some days\nare wrong:\n\n2011-02-02 resulting: 0   correct: 40\n2011-08-21 resulting: 3   correct: 133\n2012-10-22 resulting: 157 correct: 27\n\nThis is the script I am running:\n\n```python\nimport sys\nimport pandas\n\nf=pandas.read_csv(sys.stdin, index_col=1, parse_dates=True)\n\nf.sort()\nf=f.resample('D', how='sum')\nf['amount'] = f['amount'].fillna(0)\n\nf.to_csv(sys.stdout)\n```\n\nMaybe I'm using the time series methods in a wrong way.\n\nThe file with data is not too long, it is hosted here: https://raw.github.com/danse/sparkles/master/cleaned.csv"""
1576,5474829,changhiskhan,wesm,2012-07-06 21:53:23,2012-07-11 22:29:32,2012-07-11 22:29:32,closed,wesm,0.8.1,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1576,b'Index.tolist returns empty list for MultiIndex',"b'In [18]: idx\nOut[18]: \nMultiIndex\n[(0.12609189981717644, 1.103678121647419)\n (0.34279706720294867, -0.7021022763765951)\n (1.4611933034915323, -0.9892166873678242)\n (-1.8708717491582252, 1.0296013286162102)\n\nIn [19]: len(idx.tolist())\nOut[19]: 0\n'"
1572,5464224,njsmith,wesm,2012-07-06 12:39:44,2012-07-11 23:55:40,2012-07-11 23:55:08,closed,,0.8.1,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1572,"b'DataFrame.__init__(..., dtype=dt) makes unnecessary copies'","b""If I have a DataFrame `df` with homogenous type, then `DataFrame(df)` returns a view on the original dataframe. `DataFrame(df, dtype=current_type)` should be identical; but, instead, it makes an unnecessary copy.\n\n```\n>>> import pandas\n>>> pandas.__version__\n'0.8.0'\n>>> df = pandas.DataFrame([[1, 2]])\n>>> df\n   0  1\n0  1  2\n>>> df[0].dtype\ndtype('int64')\n>>> view = pandas.DataFrame(df)\n>>> view\n   0  1\n0  1  2\n>>> view[0][0] = 100\n>>> view\n     0  1\n0  100  2\n>>> df\n     0  1\n0  100  2\n>>> should_be_view = pandas.DataFrame(df, dtype=df[0].dtype)\n>>> should_be_view\n     0  1\n0  100  2\n>>> should_be_view[0][0] = 99\n>>> should_be_view\n    0  1\n0  99  2\n>>> df\n     0  1\n0  100  2\n```\n\nThe same thing seems to happen in the input is an ndarray -- `DataFrame(arr)` returns a view, `DataFrame(arr, dtype=arr.dtype)` returns a copy."""
1568,5430668,ruidc,wesm,2012-07-04 16:08:40,2012-07-11 23:08:45,2012-07-11 23:08:29,closed,,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1568,b'sum with level=0 of MultiIndex with Series length 1 failing',"b' pandas.Series([10.0], index=pandas.MultiIndex.from_tuples([(2, 3)])).sum(level=0) #level=None works, series length > 1 works\n'"
1562,5407697,wesm,wesm,2012-07-03 14:09:57,2012-07-12 00:04:27,2012-07-12 00:04:27,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1562,b'Date slicing bug',"b'This throws an exception\n\nhttp://stackoverflow.com/questions/11306167/pandas-slicing-on-a-timeseries-seems-inconsistent-with-list-slicing\n\n```\nimport pandas as pd\nimport datetime\n\ntimes = [datetime.datetime(2000, 1, 1) + datetime.timedelta(minutes=i) for i in range(1000000)]\ndf = pd.TimeSeries(range(1000000), times)\ndf.ix[datetime.datetime(1900,1,1):datetime.datetime(2100,1,1)]\n```'"
1561,5398581,wesm,wesm,2012-07-03 01:33:03,2012-07-11 23:22:30,2012-07-11 23:22:23,closed,wesm,0.8.1,5,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1561,b'datetime64 bug arising in Series.set_value',"b""from the mailing list\n\n```\nHi guys,\nI am using the new release 0.8.0. Below is what I observed. My numpy\nversion if 1.6.2. Please help. thx.\n\nIn [3]: from pandas import Series\n\nIn [4]: from datetime import datetime\n\nIn [5]: s = Series().set_value(datetime(2001,1,1),\n1.).set_value(datetime(2001,1,2),float('nan'))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/yihao/YH_Backups/YH_Work/lyh/gss/python/gss_main/<ipython-input-5-5c2e9c89865a>\nin <module>()\n----> 1 s = Series().set_value(datetime(2001,1,1),\n1.).set_value(datetime(2001,1,2),float('nan'))\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc in\nset_value(self, label, value)\n    741             return self\n    742         except KeyError:\n--> 743             new_index = np.concatenate([self.index.values, [label]])\n    744             new_values = np.concatenate([self.values, [value]])\n    745             return Series(new_values, index=new_index, name=self.name)\n\nTypeError: invalid type promotion\n```"""
1560,5390299,hmgaudecker,wesm,2012-07-02 16:51:51,2012-07-11 23:45:32,2012-07-11 23:45:32,closed,wesm,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1560,b'MultiIndex fails when passed a numpy.int64 index element instead of an int',"b""MultiIndex does not behave as expected anymore on 0.8.0 (I'm on Python 3.2.3). In particular, I get a strange error when I pass in an np.int64 object (i.e. the same type as the first level of the index). I hope the following testcase is clear, tried to stick close to the MultiIndex example in the docs.\n\n\nCode: \n\n    from pandas import Series\n    import numpy as np\n    arrays = [[0, 0, 1, 1, 2, 2, 3, 3],\n              [0, 1, 0, 1, 0, 1, 0, 1]]\n    s = Series(np.random.randn(8), index=arrays)\n    print(s[0]) # Works as expected\n    print(s[np.int64(0)]) # Fails strangely\n\nError:\n\n    In [6]: print(s[0]) # Works as expected\n    0   -0.420831\n    1   -1.818968\n\n    In [7]: print(s[np.int64(0)]) # Fails strangely\n    -1.81896761059---------------------------------------------------------------------------\n    ValueError                                Traceback (most recent call last)\n    <ipython-input-7-29c87a86dbe4> in <module>()\n    ----> 1 print(s[np.int64(0)]) # Fails strangely\n\n    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"""
1553,5371424,kieranholland,changhiskhan,2012-07-01 01:11:41,2012-07-12 15:20:54,2012-07-12 15:20:54,closed,changhiskhan,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1553,"b'csv parser fails with single line, no separator'","b""Due to _infer_columns() consuming 1 line when names provided:\n\n\n    read_csv(StringIO('1,2'), names=['a', 'b'], sep=None))\n    Traceback (most recent call last):\n        ...\n    StopIteration \n"""
1552,5370088,hmgaudecker,wesm,2012-06-30 20:54:59,2012-07-11 23:23:09,2012-07-11 23:23:09,closed,,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1552,b'to_panel() fills missings with zeros',"b""Pandas 0.8.0 -- converting a DataFrame with an unbalanced MultiIndex to a panel leads missing data to show up as zeros. At the minimum, this should be documented in the method's docstring, but I would consider it a bug (should be nan?)."""
1551,5370036,choffstein,wesm,2012-06-30 20:46:32,2012-07-13 17:53:25,2012-07-12 14:42:38,closed,,0.8.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1551,b'intersection between DatetimeIndex does not perform element-wise intersection',"b""When I have two dataframes, df1 and df2, and df1 is missing an index of df2, performing the intersection only seems to compute matching slices -- but doesn't take into account missing inner-elements.  For example, below, see that `2010-07-14 00:00:00` is missing from df1, but in df2.  The resulting index intersection, however, contains that index.  \n\n```python\n>>> df1.index\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-05-07 00:00:00, ..., 2012-06-29 00:00:00]\nLength: 542, Freq: None, Timezone: None\n\n>>> for x in df1.index[40:47]:\n...     print x\n...     \n... \n2010-07-06 00:00:00\n2010-07-07 00:00:00\n2010-07-08 00:00:00\n2010-07-09 00:00:00\n2010-07-12 00:00:00\n2010-07-13 00:00:00\n2010-07-15 00:00:00\n\n>>> df2.index\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-07-14 00:00:00, ..., 2012-06-29 00:00:00]\nLength: 3010, Freq: None, Timezone: None\n\n>>> for x in df2.index[2510:2517]:\n...     print x\n...     \n... \n2010-07-09 00:00:00\n2010-07-12 00:00:00\n2010-07-13 00:00:00\n2010-07-14 00:00:00\n2010-07-15 00:00:00\n2010-07-16 00:00:00\n2010-07-19 00:00:00\n\n>>> x = df1.index.intersection(df2.index)\n>>> for i in x[40:48]:\n...     print i\n... \n2010-07-06 00:00:00\n2010-07-07 00:00:00\n2010-07-08 00:00:00\n2010-07-09 00:00:00\n2010-07-12 00:00:00\n2010-07-13 00:00:00\n2010-07-14 00:00:00\n2010-07-15 00:00:00\n```"""
1548,5347433,ruidc,wesm,2012-06-29 09:30:33,2012-06-29 14:40:29,2012-06-29 14:40:29,closed,,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1548,b'matplotlib.pyplot called on import pandas on trunk',"b""the call added in https://github.com/pydata/pandas/commit/61aac3d0a98f748fd9fd4df9df0ef4bd8849f9b5\nadded an import of tseries.converter from tools\\plotting.py\nwhich in turn loads matplotlib:\nfrom matplotlib import pylab\n\nin rc2, the following worked:\nimport pandas\nimport matplotlib\nmatplotlib.use('qt4agg')\n\nin trunk:\nWARNING  C:\\Python27\\lib\\site-packages\\matplotlib\\__init__.py:921: UserWarning:  This call to matplotlib.use() has no effect\nbecause the the backend has already been chosen;\nmatplotlib.use() must be called *before* pylab, matplotlib.pyplot,\nor matplotlib.backends is imported for the first time.\n\n  if warn: warnings.warn(_use_error_msg)\n\nwhich leads to the default matplotlib backend being used\n\non first inspection it would appear that the line:\n from matplotlib import pylab\nin tseries/converter.py is not required as pylab does not appear to be used."""
1547,5347238,fabianp,wesm,2012-06-29 09:17:25,2012-06-29 14:54:50,2012-06-29 14:54:50,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1547,b'cannot use trace routines with Pandas',"b'Because of the way local variables are used in read_cvs, setting a tracing routine will fail at that point. Here is a test case with an arbitrarily simple tracing routine:\n\n\n```\ndef trace_memory_usage(frame, event, arg):\n    return trace_memory_usage\n\nimport sys\nsys.settrace(trace_memory_usage)\n\nif __name__ == \'__main__\':\n    from pandas.io.parsers import read_csv\n    print read_csv(\'dummy.txt\')\n```\n\nthis fails with a traceback\n\n```\nTraceback (most recent call last):\n  File ""test2.py"", line 10, in <module>\n    print read_csv(\'dummy.txt\')\n  File ""/volatile/fabian/envs/p26/lib/python2.6/site-packages/pandas/io/parsers.py"", line 226, in read_csv\n    return _read(TextParser, filepath_or_buffer, kwds2)\n  File ""/volatile/fabian/envs/p26/lib/python2.6/site-packages/pandas/io/parsers.py"", line 185, in _read\n    parser = cls(f, **kwds)\nTypeError: __init__() got an unexpected keyword argument \'sep\'\n```\n\nSome real-world utilities that are affected because of this are profile (but not cProfile) and memory_profiler. '"
1544,5322520,wesm,wesm,2012-06-28 13:41:11,2012-06-28 17:53:57,2012-06-28 17:53:57,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1544,b'read_csv parse_dates bug',b'http://stackoverflow.com/questions/11237557/typeerror-on-read-csv-working-in-pandas-7-error-in-8-0rc2-possible-dependen'
1537,5294266,gerigk,wesm,2012-06-27 10:40:43,2012-06-29 14:40:56,2012-06-29 00:01:13,closed,,0.8.0,6,Bug,https://api.github.com/repos/pydata/pandas/issues/1537,b'Problem when setting values based on MultiIndex subset',"b""```\nfrom pandas import *\ntest = read_csv('/home/arthur/transform_issue.csv')\nx =test.groupby(['A','B','C'])['revenues'].first().index\ntest.set_index(['A','B','C'], inplace=True)\ntest.ix[x]['revenues']= 999.99\n\nprint test.ix[x].shape\nprint test[test.revenues==999.99]\n\n(127, 5)\nEmpty DataFrame\nColumns: array([D, E, week, revenues, orders], dtype=object)\nIndex: array([], dtype=object)\n```\n\nI then tried setting via df[col][indexes] which simply crashed my session without any exception\n\n```\ntest = read_csv('/home/arthur/transform_issue.csv')\nx =test.groupby(['A','B','C'])['revenues'].first().index\ntest.set_index(['A','B','C'], inplace=True)\ntest['revenues'][x]= 999.99\n\nprint test.ix[x].shape\nprint test[test.revenues==999.99]\n```\n\nI sent the csv via email to wes@lambdafoundry.com\n"""
1536,5285782,wesm,changhiskhan,2012-06-26 22:55:11,2012-06-27 00:31:43,2012-06-27 00:31:43,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1536,b'index_col parser bug',"b""```\nIn [10]: !cat book_scripts/io/ex2.csv\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\nIn [11]: names\nOut[11]: ['a', 'b', 'c', 'd', 'message']\n\nIn [12]: read_csv('book_scripts/io/ex2.csv', names=names, index_col=['message'])\nOut[12]: \n         a   b   c   d\nmessage               \nhello    1   2   3   4\nworld    5   6   7   8\nfoo      9  10  11  12\n\nIn [13]: read_csv('book_scripts/io/ex2.csv', names=names, index_col='message')\nOut[13]: \n       a   b   c   d\nhello  1   2   3   4\nworld  5   6   7   8\nfoo    9  10  11  12\n```"""
1534,5281155,wesm,wesm,2012-06-26 19:02:59,2012-06-28 18:10:31,2012-06-28 18:10:31,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1534,b'MultiIndex.from_tuples issue',"b'from mailing list cc @ruidc \n\n```\nimport pandas\nmix = pandas.MultiIndex.from_tuples([(\'1a\', \'2a\'), (\'1a\', \'2b\'), (\'1a\', \'2c\'), (\'1b\', \'2d\')])\ndf = pandas.DataFrame([[1,2,3,4],[5,6,7,8]], columns=mix)\n#... manipulate columns here ... but not required to raise error here\ndf.columns = pandas.MultiIndex.from_tuples(df.columns) #df.columns.values works fine\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\index.py"", line 1538, in from_tuples\n    arrays = list(lib.tuples_to_object_array(tuples).T)\n  File ""inference.pyx"", line 687, in pandas.lib.tuples_to_object_array (pandas\\src\\tseries.c:97580)\nIndexError: Out of bounds on buffer access (axis 0)\n\nIs this a miss-use of from_tuples?\n```\n\n\n```\nimport pandas\nmix = pandas.MultiIndex.from_tuples([(\'1a\', \'2a\'), (\'1a\', \'2b\'), (\'1a\', \'2c\')])\ndf = pandas.DataFrame([[1,2],[3,4],[5,6]], index=mix)\ns = pandas.Series({(1,1): 1, (1,2): 2})\n#df[\'new\'] = s.to_dict() # Nice AssertionError about length mismatch\ndf[\'new\'] = s #bad kaboom on rc2, loads NaNs on b2 as expected\n\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 1712, in __setitem__\n    self._set_item(key, value)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 1751, in _set_item\n    value = self._sanitize_column(key, value)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 1778, in _sanitize_column\n    value = value.reindex(self.index).values\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 2044, in reindex\n    level=level, limit=limit)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\index.py"", line 791, in reindex\n    limit=limit)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\index.py"", line 722, in get_indexer\n    indexer = self._engine.get_indexer(target)\n  File ""engines.pyx"", line 245, in pandas.lib.IndexEngine.get_indexer (pandas\\src\\tseries.c:105931)\n  File ""hashtable.pyx"", line 751, in pandas.lib.PyObjectHashTable.lookup (pandas\\src\\tseries.c:21174)\n```'"
1533,5277114,CRP,wesm,2012-06-26 16:06:41,2013-10-04 15:28:52,2012-06-29 14:39:49,closed,,0.8.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1533,b'Assigning to a sliced a Panel',"b'Suppose I have:\n\na=Panel(items=[1,2,3],major_axis=[11,22,33],minor_axis=[111,222,333])\nb=DataFrame(randn(2,3),index=[111,333],columns=[1,2,3])\n\nI get an error when trying to assign b to a slice of a:\n\na.ix[:,22,[111,333]]=b\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-34-a593ce0f28a0> in <module>()\n----> 1 a.ix[:,22,[111,333]]=b\n/usr/local/lib/python2.7/site-packages/pandas/core/indexing.pyc in __setitem__(self, key, value)\n     66             indexer = self._convert_to_indexer(key)\n     67 \n---> 68         self._setitem_with_indexer(indexer, value)\n     69 \n     70     def _convert_tuple(self, key):\n/usr/local/lib/python2.7/site-packages/pandas/core/indexing.pyc in _setitem_with_indexer(self, indexer, value)\n    101             if isinstance(indexer, tuple):\n    102                 indexer = _maybe_convert_ix(*indexer)\n--> 103             self.obj.values[indexer] = value\n    104 \n    105     def _getitem_tuple(self, tup):\nValueError: array is not broadcastable to correct shape\n\nThe slice of a and b look of same shape to me, so I do not understand the broadcasting error.\n\nWhen debugging _setitem_with_indexer() I notice that the indexer returns a 2x3 array instead of 3x2, so if I do:\n\na.ix[:,22,[111,333]]=b.T\n\nthen it works. Is this a bug or the intended behaviour?\n'"
1532,5276302,leonbaum,wesm,2012-06-26 15:31:03,2012-06-28 18:21:47,2012-06-28 18:21:47,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1532,b'Segmentation fault with MultiIndex join()',"b""With pandas from the current master branch and python-3.2.2, I'm getting a seg fault with the following minimal example:\n\n```\ndf1 = pandas.DataFrame({'a': [1, 1], 'b': [1, 2], 'x': [1, 2]})\ndf2 = pandas.DataFrame({'a': [2, 2], 'b': [1, 2], 'y': [1, 2]})\ndf1 = df1.set_index(['a', 'b'])\ndf2 = df2.set_index(['a', 'b'])\ndf1.join(df2)\n\nProcess Python segmentation fault\n```"""
1520,5239012,gerigk,wesm,2012-06-24 20:31:56,2013-12-04 00:58:03,2012-06-25 13:58:59,closed,changhiskhan,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1520,b'HDFStore casting all numeric columns to datetime',"b""I will update with a self contained example if I manage to find one (please tell me if necessary).\n\n\n```\nprint df.dtypes\n0              int64\n1              int64\n2             object\n3             object\n4             object\n5             object\n6     datetime64[ns]\n7            float64\n8              int64\n9              int64\n10             int64\n11           float64\n12           float64\n13              bool\n14              bool\n15             int64\n16            object\n17             int64\n18             int64\n19             int64\n20             int64\n21             int64\n22             int64\n23           float64\n24           float64\n25           float64\n26           float64\n27           float64\n28           float64\n29           float64\n30           float64\n31           float64\n32           float64\n33           float64\n34           float64\n35           float64\n36           float64\n37           float64\n38           float64\n39           float64\n40           float64\n41           float64\n42           float64\n43           float64\n44           float64\n45           float64\n46           float64\n47           float64\n48           float64\n49             int64\n50             int64\n51           float64\n52           float64\n53             int64\nLength: 54\n\nstore = HDFStore('data.h5')\nstore['last_three_months'] = df\nstore.close()\n\nstore = HDFStore('data.h5')\ntest = store['last_three_months']\nprint test.dtypes\n\n0     datetime64[ns]\n1     datetime64[ns]\n2             object\n3             object\n4             object\n5             object\n6     datetime64[ns]\n7     datetime64[ns]\n8     datetime64[ns]\n9     datetime64[ns]\n10    datetime64[ns]\n11    datetime64[ns]\n12    datetime64[ns]\n13    datetime64[ns]\n14    datetime64[ns]\n15    datetime64[ns]\n16            object\n17    datetime64[ns]\n18    datetime64[ns]\n19    datetime64[ns]\n20    datetime64[ns]\n21    datetime64[ns]\n22    datetime64[ns]\n23    datetime64[ns]\n24    datetime64[ns]\n25    datetime64[ns]\n26    datetime64[ns]\n27    datetime64[ns]\n28    datetime64[ns]\n29    datetime64[ns]\n30    datetime64[ns]\n31    datetime64[ns]\n32    datetime64[ns]\n33    datetime64[ns]\n34    datetime64[ns]\n35    datetime64[ns]\n36    datetime64[ns]\n37    datetime64[ns]\n38    datetime64[ns]\n39    datetime64[ns]\n40    datetime64[ns]\n41    datetime64[ns]\n42    datetime64[ns]\n43    datetime64[ns]\n44    datetime64[ns]\n45    datetime64[ns]\n46    datetime64[ns]\n47    datetime64[ns]\n48    datetime64[ns]\n49    datetime64[ns]\n50    datetime64[ns]\n51    datetime64[ns]\n52    datetime64[ns]\n53    datetime64[ns]\nLength: 54\n```"""
1518,5235948,wesm,wesm,2012-06-24 14:22:36,2012-06-25 22:32:57,2012-06-25 22:32:57,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1518,b'Time zone handling / strftime issues',b'Moving issue here:\n\nhttp://stackoverflow.com/questions/11175213/pandas-read-csv-input-local-datetime-strings-tz-convert-to-utc'
1517,5232691,wesm,wesm,2012-06-23 23:20:17,2012-06-25 14:41:03,2012-06-25 14:41:03,closed,changhiskhan,0.8.0,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1517,b'Plotting both fixed frequency and irregular time series on same subplot fails',b'I presume this is due to conversion to PeriodIndex. Needs to be fixed for 0.8.0'
1513,5220633,scouredimage,wesm,2012-06-22 18:06:09,2012-06-22 19:27:26,2012-06-22 19:26:59,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1513,b'DataFrame.drop loses MultiIndex',"b""```\nIn [2]: df = DataFrame({'a': [1,2,3], 'b': [1,2,3], 'c': [4,5,6]})\n\nIn [3]: df.set_index(['a','b'], inplace=True)\nOut[3]: \n     c\na b   \n1 1  4\n2 2  5\n3 3  6\n\nIn [5]: dropped = df.drop((1,1))\n\nIn [6]: dropped\nOut[6]: \n     c\n2 2  5\n3 3  6\n\n```\nMultiIndex is gone! (2,2) and (3,3) are now plain tuples"""
1511,5219838,dalejung,wesm,2012-06-22 17:22:40,2012-06-28 22:40:21,2012-06-28 22:40:21,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1511,b'Inconsistent Behavior of cut when labels=False',"b""```python\ns = Series([0, -1, 0, 1, -3])\nind, label = cut(s, [-4, 0, 1], retbins=True, labels=False)\n# label [-4  0  1]\n# ind array([0, 0, 0, 1, 0])\n\ns = Series([0, -1, 0, 1, -3])\nind, label = cut(s, [0, 1], retbins=True, labels=False)\n# label [0 1]\n# ind array([ nan,  nan,  nan,   1.,  nan]) \n# Should the 1 be a 0?\n```\n\nI recall that there always used to be an implicit out of range level. It seems like the second example still acts like the (-inf, 0] exists. \n\nI could test the ind for nan to handle this, but wanted to make sure it wasn't a bug."""
1508,5196696,lbeltrame,wesm,2012-06-21 15:53:31,2012-11-02 16:56:11,2012-11-02 16:56:11,closed,,0.9.1,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1508,b'na_option is not used in DataFrame.rank()',"b'Currently na_option is unused in rank(). It would be nice to have at last ""top"" or ""bottom"" along with ""keep"" (it\'s actually useful as some published algorithms need this feature).'"
1505,5190505,gerigk,wesm,2012-06-21 11:10:29,2012-06-21 23:02:54,2012-06-21 23:02:54,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1505,b'itertuples converts int to float for all numeric rows.',"b""```\nfrom pandas import *\nx = DataFrame([(1,3,4.5,6.5), (1,3,4.5,6.5)])\nprint x\nprint x.dtypes\nfor row in x.itertuples():\n    print row\n\n   0  1    2    3\n0  1  3  4.5  6.5\n1  1  3  4.5  6.5\n0      int64\n1      int64\n2    float64\n3    float64\n(0, 1.0, 3.0, 4.5, 6.5)\n(1, 1.0, 3.0, 4.5, 6.5)\n```\nmixed dtype row leaves ints untouched\n\n```\nrom pandas import *\nx = DataFrame([(1,3,4.5,6.5,'a'), (1,3,4.5,6.5, 'b')])\nprint x\nprint x.dtypes\nfor row in x.itertuples():\n    print row\n\n  0  1    2    3  4\n0  1  3  4.5  6.5  a\n1  1  3  4.5  6.5  b\n0      int64\n1      int64\n2    float64\n3    float64\n4     object\n(0, 1, 3, 4.5, 6.5, 'a')\n(1, 1, 3, 4.5, 6.5, 'b')\n```\n\n```"""
1502,5174312,mrjbq7,wesm,2012-06-20 16:10:06,2016-01-14 21:35:58,2016-01-14 21:35:33,closed,,No action,2,Bug;Enhancement;Performance;Won't Fix,https://api.github.com/repos/pydata/pandas/issues/1502,b'Performance of adding columns to DataFrame',"b""I found a performance issue involving modifying ``DataFrame`` objects.  Adding columns where a value is present for every value of the index (e.g., ``len(column) == len(index)``) is really, really slow.\n\n```python\nimport numpy as np\nimport pandas\nimport time\n\nvalues = np.random.randn(100)\n\nt0 = time.time()\ncolumns = [values for i in range(10000)]\ndf = pandas.DataFrame(np.column_stack(columns),\n                      columns=range(len(columns)),\n                      index=values)\nprint 'A took %.3f seconds' % (time.time() - t0)\n\nt0 = time.time()\ndf = pandas.DataFrame(index=values)\nfor i in range(10000):\n    df[i] = values\nprint 'B took %.3f seconds' % (time.time() - t0)\n```\n\nRunning it on my laptop shows that the first version (doing all the work in ``__init__``) is much faster:\n\n```\nA took 0.070 seconds\nB took 10.717 seconds\n```\n\nThe ``cProfile`` results show hotspots involving the index:\n\n```\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    13410    2.716    0.000    2.716    0.000 {method 'astype' of 'numpy.ndarray' objects}\n    12246    2.024    0.000    2.024    0.000 {numpy.core.multiarray.concatenate}\n    22274    1.443    0.000    1.501    0.000 index.py:277(__contains__)\n    10555    1.103    0.000    1.103    0.000 {method 'copy' of 'numpy.ndarray' objects}\n      555    0.639    0.001    3.711    0.007 internals.py:823(_consolidate_inplace)\n      555    0.429    0.001    3.072    0.006 internals.py:1370(_consolidate)\n    11135    0.299    0.000    8.921    0.001 internals.py:892(insert)\n    11141    0.217    0.000    0.217    0.000 {pandas.lib.is_integer_array}\n   121164    0.163    0.000    0.163    0.000 {numpy.core.multiarray.array}\n    25105    0.140    0.000    3.493    0.000 index.py:75(__new__)\n   342808    0.107    0.000    0.107    0.000 {isinstance}\n     1691    0.106    0.000    0.115    0.000 index.py:211(is_unique)\n    11138    0.096    0.000    0.096    0.000 {pandas.lib.list_to_object_array}\n```"""
1500,5166540,gerigk,wesm,2012-06-20 10:05:07,2013-12-04 00:58:03,2012-06-20 20:35:20,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1500,b'ERROR: test_argmin_argmax (pandas.tseries.tests.test_timeseries.TestDatetimeIndex)',"b'```\n\nTraceback (most recent call last):\n  File ""/home/agerigk/python-packages/pandas/pandas/tseries/tests/test_timeseries.py"", line 1106, in test_argmin_argmax\n    self.assertEqual(idx.argmin(), 1)\n  File ""/home/agerigk/python-packages/pandas/pandas/tseries/index.py"", line 1041, in argmin\n    return (-self).argmax()\nTypeError: ufunc \'negative\' did not contain a loop with signature matching types dtype(\'<M8[ns]\') dtype(\'<M8[ns]\')\n\n----------------------------------------------------------------------\nRan 2037 tests in 23.856s\n\nFAILED (SKIP=46, errors=1)\n```'"
1499,5148649,wesm,wesm,2012-06-19 15:14:07,2012-06-19 16:33:55,2012-06-19 16:33:55,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1499,b'Series datetime64[ns] boxing issue',b'Boxing does not happen in `DatetimeEngine`'
1498,5146520,wesm,wesm,2012-06-19 13:47:44,2012-06-19 15:10:15,2012-06-19 15:10:15,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1498,"b""concat doesn't work around datetime64[ns] concatenate bug in NumPy 1.6""",
1493,5125324,wesm,lodagro,2012-06-18 14:49:20,2012-06-18 19:02:53,2012-06-18 19:02:53,closed,,0.8.0,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1493,b'Keywords not being passed on in DataFrame.boxplot',"b""```\nPandas uses the matplotlib boxplot, but it does not support passing down the argument 'notch=1' to the actual matplotlib boxplot call.\n\ndf.boxplot(notch=1) gives a traceback\n\nAlthough according to documentation there should be some kind of kwds mechanism, there is none in fact.\n\nDocstring:\nboxplot(self, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, **kwds) method of pandas.core.frame.DataFrame instance\n```"""
1491,5114914,turkeytest,wesm,2012-06-17 22:31:09,2012-06-21 22:57:20,2012-06-21 22:57:20,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1491,b'DataFrame constructor (dict of tuples)',"b""This might be an API change instead of an issue, but just in case, could you please confirm that this should crash?  \n\n      DataFrame({'a':(1,2,3)})                \n\nwith this error: \n\n     *** ValueError: If use all scalar values, must pass index\n\nUsing a dictionary where the values are equal length tuples used to work in 0.7. \n\nThanks!"""
1489,5113719,changhiskhan,changhiskhan,2012-06-17 18:48:55,2012-06-19 01:02:40,2012-06-19 01:02:40,closed,changhiskhan,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1489,b'Time series dtype bug in DataFrame constructor',"b""from pandas import Series, DataFrame\nimport pandas as pd\n\nrng1 = pd.date_range('1/1/1999', '1/1/2012', freq='MS')\ns1 = Series(np.random.randn(len(rng1)), rng1)\n\n\nrng2 = pd.date_range('1/1/1980', '12/1/2001', freq='MS')\ns2 = Series(np.random.randn(len(rng2)), rng2)\n\ndf = DataFrame({'s1' : s1, 's2' : s2})\n\n# s1 is fine\nIn [39]: s1.index.dtype\nOut[39]: dtype('datetime64[ns]')\n\nIn [41]: s1.index.values.dtype\nOut[41]: dtype('datetime64[ns]')\n\n# s2 is fine\nIn [40]: s2.index.dtype\nOut[40]: dtype('datetime64[ns]')\n\nIn [42]: s2.index.values.dtype\nOut[42]: dtype('datetime64[ns]')\n\n# df.index looks fine\nIn [43]: df.index.dtype\nOut[43]: dtype('datetime64[ns]')\n\n# WAT?!\nIn [44]: df.index.values.dtype\nOut[44]: dtype('datetime64[us]')\n"""
1486,5109684,wesm,wesm,2012-06-17 00:34:34,2012-06-19 20:54:46,2012-06-19 20:54:46,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1486,b'fillna called with a Series not behave like a dict',
1485,5109652,wesm,wesm,2012-06-17 00:23:46,2012-06-19 21:21:06,2012-06-19 21:21:06,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1485,b'DataFrame.fillna with axis=1 result unintuitive when passed dict',
1484,5109425,saltantsolutions,wesm,2012-06-16 23:24:50,2012-06-30 01:16:54,2012-06-21 23:11:06,closed,,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1484,b'TestDataFramePlots.test_scatter fails with matplotlib v1.0.1',"b'# Synopsis\n\nThe FreeBSD port for matplotlib has not yet been updated to v1.1.0. In the mean time, this test fails due to a default marker type (\'.\') that is unknown in matplotlib v1.0.1.\n\n# Details\n\n## Environment\n\n```\n# uname -a\nFreeBSD REDACTED 9.0-STABLE FreeBSD 9.0-STABLE #0 r236450: Sat Jun  2 11:08:51 EDT 2012     root@REDACTED:/usr/obj/usr/src/sys/NIPPL  amd64\n# pkg_info -oxQ pandas matplotlib\npy27-matplotlib-1.0.1_4:math/py-matplotlib\npy27-pandas-0.8.0.b2:math/py-pandas\n```\n## How to reproduce\n\n```\n#\tnosetests -xv pandas.tests.test_graphics:TestDataFramePlots.test_scatter\ntest_scatter (pandas.tests.test_graphics.TestDataFramePlots) ... ERROR\n\n======================================================================\nERROR: test_scatter (pandas.tests.test_graphics.TestDataFramePlots)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/site-packages/pandas-0.8.0b2-py2.7-freebsd-9.0-STABLE-amd64.egg/pandas/tests/test_graphics.py"", line 252, in test_scatter\n    _check_plot_works(scat)\n  File ""/usr/local/lib/python2.7/site-packages/pandas-0.8.0b2-py2.7-freebsd-9.0-STABLE-amd64.egg/pandas/tests/test_graphics.py"", line 301, in _check_plot_works\n    ret = f(*args, **kwargs)\n  File ""/usr/local/lib/python2.7/site-packages/pandas-0.8.0b2-py2.7-freebsd-9.0-STABLE-amd64.egg/pandas/tests/test_graphics.py"", line 251, in scat\n    return plt.scatter_matrix(df, **kwds)\n  File ""/usr/local/lib/python2.7/site-packages/pandas-0.8.0b2-py2.7-freebsd-9.0-STABLE-amd64.egg/pandas/tools/plotting.py"", line 71, in scatter_matrix\n    marker=marker, alpha=alpha, **kwds)\n  File ""/usr/local/lib/python2.7/site-packages/matplotlib/axes.py"", line 5738, in scatter\n    raise ValueError(\'Unknown marker symbol to scatter\')\nValueError: Unknown marker symbol to scatter\n\n----------------------------------------------------------------------\nRan 1 test in 0.633s\n\nFAILED (errors=1)\n```\n\n## Code analysis\n\n[pandas/tests/test_graphics.py:252](https://github.com/pydata/pandas/blob/fde270b20861fbf36d3063d504fb299d0b58695b/pandas/tests/test_graphics.py#L252) does not specify a marker type.\n[pandas/tools/plotting.py:22]\n(https://github.com/pydata/pandas/blob/d7966d22f14375547e263347ec648dcba7b65617/pandas/tools/plotting.py#L22) defines `\'.\'` as the default value for the `marker`\n[matplotlib/axes.py:5669-5682](https://github.com/matplotlib/matplotlib/blob/a9f3f3a50745a1ca0e666bb1b6d0b9d782553dd9/lib/matplotlib/axes.py#L5669-5682) list the marker types recognized in matplotlib v1.0.1.\n\n## Suggested resolution\n\nChoose a marker type available in matplotlib v1.0.1 (e.g. `\'o\'`).'"
1483,5101181,dalejung,wesm,2012-06-15 21:32:02,2012-06-19 20:46:47,2012-06-19 20:46:47,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1483,b'MonthStart resample errors on binning.',"b""```python\nfrom pandas import *\nN = 1000\ns = Series(np.arange(N), index=date_range(start='1/1/2012', freq='5min', periods=N))\ns.resample('MS')\n```\nNot sure if this is new, first time using MS."""
1482,5099712,changhiskhan,wesm,2012-06-15 20:11:25,2012-06-19 18:32:41,2012-06-19 18:32:41,closed,,0.8.0,0,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1482,"b""Don't convert bday freq in ts plots""",b'X-axis should omit weekends if bday freq'
1480,5099145,wesm,wesm,2012-06-15 19:38:31,2012-06-19 17:36:17,2012-06-19 17:36:17,closed,wesm,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1480,b'Slicing with duplicate index values error',"b'```\nIf one has data with non-unique indices like this one:\n                        X.3 \nX.1_X.2                            \n2004-01-02 08:12:30  1.2586  \n2004-01-02 08:12:31  1.2586  \n2004-01-02 08:12:32  1.2586  \n2004-01-02 08:12:32  1.2585  \n2004-01-02 08:12:36  1.2585  \n2004-01-02 08:12:37  1.2585  \n\nand one does a slicing like:\ndata[datetime(2004,1,2,8,12,32):]\nit fails with\n\nIndexError: invalid slice\n\nShould it work ?\n\nJochen\n\nwhole traceback:\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.0b2-py2.6-linux-x86_64.egg/pandas/core/frame.pyc in __getitem__(self, key)\n   1572             else:\n   1573                 indexer = self.ix._convert_to_indexer(key, axis=0)\n-> 1574             new_data = self._data.get_slice(indexer, axis=1)\n   1575             return self._constructor(new_data)\n   1576         # either boolean or fancy integer index\n\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.0b2-py2.6-linux-x86_64.egg/pandas/core/internals.pyc in get_slice(self, slobj, axis)\n    651     def get_slice(self, slobj, axis=0):\n    652         new_axes = list(self.axes)\n--> 653         new_axes[axis] = new_axes[axis][slobj]\n    654 \n    655         if axis == 0:\n\n/home/jgarcke/Prog/lib64/python2.6/site-packages/pandas-0.8.0b2-py2.6-linux-x86_64.egg/pandas/tseries/index.pyc in __getitem__(self, key)\n   1040                     new_offset = self.offset\n   1041 \n-> 1042             result = arr_idx[key]\n   1043             if result.ndim > 1:\n   1044                 return result\n\nIndexError: invalid slice\n```'"
1475,5090916,lesteve,wesm,2012-06-15 12:01:41,2012-06-22 16:21:39,2012-06-21 17:03:18,closed,,0.8.0,8,Bug,https://api.github.com/repos/pydata/pandas/issues/1475,b'beta 0.8.0b2: creating timeseries with index in year 1400 yields wrong timestamp',"b""There seems to be some kind of wrap-around phenomenon if your index is too far away from now. For example if I create a timeseries with an index in 1400, the timeseries I get back has an index which is some time in 1984:\n\n```python\nIn [1]: import datetime, pandas\n\nIn [2]: pandas.version.version\nOut[2]: '0.8.0b2'\n\nIn [3]: ts = pandas.TimeSeries(index=[ datetime.datetime(1400,1,2) ], data = [1])\n\nIn [4]: ts\nOut[4]: 1984-07-22 23:34:33.709551    1\n\n```\n\nIt doesn't seem to be a display issue because I can do:\n```python\nIn [7]: ts.index[0].to_pydatetime()\nWarning: discarding nonzero nanoseconds\nOut[7]: datetime.datetime(1984, 7, 22, 23, 34, 33, 709551)\n\n```\n\nI tested I could get the same behaviour both using 32-bit and 64-bit python on windows. It seems to work fine for 0.7.3:\n\n```python\nIn [1]: import datetime, pandas\n\nIn [2]: pandas.version.version\nOut[2]: '0.7.3'\n\nIn [3]: ts = pandas.TimeSeries(index=[datetime.datetime(1400,1,2)], data = [1])\n\nIn [4]: ts.index[0]\nOut[4]: datetime.datetime(1400, 1, 2, 0, 0)\n```"""
1471,5074779,wesm,wesm,2012-06-14 16:54:28,2013-11-24 13:44:10,2012-06-14 19:09:35,closed,,0.8.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/1471,b'More care with resampling intraday data to anchored frequencies',"b'The convention of, say, MonthEnd at midnight will result in on *on that day* being placed in the wrong bucket\n\nReferenced in #1458'"
1470,5072164,gerigk,wesm,2012-06-14 14:55:06,2012-06-14 15:01:15,2012-06-14 15:01:15,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1470,b'makeQuantiles accesses remove_na which is not defined',
1466,5061799,changhiskhan,wesm,2012-06-14 02:58:52,2012-06-19 17:20:00,2012-06-19 17:20:00,closed,changhiskhan,0.8.0,3,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1466,b'Treatment of gaps in timeseries plots',"b'add option to drop parts of the xaxis without data\n\nfrom mailing list:\n\nHi,\n\nI am having difficulty sorting out how to do any intra-day plot of time series data in matplotlib or pandas.  I have looked through the documentation and also purchased the Python_for_Data_Analysis ebook but it does not seem covered in either.  There are many references to daily samplings.\n\nI can produce images by using the DataFrame.plot or Series.plot function.  This results in a well laid out plot, except for the hours outside of my timeseries (in the example of exchange hours for financial data) are included in the axis and interpolated.  If I give it the criteria use_index=False, the plot looks as I would expect but I look my Timestamp index and it becomes an integer index as I would receive if I used matplotlib/pyplot.  \n\nI am looking for direction on this and to recommend an inclusion of this topic in either the references and documentation.  Pandas is great and I love learning and working with it.  \n\n'"
1465,5061189,dalejung,wesm,2012-06-14 01:40:20,2013-12-04 00:43:13,2012-06-14 03:38:05,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1465,"b""resample error with closed='left' and label='right'.""","b""```python\nfrom pandas import *\n\n# ERROR\ns = Series(0, index=date_range(start='1/1/2012 9:30', freq='1min', periods=21))\ns.resample('10min', how='mean',closed='left', label='right')\n\n# Works. Only changed the start time. \ns = Series(0, index=date_range(start='1/1/2012 9:31', freq='1min', periods=21))\ns.resample('10min', how='mean',closed='left', label='right')\n```\n\nHappens whenever the binlabels are trimmed and the label is set to 'right'. It ends up reducing the labels twice. """
1464,5059569,dalejung,wesm,2012-06-13 23:21:03,2012-06-14 02:36:24,2012-06-14 02:36:24,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1464,b'Resample bug with sparse indexes. Incorrectly assumes upsample',"b""```python\nfrom pandas import *\n\ndr = date_range(start='1/1/2012', freq='5min', periods=1000)\ns = Series(np.array(100), index=dr)\n# subset the data. \ns = s[s.index.hour < 7]\n# len(s) is now 336\n\n\ns.resample('10min', how=lambda x: len(x))\n# Wrong, thinks we're upsampling\n# 2012-01-01 00:00:00    100\n# 2012-01-01 00:15:00    100\n# 2012-01-01 00:30:00    100\n# 2012-01-01 00:45:00    100\n# 2012-01-01 01:00:00    100\n\n# 30min index has less labels.\ns.resample('30min', how=lambda x: len(x))\n# Right\n# 2012-01-01 00:00:00    1\n# 2012-01-01 00:30:00    6\n# 2012-01-01 01:00:00    6\n# 2012-01-01 01:30:00    6\n# 2012-01-01 02:00:00    6\n```\n\nWhat's going on here is that resample tests whether you are down/up sampling by the len of label arrays. If your source data is ordered but sparse, the resampling will sometimes assume you're up-sampling when you're not. \n\nI run into this with intraday finance data since mine is restricted to trading hours. """
1461,5058144,wesm,wesm,2012-06-13 21:48:19,2012-06-13 22:23:26,2012-06-13 22:23:26,closed,wesm,0.8.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1461,b'Enable / test assigning to categorical / factor levels',
1460,5058121,wesm,wesm,2012-06-13 21:47:32,2012-06-13 22:23:18,2012-06-13 22:23:18,closed,wesm,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1460,"b""labels argument to cut not exactly what's desired""","b'If pass `len(bins) - 1` labels, should use those without forming intervals'"
1459,5055498,petergx,wesm,2012-06-13 19:42:48,2014-12-20 23:39:38,2012-06-14 02:50:37,closed,,0.8.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1459,b'Resample bug after converting tz',"b""```\ndr = date_range(start='2012-4-13', end='2012-5-1')\nts = Series(range(len(dr)), dr)\nts_utc = ts.tz_convert('UTC')\nts_local = ts_utc.tz_convert('America/Los_Angeles')\nts_local.resample('W')\n```\n\nyields\n\n```traceback\n~/env/lib/python2.7/site-packages/pandas/tseries/resample.pyc in _get_time_bins(self, axis)\n    107 \n    108         # general version, knowing nothing about relative frequencies\n\n--> 109         bins = lib.generate_bins_dt64(axis.asi8, binner.asi8, self.closed)\n    110 \n    111         if self.label == 'right':\n\n~/env/lib/python2.7/site-packages/pandas/lib.so in pandas.lib.generate_bins_dt64 (pandas/src/tseries.c:60938)()\n\nValueError: Values falls before first bin\n```"""
1458,5055165,wesm,wesm,2012-06-13 19:26:49,2012-06-14 19:09:32,2012-06-14 19:09:32,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1458,b'Resampling issue described in stackoverflow',b'http://stackoverflow.com/questions/11018120/bug-in-resampling-with-pandas-0-8'
1455,5038354,dalejung,wesm,2012-06-13 00:46:11,2012-06-14 14:49:05,2012-06-14 14:48:58,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1455,b'SeriesGroupBy.count() errors when original series is MultiIndex with Datetime as level 0',"b""```python\nfrom pandas import *\n\ndr = date_range(start='1/1/2012', freq='5min', periods=10)\n\n# BAD Example, datetimes first\nlabels = zip(dr, range(10))\ns = Series(np.arange(10), index=MultiIndex.from_tuples(labels, names=['a','b']))\ngrouped = s.groupby(lambda x: x[1] % 2 == 0)\n\ngrouped.count()\n# DateParseError: Could not parse COUNT\n\n# Works fine\nlabels = zip(range(10), dr)\ns = Series(np.arange(10), index=MultiIndex.from_tuples(labels, names=['a','b']))\ngrouped = s.groupby(lambda x: x[0] % 2 == 0)\ngrouped.count()\n# False    5\n# True     5\n```\n\nSeems to have happened within the last few days, maybe even just today. """
1451,5034443,wesm,wesm,2012-06-12 21:49:29,2012-06-14 16:47:57,2012-06-14 16:47:57,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1451,b'Monthly resampling bug encountered at PyGotham',"b""To reproduce:\n\n```\nfrom pandas import *\ndates = date_range('4/16/2012 20:00', periods=5000, freq='h')\nts = Series(randn(len(dates)), index=dates)\nts.resample('M')\n```"""
1437,4980238,saroele,wesm,2012-06-08 22:34:54,2012-06-11 16:31:10,2012-06-11 16:30:50,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1437,b'datetimeindex returns wrong datetimes with tolist() and values()',"b""In [82]: dr = pandas.date_range(start='2012-01-01', periods=10, freq='30Min')\n\nIn [83]: dr\nOut[83]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-01-01 00:00:00, ..., 2012-01-01 04:30:00]\nLength: 10, Freq: 30T, Timezone: None\n\nIn [84]: dr.values\nOut[84]: \narray([1970-01-16 224:00:00, 1970-01-16 224:30:00, 1970-01-16 225:00:00,\n       1970-01-16 225:30:00, 1970-01-16 226:00:00, 1970-01-16 226:30:00,\n       1970-01-16 227:00:00, 1970-01-16 227:30:00, 1970-01-16 228:00:00,\n       1970-01-16 228:30:00], dtype=datetime64[ns])\n"""
1435,4976877,grantf,wesm,2012-06-08 19:17:39,2012-06-14 03:08:38,2012-06-14 03:08:38,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1435,b'Inconsistent results using custom function though resample & how',"b""I have noticed some inconsistent results between using resample & how calling a defined function.  It might be I have misunderstood how this is meant to function.\n\nHere's some code to illustrate it:\n\n```\nimport numpy as np\nimport pandas\nfrom datetime import *\n\nstart = datetime(2010,6,1)\nend = datetime(2012,1,1)\n\nrng = pandas.date_range(start, end, freq='BM')\n\nts = pandas.Series(np.random.randn(len(rng)), index=rng)\n\ndef testfunction1(data):\n    return np.sum(data) / len(data)\n\ndef testfunction2(data):\n    return np.mean(data) / np.std(data)\n\nt1 = ts.resample('Q', how=testfunction1) \n\nt0 = ts.resample('Q', how=np.mean)\n\nt2 = ts.resample('Q', how=np.sum)\n\nt3 = ts.resample('Q', how=len)\n\n# Compare defined function with numpy function\n\nt1 == t0\n\n# Everything is True\n\n# Compare defined function (t1) with explicit calculation on timeseries\n\nt2/t3 == t1\n\n# Everything is True\n\n# Apply second function involving mean/std\n\nd0 = ts.resample('Q', how=testfunction2)\n\nd1 = ts.resample('Q', how=np.mean)\n\nd2 = ts.resample('Q', how=np.std)\n\n# Compare defined function with explicit calculation on timeseries\n\nd1/d2 == d0\n\n# Everything is False\n```"""
1433,4972912,gerigk,wesm,2012-06-08 15:34:05,2012-06-14 03:04:56,2012-06-14 03:04:37,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1433,b'Unexpected behaviour when converting datetime column to np.datetime64',"b'To me this behaviour appeared to be weird.\nNumpy 1.7 and latest Pandas\n\n```\nfrom pandas import *\nfrom datetime import datetime\nx = DataFrame([datetime.now(), datetime.now()])\nprint x[0].dtype\nprint to_datetime(x[0]).dtype\nx[0] = to_datetime(x[0])\nprint x[0].dtype, ""shouldn\'t this be datetime64?""\nx[1] = to_datetime(x[0])\nprint x[1].dtype, ""this is""\n\nobject\ndatetime64[ns]\nobject shouldn\'t this be datetime64?\ndatetime64[ns] this is\n```'"
1432,4972825,lodagro,wesm,2012-06-08 15:28:58,2012-06-11 16:55:22,2012-06-11 16:55:14,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1432,b'DataFrame.ix assign fails for mixed dtypes',"b'from [mailing list](http://groups.google.com/group/pydata/browse_thread/thread/43c1bc3a314a9bb3#)\n\n```python\ndata = pandas.DataFrame.from_dict({1: [1.,2.,3.], 2: [3,4,5]})\n\nprint data.dtypes\n\ndata.ix[1] = [10., 20] # fails\n```'"
1430,4971064,changhiskhan,wesm,2012-06-08 13:57:30,2013-12-04 00:58:08,2012-06-08 21:19:26,closed,,0.8.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1430,b'timeseries related .groups bug',"b""related to #1423\n\n```\nfrom pandas import *                                                                                  \nimport numpy as np                                                                                    \nperiods = 1000                                                                                        \nind = DatetimeIndex(start='2012/1/1', freq='5min', periods=periods)                                   \ndf = DataFrame({'high': np.arange(periods), 'low': np.arange(periods)}, index=ind)                    \n\ngrouped = df.groupby(lambda x: datetime(x.year, x.month, x.day))      \n\n\n\nIn [9]: grouped.groups\n\nValueError                                Traceback (most recent call last)\n<ipython-input-9-aec229cf123a> in <module>()\n----> 1 grouped.groups\n\n~/pandas/pandas/core/groupby.pyc in groups(self)\n    150     @property\n    151     def groups(self):\n--> 152         return self.grouper.groups\n    153 \n    154     @property\n\n~/pandas/pandas/core/index.pyc in groupby(self, to_groupby)\n    740 \n    741     def groupby(self, to_groupby):\n--> 742         return self._groupby(self.values, to_groupby)\n    743 \n    744     def map(self, mapper):\n\n...\n...\n...\n\n~/pandas/pandas/lib.so in pandas.lib.groupby_arrays (pandas/src/tseries.c:67246)()\n\nValueError: Buffer dtype mismatch, expected 'int64_t' but got Python object\n```"""
1426,4962287,wesm,wesm,2012-06-08 00:17:44,2012-06-11 17:00:19,2012-06-11 17:00:19,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1426,b'Raise exception on joins between tz-naive DatetimeIndex and tz-aware ones',
1424,4961561,wesm,wesm,2012-06-07 23:18:57,2012-06-08 01:00:40,2012-06-08 01:00:40,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1424,b'Limit argument not respected in `resample`',"b""```\nIn [5]: rng = date_range('1/1/2000', periods=3, freq='5t')\n\nIn [6]: ts = Series(np.random.randn(len(rng)), rng)\n\nIn [7]: ts.resample('t', fill_method='ffill', limit=2)\nOut[7]: \n2000-01-01 00:00:00    0.204905\n2000-01-01 00:01:00    0.204905\n2000-01-01 00:02:00    0.204905\n2000-01-01 00:03:00    0.204905\n2000-01-01 00:04:00    0.204905\n2000-01-01 00:05:00    0.804447\n2000-01-01 00:06:00    0.804447\n2000-01-01 00:07:00    0.804447\n2000-01-01 00:08:00    0.804447\n2000-01-01 00:09:00    0.804447\n2000-01-01 00:10:00   -0.602923\nFreq: T\n```"""
1423,4961186,dalejung,wesm,2012-06-07 22:50:52,2012-06-11 21:03:10,2012-06-11 18:54:21,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1423,b'select() within a function closure not working as agg function',"b'I\'m running into a weird issue with groupby and function closure. For some reason the function closure doesn\'t work unless I access the grouped series. You can see in agg_before I have a fix flag that will just access the data var. \n\n```python\nfrom pandas import *                                                                                  \nimport numpy as np                                                                                    \n                                                                                                      \nperiods = 1000                                                                                        \nind = DatetimeIndex(start=\'2012/1/1\', freq=\'5min\', periods=periods)                                   \ndf = DataFrame({\'high\': np.arange(periods), \'low\': np.arange(periods)}, index=ind)                    \n                                                                                                      \ndef agg_before(hour, func, fix=False):                                                                \n    """"""                                                                                               \n        Run an aggregate func on the subset of data.                                                  \n    """"""                                                                                               \n    def _func(data):                                                                                  \n        d = data.select(lambda x: x.hour < 11).dropna()                                               \n        if fix:                                                                                       \n            data[data.index[0]]                                                                       \n        if len(d) == 0:                                                                               \n            return None                                                                               \n        return func(d)                                                                                \n    return _func                                                                                      \n                                                                                                      \ndef afunc(data):                                                                                      \n    d = data.select(lambda x: x.hour < 11).dropna()                                                   \n    return np.max(d)                                                                                  \n                                                                                                      \ngrouped = df.groupby(lambda x: datetime(x.year, x.month, x.day))                                      \n                                                                                                      \nclosure_bad = grouped.agg({\'high\': agg_before(11, np.max)})                                           \nclosure_good = grouped.agg({\'high\': agg_before(11, np.max, True)})                                    \nlambda_good = grouped.agg({\'high\': afunc})                         \n```\n```\nIn [33]: np.__version__\nOut[39]: \'1.6.2\'\n\nIn [34]: pandas.__version__\nOut[34]: \'0.8.0.dev-dc6ce90\'\n\nIn [35]: closure_bad\nOut[35]: \n            high\n2012-01-01   131\n2012-01-02   NaN\n2012-01-03   NaN\n2012-01-04   NaN\n\nIn [36]: closure_good\nOut[36]: \n            high\n2012-01-01   131\n2012-01-02   419\n2012-01-03   707\n2012-01-04   995\n\nIn [37]: lambda_good\nOut[37]: \n            high\n2012-01-01   131\n2012-01-02   419\n2012-01-03   707\n2012-01-04   995\n```\n\nRunning an agg function that isn\'t a closure works fine. Any ideas on this?'"
1422,4960055,wesm,wesm,2012-06-07 21:41:42,2012-06-11 17:17:08,2012-06-11 17:17:08,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1422,"b'Joins between equal, but not same time zone DatetimeIndex yields non-UTC DatetimeIndex'",
1421,4954411,manuteleco,wesm,2012-06-07 17:01:36,2012-06-11 21:07:40,2012-06-11 21:07:40,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1421,b'Missing rows on DataFrame outer join with MultiIndex',"b'Hi,\n\nI\'m trying to compute an outer join on several columns applied to several DataFrame objects in one step. However, the result I get seems to force the uniqueness on the set of join columns and, as a consequence, some rows are missing.\n\nHere is some example code that shows the output from a join operation over 3 dataframes in one step and a merge operation (in 2 steps) over the same data. Comparing both, we see that the join operation doesn\'t include the row ""1  1  10  100  1000"".\n\n\n```\nfrom pandas import DataFrame, merge\n\ndef multiple_join():\n\tdf1 = DataFrame({""a"": [1,1], ""b"": [1,1], ""c"": [10,20]})\n\tdf2 = DataFrame({""a"": [1,1], ""b"": [1,2], ""d"": [100,200]})\n\tdf3 = DataFrame({""a"": [1,1], ""b"": [1,2], ""e"": [1000,2000]})\n\tdf1.set_index([""a"", ""b""], inplace=True)\n\tdf2.set_index([""a"", ""b""], inplace=True)\n\tdf3.set_index([""a"", ""b""], inplace=True)\n\tdf_joined = df1.join([df2, df3], how=\'outer\')\n\treturn df_joined\n\ndef cascade_merge():\n\tdf1 = DataFrame({""a"": [1,1], ""b"": [1,1], ""c"": [10,20]})\n\tdf2 = DataFrame({""a"": [1,1], ""b"": [1,2], ""d"": [100,200]})\n\tdf3 = DataFrame({""a"": [1,1], ""b"": [1,2], ""e"": [1000,2000]})\n\tdf_partially_merged = merge(df1, df2, on=[\'a\', \'b\'], how=\'outer\')\n\tdf_merged = merge(df_partially_merged, df3, on=[\'a\', \'b\'], how=\'outer\')\n\treturn df_merged\n\nif __name__ == ""__main__"":\n\tprint multiple_join()\n\tprint cascade_merge()\n\n\n\n\n#\t      c    d     e\n#\ta b               \n#\t1 1  20  100  1000\n#\t  2 NaN  200  2000\n#\n#\t   a  b   c    d     e\n#\t0  1  1  10  100  1000\n#\t1  1  1  20  100  1000\n#\t2  1  2 NaN  200  2000\n```\n\n\n\nHowever, this problem doesn\'t seem to arise when we specify ""how={anything other that outer}"" in the join operation.\n\nSo, either this is a bug or I\'m missing something here. In either case, I would appreciate any comment regarding this issue. And, BTW, it would be really could if ""merge"" could accept a list of DataFrames and join them efficiently in one step.\n\nThanks and regards.'"
1412,4926227,grsr,changhiskhan,2012-06-06 10:05:35,2012-06-08 03:50:28,2012-06-08 03:50:28,closed,,0.8.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1412,b'isin method buggy on SparseSeries',"b""The isin method doesn't seem to work correctly for SparseSeries objects, but does work on an equivalent dense object, example code below:\n\n```python\nIn [203]: pandas.__version__\nOut[203]: '0.7.3'\n\nIn [204]: sparse_df = DataFrame({'flag': [1., 0., 1.]}).to_sparse(fill_value=0.)\n\nIn [205]: sparse_df[sparse_df.flag == 1.]\nOut[205]: \n   flag\n0     1\n2     1\n\nIn [206]: sparse_df[sparse_df.flag.isin([1.])]\nOut[206]: \n   flag\n0     1\n1     0\n\nIn [207]: dense_df = DataFrame({'flag': [1., 0., 1.]})\n\nIn [208]: dense_df[dense_df.flag == 1.]\nOut[208]: \n   flag\n0     1\n2     1\n\nIn [209]: dense_df[dense_df.flag.isin([1.])]\nOut[209]: \n   flag\n0     1\n2     1\n```"""
1408,4918501,wesm,wesm,2012-06-05 21:44:17,2012-06-06 21:37:01,2012-06-06 21:37:01,closed,,0.8.0,1,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1408,b'Quarterly plots place year markers under Q2 instead of Q1',
1406,4915840,wesm,wesm,2012-06-05 19:37:23,2012-06-12 01:32:35,2012-06-12 01:32:35,closed,wesm,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1406,"b""Raise exception if group keys and levels don't overlap in concat""",b'If no keys are found in the levels'
1404,4913405,wesm,wesm,2012-06-05 17:40:54,2012-06-05 18:00:38,2012-06-05 18:00:38,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1404,b'Raise exception in comparisons between tz-naive and tz-aware Timestamps',
1401,4911554,ruidc,wesm,2012-06-05 16:02:02,2012-06-12 07:45:52,2012-06-11 22:14:31,closed,,0.8.0,3,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1401,"b'MultiIndex groupby bugs in 0.7.3, 0.8.0b1 and 0.8.0dev'","b""```\nimport pandas\nl = [['count', 'values'], ['to filter', '']]\nmidx = pandas.MultiIndex.from_tuples(l)\ndf = pandas.DataFrame([[1L, 'A']], columns=midx) #one line\nprint(df.groupby('to filter').groups)\n#Out: {'to filter': [0L]} #was expecting 'A': [0L]\nprint(df.groupby([('to filter', '')]).groups)\n#Out: {'to filter': [0L]} #was expecting same as above\ndf = pandas.DataFrame([[1L, 'A'], [2L, 'B']], columns=midx) #two lines, different group\nprint(df.groupby('to filter').groups)\n#Out: {'A': [0L], 'B': [1L]} #fine\nprint(df.groupby([('to filter', '')]).groups)\n#Out: {'': [1L], 'to filter': [0L]} #was expecting same as above\ndf = pandas.DataFrame([[1L, 'A'], [2L, 'A']], columns=midx) #two lines, same group\nprint(df.groupby('to filter').groups)\n#Out: {'A': [0L, 1L]} #fine\nprint(df.groupby([('to filter', '')]).groups)\n#Out: {'': [1L], 'to filter': [0L]} #was expecting same as above\n```"""
1397,4907783,wesm,wesm,2012-06-05 12:56:30,2012-06-05 16:38:04,2012-06-05 16:38:04,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1397,b'UTC localization / conversion broken in Timestamp',"b""```\nIn [5]: ts_orig = Timestamp('3/11/2012 04:00', tz='utc')\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-5-709530384ffe> in <module>()\n----> 1 ts_orig = Timestamp('3/11/2012 04:00', tz='utc')\n\n/home/wesm/code/pandas/pandas/lib.so in pandas.lib.Timestamp.__new__ (pandas/src/tseries.c:29685)()\n\n/home/wesm/code/pandas/pandas/lib.so in pandas.lib.convert_to_tsobject (pandas/src/tseries.c:34455)()\n\nAttributeError: 'NoneType' object has no attribute '_utcoffset'\n```"""
1396,4907398,gerigk,wesm,2012-06-05 12:29:01,2012-06-07 15:27:05,2012-06-07 15:26:45,closed,,0.8.0,1,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/1396,b'New errors in nosetests',"b'```\nERROR: test_index_astype_datetime64 (pandas.tseries.tests.test_timeseries.TestTimeSeries)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/hadoop/pandas/pandas/tseries/tests/test_timeseries.py"", line 402, in test_index_astype_datetime64\n    casted = idx.astype(np.dtype(\'M8[D]\'))\n  File ""/home/hadoop/pandas/pandas/core/index.py"", line 126, in astype\n    return Index(self.values.astype(dtype), name=self.name,\nTypeError: Cannot cast datetime.datetime object from metadata [us] to [D] according to the rule \'same_kind\'\n\n======================================================================\nERROR: test_operators_date (pandas.tests.test_series.TestSeries)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/hadoop/pandas/pandas/tests/test_series.py"", line 1485, in test_operators_date\n    result = self.objSeries + timedelta(1)\n  File ""/home/hadoop/pandas/pandas/core/series.py"", line 85, in wrapper\n    return Series(na_op(self.values, other),\n  File ""/home/hadoop/pandas/pandas/core/series.py"", line 61, in na_op\n    result[mask] = op(x[mask], y)\nTypeError: unsupported operand type(s) for +: \'long\' and \'datetime.timedelta\'\n\n======================================================================\nFAIL: test_from_json_to_json (pandas.tests.test_series.TestSeries)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/hadoop/pandas/pandas/tests/test_series.py"", line 377, in test_from_json_to_json\n    _check_all_orients(objSeries)\n  File ""/home/hadoop/pandas/pandas/tests/test_series.py"", line 357, in _check_all_orients\n    _check_orient(series, ""columns"", dtype=dtype)\n  File ""/home/hadoop/pandas/pandas/tests/test_series.py"", line 350, in _check_orient\n    assert_series_equal(series, unser)\n  File ""/home/hadoop/pandas/pandas/util/testing.py"", line 126, in assert_series_equal\n    assert_almost_equal(left.values, right.values)\n  File ""/home/hadoop/pandas/pandas/util/testing.py"", line 87, in assert_almost_equal\n    assert_almost_equal(a[i], b[i])\n  File ""/home/hadoop/pandas/pandas/util/testing.py"", line 77, in assert_almost_equal\n    assert a == b, (a, b)\nAssertionError: (\'958435200000000000\', 9.584352e+17)\n\n----------------------------------------------------------------------\nRan 2047 tests in 52.125s\n\n```'"
1395,4906924,sadruddin,wesm,2012-06-05 11:52:39,2012-09-18 15:04:48,2012-09-18 15:04:48,closed,,0.9,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1395,b'datetime.date support in 0.8 new timeseries framework',"b""Is is the intention that datetime.date would be supported as a valid input type for some methods? I have observed that it is only partially supported. Example, when using 0.8.0b1:\n\n    # Works\n    pandas.BDay().rollforward(datetime.date(2011, 11, 30))\n    # Doesn't work (returns None)\n    pandas.Day().rollforward(datetime.date(2011, 11, 30)) \n\n    # Works\n    pandas.QuarterEnd().rollback(datetime.date(2011, 11, 30))\n    # Doesn't work (raises Exception, code expected datetime.datetime object)\n    pandas.YearEnd().rollback(datetime.date(2011, 11, 30)) \n"""
1394,4903043,lenolib,wesm,2012-06-05 06:47:28,2012-06-05 15:42:46,2012-06-05 15:41:50,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1394,b'Series.append() excepts with some funky overlapping dates',"b'With pandas v0.8.0b1\n\nIn [90]: u\nOut[90]: \n2012-05-08 01:45:00     86\n2012-05-08 01:50:00    170\n2012-05-08 01:55:00    130\n2012-05-08 02:00:00    206\n2012-05-08 02:05:00     52\n2012-05-08 02:10:00      4\nFreq: 5T\n\nIn [91]: u.append(u)\n\nTraceback (most recent call last):\n  File ""<ipython console>"", line 1, in <module>\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 1566, in append\n    return concat(to_concat, ignore_index=False, verify_integrity=True)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py"", line 836, in concat\n    verify_integrity=verify_integrity)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py"", line 895, in __init__\n    self.new_axes = self._get_new_axes()\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py"", line 1066, in _get_new_axes\n    concat_axis = self._get_concat_axis()\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py"", line 1097, in _get_concat_axis\n    self._maybe_check_integrity(concat_axis)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py"", line 1106, in _maybe_check_integrity\n    % str(overlap))\nException: Indexes have overlapping values: [1970-01-16 225:45:00, 1970-01-16 225:50:00, 1970-01-16 225:55:00, 1970-01-16 226:00:00, 1970-01-16 226:05:00, 1970-01-16 226:10:00]'"
1389,4895514,wesm,wesm,2012-06-04 20:00:26,2012-06-05 00:55:51,2012-06-05 00:55:51,closed,wesm,0.8.0,0,Bug;Enhancement;Testing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1389,b'Test Timestamp adding timedelta pushing over DST boundary',"b""Almost 100% sure this won't do the right thing"""
1387,4894053,wesm,y-p,2012-06-04 18:44:37,2013-07-29 05:02:13,2013-07-29 05:02:13,closed,,0.13,2,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/1387,b'mingw32 build / test failures in ujson',b'cc @Komnomnomnom'
1384,4888727,wesm,wesm,2012-06-04 14:06:12,2012-06-04 20:08:05,2012-06-04 20:08:05,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1384,b'Cast other datetime64 units to nanoseconds in Index constructor',b'Currently being interpreted as nanoseconds instead of being converted\n\ndtypes also seem to be ignored in Index constructor'
1379,4865731,leonbaum,wesm,2012-06-01 21:44:04,2012-06-02 18:48:48,2012-06-02 18:47:55,closed,,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1379,b'Setting datetime64 array as DataFrame column does not convert unit to nanoseconds',"b'Using pandas 0.8 and numpy 1.7,  I get errors when I add columns that are of type datetime64 for everything but very short columns:\n\n```\nIn [105]: t = np.arange(start=\'2012-01-01 00:00\', stop=\'2012-01-02 00:00\', dtype=\'datetime64[h]\')\n\nIn [106]: df = pandas.DataFrame(np.arange(len(t)))\n\nIn [107]: df[\'t\'] = t\n\nIn [108]: df\nOut[108]: \n     0                   t\n0    0 2012-01-01 05:00:00\n1    1 2012-01-01 06:00:00\n2    2 2012-01-01 07:00:00\n3    3 2012-01-01 08:00:00\n4    4 2012-01-01 09:00:00\n5    5 2012-01-01 10:00:00\n6    6 2012-01-01 11:00:00\n7    7 2012-01-01 12:00:00\n8    8 2012-01-01 13:00:00\n9    9 2012-01-01 14:00:00\n10  10 2012-01-01 15:00:00\n11  11 2012-01-01 16:00:00\n12  12 2012-01-01 17:00:00\n13  13 2012-01-01 18:00:00\n14  14 2012-01-01 19:00:00\n15  15 2012-01-01 20:00:00\n16  16 2012-01-01 21:00:00\n17  17 2012-01-01 22:00:00\n18  18 2012-01-01 23:00:00\n19  19 2012-01-02 00:00:00\n20  20 2012-01-02 01:00:00\n21  21 2012-01-02 02:00:00\n22  22 2012-01-02 03:00:00\n23  23 2012-01-02 04:00:00\n\nIn [109]: t = np.arange(start=\'2012-01-01 00:00\', stop=\'2012-01-02 00:00\', dtype=\'datetime64[m]\')\n\nIn [110]: df = pandas.DataFrame(np.arange(len(t)))\n\nIn [111]: df[\'t\'] = t\n\nIn [112]: df\nOut[112]: ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/work/<ipython-input-112-7ed0097d7e9e> in <module>()\n----> 1 df\n\n/usr/local/lib/python3.2/site-packages/ipython-0.12-py3.2.egg/IPython/core/displayhook.py in __call__(self, result)\n    236             self.start_displayhook()\n    237             self.write_output_prompt()\n--> 238             format_dict = self.compute_format_data(result)\n    239             self.write_format_data(format_dict)\n    240             self.update_user_ns(result)\n\n/usr/local/lib/python3.2/site-packages/ipython-0.12-py3.2.egg/IPython/core/displayhook.py in compute_format_data(self, result)\n    148             MIME type representation of the object.\n    149         """"""\n--> 150         return self.shell.display_formatter.format(result)\n    151 \n    152     def write_format_data(self, format_dict):\n\n/usr/local/lib/python3.2/site-packages/ipython-0.12-py3.2.egg/IPython/core/formatters.py in format(self, obj, include, exclude)\n    124                     continue\n    125             try:\n--> 126                 data = formatter(obj)\n    127             except:\n    128                 # FIXME: log the exception\n\n\n/usr/local/lib/python3.2/site-packages/ipython-0.12-py3.2.egg/IPython/core/formatters.py in __call__(self, obj)\n    444                 type_pprinters=self.type_printers,\n    445                 deferred_pprinters=self.deferred_printers)\n--> 446             printer.pretty(obj)\n    447             printer.flush()\n    448             return stream.getvalue()\n\n/usr/local/lib/python3.2/site-packages/ipython-0.12-py3.2.egg/IPython/lib/pretty.py in pretty(self, obj)\n    349             if hasattr(obj_class, \'_repr_pretty_\'):\n    350                 return obj_class._repr_pretty_(obj, self, cycle)\n--> 351             return _default_pprint(obj, self, cycle)\n    352         finally:\n    353             self.end_group()\n\n/usr/local/lib/python3.2/site-packages/ipython-0.12-py3.2.egg/IPython/lib/pretty.py in _default_pprint(obj, p, cycle)\n    469     if getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\n    470         # A user-provided repr.\n\n--> 471         p.text(repr(obj))\n    472         return\n    473     p.begin_group(1, \'<\')\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/frame.py in __repr__(self)\n    570         buf = StringIO()\n    571         if self._need_info_repr_():\n--> 572             self.info(buf=buf, verbose=self._verbose_info)\n    573         else:\n    574             self.to_string(buf=buf)\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/frame.py in info(self, verbose, buf)\n   1343             lines.append(\'Data columns:\')\n   1344             space = max([len(_stringify(k)) for k in self.columns]) + 4\n-> 1345             counts = self.count()\n   1346             assert(len(cols) == len(counts))\n   1347             for col, count in counts.items():\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/frame.py in count(self, axis, level, numeric_only)\n   3997                 result = Series(counts, index=frame._get_agg_axis(axis))\n   3998             else:\n-> 3999                 result = DataFrame.apply(frame, Series.count, axis=axis)\n   4000 \n   4001         return result\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/frame.py in apply(self, func, axis, broadcast, raw, args, **kwds)\n   3537                     return self._apply_raw(f, axis)\n   3538                 else:\n-> 3539                     return self._apply_standard(f, axis)\n   3540             else:\n   3541                 return self._apply_broadcast(f, axis)\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/frame.py in _apply_standard(self, func, axis, ignore_failures)\n   3592             try:\n   3593                 for k, v in series_gen:\n-> 3594                     results[k] = func(v)\n   3595             except Exception as e:\n   3596                 try:\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/series.py in count(self, level)\n   1072             return Series(counts, index=level_index)\n   1073 \n-> 1074         return notnull(self.values).sum()\n   1075 \n   1076     def value_counts(self):\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/common.py in notnull(obj)\n     93     boolean ndarray or boolean\n     94     \'\'\'\n---> 95     res = isnull(obj)\n     96     if np.isscalar(res):\n     97         return not res\n\n/usr/local/lib/python3.2/site-packages/pandas-0.8.0.dev-py3.2-linux-x86_64.egg/pandas/core/common.py in isnull(obj)\n     72             result = np.array(obj).view(\'i8\') == lib.iNaT\n     73         else:\n---> 74             result = -np.isfinite(obj)\n     75         return result\n     76     elif isinstance(obj, PandasObject):\n\nTypeError: (""ufunc \'isfinite\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'"", \'occurred at index t\')\n\n```'"
1375,4857639,changhiskhan,changhiskhan,2012-06-01 14:10:13,2012-06-01 16:03:49,2012-06-01 16:01:43,closed,changhiskhan,0.8.0,2,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/1375,b'to_sparse bug with fill_value specified',"b""originally raised on pydata mailing list:\n\nIn [50]: DataFrame({'x': [1., 1.]}).to_sparse(fill_value=0).x.mean()\nOut[50]: 0.5\n\nIn [51]: DataFrame({'x': [1., 1.]}).to_sparse().x.mean()\nOut[51]: 1.0\n\n"""
1374,4856555,lodagro,wesm,2012-06-01 13:17:57,2012-06-03 17:11:58,2012-06-03 17:11:15,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1374,b'What to do with irow in case of duplicate index?',"b""Since irow translates the integer index to a label one and then retrieves the row, in case of duplicate index multiple rows are returned.\n\n```python\nIn [34]: df = pandas.DataFrame(np.random.rand(3,3), columns=list('ABC'), index=list('aab'))\n\nIn [35]: df\nOut[35]:\n          A         B         C\na  0.031057  0.528825  0.694592\na  0.705572  0.122005  0.754107\nb  0.475423  0.048636  0.224495\n\nIn [36]: df.irow(0)\nOut[36]:\n          A         B         C\na  0.031057  0.528825  0.694592\na  0.705572  0.122005  0.754107\n\nIn [37]: df.irow(1)\nOut[37]:\n          A         B         C\na  0.031057  0.528825  0.694592\na  0.705572  0.122005  0.754107\n\nIn [38]: df.irow(2)\nOut[38]:\nA    0.475423\nB    0.048636\nC    0.224495\nName: b\n\n```"""
1373,4856442,lodagro,lodagro,2012-06-01 13:11:34,2012-06-07 10:24:52,2012-06-07 10:24:52,closed,,0.8.0,1,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1373,b'Allow duplicate index values for to_csv',"b""```python\nIn [22]: df\nOut[22]:\n              0         1         2\nA 1 2  0.659855  0.340490  0.351028\n    2  0.695581  0.264714  0.389929\nB   2  0.105016  0.070774  0.114629\n\nIn [23]: df.to_csv('foo.csv')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\nTypeError: only integer arrays with one element can be converted to an index\n\nIn [24]: df1\nOut[24]:\n          0         1         2\na  0.313015  0.631747  0.091818\na  0.505238  0.260553  0.793378\nb  0.441791  0.029851  0.139490\n\nIn [25]: df1.to_csv('foo.csv')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n```"""
1366,4843015,JWCornV,wesm,2012-05-31 18:33:28,2012-06-02 21:31:21,2012-06-02 21:31:02,closed,,0.8.0,2,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1366,b'Cannot index a dataframe with a boolean dataframe',"b""Sorry if there is a specific reason why this is not supported, but with boolean dataframe indexing, I would expect the following to work:\n\nhttp://pandas.pydata.org/pandas-docs/dev/indexing.html#indexing-a-dataframe-with-a-boolean-dataframe\n\n```\n\nIn [36]: df_rand = pd.DataFrame(np.random.randn(10,10))\n\nIn [37]: df_rand < 0\nOut[37]:\n       0      1      2      3      4      5      6      7      8      9\n0  False  False   True  False   True  False   True  False   True   True\n1  False   True   True  False   True  False  False   True  False   True\n2   True   True   True  False   True  False  False   True  False  False\n3  False  False   True  False   True   True   True  False  False   True\n4   True  False  False   True  False  False   True  False  False  False\n5  False  False  False  False  False  False  False  False  False  False\n6  False   True  False  False   True   True   True  False  False   True\n7  False  False  False   True  False  False   True  False   True  False\n8   True   True   True  False   True   True   True  False  False  False\n9  False  False   True   True   True  False   True   True   True  False\n\nIn [38]: df_rand[df_rand < 0] = -0.1\n\nIn [39]: df_rand[df_rand < 0] += 0.1\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nC:\\Python27\\Scripts\\<ipython-input-39-6dc58d9235e9> in <module>()\n----> 1 df_rand[df_rand < 0] += 0.1\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __getitem__(self, key)\n   1655             return self._getitem_multilevel(key)\n   1656         else:\n-> 1657             return self._get_item_cache(key)\n   1658\n   1659     def _getitem_array(self, key):\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in _get_item_cache(self, item)\n    479                 from pandas.core.frame import DataFrame\n    480                 if isinstance(item, DataFrame):\n--> 481                     raise ValueError('Cannot index using (boolean) dataframe')\n    482                 raise\n    483\n\nValueError: Cannot index using (boolean) dataframe\n\nIn [40]: print pd.__version__\n0.8.0b1\n```\n"""
1365,4839733,wesm,wesm,2012-05-31 15:53:09,2012-06-02 19:44:02,2012-06-02 19:44:02,closed,wesm,0.8.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1365,b'Column selection not honored in GroupBy.transform',
1364,4839709,wesm,wesm,2012-05-31 15:52:00,2012-06-02 19:55:10,2012-06-02 19:55:10,closed,wesm,0.8.0,0,Bug;Enhancement;Groupby,https://api.github.com/repos/pydata/pandas/issues/1364,b'Should GroupBy.transform exclude nuisance columns?',"b""```\nIn [46]: print df; df.groupby('key1').transform(lambda x: x.mean())\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/wesm/Dropbox/book/svn/book_scripts/notebooks/<ipython-input-46-656cf72f4323> in <module>()\n      1 print df\n----> 2 df.groupby('key1').transform(lambda x: x.mean())\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in transform(self, func, *args, **kwargs)\n   1639                     group.T.values[:] = res\n   1640                 else:\n-> 1641                     group.values[:] = res\n   1642 \n   1643                 applied.append(group)\n\nValueError: operands could not be broadcast together with shapes (3,4) (2) \n\n      data1     data2 key1 key2\n0  0.984916  0.904550    a  one\n1 -1.879638  0.585887    a  two\n2  0.457879 -0.721776    b  one\n3  1.585887  0.050336    b  two\n4 -0.701547  0.525808    a  one\n```"""
1361,4835411,gerigk,wesm,2012-05-31 12:24:07,2014-12-28 20:08:39,2012-06-03 14:05:21,closed,,0.8.0,9,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/1361,b'Error and failure in nosetests (Ubuntu 12.04)',"b'```\n======================================================================\nERROR: test_map (pandas.tseries.tests.test_period.TestPeriodIndex)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/arthur/python-packages/pandas/pandas/tseries/tests/test_period.py"", line 1523, in test_map\n    assert_array_equal(result, expected)\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 753, in assert_array_equal\n    verbose=verbose, header=\'Arrays are not equal\')\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 612, in assert_array_compare\n    x_isnan[x_isna] = False\n  File ""/home/arthur/python-packages/pandas/pandas/core/index.py"", line 282, in __setitem__\n    raise Exception(str(self.__class__) + \' object is immutable\')\nException: <class \'pandas.tseries.period.PeriodIndex\'> object is immutable\n\n======================================================================\nFAIL: test_unicode_index (pandas.io.tests.test_pytables.TestHDFStore)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/home/arthur/python-packages/pandas/pandas/io/tests/test_pytables.py"", line 661, in test_unicode_index\n    self._check_roundtrip(s, tm.assert_series_equal)\n  File ""/home/arthur/python-packages/pandas/pandas/io/tests/test_pytables.py"", line 566, in _check_roundtrip\n    comparator(retrieved, obj, **kwargs)\n  File ""/home/arthur/python-packages/pandas/pandas/util/testing.py"", line 129, in assert_series_equal\n    assert(left.index.equals(right.index))\nAssertionError\n```'"
1357,4827805,wesm,changhiskhan,2012-05-31 00:22:16,2012-06-01 20:53:24,2012-06-01 20:53:24,closed,,0.8.0,0,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1357,b'Plotting broken if inferred_freq has a multiple',"b'Also reported on mailing list\n\n```\nIn [4]: ts.plot()\nOut[4]: <matplotlib.axes.AxesSubplot at 0x41cc310>\n\nIn [5]: ts.index.fr\nts.index.freq     ts.index.freqstr  \n\nIn [5]: ts.index.freq\nOut[5]: <10 Days>\n\nIn [6]: ts.index.freq = None\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/home/wesm/Dropbox/book/svn/<ipython-input-6-d89557d7f8e5> in <module>()\n----> 1 ts.index.freq = None\n\nAttributeError: can\'t set attribute\n\nIn [7]: ts.index.offset = None\n\nIn [8]: ts\nOut[8]: \n2000-01-01    2.963378\n2000-01-11   -0.685998\n2000-01-21    0.392812\n2000-01-31   -0.143893\n2000-02-10   -1.162283\n2000-02-20   -0.460511\n2000-03-01   -0.751741\n2000-03-11   -0.567428\n2000-03-21   -0.439105\n2000-03-31    0.040322\n2000-04-10   -0.929786\n2000-04-20   -0.620005\n2000-04-30    0.482502\n2000-05-10    0.109013\n2000-05-20    0.657817\n2000-05-30   -0.668386\n2000-06-09   -1.797074\n2000-06-19   -0.197738\n2000-06-29   -0.312481\n2000-07-09    1.240477\n2000-07-19    0.447583\n2000-07-29    0.188429\n2000-08-08   -0.117540\n2000-08-18   -1.145495\n2000-08-28    0.479124\n2000-09-07    0.309968\n2000-09-17    0.769928\n2000-09-27   -0.501629\n2000-10-07    0.164322\n2000-10-17   -0.554113\n2000-10-27    0.468588\n2000-11-06    0.857458\n2000-11-16    0.508256\n2000-11-26   -0.452384\n2000-12-06    1.732538\n2000-12-16    0.695495\n2000-12-26    0.441233\n2001-01-05   -0.271561\n\nIn [9]: ts.index.offset = None\n\nIn [10]: ts.plot()\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/wesm/Dropbox/book/svn/<ipython-input-10-132f3667ee95> in <module>()\n----> 1 ts.plot()\n\n/home/wesm/code/pandas/pandas/tools/plotting.pyc in plot_series(series, label, kind, use_index, rot, xticks, yticks, xlim, ylim, ax, style, grid, logy, **kwds)\n    758                      legend=False, grid=grid, label=label, **kwds)\n    759 \n--> 760     plot_obj.generate()\n    761     plot_obj.draw()\n    762 \n\n/home/wesm/code/pandas/pandas/tools/plotting.pyc in generate(self)\n    239         self._compute_plot_data()\n    240         self._setup_subplots()\n--> 241         self._make_plot()\n    242         self._post_plot_logic()\n    243         self._adorn_subplots()\n\n/home/wesm/code/pandas/pandas/tools/plotting.pyc in _make_plot(self)\n    426         if self.use_index and self.has_ts_index:\n    427             data = self._maybe_convert_index(self.data)\n--> 428             self._make_ts_plot(data)\n    429         else:\n    430             x = self._get_xticks()\n\n/home/wesm/code/pandas/pandas/tools/plotting.pyc in _make_ts_plot(self, data, **kwargs)\n    478 \n    479             label = com._stringify(self.label)\n--> 480             tsplot(data, plotf, ax=ax, label=label, **kwargs)\n    481             ax.grid(self.grid)\n    482         else:\n\n/home/wesm/code/pandas/pandas/tseries/plotting.pyc in tsplot(series, plotf, *args, **kwargs)\n    114     # format args and lot\n    115     args = _check_plot_params(series, series.index, freq, *args)\n--> 116     plotted = plotf(ax, *args,  **kwargs)\n    117 \n    118     format_dateaxis(ax, ax.freq)\n\n/home/wesm/code/repos/matplotlib/lib/matplotlib/axes.pyc in plot(self, *args, **kwargs)\n   3892         lines = []\n   3893 \n-> 3894         for line in self._get_lines(*args, **kwargs):\n   3895             self.add_line(line)\n   3896             lines.append(line)\n\n/home/wesm/code/repos/matplotlib/lib/matplotlib/axes.pyc in _grab_next_args(self, *args, **kwargs)\n    321                 return\n    322             if len(remaining) <= 3:\n--> 323                 for seg in self._plot_args(remaining, kwargs):\n    324                     yield seg\n    325                 return\n\n/home/wesm/code/repos/matplotlib/lib/matplotlib/axes.pyc in _plot_args(self, tup, kwargs)\n    299             x = np.arange(y.shape[0], dtype=float)\n    300 \n--> 301         x, y = self._xy_from_xy(x, y)\n    302 \n    303         if self.command == \'plot\':\n\n/home/wesm/code/repos/matplotlib/lib/matplotlib/axes.pyc in _xy_from_xy(self, x, y)\n    215     def _xy_from_xy(self, x, y):\n    216         if self.axes.xaxis is not None and self.axes.yaxis is not None:\n--> 217             bx = self.axes.xaxis.update_units(x)\n    218             by = self.axes.yaxis.update_units(y)\n    219 \n\n/home/wesm/code/repos/matplotlib/lib/matplotlib/axis.pyc in update_units(self, data)\n   1276         """"""\n   1277 \n-> 1278         converter = munits.registry.get_converter(data)\n   1279         if converter is None:\n   1280             return False\n\n/home/wesm/code/repos/matplotlib/lib/matplotlib/units.pyc in get_converter(self, x)\n    131 \n    132         if converter is None and iterable(x):\n--> 133             for thisx in x:\n    134                 # Make sure that recursing might actually lead to a solution, if\n    135                 # we are just going to re-examine another item of the same kind,\n\n/home/wesm/code/pandas/pandas/tseries/period.pyc in __iter__(self)\n    685     def __iter__(self):\n    686         for val in self.values:\n--> 687             yield Period(ordinal=val, freq=self.freq)\n    688 \n    689     @property\n\n/home/wesm/code/pandas/pandas/tseries/period.pyc in __init__(self, value, freq, ordinal, year, month, quarter, day, hour, minute, second)\n    136         base, mult = _gfc(freq)\n    137         if mult != 1:\n--> 138             raise ValueError(\'Only mult == 1 supported\')\n    139 \n    140         if self.ordinal is None:\n\nValueError: Only mult == 1 supported\n```'"
1353,4824224,wesm,wesm,2012-05-30 20:44:33,2012-06-03 15:19:54,2012-06-03 15:19:54,closed,wesm,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1353,b'pandas.unique should return ndarray',
1352,4822253,adamklein,wesm,2012-05-30 19:05:27,2012-06-03 15:57:36,2012-06-03 15:56:58,closed,wesm,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1352,b'MultiIndex partial slicing error',"b'\tIn [10]: idx = MultiIndex(labels=[[0,0,0],[0,1,1],[1,0,1]],levels=[[\'a\',\'b\'],[\'x\',\'y\'],[\'p\',\'q\']])\n\n\tIn [11]: df = DataFrame(np.random.rand(3,2),index=idx)\n\n\tIn [12]: df.ix[(\'a\',\'y\')]\n\tOut[12]: \n\t\t  0         1\n\tp  0.788191  0.781931\n\tq  0.566332  0.486645\n\n\tIn [13]: df[(\'a\',\'y\')]\n\t---------------------------------------------------------------------------\n\tKeyError                                  Traceback (most recent call last)\n\t/home/adam/code/qplib/<ipython-input-13-d3c63a882cb1> in <module>()\n\t----> 1 df[(\'a\',\'y\')]\n\n\t/home/adam/code/pandas/pandas/core/frame.pyc in __getitem__(self, key)\n\t   1655             return self._getitem_multilevel(key)\n\t   1656         else:\n\t-> 1657             return self._get_item_cache(key)\n\t   1658 \n\t   1659     def _getitem_array(self, key):\n\n\t/home/adam/code/pandas/pandas/core/generic.pyc in _get_item_cache(self, item)\n\t    472         except Exception:\n\t    473             try:\n\t--> 474                 values = self._data.get(item)\n\t    475                 res = self._box_item_values(item, values)\n\t    476                 cache[item] = res\n\n\t/home/adam/code/pandas/pandas/core/internals.pyc in get(self, item)\n\t    756 \n\t    757     def get(self, item):\n\t--> 758         _, block = self._find_block(item)\n\t    759         return block.get(item)\n\t    760 \n\n\t/home/adam/code/pandas/pandas/core/internals.pyc in _find_block(self, item)\n\t    846 \n\t    847     def _find_block(self, item):\n\t--> 848         self._check_have(item)\n\t    849         for i, block in enumerate(self.blocks):\n\t    850             if item in block:\n\n\t/home/adam/code/pandas/pandas/core/internals.pyc in _check_have(self, item)\n\t    853     def _check_have(self, item):\n\t    854         if item not in self.items:\n\t--> 855             raise KeyError(\'no item named %s\' % str(item))\n\t    856 \n\t    857     def reindex_axis(self, new_axis, method=None, axis=0, copy=True):\n\n\tKeyError: ""no item named (\'a\', \'y\')""\n\n\tIn [14]: df.ix[(\'a\',\'y\'),:]\n\t---------------------------------------------------------------------------\n\tKeyError                                  Traceback (most recent call last)\n\t/home/adam/code/qplib/<ipython-input-14-19a9d60d6a3b> in <module>()\n\t----> 1 df.ix[(\'a\',\'y\'),:]\n\n\t/home/adam/code/pandas/pandas/core/indexing.pyc in __getitem__(self, key)\n\t     31                 pass\n\t/home/adam/code/pandas/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\n\t     96     def _getitem_tuple(self, tup):\n\t     97         try:\n\t---> 98             return self._getitem_lowerdim(tup)\n\t     99         except IndexingError:\n\t    100             pass\n\n\t/home/adam/code/pandas/pandas/core/indexing.pyc in _getitem_lowerdim(self, tup)\n\t    165         if isinstance(ax0, MultiIndex):\n\t    166             try:\n\t--> 167                 return self._get_label(tup, axis=0)\n\t    168             except TypeError:\n\t    169                 # slices are unhashable\n\n\n\t/home/adam/code/pandas/pandas/core/indexing.pyc in _get_label(self, label, axis)\n\t     39             return self.obj.xs(label, axis=axis, copy=False)\n\t     40         except Exception:\n\t---> 41             return self.obj.xs(label, axis=axis, copy=True)\n\t     42 \n\t     43     def _slice(self, obj, axis=0):\n\n\t/home/adam/code/pandas/pandas/core/frame.pyc in xs(self, key, axis, level, copy)\n\t   1894         index = self.index\n\t   1895         if isinstance(index, MultiIndex):\n\t-> 1896             loc, new_index = self.index.get_loc_level(key)\n\t   1897         else:\n\t   1898             loc = self.index.get_loc(key)\n\n\t/home/adam/code/pandas/pandas/core/index.pyc in get_loc_level(self, key, level)\n\t   2000                 for i, k in enumerate(key):\n\t   2001                     if not isinstance(k, slice):\n\t-> 2002                         k = self._get_level_indexer(k, level=i)\n\t   2003                         if isinstance(k, slice):\n\t   2004                             # everything\n\n\n\t/home/adam/code/pandas/pandas/core/index.pyc in _get_level_indexer(self, key, level)\n\t   2030     def _get_level_indexer(self, key, level=0):\n\t   2031         level_index = self.levels[level]\n\t-> 2032         loc = level_index.get_loc(key)\n\t   2033         labels = self.labels[level]\n\t   2034 \n\n\t/home/adam/code/pandas/pandas/core/index.pyc in get_loc(self, key)\n\t    623         loc : int\n\t    624         """"""\n\t--> 625         return self._engine.get_loc(key)\n\t    626 \n\t    627     def get_value(self, series, key):\n\n\t/home/adam/code/pandas/pandas/lib.so in pandas.lib.IndexEngine.get_loc (pandas/src/tseries.c:112968)()\n\n\t/home/adam/code/pandas/pandas/lib.so in pandas.lib.IndexEngine.get_loc (pandas/src/tseries.c:112835)()\n\n\t/home/adam/code/pandas/pandas/lib.so in pandas.lib.PyObjectHashTable.get_item (pandas/src/tseries.c:22698)()\n\n\t/home/adam/code/pandas/pandas/lib.so in pandas.lib.PyObjectHashTable.get_item (pandas/src/tseries.c:22652)()\n\n\tKeyError: (\'a\', \'y\')\n'"
1351,4818566,gerigk,wesm,2012-05-30 15:55:36,2012-06-02 23:17:57,2012-06-02 23:17:57,closed,,0.8.0,4,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/1351,"b'Segfault in test_conv_annual (test_period.py , when running nosetests)'","b'This happened with the current Pandas on an EC2 machine (Amazon Linux/ Debian squeeze).\n\nI attached the gdb trace. Unfortunately I don\'t understand what\'s going on but maybe somebody else does.\n\n```\n(gdb) run test_period.py\nStarting program: /usr/local/bin/python test_period.py\n[Thread debugging using libthread_db enabled]\nnose.config: INFO: Ignoring files matching [\'^\\\\.\', \'^_\', \'^setup\\\\.py$\']\ntest_conv_annual (__main__.TestFreqConversion) ... \nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff6dde819 in free () from /lib/libc.so.6\n(gdb) where\n#0  0x00007ffff6dde819 in free () from /lib/libc.so.6\n#1  0x00007ffff28659ee in skts_strftime (ordinal=148, freq=2000, \n    args=0x1f97300) at pandas/src/period.c:1234\n#2  0x00007ffff2865fc1 in period_to_string (value=148, freq=2000)\n    at pandas/src/period.c:1333\n#3  0x00007ffff26e2395 in __pyx_pf_6pandas_3lib_120period_ordinal_to_string (\n    __pyx_self=0x0, __pyx_v_value=148, __pyx_v_freq=2000)\n    at pandas/src/tseries.c:48155\n#4  0x00007ffff26e2339 in __pyx_pw_6pandas_3lib_121period_ordinal_to_string (\n    __pyx_self=0x0, __pyx_args=0x1f599c0, __pyx_kwds=0x0)\n    at pandas/src/tseries.c:48123\n#5  0x00007ffff7a232d9 in PyCFunction_Call (func=0x15c7d08, arg=0x1f599c0, \n    kw=0x0) at Objects/methodobject.c:85\n#6  0x00007ffff7aca504 in call_function (pp_stack=0x7fffffff6420, oparg=2)\n    at Python/ceval.c:4021\n#7  0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x2078090, throwflag=0)\n    at Python/ceval.c:2666\n#8  0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1648ca0, globals=0x16e6ba0, \n    locals=0x0, args=0x1f97248, argcount=1, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n#9  0x00007ffff79ff7a1 in function_call (func=0x137e990, arg=0x1f97220, kw=0x0)\n    at Objects/funcobject.c:526\n#10 0x00007ffff79c0453 in PyObject_Call (func=0x137e990, arg=0x1f97220, kw=0x0)\n---Type <return> to continue, or q <return> to quit---\n    at Objects/abstract.c:2529\n#11 0x00007ffff79dc87c in instancemethod_call (func=0x137e990, arg=0x1f97220, \n    kw=0x0) at Objects/classobject.c:2578\n#12 0x00007ffff79c0453 in PyObject_Call (func=0x1f95860, arg=0x7ffff7fa7060, \n    kw=0x0) at Objects/abstract.c:2529\n#13 0x00007ffff7ac9bf5 in PyEval_CallObjectWithKeywords (func=0x1f95860, \n    arg=0x7ffff7fa7060, kw=0x0) at Python/ceval.c:3890\n#14 0x00007ffff7a605ad in slot_tp_repr (self=0x2038300)\n    at Objects/typeobject.c:5326\n#15 0x00007ffff7a24f82 in PyObject_Repr (v=0x2038300) at Objects/object.c:381\n#16 0x00007ffff7ab90d8 in builtin_repr (self=0x0, v=0x2038300)\n    at Python/bltinmodule.c:2139\n#17 0x00007ffff7aca2c2 in call_function (pp_stack=0x7fffffff6d20, oparg=1)\n    at Python/ceval.c:4009\n#18 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x2077e40, throwflag=0)\n    at Python/ceval.c:2666\n#19 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0xa991a0, globals=0xadced0, \n    locals=0x0, args=0x2077d90, argcount=2, kws=0x2077da0, kwcount=0, \n    defs=0xaa0d08, defcount=3, closure=0x0) at Python/ceval.c:3253\n#20 0x00007ffff7acab78 in fast_function (func=0xaaaed0, \n    pp_stack=0x7fffffff7240, n=2, na=2, nk=0) at Python/ceval.c:4117\n#21 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffff7240, oparg=2)\n    at Python/ceval.c:4042\n---Type <return> to continue, or q <return> to quit---\n#22 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x2077bc0, throwflag=0)\n    at Python/ceval.c:2666\n#23 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0xf0b7d0, globals=0xd26c20, \n    locals=0x0, args=0x2036308, argcount=2, kws=0x2036318, kwcount=0, \n    defs=0xf182b8, defcount=1, closure=0x0) at Python/ceval.c:3253\n#24 0x00007ffff7acab78 in fast_function (func=0xf163a8, \n    pp_stack=0x7fffffff7760, n=2, na=2, nk=0) at Python/ceval.c:4117\n#25 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffff7760, oparg=2)\n    at Python/ceval.c:4042\n#26 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x20360a0, throwflag=0)\n    at Python/ceval.c:2666\n#27 0x00007ffff7acaa47 in fast_function (func=0x19c54f8, \n    pp_stack=0x7fffffff7b00, n=1, na=1, nk=0) at Python/ceval.c:4107\n#28 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffff7b00, oparg=0)\n    at Python/ceval.c:4042\n#29 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x20340e0, throwflag=0)\n    at Python/ceval.c:2666\n#30 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0xfd2670, globals=0xfc2110, \n    locals=0x0, args=0x1f553d0, argcount=2, kws=0x7ffff7fa7088, kwcount=0, \n    defs=0xfff638, defcount=1, closure=0x0) at Python/ceval.c:3253\n#31 0x00007ffff79ff7a1 in function_call (func=0x1006798, arg=0x1f553a8, \n    kw=0x2033830) at Objects/funcobject.c:526\n#32 0x00007ffff79c0453 in PyObject_Call (func=0x1006798, arg=0x1f553a8, \n---Type <return> to continue, or q <return> to quit---\n    kw=0x2033830) at Objects/abstract.c:2529\n#33 0x00007ffff7acbaf6 in ext_do_call (func=0x1006798, \n    pp_stack=0x7fffffff8038, flags=3, na=1, nk=0) at Python/ceval.c:4334\n#34 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x2033ed0, throwflag=0)\n    at Python/ceval.c:2705\n#35 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0xfd2880, globals=0xfc2110, \n    locals=0x0, args=0x1f50d30, argcount=2, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n#36 0x00007ffff79ff7a1 in function_call (func=0x10068e8, arg=0x1f50d08, kw=0x0)\n    at Objects/funcobject.c:526\n#37 0x00007ffff79c0453 in PyObject_Call (func=0x10068e8, arg=0x1f50d08, kw=0x0)\n    at Objects/abstract.c:2529\n#38 0x00007ffff79dc87c in instancemethod_call (func=0x10068e8, arg=0x1f50d08, \n    kw=0x0) at Objects/classobject.c:2578\n#39 0x00007ffff79c0453 in PyObject_Call (func=0x1d50560, arg=0x1f974c0, kw=0x0)\n    at Objects/abstract.c:2529\n#40 0x00007ffff7a60a91 in slot_tp_call (self=0x1f97c30, args=0x1f974c0, \n    kwds=0x0) at Objects/typeobject.c:5403\n#41 0x00007ffff79c0453 in PyObject_Call (func=0x1f97c30, arg=0x1f974c0, kw=0x0)\n    at Objects/abstract.c:2529\n#42 0x00007ffff7acb42b in do_call (func=0x1f97c30, pp_stack=0x7fffffff8970, \n    na=1, nk=0) at Python/ceval.c:4239\n#43 0x00007ffff7aca726 in call_function (pp_stack=0x7fffffff8970, oparg=1)\n---Type <return> to continue, or q <return> to quit---\n    at Python/ceval.c:4044\n#44 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x2033b70, throwflag=0)\n    at Python/ceval.c:2666\n#45 0x00007ffff7acaa47 in fast_function (func=0x1ce8450, \n    pp_stack=0x7fffffff8d10, n=2, na=2, nk=0) at Python/ceval.c:4107\n#46 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffff8d10, oparg=1)\n    at Python/ceval.c:4042\n#47 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x2032d10, throwflag=0)\n    at Python/ceval.c:2666\n#48 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1ce00f0, globals=0x1d15470, \n    locals=0x0, args=0x1d3c6a0, argcount=2, kws=0x7ffff7fa7088, kwcount=0, \n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3253\n#49 0x00007ffff79ff7a1 in function_call (func=0x1ce83a8, arg=0x1d3c678, \n    kw=0x2032230) at Objects/funcobject.c:526\n#50 0x00007ffff79c0453 in PyObject_Call (func=0x1ce83a8, arg=0x1d3c678, \n    kw=0x2032230) at Objects/abstract.c:2529\n#51 0x00007ffff7acbaf6 in ext_do_call (func=0x1ce83a8, \n    pp_stack=0x7fffffff9248, flags=3, na=1, nk=0) at Python/ceval.c:4334\n#52 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x2032b00, throwflag=0)\n    at Python/ceval.c:2705\n#53 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1ccfa90, globals=0x1d15470, \n    locals=0x0, args=0x1f762e0, argcount=2, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n---Type <return> to continue, or q <return> to quit---\n#54 0x00007ffff79ff7a1 in function_call (func=0x1cd6d80, arg=0x1f762b8, kw=0x0)\n    at Objects/funcobject.c:526\n#55 0x00007ffff79c0453 in PyObject_Call (func=0x1cd6d80, arg=0x1f762b8, kw=0x0)\n    at Objects/abstract.c:2529\n#56 0x00007ffff79dc87c in instancemethod_call (func=0x1cd6d80, arg=0x1f762b8, \n    kw=0x0) at Objects/classobject.c:2578\n#57 0x00007ffff79c0453 in PyObject_Call (func=0x1f86960, arg=0x1f975a0, kw=0x0)\n    at Objects/abstract.c:2529\n#58 0x00007ffff7a60a91 in slot_tp_call (self=0x1f97e60, args=0x1f975a0, \n    kwds=0x0) at Objects/typeobject.c:5403\n#59 0x00007ffff79c0453 in PyObject_Call (func=0x1f97e60, arg=0x1f975a0, kw=0x0)\n    at Objects/abstract.c:2529\n#60 0x00007ffff7acb42b in do_call (func=0x1f97e60, pp_stack=0x7fffffff9b80, \n    na=1, nk=0) at Python/ceval.c:4239\n#61 0x00007ffff7aca726 in call_function (pp_stack=0x7fffffff9b80, oparg=1)\n    at Python/ceval.c:4044\n#62 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x2031c70, throwflag=0)\n    at Python/ceval.c:2666\n#63 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1cefbf0, globals=0x1d16290, \n    locals=0x0, args=0x1f76100, argcount=2, kws=0x7ffff7fa7088, kwcount=0, \n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3253\n#64 0x00007ffff79ff7a1 in function_call (func=0x1d27ed0, arg=0x1f760d8, \n    kw=0x20308e0) at Objects/funcobject.c:526\n---Type <return> to continue, or q <return> to quit---\n#65 0x00007ffff79c0453 in PyObject_Call (func=0x1d27ed0, arg=0x1f760d8, \n    kw=0x20308e0) at Objects/abstract.c:2529\n#66 0x00007ffff7acbaf6 in ext_do_call (func=0x1d27ed0, \n    pp_stack=0x7fffffffa0b8, flags=3, na=1, nk=0) at Python/ceval.c:4334\n#67 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x2031a60, throwflag=0)\n    at Python/ceval.c:2705\n#68 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1cef930, globals=0x1d16290, \n    locals=0x0, args=0x1f76358, argcount=2, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n#69 0x00007ffff79ff7a1 in function_call (func=0x1d27cd8, arg=0x1f76330, kw=0x0)\n    at Objects/funcobject.c:526\n#70 0x00007ffff79c0453 in PyObject_Call (func=0x1d27cd8, arg=0x1f76330, kw=0x0)\n    at Objects/abstract.c:2529\n#71 0x00007ffff79dc87c in instancemethod_call (func=0x1d27cd8, arg=0x1f76330, \n    kw=0x0) at Objects/classobject.c:2578\n#72 0x00007ffff79c0453 in PyObject_Call (func=0x1f86560, arg=0x1f976f0, kw=0x0)\n    at Objects/abstract.c:2529\n#73 0x00007ffff7a60a91 in slot_tp_call (self=0x1f97ca0, args=0x1f976f0, \n    kwds=0x0) at Objects/typeobject.c:5403\n#74 0x00007ffff79c0453 in PyObject_Call (func=0x1f97ca0, arg=0x1f976f0, kw=0x0)\n    at Objects/abstract.c:2529\n#75 0x00007ffff7acb42b in do_call (func=0x1f97ca0, pp_stack=0x7fffffffa9f0, \n    na=1, nk=0) at Python/ceval.c:4239\n---Type <return> to continue, or q <return> to quit---\n#76 0x00007ffff7aca726 in call_function (pp_stack=0x7fffffffa9f0, oparg=1)\n    at Python/ceval.c:4044\n#77 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x20312a0, throwflag=0)\n    at Python/ceval.c:2666\n#78 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1cefbf0, globals=0x1d16290, \n    locals=0x0, args=0x1f76268, argcount=2, kws=0x7ffff7fa7088, kwcount=0, \n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3253\n#79 0x00007ffff79ff7a1 in function_call (func=0x1d27ed0, arg=0x1f76240, \n    kw=0x2030790) at Objects/funcobject.c:526\n#80 0x00007ffff79c0453 in PyObject_Call (func=0x1d27ed0, arg=0x1f76240, \n    kw=0x2030790) at Objects/abstract.c:2529\n#81 0x00007ffff7acbaf6 in ext_do_call (func=0x1d27ed0, \n    pp_stack=0x7fffffffaf28, flags=3, na=1, nk=0) at Python/ceval.c:4334\n#82 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x2031090, throwflag=0)\n    at Python/ceval.c:2705\n#83 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1cef930, globals=0x1d16290, \n    locals=0x0, args=0x1f43cb8, argcount=2, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n#84 0x00007ffff79ff7a1 in function_call (func=0x1d27cd8, arg=0x1f43c90, kw=0x0)\n    at Objects/funcobject.c:526\n#85 0x00007ffff79c0453 in PyObject_Call (func=0x1d27cd8, arg=0x1f43c90, kw=0x0)\n    at Objects/abstract.c:2529\n#86 0x00007ffff79dc87c in instancemethod_call (func=0x1d27cd8, arg=0x1f43c90, \n---Type <return> to continue, or q <return> to quit---\n    kw=0x0) at Objects/classobject.c:2578\n#87 0x00007ffff79c0453 in PyObject_Call (func=0x1f57f60, arg=0x1f97760, kw=0x0)\n    at Objects/abstract.c:2529\n#88 0x00007ffff7a60a91 in slot_tp_call (self=0x1fa1ed0, args=0x1f97760, \n    kwds=0x0) at Objects/typeobject.c:5403\n#89 0x00007ffff79c0453 in PyObject_Call (func=0x1fa1ed0, arg=0x1f97760, kw=0x0)\n    at Objects/abstract.c:2529\n#90 0x00007ffff7acb42b in do_call (func=0x1fa1ed0, pp_stack=0x7fffffffb860, \n    na=1, nk=0) at Python/ceval.c:4239\n#91 0x00007ffff7aca726 in call_function (pp_stack=0x7fffffffb860, oparg=1)\n    at Python/ceval.c:4044\n#92 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x202f280, throwflag=0)\n    at Python/ceval.c:2666\n#93 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1cefbf0, globals=0x1d16290, \n    locals=0x0, args=0x1f761f0, argcount=2, kws=0x7ffff7fa7088, kwcount=0, \n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3253\n#94 0x00007ffff79ff7a1 in function_call (func=0x1d27ed0, arg=0x1f761c8, \n    kw=0x202f130) at Objects/funcobject.c:526\n#95 0x00007ffff79c0453 in PyObject_Call (func=0x1d27ed0, arg=0x1f761c8, \n    kw=0x202f130) at Objects/abstract.c:2529\n#96 0x00007ffff7acbaf6 in ext_do_call (func=0x1d27ed0, \n    pp_stack=0x7fffffffbd98, flags=3, na=1, nk=0) at Python/ceval.c:4334\n#97 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x202ef20, throwflag=0)\n---Type <return> to continue, or q <return> to quit---\n    at Python/ceval.c:2705\n#98 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1cef930, globals=0x1d16290, \n    locals=0x0, args=0x1f59f10, argcount=2, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n#99 0x00007ffff79ff7a1 in function_call (func=0x1d27cd8, arg=0x1f59ee8, kw=0x0)\n    at Objects/funcobject.c:526\n#100 0x00007ffff79c0453 in PyObject_Call (func=0x1d27cd8, arg=0x1f59ee8, \n    kw=0x0) at Objects/abstract.c:2529\n#101 0x00007ffff79dc87c in instancemethod_call (func=0x1d27cd8, arg=0x1f59ee8, \n    kw=0x0) at Objects/classobject.c:2578\n#102 0x00007ffff79c0453 in PyObject_Call (func=0x1e450e0, arg=0x1d32ca0, \n    kw=0x0) at Objects/abstract.c:2529\n#103 0x00007ffff7a60a91 in slot_tp_call (self=0x1f8f990, args=0x1d32ca0, \n    kwds=0x0) at Objects/typeobject.c:5403\n#104 0x00007ffff79c0453 in PyObject_Call (func=0x1f8f990, arg=0x1d32ca0, \n    kw=0x0) at Objects/abstract.c:2529\n#105 0x00007ffff7acb42b in do_call (func=0x1f8f990, pp_stack=0x7fffffffc6d0, \n    na=1, nk=0) at Python/ceval.c:4239\n#106 0x00007ffff7aca726 in call_function (pp_stack=0x7fffffffc6d0, oparg=1)\n    at Python/ceval.c:4044\n#107 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x202dec0, throwflag=0)\n    at Python/ceval.c:2666\n#108 0x00007ffff7acaa47 in fast_function (func=0x1d2c300, \n---Type <return> to continue, or q <return> to quit---\n    pp_stack=0x7fffffffca70, n=2, na=2, nk=0) at Python/ceval.c:4107\n#109 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffffca70, oparg=1)\n    at Python/ceval.c:4042\n#110 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x202d1c0, throwflag=0)\n    at Python/ceval.c:2666\n#111 0x00007ffff7acaa47 in fast_function (func=0x1d2c648, \n    pp_stack=0x7fffffffce10, n=1, na=1, nk=0) at Python/ceval.c:4107\n#112 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffffce10, oparg=0)\n    at Python/ceval.c:4042\n#113 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x1d74860, throwflag=0)\n    at Python/ceval.c:2666\n#114 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1033460, globals=0xfc4cc0, \n    locals=0x0, args=0x1d32d38, argcount=1, kws=0x1c5be98, kwcount=6, \n    defs=0x1026928, defcount=10, closure=0x0) at Python/ceval.c:3253\n#115 0x00007ffff79ff7a1 in function_call (func=0x1055798, arg=0x1d32d10, \n    kw=0x1d6ee60) at Objects/funcobject.c:526\n#116 0x00007ffff79c0453 in PyObject_Call (func=0x1055798, arg=0x1d32d10, \n    kw=0x1d6ee60) at Objects/abstract.c:2529\n#117 0x00007ffff79dc87c in instancemethod_call (func=0x1055798, arg=0x1d32d10, \n    kw=0x1d6ee60) at Objects/classobject.c:2578\n#118 0x00007ffff79c0453 in PyObject_Call (func=0x1d3e760, arg=0x1d32d10, \n    kw=0x1d6ee60) at Objects/abstract.c:2529\n#119 0x00007ffff7acbaf6 in ext_do_call (func=0x1d3e760, \n---Type <return> to continue, or q <return> to quit---\n    pp_stack=0x7fffffffd628, flags=2, na=1, nk=5) at Python/ceval.c:4334\n#120 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x1b06250, throwflag=0)\n    at Python/ceval.c:2705\n#121 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x19bcca0, globals=0x19f5580, \n    locals=0x0, args=0xf18638, argcount=1, kws=0x1d2f088, kwcount=3, \n    defs=0x1cdbb08, defcount=11, closure=0x0) at Python/ceval.c:3253\n#122 0x00007ffff79ff7a1 in function_call (func=0x1d2c3a8, arg=0xf18610, \n    kw=0x1d6dab0) at Objects/funcobject.c:526\n#123 0x00007ffff79c0453 in PyObject_Call (func=0x1d2c3a8, arg=0xf18610, \n    kw=0x1d6dab0) at Objects/abstract.c:2529\n#124 0x00007ffff79dc87c in instancemethod_call (func=0x1d2c3a8, arg=0xf18610, \n    kw=0x1d6dab0) at Objects/classobject.c:2578\n#125 0x00007ffff79c0453 in PyObject_Call (func=0x1d3e6e0, arg=0x7ffff7fa7060, \n    kw=0x1d6dab0) at Objects/abstract.c:2529\n#126 0x00007ffff7a61898 in slot_tp_init (self=0x1d32c30, args=0x7ffff7fa7060, \n    kwds=0x1d6dab0) at Objects/typeobject.c:5663\n#127 0x00007ffff7a4e3e1 in type_call (type=0x1d67230, args=0x7ffff7fa7060, \n    kwds=0x1d6dab0) at Objects/typeobject.c:737\n#128 0x00007ffff79c0453 in PyObject_Call (func=0x1d67230, arg=0x7ffff7fa7060, \n    kw=0x1d6dab0) at Objects/abstract.c:2529\n#129 0x00007ffff7acbaf6 in ext_do_call (func=0x1d67230, \n    pp_stack=0x7fffffffdf28, flags=2, na=0, nk=1) at Python/ceval.c:4334\n#130 0x00007ffff7ac5101 in PyEval_EvalFrameEx (f=0x1d13e10, throwflag=0)\n---Type <return> to continue, or q <return> to quit---\n    at Python/ceval.c:2705\n#131 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x19cc670, globals=0x19f5580, \n    locals=0x0, args=0x683b00, argcount=0, kws=0x683b00, kwcount=2, \n    defs=0x1d2a1d8, defcount=1, closure=0x0) at Python/ceval.c:3253\n#132 0x00007ffff7acab78 in fast_function (func=0x1d2c840, \n    pp_stack=0x7fffffffe450, n=4, na=0, nk=2) at Python/ceval.c:4117\n#133 0x00007ffff7aca704 in call_function (pp_stack=0x7fffffffe450, oparg=512)\n    at Python/ceval.c:4042\n#134 0x00007ffff7ac4e9e in PyEval_EvalFrameEx (f=0x683970, throwflag=0)\n    at Python/ceval.c:2666\n#135 0x00007ffff7ac79ba in PyEval_EvalCodeEx (co=0x1185eb0, globals=0x6836d0, \n    locals=0x6836d0, args=0x0, argcount=0, kws=0x0, kwcount=0, defs=0x0, \n    defcount=0, closure=0x0) at Python/ceval.c:3253\n#136 0x00007ffff7abcff8 in PyEval_EvalCode (co=0x1185eb0, globals=0x6836d0, \n    locals=0x6836d0) at Python/ceval.c:667\n#137 0x00007ffff7afb1d6 in run_mod (mod=0x1237138, \n    filename=0x7fffffffed79 ""test_period.py"", globals=0x6836d0, \n    locals=0x6836d0, flags=0x7fffffffe9c0, arena=0x686f90)\n    at Python/pythonrun.c:1353\n#138 0x00007ffff7afb153 in PyRun_FileExFlags (fp=0x683940, \n    filename=0x7fffffffed79 ""test_period.py"", start=257, globals=0x6836d0, \n    locals=0x6836d0, closeit=1, flags=0x7fffffffe9c0)\n    at Python/pythonrun.c:1339\n---Type <return> to continue, or q <return> to quit---\n#139 0x00007ffff7af98bb in PyRun_SimpleFileExFlags (fp=0x683940, \n    filename=0x7fffffffed79 ""test_period.py"", closeit=1, flags=0x7fffffffe9c0)\n    at Python/pythonrun.c:943\n#140 0x00007ffff7af8ec7 in PyRun_AnyFileExFlags (fp=0x683940, \n    filename=0x7fffffffed79 ""test_period.py"", closeit=1, flags=0x7fffffffe9c0)\n    at Python/pythonrun.c:747\n#141 0x00007ffff7b15234 in Py_Main (argc=2, argv=0x7fffffffeb48)\n    at Modules/main.c:639\n#142 0x0000000000400784 in main (argc=2, argv=0x7fffffffeb48)\n    at ./Modules/python.c:23\n```\n\n\n'"
1344,4805159,changhiskhan,changhiskhan,2012-05-29 21:55:18,2012-05-30 00:02:55,2012-05-30 00:02:55,closed,changhiskhan,0.8.0,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1344,b'Parser error on complex date parsing with multiple index col specification',
1341,4804082,wesm,wesm,2012-05-29 20:58:23,2012-05-29 21:30:42,2012-05-29 21:30:42,closed,,0.8.0,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1341,b'logy argument ignored in KdePlot',
1340,4803035,wesm,wesm,2012-05-29 20:03:25,2012-06-02 19:08:43,2012-06-02 19:08:43,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1340,"b""Raise exception on NaT in DatetimeIndex.astype('O')""",
1339,4800143,wesm,wesm,2012-05-29 17:38:08,2012-06-01 20:23:52,2012-06-01 20:23:52,closed,,0.8.0,0,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1339,"b""set_xlim doesn't work with time series plots""","b""Ideally would like to be able to do:\n\n`plt.set_xlim(['1/1/2005', '1/1/2012'])`\n\nor even pass datetime.datetime objects. Not sure how feasible this is. I think the trick is setting the `converter` field on the subplot's X axis according to:\n\n```\nIn [22]: ax.xaxis.convert_units??\nType:       instancemethod\nBase Class: <type 'instancemethod'>\nString Form:<bound method XAxis.convert_units of <matplotlib.axis.XAxis object at 0x3809310>>\nNamespace:  Interactive\nFile:       /home/wesm/code/repos/matplotlib/lib/matplotlib/axis.py\nDefinition: ax.xaxis.convert_units(self, x)\nSource:\n    def convert_units(self, x):\n        if self.converter is None:\n            self.converter = munits.registry.get_converter(x)\n\n        if self.converter is None:\n            #print 'convert_units returning identity: units=%s, converter=%s'%(self.units, self.converter)\n            return x\n\n        ret =  self.converter.convert(x, self.units, self)\n        #print 'convert_units converting: axis=%s, units=%s, converter=%s, in=%s, out=%s'%(self, self.units, self.converter, x, ret)\n        return ret\n```"""
1338,4797514,wesm,wesm,2012-05-29 15:25:25,2012-06-02 21:20:55,2012-06-02 21:20:55,closed,,0.8.0,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1338,"b""tsplot doesn't respect style parameter""",
1336,4787122,wesm,wesm,2012-05-29 00:15:16,2012-05-29 00:18:46,2012-05-29 00:18:46,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1336,b'Display local time instead of UTC in DatetimeIndex.__repr__',"b""```\nIn [33]: rng\nOut[33]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-04-13 00:00:00, ..., 2012-03-06 00:00:00]\nLength: 100, Freq: W-TUE, Timezone: None\n\nIn [34]: rng.tz_localize('US/Eastern')\nOut[34]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-04-13 04:00:00, ..., 2012-03-06 05:00:00]\nLength: 100, Freq: W-TUE, Timezone: US/Eastern\n```"""
1332,4784429,wesm,wesm,2012-05-28 18:58:38,2012-05-28 22:47:59,2012-05-28 22:47:59,closed,,0.8.0,0,Bug;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1332,"b""tsplot incorrect for PeriodIndex with freq='B'""",b'Broke during ordinal representation change'
1329,4782562,wesm,wesm,2012-05-28 16:20:36,2012-05-28 16:46:33,2012-05-28 16:46:33,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1329,"b""Scalars don't propagate in DataFrame constructor""","b""This only works if an index is specified; fails in the index inference process\n\n```\nIn [1]: DataFrame({'a' : randn(10), 'b' : 6})\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-1-16dbfdf3cea7> in <module>()\n----> 1 DataFrame({'a' : randn(10), 'b' : 6})\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    367             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    368         elif isinstance(data, dict):\n--> 369             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    370         elif isinstance(data, ma.MaskedArray):\n    371             mask = ma.getmaskarray(data)\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in _init_dict(self, data, index, columns, dtype)\n    446         # figure out the index, if necessary\n    447         if index is None:\n--> 448             index = extract_index(data)\n    449         else:\n    450             index = _ensure_index(index)\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in extract_index(data)\n   4540             else:\n   4541                 have_raw_arrays = True\n-> 4542                 raw_lengths.append(len(v))\n   4543 \n   4544         if have_series or have_dicts:\n\nTypeError: object of type 'int' has no len()\n```"""
1328,4782467,changhiskhan,wesm,2012-05-28 16:12:31,2012-05-28 22:58:47,2012-05-28 22:58:47,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1328,b'Better error msg for malformed freq alias',"b'```\nIn [17]: date_range(datetime.datetime.today(), periods=10, freq=\'2h20m\')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/chang/Dropbox/git/pandas/<ipython-input-17-ff4e03382573> in <module>()\n----> 1 date_range(datetime.datetime.today(), periods=10, freq=\'2h20m\')\n\n/home/chang/Dropbox/git/pandas/pandas/tseries/index.pyc in date_range(start, end, periods, freq, tz, normalize)\n   1209     """"""\n   1210     return DatetimeIndex(start=start, end=end, periods=periods,\n-> 1211                          freq=freq, tz=tz, normalize=normalize)\n   1212 \n   1213 \n\n/home/chang/Dropbox/git/pandas/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, **kwds)\n    202 \n    203         if data is None and offset is None:\n--> 204             raise ValueError(""Must provide freq argument if no data is ""\n    205                              ""supplied"")\n    206 \n\nValueError: Must provide freq argument if no data is supplied\n```'"
1327,4780870,changhiskhan,wesm,2012-05-28 14:11:19,2012-05-28 17:59:39,2012-05-28 17:59:39,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1327,b'Resampling to unanchored week fails',"b""```\nIn [16]: ts = tm.makeTimeSeries()\n\nIn [17]: ts.resample('W')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/pandas/<ipython-input-17-d49462deed14> in <module>()\n----> 1 ts.resample('W')\n\n...\n\n\n~/pandas/pandas/lib.so in pandas.lib.generate_bins_dt64 (pandas/src/tseries.c:60325)()\n\nValueError: Values falls after last bin\n```"""
1323,4768095,wesm,wesm,2012-05-26 21:05:25,2013-12-04 00:58:08,2012-05-28 16:33:23,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1323,"b""Take care with series.astype('O') and DataFrame.value with datetime64""","b""```\nIn [7]: s\nOut[7]: \n0   2000-01-01 00:00:00\n1   2000-01-02 00:00:00\n2   2000-01-03 00:00:00\n3   2000-01-04 00:00:00\n4   2000-01-05 00:00:00\n5   2000-01-06 00:00:00\n6   2000-01-07 00:00:00\n7   2000-01-08 00:00:00\n8   2000-01-09 00:00:00\n9   2000-01-10 00:00:00\n\nIn [8]: s.astype('O')\nOut[8]: \n0    1970-01-11 184:00:00\n1    1970-01-11 208:00:00\n2    1970-01-11 232:00:00\n3     1970-01-11 00:00:00\n4     1970-01-11 24:00:00\n5     1970-01-11 48:00:00\n6     1970-01-11 72:00:00\n7     1970-01-11 96:00:00\n8    1970-01-11 120:00:00\n9    1970-01-11 144:00:00\n```"""
1321,4761598,changhiskhan,wesm,2012-05-25 22:02:55,2012-05-26 16:28:15,2012-05-26 16:28:15,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1321,b'DataFrame.ix with boolean arrays',"b'Works right in one dimension but not both:\n\nIn [19]: df = DataFrame(np.random.randn(3, 2))\n\nIn [20]: df\nOut[20]: \n          0         1\n0  0.666325  0.812192\n1  0.023781  1.719397\n2  0.318738  0.227240\n\nIn [21]: df.ix[df.index==0, :]\nOut[21]: \n          0         1\n0  0.666325  0.812192\n\nIn [24]: df.ix[:, df.columns==1]\nOut[24]: \n          1\n0  0.812192\n1  1.719397\n2  0.227240\n\nBut not both:\n\nIn [23]: df.ix[df.index==0, df.columns==1]\nOut[23]: \n              0         1\nTrue   0.023781  1.719397\nFalse  0.666325  0.812192\nFalse  0.666325  0.812192\n'"
1319,4758516,wesm,wesm,2012-05-25 18:32:36,2012-05-28 15:51:39,2012-05-28 15:51:39,closed,,0.8.0,16,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/1319,b'Get JSON extension to work with Python 3',b'@Komnomnomnom this is a good one. UltraJSON does not work on Python 3. This is going to be fun'
1318,4756435,wesm,wesm,2012-05-25 16:14:39,2012-07-09 19:38:35,2012-07-09 19:38:35,closed,,0.8.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1318,b'Handling TypeErrors in PyObject khash-table',
1316,4753385,wesm,wesm,2012-05-25 13:28:18,2012-05-25 21:42:26,2012-05-25 21:42:26,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1316,b'No equals check when joining non-unique indexes?',b'Need to think about this one'
1314,4744538,wesm,wesm,2012-05-24 22:12:09,2012-05-25 20:42:13,2012-05-25 20:42:13,closed,,0.8.0,8,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1314,b'Handle other unit types in datetime64 scalar objects',"b""```\nIn [4]: val\nOut[4]: 2012-02-05 00:00:00\n\nIn [5]: type(val)\nOut[5]: numpy.datetime64\n\nIn [6]: val.dtype\nOut[6]: dtype('datetime64[D]')\n\nIn [7]: DatetimeIndex([val])\nOut[7]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[1970-01-01 00:00:00.000015375, ..., 1970-01-01 00:00:00.000015375]\nLength: 1, Freq: None, Timezone: None\n```"""
1313,4739440,wesm,wesm,2012-05-24 17:35:56,2012-05-25 16:31:55,2012-05-25 16:31:55,closed,,0.8.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1313,b'groupby on level=0 with normal Index discards name',"b""example:\n\n```\na = pandas.Series([1,2,3,10,4,5,20,6], Index([1,2,3,1,4,5,2,6], name='foo'))\na.groupby(level=0).sum()\n```"""
1310,4734698,wesm,wesm,2012-05-24 14:37:57,2012-05-28 16:11:13,2012-05-28 16:11:13,closed,,0.8.0,4,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1310,b'NaT field access?',
1307,4725147,wesm,wesm,2012-05-24 03:28:21,2012-05-24 22:10:03,2012-05-24 22:10:03,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1307,b'Odd merge error',"b""```\nIn [6]: df1\nOut[6]: \n                     x\n1970-01-16 08:09:36  a\n\nIn [7]: df2\nOut[7]: \n                     y\n1970-01-16 08:09:36  b\n1970-01-16 08:09:36  c\n\nIn [8]: merge(df1, df2)\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-8-90d5dd612b4d> in <module>()\n----> 1 merge(df1, df2)\n\n/home/wesm/code/pandas/pandas/tools/merge.pyc in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy)\n     30                          right_index=right_index, sort=sort, suffixes=suffixes,\n     31                          copy=copy)\n---> 32     return op.get_result()\n     33 if __debug__: merge.__doc__ = _merge_doc % '\\nleft : DataFrame'\n     34 \n\n/home/wesm/code/pandas/pandas/tools/merge.pyc in get_result(self)\n    174 \n    175     def get_result(self):\n--> 176         join_index, left_indexer, right_indexer = self._get_join_info()\n    177 \n    178         # this is a bit kludgy\n\n/home/wesm/code/pandas/pandas/tools/merge.pyc in _get_join_info(self)\n    247         else:\n    248             # max groups = largest possible number of distinct groups\n--> 249             left_key, right_key, max_groups = self._get_group_keys()\n    250 \n    251             join_func = _join_functions[self.how]\n\n/home/wesm/code/pandas/pandas/tools/merge.pyc in _get_group_keys(self)\n    411             group_sizes.append(count)\n    412 \n--> 413         left_group_key = get_group_index(left_labels, group_sizes)\n    414         right_group_key = get_group_index(right_labels, group_sizes)\n    415 \n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in get_group_index(label_list, shape)\n   1921         return label_list[0]\n   1922 \n-> 1923     n = len(label_list[0])\n   1924     group_index = np.zeros(n, dtype=np.int64)\n   1925     mask = np.zeros(n, dtype=bool)\n\nIndexError: list index out of range\n```"""
1306,4724923,leonbaum,wesm,2012-05-24 02:52:45,2012-05-25 02:47:03,2012-05-25 02:45:12,closed,,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1306,b'Incorrect join of DataFrames with non-unique datetime indices',"b""I'm not sure whether joining of DFs with non-unique indices is now supported, but it's not giving an error and this simple example don't make sense:\n\n```\nIn [11]: df1 = pandas.DataFrame({'x': ['a']}, index=[np.datetime64('2012')])\n\nIn [12]: df2 = pandas.DataFrame({'y': ['b', 'c']}, index=[np.datetime64('2012')] * 2)\n\nIn [13]: df1\nOut[13]: \n                     x\n1970-01-16 08:09:36  a\n\nIn [14]: df2\nOut[14]: \n                     y\n1970-01-16 08:09:36  b\n1970-01-16 08:09:36  c\n\nIn [15]: df1.join(df2, how='inner')\nOut[15]: \n                     x  y\n1970-01-16 08:09:36  a  b\n```\n\nShouldn't the 1st row of df1 join to both rows of df2?"""
1304,4723774,wesm,wesm,2012-05-24 00:21:23,2012-05-25 23:24:09,2012-05-25 23:24:09,closed,,0.8.0,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1304,b'Second vs. nanosecond resolution in encoding indexes vs datetime.datetime objects in JSON encoding',"b""related to #1263\n\n```\nIn [2]: ujson.encode(datetime.now())\nOut[2]: '1337804262'\n\nIn [3]: np.array(datetime.now(), dtype='M8[ns]').view('i8')\nOut[3]: array(1337804288505948000)\n```"""
1303,4723519,wesm,wesm,2012-05-23 23:58:23,2012-05-25 21:53:26,2012-05-25 21:53:26,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1303,b'DatetimeIndex should accept list of integers',"b""brought up by #1263\n\n```\nIn [5]: DatetimeIndex(ujson.decode(ujson.encode(rng)))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-5-e33bac335db8> in <module>()\n----> 1 DatetimeIndex(ujson.decode(ujson.encode(rng)))\n\n/home/wesm/code/pandas/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, **kwds)\n    227                 data = _str_to_dt_array(data, offset)\n    228             else:\n--> 229                 data = tools.to_datetime(data)\n    230                 data = np.asarray(data, dtype='M8[ns]')\n    231 \n\n/home/wesm/code/pandas/pandas/tseries/tools.pyc in to_datetime(arg, errors, dayfirst)\n     93         return lib.string_to_datetime(com._ensure_object(arg),\n     94                                       raise_=errors == 'raise',\n---> 95                                       dayfirst=dayfirst)\n     96 \n     97     try:\n\n/home/wesm/code/pandas/pandas/_tseries.so in pandas._tseries.string_to_datetime (pandas/src/tseries.c:34314)()\n\nTypeError: object of type 'long' has no len()\n```"""
1286,4696116,wesm,wesm,2012-05-22 18:01:54,2012-05-28 02:24:37,2012-05-28 02:24:37,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1286,b'Handling of NaT scalars in NumPy 1.6/1.7',"b'Currently boxing as Timestamp, probably not right'"
1282,4685636,lbeltrame,wesm,2012-05-22 07:37:10,2012-05-23 21:48:58,2012-05-23 21:40:22,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1282,b'Conversion to R matrix wreaks havoc in case of mixed type DataFrames',"b""When converting a mixed_type DataFrame to a R matrix (not a data.frame), everything is casted to string vectors, because a matrix can't be heterogeneously typed, to my knowledge.\n\nI have two possible solutions I can send as pull request, and I would like to ask which is better:\n\n1. raise ValueError for mixed type dataframes\n2. raise a warning\n\nGiven that most matrices needed in R are the numeric ones, I would go for option 1.\n\n"""
1279,4682497,wesm,wesm,2012-05-22 00:49:36,2012-05-23 21:19:18,2012-05-23 21:19:18,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1279,b'Bad Series.__repr__',"b'```\nPort-au-Prince    184\nJacmel             45\nP\xa8\xa6tionville       33\nLeogane            24\nL\xa8\xa6ogne          22\n\nindex : Index([Port-au-Prince, Jacmel, P\xa8\xa6tionville, Leogane, L\xa8\xa6ogne], dtype=object)\n```'"
1278,4682483,wesm,wesm,2012-05-22 00:47:34,2013-12-04 00:43:19,2012-05-23 21:28:26,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1278,b'DataFrame.describe fails for dtype=datetime64',"b""```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/home/wesm/Dropbox/book/svn/data/haiti/<ipython-input-165-efa692752119> in <module>()\n----> 1 data.describe()\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in describe(self, percentile_width)\n   3772         for column in numdata.columns:\n   3773             series = self[column]\n-> 3774             destat.append([series.count(), series.mean(), series.std(),\n   3775                            series.min(), series.quantile(lb), series.median(),\n   3776                            series.quantile(ub), series.max()])\n\n/home/wesm/code/pandas/pandas/core/series.pyc in f(self, axis, dtype, out, skipna, level)\n    258         if level is not None:\n    259             return self._agg_by_level(shortname, level=level, skipna=skipna)\n--> 260         return nanop(self.values, skipna=skipna)\n    261     f.__name__ = shortname\n    262     return f\n\n/home/wesm/code/pandas/pandas/core/nanops.pyc in f(values, axis, skipna, **kwds)\n     41                 result = alt(values, axis=axis, skipna=skipna, **kwds)\n     42         except Exception:\n---> 43             result = alt(values, axis=axis, skipna=skipna, **kwds)\n     44 \n     45         return result\n\n/home/wesm/code/pandas/pandas/core/nanops.pyc in _nanmean(values, axis, skipna)\n     77         np.putmask(values, mask, 0)\n     78 \n---> 79     the_sum = _ensure_numeric(values.sum(axis))\n     80     count = _get_counts(mask, axis)\n     81 \n\nValueError: could not find a matching type for add.reduce, requested type has type code 'M'\n```"""
1268,4675902,wesm,wesm,2012-05-21 17:52:47,2012-05-21 18:41:33,2012-05-21 18:41:33,closed,wesm,0.8.0,0,Bug;Groupby,https://api.github.com/repos/pydata/pandas/issues/1268,b'Multiple aggregation inflexbility',"b'```\nIn [7]: tips.groupby(\'sex\').agg({\'tip\' : {\'mean\' : \'mean\', \'std\' : \'std\'}, \'size\' : \'sum\'})\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-7-850d34319c48> in <module>()\n----> 1 tips.groupby(\'sex\').agg({\'tip\' : {\'mean\' : \'mean\', \'std\' : \'std\'}, \'size\' : \'sum\'})\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\n    278         See docstring for aggregate\n    279         """"""\n--> 280         return self.aggregate(func, *args, **kwargs)\n    281 \n    282     def _iterate_slices(self):\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\n   1430                     colg = SeriesGroupBy(obj[col], selection=col,\n   1431                                          grouper=self.grouper)\n-> 1432                     result[col] = colg.aggregate(agg_how)\n   1433                     keys.append(col)\n   1434 \n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\n   1185 \n   1186         if hasattr(func_or_funcs,\'__iter__\'):\n-> 1187             ret = self._aggregate_multiple_funcs(func_or_funcs)\n   1188         else:\n   1189             cyfunc = _intercept_cython(func_or_funcs)\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _aggregate_multiple_funcs(self, arg)\n   1216         else:\n   1217             # list of functions\n-> 1218             columns = [func.__name__ for func in arg]\n   1219             arg = zip(columns, arg)\n   1220 \n\nAttributeError: \'str\' object has no attribute \'__name__\'\n\nIn [8]: tips.groupby(\'sex\').agg({\'tip\' : {\'mean\' : \'mean\', \'std\' : \'std\'}, \'size\' : [\'sum\']})\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-8-6961866fbf6c> in <module>()\n----> 1 tips.groupby(\'sex\').agg({\'tip\' : {\'mean\' : \'mean\', \'std\' : \'std\'}, \'size\' : [\'sum\']})\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\n    278         See docstring for aggregate\n    279         """"""\n--> 280         return self.aggregate(func, *args, **kwargs)\n    281 \n    282     def _iterate_slices(self):\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\n   1430                     colg = SeriesGroupBy(obj[col], selection=col,\n   1431                                          grouper=self.grouper)\n-> 1432                     result[col] = colg.aggregate(agg_how)\n   1433                     keys.append(col)\n   1434 \n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\n   1185 \n   1186         if hasattr(func_or_funcs,\'__iter__\'):\n-> 1187             ret = self._aggregate_multiple_funcs(func_or_funcs)\n   1188         else:\n   1189             cyfunc = _intercept_cython(func_or_funcs)\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _aggregate_multiple_funcs(self, arg)\n   1216         else:\n   1217             # list of functions\n-> 1218             columns = [func.__name__ for func in arg]\n   1219             arg = zip(columns, arg)\n   1220 \n\nAttributeError: \'str\' object has no attribute \'__name__\'\n\nIn [9]: tips.groupby(\'sex\').agg({\'tip\' : {\'mean\' : \'mean\', \'std\' : \'std\'}, \'size\' : {\'sum\' : \'sum\'}})\nOut[9]: \n             tip            size\n             std      mean   sum\nsex                             \nFemale  1.159495  2.833448   214\nMale    1.489102  3.089618   413\n```'"
1267,4675610,wesm,wesm,2012-05-21 17:36:25,2012-05-21 18:47:29,2012-05-21 18:47:29,closed,,0.8.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1267,b'Better error message if nothing passed to reindex(...)',
1265,4674330,wesm,wesm,2012-05-21 16:23:50,2012-05-23 21:00:59,2012-05-23 21:00:59,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1265,b'Time series plotting borked after move to nanoseconds',
1262,4661437,wesm,wesm,2012-05-20 17:41:17,2012-05-20 18:37:35,2012-05-20 18:37:35,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1262,b'pandas should not import scipy automatically',"b""```\n>>> import pandas\n>>> import sys\n>>> 'scipy' in sys.modules\nTrue\n```"""
1259,4655238,wesm,wesm,2012-05-19 16:08:09,2012-05-20 02:57:41,2012-05-20 02:57:41,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1259,b'Period resampling buglet',"b""```\nIn [20]: rng = date_range('1/1/2000','12/31/2000')\n\nIn [21]: ts = Series(np.random.randn(len(rng)), index=rng)\n\nIn [22]: ts.resample('M', kind='period')\nOut[22]: \nJan-2000    0.280017\nFeb-2000    0.010740\nMar-2000   -0.175855\nApr-2000   -0.080775\nMay-2000    0.253388\nJun-2000    0.072629\nJul-2000    0.136193\nAug-2000   -0.039670\nSep-2000   -0.300413\nOct-2000   -0.200501\nNov-2000    0.037753\nDec-2000   -0.053187\nJan-2001         NaN\nFreq: M\n```"""
1258,4655194,wesm,wesm,2012-05-19 15:59:22,2012-05-19 20:16:42,2012-05-19 20:16:42,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1258,b'MultiIndex.from_arrays does not preserve DatetimeIndex properly',b'Loss of frequency info observed'
1257,4650281,wesm,wesm,2012-05-18 22:00:20,2012-05-19 20:12:40,2012-05-19 20:12:40,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1257,b'GroupBy[Index] raises TypeError',"b'```\nIn [5]: means_by_hour_of_day = training_data.groupby(\'hour\')[target_names].mean()\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/wesm/Dropbox/book/svn/data/air_quality/<ipython-input-5-141d964fe4bb> in <module>()\n----> 1 means_by_hour_of_day = training_data.groupby(\'hour\')[target_names].mean()\n\n/home/wesm/code/pandas/pandas/core/groupby.pyc in __getitem__(self, key)\n   1663                                     as_index=self.as_index)\n   1664         else:\n-> 1665             if key not in self.obj:  # pragma: no cover\n   1666                 raise KeyError(str(key))\n   1667             # kind of a kludge\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in __contains__(self, key)\n    566     def __contains__(self, key):\n    567         """"""True if DataFrame has this column""""""\n--> 568         return key in self.columns\n    569 \n    570     #----------------------------------------------------------------------\n\n/home/wesm/code/pandas/pandas/core/index.pyc in __contains__(self, key)\n    263 \n    264     def __contains__(self, key):\n--> 265         hash(key)\n    266         # work around some kind of odd cython bug\n    267         try:\n\n/home/wesm/code/pandas/pandas/core/index.pyc in __hash__(self)\n    271 \n    272     def __hash__(self):\n--> 273         return hash(self.view(np.ndarray))\n    274 \n    275     def __setitem__(self, key, value):\n\nTypeError: unhashable type: \'numpy.ndarray\'\n```'"
1254,4646144,tkf,wesm,2012-05-18 17:31:16,2012-07-12 21:44:13,2012-07-12 21:43:53,closed,,0.8.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1254,b'HDFStore.get(non_existing_key) raises NoSuchNodeError',"b'Here is clipped traceback.  I guess you need to catch NoSuchNodeError also.  Or maybe it is better to ask pytables to make NoSuchNodeError a subclass of AttributeError?  I am using PyTables 2.3.1 and 0.7.3.\n\n```\n/.../local/lib/python2.7/site-packages/pandas/io/pytables.pyc in get(self, key)\n    258         """"""\n    259         try:\n--> 260             group = getattr(self.handle.root, key)\n    261             return self._read_group(group)\n    262         except AttributeError:\n\n/.../src/tables/tables/group.pyc in __getattr__(self, name)\n    824             self._g_addChildrenNames()\n    825             return myDict[name]\n--> 826         return self._f_getChild(name)\n    827 \n    828 \n\n/.../src/tables/tables/group.pyc in _f_getChild(self, childname)\n    695         self._g_checkOpen()\n    696 \n--> 697         self._g_checkHasChild(childname)\n    698 \n    699         childPath = joinPath(self._v_pathname, childname)\n\n/.../src/tables/tables/group.pyc in _g_checkHasChild(self, name)\n    434             raise NoSuchNodeError(\n    435                 ""group ``%s`` does not have a child named ``%s``""\n--> 436                 % (self._v_pathname, name))\n    437         return node_type\n    438 \n\nNoSuchNodeError: group ``/`` does not have a child named ``power_change_9``\n```\n'"
1249,4633510,changhiskhan,wesm,2012-05-17 22:36:39,2012-05-19 18:50:56,2012-05-19 18:50:56,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1249,b'resample.values_at_time error',"b'In [66]: ts = tm.makeTimeSeries()\n\nIn [67]: ts.at_time(ts.index[2])\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n/home/chang/Dropbox/git/pandas/<ipython-input-67-324c3adda78b> in <module>()\n----> 1 ts.at_time(ts.index[2])\n\n/home/chang/Dropbox/git/pandas/pandas/core/series.py in at_time(self, time, tz, asof)\n   2676         """"""\n   2677         from pandas.tseries.resample import values_at_time\n-> 2678         return values_at_time(self, time, tz=tz, asof=asof)\n   2679 \n   2680     def tz_convert(self, tz, copy=True):\n\n/home/chang/Dropbox/git/pandas/pandas/tseries/resample.py in values_at_time(obj, time, tz, asof)\n    363 \n    364     mus = _time_to_microsecond(time)\n--> 365     indexer = lib.values_at_time(obj.index.asi8, mus)\n    366     indexer = com._ensure_platform_int(indexer)\n    367     return obj.take(indexer)\n\n/home/chang/Dropbox/git/pandas/pandas/_tseries.so in pandas._tseries.values_at_time (pandas/src/tseries.c:38356)()\n\nIndexError: Out of bounds on buffer access (axis 0)\n> /home/chang/Dropbox/git/pandas/datetime.pyx(1155)pandas._tseries.values_at_time (pandas/src/tseries.c:38356)()\n'"
1243,4593665,mirage007,wesm,2012-05-15 21:30:56,2012-05-19 20:15:34,2012-05-19 20:15:34,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1243,b'apply fails when some data is None',"b'In [91]: from pandas import DataFrame\n\nIn [92]: a = DataFrame([1,2, None, 3, 4])\n\nIn [93]: a\nOut[93]:\n      0\n0     1\n1     2\n2  None\n3     3\n4     4\n\nIn [94]: a.apply(lambda x: str(x[0]), axis=1)\nOut[94]:\n0       1\n1       2\n2    None\n3       3\n4       4\n\nIn [95]: a.apply(lambda x: int(x[0]), axis=1)\nOut[95]:\n0     1\n1     2\n2   NaN\n3   NaN\n4   NaN\n\n________________\n\nI was expecting elements 3 and 4 to still evaluate properly in this case to something like this:\n\n0     1\n1     2\n2   NaN\n3     3\n4   4\n\ni am using windows xp 32 bit with pandas 0.7.1 on python 2.7.2'"
1242,4591221,wesm,wesm,2012-05-15 19:49:03,2012-05-19 16:29:46,2012-05-19 16:29:46,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1242,b'pandas < 0.8.0 HDFStore files have Cython 32/64 bit issues',"b""```\n# Save to hdf5.\nkw = dict(table=True, compression='zlib')\nstore = pandas.HDFStore('ADCP_series.h5', 'w')\nstore.put('ADCP', ADCP, **kw)\nstore.put('Ancillary', Anc, **kw)\nstore.close()\n\nOn my Enthought Python Distribution I can load it fine (also with my previous setup),\n\npandas.__version__\nOut[2]: '0.6.1'\ntables.__version__\nOut[4]: '2.3.1'\n\nBut on pandas dev,\npandas.__version__\nOut[8]: '0.8.0.dev'\n\nI get the following:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n----> 1 ADCP = f['ADCP']\n\n/usr/lib/python2.7/site-packages/pandas/io/pytables.pyc in __getitem__(self, key)\n   181\n   182     def __getitem__(self, key):\n--> 183         return self.get(key)\n   184\n   185     def __setitem__(self, key, value):\n\n/usr/lib/python2.7/site-packages/pandas/io/pytables.pyc in get(self, key)\n   280         try:\n   281             group = getattr(self.handle.root, key)\n--> 282             return self._read_group(group)\n   283         except AttributeError:\n   284             raise\n\n/usr/lib/python2.7/site-packages/pandas/io/pytables.pyc in _read_group(self, group, where)\n   792         kind = _LEGACY_MAP.get(kind, kind)\n   793         handler = self._get_handler(op='read', kind=kind)\n--> 794         return handler(group, where)\n   795\n   796     def _read_series(self, group, where=None):\n\n/usr/lib/python2.7/site-packages/pandas/io/pytables.pyc in _read_wide_table(self, group, where)\n   552\n   553     def _read_wide_table(self, group, where=None):\n--> 554         return self._read_panel_table(group, where)\n   555\n   556     def _write_index(self, group, key, index):\n\n/usr/lib/python2.7/site-packages/pandas/io/pytables.pyc in _read_panel_table(self, group, where)\n   843\n   844         if len(unique(key)) == len(key):\n--> 845             sorter, _ = lib.groupsort_indexer(key, J * K)\n   846\n   847             # the data need to be sorted\n\n/usr/lib/python2.7/site-packages/pandas/_tseries.so in pandas._tseries.groupsort_indexer (pandas/src/tseries.c:49539)()\n\nValueError: Buffer dtype mismatch, expected 'int64_t' but got 'long'```"""
1229,4548501,wesm,wesm,2012-05-12 18:15:41,2012-05-12 19:46:37,2012-05-12 19:46:37,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1229,b'Series.repeat does not do the right thing',"b""```\nIn [4]: Series(np.array([tm.rands(10) for _ in xrange(10)], dtype='O'))\nOut[4]: \n0    uU7IsfsSR0\n1    8so3eV8AAA\n2    ITSfW3Ce68\n3    WNDTb9pMg0\n4    FCmWYJyWri\n5    70hQk6Hy4b\n6    19gZtoizil\n7    CTvpYNkem8\n8    rMZijLdFL9\n9    yVd35ga8Ap\n\nIn [5]: Series(np.array([tm.rands(10) for _ in xrange(10)], dtype='O')).repeat(10)\nOut[5]: \n0    oP0SHbkuc5\n1    oP0SHbkuc5\n2    oP0SHbkuc5\n3    oP0SHbkuc5\n4    oP0SHbkuc5\n5    oP0SHbkuc5\n6    oP0SHbkuc5\n7    oP0SHbkuc5\n8    oP0SHbkuc5\n9    oP0SHbkuc5\nLength: 100\n```"""
1228,4547961,wesm,wesm,2012-05-12 16:33:05,2012-05-12 19:41:52,2012-05-12 19:41:52,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1228,b'Broken quarter -> Period parsing',"b""```\nIn [1]: Period('2007Q2', freq='Q-JUN')\nOut[1]: Period('2007Q4', 'Q-JUN')\n```"""
1222,4492031,wesm,wesm,2012-05-09 13:04:45,2012-05-12 17:04:19,2012-05-12 17:04:19,closed,,0.8.0,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1222,b'DataFrame.sort_index use with MultiIndex',
1219,4483849,wesm,wesm,2012-05-08 23:29:38,2012-05-08 23:49:03,2012-05-08 23:49:03,closed,wesm,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1219,b'Test failure on 32-bit',"b'======================================================================\r\nFAIL: test_reindex_pad (pandas.tests.test_series.TestSeries)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Program Files (x86)\\Jenkins\\workspace\\pandas-windows-test-py27\\pandas\\tests\\test_series.py"", line 2263, in test_reindex_pad\r\n    assert_series_equal(reindexed, expected)\r\n  File ""C:\\Program Files (x86)\\Jenkins\\workspace\\pandas-windows-test-py27\\pandas\\util\\testing.py"", line 128, in assert_series_equal\r\n    assert(left.dtype == right.dtype)\r\nAssertionError\r\n\r\n----------------------------------------------------------------------\r\nRan 1762 tests in 58.531s'"
1217,4474589,wesm,wesm,2012-05-08 15:13:49,2012-05-08 23:33:05,2012-05-08 23:33:05,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1217,b'MultiIndex formatting failure when index name coincides with first value',"b""```\r\n(Pdb) grouped\r\n            value\r\na b c            \r\n  0 foo  3.169106\r\nb 1 bar  1.060375\r\n(Pdb) grouped.index\r\nMultiIndex([('a', 0, 'foo'), ('b', 1, 'bar')], dtype=object)\r\n```"""
1214,4473149,wesm,wesm,2012-05-08 13:58:52,2012-05-12 22:00:07,2012-05-12 22:00:07,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1214,"b""lib.reduce can't handle non-object index types""",b'Not sure how much work this would be but would be a big speed boost'
1213,4471701,changhiskhan,wesm,2012-05-08 12:26:28,2012-05-14 14:12:30,2012-05-14 14:12:30,closed,,0.8.0,0,Bug;Data IO,https://api.github.com/repos/pydata/pandas/issues/1213,b'Reading NA values properly from XLS files',"b'From pystatsmodels mailing list:\r\n\r\n""I\'ve got an excel file created from Paste Values that when read with pandas.ExcelFile returns int 42 for #N/A values.\r\n\r\nLooking at http://stackoverflow.com/questions/4928629/xlrd-excel-script-converting-n-a-to-42 it would seem I\'d need to be able to access the xlrd cell type rather than the value to interpret as NaN. Is there a way to do this?""\r\n\r\n\r\n\r\nAs suggested by the stackoverflow article, we should use the error_text_from_code dict in the xlrd module to convert error codes to the proper text.'"
1211,4465272,changhiskhan,wesm,2012-05-08 00:32:18,2012-05-08 14:31:25,2012-05-08 14:31:25,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1211,"b""Can't use PeriodIndex as DataFrame columns""","b""This raises an Exception:\r\n\r\nDataFrame(randn(10, 5), columns=period_range('1/1/2000', periods=5))\r\n\r\n\r\n/pandas/pandas/core/frame.pyc in _apply_standard(self, func, axis, ignore_failures)\r\n   3209                 try:\r\n   3210                     if hasattr(e, 'args'):\r\n-> 3211                         e.args = e.args + ('occurred at index %s' % str(k),)\r\n   3212                 except NameError:\r\n   3213                     # no k defined yet\r\n\r\nUnboundLocalError: local variable 'k' referenced before assignment\r\n"""
1206,4460500,GlenHertz,wesm,2012-05-07 19:24:40,2012-07-21 16:51:11,2012-07-21 16:51:10,closed,,0.8.1,4,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1206,b'Series.interpolate with index of type float gives wrong result',"b""Hi,\r\n\r\nThe logic for Series.interpolate assumes the indexes are equally spaced.  With a floating point index this is not the desired interpolation.  For example:\r\n\r\n```python\r\nx1 = np.array([0, 0.25, 0.77, 1.2, 1.4, 2.6, 3.1])\r\ny1 = np.array([0, 1.1, 0.5, 1.5, 1.2, 2.1, 2.4])\r\nx2 = np.array([0, 0.25, 0.66, 1.0, 1.2, 1.4, 3.1])\r\ny2 = np.array([0, 0.2, 0.8, 1.1, 2.2, 0.1, 2.4])\r\n\r\ndf1 = DataFrame(data=y1, index=x1, columns=['A'])\r\ndf1.plot(marker='o')\r\n\r\ndf2 = DataFrame(data=y2, index=x2, columns=['A'])\r\ndf2.plot(marker='o')\r\n\r\ndf3=df1 - df2\r\ndf3.plot(marker='o')\r\nprint df3\r\n\r\ndef resample(signals):\r\n    aligned_x_vals = reduce(lambda s1, s2: s1.index.union(s2.index), signals)\r\n    return map(lambda s: s.reindex(aligned_x_vals).apply(Series.interpolate), signals)\r\n\r\nsig1, sig2 = resample([df1, df2])\r\nsig3 = sig1 - sig2\r\nplt.plot(df1.index, df1.values, marker='D')\r\nplt.plot(sig1.index, sig1.values, marker='o')\r\nplt.grid()\r\nplt.figure()\r\nplt.plot(df2.index, df2.values, marker='o')\r\nplt.plot(sig2.index ,sig2.values, marker='o')\r\nplt.grid()\r\n```\r\n\r\nI expect sig1 and sig2 to have more points than df1 and df2 but with the values interpolated. There are a few points that are not overlapping because it is assumed they are equally spaced.  In my opinion if the index is a floating point the user wants to interpolate by the index's value and don't assume they are equally spaced.  It should do something like this:\r\n\r\n```python\r\nimport numpy as np\r\nfrom pandas import *\r\n\r\ndef interpolate(serie):\r\n    try:\r\n        inds = np.array([float(d) for d in serie.index])\r\n    except ValueError:\r\n        inds = np.arange(len(serie))\r\n\r\n    values = serie.values\r\n\r\n    invalid = isnull(values)\r\n    valid = -invalid\r\n\r\n    firstIndex = valid.argmax()\r\n    valid = valid[firstIndex:]\r\n    invalid = invalid[firstIndex:]\r\n    inds = inds[firstIndex:]\r\n\r\n    result = values.copy()\r\n    result[firstIndex:][invalid] = np.interp(inds[invalid], inds[valid],\r\n                                             values[firstIndex:][valid])\r\n\r\n    return Series(result, index=serie.index, name=serie.name)\r\n```\r\n\r\nThanks """
1202,4455611,wesm,wesm,2012-05-07 14:57:33,2012-05-08 14:52:47,2012-05-08 14:52:47,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1202,b'Business frequencies error in tsplot',"b""Should alias these to their calendar frequencies for compatibility, or something\r\n\r\n```\r\n/home/wesm/code/pandas/pandas/tseries/plotting.py in tsplot(series, plotf, *args, **kwargs)\r\n     77     # Convert DatetimeIndex to PeriodIndex\r\n     78     if isinstance(series.index, DatetimeIndex):\r\n---> 79         idx = series.index.to_period(freq=freq)\r\n     80         series = Series(series.values, idx, name=series.name)\r\n     81 \r\n\r\n/home/wesm/code/pandas/pandas/tseries/index.pyc in to_period(self, freq)\r\n    528             freq = self.freq\r\n    529 \r\n--> 530         return PeriodIndex(self.values, freq=freq)\r\n    531 \r\n    532     def order(self, return_indexer=False, ascending=True):\r\n\r\n/home/wesm/code/pandas/pandas/tseries/period.pyc in __new__(cls, data, freq, start, end, periods, copy, name)\r\n    516             freq = freq.freq\r\n    517         else:\r\n--> 518             freq = _freq_mod.get_standard_freq(freq)\r\n    519 \r\n    520         if data is None:\r\n\r\n/home/wesm/code/pandas/pandas/tseries/frequencies.pyc in get_standard_freq(freq)\r\n    454         return get_offset_name(freq)\r\n    455 \r\n--> 456     code, stride = get_freq_code(freq)\r\n    457     return _get_freq_str(code, stride)\r\n    458 \r\n\r\n/home/wesm/code/pandas/pandas/tseries/frequencies.pyc in get_freq_code(freqstr)\r\n     64 \r\n     65     base, stride = _base_and_stride(freqstr)\r\n---> 66     code = _period_str_to_code(base)\r\n     67 \r\n     68     return code, stride\r\n\r\n/home/wesm/code/pandas/pandas/tseries/frequencies.pyc in _period_str_to_code(freqstr)\r\n    646         return _period_code_map[freqstr]\r\n    647     except:\r\n--> 648         alias = _period_alias_dict[freqstr]\r\n    649         try:\r\n    650             return _period_code_map[alias]\r\n\r\nKeyError: 'BM'\r\n```"""
1201,4454896,wesm,wesm,2012-05-07 14:21:18,2012-08-16 03:14:10,2012-07-12 21:36:26,closed,,0.8.1,5,Bug,https://api.github.com/repos/pydata/pandas/issues/1201,"b"".ix indexing can't handle duplicate index values""","b""```\r\n(Pdb) result\r\n      A   B   C   D\r\nfoo   2   3   4   5\r\nbar   7   8   9  10\r\nbaz  12  13  14  15\r\nqux  12  13  14  15\r\nfoo  12  13  14  15\r\nbar  12  13  14  15\r\n(Pdb) result.ix['bar']\r\n*** Exception: Reindexing only valid with uniquely valued Index objects\r\n```"""
1200,4453175,candre717,wesm,2012-05-07 12:32:10,2013-12-04 00:43:23,2012-05-18 19:24:50,closed,,0.8.0,1,Bug;Build,https://api.github.com/repos/pydata/pandas/issues/1200,b'failed tests',"b'I\'m running on Windows 7 32-bit, with Python 2.7.2. Dependencies installed, pandas cloned and built. \r\n\r\nHere is the output when I run ""nosetests pandas"" :\r\n\r\n................SS.........S...............S.................C:\\Python27\\lib\\site-packages\\scikits\\statsmodels\\tools\\tools.py:256: FutureWarning: The default of `prepend` will be changed to True in the next release, use explicit prepend\r\n  ""next release, use explicit prepend"", FutureWarning)\r\n...........S...................EEEEEE...............SS.....................C:\\Users\\candre4\\pandas\\pandas\\core\\index.py:441: RuntimeWarning: tp_compare didn\'t return -1 or -2 for exception\r\n  return self.view(np.ndarray).argsort(*args, **kwargs)\r\n.........S.................................S.......................S......\r\n=================================\r\nERROR: test_line_plot_datetime_frame (pandas.tseries.tests.test_plotting.TestTSPlot)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 82, in test_line_plot_datetime_frame\r\n    _check_plot_works(df.plot, freq)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 118, in _check_plot_works\r\n    plt.savefig(PNG_PATH)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.py"", line 363, in savefig\r\n    return fig.savefig(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 1084, in savefig\r\n    self.canvas.print_figure(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backend_bases.py"", line 1923, in print_figure\r\n    **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 438, in print_png\r\n    FigureCanvasAgg.draw(self)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 394, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 798, in draw\r\n    func(*args)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axes.py"", line 1946, in draw\r\n    a.draw(renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 971, in draw\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 907, in iter_ticks\r\n    majorLabels = [self.major.formatter(val, i) for i, val in enumerate(majorLocs)]\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\plotting.py"", line 812, in __call__\r\n    return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\period.py"", line 101, in __init__\r\n    raise ValueError(""Ordinal must be positive"")\r\nValueError: Ordinal must be positive\r\n\r\n======================================================================\r\nERROR: test_line_plot_datetime_series (pandas.tseries.tests.test_plotting.TestTSPlot)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 71, in test_line_plot_datetime_series\r\n    _check_plot_works(s.plot, s.index.freq.rule_code)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 118, in _check_plot_works\r\n    plt.savefig(PNG_PATH)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.py"", line 363, in savefig\r\n    return fig.savefig(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 1084, in savefig\r\n    self.canvas.print_figure(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backend_bases.py"", line 1923, in print_figure\r\n    **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 438, in print_png\r\n    FigureCanvasAgg.draw(self)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 394, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 798, in draw\r\n    func(*args)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axes.py"", line 1946, in draw\r\n    a.draw(renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 971, in draw\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 907, in iter_ticks\r\n    majorLabels = [self.major.formatter(val, i) for i, val in enumerate(majorLocs)]\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\plotting.py"", line 812, in __call__\r\n    return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\period.py"", line 101, in __init__\r\n    raise ValueError(""Ordinal must be positive"")\r\nValueError: Ordinal must be positive\r\n\r\n======================================================================\r\nERROR: test_line_plot_inferred_freq (pandas.tseries.tests.test_plotting.TestTSPlot)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 88, in test_line_plot_inferred_freq\r\n    _check_plot_works(ser.plot, ser.index.inferred_freq)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 118, in _check_plot_works\r\n    plt.savefig(PNG_PATH)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.py"", line 363, in savefig\r\n    return fig.savefig(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 1084, in savefig\r\n    self.canvas.print_figure(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backend_bases.py"", line 1923, in print_figure\r\n    **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 438, in print_png\r\n    FigureCanvasAgg.draw(self)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 394, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 798, in draw\r\n    func(*args)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axes.py"", line 1946, in draw\r\n    a.draw(renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 971, in draw\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 907, in iter_ticks\r\n    majorLabels = [self.major.formatter(val, i) for i, val in enumerate(majorLocs)]\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\plotting.py"", line 812, in __call__\r\n    return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\period.py"", line 101, in __init__\r\n    raise ValueError(""Ordinal must be positive"")\r\nValueError: Ordinal must be positive\r\n\r\n======================================================================\r\nERROR: test_line_plot_period_frame (pandas.tseries.tests.test_plotting.TestTSPlot)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 76, in test_line_plot_period_frame\r\n    _check_plot_works(df.plot, df.index.freq)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 118, in _check_plot_works\r\n    plt.savefig(PNG_PATH)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.py"", line 363, in savefig\r\n    return fig.savefig(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 1084, in savefig\r\n    self.canvas.print_figure(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backend_bases.py"", line 1923, in print_figure\r\n    **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 438, in print_png\r\n    FigureCanvasAgg.draw(self)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 394, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 798, in draw\r\n    func(*args)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axes.py"", line 1946, in draw\r\n    a.draw(renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 971, in draw\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 907, in iter_ticks\r\n    majorLabels = [self.major.formatter(val, i) for i, val in enumerate(majorLocs)]\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\plotting.py"", line 812, in __call__\r\n    return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\period.py"", line 101, in __init__\r\n    raise ValueError(""Ordinal must be positive"")\r\nValueError: Ordinal must be positive\r\n\r\n======================================================================\r\nERROR: test_line_plot_period_series (pandas.tseries.tests.test_plotting.TestTSPlot)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 66, in test_line_plot_period_series\r\n    _check_plot_works(s.plot, s.index.freq)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 118, in _check_plot_works\r\n    plt.savefig(PNG_PATH)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.py"", line 363, in savefig\r\n    return fig.savefig(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 1084, in savefig\r\n    self.canvas.print_figure(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backend_bases.py"", line 1923, in print_figure\r\n    **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 438, in print_png\r\n    FigureCanvasAgg.draw(self)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 394, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 798, in draw\r\n    func(*args)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axes.py"", line 1946, in draw\r\n    a.draw(renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 971, in draw\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 907, in iter_ticks\r\n    majorLabels = [self.major.formatter(val, i) for i, val in enumerate(majorLocs)]\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\plotting.py"", line 812, in __call__\r\n    return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\period.py"", line 101, in __init__\r\n    raise ValueError(""Ordinal must be positive"")\r\nValueError: Ordinal must be positive\r\n\r\n======================================================================\r\nERROR: test_tsplot (pandas.tseries.tests.test_plotting.TestTSPlot)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 59, in test_tsplot\r\n    _check_plot_works(f, s.index.freq, ax=ax, series=s)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\tests\\test_plotting.py"", line 118, in _check_plot_works\r\n    plt.savefig(PNG_PATH)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.py"", line 363, in savefig\r\n    return fig.savefig(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 1084, in savefig\r\n    self.canvas.print_figure(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backend_bases.py"", line 1923, in print_figure\r\n    **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 438, in print_png\r\n    FigureCanvasAgg.draw(self)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py"", line 394, in draw\r\n    self.figure.draw(self.renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\figure.py"", line 798, in draw\r\n    func(*args)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axes.py"", line 1946, in draw\r\n    a.draw(renderer)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\artist.py"", line 55, in draw_wrapper\r\n    draw(artist, renderer, *args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 971, in draw\r\n    tick_tups = [ t for t in self.iter_ticks()]\r\n  File ""C:\\Python27\\lib\\site-packages\\matplotlib\\axis.py"", line 907, in iter_ticks\r\n    majorLabels = [self.major.formatter(val, i) for i, val in enumerate(majorLocs)]\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\plotting.py"", line 812, in __call__\r\n    return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\r\n  File ""C:\\Users\\candre4\\pandas\\pandas\\tseries\\period.py"", line 101, in __init__\r\n    raise ValueError(""Ordinal must be positive"")\r\nValueError: Ordinal must be positive\r\n\r\n----------------------------------------------------------------------\r\nRan 1743 tests in 60.531s\r\n\r\nFAILED (SKIP=10, errors=6)\r\n\r\n'"
1197,4427084,changhiskhan,wesm,2012-05-04 16:33:52,2012-05-07 14:47:10,2012-05-07 14:47:10,closed,changhiskhan,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1197,b'Add limit keyword to fillna method of sparse data frame',
1196,4423581,changhiskhan,changhiskhan,2012-05-04 13:25:02,2012-05-04 14:33:21,2012-05-04 14:33:21,closed,changhiskhan,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1196,b'DataFrame.take raises exception on list input',"b""In [24]: df.take([1,2])\r\n\r\n/pandas/pandas/core/common.pyc in _ensure_int32(arr)\r\n    687 \r\n    688 def _ensure_int32(arr):\r\n--> 689     if arr.dtype != np.int32:\r\n    690         arr = arr.astype(np.int32)\r\n    691     return arr\r\n\r\nAttributeError: 'list' object has no attribute 'dtype'\r\n"""
1193,4411935,wesm,wesm,2012-05-03 19:29:50,2012-05-03 19:52:28,2012-05-03 19:52:28,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1193,b'logy argument not respected in pandas.tseries.plotting',
1187,4403739,changhiskhan,wesm,2012-05-03 12:10:01,2012-05-08 19:29:13,2012-05-08 19:29:13,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1187,"b'reindexing / ""\'Index\' object has no attribute \'freq\'""'","b'from pystatsmodels mailing list:\r\n\r\nI\'ve created a DataFrame using read_clipboard() (Pandas built from Git trunk) and am having problems reindexing unless I select rows using the [:] notation.  IIRC the index was defined using \'set_index(""Timestamp"")\'.  \r\n\r\nIn [47]:\r\n\r\ndf\r\nOut[47]:\r\n<class \'pandas.core.frame.DataFrame\'>\r\nIndex: 61507 entries, 2010-01-31 15:30:00 to 2011-11-03 09:15:00\r\nData columns:\r\nQ      61507  non-null values\r\nNTU    61507  non-null values\r\ndtypes: float64(2)\r\n\r\nIn [50]:\r\n\r\ndf[:50000000][\'Q\'] < 14\r\nOut[50]:\r\ntime\r\n2010-01-31 15:30:00     True\r\n2010-01-31 15:45:00     True\r\n2010-01-31 16:00:00     True\r\n2010-01-31 16:15:00    False\r\n2010-01-31 16:30:00    False\r\n<<snip; all fine here>>\r\n\r\nIn [51]:\r\n\r\ndf[\'Q\'] < 14\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/home/will/Dropbox/PhD/AI_general/Semblance/<ipython-input-51-2f53055c9488> in <module>()\r\n----> 1 q_and_turb_df[\'Q\'] < 14\r\n\r\n/usr/lib/python2.7/dist-packages/IPython/core/displayhook.pyc in __call__(self, result)\r\n    236             self.start_displayhook()\r\n    237             self.write_output_prompt()\r\n--> 238             format_dict = self.compute_format_data(result)\r\n    239             self.write_format_data(format_dict)\r\n    240             self.update_user_ns(result)\r\n\r\n/usr/lib/python2.7/dist-packages/IPython/core/displayhook.pyc in compute_format_data(self, result)\r\n    148             MIME type representation of the object.\r\n    149         """"""\r\n--> 150         return self.shell.display_formatter.format(result)\r\n    151 \r\n    152     def write_format_data(self, format_dict):\r\n\r\n/usr/lib/python2.7/dist-packages/IPython/core/formatters.pyc in format(self, obj, include, exclude)\r\n    124                     continue\r\n    125             try:\r\n--> 126                 data = formatter(obj)\r\n    127             except:\r\n    128                 # FIXME: log the exception\r\n\r\n\r\n/usr/lib/python2.7/dist-packages/IPython/core/formatters.pyc in __call__(self, obj)\r\n    445                 type_pprinters=self.type_printers,\r\n    446                 deferred_pprinters=self.deferred_printers)\r\n--> 447             printer.pretty(obj)\r\n    448             printer.flush()\r\n    449             return stream.getvalue()\r\n\r\n/usr/lib/python2.7/dist-packages/IPython/lib/pretty.pyc in pretty(self, obj)\r\n    352                 if callable(obj_class._repr_pretty_):\r\n    353                     return obj_class._repr_pretty_(obj, self, cycle)\r\n--> 354             return _default_pprint(obj, self, cycle)\r\n    355         finally:\r\n    356             self.end_group()\r\n\r\n/usr/lib/python2.7/dist-packages/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)\r\n    472     if getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\r\n    473         # A user-provided repr.\r\n\r\n--> 474         p.text(repr(obj))\r\n    475         return\r\n    476     p.begin_group(1, \'<\')\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_1ea766c-py2.7-linux-x86_64.egg/pandas/core/series.pyc in __repr__(self)\r\n    710                     else fmt.print_config.max_rows)\r\n    711         if len(self.index) > max_rows:\r\n--> 712             result = self._tidy_repr(min(30, max_rows - 4))\r\n    713         elif len(self.index) > 0:\r\n    714             result = self._get_repr(print_header=True,\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_1ea766c-py2.7-linux-x86_64.egg/pandas/core/series.pyc in _tidy_repr(self, max_vals)\r\n    728                                                   name=False)\r\n    729         result = head + \'\\n...\\n\' + tail\r\n--> 730         return \'%s\\n%s\' % (result, self._repr_footer())\r\n    731 \r\n    732     def _repr_footer(self):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_1ea766c-py2.7-linux-x86_64.egg/pandas/core/series.pyc in _repr_footer(self)\r\n   2551 \r\n   2552     def _repr_footer(self):\r\n-> 2553         if self.index.freq is not None:\r\n   2554             freqstr = \'Freq: %s, \' % self.index.freqstr\r\n   2555         else:\r\n\r\nAttributeError: \'Index\' object has no attribute \'freq\'\r\n'"
1183,4390730,wesm,wesm,2012-05-02 18:38:53,2012-05-07 18:23:41,2012-05-07 18:23:41,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1183,b'Series.apply bug when result same length as series',"b'```\r\nIn [2]: s = pandas.Series([1,2])\r\n\r\nIn [3]: s\r\nOut[3]: \r\n0    1\r\n1    2\r\n\r\nIn [4]: s.apply(lambda x: (x, x+1))\r\nOut[4]: \r\n0    0    1\r\n1    2\r\n1    0    2\r\n1    3\r\n```\r\n\r\nBut:\r\n\r\n```\r\nIn [8]: s = pandas.Series([1,2,3])\r\n\r\nIn [9]: s\r\nOut[9]: \r\n0    1\r\n1    2\r\n2    3\r\n\r\nIn [10]: s.apply(lambda x: (x, x+1))\r\nOut[10]: \r\n0    (1, 2)\r\n1    (2, 3)\r\n2    (3, 4)\r\n```'"
1181,4389620,wesm,wesm,2012-05-02 17:39:01,2012-05-09 17:54:02,2012-05-08 18:27:05,closed,wesm,0.8.0,10,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1181,"b""Don't create (or add option to not create) empty columns when unstacking""","b""from @rkern on the pydata mailing list:\r\n```\r\nHello! I am using the pivot table functionality to good effect. One\r\nissue I am running to, however, is that the columns end up being\r\nroughly a full Cartesian product of the column pivots. That is, there\r\nare blocks of all-NA columns. The row pivots appear to be pruned\r\nalready; all-NA rows don't show up. For pivot values that are roughly\r\ntree-structured (i.e. the children of one node mostly don't show up as\r\nchildren of a neighboring node), this can create pivot tables with\r\nvery many columns. Currently, I post-process the pivot tables using\r\n.dropna(axis=1, how='all'), but I have just run into a case there the\r\nintermediate table is too large for my 32-bit machine. Would there be\r\na good way to change the pivot table computation to get appropriately\r\nsparse trees for both the rows and the columns? I am happy to look\r\ninto it myself, but I did want to check to see if it was on anyone's\r\nradar or if anyone had suggestions.\r\n\r\n```\r\n\r\nto my response\r\n\r\n```\r\nThat's actually a good point and it's basically an oversight in the pivot_table implementation. If you look at the code, it's basically a convenience function that uses groupby and calls unstack on the aggregated result. I suspect the problem with unstack is that it's creating lots of empty columns; \r\n```\r\n```"""
1172,4353314,wesm,wesm,2012-04-30 16:36:22,2012-05-05 20:18:16,2012-05-05 20:18:16,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1172,b'Should only store UTC timestamps in DatetimeIndex',"b'The time zone only affects what the user ""sees""; will make set ops and operations between different-timezone time series much more straightforward'"
1170,4343736,wesm,changhiskhan,2012-04-29 22:15:43,2012-05-03 01:20:06,2012-05-03 01:20:06,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1170,b'Dubious Period interpreting monthly date',"b""```\r\nIn [3]: Period(200701, freq='M')\r\nOut[3]: Period('Jan-16726', 'M')\r\n\r\nIn [4]: Period('200701', freq='M')\r\nOut[4]: Period('Jul-2020', 'M')\r\n```"""
1167,4343332,wesm,wesm,2012-04-29 21:12:14,2012-05-07 16:06:56,2012-05-07 16:06:56,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1167,b'Floats stored in object Index may not slice properly when passed float64 scalars',b'Should include this fix in 0.7.x maintenance branch'
1166,4342452,wesm,changhiskhan,2012-04-29 18:41:32,2012-05-03 01:20:17,2012-05-03 01:20:17,closed,,0.8.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1166,b'np.ptp does not produce scalar value from Series',
1165,4342153,wesm,wesm,2012-04-29 17:53:16,2012-05-12 21:28:11,2012-05-12 21:28:11,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1165,b'Minutely resampling needs to be independent of start timestamp',"b""Date ranges are very naively generated:\r\n\r\n```\r\nIn [44]: ts.resample('5min')\r\nOut[44]: \r\n2000-01-01 00:00:00   -0.744244\r\n2000-01-01 00:05:00   -0.934083\r\n2000-01-01 00:10:00    0.198829\r\n2000-01-01 00:15:00    1.786013\r\nFreq: 5T\r\n\r\nIn [45]: ts[2:].resample('5min')\r\nOut[45]: \r\n2000-01-01 00:02:00   -0.903645\r\n2000-01-01 00:07:00   -0.496863\r\n2000-01-01 00:12:00    0.461970\r\n2000-01-01 00:17:00         NaN\r\nFreq: 5T\r\n```"""
1163,4342082,wesm,changhiskhan,2012-04-29 17:39:46,2012-04-29 21:43:57,2012-04-29 21:43:57,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1163,"b""Frequency strings don't work with loffset parameter in resampling""","b""```\r\n\r\nIn [7]: ts\r\nOut[7]: \r\n2000-01-01 00:00:00   -1.135382\r\n2000-01-01 00:01:00    1.295421\r\n2000-01-01 00:02:00    0.625706\r\n2000-01-01 00:03:00    0.235525\r\n2000-01-01 00:04:00    0.254634\r\n2000-01-01 00:05:00   -0.393107\r\n2000-01-01 00:06:00   -1.881196\r\n2000-01-01 00:07:00   -2.447050\r\n2000-01-01 00:08:00    1.055716\r\n2000-01-01 00:09:00    1.156710\r\n2000-01-01 00:10:00   -1.108877\r\n2000-01-01 00:11:00   -0.710064\r\nFreq: T\r\n\r\nIn [8]: ts.resample('5min', loffset=datetools.Second(-1))\r\nOut[8]: \r\n1999-12-31 23:59:59   -1.135382\r\n2000-01-01 00:04:59    0.403636\r\n2000-01-01 00:09:59   -0.644939\r\n2000-01-01 00:14:59   -0.710064\r\nFreq: 5T\r\n\r\nIn [9]: ts.resample('5min', loffset='1min')\r\nOut[9]: \r\n2000-01-01 00:00:00   -1.135382\r\n2000-01-01 00:05:00    0.403636\r\n2000-01-01 00:10:00   -0.644939\r\n2000-01-01 00:15:00   -0.710064\r\nFreq: 5T\r\n```"""
1162,4342059,wesm,changhiskhan,2012-04-29 17:33:45,2012-05-01 16:54:55,2012-05-01 16:54:55,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1162,b'Negative numbers not handled correctly in frequency strings',"b""```\r\nIn [2]: datetools.to_offset('-1S')\r\nOut[2]: <1 Second>\r\n```"""
1157,4341199,wesm,changhiskhan,2012-04-29 14:51:45,2012-05-03 01:20:25,2012-05-03 01:20:25,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1157,"b""Series.shift doesn't work with PeriodIndex""",
1156,4338318,wesm,wesm,2012-04-29 00:17:09,2012-05-05 19:26:42,2012-05-05 19:26:42,closed,wesm,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1156,"b""Timestamps don't always get the right tzinfo attached""","b""Each timezone in pytz has a `_tzinfos` dict:\r\n\r\nIn [3]: type(pytz.timezone('US/Eastern'))\r\nOut[3]: pytz.tzfile.US/Eastern\r\n\r\nIn [4]: pytz.timezone('US/Eastern')._tzinfos\r\nOut[4]: \r\n{(datetime.timedelta(-1, 68400),\r\n  datetime.timedelta(0),\r\n  'EST'): <DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>,\r\n (datetime.timedelta(-1, 72000),\r\n  datetime.timedelta(0, 3600),\r\n  'EDT'): <DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>,\r\n (datetime.timedelta(-1, 72000),\r\n  datetime.timedelta(0, 3600),\r\n  'EPT'): <DstTzInfo 'US/Eastern' EPT-1 day, 20:00:00 DST>,\r\n (datetime.timedelta(-1, 72000),\r\n  datetime.timedelta(0, 3600),\r\n  'EWT'): <DstTzInfo 'US/Eastern' EWT-1 day, 20:00:00 DST>}\r\n\r\nNeed some kind of sane (read: performant) way to retrieve the right one of these to attach when boxing a timestamp. Is not as trivial as I thought"""
1155,4338312,wesm,wesm,2012-04-29 00:15:03,2012-05-05 19:30:52,2012-05-05 19:30:52,closed,,0.8.0,1,Bug;Testing;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1155,b'Time zone conversion borked',"b'I did some refactoring recently and probably broke this myself, but there was no unit test to catch it. To reproduce: convert from UTC to local around a DST transition'"
1151,4337977,FedericoV,jreback,2012-04-28 22:43:53,2013-09-20 18:09:22,2013-09-20 18:09:22,closed,jtratner,0.13,1,Bug;Data IO;Sparse,https://api.github.com/repos/pydata/pandas/issues/1151,b'Pickling sparse dataframes with hierarchical indexing results in loss of data',"b""Hi,\r\n\r\nthanks for all the awesome work with Pandas.  I just started working on it, and I ran into two distinct bugs:\r\n\r\n- The HDFStore function won't accept sparse_dataframes as objects - doing so raises a KeyError.\r\n- If you pickle a sparse data frame with a hierarchical index, then reload the matrix, the resulting matrix loses all hierarchical indexing information and all the indexes are collapsed as tuples.\r\n\r\nI am using the latest version of pandas on Ubuntu 12.04 with the latest version of EPD.\r\n\r\nFederico"""
1142,4314751,vincentarelbundock,wesm,2012-04-27 01:49:47,2012-05-07 15:43:33,2012-05-07 15:36:15,closed,wesm,0.8.0,9,Bug,https://api.github.com/repos/pydata/pandas/issues/1142,b'.ix column assignment',"b'Hi, \r\n\r\nI was wondering if there was a good reason why this works: \r\n\r\n```python\r\ndat.ix[:,2] + 2\r\n```\r\n\r\nbut not this: \r\n\r\n```python\r\ndat.ix[:,2] = dat.ix[:,2] + 2\r\n```'"
1138,4309981,wesm,wesm,2012-04-26 19:43:25,2012-04-27 21:52:56,2012-04-27 21:52:56,closed,,0.8.0,0,Bug;Enhancement;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1138,b'Test joins and set ops with PeriodIndex-ed data',"b""```\r\nIn [5]: idx\r\nOut[5]: \r\n<class 'pandas.tseries.period.PeriodIndex'>\r\nfreq: D\r\n[01-Jan-2000, ..., 20-Jan-2000]\r\nlength: 20\r\n\r\nIn [6]: idx.join(idx[:-5], how='inner')\r\nOut[6]: \r\nInt64Index([730120, 730121, 730122, 730123, 730124, 730125, 730126, 730127,\r\n       730128, 730129, 730130, 730131, 730132, 730133, 730134])\r\n\r\nIn [7]: idx.union(idx[:-5])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/wesm/code/pandas/<ipython-input-7-e8f9b3613a4d> in <module>()\r\n----> 1 idx.union(idx[:-5])\r\n\r\n/home/wesm/code/pandas/pandas/core/index.pyc in union(self, other)\r\n    496 \r\n    497         # for subclasses\r\n--> 498         return self._wrap_union_result(other, result)\r\n    499 \r\n    500     def _wrap_union_result(self, other, result):\r\n\r\n/home/wesm/code/pandas/pandas/core/index.pyc in _wrap_union_result(self, other, result)\r\n    500     def _wrap_union_result(self, other, result):\r\n    501         name = self.name if self.name == other.name else None\r\n--> 502         return type(self)(data=result, name=name)\r\n    503 \r\n    504     def intersection(self, other):\r\n\r\n/home/wesm/code/pandas/pandas/tseries/period.pyc in __new__(cls, data, freq, start, end, periods, copy, name)\r\n    518             else:\r\n    519                 if freq is None:\r\n--> 520                     raise ValueError('freq cannot be none')\r\n    521 \r\n    522                 if data.dtype == np.datetime64:\r\n\r\nValueError: freq cannot be none\r\n```"""
1137,4308448,mrjbq7,wesm,2012-04-26 18:14:37,2012-06-21 22:44:55,2012-06-21 22:44:55,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1137,b'Different types returned for numpy 1.5.1 and 1.6.1',"b'I noticed that certain ``numpy`` functions return ``pandas.Series`` in 1.5.1 and ``numpy.array`` in 1.6.1.  This makes it hard to write code that works for both versions when the index is not ``0..n``, but something else like timestamps.\r\n\r\nFor example on Mac OS X 10.7.3 with numpy 1.5.1:\r\n\r\n```python\r\nPython 2.7.1 (r271:86832, Jun 25 2011, 05:09:01) \r\n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\n>>> import numpy\r\n>>> numpy.__version__\r\n\'1.5.1\'\r\n\r\n>>> import pandas\r\n>>> pandas.__version__\r\n\'0.7.3\'\r\n\r\n>>> a = numpy.array(range(10))\r\n>>> s = pandas.Series(a)\r\n\r\n>>> numpy.where(~numpy.isnan(s))\r\n(0    0\r\n1    1\r\n2    2\r\n3    3\r\n4    4\r\n5    5\r\n6    6\r\n7    7\r\n8    8\r\n9    9,)\r\n\r\n>>> type(numpy.where(~numpy.isnan(s))[0])\r\n<class \'pandas.core.series.Series\'>\r\n```\r\n\r\nversus same system with numpy 1.6.1\r\n\r\n```python\r\nPython 2.7.1 (r271:86832, Jun 25 2011, 05:09:01) \r\n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\n>>> import numpy\r\n>>> numpy.__version__\r\n\'1.6.1\'\r\n\r\n>>> import pandas\r\n>>> pandas.__version__\r\n\'0.7.3\'\r\n\r\n>>> a = numpy.array(range(10))\r\n>>> s = pandas.Series(a)\r\n\r\n>>> numpy.where(~numpy.isnan(s))\r\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),)\r\n\r\n>>> type(numpy.where(~numpy.isnan(s))[0])\r\n<type \'numpy.ndarray\'>\r\n```'"
1129,4265400,wesm,wesm,2012-04-24 17:57:36,2012-04-27 15:38:42,2012-04-27 15:38:42,closed,,0.8.0,0,Bug;Enhancement;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1129,b'Series/DataFrame.asfreq not implemented for PeriodIndex',b'This is not a surprise but needs to be fixed'
1126,4261760,wesm,changhiskhan,2012-04-24 15:17:49,2012-05-01 20:31:05,2012-05-01 20:31:05,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1126,b'Lost PeriodIndex plotting support',b'Lost it during the merge and the plot refactor. All that code (pandas/tseries/plotting.py) needs to be cleaned up'
1125,4256643,lodagro,wesm,2012-04-24 09:51:41,2012-04-26 21:39:35,2012-04-26 21:39:35,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1125,b'failing unittests on 32b linux machine',"b'Just did a fresh install of pandas on a 32bits linux machine (ubuntu), i suppose this is a supported platform.\r\nRunning nosetests shows many failures (one example at the  bottom).\r\n\r\nnumpy 1.6.1\r\nCython 0.16\r\npandas from master\r\n\r\nA lot of them are related to numpy issue:\r\n\r\n```python\r\nIn [11]: values = np.random.randn(100000, 5)\r\n\r\nIn [12]: values.take(np.arange(10, dtype=\'int32\'))\r\nOut[12]: \r\narray([-0.49410797, -0.24884862,  0.76527299, -0.96366541, -2.64132367,\r\n        0.21082825, -0.09124413, -0.89438207, -0.16012725,  0.20569685])\r\n\r\nIn [13]: values.take(np.arange(10, dtype=\'int64\'))\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/home/lodagro/projects/pandas/<ipython-input-13-6ad7fa396ff4> in <module>()\r\n----> 1 values.take(np.arange(10, dtype=\'int64\'))\r\n\r\nTypeError: array cannot be safely cast to required type\r\n\r\nIn [14]: \r\n```\r\n\r\n```\r\n======================================================================\r\nERROR: test_append (pandas.sparse.tests.test_sparse.TestSparseDataFrame)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/lodagro/.virtualenvs/pandas/lib/python2.7/site-packages/pandas-0.8.0.dev_789300e-py2.7-linux-i686.egg/pandas/sparse/tests/test_sparse.py"", line 1014, in test_append\r\n    a = self.frame[:5]\r\n  File ""/home/lodagro/.virtualenvs/pandas/lib/python2.7/site-packages/pandas-0.8.0.dev_789300e-py2.7-linux-i686.egg/pandas/sparse/frame.py"", line 318, in __getitem__\r\n    return self.reindex(date_rng)\r\n  File ""/home/lodagro/.virtualenvs/pandas/lib/python2.7/site-packages/pandas-0.8.0.dev_789300e-py2.7-linux-i686.egg/pandas/core/frame.py"", line 1916, in reindex\r\n    fill_value, limit)\r\n  File ""/home/lodagro/.virtualenvs/pandas/lib/python2.7/site-packages/pandas-0.8.0.dev_789300e-py2.7-linux-i686.egg/pandas/sparse/frame.py"", line 511, in _reindex_index\r\n    new = values.take(indexer)\r\nTypeError: array cannot be safely cast to required type\r\n```'"
1123,4250962,takluyver,takluyver,2012-04-23 23:08:34,2012-04-24 11:13:37,2012-04-24 11:13:37,closed,,,3,Bug;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1123,b'Integer overflow with period_asfreq on 32-bit system',"b'This causes a lot of test failures on my system.\r\n\r\n```\r\nIn [4]: ival_H = Period(freq=\'H\', year=2007, month=1, day=1, hour=0)\r\n\r\nIn [5]: ival_H.asfreq(\'S\',\'S\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/thomas/Code/<ipython-input-5-1380e902ce36> in <module>()\r\n----> 1 ival_H.asfreq(\'S\',\'S\')\r\n\r\n/home/thomas/Code/virtualenvs/pandas-py2/local/lib/python2.7/site-packages/pandas-0.8.0.dev_4bf61b5-py2.7-linux-i686.egg/pandas/tseries/period.pyc in asfreq(self, freq, how)\r\n    168                                         base2, mult2, py3compat.str_to_bytes(how))\r\n    169 \r\n--> 170         return Period(new_ordinal, (base2, mult2))\r\n    171 \r\n    172     def start_time(self):\r\n\r\n/home/thomas/Code/virtualenvs/pandas-py2/local/lib/python2.7/site-packages/pandas-0.8.0.dev_4bf61b5-py2.7-linux-i686.egg/pandas/tseries/period.pyc in __init__(self, value, freq, year, month, quarter, day, hour, minute, second)\r\n    112         elif isinstance(value, (int, long, np.integer)):\r\n    113             if value <= 0:\r\n--> 114                 raise ValueError(""Value must be positive"")\r\n    115             self.ordinal = value\r\n    116             if freq is None:\r\n\r\nValueError: Value must be positive\r\n```\r\n\r\nThe C code is ultimately doing `ival_H.ordinal * 60 * 60`, and trying to store the result in a `long` - which according to [this table](http://en.wikipedia.org/wiki/Integer_%28computer_science%29#Common_long_integer_sizes) will be 32 bits on my system, and therefore isn\'t enough to hold it. From the debugger, `lib.period_asfreq()` returns -1121299440, which is equal to `(ival_H.ordinal*60*60 & sys.maxsize) - sys.maxsize - 1`.\r\n\r\nI\'ve tried substituing `long long` or `int64_t`, but the problem persisted, so I must be missing something.'"
1118,4248347,wesm,wesm,2012-04-23 20:19:39,2012-04-27 17:48:37,2012-04-27 17:48:37,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1118,b'PeriodIndex constructor should take frequency from start period if has one',"b""EDIT: actually just need a better error message\r\n\r\n```\r\nIn [10]: PeriodIndex(val, periods=20)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/Users/wesm/code/pandas/<ipython-input-10-e39b42220b85> in <module>()\r\n----> 1 PeriodIndex(val, periods=20)\r\n\r\n/Users/wesm/code/pandas/pandas/tseries/period.pyc in __new__(cls, data, freq, start, end, periods, copy, name)\r\n    568 \r\n    569             if freq is None:\r\n--> 570                 raise ValueError('freq cannot be none')\r\n    571 \r\n    572             data = _period_unbox_array(data, check=freq)\r\n\r\nValueError: freq cannot be none\r\n\r\nIn [11]: val\r\nOut[11]: Period('02-Apr-2012', 'B')\r\n```"""
1117,4248211,wesm,wesm,2012-04-23 20:12:05,2012-04-26 19:34:51,2012-04-26 19:34:51,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1117,b'Implement take on PeriodIndex',"b""```\r\nIn [5]: rng\r\nOut[5]: \r\n<class 'pandas.tseries.period.PeriodIndex'>\r\nfreq: B\r\n[28-Dec-1999, ..., 02-Jan-2001]\r\nlength: 54\r\n\r\nIn [6]: rng.take(np.random.permutation(len(rng)))\r\nOut[6]: \r\nInt64Index([521672, 521592, 521572, 521587, 521732, 521712, 521512, 521627,\r\n       521552, 521582, 521717, 521752, 521737, 521777, 521647, 521517,\r\n       521597, 521677, 521722, 521622, 521542, 521652, 521682, 521707,\r\n       521632, 521612, 521577, 521772, 521637, 521547, 521557, 521532,\r\n       521667, 521617, 521747, 521527, 521742, 521562, 521607, 521522,\r\n       521642, 521657, 521537, 521662, 521727, 521692, 521702, 521567,\r\n       521697, 521767, 521757, 521602, 521687, 521762])\r\n```"""
1116,4247876,wesm,wesm,2012-04-23 19:52:57,2012-04-26 22:58:14,2012-04-26 22:58:14,closed,,0.8.0,0,Bug;Enhancement;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1116,b'Test/fix Period.to_timestamp issues',"b'Should conform with new PeriodIndex.to_timestamp, add lots of test for annual, quarterly, other frequencies\r\n\r\n```\r\n    def start_time(self):\r\n        return self.to_timestamp(which_end=\'S\')\r\n\r\n    def end_time(self):\r\n        return self.to_timestamp(which_end=\'E\')\r\n\r\n    def to_timestamp(self, which_end=\'S\'):\r\n        """"""\r\n        Return the Timestamp at the start/end of the period\r\n\r\n        Parameters\r\n        ----------\r\n        which_end: str, default \'S\' (start)\r\n            \'S\', \'E\'. Can be aliased as case insensitive\r\n            \'Start\', \'Finish\', \'Begin\', \'End\'\r\n\r\n        Returns\r\n        -------\r\n        Timestamp\r\n        """"""\r\n        which_end = _validate_end_alias(which_end)\r\n        new_val = self.asfreq(\'S\', which_end)\r\n        base, mult = _gfc(new_val.freq)\r\n        return Timestamp(lib.period_ordinal_to_dt64(new_val.ordinal, base, mult))\r\n```'"
1113,4247704,wesm,wesm,2012-04-23 19:44:17,2012-04-26 23:04:23,2012-04-26 23:04:23,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1113,"b""Check that quarterly time rules don't normalize away time of day""",
1112,4246195,wesm,wesm,2012-04-23 18:19:52,2012-04-23 19:54:24,2012-04-23 19:54:24,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1112,b'Poor weekofyear performance',"b""```\r\nIn [14]: rng\r\nOut[14]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\nfreq: <1 Day>, timezone: None\r\n[1800-01-01 00:00:00, ..., 2000-01-01 00:00:00]\r\nlength: 73049\r\n\r\nIn [15]: timeit rng.weekofyear\r\n1 loops, best of 3: 501 ms per loop\r\n\r\nIn [16]: timeit rng.year\r\n100 loops, best of 3: 3.48 ms per loop\r\n```"""
1110,4245758,wesm,wesm,2012-04-23 17:54:08,2012-04-25 20:32:03,2012-04-25 20:32:03,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1110,b'DatetimeIndex.repeat needs to be overridden',"b""```\r\n(Pdb) rng\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\nfreq: <1 Day>, timezone: None\r\n[2000-01-01 00:00:00, ..., 2000-01-31 00:00:00]\r\nlength: 31\r\n(Pdb) rng2 = rng.repeat(5)\r\n(Pdb) rng2\r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\nfreq: <1 Day>, timezone: None\r\n[2000-01-01 00:00:00, ..., 2000-01-31 00:00:00]\r\nlength: 155\r\n```"""
1107,4235731,lodagro,wesm,2012-04-23 07:29:07,2012-04-23 15:59:59,2012-04-23 15:54:32,closed,,0.8.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/1107,"b""nosetests  pandas.tseries.tests.test_resample 'double free or corruption'""","b""```\r\n(pandas)[1837][n] nosetests pandas.tseries.tests.test_resample\r\n\r\n*** glibc detected *** ~/.virtualenvs/pandas/bin/python: double free or corruption (!prev): 0x0000000003d319f0 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6[0x3d7047245f]\r\n/lib64/libc.so.6(cfree+0x4b)[0x3d704728bb]\r\n~/.virtualenvs/pandas/lib/python2.7/site-packages/numpy/core/multiarray.so[0x2adc0147126f]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e40083]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ef9)[0x2adbf9eb00b9]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ed3)[0x2adbf9eb0093]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x54f6)[0x2adbf9eaf6b6]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e41522]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2adbf9eaaee9]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x54f6)[0x2adbf9eaf6b6]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ed3)[0x2adbf9eb0093]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e41522]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2adbf9eaaee9]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e41522]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e2918d]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e7115a]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1268)[0x2adbf9eab428]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ed3)[0x2adbf9eb0093]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e41522]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2adbf9eaaee9]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e41522]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e2918d]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e7115a]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1268)[0x2adbf9eab428]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2adbf9eb1722]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2adbf9e41522]\r\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2adbf9e1ad18]\r\n~/.virtualenvs/pandas/lib/libpyzsh: abort      nosetests pandas.tseries.tests.test_resample\r\n```\r\n\r\n```python\r\nIn [2]: pandas.__version__\r\nOut[2]: '0.8.0.dev-ce8629a'\r\n```\r\n\r\n```\r\nLSB Version:    :core-4.0-amd64:core-4.0-ia32:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-ia32:printing-4.0-noarch\r\nDistributor ID: RedHatEnterpriseServer\r\nDescription:    Red Hat Enterprise Linux Server release 5.6 (Tikanga)\r\nRelease:        5.6\r\nCodename:       Tikanga\r\n```\r\n"""
1101,4227136,wesm,wesm,2012-04-22 02:23:21,2012-04-22 04:49:15,2012-04-22 04:49:15,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1101,b'TimeSeries.take loses tz',"b""```\r\nIn [24]: ts2.index\r\nOut[24]: \r\n<class 'pandas.tseries.index.DatetimeIndex'>\r\nfreq: <1 Day>, timezone: US/Central\r\n[1990-01-01 00:00:00-06:00, ..., 2000-01-01 00:00:00-06:00]\r\nlength: 3653\r\n\r\nIn [25]: ts2.take(range(5)).index\r\nOut[25]: \r\nDatetimeIndex([1990-01-01 00:00:00, 1990-01-02 00:00:00, 1990-01-03 00:00:00,\r\n       1990-01-04 00:00:00, 1990-01-05 00:00:00], dtype=datetime64[us])\r\n```"""
1100,4227095,wesm,wesm,2012-04-22 02:07:48,2012-04-23 18:09:05,2012-04-23 18:08:57,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1100,b'Datetime formatting for year < 1900',"b'```\r\nIn [1]: rng = date_range(\'1/1/1800\', \'1/1/2000\')\r\n\r\nIn [2]: rng\r\nOut[2]: \r\n<class \'pandas.tseries.index.DatetimeIndex\'>\r\nfreq: <1 Day>, timezone: None\r\n[1800-01-01 00:00:00, ..., 2000-01-01 00:00:00]\r\nlength: 73049\r\n\r\nIn [3]: ts = Series(randn(len(rng)), index=rng)\r\n\r\nIn [4]: ts\r\nOut[4]: ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/Users/wesm/code/pandas/<ipython-input-4-a03e77341a8b> in <module>()\r\n----> 1 ts\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/displayhook.pyc in __call__(self, result)\r\n    236             self.start_displayhook()\r\n    237             self.write_output_prompt()\r\n--> 238             format_dict = self.compute_format_data(result)\r\n    239             self.write_format_data(format_dict)\r\n    240             self.update_user_ns(result)\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/displayhook.pyc in compute_format_data(self, result)\r\n    148             MIME type representation of the object.\r\n    149         """"""\r\n--> 150         return self.shell.display_formatter.format(result)\r\n    151 \r\n    152     def write_format_data(self, format_dict):\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/formatters.pyc in format(self, obj, include, exclude)\r\n    124                     continue\r\n    125             try:\r\n--> 126                 data = formatter(obj)\r\n    127             except:\r\n    128                 # FIXME: log the exception\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/formatters.pyc in __call__(self, obj)\r\n    445                 type_pprinters=self.type_printers,\r\n    446                 deferred_pprinters=self.deferred_printers)\r\n--> 447             printer.pretty(obj)\r\n    448             printer.flush()\r\n    449             return stream.getvalue()\r\n\r\n/Users/wesm/code/repos/ipython/IPython/lib/pretty.pyc in pretty(self, obj)\r\n    358                             if callable(meth):\r\n    359                                 return meth(obj, self, cycle)\r\n--> 360             return _default_pprint(obj, self, cycle)\r\n    361         finally:\r\n    362             self.end_group()\r\n\r\n/Users/wesm/code/repos/ipython/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)\r\n    478     if getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\r\n    479         # A user-provided repr.\r\n--> 480         p.text(repr(obj))\r\n    481         return\r\n    482     p.begin_group(1, \'<\')\r\n\r\n/Users/wesm/code/pandas/pandas/core/series.pyc in __repr__(self)\r\n    699                     else fmt.print_config.max_rows)\r\n    700         if len(self.index) > max_rows:\r\n--> 701             result = self._tidy_repr(min(30, max_rows - 4))\r\n    702         elif len(self.index) > 0:\r\n    703             result = self._get_repr(print_header=True,\r\n\r\n/Users/wesm/code/pandas/pandas/core/series.pyc in _tidy_repr(self, max_vals)\r\n    712         num = max_vals // 2\r\n    713         head = self[:num]._get_repr(print_header=True, length=False,\r\n--> 714                                     name=False)\r\n    715         tail = self[-(max_vals - num):]._get_repr(print_header=False,\r\n    716                                                   length=False,\r\n\r\n/Users/wesm/code/pandas/pandas/core/series.pyc in _get_repr(self, name, print_header, length, na_rep, float_format)\r\n    762                                         length=length, na_rep=na_rep,\r\n    763                                         float_format=float_format)\r\n--> 764         return formatter.to_string()\r\n    765 \r\n    766     def __str__(self):\r\n\r\n/Users/wesm/code/pandas/pandas/core/format.pyc in to_string(self)\r\n     98             return \'\'\r\n     99 \r\n--> 100         fmt_index, have_header = self._get_formatted_index()\r\n    101         fmt_values = self._get_formatted_values()\r\n    102 \r\n\r\n/Users/wesm/code/pandas/pandas/core/format.pyc in _get_formatted_index(self)\r\n     84         else:\r\n     85             have_header = index.name is not None\r\n---> 86             fmt_index = index.format(name=True)\r\n     87         return fmt_index, have_header\r\n     88 \r\n\r\n/Users/wesm/code/pandas/pandas/core/index.pyc in format(self, name)\r\n    333                 if dt.time() != zero_time or dt.tzinfo is not None:\r\n    334                     return header + [\'%s\' % x for x in self]\r\n--> 335                 result.append(dt.strftime(""%Y-%m-%d""))\r\n    336             return header + result\r\n    337 \r\n\r\nValueError: year=1800 is before 1900; the datetime strftime() methods require year >= 1900\r\n```'"
1095,4217724,wesm,wesm,2012-04-20 20:38:32,2012-04-22 01:49:53,2012-04-22 01:49:53,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1095,b'Better error message when date_range improperly specified',"b'noticed in test failure:\r\n\r\n```\r\n======================================================================\r\nERROR: test_shift_multiple_of_same_base (__main__.TestLegacySupport)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""pandas/tests/test_timeseries.py"", line 624, in test_shift_multiple_of_same_base\r\n    ts = Series(np.random.randn(5), index=date_range(\'1/1/2000\', freq=\'H\'))\r\n  File ""/home/wesm/code/pandas/pandas/core/daterange.py"", line 65, in date_range\r\n    freq=freq, tz=tz, normalize=normalize)\r\n  File ""/home/wesm/code/pandas/pandas/core/index.py"", line 1279, in __new__\r\n    b, e = Timestamp(start), Timestamp(end)\r\n  File ""datetime.pyx"", line 59, in pandas._tseries.Timestamp.__new__ (pandas/src/tseries.c:27720)\r\nAttributeError: \'NoneType\' object has no attribute \'dtval\'\r\n\r\n----------------------------------------------------------------------\r\n```'"
1090,4213089,turkeytest,wesm,2012-04-20 15:42:53,2012-04-22 14:51:03,2012-04-22 14:51:03,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1090,b'negative variances',"b'Hello, \r\n\r\nIt seems possible to have negative variances due to numerical inaccuracies.  This is because nanops.py, line 120 does not take the absolute value of the result.  Having negative values will cause std() to return NaN when it should be 0.\r\n\r\nThe code below should [probabilistically] recreate the problem.  It could also be turned into a unit test. \r\n\r\nThanks!  \r\n\r\n\r\n   \r\n\r\nfrom pandas import DataFrame\r\nimport numpy as np\r\n\r\nrandom_repeated_rows = np.array( [np.random.random((10000,)),] * 10  )\r\nmy_var = DataFrame( random_repeated_rows ).var()\r\n\r\nlen( my_var[ my_var < 0 ] )                                                                           # returns a negative slightly less than half of the time \r\nnp.min( DataFrame( random_repeated_rows ).var() )                          # returns a tiny negative -9.8686491077791697e-16\r\nnp.min( DataFrame( random_repeated_rows ).values.var(axis=0) ) # returns 0\r\n    \r\n\r\n\r\n\r\n\r\n'"
1085,4201988,wesm,wesm,2012-04-19 22:34:34,2012-04-20 18:02:56,2012-04-20 18:02:56,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1085,b'Empty string -> NaT in to_datetime',"b""```\r\nIn [8]: to_datetime('')\r\nOut[8]: datetime.datetime(2012, 4, 19, 0, 0)\r\n```"""
1081,4188643,bmu,wesm,2012-04-19 10:00:10,2012-05-14 15:24:50,2012-05-14 15:24:50,closed,,0.8.0,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1081,b'duplicated time series index entries after hdf5 store (dayligth saving time related)',"b""I have a data frame like this\r\n\r\n    >> df\r\n    <class 'pandas.core.frame.DataFrame'>\r\n    Index: 596520 entries, 2006-04-14 00:00:00 to 2011-12-31 23:55:00\r\n    Data columns:\r\n    g_m_pyr__0       596520  non-null values\r\n    e_wr__0          596520  non-null values\r\n    p_nenn_sg__0     596520  non-null values\r\n    flaeche_sg__0    596520  non-null values\r\n    dtypes: float64(4)\r\n    >> df.index[0].timetuple()\r\n    time.struct_time(tm_year=2006, tm_mon=4, tm_mday=14, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=104, tm_isdst=-1)\r\n    >> df.index.get_duplicates()\r\n    []\r\n    \r\nsaving this dataframe to hdf5 and reloading it results in duplicated index entries:\r\n\r\n    >> store = pd.io.pytables.HDFStore('store.h5')\r\n    >> store['df'] = df \r\n    >> df2 = store['df']\r\n    >> df2.index.get_duplicates()\r\n    [datetime.datetime(2007, 3, 25, 3, 0),\r\n     datetime.datetime(2007, 3, 25, 3, 5),\r\n     datetime.datetime(2007, 3, 25, 3, 10),\r\n     datetime.datetime(2007, 3, 25, 3, 15),\r\n     datetime.datetime(2007, 3, 25, 3, 20),\r\n     datetime.datetime(2007, 3, 25, 3, 25),\r\n     datetime.datetime(2007, 3, 25, 3, 30),\r\n     datetime.datetime(2007, 3, 25, 3, 35),\r\n     datetime.datetime(2007, 3, 25, 3, 40),\r\n     datetime.datetime(2007, 3, 25, 3, 45),\r\n     datetime.datetime(2007, 3, 25, 3, 50),\r\n     datetime.datetime(2007, 3, 25, 3, 55),\r\n     datetime.datetime(2008, 3, 30, 3, 0),\r\n     datetime.datetime(2008, 3, 30, 3, 5),\r\n     datetime.datetime(2008, 3, 30, 3, 10),\r\n     datetime.datetime(2008, 3, 30, 3, 15),\r\n     datetime.datetime(2008, 3, 30, 3, 20),\r\n     datetime.datetime(2008, 3, 30, 3, 25),\r\n     datetime.datetime(2008, 3, 30, 3, 30),\r\n     datetime.datetime(2008, 3, 30, 3, 35),\r\n     datetime.datetime(2008, 3, 30, 3, 40),\r\n     datetime.datetime(2008, 3, 30, 3, 45),\r\n     datetime.datetime(2008, 3, 30, 3, 50),\r\n     datetime.datetime(2008, 3, 30, 3, 55),\r\n     datetime.datetime(2009, 3, 29, 3, 0),\r\n     datetime.datetime(2009, 3, 29, 3, 5),\r\n     datetime.datetime(2009, 3, 29, 3, 10),\r\n     datetime.datetime(2009, 3, 29, 3, 15),\r\n     datetime.datetime(2009, 3, 29, 3, 20),\r\n     datetime.datetime(2009, 3, 29, 3, 25),\r\n     datetime.datetime(2009, 3, 29, 3, 30),\r\n     datetime.datetime(2009, 3, 29, 3, 35),\r\n     datetime.datetime(2009, 3, 29, 3, 40),\r\n     datetime.datetime(2009, 3, 29, 3, 45),\r\n     datetime.datetime(2009, 3, 29, 3, 50),\r\n     datetime.datetime(2009, 3, 29, 3, 55),\r\n     datetime.datetime(2010, 3, 28, 3, 0),\r\n     datetime.datetime(2010, 3, 28, 3, 5),\r\n     datetime.datetime(2010, 3, 28, 3, 10),\r\n     datetime.datetime(2010, 3, 28, 3, 15),\r\n     datetime.datetime(2010, 3, 28, 3, 20),\r\n     datetime.datetime(2010, 3, 28, 3, 25),\r\n     datetime.datetime(2010, 3, 28, 3, 30),\r\n     datetime.datetime(2010, 3, 28, 3, 35),\r\n     datetime.datetime(2010, 3, 28, 3, 40),\r\n     datetime.datetime(2010, 3, 28, 3, 45),\r\n     datetime.datetime(2010, 3, 28, 3, 50),\r\n     datetime.datetime(2010, 3, 28, 3, 55),\r\n     datetime.datetime(2011, 3, 27, 3, 0),\r\n     datetime.datetime(2011, 3, 27, 3, 5),\r\n     datetime.datetime(2011, 3, 27, 3, 10),\r\n     datetime.datetime(2011, 3, 27, 3, 15),\r\n     datetime.datetime(2011, 3, 27, 3, 20),\r\n     datetime.datetime(2011, 3, 27, 3, 25),\r\n     datetime.datetime(2011, 3, 27, 3, 30),\r\n     datetime.datetime(2011, 3, 27, 3, 35),\r\n     datetime.datetime(2011, 3, 27, 3, 40),\r\n     datetime.datetime(2011, 3, 27, 3, 45),\r\n     datetime.datetime(2011, 3, 27, 3, 50),\r\n     datetime.datetime(2011, 3, 27, 3, 55)]\r\n\r\nI get the same duplicates with other time series os the same kind and the duplicates are in march for every dataframe.\r\n\r\nIs this a pandas problem?"""
1079,4180102,nmichaud,wesm,2012-04-18 20:16:09,2012-05-08 19:13:42,2012-05-08 19:13:42,closed,,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1079,"b""Can't compare Dataframe to None""","b""> import pandas\r\n> import numpy\r\n> df = pandas.DataFrame(numpy.random.randn(8, 3), index=range(8),columns=['A', 'B', 'C'])\r\n> None == df\r\n\r\nraises: pandas.core.common.PandasError: DataFrame constructor not properly called!\r\n"""
1078,4178288,tkf,changhiskhan,2012-04-18 18:46:54,2012-05-03 23:31:43,2012-05-03 22:35:31,closed,changhiskhan,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1078,b'A bug in HDFStore with MultiIndex',"b""```python\r\nrandn = np.random.randn\r\nfrom pandas import *\r\nindex = MultiIndex.from_tuples([('a', 0), ('b', 'y')])\r\ndf = DataFrame(randn(3, 2), columns=index)\r\nstore = HDFStore('/tmp/testpandas.hdf5')\r\nstore['multi'] = df\r\n```\r\n\r\nraises `ValueError: invalid literal for long() with base 10: 'y'`"""
1074,4174022,elpres,wesm,2012-04-18 15:06:23,2012-05-07 14:21:19,2012-05-07 14:17:05,closed,wesm,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1074,b'Crash when combining columns containing NaNs',"b'Here\'s the code:\r\n\r\n    In [23]: d = DataFrame({\'a\': [np.nan, False], \'b\': [True, True]})\r\n\r\n    In [24]: d[\'a\'] | d[\'b\']\r\n    ERROR: An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: (\'EOF in multi-line statement\', (1025, 0))\r\n    ERROR: An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: (\'EOF in multi-line statement\', (1036, 0))\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    /media/DATA/Code/Epidemic/<ipython-input-25-f18359aab663> in <module>()\r\n    ----> 1 d[\'a\'].fillna(False) | d[\'b\']\r\n\r\n    /usr/lib/python2.7/site-packages/pandas/core/series.pyc in wrapper(self, other)\r\n        143         if isinstance(other, Series):\r\n        144             name = _maybe_match_name(self, other)\r\n    --> 145             return Series(na_op(self.values, other.values),\r\n        146                           index=self.index, name=name)\r\n        147         elif isinstance(other, DataFrame):\r\n\r\n    /usr/lib/python2.7/site-packages/pandas/core/series.pyc in na_op(x, y)\r\n        132 \r\n        133             if isinstance(y, np.ndarray):\r\n    --> 134                 result = lib.vec_binop(x, y, op)\r\n        135             else:\r\n        136                 result = lib.scalar_binop(x, y, op)\r\n\r\n    /usr/lib/python2.7/site-packages/pandas/_tseries.so in pandas._tseries.vec_binop (pandas/src/tseries.c:9600)()\r\n\r\n    ValueError: Buffer dtype mismatch, expected \'Python object\' but got \'double\'\r\n\r\nThis seems to happen when performing any kind of boolean operation on a column containing a NaN. Calling .fillna() on it before the operation doesn\'t help (i.e. ""d[\'a\'].fillna(False) | d[\'b\']"" crashes as well). Also, ""d[\'a\'] & True"" will fail too, but without the messages about ""tokenizing input"".\r\n'"
1068,4142024,wesm,wesm,2012-04-16 20:35:37,2012-04-16 20:45:45,2012-04-16 20:45:45,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1068,b'Partial date indexing off by one error',"b""```\r\nIn [47]: rets.ix['2000']\r\nOut[47]:\r\n<class 'pandas.core.frame.DataFrame'>\r\nDatetimeIndex: 261 entries, 2000-01-03 00:00:00 to 2001-01-01 00:00:00\r\nData columns:\r\nAAPL    261  non-null values\r\nGOOG    261  non-null values\r\nMSFT    261  non-null values\r\ndtypes: float64(3)\r\n```"""
1065,4132374,gerigk,wesm,2012-04-16 10:58:16,2012-04-23 17:46:38,2012-04-23 17:46:20,closed,wesm,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/1065,b'Group By datatype screwed when group consists of one value',"b""x = DataFrame(np.arange(9).reshape(3,3))\r\nx['test']=0\r\nx['fl']= [1.3,1.5,1.6]\r\nx.groupby('test').agg({'fl':'sum',2:'size'}).dtypes\r\nOut[]:\r\n2      int64\r\nfl    object\r\n(version 0.8)\r\n\r\nOutput in 0.7.3\r\nOut[]:\r\n2       int64\r\nfl    float64\r\n\r\nThe output for x['test'] = [1,2,3] works fine\r\n"""
1063,4128235,wesm,wesm,2012-04-16 03:32:17,2012-04-22 00:14:11,2012-04-22 00:14:11,closed,,0.8.0,0,Bug;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1063,"b'Series.shift bugs, set default to periods=1'","b""```\r\nIn [46]: ts[:5]\r\nOut[46]:\r\n2000-01-01 00:00:00    1.126328\r\n2000-01-01 01:00:00   -1.551378\r\n2000-01-01 02:00:00   -0.353657\r\n2000-01-01 03:00:00    1.447303\r\n2000-01-01 04:00:00   -0.423588\r\n\r\nIn [45]: ts[:5].shift(1, offset=pd.datetools.Hour(4))\r\n/home/wesm/code/pandas/pandas/core/series.py:2517: FutureWarning: 'timeRule' and 'offset' parameters are deprecated, please use 'freq' instead\r\n  FutureWarning)\r\nOut[45]:\r\n2000-01-01 01:00:00    1.126328\r\n2000-01-01 02:00:00   -1.551378\r\n2000-01-01 03:00:00   -0.353657\r\n2000-01-01 04:00:00    1.447303\r\n2000-01-01 05:00:00   -0.423588\r\n```"""
1062,4128128,wesm,wesm,2012-04-16 03:08:29,2012-04-22 02:02:50,2012-04-22 02:02:50,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1062,b'KeyError raised out of DatetimeEngine formatted as int64',"b'```\r\nIn [12]:\r\n\r\naapl[datetime(2011, 12, 7)]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/home/wesm/code/teaching/ga/<ipython-input-12-d32cb44b8294> in <module>()\r\n----> 1 aapl[datetime(2011, 12, 7)]\r\n\r\n/home/wesm/code/pandas/pandas/core/series.pyc in __getitem__(self, key)\r\n    379     def __getitem__(self, key):\r\n    380         try:\r\n--> 381             return self.index.get_value(self, key)\r\n    382         except InvalidIndexError:\r\n    383             pass\r\n\r\n/home/wesm/code/pandas/pandas/core/index.pyc in get_value(self, series, key)\r\n   1747                 pass\r\n   1748 \r\n-> 1749             return self._engine.get_value(series, to_timestamp(key))\r\n   1750 \r\n   1751     def get_loc(self, key):\r\n\r\n/home/wesm/code/pandas/pandas/_engines.so in pandas._engines.IndexEngine.get_value (pandas/src/engines.c:16263)()\r\n\r\n/home/wesm/code/pandas/pandas/_engines.so in pandas._engines.IndexEngine.get_value (pandas/src/engines.c:16129)()\r\n\r\n/home/wesm/code/pandas/pandas/_engines.so in pandas._engines.DatetimeEngine.get_loc (pandas/src/engines.c:20163)()\r\n\r\n/home/wesm/code/pandas/pandas/_engines.so in pandas._engines.Int64HashTable.get_item (pandas/src/engines.c:5498)()\r\n\r\n/home/wesm/code/pandas/pandas/_engines.so in pandas._engines.Int64HashTable.get_item (pandas/src/engines.c:5452)()\r\n\r\nKeyError: 1323216000000000\r\n```'"
1055,4120077,wesm,wesm,2012-04-14 21:09:51,2012-05-09 23:53:39,2012-05-09 23:53:39,closed,,0.8.0,1,Bug;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1055,b'statsmodels incompatibility',b'cross-linking: https://github.com/statsmodels/statsmodels/issues/226'
1053,4117718,tkf,lodagro,2012-04-14 13:38:21,2012-04-16 20:56:14,2012-04-16 19:33:22,closed,lodagro,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1053,b'Bug in DataFrame.to_html with MultiIndex',"b'```python\r\nDataFrame(\r\n    rand(2, 4),\r\n    columns=MultiIndex.from_tuples(zip(\'aabb\', \'xyxy\'))\r\n    ).to_html()\r\n```\r\n\r\ngenerates\r\n\r\n<table border=""1"">\r\n  <thead>\r\n    <tr>\r\n      <th><table><tbody><tr><td>a</td></tr><tr><td>x</td></tr></tbody></table></th>\r\n      <th><table><tbody><tr><td>a</td></tr><tr><td>y</td></tr></tbody></table></th>\r\n      <th><table><tbody><tr><td>b</td></tr><tr><td>x</td></tr></tbody></table></th>\r\n      <th><table><tbody><tr><td>b</td></tr><tr><td>y</td></tr></tbody></table></th>\r\n    </tr>\r\n    </thead>\r\n    <tbody>\r\n    <tr>\r\n      <td><strong>0</strong></td>\r\n      <td> 0.280664</td>\r\n      <td> 0.465508</td>\r\n      <td> 0.616751</td>\r\n      <td> 0.246239</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>1</strong></td>\r\n      <td> 0.124957</td>\r\n      <td> 0.867052</td>\r\n      <td> 0.504789</td>\r\n      <td> 0.178313</td>\r\n    </tr>\r\n  </tbody>\r\n</table>'"
1051,4116038,albertjmenkveld,wesm,2012-04-14 06:07:16,2012-08-20 17:26:58,2012-07-12 21:47:19,closed,,0.8.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1051,b'nosetests fails one test',"b'Hi Wes, \r\n\r\nI have just watched your 3+ hour talk on pandas and got enthusiastic.  So, I am exploring pandas for my scientific work on high-frequency finance data.  Thanks for all the good stuff. \r\n\r\nThere is one issue you might want to know about.  This morning I upgraded to the latest Enthought python distribution and then tried to install pandas as indicated in your manual.  But, trying separately on two machines, I keep getting the following failure (see below).  It seems so far to go unreported so I thought I should let you know about it (I tried it on a MB Air and a Mac Pro which both have Mac OS Lion installed (and I always run Mac software upgrades)). \r\n\r\nKeep up the good work!  Greetings from Amsterdam.\r\n\r\nCheers, Albert\r\n\r\nFAIL: test_timeseries_preepoch (pandas.io.tests.test_pytables.TesttHDFStore)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/albertjmenkveld/Downloads/pandas/pandas/io/tests/test_pytables.py"", line 216, in test_timeseries_preepoch\r\n    self._check_roundtrip(ts, tm.assert_series_equal)\r\n  File ""/Users/albertjmenkveld/Downloads/pandas/pandas/io/tests/test_pytables.py"", line 459, in _check_roundtrip\r\n    comparator(retrieved, obj)\r\n  File ""/Users/albertjmenkveld/Downloads/pandas/pandas/util/testing.py"", line 124, in assert_series_equal\r\n    assert(left.index.equals(right.index))\r\nAssertionError'"
1049,4107465,bburan,wesm,2012-04-13 16:51:19,2012-09-18 14:44:43,2012-09-18 14:44:43,closed,,0.9,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1049,b'Adding float32 series to dataframe and then attempting to join()',"b""Tested with Pandas 0.7.3\r\n\r\nI have a IPython notebook that explores this issue (basically it only seems to occur when you add a float32 series to the DataFrame **after** creating it, no other datatype I tested seems to trigger this bug).  Please contact me if you want the notebook since I don't seem to be able to attach files to this issue.  The simplest way to replicate it is via the following lines:\r\n\r\n    import numpy as np\r\n    import pandas\r\n    a = np.random.randint(0, 5, 100)\r\n    df = pandas.DataFrame({'a': a})\r\n    s = pandas.Series(np.random.random(5), name='md')\r\n    df.join(s, on='a') # this is OK\r\n    df['b'] = np.random.randint(0, 5, 100)\r\n    df.join(s, on='a') # this is still OK\r\n    df['c'] = np.random.randint(0, 5, 100).astype('f')\r\n    df.join(s, on='a') # this fails\r\n\r\nThe traceback is:\r\n\r\n    ValueError                                Traceback (most recent call last)\r\n    C:\\Users\\brad\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Enthought\\<ipython-input-1-454edb5f6546> in <module>()\r\n          8 df.join(s, on='a') # this is still OK\r\n          9 df['c'] = np.random.randint(0, 5, 100).astype('f')\r\n    ---> 10 df.join(s, on='a') # this fails\r\n         11 \r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in join(self, other, on, how, lsuffix, rsuffix, sort)\r\n       3285         # For SparseDataFrame's benefit\r\n    \r\n       3286         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\r\n    -> 3287                                  rsuffix=rsuffix, sort=sort)\r\n       3288 \r\n       3289     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _join_compat(self, other, on, how, lsuffix, rsuffix, sort)\r\n       3298             return merge(self, other, left_on=on, how=how,\r\n       3299                          left_index=on is None, right_index=True,\r\n    -> 3300                          suffixes=(lsuffix, rsuffix), sort=sort)\r\n       3301         else:\r\n       3302             if on is not None:\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy)\r\n         29                          right_index=right_index, sort=sort, suffixes=suffixes,\r\n         30                          copy=copy)\r\n    ---> 31     return op.get_result()\r\n         32 if __debug__: merge.__doc__ = _merge_doc % '\\nleft : DataFrame'\r\n         33 \r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in get_result(self)\r\n         80                                       copy=self.copy)\r\n         81 \r\n    ---> 82         result_data = join_op.get_result()\r\n         83         result = DataFrame(result_data)\r\n         84 \r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in get_result(self)\r\n        495         for klass in kinds:\r\n        496             klass_blocks = [mapping.get(klass) for mapping in blockmaps]\r\n    --> 497             res_blk = self._get_merged_block(klass_blocks)\r\n        498             result_blocks.append(res_blk)\r\n        499 \r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in _get_merged_block(self, blocks)\r\n        509 \r\n        510         if len(to_merge) > 1:\r\n    --> 511             return self._merge_blocks(to_merge)\r\n        512         else:\r\n        513             unit, block = to_merge[0]\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.pyc in _merge_blocks(self, merge_chunks)\r\n        545                 com.take_fast(blk.values, unit.indexer,\r\n        546                               None, False,\r\n    --> 547                               axis=self.axis, out=out_chunk)\r\n        548 \r\n        549             sofar += len(blk)\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\common.pyc in take_fast(arr, indexer, mask, needs_masking, axis, out, fill_value)\r\n        264         return take_2d(arr, indexer, out=out, mask=mask,\r\n        265                        needs_masking=needs_masking,\r\n    --> 266                        axis=axis, fill_value=fill_value)\r\n        267 \r\n        268     result = arr.take(indexer, axis=axis, out=out)\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\core\\common.pyc in take_2d(arr, indexer, out, mask, needs_masking, axis, fill_value)\r\n        236             out = np.empty(out_shape, dtype=arr.dtype)\r\n        237         take_f = _get_take2d_function(dtype_str, axis=axis)\r\n    --> 238         take_f(arr, indexer, out=out, fill_value=fill_value)\r\n        239         return out\r\n        240     else:\r\n    \r\n    C:\\Python27\\lib\\site-packages\\pandas\\_tseries.pyd in pandas._tseries.take_2d_axis1_float64 (pandas\\src\\tseries.c:49365)()\r\n    \r\n    ValueError: Buffer dtype mismatch, expected 'float64_t' but got 'float'\r\n    \r\n    \r\n"""
1048,4102606,twiecki,wesm,2012-04-13 12:50:06,2012-05-07 16:36:18,2012-05-07 16:36:05,closed,changhiskhan,0.8.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/1048,b'Segfault in .groupby() with empty groups',"b""I get a segfault when running:\r\n\r\npandas.DataFrame([1,2,3]).groupby([]).groups\r\n\r\nAs an aside, in my scenario (which I don't know how general it is) the user can supply the grouping. If no grouping should take place (i.e. groupby([])) I would like .groupby() to return the whole data, instead there is a:\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in _get_compressed_labels(self)\r\n    547                 group_index = get_group_index(all_labels, self.shape)\r\n    548             else:\r\n--> 549                 group_index = all_labels[0]\r\n    550             comp_ids, obs_group_ids = _compress_group_index(group_index)\r\n    551             return comp_ids, obs_group_ids\r\n\r\nIndexError: list index out of range\r\n\r\nI'm not sure whether it would be sensible to change the behavior so that the whole DataFrame is returned, but to me it would be more useful than raising an error."""
1046,4097801,wesm,wesm,2012-04-13 04:53:29,2012-04-13 05:22:06,2012-04-13 05:22:06,closed,,0.8.0,0,Bug;Indexing;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1046,b'Alignment ops with irregular DatetimeIndex performance problems',"b""Improperly boxing time stamps:\r\n\r\n```\r\n%prun -s cumulative result = left + right\r\n\r\n         1000435 function calls (1000434 primitive calls) in 4.870 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000    4.870    4.870 <string>:1(<module>)\r\n        1    0.000    0.000    4.870    4.870 series.py:62(wrapper)\r\n        1    0.000    0.000    4.866    4.866 series.py:1824(align)\r\n        1    0.000    0.000    4.856    4.856 index.py:1561(join)\r\n        1    0.001    0.001    4.856    4.856 index.py:710(join)\r\n        1    0.254    0.254    4.778    4.778 index.py:1542(union)\r\n        1    0.076    0.076    4.524    4.524 index.py:418(union)\r\n        1    0.000    0.000    2.767    2.767 index.py:1830(__iter__)\r\n        1    0.000    0.000    2.767    2.767 index.py:1464(asobject)\r\n        1    0.203    0.203    2.767    2.767 datetools.py:34(_dt_box_array)\r\n   500000    0.215    0.000    2.564    0.000 datetools.py:41(<lambda>)\r\n   500000    2.349    0.000    2.349    0.000 datetools.py:28(_dt_box)\r\n        1    0.000    0.000    0.901    0.901 index.py:473(_wrap_union_result)\r\n        1    0.010    0.010    0.901    0.901 index.py:1197(__new__)\r\n       18    0.890    0.049    0.890    0.049 {numpy.core.multiarray.array}\r\n       16    0.000    0.000    0.890    0.056 numeric.py:167(asarray)\r\n        1    0.743    0.743    0.743    0.743 {method 'sort' of 'list' objects}\r\n        3    0.000    0.000    0.108    0.036 index.py:597(get_indexer)\r\n        3    0.106    0.035    0.106    0.035 {method 'get_indexer' of 'pandas._engines.DatetimeEngine' objects}\r\n        2    0.000    0.000    0.010    0.005 series.py:1860(_reindex_indexer)\r\n        2    0.000    0.000    0.009    0.005 common.py:170(take_1d)\r\n        2    0.007    0.004    0.007    0.004 {pandas._tseries.take_1d_float64}\r\n        1    0.004    0.004    0.004    0.004 {method 'nonzero' of 'numpy.ndarray' objects}\r\n        2    0.000    0.000    0.003    0.001 index.py:1866(equals)\r\n        2    0.003    0.001    0.003    0.001 numeric.py:1927(array_equal)\r\n        2    0.000    0.000    0.002    0.001 common.py:666(_ensure_int32)\r\n        2    0.002    0.001    0.002    0.001 {method 'astype' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.002    0.002 series.py:47(na_op)\r\n        1    0.002    0.002    0.002    0.002 {operator.add}\r\n       13    0.000    0.000    0.001    0.000 index.py:1858(dtype)\r\n       18    0.000    0.000    0.001    0.000 _internal.py:180(_datetimestring)\r\n       18    0.001    0.000    0.001    0.000 {method 'match' of '_sre.SRE_Pattern' objects}\r\n        1    0.001    0.001    0.001    0.001 {method 'take' of 'numpy.ndarray'\r\n```"""
1045,4097701,wesm,wesm,2012-04-13 04:37:55,2012-04-14 20:09:47,2012-04-14 20:09:47,closed,,0.8.0,0,Bug;Groupby;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1045,"b""NumPy reductions don't work with convert / TimeGrouper""","b'```\r\n(1 + returns).convert(\'BM\', how=np.cumprod)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/home/wesm/code/rapidquant/<ipython-input-22-c8b05a51e175> in <module>()\r\n----> 1 (1 + returns).convert(\'BM\', how=np.cumprod)\r\n\r\n/home/wesm/code/pandas/pandas/core/generic.pyc in convert(self, rule, method, how, axis, as_index, closed, label)\r\n    176                 how = translate_grouping(how)\r\n    177 \r\n--> 178             result = grouped.agg(how)\r\n    179         else:\r\n    180             # upsampling\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\r\n    245         See docstring for aggregate\r\n    246         """"""\r\n--> 247         return self.aggregate(func, *args, **kwargs)\r\n    248 \r\n    249     def _iterate_slices(self):\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\r\n   1195                 return self._python_agg_general(func_or_funcs, *args, **kwargs)\r\n   1196             except Exception:\r\n-> 1197                 result = self._aggregate_named(func_or_funcs, *args, **kwargs)\r\n   1198 \r\n   1199             index = Index(sorted(result), name=self.grouper.names[0])\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _aggregate_named(self, func, *args, **kwargs)\r\n   1266         result = {}\r\n   1267 \r\n-> 1268         for name in self.grouper:\r\n   1269             grp = self.get_group(name)\r\n   1270             grp.name = name\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in __iter__(self)\r\n    448 \r\n    449     def __iter__(self):\r\n--> 450         return iter(self.indices)\r\n    451 \r\n    452     def numkeys(self):\r\n\r\n/home/wesm/code/pandas/pandas/_tseries.so in pandas._tseries.cache_readonly.__get__ (pandas/src/tseries.c:111266)()\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in indices(self)\r\n    491     @cache_readonly\r\n    492     def indices(self):\r\n--> 493         if len(self.groupings) == 1:\r\n    494             return self.groupings[0].indices\r\n    495         else:\r\n\r\nAttributeError: \'TimeGrouper\' object has no attribute \'groupings\'\r\n\r\n(1 + returns).convert(\'BM\', how=np.cumprod)\r\n```'"
1037,4089802,wesm,wesm,2012-04-12 19:04:25,2012-04-12 21:52:34,2012-04-12 21:52:34,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1037,b'Arrays of datetime64 not handled in DataFrame constructor with passed dict',
1036,4089776,wesm,wesm,2012-04-12 19:02:52,2012-04-12 21:42:17,2012-04-12 21:42:17,closed,,0.8.0,0,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1036,b'time_rule/timeRule broken in DateRange',
1035,4089697,wesm,wesm,2012-04-12 18:59:08,2012-04-12 23:27:26,2012-04-12 23:27:26,closed,,0.8.0,0,Bug;Prio-high;Timeseries,https://api.github.com/repos/pydata/pandas/issues/1035,b'DatetimeBlock -> FloatBlock in reindex op',b'```\r\n     floats  ints strings  bools              objects\r\n0 -1.478427     0     foo  False  2000-01-03 00:00:00\r\n1  0.524988     1     bar   True  2000-01-04 00:00:00\r\n2  0.404705     2     foo   True  2000-01-05 00:00:00\r\n3  0.577046     3     bar   True  2000-01-06 00:00:00\r\n4 -1.715002     4     foo  False  2000-01-07 00:00:00\r\n5 -1.039268     5     bar  False  2000-01-10 00:00:00\r\n6 -0.370647     6     foo  False  2000-01-11 00:00:00\r\n7 -1.157892     7     bar  False  2000-01-12 00:00:00\r\n8 -1.344312     8     foo  False  2000-01-13 00:00:00\r\n9  0.844885     9     bar   True  2000-01-14 00:00:00\r\n\r\nIn [67]: foo.reindex(range(15))\r\nOut[67]:\r\n      floats  ints strings  bools       objects\r\n0  -1.478427     0     foo  False  9.468576e+14\r\n1   0.524988     1     bar   True  9.469440e+14\r\n2   0.404705     2     foo   True  9.470304e+14\r\n3   0.577046     3     bar   True  9.471168e+14\r\n4  -1.715002     4     foo  False  9.472032e+14\r\n5  -1.039268     5     bar  False  9.474624e+14\r\n6  -0.370647     6     foo  False  9.475488e+14\r\n7  -1.157892     7     bar  False  9.476352e+14\r\n8  -1.344312     8     foo  False  9.477216e+14\r\n9   0.844885     9     bar   True  9.478080e+14\r\n10       NaN   NaN     NaN    NaN           NaN\r\n11       NaN   NaN     NaN    NaN           NaN\r\n12       NaN   NaN     NaN    NaN           NaN\r\n13       NaN   NaN     NaN    NaN           NaN\r\n14       NaN   NaN     NaN    NaN           NaN\r\n```'
1026,4068531,echlebek,wesm,2012-04-11 16:42:23,2012-05-07 18:02:34,2012-05-07 18:02:34,closed,,0.8.0,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1026,b'Indexing with namedtuple is broken',"b'Although it is possible to index MultiIndexed DataFrames with multiple index columns, one or more of which have a compound type, it is not possible to index an Indexed DataFrame with a compound type for its column, nor is it possible to index a MultiIndexed Dataframe with a single column that has a compound type.\r\n\r\ntl;dr - I can\'t index a DataFrame with a namedtuple, even though I can create one.\r\n\r\nIn the first example, I try to index a dataframe with a namedtuple with a regular Index, which fails.\r\n\r\nIn the second example, I index a dataframe with a tuple of namedtuples (MultiIndex), which succeeds.\r\n\r\nIn the third example, I try to index a dataframe with a length-1 tuple of namedtuples, again with a MultiIndex, which fails.\r\n\r\n    from collections import namedtuple\r\n    import pandas\r\n\r\n    # First example\r\n    """""" \r\n    >>> IndexType = namedtuple(""IndexType"", [""a"", ""b""])\r\n    >>> idx1 = IndexType(""foo"", ""bar"")\r\n    >>> idx2 = IndexType(""baz"", ""bof"")\r\n    >>> index = pandas.Index([idx1, idx2], name=""composite_index"")\r\n    >>> index\r\n    Index([IndexType(a=\'foo\', b=\'bar\'), IndexType(a=\'baz\', b=\'bof\')], dtype=object)\r\n    >>> df = pandas.DataFrame([(1, 2), (3, 4)], index=index, columns=[""A"", ""B""])\r\n    >>> df\r\n                                 A  B\r\n    composite_index..................\r\n    IndexType(a=\'foo\', b=\'bar\')  1  2\r\n    IndexType(a=\'baz\', b=\'bof\')  3  4\r\n    >>> df.ix[IndexType(""foo"", ""bar"")]\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._getitem_tuple(key)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._getitem_lowerdim(tup)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        section = self._getitem_axis(key, axis=i)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._get_label(idx, axis=0)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self.obj.xs(label, axis=axis, copy=True)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        loc = self.index.get_loc(key)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._engine.get_loc(key)\r\n      File ""engines.pyx"", line 101, in pandas._engines.DictIndexEngine.get_loc (pandas/src/engines.c:2498)\r\n      File ""engines.pyx"", line 108, in pandas._engines.DictIndexEngine.get_loc (pandas/src/engines.c:2460)\r\n    KeyError: \'foo\'\r\n    """""" \r\n\r\n    # Second example\r\n\r\n    """""" \r\n    >>> mult_index = pandas.MultiIndex.from_tuples([(idx1, idx2)], names=[""comp_1"", ""comp_2""])\r\n    >>> mult_index\r\n    MultiIndex([(IndexType(a=\'foo\', b=\'bar\'), IndexType(a=\'baz\', b=\'bof\'))], dtype=object)\r\n    >>> df = pandas.DataFrame([(1, 2, 3, 4)], index=mult_index, columns=[""A"", ""B"", ""C"", ""D""])\r\n    >>> df\r\n                                                             A  B  C  D\r\n    comp_1                      comp_2.................................\r\n    IndexType(a=\'foo\', b=\'bar\') IndexType(a=\'baz\', b=\'bof\')  1  2  3  4\r\n    >>> df.ix[(IndexType(""foo"", ""bar""), IndexType(""baz"", ""bof""))]\r\n    A    1   \r\n    B    2   \r\n    C    3   \r\n    D    4   \r\n    Name: (IndexType(a=\'foo\', b=\'bar\'), IndexType(a=\'baz\', b=\'bof\'))\r\n    """""" \r\n\r\n    # Third example\r\n\r\n    """""" \r\n    >>> index = pandas.MultiIndex.from_tuples([(IndexType(""foo"", ""bar""),), (IndexType(""baz"", ""bof""),)], names=[""ind#\r\n    >>> index\r\n    Index([IndexType(a=\'foo\', b=\'bar\'), IndexType(a=\'baz\', b=\'bof\')], dtype=object\r\n    >>> df = pandas.DataFrame([(1, 2), (3, 4)], index=index, columns=[""A"", ""B""])\r\n    >>> df\r\n                                 A  B\r\n    index............................\r\n    IndexType(a=\'foo\', b=\'bar\')  1  2\r\n    IndexType(a=\'baz\', b=\'bof\')  3  4\r\n    >>> df.ix[IndexType(""foo"", ""bar"")]\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._getitem_tuple(key)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._getitem_lowerdim(tup)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        section = self._getitem_axis(key, axis=i)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._get_label(idx, axis=0)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self.obj.xs(label, axis=axis, copy=True)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        loc = self.index.get_loc(key)\r\n      File ""/Network/Cluster/home/echlebek/.virtualenvs/pandas/lib/python2.6/site-packages/pandas-0.7.3.dev_3d4d5af#\r\n        return self._engine.get_loc(key)\r\n      File ""engines.pyx"", line 101, in pandas._engines.DictIndexEngine.get_loc (pandas/src/engines.c:2498)\r\n      File ""engines.pyx"", line 108, in pandas._engines.DictIndexEngine.get_loc (pandas/src/engines.c:2460)\r\n    KeyError: \'foo\'\r\n    >>> df.ix[(IndexType(""foo"", ""bar""),)]\r\n          A   B\r\n    foo NaN NaN\r\n    bar NaN NaN\r\n    """"""'"
1016,4045170,changhiskhan,wesm,2012-04-10 12:46:41,2012-04-10 16:04:07,2012-04-10 16:04:07,closed,,0.7.3,0,Bug,https://api.github.com/repos/pydata/pandas/issues/1016,b'Series comparison causes crash',"b""In [68]: a = Series(['a', 'b', 'c'])\r\n\r\nIn [69]: b = Series(['b', 'a'])\r\n\r\nIn [70]: a > b\r\n                                                                                                         \r\nWarning: Program '/home/chang/epd/bin/ipython' crashed.                                                  """
1015,4028849,wesm,wesm,2012-04-09 13:20:12,2012-04-09 13:44:54,2012-04-09 13:44:54,closed,,0.7.3,0,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/1015,b'SparsePanel-Panel buggy interactions',b'SparsePanel.multiply(Panel)\r\nSparsePanel.subtract(Panel)\r\n'
1014,4025449,wesm,jreback,2012-04-09 04:14:44,2014-02-16 21:28:19,2014-02-16 21:28:19,closed,,Someday,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/1014,b'Possibly attempt object -> float64 coercion in to_sparse',"b'something like\r\n\r\n```array([82.87, 82.1, 84.25, 84.53, 85.9, 85.9, 86.95, 88.13, 89.49, 89.76,\r\n       90.76, 88.99, 89.42, 89.0, 89.49, 88.95, 89.09, 88.65, 88.52, 88.98], dtype=object)```\r\n\r\nfrom the mailing list'"
1013,4022114,changhiskhan,wesm,2012-04-08 16:46:29,2012-04-10 18:29:12,2012-04-10 18:29:12,closed,changhiskhan,0.7.3,0,Bug;Indexing,https://api.github.com/repos/pydata/pandas/issues/1013,"b'DataFrame.ix[tup, list] throws exception'","b""per pydata email:\r\n\r\n---\r\nI created a dataframe with multi index like this:\r\n                      a   b   c\r\ny        m d             \r\n2000 1 1       1   2  3\r\n             2       7   8  9\r\n             3       4   5  6\r\n\r\n...\r\n\r\nit doesn't work to do this:  df.ix[(2000,1,1), ['a','b']]\r\nit gives KeyError = 1\r\n---\r\n\r\nin _NDFrameIndexer._getitem_tuple, the return value of the first ix._getitem_axis(key, axis=i) call returns a Series so the second call with axis=1 fails with KeyError=1"""
1011,4017905,changhiskhan,wesm,2012-04-07 21:34:28,2012-04-10 03:51:04,2012-04-10 03:51:04,closed,changhiskhan,0.7.3,0,Bug;Visualization,https://api.github.com/repos/pydata/pandas/issues/1011,b'DataFrame.plot(logy=True) seems to have no effect',"b'df = DataFrame(np.random.randn(100, 5)*1000000)\r\n\r\ndf.plot() looks the same as df.plot(logy=True)'"
1010,4017887,wesm,wesm,2012-04-07 21:30:55,2012-04-10 03:41:58,2012-04-10 03:41:58,closed,,0.7.3,0,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/1010,b'Unicode repr issues with MultiIndex',"b'from mailing list\r\n\r\n""""""\r\nHello All,\r\n\r\nWorking on 0.7.2.\r\n\r\nI have a DataFrame with a MultiIndex (2 levels).\r\nBecause my second level contains non ascii character I can\'t use the\r\nmethods ;\r\n- DataFrame.index.levels\r\n- DataFrame.get_level_values(1)\r\n\r\nIs it a known issue?\r\n\r\nThanks!\r\n""""""'"
1003,3998873,wesm,wesm,2012-04-06 01:18:37,2012-04-06 04:39:46,2012-04-06 04:39:46,closed,,0.8.0,0,Bug;Prio-high;Timeseries;Visualization,https://api.github.com/repos/pydata/pandas/issues/1003,b'autofmt_xdate issues with datetime64',"b'The way that matplotlib is being told about the x ticks causes autofmt_xdate to now work. Figure out how to have sane, attractive default tick labeling in time series plots'"
1001,3993670,ijmcf,adamklein,2012-04-05 18:54:21,2012-04-09 14:25:01,2012-04-09 14:25:01,closed,,0.7.3,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1001,b'Problem with DataFrame.lookup()',"b""If the row and/or column labels are not present in the DataFrame, the lookup() method returns ... whatever it wants, apparently. I would expect either a NaN or an exception, personally - but whatever the result, it shouldn't be the wrong piece of data.\r\n\r\n>>> df = pandas.DataFrame({'A': pandas.Series(['A', 'B', 'C', 'D', 'E'], index=range(5))})\r\n\r\n>>> df.lookup([2], ['A'])              # Correct\r\narray([C], dtype=object)\r\n\r\n>>> df.lookup([10], ['A'])            # 10 is not present in the index, so we get .. the last value in A?\r\narray([E], dtype=object)\r\n\r\n>>> df.lookup([3], ['B'])               # There is no column 'B', so we get ... the value from the last column (A) in that row?\r\narray([C], dtype=object)\r\n\r\n>>> df.lookup([10], ['B'])             # Neither row or column label is valid, so we get ... I can't rationalize this one.\r\narray([D], dtype=object)\r\n\r\n\r\n"""
1000,3993239,tejrai,wesm,2012-04-05 18:32:58,2012-11-29 20:32:13,2012-11-29 20:32:13,closed,,0.10,1,Bug,https://api.github.com/repos/pydata/pandas/issues/1000,b'col_space parameter in to_html and to_string for dataframe',"b'The col_space parameter in the to_html and to_string functions for dataframe doesn\'t seem to affect the output. Here\'s an example using to_html(). Note that the HTML code is identical across the two function calls.\r\n\r\nThanks,\r\nTej\r\n\r\n\r\noutput.to_html()\r\n\r\n<table border=""1"">\r\n  <thead>\r\n    <tr>\r\n      <th></th>\r\n      <th>nearby = 1      </th>\r\n      <th>nearby = 2      </th>\r\n    </tr>\r\n    </thead>\r\n    <tbody>\r\n    <tr>\r\n      <td><strong>CM_CL</strong></td>\r\n      <td> K2</td>\r\n      <td>  M2</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>FI_TY</strong></td>\r\n      <td> M2</td>\r\n      <td> NaN</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\noutput.to_html(col_space = 2000)\r\n\r\n<table border=""1"">\r\n  <thead>\r\n    <tr>\r\n      <th></th>\r\n      <th>nearby = 1      </th>\r\n      <th>nearby = 2      </th>\r\n    </tr>\r\n    </thead>\r\n    <tbody>\r\n    <tr>\r\n      <td><strong>CM_CL</strong></td>\r\n      <td> K2</td>\r\n      <td>  M2</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>FI_TY</strong></td>\r\n      <td> M2</td>\r\n      <td> NaN</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n'"
995,3981239,wesm,wesm,2012-04-05 00:24:46,2012-04-10 04:35:19,2012-04-10 04:35:19,closed,,0.7.3,0,Bug;Groupby;Prio-high,https://api.github.com/repos/pydata/pandas/issues/995,"b""Don't add nonsense 'result' name in groupby results with no obvious name""",
992,3972468,nspies,wesm,2012-04-04 15:18:13,2012-04-10 19:23:24,2012-04-10 19:23:24,closed,,0.7.3,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/992,b'None not always considered missing',"b'I expected pandas to know that ```None``` should be ignored as a missing value in the following simple example:\r\n\r\n```\r\nimport pandas\r\nimport numpy\r\n\r\n# works fine\r\ndf = pandas.DataFrame({""col1"":[2,5.0,123,numpy.nan], ""col2"":[1,2,3,4]})\r\nprint df.sub(3)\r\n\r\n# fails -- tries to do the subtraction with None\r\ndf = pandas.DataFrame({""col1"":[2,5.0,123,None], ""col2"":[1,2,3,4]})\r\nprint df.sub(3)\r\n\r\n```'"
991,3959651,sadruddin,wesm,2012-04-03 22:09:18,2012-04-07 21:20:32,2012-04-07 21:20:32,closed,,0.7.3,1,Bug,https://api.github.com/repos/pydata/pandas/issues/991,b'apply method of DataFrameGroupBy object returns surprising result',"b'In the example below, column ""name"" of groupby apply method has the same content as column ""year"". A similar column with a copy of the ""name"" values will remain untouched (as expected).\r\n\r\n    import pandas\r\n    import StringIO\r\n    \r\n    csv = """"""""year"",""name"",""percent"",""sex""\r\n    1880,""John"",0.081541,""boy""\r\n    1880,""William"",0.080511,""boy""\r\n    1880,""James"",0.050057,""boy""\r\n    1881,""Charles"",0.045167,""boy""\r\n    1881,""George"",0.043292,""boy""\r\n    """"""\r\n    \r\n    names = pandas.read_csv(StringIO.StringIO(csv))\r\n    names[\'name2\'] = names[\'name\']\r\n    grouped = names.groupby(\'year\')\r\n    \r\n    print \'Pandas version\', pandas.__version__\r\n    print names\r\n    print grouped.apply(lambda x:x) \r\n\r\nOUTPUT:\r\n\r\n    Pandas version 0.7.2\r\n       year     name   percent  sex    name2\r\n    0  1880     John  0.081541  boy     John\r\n    1  1880  William  0.080511  boy  William\r\n    2  1880    James  0.050057  boy    James\r\n    3  1881  Charles  0.045167  boy  Charles\r\n    4  1881   George  0.043292  boy   George\r\n       year  name   percent  sex    name2\r\n    0  1880  1880  0.081541  boy     John\r\n    1  1880  1880  0.080511  boy  William\r\n    2  1880  1880  0.050057  boy    James\r\n    3  1881  1881  0.045167  boy  Charles\r\n    4  1881  1881  0.043292  boy   George\r\n'"
989,3944350,jseabold,wesm,2012-04-03 13:08:29,2012-04-09 15:12:20,2012-04-09 15:11:55,closed,,0.7.3,4,Bug,https://api.github.com/repos/pydata/pandas/issues/989,b'Slicing a Series returns an Array and fails',"b'From pystatsmodels ML ""[pandas] slicing a series returns an array?""\r\n\r\nGot it. It only happens if the series is longer than 200\r\n\r\n```python\r\nx = pandas.Series(np.random.random(201), name=\'x\')\r\nx.reshape((-1,1))\r\n<snip>\r\n```\r\n\r\nThis is part of what I was talking about that Series aren\'t quite\r\narray-like. Something like this\r\n\r\nnp.dot(x, np.random.random((1,10))\r\n\r\ndoesn\'t work, and I can\'t coerce x to be 2d. However, if X is less\r\nthan length 201, I sure can coerce it to be 2d, which is why I\r\ncouldn\'t reproduce\r\n\r\n```python\r\nx = pandas.Series(np.random.random(199), name=\'x\')\r\nx = x.reshape((-1,1))\r\n```\r\n\r\nBut I still can\'t dot\r\n\r\n```python\r\nnp.dot(x,np.random.random((1,10)))\r\n<snip>\r\nValueError: The truth value of an array with more than one element is\r\nambiguous. Use a.any() or a.all()\r\n```\r\n\r\nI realize that trying to \'dot\' might not make sense with a Series or\r\nthere\'s no intuitive interpretation to the output in terms of the\r\nSeries that I can think of offhand. Just an example of the 1d vs 2d\r\nissue. Mainly, I guess I\'d expect a series to be able to pass through\r\nan atleast_2d check.'"
988,3926592,mcobzarenco,wesm,2012-04-02 13:46:19,2012-04-10 04:48:35,2012-04-10 04:48:34,closed,,0.7.3,1,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/988,b'NaN equality issue in Series',"b""Hi Wes,\r\n\r\nJust a quick question on NaN equality, I haven't specifically looked it up, but upgrading to 0.7.2 broke my code and it seems it's down to how testing for equality with NaN behaves:\r\n\r\n    a = pandas.Series(['abc', nan])\r\n\r\na == 'cba'    outputs:\r\n\r\n    0    False\r\n    1      NaN\r\n\r\n\r\nrather than all False. This seems to most definitely be a bug. Especially as \r\n\r\npandas.DataFrame(['abc', nan]) == 'cba'    outputs the correct version:\r\n\r\n           0\r\n    0  False\r\n    1  False\r\n\r\n\r\nMany thanks,\r\nMarius"""
982,3896653,lbeltrame,wesm,2012-03-30 12:10:59,2012-04-09 20:04:35,2012-04-09 20:04:35,closed,,0.7.3,4,Bug;Missing-data,https://api.github.com/repos/pydata/pandas/issues/982,b'Boolean operations on Series (object dtype) with NaNs retain NaNs: breaks boolean indexing',"b'Case in point:\r\n\r\n````python\r\nIn [14]: example = pandas.Series([""Activated"", ""Inhibited"", np.nan], dtype=""object"")\r\n\r\nIn [15]: example == ""Inhibited""\r\nOut[15]: \r\n0    False\r\n1     True\r\n2      NaN\r\n````\r\nThis breaks boolean indexing:\r\n\r\n````python\r\nexample[example == ""Inihibted""]\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.pyc in __getitem__(self, key)\r\n    392         # special handling of boolean data with NAs stored in object\r\n\r\n    393         # arrays. Since we can\'t represent NA with dtype=bool\r\n\r\n--> 394         if _is_bool_indexer(key):\r\n    395             key = self._check_bool_indexer(key)\r\n    396             key = np.asarray(key, dtype=bool)\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/common.pyc in _is_bool_indexer(key)\r\n    321         if not lib.is_bool_array(key):\r\n    322             if isnull(key).any():\r\n--> 323                 raise ValueError(\'cannot index with vector containing \'\r\n    324                                  \'NA / NaN values\')\r\n    325             return False\r\n\r\nValueError: cannot index with vector containing NA / NaN value\r\n```'"
980,3884847,wesm,wesm,2012-03-29 17:58:19,2012-04-02 03:46:58,2012-04-02 03:46:58,closed,,0.7.3,0,Bug,https://api.github.com/repos/pydata/pandas/issues/980,b'DataFrame.append can lose index name',"b""from mailing list\r\n\r\n```\r\nimport pandas\r\nimport numpy as np\r\nimport pandas\r\nprint pandas.__version__\r\n0.7.2.dev-12d511f\r\n\r\n\r\ndf1 = pandas.DataFrame(data=None, columns=['A','B','C'])\r\ndf1 = df1.set_index(['A'])\r\ndf2 = pandas.DataFrame(data=[[1,4,7],[2,5,8],[3,6,9]],\r\ncolumns=['A','B','C'])\r\ndf2 = df2.set_index(['A'])\r\nprint df1.append(df2).index.names\r\n\r\n[None]\r\n```"""
973,3827534,saroele,wesm,2012-03-27 13:37:39,2012-05-28 23:01:04,2012-05-28 23:01:04,closed,,0.8.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/973,b'HDFStore does not conserve index correctly',"b'I have DataFrame with DateRange index, and i store it with HDFStore to h5.\r\n\r\nBut when I retreive my DataFrame, there is a problem with the indices cause I get this error:\r\nException: Index values are not unique\r\n\r\nHere are the commands that produce the exception, I didn\'t test every possible case so I keep the commands in between that I executed:\r\n\r\ndffc=pandas.DataFrame(vpp.forecast.data, index=dr_year_900[:-1])\r\ndffc[start_dt:stop_dt]\r\ndffc.ix[start_dt:stop_dt]\r\ndffc.drop([\'Counter\'], axis=1)\r\ndffc=dffc.drop([\'Counter\'], axis=1)\r\ndffc.ix[start_dt:stop_dt]\r\ndffc[\'EFor\'] = dffc[\'Power\'] - dffc[\'Forecast\']\r\ndffc.ix[start_dt:stop_dt]\r\ndffc = dffc.rename(columns={\'Power\':\'PRef\', \'Forecast\':\'PFor\'})\r\ndffc\r\ndffc.ix[start_dt:stop_dt]\r\ndfs = pandas.HDFStore(\'test.h5\', \'w\')\r\ndfs.put(\'dffc\', dffc)\r\ndfs.close()\r\ndel(dffc)\r\ndfs = pandas.HDFStore(\'test.h5\', \'r\')\r\ndffc\r\ndffc = dfs[\'dffc\']\r\ndffc\r\ndffc.ix[start_dt:stop_dt]\r\n==> here is the output:\r\nIn [49]: dffc.ix[start_dt:stop_dt]\r\nOut[49]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nDateRange: 961 entries, 2010-01-01 00:00:00 to 2010-01-11 00:00:00\r\noffset: <900 Seconds>\r\nData columns:\r\nCounter        961  non-null values\r\nForecast       961  non-null values\r\nImbalance      961  non-null values\r\nNegImbPrice    961  non-null values\r\nPosImbPrice    961  non-null values\r\nPower          961  non-null values\r\nPrice          961  non-null values\r\ndtypes: float64(6), object(1)\r\n\r\nIn [50]: dffc.drop([\'Counter\'], axis=1)\r\nOut[50]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nDateRange: 35040 entries, 2010-01-01 00:00:00 to 2010-12-31 23:45:00\r\noffset: <900 Seconds>\r\nData columns:\r\nForecast       35040  non-null values\r\nImbalance      35040  non-null values\r\nNegImbPrice    35040  non-null values\r\nPosImbPrice    35040  non-null values\r\nPower          35040  non-null values\r\nPrice          35040  non-null values\r\ndtypes: float64(6)\r\n\r\nIn [51]: dffc=dffc.drop([\'Counter\'], axis=1)\r\n\r\nIn [52]: dffc.ix[start_dt:stop_dt]\r\nOut[52]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nDateRange: 961 entries, 2010-01-01 00:00:00 to 2010-01-11 00:00:00\r\noffset: <900 Seconds>\r\nData columns:\r\nForecast       961  non-null values\r\nImbalance      961  non-null values\r\nNegImbPrice    961  non-null values\r\nPosImbPrice    961  non-null values\r\nPower          961  non-null values\r\nPrice          961  non-null values\r\ndtypes: float64(6)\r\n\r\nIn [53]: dffc[\'EFor\'] = dffc[\'Power\'] - dffc[\'Forecast\']\r\n\r\nIn [54]: dffc.ix[start_dt:stop_dt]\r\nOut[54]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nDateRange: 961 entries, 2010-01-01 00:00:00 to 2010-01-11 00:00:00\r\noffset: <900 Seconds>\r\nData columns:\r\nForecast       961  non-null values\r\nImbalance      961  non-null values\r\nNegImbPrice    961  non-null values\r\nPosImbPrice    961  non-null values\r\nPower          961  non-null values\r\nPrice          961  non-null values\r\nEFor           961  non-null values\r\ndtypes: float64(7)\r\n\r\nIn [55]: dffc = dffc.rename(columns={\'Power\':\'PRef\', \'Forecast\':\'PFor\'})\r\n\r\nIn [56]: dffc\r\nOut[56]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nDateRange: 35040 entries, 2010-01-01 00:00:00 to 2010-12-31 23:45:00\r\noffset: <900 Seconds>\r\nData columns:\r\nPFor           35040  non-null values\r\nImbalance      35040  non-null values\r\nNegImbPrice    35040  non-null values\r\nPosImbPrice    35040  non-null values\r\nPRef           35040  non-null values\r\nPrice          35040  non-null values\r\nEFor           35040  non-null values\r\ndtypes: float64(7)\r\n\r\nIn [57]: dffc.ix[start_dt:stop_dt]\r\nOut[57]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nDateRange: 961 entries, 2010-01-01 00:00:00 to 2010-01-11 00:00:00\r\noffset: <900 Seconds>\r\nData columns:\r\nPFor           961  non-null values\r\nImbalance      961  non-null values\r\nNegImbPrice    961  non-null values\r\nPosImbPrice    961  non-null values\r\nPRef           961  non-null values\r\nPrice          961  non-null values\r\nEFor           961  non-null values\r\ndtypes: float64(7)\r\n\r\nIn [58]: dfs = pandas.HDFStore(\'test.h5\', \'w\')\r\n\r\nIn [59]: dfs.put(\'dffc\', dffc)\r\n\r\nIn [60]: dfs.close()\r\n\r\nIn [61]: del(dffc)\r\n\r\nIn [62]: dfs = pandas.HDFStore(\'test.h5\', \'r\')\r\n\r\nIn [63]: dffc\r\n------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""<ipython console>"", line 1, in <module>\r\nNameError: name \'dffc\' is not defined\r\n\r\n\r\nIn [64]: dffc = dfs[\'dffc\']\r\n\r\nIn [65]: dffc\r\nOut[65]: \r\n<class \'pandas.core.frame.DataFrame\'>\r\nIndex: 35040 entries, 2010-01-01 00:00:00 to 2010-12-31 23:45:00\r\nData columns:\r\nPFor           35040  non-null values\r\nImbalance      35040  non-null values\r\nNegImbPrice    35040  non-null values\r\nPosImbPrice    35040  non-null values\r\nPRef           35040  non-null values\r\nPrice          35040  non-null values\r\nEFor           35040  non-null values\r\ndtypes: float64(7)\r\n\r\nIn [66]: dffc.ix[start_dt:stop_dt]\r\n------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""<ipython console>"", line 1, in <module>\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas-0.7.1-py2.7-win32.egg\\pandas\\core\\indexing.py"", line 35, in __getitem__\r\n    return self._getitem_axis(key, axis=0)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas-0.7.1-py2.7-win32.egg\\pandas\\core\\indexing.py"", line 167, in _getitem_axis\r\n    return self._get_slice_axis(key, axis=axis)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas-0.7.1-py2.7-win32.egg\\pandas\\core\\indexing.py"", line 345, in _get_slice_axis\r\n    i, j = labels.slice_locs(start, stop)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas-0.7.1-py2.7-win32.egg\\pandas\\core\\index.py"", line 819, in slice_locs\r\n    beg_slice = self.get_loc(start)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas-0.7.1-py2.7-win32.egg\\pandas\\core\\index.py"", line 499, in get_loc\r\n    return self._engine.get_loc(key)\r\n  File ""engines.pyx"", line 101, in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2498)\r\n  File ""engines.pyx"", line 107, in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2447)\r\nException: Index values are not unique\r\n\r\nHow can I avoid this problem?\r\n'"
969,3811848,ijmcf,wesm,2012-03-26 16:34:55,2012-05-29 00:20:09,2012-05-29 00:20:09,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/969,b'DateRange.tz_normalize(tz) problem when tz=pytz.utc',"b""Hello\r\n\r\nI believe there is a problem when using DateRange.tz_normalize(tz) when the tz is pytz.utc (but not other pytz timezones):\r\n\r\nI create a DateRange using a DateOffset that is Week(weekday=1), and then use the built-in DateRange methods tz_localize and tz_normalize to account for time zones. However, I get something weird if the time zone I use is pytz.UTC:\r\n\r\n>>> dr = pandas.DateRange(start=start, end=end, offset=pandas.core.datetools.Week(weekday=1))\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 Week: kwds={'weekday': 1}, weekday=1>, tzinfo: None\r\n[2010-04-13 00:00:00, ..., 2012-03-06 00:00:00]\r\nlength: 100 \r\n\r\nNow localize:\r\n>>> drl = dr.tz_localize(pytz.timezone('US/Eastern'))\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 Week: kwds={'weekday': 1}, weekday=1>, tzinfo: US/Eastern\r\n[2010-04-13 00:00:00-04:00, ..., 2012-03-06 00:00:00-05:00]\r\nlength: 100 \r\n\r\nAnd normalize:\r\n>>>  drl.tz_normalize(pytz.timezone('UTC'))\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 Week: kwds={'weekday': 1}, weekday=1>, tzinfo: UTC\r\n[2010-04-13 00:00:00+00:00, ..., 2012-03-06 00:00:00+00:00]\r\nlength: 100 \r\n\r\nBzzzt. The time zone info has changed, but the times have remained the same! \r\n\r\nBut if I use any other timezone, it works:\r\n>>> drl.tz_normalize(pytz.timezone('Europe/London'))\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 Week: kwds={'weekday': 1}, weekday=1>, tzinfo: Europe/London\r\n[2010-04-13 05:00:00+01:00, ..., 2012-03-06 05:00:00+00:00]\r\nlength: 100 \r\n\r\nOr even with 'Etc/UTC':\r\n>>> drl.tz_normalize(pytz.timezone('Etc/UTC'))\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 Week: kwds={'weekday': 1}, weekday=1>, tzinfo: Etc/UTC\r\n[2010-04-13 04:00:00+00:00, ..., 2012-03-06 05:00:00+00:00]\r\nlength: 100\r\n\r\nWhy does the DateRange.tz_localize() method fail with 'UTC' but work with other timezones (including 'Etc/UTC')? I looked at the code for DateRange.tz_localize(), and it's a very simple invocation of pytz tz.normalize(). \r\n\r\nBut the pytz documentation (which isn't easy to parse, IMO) suggests that when using UTC normalize() alone is insufficient. For example:\r\n>>> dt =  datetime.datetime(2012, 3, 26, 12, 0)\r\n>>> dtl = pytz.timezone('US/Eastern').localize(dt)\r\ndatetime.datetime(2012, 3, 26, 12, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)\r\n>>> print pytz.timezone('Etc/UTC').normalize(dtl)  <- as expected\r\n2012-03-26 16:00:00+00:00\r\n>>> print pytz.timezone('UTC').normalize(dtl)  <- bzzzt\r\n2012-03-26 12:00:00+00:00\r\n>>> print dtl.astimezone(pytz.timezone('UTC')) <- as expected\r\n2012-03-26 16:00:00+00:00\r\n>>> print pytz.timezone('UTC').normalize(dtl.astimezone(pytz.timezone('UTC')))   <- as expected\r\n2012-03-26 16:00:00+00:00\r\n\r\nWhy this behavior of normalize should be limited to the canonical pytz UTC timezone (versus e.g. 'Etc/UTC'), I don't know, but it appears to be due to the fact that pytz.utc is a specialized singleton class that has different character than the other TZ classes.\r\n\r\nI would suggest that DateRange.tz_normalize() should use either dt.astimezone(pytz.utc) or pytz.utc.normalize(dt.astimezone(pytz.utc)) at line 417.\r\n\r\nIain"""
958,3777520,CRP,wesm,2012-03-23 09:58:51,2012-04-10 04:55:58,2012-04-10 04:55:58,closed,,0.7.3,1,Bug,https://api.github.com/repos/pydata/pandas/issues/958,b'DataFrame bar plot ignores kwds argument if subplots==True',"b""This plots black bars, as expected:\r\nSeries(randn(10)).plot(kind='bar',color='black')\r\n\r\nWhile this plots bars in default blue color, ignoring color keyword:\r\nDataFrame(randn(10,3)).plot(kind='bar',subplots=True,color='black')\r\n"""
953,3765729,jcreixell,wesm,2012-03-22 16:11:16,2012-04-09 20:06:23,2012-04-09 20:06:23,closed,,0.7.3,2,Bug,https://api.github.com/repos/pydata/pandas/issues/953,b'ValueError when slicing with NaN values',"b""this didn't fail before:\r\n\r\n\r\nIn [30]: a = DataFrame([[1,2,3],['b',None,'a'],[1,'b',9]])\r\n\r\nIn [31]: a\r\nOut[31]: \r\n   0     1  2\r\n0  1     2  3\r\n1  b  None  a\r\n2  1     b  9\r\n\r\nIn [32]: a[a[1]=='a']\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/Users/jorge/<ipython-input-32-3a0cb4ca7415> in <module>()\r\n----> 1 a[a[1]=='a']\r\n\r\n/Users/jorge/.python_unstable/lib/python2.7/site-packages/pandas-0.7.2-py2.7-macosx-10.7-intel.egg/pandas/core/frame.pyc in __getitem__(self, key)\r\n   1422 \r\n   1423             # also raises Exception if object array with NA values\r\n\r\n-> 1424             if com._is_bool_indexer(key):\r\n   1425                 key = np.asarray(key, dtype=bool)\r\n   1426             return self._getitem_array(key)\r\n\r\n/Users/jorge/.python_unstable/lib/python2.7/site-packages/pandas-0.7.2-py2.7-macosx-10.7-intel.egg/pandas/core/common.pyc in _is_bool_indexer(key)\r\n    321         if not lib.is_bool_array(key):\r\n    322             if isnull(key).any():\r\n--> 323                 raise ValueError('cannot index with vector containing '\r\n    324                                  'NA / NaN values')\r\n    325             return False\r\n\r\nValueError: cannot index with vector containing NA / NaN values\r\n\r\n\r\nNote: With integer values the exception is not raised:\r\n\r\n\r\nIn [34]: a = DataFrame([[1,2,3],['b',None,'a'],[1,2,9]])\r\n\r\nIn [35]: a\r\nOut[35]: \r\n   0   1  2\r\n0  1   2  3\r\n1  b NaN  a\r\n2  1   2  9\r\n\r\nIn [36]: a[a[1]=='a']\r\nOut[36]: \r\nEmpty DataFrame\r\nColumns: array([0, 1, 2])\r\nIndex: array([], dtype=int64)\r\n"""
948,3753152,adamklein,adamklein,2012-03-21 20:45:38,2012-03-21 21:45:02,2012-03-21 21:45:02,closed,,0.7.3,0,Bug;Prio-low,https://api.github.com/repos/pydata/pandas/issues/948,b'inconsistent comparison operation results',"b'In [4]: i = Index([1,2,3,4])\r\n\r\nIn [5]: i\r\nOut[5]: Int64Index([1, 2, 3, 4])\r\n\r\nIn [6]: i == i\r\nOut[6]: Int64Index([   1,    1,    1,    1])\r\n\r\nIn [7]: i == 0\r\nOut[7]: array([False, False, False, False], dtype=bool)\r\n\r\nIn [8]: i == 1\r\nOut[8]: array([ True, False, False, False], dtype=bool)\r\n\r\nIn [9]: i == np.array([1,2,3,4])\r\nOut[9]: array([ True,  True,  True,  True], dtype=bool)\r\n\r\nIn [10]: i == Index([1,2,3,4])\r\nOut[10]: Int64Index([   1,    1,    1,    1])\r\n'"
941,3714134,wesm,wesm,2012-03-19 17:18:14,2012-03-19 17:43:44,2012-03-19 17:43:44,closed,,0.7.3,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/941,b'Pass more than column names to rows/cols args in pivot_table',b'Should be same argument types as groupby'
934,3697802,dalejung,wesm,2012-03-18 02:01:11,2012-04-02 19:16:24,2012-04-02 19:15:29,closed,,0.7.3,1,Bug,https://api.github.com/repos/pydata/pandas/issues/934,b'ddof no longer passed for Series.std and Series.var',"b""As part of the following commit:\r\n\r\nhttps://github.com/pydata/pandas/commit/fbb110247ea81e980bf9265cc1b0b26c8eb549be#L5L799\r\n\r\nSeries.std and Series.var no longer pass along ddof to the nanops.nanvar function. \r\n\r\nAlso since ddof defaults to 1 in pandas and 0 in numpy.\r\n\r\n````python\r\narr = np.arange(10)\r\ns = Series(arr)\r\n# False\r\nnp.std(s) == np.std(arr)\r\n# True\r\nnp.std(s) == np.std(arr, ddof=1)\r\n````\r\n\r\nWhich I don't know if it's a bug but it's non-intuitive."""
933,3693175,xdong,wesm,2012-03-17 07:08:37,2012-04-09 20:05:54,2012-04-09 20:05:54,closed,,0.7.3,2,Bug,https://api.github.com/repos/pydata/pandas/issues/933,"b""(series == x) doesn't always produce a boolean series: bug?""","b""In [2]: a = pandas.Series(['a', nan, 'b'])\r\n\r\nIn [3]: a == 'a'\r\nOut[3]: \r\n0     True\r\n1      NaN\r\n2    False\r\n\r\n\r\n# There is a NaN in Out[3]. This can't be the desired result because of the following error:\r\n\r\n\r\n\r\nIn [4]: b = pandas.Series(['a', 'b', 'c'])\r\n\r\nIn [5]: (a == 'a') & (b == 'a')\r\nValueError                                Traceback (most recent call last)\r\n/home/dong/<ipython-input-5-89f377c08b91> in <module>()\r\n----> 1 (a == 'a') & (b == 'a')\r\n\r\n/home/dong/.virtualenv/src/pandas/pandas/core/series.pyc in wrapper(self, other)\r\n    143         if isinstance(other, Series):\r\n    144             name = _maybe_match_name(self, other)\r\n--> 145             return Series(na_op(self.values, other.values),\r\n    146                           index=self.index, name=name)\r\n    147         elif isinstance(other, DataFrame):\r\n\r\n/home/dong/.virtualenv/src/pandas/pandas/core/series.pyc in na_op(x, y)\r\n    132 \r\n    133             if isinstance(y, np.ndarray):\r\n--> 134                 result = lib.vec_binop(x, y, op)\r\n    135             else:\r\n    136                 result = lib.scalar_binop(x, y, op)\r\n\r\n/home/dong/.virtualenv/src/pandas/pandas/_tseries.so in pandas._tseries.vec_binop (pandas/src/tseries.c:9302)()\r\n\r\nValueError: Does not understand character buffer dtype format string ('?')\r\n\r\n\r\n\r\n# BTW for series of dtype float there is no such problem:\r\n\r\nIn [6]: c = pandas.Series([1.0, nan, 3.0])\r\n\r\nIn [7]: c == 1\r\nOut[7]: \r\n0     True\r\n1    False\r\n2    False\r\n\r\n\r\n"""
925,3682815,wesm,wesm,2012-03-16 14:22:36,2012-03-16 16:11:29,2012-03-16 16:11:29,closed,,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/925,b'Inconsistent NA handling in comparison ops with dtype=object',"b""```\r\ntest=PD.Series(index=PD.DateRange(DT.datetime(2012,3,15), DT.datetime(2012,3,16)), data=NP.ones(2))\r\ntest == test.shift(1)\r\n```\r\n\r\nvs\r\n\r\n```\r\ntest2=PD.Series(index=PD.DateRange(DT.datetime(2012,3,15), DT.datetime(2012,3,16)), data=['a','a'])\r\ntest2 == test2.shift(1)\r\n```"""
917,3660227,wesm,wesm,2012-03-15 02:41:35,2012-03-15 02:49:31,2012-03-15 02:49:31,closed,,,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/917,b'Potential out-of-bounds array access in Series',"b'Can cause segfault if dtype=object\r\n\r\n```\r\ns = Series([], index=[])\r\ns[-1]\r\n```'"
916,3659998,wesm,wesm,2012-03-15 02:05:48,2012-03-15 02:28:16,2012-03-15 02:26:50,closed,,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/916,b'Malformed block from DataFrame.GroupBy',"b'When excluding nuisance columns, block ref_items are not updated'"
911,3644419,lodagro,lodagro,2012-03-14 08:15:11,2012-03-14 20:16:42,2012-03-14 10:08:53,closed,,0.7.2,1,Bug,https://api.github.com/repos/pydata/pandas/issues/911,b'DataFrame.set_value() issue',"b""Accedentically swapping the index and column label in using ```DataFrame.set_value()``` gave an unexpected result.\r\nAll values, except the one to be set, in the *DataFrame* became *NaN*.\r\n\r\n```python\r\nIn [4]: df\r\nOut[4]:\r\n          A         B         C\r\n0  0.463981  0.163885  0.644793\r\n1  0.979047  0.347373  0.736267\r\n2  0.103155  0.304439  0.911314\r\n\r\nIn [5]: df.set_value('C', 2, 1.0)\r\nOut[5]:\r\n    A   B   C   2\r\n0 NaN NaN NaN NaN\r\n1 NaN NaN NaN NaN\r\n2 NaN NaN NaN NaN\r\nC NaN NaN NaN   1\r\n\r\nIn [6]: pandas.__version__\r\nOut[6]: '0.7.2.dev-79decd7'\r\n```"""
910,3639116,aman-thakral,wesm,2012-03-13 21:49:05,2012-03-15 01:10:22,2012-03-15 01:09:59,closed,,0.7.2,3,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/910,b'Error when subtracting mixed type DataFrame',"b'Sample code to reproduce the error:\r\n\r\nimport numpy as np\r\nimport pandas\r\n\r\nX = np.random.rand(10,10)\r\nY = np.ones((10,1),dtype=int)\r\ndf1 = pandas.DataFrame(X)\r\ndf1 = df1.join(pandas.DataFrame(Y),lsuffix=\'.X\')\r\n\r\nprint df1-df1.mean()\r\n\r\n-----------------------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""pls_model.py"", line 383, in <module>\r\n    run(df)\r\n  File ""pls_model.py"", line 305, in run\r\n    report(df.copy(),n_comp,priors=priors,plot=False)\r\n  File ""pls_model.py"", line 83, in report\r\n    plsgnb.fit(X,Y,priors=priors)\r\n  File ""C:\\src\\multivariate\\src\\multivariate\\PLS.py"", line 477, in fit\r\n    self.model.fit(X,Y)\r\n  File ""C:\\src\\multivariate\\src\\multivariate\\PLS.py"", line 145, in fit\r\n    print x - self.Xmean\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 177, in f\r\n    return self._combine_series(other, func, fill_value, axis, level)\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 2526, in _combine_series\r\n    return self._combine_series_infer(other, func, fill_value)\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 2541, in _combine_series_infer\r\n    return self._combine_match_columns(other, func, fill_value)\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 2552, in _combine_match_columns\r\n    left, right = self.align(other, join=\'outer\', axis=1, copy=False)\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 1735, in align\r\n    method=method)\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 1804, in _align_series\r\n    return (left_result.fillna(fill_value, method=method),\r\n  File ""C:\\src\\pandas\\pandas\\core\\frame.py"", line 2426, in fillna\r\n    new_data = self._data.fillna(value)\r\n  File ""C:\\src\\pandas\\pandas\\core\\internals.py"", line 938, in fillna\r\n    new_blocks = [b.fillna(value) for b in self.blocks]\r\n  File ""C:\\src\\pandas\\pandas\\core\\internals.py"", line 211, in fillna\r\n    new_values.flat[mask] = value\r\nValueError: cannot convert float NaN to integer\r\n\r\n'"
900,3602535,adamklein,changhiskhan,2012-03-11 18:27:25,2012-05-01 17:05:56,2012-05-01 17:05:56,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/900,"b'timeseries branch, fix labels in plotting'",b'They come out as ints'
896,3592363,timmie,wesm,2012-03-10 01:08:46,2012-04-21 17:45:17,2012-04-21 17:45:17,closed,,0.8.0,7,Bug;Prio-medium;Timeseries,https://api.github.com/repos/pydata/pandas/issues/896,b'DateRange for 12 month not created',"b""Target: create a 12 month date range:\r\n\r\nbase data\r\n```\r\nrng = DateRange('1/1/2000 00:00', periods=8760, offset=datetools.Hour())\r\nts = Series(randn(len(rng)), index=rng)\r\n```\r\n\r\nthe range\r\n\r\n```\r\nts.index[0]\r\nOut[95]: datetime.datetime(2000, 1, 1, 0, 0)\r\n\r\nts.index[-1]\r\nOut[96]: datetime.datetime(2000, 12, 30, 23, 0)\r\n\r\nmrg = DateRange(start=ts.index[0], end=ts.index[-1], offset=datetools.MonthEnd())\r\n\r\nmrg\r\nOut[98]: \r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 MonthEnd>, tzinfo: None\r\n[2000-01-31 00:00:00, ..., 2000-11-30 00:00:00]\r\nlength: 11\r\n```\r\n\r\nWhy do I not get 12 months? Jan-Dec?\r\nAnd how could I get a range out of 12 months from the ts?"""
892,3585494,ghost,wesm,2012-03-09 16:32:02,2012-03-13 01:29:02,2012-03-13 01:29:02,closed,,0.7.2,4,Bug,https://api.github.com/repos/pydata/pandas/issues/892,b'Weird None /nan behaviour',"b""Somehow I can't set single NaN entries to be None, but I can set a whole column of NaN entries to a column of None entries.  ( x[1].ix[3]=None doesnt work either)\r\n\r\n>>> from pandas import *\r\n>>> x = DataFrame(columns=[0,1], index=[3,4])\r\n>>> x\r\n    0   1\r\n3 NaN NaN\r\n4 NaN NaN\r\n\r\n>>> x[0]= [None,None]   # this works fine\r\n>>> x\r\n      0   1\r\n3  None NaN\r\n4  None NaN\r\n\r\n>>> x[1][3]=None   # this has no effect  and is my issue\r\n>>> x\r\n      0   1\r\n3  None NaN\r\n4  None NaN\r\n\r\n\r\n"""
891,3583502,lodagro,wesm,2012-03-09 14:15:05,2012-03-15 21:10:38,2012-03-15 21:09:53,closed,,0.7.2,2,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/891,b'DataFrame.to_html encoding',"b""see also [mailing list](https://groups.google.com/forum/?fromgroups#!topic/pystatsmodels/e5J1yvCGmmI)\r\n\r\n```python\r\nIn [34]: df = pandas.DataFrame({'A': [u'\\u03c3']})\r\n\r\nIn [35]: df\r\nOut[35]:\r\n   A\r\n0  \xa6\xd2\r\n\r\nIn [36]: print df.to_html()\r\n---------------------------------------------------------------------------\r\nUnicodeEncodeError                        Traceback (most recent call last)\r\n...\r\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03c3' in position 1: ordinal not in range(128)\r\n\r\nIn [37]:  \r\n```\r\n\r\nfurther more from the thread:\r\n\r\n```\r\n... I wanted to spot as well that\r\nforce_unicode is in the doc but is not available.\r\nTypeError: to_html() got an unexpected keyword argument 'force_unicode'\r\n```"""
890,3583428,lodagro,wesm,2012-03-09 14:07:51,2012-03-11 16:26:59,2012-03-11 16:26:59,closed,,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/890,b'DataFrame.to_html columns broken',"b'Reported on [mailing list](https://groups.google.com/forum/?fromgroups#!topic/pystatsmodels/lK1odlK5zjU), columns argument in ```DataFrame.to_html()``` broken.\r\n\r\n```python\r\nIn [31]: df\r\nOut[31]:\r\n   a         b         c   d\r\n0  4 -0.548572 -0.333272 NaN\r\n1  1 -0.502314  0.975798 NaN\r\n2  9  0.283227  1.486831 NaN\r\n3  3  1.514658 -0.747403 NaN\r\n4  7 -1.415763 -0.697034 NaN\r\n\r\nIn [32]: print df.to_html(columns=[\'a\'])\r\n<table border=""1"">\r\n  <thead>\r\n    <tr>\r\n      <th></th>\r\n      <th>a</th>\r\n      <th>b</th>\r\n      <th>c</th>\r\n      <th>d</th>\r\n    </tr>\r\n    </thead>\r\n    <tbody>\r\n    <tr>\r\n      <td><strong>0</strong></td>\r\n      <td> 4</td>\r\n      <td>-0.548572</td>\r\n      <td>-0.333272</td>\r\n      <td>NaN</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>1</strong></td>\r\n      <td> 1</td>\r\n      <td>-0.502314</td>\r\n      <td> 0.975798</td>\r\n      <td>NaN</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>2</strong></td>\r\n      <td> 9</td>\r\n      <td> 0.283227</td>\r\n      <td> 1.486831</td>\r\n      <td>NaN</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>3</strong></td>\r\n      <td> 3</td>\r\n      <td> 1.514658</td>\r\n      <td>-0.747403</td>\r\n      <td>NaN</td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>4</strong></td>\r\n      <td> 7</td>\r\n      <td>-1.415763</td>\r\n      <td>-0.697034</td>\r\n      <td>NaN</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n```'"
881,3566559,adamklein,adamklein,2012-03-08 17:05:02,2012-03-08 17:41:16,2012-03-08 17:41:16,closed,adamklein,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/881,b'dataframe slice assignment failure',"b""\r\n    In [27]: df\r\n    Out[27]: \r\n       a         b         c   d\r\n    0  2  0.103795 -0.066523 NaN\r\n    1  3 -0.939895 -0.208760 NaN\r\n    2  9  1.968456  0.419374 NaN\r\n    3  6 -0.105170  0.162064 NaN\r\n    4  7  0.381707  2.126133 NaN\r\n\r\n    In [28]: df[df['a']==3]\r\n    Out[28]: \r\n       a         b        c   d\r\n    1  3 -0.939895 -0.20876 NaN\r\n\r\n    In [29]: df[df['a']==2]\r\n    Out[29]: \r\n       a         b         c   d\r\n    0  2  0.103795 -0.066523 NaN\r\n\r\n    In [30]: df[df['a']==2] = 100\r\n    ---------------------------------------------------------------------------\r\n    IndexingError                             Traceback (most recent call last)\r\n    /home/adam/code/pandas/<ipython-input-30-f7da2453a9f6> in <module>()\r\n    ----> 1 df[df['a']==2] = 100\r\n\r\n    /home/adam/code/pandas/pandas/core/frame.py in __setitem__(self, key, value)\r\n       1491             self._boolean_set(key, value)\r\n       1492         elif isinstance(key, (np.ndarray, list)):\r\n    -> 1493             return self._set_item_multiple(key, value)\r\n       1494         else:\r\n       1495             # set column\r\n\r\n\r\n    /home/adam/code/pandas/pandas/core/frame.py in _set_item_multiple(self, keys, value)\r\n       1516                 self[k1] = value[k2]\r\n       1517         else:\r\n    -> 1518             self.ix[:, keys] = value\r\n       1519 \r\n       1520     def _set_item(self, key, value):\r\n\r\n    /home/adam/code/pandas/pandas/core/indexing.py in __setitem__(self, key, value)\r\n         59                 raise IndexingError('only tuples of length <= %d supported',\r\n         60                                     self.ndim)\r\n    ---> 61             indexer = self._convert_tuple(key)\r\n         62         else:\r\n         63             indexer = self._convert_to_indexer(key)\r\n\r\n    /home/adam/code/pandas/pandas/core/indexing.py in _convert_tuple(self, key)\r\n         68         keyidx = []\r\n         69         for i, k in enumerate(key):\r\n    ---> 70             idx = self._convert_to_indexer(k, axis=i)\r\n         71             keyidx.append(idx)\r\n         72         return tuple(keyidx)\r\n\r\n    /home/adam/code/pandas/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis)\r\n        282         elif _is_list_like(obj):\r\n        283             if com._is_bool_indexer(obj):\r\n    --> 284                 objarr = _check_bool_indexer(labels, obj)\r\n        285                 return objarr\r\n        286             else:\r\n\r\n    /home/adam/code/pandas/pandas/core/indexing.py in _check_bool_indexer(ax, key)\r\n        417         mask = com.isnull(result)\r\n        418         if mask.any():\r\n    --> 419             raise IndexingError('cannot index with vector containing '\r\n        420                                 'NA / NaN values')                                                                                   \r\n        421 \r\n\r\n    IndexingError: cannot index with vector containing NA / NaN values\r\n"""
880,3566418,adamklein,adamklein,2012-03-08 16:56:48,2012-03-08 18:15:50,2012-03-08 18:15:48,closed,,0.7.2,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/880,b'failure in multiindex-based access in  Panel',"b""Example:\r\n\r\nimport pandas as pn\r\nind = pn.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b',2)], names=['fist', 'second'])\r\nwp = pn.Panel(np.random.random((4,5,5)), items=ind,\r\nmajor_axis=np.arange(5), minor_axis=np.arange(5))\r\n\r\nIn [10]: wp['a']\r\n...\r\nKeyError: 'no item named a'\r\n\r\nIn [11]: wp.ix['a']\r\n...\r\nKeyError: 'no item named a'\r\n\r\n\r\n"""
871,3521384,lbeltrame,adamklein,2012-03-06 09:33:27,2012-03-06 21:34:15,2012-03-06 21:34:14,closed,,0.7.2,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/871,b'concat on axis=1 and ignore_index=True raises TypeError',"b'Test case:\r\n\r\n```python\r\n\r\n>>> frame1 = pandas.DataFrame({""test1"":[""a"", ""b"", ""c""], ""test2"":[1,2,3], ""test3"": [4.5, 3.2, 1.2]})\r\n>>> frame2 = pandas.DataFrame({""test4"": [5.2, 2.2, 4.3]})\r\n>>> pandas.concat([frame1, frame2], ignore_index=True, axis=1)\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names,\r\n    689                        keys=keys, levels=levels, names=names,\r\n    690                        verify_integrity=verify_integrity)\r\n--> 691     return op.get_result()\r\n    692 \r\n    693 \r\n\r\n/usr/lib64/python2.7/site-packages//pandas/tools/merge.pyc in get_result(self)\r\n    759                              columns=self.new_axes[1])\r\n    760         else:\r\n--> 761             new_data = self._get_concatenated_data()\r\n    762             return self.objs[0]._from_axes(new_data, self.new_axes)\r\n    763 \r\n\r\n/usr/lib64/python2.7/site-packages/pandas/tools/merge.pyc in _get_concatenated_data(self)\r\n    776             for kind in kinds:\r\n    777                 klass_blocks = [mapping.get(kind) for mapping in blockmaps]\r\n--> 778                 stacked_block = self._concat_blocks(klass_blocks)\r\n    779                 new_blocks.append(stacked_block)\r\n    780             new_data = BlockManager(new_blocks, self.new_axes)\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/tools/merge.pyc in _concat_blocks(self, blocks)\r\n    832                 concat_items = _concat_indexes(all_items)\r\n    833 \r\n--> 834             return make_block(concat_values, concat_items, self.new_axes[0])\r\n    835 \r\n    836     def _concat_single_item(self, item):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc in make_block(values, items, ref_items, do_integrity_check)\r\n    287 \r\n    288     return klass(values, items, ref_items, ndim=values.ndim,\r\n--> 289                  do_integrity_check=do_integrity_check)\r\n    290 \r\n    291 # TODO: flexible with index=None and/or items=None\r\n\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc in __init__(self, values, items, ref_items, ndim, do_integrity_check)\r\n     28         self.ndim = ndim\r\n     29         self.items = _ensure_index(items)\r\n---> 30         self.ref_items = _ensure_index(ref_items)\r\n     31 \r\n     32         if do_integrity_check:\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/index.pyc in _ensure_index(index_like)\r\n   2152     if hasattr(index_like, \'name\'):\r\n   2153         return Index(index_like, name=index_like.name)\r\n-> 2154     return Index(index_like)\r\n   2155 \r\n   2156 def _validate_join_method(method):\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/index.pyc in __new__(cls, data, dtype, copy, name)\r\n     70         else:\r\n     71             # other iterable of some kind\r\n\r\n---> 72             subarr = com._asarray_tuplesafe(data, dtype=object)\r\n     73 \r\n     74         if lib.is_integer_array(subarr) and dtype is None:\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/common.pyc in _asarray_tuplesafe(values, dtype)\r\n    493 def _asarray_tuplesafe(values, dtype=None):\r\n    494     if not isinstance(values, (list, tuple, np.ndarray)):\r\n--> 495         values = list(values)\r\n    496 \r\n    497     if isinstance(values, list) and dtype in [np.object_, object]:\r\n\r\nTypeError: \'NoneType\' object is not iterable\r\n````\r\n\r\nFrom today\'s git.'"
870,3517464,welch,adamklein,2012-03-06 01:22:25,2012-03-06 16:56:02,2012-03-06 16:56:02,closed,,0.7.2,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/870,b'nosetest failures in reshape',"b'I\'ve just done a git clone/install of pandas on Mac OSX 10.6.8 after a fresh install of EPD_free-7.2 to get the dependencies. The build was successful, but in running nostests I get 27 failures, each bottoming out at:\r\n\r\n  File ""/Users/welch/src/pandas/pandas/core/reshape.py"", line 95, in _make_selectors\r\n    group_mask.put(group_index, True)\r\nTypeError: array cannot be safely cast to required type\r\n\r\nhere are the failed tests:\r\n\r\ntest_crosstab_margins (pandas.tools.tests.test_pivot.TestCrosstab)\r\ntest_crosstab_multiple (pandas.tools.tests.test_pivot.TestCrosstab)\r\ntest_crosstab_ndarray (pandas.tools.tests.test_pivot.TestCrosstab)\r\ntest_crosstab_pass_values (pandas.tools.tests.test_pivot.TestCrosstab)\r\ntest_crosstab_single (pandas.tools.tests.test_pivot.TestCrosstab)\r\ntest_margins (pandas.tools.tests.test_pivot.TestPivotTable)\r\ntest_pivot_integer_columns (pandas.tools.tests.test_pivot.TestPivotTable)\r\ntest_pivot_multi_functions (pandas.tools.tests.test_pivot.TestPivotTable)\r\ntest_pivot_multi_values (pandas.tools.tests.test_pivot.TestPivotTable)\r\ntest_pivot_table (pandas.tools.tests.test_pivot.TestPivotTable)\r\ntest_pivot_table_multiple (pandas.tools.tests.test_pivot.TestPivotTable)\r\ntest_pivot (pandas.tests.test_frame.TestDataFrame)\r\ntest_pivot_empty (pandas.tests.test_frame.TestDataFrame)\r\ntest_stack_unstack (pandas.tests.test_frame.TestDataFrame)\r\ntest_unstack_to_series (pandas.tests.test_frame.TestDataFrame)\r\ntest_frame_describe_multikey (pandas.tests.test_groupby.TestGroupBy)\r\ntest_groupby_multiple_columns (pandas.tests.test_groupby.TestGroupBy)\r\ntest_stack (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_stack_level_name (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_stack_unstack_multiple (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_stack_unstack_preserve_names (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_unstack (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_unstack_bug (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_unstack_level_name (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_unstack_preserve_types (pandas.tests.test_multilevel.TestMultiLevel)\r\ntest_pivot (pandas.tests.test_panel.TestLongPanel)\r\ntest_unstack (pandas.tests.test_series.TestSeries)\r\n\r\nAny advice for me? \r\n(And is this the proper place for such a question?)'"
869,3517302,wesm,adamklein,2012-03-06 01:08:39,2012-03-06 22:01:21,2012-03-06 22:01:21,closed,,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/869,b'Series.count cannot accept a string (level name) in the level argument',
866,3510201,adamklein,wesm,2012-03-05 17:26:58,2012-05-10 05:40:08,2012-05-10 05:40:08,closed,,0.8.0,1,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/866,b'resolve forecast incompatibilities in statsmodels',"b'cross-linking to statsmodel issue\r\n\r\nhttps://github.com/statsmodels/statsmodels/issues/168\r\n\r\nWe may have to change some of the forecast dates code when pandas switches over to the ordinal timestamp representation. Right now, our current implementation is broken with the new datetime branch installed. To replicate, install pandas datetime branch and run examples/tsa/ex_dates.py. Make sure to write tests for this whenever the pandas code settles down.\r\n'"
859,3497482,adamklein,adamklein,2012-03-04 17:35:54,2012-03-06 22:33:56,2012-03-06 22:33:56,closed,,0.7.2,3,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/859,b'better err msg for failed boolean slicing of dataframe',"b""In [57]: df = DataFrame(np.random.rand(5,5))\r\n\r\nIn [58]: df[df > 0.5]\r\nERROR: An unexpected error occurred while tokenizing input\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: ('EOF in multi-line statement', (63, 0))\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/Volumes/HD2/adam/Documents/Code/python/rapidquant/<ipython-input-58-8ec7cde9a766> in <module>()\r\n----> 1 df[df > 0.5]\r\n\r\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/frame.pyc in __getitem__(self, key)\r\n   1419             return self._getitem_multilevel(key)\r\n   1420         else:\r\n-> 1421             return self._get_item_cache(key)\r\n   1422 \r\n   1423     def _getitem_array(self, key):\r\n\r\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/generic.pyc in _get_item_cache(self, item)\r\n    280             return cache[item]\r\n    281         except Exception:\r\n--> 282             values = self._data.get(item)\r\n    283             res = self._box_item_values(item, values)\r\n    284             cache[item] = res\r\n\r\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/internals.pyc in get(self, item)\r\n    612 \r\n    613     def get(self, item):\r\n--> 614         _, block = self._find_block(item)\r\n    615         return block.get(item)\r\n    616 \r\n\r\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/internals.pyc in _find_block(self, item)\r\n    702 \r\n    703     def _find_block(self, item):\r\n--> 704         self._check_have(item)\r\n    705         for i, block in enumerate(self.blocks):\r\n    706             if item in block:\r\n\r\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/internals.pyc in _check_have(self, item)\r\n    709     def _check_have(self, item):\r\n    710         if item not in self.items:\r\n--> 711             raise KeyError('no item named %s' % str(item))\r\n    712 \r\n    713     def reindex_axis(self, new_axis, method=None, axis=0, copy=True):\r\n\r\nKeyError: 'no item named        0     1      2      3      4\\n0  False  True  False   True  False\\n1   True  True  False   True   True\\n2   True  True   True   True  False\\n3  False  True  False  False   True\\n4  False  True  False  False   True'\r\n\r\n\r\nAnd then, this actually silently fails:\r\n\r\ndf.ix[df > 0.5]\r\n\r\nIe, it returns the wrong data"""
855,3497260,wesm,wesm,2012-03-04 16:57:52,2012-05-09 18:57:59,2012-05-09 18:57:59,closed,wesm,0.8.0,1,Bug;Enhancement;Prio-high,https://api.github.com/repos/pydata/pandas/issues/855,b'Eradicate usage of int32 in Cython code',"b""Inevitable to cause problems eventually when data sizes continue to increase. Want to insert checks in a few key places to check for int64 overflows-- only a couple places that are likely to happen and don't need to worry about the other places since there are no machines (?) which have over 2^63 bytes of RAM right now."""
851,3493322,wesm,wesm,2012-03-03 23:40:52,2012-03-04 19:36:23,2012-03-04 19:36:23,closed,,0.7.2,1,Bug;Prio-high;Testing,https://api.github.com/repos/pydata/pandas/issues/851,b'Test for possible int64 overflow in groupby (!)',b're: #850'
850,3493301,wesm,wesm,2012-03-03 23:36:59,2012-03-03 23:42:22,2012-03-03 23:42:22,closed,,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/850,b'Fix int32 overflow in groupby',
847,3478669,wesm,wesm,2012-03-02 15:48:22,2012-03-15 16:54:08,2012-03-15 16:54:08,closed,,0.7.2,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/847,"b""Don't raise exception if HDFStore file is read only""",
846,3468266,wesm,wesm,2012-03-01 23:48:54,2012-04-09 14:58:57,2012-04-09 14:58:31,closed,,0.7.3,2,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/846,b'Buggy creation of int DataFrame from data containing NAs',"b'```\r\nIn [5]: df = DataFrame([[np.nan, 1], [1, 0]], dtype=np.int64)\r\n\r\nIn [6]: df\r\nOut[6]: \r\n                     0  1\r\n0 -9223372036854775808  1\r\n1                    1  0\r\n```'"
844,3467328,scouredimage,wesm,2012-03-01 23:11:21,2012-03-15 20:36:07,2012-03-15 20:35:43,closed,,0.7.2,2,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/844,b'Applying np.sum to empty series results in NaN',"b""```python\r\nIn [1]: import pandas\r\n\r\nIn [2]: pandas.__version__\r\nOut[2]: '0.7.1'\r\n\r\nIn [3]: empty = pandas.Series([], index=[])\r\n\r\nIn [4]: import numpy as np\r\n\r\nIn [5]: np.__version__\r\nOut[5]: '1.6.1'\r\n\r\nIn [6]: np.sum(empty)\r\nOut[6]: nan\r\n\r\nIn [7]: np.sum(empty.values)\r\nOut[7]: 0.0\r\n```"""
839,3429079,adamklein,adamklein,2012-02-29 00:19:35,2012-02-29 18:04:21,2012-02-29 18:04:21,closed,,0.7.1,3,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/839,b'int64 Series and NA assignment',"b""I think a bug:\r\n\r\nIn [1]: s = Series(arange(10))\r\n\r\nIn [2]: s\r\nOut[2]: \r\n0    0\r\n1    1\r\n2    2\r\n3    3\r\n4    4\r\n5    5\r\n6    6\r\n7    7\r\n8    8\r\n9    9\r\n\r\nIn [3]: s[::2] = np.nan\r\n\r\nIn [4]: s\r\nOut[4]: \r\n0   -9223372036854775808\r\n1                      1\r\n2   -9223372036854775808\r\n3                      3\r\n4   -9223372036854775808\r\n5                      5\r\n6   -9223372036854775808\r\n7                      7\r\n8   -9223372036854775808\r\n9                      9\r\n\r\nIn [5]: s.dtype\r\nOut[5]: dtype('int64')\r\n\r\nDataframe seems fine:\r\n\r\n\r\nIn [12]: df = DataFrame(np.random.random_integers(5, size=(5,5)))\r\n\r\nIn [13]: df\r\nOut[13]: \r\n   0  1  2  3  4\r\n0  5  5  3  1  2\r\n1  1  3  3  3  2\r\n2  1  5  2  2  3\r\n3  5  3  1  1  4\r\n4  2  5  2  3  2\r\n\r\nIn [14]: df[2] = np.nan \r\n\r\nIn [15]: df\r\nOut[15]: \r\n   0  1   2  3  4\r\n0  5  5 NaN  1  2\r\n1  1  3 NaN  3  2\r\n2  1  5 NaN  2  3\r\n3  5  3 NaN  1  4\r\n4  2  5 NaN  3  2\r\n"""
838,3419580,mattias-lundell,wesm,2012-02-28 14:45:23,2012-02-28 19:32:58,2012-02-28 19:32:58,closed,wesm,0.7.1,3,Bug;Timeseries,https://api.github.com/repos/pydata/pandas/issues/838,b'DateRange - minute vs. minutes',"b""I tried to create a DateRange in five minute steps.\r\n\r\n```In [40]: DateRange(datetime(2011,11,11), datetime(2011,11,12), offset=DateOffset(minute=5))```\r\n\r\nThe %CPU is 100  and it consumes a lot of memory but nothing happens, no result and no error. I continued to experiment and realized that i wrote 'minute' instead of 'minutes'\r\n\r\n```In [41]: DateRange(datetime(2011,11,11), datetime(2011,11,12), offset=DateOffset(minutes=5))```\r\n\r\nMy suggestion is to maybe add some kind of argument check."""
837,3415391,lbeltrame,adamklein,2012-02-28 09:20:30,2012-02-28 18:22:10,2012-02-28 18:22:09,closed,,0.7.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/837,b'ExcelFile throws an exception if parsing a file with only two lines',"b'Example (paste this in an Excel file):\r\n\r\nTest    Test1\r\naaaa   bbbbb\r\n\r\n````python\r\nexcel_data = pandas.ExcelFile(""test.xls"")\r\nparsed = pandas.parse(""Sheet1"")\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc in parse(self, sheetname, header, skiprows, index_col, parse_dates, date_parser, na_values, chunksize)\r\n    629                                    parse_dates=parse_dates,\r\n    630                                    date_parser=date_parser,\r\n--> 631                                    na_values=na_values, chunksize=chunksize)\r\n    632 \r\n    633     def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc in _parse_xls(self, sheetname, header, skiprows, index_col, parse_dates, date_parser, na_values, chunksize)\r\n    684                             date_parser=date_parser,\r\n    685                             skiprows=skiprows,\r\n--> 686                             chunksize=chunksize)\r\n    687 \r\n    688         return parser.get_chunk()\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc in __init__(self, f, delimiter, names, header, index_col, na_values, parse_dates, date_parser, chunksize, skiprows, skip_footer, converters, verbose, encoding)\r\n    262             self.data = f\r\n    263         self.columns = self._infer_columns()\r\n--> 264         self.index_name = self._get_index_name()\r\n    265         self._first_chunk = True                                                                                                                                                                                                           \r\n    266                                                                                                                                                                                                                                            \r\n                                                                                                                                                                                                                                                   \r\n/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc in _get_index_name(self)                                                                                                                  \r\n    381                                                                                                                                                                                                                                            \r\n    382         try:                                                                                                                                                                                                                               \r\n--> 383             next_line = self._next_line()                                                                                                                                                                                                  \r\n    384         except StopIteration:                                                                                                                                                                                                              \r\n    385             next_line = None                                                                                                                                                                                                               \r\n                                                                                                                                                                                                                                                   \r\n/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc in _next_line(self)                                                                                                                       \r\n    351                 self.pos += 1\r\n    352 \r\n--> 353             line = self.data[self.pos]\r\n    354         else:\r\n    355             while self.pos in self.skiprows:\r\n\r\nIndexError: list index out of range\r\n````\r\n\r\nWorks correctly if the number of lines is at least 3.  Pandas is from yesterday\'s git (path names have been slightly trimmed to improve readability).'"
826,3378547,wesm,wesm,2012-02-24 20:18:45,2012-06-11 17:54:35,2012-06-11 17:53:35,closed,,0.8.0,2,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/826,"b'Try to workaround Yahoo! finance 404 errors, better error messages'","b'randomly get this crap\r\n\r\n```\r\n\r\nimport pandas\r\nimport pandas.io.data as pd\r\n\r\ndef get_close(ticker, start, end):\r\n\r\n    stock = pd.DataReader(ticker, \'yahoo\', start, end)\r\n    adj_close = stock[\'Adj Close\']\r\n\r\n    return adj_close[-1]\r\n\r\n\r\nstart = datetime.datetime(2010,1,1)\r\nend = datetime.datetime(2012,1,24)\r\nticker = [\'TLT\', \'IEF\', \'IYR\', \'XLY\', \'XLK\', \'XLU\', \'XLV\', \'XLP\', \'XLI\', \'XLF\', \'XLB\', \'XLE\', \'SPY\', \'EFA\', \'EEM\', \'EWJ\', \'IWM\', \'QQQ\', \'LQD\', \'JNK\']\r\n\r\nfor t in ticker:\r\n    result = get_close(t, start, end)\r\n    print t,result\r\n## -- End pasted text --\r\nTLT 116.17\r\nIEF 104.15\r\nIYR 59.52\r\nXLY 41.25\r\nXLK 26.87\r\nXLU 34.47\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/home/wesm/code/pandas/<ipython-input-7-eb9190ce71e6> in <module>()\r\n     17 \r\n     18 for t in ticker:\r\n---> 19     result = get_close(t, start, end)\r\n     20     print t,result\r\n\r\n/home/wesm/code/pandas/<ipython-input-7-eb9190ce71e6> in get_close(ticker, start, end)\r\n      6 def get_close(ticker, start, end):\r\n      7 \r\n----> 8     stock = pd.DataReader(ticker, \'yahoo\', start, end)\r\n      9     adj_close = stock[\'Adj Close\']\r\n     10 \r\n\r\n/home/wesm/code/pandas/pandas/io/data.pyc in DataReader(name, data_source, start, end)\r\n     51 \r\n     52     if(data_source == ""yahoo""):\r\n---> 53         return get_data_yahoo(name=name, start=start, end=end)\r\n     54     elif(data_source == ""fred""):\r\n     55         return get_data_fred(name=name, start=start, end=end)\r\n\r\n/home/wesm/code/pandas/pandas/io/data.pyc in get_data_yahoo(name, start, end)\r\n    133 \r\n    134     lines = urllib.urlopen(url).read()\r\n--> 135     return read_csv(StringIO(lines), index_col=0, parse_dates=True)[::-1]\r\n    136 \r\n    137 def get_data_fred(name=None, start=dt.datetime(2010, 1, 1),\r\n\r\n/home/wesm/code/pandas/pandas/io/parsers.pyc in read_csv(filepath_or_buffer, sep, header, index_col, names, skiprows, na_values, parse_dates, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding)\r\n    125         return parser\r\n    126 \r\n--> 127     return parser.get_chunk()\r\n    128 \r\n    129 @Appender(_read_table_doc)\r\n\r\n/home/wesm/code/pandas/pandas/io/parsers.pyc in get_chunk(self, rows)\r\n    483 \r\n    484         if len(self.columns) != len(zipped_content):\r\n--> 485             raise Exception(\'wrong number of columns\')\r\n    486 \r\n    487         data = dict((k, v) for k, v in izip(self.columns, zipped_content))\r\n\r\nException: wrong number of columns\r\n\r\nIn [8]: %debug\r\n> /home/wesm/code/pandas/pandas/io/parsers.py(485)get_chunk()\r\n    484         if len(self.columns) != len(zipped_content):\r\n--> 485             raise Exception(\'wrong number of columns\')\r\n    486 \r\n\r\nipdb> u\r\n> /home/wesm/code/pandas/pandas/io/parsers.py(127)read_csv()\r\n    126 \r\n--> 127     return parser.get_chunk()\r\n    128 \r\n\r\nipdb> \r\n> /home/wesm/code/pandas/pandas/io/data.py(135)get_data_yahoo()\r\n    134     lines = urllib.urlopen(url).read()\r\n--> 135     return read_csv(StringIO(lines), index_col=0, parse_dates=True)[::-1]\r\n    136 \r\n\r\nipdb> lines\r\n\'<!doctype html public ""-//W3C//DTD HTML 4.01//EN"" ""http://www.w3.org/TR/html4/strict.dtd"">\\n<html><head><title>Yahoo! - 404 Not Found</title><style>\\n/* nn4 hide */ \\n/*/*/\\nbody {font:small/1.2em arial,helvetica,clean,sans-serif;font:x-small;text-align:center;}table {font-size:inherit;font:x-small;}\\nhtml>body {font:83%/1.2em arial,helvetica,clean,sans-serif;}input {font-size:100%;vertical-align:middle;}p, form {margin:0;padding:0;}\\np {padding-bottom:6px;margin-bottom:10px;}#doc {width:48.5em;margin:0 auto;border:1px solid #fff;text-align:center;}#ygma {text-align:right;margin-bottom:53px}\\n#ygma img {float:left;}#ygma div {border-bottom:1px solid #ccc;padding-bottom:8px;margin-left:152px;}#bd {clear:both;text-align:left;width:75%;margin:0 auto 20px;}\\nh1 {font-size:135%;text-align:center;margin:0 0 15px;}legend {display:none;}fieldset {border:0 solid #fff;padding:.8em 0 .8em 4.5em;}\\nform {position:relative;background:#eee;margin-bottom:15px;border:1px solid #ccc;border-width:1px 0;}\\n#s1p {width:15em;margin-right:.1em;}\\nform span {position:absolute;left:70%;top:.8em;}form a {font:78%/1.2em arial;display:block;padding-left:.8em;white-space:nowrap;background: url(http://l.yimg.com/a/i/s/bullet.gif) no-repeat left center;} \\nform .sep {display:none;}.more {text-align:center;}#ft {padding-top:10px;border-top:1px solid #999;}#ft p {text-align:center;font:78% arial;}\\n/* end nn4 hide */\\n</style></head>\\n<body><div id=""doc"">\\n<div id=""ygma""><a href=""http://us.rd.yahoo.com/404/*http://www.yahoo.com""><img\\nsrc=http://l.yimg.com/a/i/yahoo.gif\\nwidth=147 height=31 border=0 alt=""Yahoo!""></a><div><a\\nhref=""http://us.rd.yahoo.com/404/*http://www.yahoo.com"">Yahoo!</a>\\n - <a href=""http://us.rd.yahoo.com/404/*http://help.yahoo.com"">Help</a></div></div>\\n<div id=""bd""><h1>Sorry, the page you requested was not found.</h1>\\n<p>Please check the URL for proper spelling and capitalization. If\\nyou\\\'re having trouble locating a destination on Yahoo!, try visiting the\\n<strong><a\\nhref=""http://us.rd.yahoo.com/404/*http://www.yahoo.com"">Yahoo! home\\npage</a></strong> or look through a list of <strong><a\\nhref=""http://us.rd.yahoo.com/404/*http://docs.yahoo.com/docs/family/more/"">Yahoo!\\\'s\\nonline services</a></strong>. Also, you may find what you\\\'re looking for\\nif you try searching below.</p>\\n<form name=""s1"" action=""http://us.rd.yahoo.com/404/*-http://search.yahoo.com/search""><fieldset>\\n<legend><label for=""s1p"">Search the Web</label></legend>\\n<input type=""text"" size=30 name=""p"" id=""s1p"" title=""enter search terms here"">\\n<input type=""submit"" value=""Search"">\\n<span><a href=""http://us.rd.yahoo.com/404/*http://search.yahoo.com/search/options?p="">advanced search</a> <span class=sep>|</span> <a href=""http://us.rd.yahoo.com/404/*http://buzz.yahoo.com"">most popular</a></span>\\n</fieldset></form>\\n<p class=""more"">Please try <strong><a\\nhref=""http://us.rd.yahoo.com/404/*http://help.yahoo.com"">Yahoo!\\nHelp Central</a></strong> if you need more assistance.</p>\\n</div><div id=""ft""><p>Copyright &copy; 2012 Yahoo! Inc.\\nAll rights reserved. <a\\nhref=""http://us.rd.yahoo.com/404/*http://privacy.yahoo.com"">Privacy\\nPolicy</a> - <a\\nhref=""http://us.rd.yahoo.com/404/*http://docs.yahoo.com/info/terms/"">Terms\\nof Service</a></p></div>\\n</div></body></html>\\n\'\r\nipdb> \r\n```'"
822,3363879,wesm,adamklein,2012-02-23 21:59:08,2012-02-24 20:58:23,2012-02-24 20:58:23,closed,adamklein,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/822,b'Non-string columns should get casted to string in DataFrame.to_records',"b'data like this\r\n\r\n```\r\nIn [36]: df\r\nOut[36]: \r\n        0       1        2       3       4       5       6       7       8       9\r\n0  2.1670  1.7100  0.61340  3.0150  2.5060  0.6787  1.6970  4.0460  1.2010  0.6134\r\n1  1.2920  0.6526  0.33930  0.9789  1.9320  0.3915  0.6526  1.9710  1.0310  0.5221\r\n2  0.4176  0.3002  0.01305  0.3132  0.5351  0.1566  0.2088  0.5612  0.1436  0.2349\r\n3  3.2760  1.0700  0.52210  1.2140  1.8010  0.6917  1.7620  2.3360  0.7961  0.6134\r\n4  2.2840  2.4800  0.41760  1.4100  1.8010  0.5221  0.8092  3.8370  1.1490  0.5743\r\n5  0.4699  0.2349  0.26100  0.3393  0.4699  0.1044  0.2088  0.6526  0.3915  0.1175\r\n6  1.6310  0.8222  0.30020  1.2920  1.1090  0.1697  0.4960  1.1350  0.4568  0.5743\r\n7  3.8760  2.0100  0.53510  2.2840  3.5110  0.8875  1.2010  2.3100  1.2790  0.8222\r\n8  0.7831  0.9397  0.18270  0.4960  0.6265  0.4046  0.3524  0.7178  0.3132  0.1436\r\n9  0.8353  0.4829  0.28710  0.6265  0.7570  0.2219  0.5351  0.6787  0.3002  0.1044\r\n```\r\n\r\nand\r\n\r\n```\r\nIn [35]: df.to_records()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/home/wesm/Dropbox/book/svn/<ipython-input-35-6d3142e97d2d> in <module>()\r\n----> 1 df.to_records()\r\n\r\n/home/wesm/code/pandas/pandas/core/frame.pyc in to_records(self, index)\r\n    716             names = list(self.columns)\r\n    717 \r\n--> 718         return np.rec.fromarrays(arrays, names=names)\r\n    719 \r\n    720     @classmethod\r\n\r\n/home/wesm/epd/lib/python2.7/site-packages/numpy/core/records.pyc in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\r\n    540         _names = descr.names\r\n    541     else:\r\n--> 542         parsed = format_parser(formats, names, titles, aligned, byteorder)\r\n    543         _names = parsed._names\r\n    544         descr = parsed._descr\r\n\r\n/home/wesm/epd/lib/python2.7/site-packages/numpy/core/records.pyc in __init__(self, formats, names, titles, aligned, byteorder)\r\n    142     def __init__(self, formats, names, titles, aligned=False, byteorder=None):\r\n    143         self._parseFormats(formats, aligned)\r\n--> 144         self._setfieldnames(names, titles)\r\n    145         self._createdescr(byteorder)\r\n    146         self.dtype = self._descr\r\n\r\n/home/wesm/epd/lib/python2.7/site-packages/numpy/core/records.pyc in _setfieldnames(self, names, titles)\r\n    177                 raise NameError, ""illegal input names %s"" % `names`\r\n    178 \r\n--> 179             self._names = [n.strip() for n in names[:self._nfields]]\r\n    180         else:\r\n    181             self._names = []\r\n\r\nAttributeError: \'numpy.int64\' object has no attribute \'strip\'\r\n```'"
820,3358752,adamklein,adamklein,2012-02-23 16:34:54,2012-02-23 17:03:50,2012-02-23 17:03:50,closed,,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/820,b'dtype of result of string comparison wrong',"b'Recreation from mailing list, works in 0.7 but not in master:\r\n\r\n    from pandas import *\r\n    df = DataFrame([{ ""a"" : 1, ""b"" : ""foo"" }, {""a"" : 2, ""b"" : ""bar""}])\r\n    print df\r\n    mask_a = df.a > 1\r\n    print ""Mask A""\r\n    print mask_a\r\n    print mask_a.dtype\r\n    print df[mask_a]\r\n    print df[-mask_a]\r\n    \r\n    mask_b = df.b == ""foo""\r\n    print ""Mask B""\r\n    print mask_b\r\n    print mask_b.values.dtype\r\n    print df[mask_b]\r\n    print df[-mask_b]\r\n'"
819,3358618,adamklein,adamklein,2012-02-23 16:28:07,2012-02-24 19:01:43,2012-02-24 19:01:40,closed,adamklein,0.7.1,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/819,b'Bug in groupby when as_index=False',"b""Recreation:\r\n\r\n    In [2]: paste\r\n    In [357]: df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\r\n       .....:                        'foo', 'bar', 'foo', 'foo'],\r\n       .....:                 'B' : ['one', 'one', 'two', 'three',\r\n       .....:                        'two', 'two', 'one', 'three'],\r\n       .....:                 'C' : randn(8), 'D' : randn(8)})\r\n    ## -- End pasted text --\r\n    \r\n    In [3]: grouped = df.groupby(['A', 'B'], as_index=False)\r\n    \r\n    In [4]: x = grouped['C'].agg({ 'Q' : np.sum})\r\n"""
816,3351607,wesm,jreback,2012-02-23 05:05:39,2013-08-16 19:27:44,2013-08-16 19:27:44,closed,,0.13,6,Bug;Internals;Prio-high,https://api.github.com/repos/pydata/pandas/issues/816,b'np.diff fails when called on a Series',"b""```\r\nIn [13]: s = Series(np.arange(10))\r\n\r\nIn [14]: np.diff(s)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/wesm/code/pandas/<ipython-input-14-b5f9fe77ab7c> in <module>()\r\n----> 1 np.diff(s)\r\n\r\n/usr/lib/epd-7.1/lib/python2.7/site-packages/numpy/lib/function_base.pyc in diff(a, n, axis)\r\n    975         return diff(a[slice1]-a[slice2], n-1, axis=axis)\r\n    976     else:\r\n--> 977         return a[slice1]-a[slice2]\r\n    978 \r\n    979 def interp(x, xp, fp, left=None, right=None):\r\n\r\n/home/wesm/code/pandas/pandas/core/series.pyc in __getitem__(self, key)\r\n    392             key = np.asarray(key, dtype=bool)\r\n    393 \r\n--> 394         return self._get_with(key)\r\n    395 \r\n    396     def _get_with(self, key):\r\n\r\n/home/wesm/code/pandas/pandas/core/series.pyc in _get_with(self, key)\r\n    406         else:\r\n    407             if isinstance(key, tuple):\r\n--> 408                 return self._get_values_tuple(key)\r\n    409 \r\n    410             if not isinstance(key, (list, np.ndarray)):  # pragma: no cover\r\n\r\n/home/wesm/code/pandas/pandas/core/series.pyc in _get_values_tuple(self, key)\r\n    437 \r\n    438         if not isinstance(self.index, MultiIndex):\r\n--> 439             raise ValueError('Can only tuple-index with a MultiIndex')\r\n    440 \r\n    441         # If key is contained, would have returned by now\r\n\r\nValueError: Can only tuple-index with a MultiIndex\r\n```"""
814,3351401,wesm,wesm,2012-02-23 04:25:57,2012-02-23 04:57:24,2012-02-23 04:57:24,closed,,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/814,b'GroupBy case results in malformed DataFrame',"b""```\r\nIn [19]: d\r\nOut[19]: \r\n  group  zeros  ones label\r\n0    g1      0     1    l1\r\n1    g2      0     1    l2\r\n\r\nIn [20]: d.groupby(['group']).mean()\r\nOut[20]: \r\n       ones  zeros\r\ngroup             \r\ng1        1      0\r\ng2        1      0\r\n\r\nIn [21]: d.groupby(['group']).mean().values\r\nOut[21]: \r\narray([[ 0.,  1.],\r\n       [ 0.,  1.]])\r\n```"""
812,3347341,wesm,adamklein,2012-02-22 22:05:00,2012-02-24 18:56:22,2012-02-24 18:56:22,closed,adamklein,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/812,b'Reindex with Series with name field drops name',"b""```\r\nIn [34]: df2.pivot('Date', 'Individual')\r\nOut[34]: \r\n            Exercises    \r\nIndividual          1   2\r\nDate                     \r\n1/1/2001           10 NaN\r\n1/3/2001          NaN   5\r\n\r\nIn [35]: df2.pivot('Date', 'Individual').reindex(df['Date'])\r\nOut[35]: \r\n            Exercises    \r\nIndividual          1   2\r\n1/1/2001           10 NaN\r\n1/2/2001          NaN NaN\r\n1/3/2001          NaN   5\r\n```"""
811,3340762,turkeytest,wesm,2012-02-22 19:06:53,2012-02-24 22:40:46,2012-02-24 22:40:46,closed,,0.7.1,4,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/811,b'DataFrame.join can not have datetime as column names',"b""Issue:   \r\n    For DataFrameA.join( DataFrameB , on = 'somecol' ), DataFrameB can not have datetime objects as column labels\r\n\r\nExample: \r\n\r\nfrom datetime import datetime\r\nfrom pandas import DataFrame\r\n\r\nstr_dates = [ '20120209'         , '20120222'         ]\r\ndt_dates  = [ datetime(2012,2,9) , datetime(2012,2,22)]\r\n\r\nA = DataFrame(str_dates     , index = range(2)  , columns = ['aa']   )\r\n\r\nB = DataFrame([[1,2],[3,4]] , index = str_dates , columns = str_dates)\r\nC = DataFrame([[1,2],[3,4]] , index = str_dates , columns = dt_dates )\r\n\r\nworks = A.join( B , on = 'aa' ) # works -- extra column labels are string\r\nfails = A.join( C , on = 'aa' ) # fails -- extra column labels are datetime\r\n"""
801,3280524,wesm,wesm,2012-02-18 18:37:25,2012-02-19 00:11:03,2012-02-19 00:11:03,closed,,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/801,b'Investigate NA / datetime handling when doing subset/filtering operations',"b""re:\r\n\r\n```\r\nmask = (df['Quradate'] >= date1) & (df['Quradate'] < date2) \r\n```"""
798,3276573,wesm,wesm,2012-02-18 01:14:09,2012-02-18 01:48:41,2012-02-18 01:48:41,closed,,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/798,b'Question: disable integer indexing altogether for object indexing CONTAINING integers?',b'interesting corner case raised by #780'
797,3265018,CRP,jreback,2012-02-17 09:42:28,2013-03-14 10:44:33,2013-03-14 10:44:33,closed,,0.11,3,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/797,b'Panel constructor ignores dtype',"b'I thought this was fixed(see issue 411), but it appears to keep resurfacing ;)\r\n\r\n\r\nIn [27]: a=Panel(items=range(2),major_axis=range(10),minor_axis=range(5),dtype=np.float32)\r\n\r\nIn [28]: a[0].dtypes\r\nOut[28]: \r\n0    float64\r\n1    float64\r\n2    float64\r\n3    float64\r\n4    float64\r\n\r\nA quick debug shows that the routine form_blocks just considers generic ""float"", ""int"" etc datatypes, so maybe it does not distinguish between float32 and float64?'"
796,3263141,wesm,wesm,2012-02-17 04:50:02,2012-05-07 14:48:58,2012-05-07 14:48:58,closed,changhiskhan,0.8.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/796,"b""Integers with commas don't get parsed correctly in read_csv/table/clipboard""",
795,3263130,wesm,wesm,2012-02-17 04:48:12,2012-03-15 19:30:54,2012-03-15 19:30:54,closed,,0.7.2,7,Bug;Prio-high;Unicode,https://api.github.com/repos/pydata/pandas/issues/795,b'Unicode repr failure in DataFrame',"b'```\r\nIn [9]: df = read_clipboard(header=None, sep=\'\\s+\')\r\n\r\nIn [10]: df\r\nOut[10]: ---------------------------------------------------------------------------\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\n/Users/wesm/<ipython-input-10-7ed0097d7e9e> in <module>()\r\n----> 1 df\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/displayhook.pyc in __call__(self, result)\r\n    236             self.start_displayhook()\r\n    237             self.write_output_prompt()\r\n--> 238             format_dict = self.compute_format_data(result)\r\n    239             self.write_format_data(format_dict)\r\n    240             self.update_user_ns(result)\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/displayhook.pyc in compute_format_data(self, result)\r\n    148             MIME type representation of the object.\r\n    149         """"""\r\n--> 150         return self.shell.display_formatter.format(result)\r\n    151 \r\n    152     def write_format_data(self, format_dict):\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/formatters.pyc in format(self, obj, include, exclude)\r\n    124                     continue\r\n    125             try:\r\n--> 126                 data = formatter(obj)\r\n    127             except:\r\n    128                 # FIXME: log the exception\r\n\r\n/Users/wesm/code/repos/ipython/IPython/core/formatters.pyc in __call__(self, obj)\r\n    445                 type_pprinters=self.type_printers,\r\n    446                 deferred_pprinters=self.deferred_printers)\r\n--> 447             printer.pretty(obj)\r\n    448             printer.flush()\r\n    449             return stream.getvalue()\r\n\r\n/Users/wesm/code/repos/ipython/IPython/lib/pretty.pyc in pretty(self, obj)\r\n    349             if hasattr(obj_class, \'_repr_pretty_\'):\r\n    350                 return obj_class._repr_pretty_(obj, self, cycle)\r\n--> 351             return _default_pprint(obj, self, cycle)\r\n    352         finally:\r\n    353             self.end_group()\r\n\r\n/Users/wesm/code/repos/ipython/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)\r\n    469     if getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\r\n    470         # A user-provided repr.\r\n--> 471         p.text(repr(obj))\r\n    472         return\r\n    473     p.begin_group(1, \'<\')\r\n\r\n/Users/wesm/code/pandas/pandas/core/frame.pyc in __repr__(self)\r\n    458                 self.info(buf=buf, verbose=self._verbose_info)\r\n    459             else:\r\n--> 460                 self.to_string(buf=buf)\r\n    461                 value = buf.getvalue()\r\n    462                 if max([len(l) for l in value.split(\'\\n\')]) > terminal_width:\r\n\r\n/Users/wesm/code/pandas/pandas/core/frame.pyc in to_string(self, buf, columns, col_space, colSpace, header, index, na_rep, formatters, float_format, sparsify, nanRep, index_names, justify, force_unicode)\r\n   1038                                            index_names=index_names,\r\n   1039                                            header=header, index=index)\r\n-> 1040         formatter.to_string(force_unicode=force_unicode)\r\n   1041 \r\n   1042         if buf is None:\r\n\r\n/Users/wesm/code/pandas/pandas/core/format.pyc in to_string(self, force_unicode)\r\n    193 \r\n    194             if self.index:\r\n--> 195                 to_write.append(adjoin(1, str_index, *stringified))\r\n    196             else:\r\n    197                 to_write.append(adjoin(1, *stringified))\r\n\r\n/Users/wesm/code/pandas/pandas/core/common.pyc in adjoin(space, *lists)\r\n    398     toJoin = zip(*newLists)\r\n    399     for lines in toJoin:\r\n--> 400         outLines.append(\'\'.join(lines))\r\n    401     return \'\\n\'.join(outLines)\r\n    402 \r\n\r\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 1: ordinal not in range(128)\r\n```\r\n\r\nhere are lines\r\n\r\n```\r\n(\'0  \', u\'                        .gitignore \', u\'     5 \', \' \\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2\')\r\n```'"
792,3245031,wesm,wesm,2012-02-16 01:28:19,2012-02-16 01:29:32,2012-02-16 01:29:32,closed,,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/792,b'List of Series passed to DataFrame ctor does not behave like list of dicts',
790,3242620,wesm,wesm,2012-02-15 21:58:55,2012-02-17 22:01:50,2012-02-17 22:01:50,closed,,0.7.1,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/790,b'Memory leak #467 redux',b'This nastiness has crept up again. Critical and must be fixed and 0.7.1 released as soon as humanly possible\r\n\r\n```\r\ndf = DataFrame(index=np.arange(100))\r\nfor i in range(5000):\r\n    df[i] = 5\r\n```'
782,3236647,adamklein,wesm,2012-02-15 16:06:14,2012-02-18 01:11:36,2012-02-18 01:11:36,closed,,0.7.1,0,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/782,b'Buglet in Dataframe.append with no index',"b""This works as expected:\r\n\r\n        df = DataFrame(index=[0], columns=['A', 'B', 'C'])\r\n        df3 = DataFrame(index=[1, 2], columns=['A', 'B'])\r\n        df5 = df.append(df3)\r\n\r\n        expected = DataFrame({'A': [nan, nan, nan],\r\n                              'B': [nan, nan, nan],\r\n                              'C': [nan, nan, nan]})\r\n\r\nThis doesn't:\r\n\r\n        df = DataFrame(columns=['A', 'B', 'C'])\r\n        df3 = DataFrame(index=[0, 1], columns=['A', 'B'])\r\n        df5 = df.append(df3)\r\n\r\n        expected = DataFrame({'A': [nan, nan],\r\n                              'B': [nan, nan],\r\n                              'C': [nan, nan]})"""
780,3214880,xdong,wesm,2012-02-14 07:23:50,2012-02-18 00:50:47,2012-02-18 00:50:47,closed,,0.7.1,2,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/780,b'.ix strange bug for float index',"b'In [1]: import pandas\r\n\r\nIn [2]: index = [52195.504153, 52196.303147, 52198.369883]\r\n\r\nIn [3]: a = pandas.DataFrame(randn(3, 2), index)\r\n\r\nIn [4]: a\r\nOut[4]: \r\n                     0         1\r\n52195.504153  1.367681  0.243237\r\n52196.303147 -0.745796 -1.054106\r\n52198.369883 -1.462461 -0.683286\r\n\r\nIn [5]: a.ix[52195.:52196.]\r\nOut[5]: \r\nEmpty DataFrame\r\nColumns: array([0, 1])\r\nIndex: array([], dtype=object)\r\n\r\nIn [6]: a.ix[52195.1:52196.5]\r\nOut[6]: \r\nEmpty DataFrame\r\nColumns: array([0, 1])\r\nIndex: array([], dtype=object)\r\n\r\nIn [7]: a.ix[52195.1:52196.6]\r\nOut[7]: \r\n                     0         1\r\n52195.504153  1.367681  0.243237\r\n52196.303147 -0.745796 -1.054106\r\n'"
779,3208633,wesm,wesm,2012-02-13 20:32:37,2012-02-18 00:17:19,2012-02-18 00:17:19,closed,,0.7.1,1,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/779,b'Integer column formatting a bit off',b'Completely my fault when I refactored the to_string code recently \r\n\r\n```\r\n  ticker      date  pnl    mv\r\n0   aapl  01/01/12  100  1000\r\n1   aapl  01/02/12  -50   950\r\n2   aapl  01/03/12  200  1150\r\n3   goog  01/01/12  100  2000\r\n4   goog  01/02/12  100  2100\r\n5   goog  01/03/12  -50  2050\r\n6   goog  01/04/12  -50   200\r\n```'
778,3207840,wesm,wesm,2012-02-13 19:45:28,2012-02-18 00:03:15,2012-02-18 00:03:15,closed,,0.7.1,1,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/778,"b'None handling in groupby, Series.unique'","b""from mailing list. this is caused by the change in handling of None in `PyObjectHashTable`\r\n\r\n```\r\nimport datetime\r\nimport numpy\r\nimport pandas\r\n\r\ndef simple_test2():\r\n   data = [\r\n           [1, 'string1', 1.0],\r\n           [2, 'string2', 2.0],\r\n           [3, None, 3.0]\r\n          ]\r\n\r\n   df = pandas.DataFrame({'key': [x[0] for x in data], 'grouper':\r\n[x[1] for x in data], 'value': [x[2] for x in data]})\r\n   df['weights'] = df['value']/df['value'].sum()\r\n   gb = df.groupby('grouper').aggregate(numpy.sum)\r\n\r\n   print\r\n   print df\r\n   print ''\r\n   print gb\r\n\r\nif __name__ == '__main__':\r\n   simple_test2()\r\n\r\n\r\n\r\n\r\n0.7rc1 yields:\r\n\r\n grouper  key  value  weights\r\n0 string1  1    1      0.1667\r\n1 string2  2    2      0.3333\r\n2 NaN      3    3      0.5000\r\n\r\n        key  value  weights\r\ngrouper\r\nNone     3    3      0.5000\r\nstring1  1    1      0.1667\r\nstring2  2    2      0.3333\r\n\r\n0.7 final yields:\r\n  grouper  key  value   weights\r\n0  string1    1      1  0.166667\r\n1  string2    2      2  0.333333\r\n2     None    3      3  0.500000\r\n\r\n        key  value   weights\r\ngrouper\r\nstring1    1      1  0.166667\r\nstring2    2      2  0.333333\r\n\r\nmissing the last line\r\n```"""
777,3194443,davidbrai,adamklein,2012-02-13 00:26:37,2012-02-13 14:54:05,2012-02-13 14:54:05,closed,,0.7.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/777,b'DataFrame with high ascii cannot be displayed in IPython (regression from 0.6.1)',"b""edit: This happens on Windows 7 running IPython from cmd.\r\n\r\nIn pandas 0.6.1:\r\nIn [13]: pandas.version.version\r\nOut[13]: '0.6.1'\r\n\r\nIn [14]: pandas.DataFrame(['\\xc2'])\r\nOut[14]:\r\n   0\r\n0  \xa9\xd0\r\n\r\nIn pandas 0.7.0:\r\nIn [2]: pandas.version.version\r\nOut[2]: '0.7.0'\r\n\r\nIn [3]: pandas.DataFrame(['\\xc2'])\r\n.....\r\nC:\\Python26\\lib\\site-packages\\pandas\\core\\common.pyc in _stringify(col)\r\n    503 def _stringify(col):\r\n    504     # unicode workaround\r\n\r\n--> 505     return unicode(col)\r\n    506\r\n    507 def _maybe_make_list(obj):\r\n\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)"""
771,3161534,tejrai,wesm,2012-02-09 18:46:40,2012-02-09 20:28:08,2012-02-09 20:28:07,closed,,0.7.0,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/771,b'daterange intersection issue',"b""When using the daterange intersection function, we found that the intersection only partially works as follows:\r\n\r\n>>> a = pandas.DateRange('11/30/2011','12/31/2011')\r\n>>> a\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 BusinessDay>, tzinfo: None\r\n[2011-11-30 00:00:00, ..., 2011-12-30 00:00:00]\r\nlength: 23\r\n\r\n>>> b = pandas.DateRange('12/10/2011','12/20/2011')\r\n>>> b\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 BusinessDay>, tzinfo: None\r\n[2011-12-12 00:00:00, ..., 2011-12-20 00:00:00]\r\nlength: 7\r\n\r\n>>> a.intersection(b)\r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 BusinessDay>, tzinfo: None\r\n[2011-12-12 00:00:00, ..., 2011-12-30 00:00:00]\r\nlength: 15\r\n\r\nThe intersection function restricts the output on the left side of the range but does not restrict the output on the right side of the range. Ideally, an intersection function should produce the true overlap between the two dateranges instead of the partial (one-sided) overlap.\r\n\r\nThanks,\r\nTej"""
770,3160419,wesm,wesm,2012-02-09 17:29:53,2012-02-09 17:42:11,2012-02-09 17:42:11,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/770,b'DateRange logic error with DateOffset(months=3)',"b'This yields incorrect results:\r\n\r\n```\r\nDateRange(""2011-1-1"",""2012-1-31"",offset=DateOffset(months=3))\r\n```\r\n'"
766,3147634,wesm,wesm,2012-02-08 21:02:07,2012-02-08 22:38:37,2012-02-08 22:38:37,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/766,b'Improper handling of None key inserted as DataFrame column',
764,3142541,adamklein,adamklein,2012-02-08 15:56:21,2012-02-08 16:46:19,2012-02-08 16:46:19,closed,,0.7.0,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/764,b'set_index with multindex on columns destroys column-multiindex',"b""Per Chris Billington on pystatsmodels\r\n\r\n    In [8]: df\r\n    Out[8]: \r\n              a         b         c  routine1            routine2       top          \r\n                                      result1   result2   result1        OD          \r\n                                                                         wx        wy\r\n    0 -0.914239 -0.230830 -0.421134  0.131751 -0.902347  0.382893  0.334615  0.909842\r\n    1 -0.533285 -0.878316 -3.046224 -0.760047  0.299502 -0.268321 -0.709366 -0.878868\r\n    2  0.804353 -1.203268 -1.221442 -1.948503  0.156240 -0.439375 -0.921254  1.119102\r\n    \r\n    In [9]: df2 = df.set_index([('a','','')])\r\n    \r\n    In [10]: df2\r\n    Out[10]: \r\n    <class 'pandas.core.frame.DataFrame'>\r\n    Index: 3 entries, -0.914238702285 to 0.804353019197\r\n    Data columns:\r\n    ('b', '', '')                  3  non-null values\r\n    ('c', '', '')                  3  non-null values\r\n    ('routine1', 'result1', '')    3  non-null values\r\n    ('routine1', 'result2', '')    3  non-null values\r\n    ('routine2', 'result1', '')    3  non-null values\r\n    ('top', 'OD', 'wx')            3  non-null values\r\n    ('top', 'OD', 'wy')            3  non-null values\r\n    dtypes: float64(7)\r\n    \r\n    In [11]: df2.columns\r\n    Out[11]: \r\n    Index([('b', '', ''), ('c', '', ''), ('routine1', 'result1', ''),\r\n           ('routine1', 'result2', ''), ('routine2', 'result1', ''),\r\n           ('top', 'OD', 'wx'), ('top', 'OD', 'wy')], dtype=object)\r\n"""
759,3126439,wesm,wesm,2012-02-07 16:18:27,2012-02-07 21:38:53,2012-02-07 21:38:53,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/759,b'ols summary fails with dtype=object arrays',b'Some casts to float are needed'
758,3121128,soramame0518,wesm,2012-02-07 09:10:33,2012-02-08 01:30:48,2012-02-07 18:41:45,closed,,0.7.0,4,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/758,b'Strange behavior of Series/TimeSeries with datetime index',"b""I had a trouble with Series and TimeSeries using datetime index. Here is an example:\r\n\r\nIn [1]: from pandas import *\r\n\r\nIn [2]: t = [datetime(2012, 2, 7, 0, 0, 0), datetime(2012, 2, 7, 23, 0, 0)]\r\n\r\nIn [3]: s = Series([0, 1], index=t)\r\n\r\nIn [4]: print s\r\n2012-02-07             0\r\n2012-02-07 00:00:00    1\r\n\r\nThe first index value should be '2012-02-07 00:00:00', and the second should be '2012-02-07 23:00:00' Am I doing wrong?"""
753,3101194,wesm,wesm,2012-02-05 19:44:17,2012-02-05 21:25:25,2012-02-05 21:25:25,closed,,0.7.0,2,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/753,b'np.vectorize can cause failures in TextParser if converter returns ints and floats',"b'```\r\n\r\nimport StringIO\r\nimport numpy as np\r\nimport pandas\r\ncsv = """"""\\\r\nid,score,days\r\n1,2,12\r\n2,2-5,\r\n3,,14+\r\n4,6-12,2\r\n""""""\r\n\r\ndef convert_days(x):\r\n   x = x.strip()\r\n   if not x: return np.nan\r\n\r\n   is_plus = x.endswith(\'+\')\r\n   if is_plus:\r\n       x = int(x[:-1]) + 1\r\n   else:\r\n       x = int(x)\r\n   return x\r\n\r\n\r\n\r\ndef convert_score(x):\r\n   x = x.strip()\r\n   if not x: return np.nan\r\n   if x.find(\'-\')>0:\r\n       valmin, valmax = map(int, x.split(\'-\'))\r\n       val = 0.5*(valmin + valmax)\r\n   else:\r\n       val = float(x)\r\n\r\n   return val\r\n\r\nfh = StringIO.StringIO(csv)\r\n\r\np = pandas.read_csv(fh, converters={\'score\':convert_score,\r\n\'days\':convert_days}, na_values=[-1,\'\',None])\r\n\r\nprint p\r\n```\r\n\r\ncc @jdh2358 \r\n\r\n'"
752,3100261,wesm,jreback,2012-02-05 17:09:18,2013-09-21 12:35:26,2013-09-21 12:35:26,closed,,0.13,1,Bug;Enhancement;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/752,b'Preserve dtype of integer join keys',"b'A bit too thorny for 0.7.0 but should get done at some point. Need to construct a special ""key indexer"" '"
748,3096147,wesm,wesm,2012-02-04 21:56:21,2012-02-04 22:21:38,2012-02-04 22:21:38,closed,,0.7.0,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/748,b'Join failure with multiple blocks of one type (unconsolidated)',"b'Working on failing test case, reported by user. Fix is clear but need to reproduce'"
747,3096043,wesm,wesm,2012-02-04 21:31:21,2012-02-04 22:20:45,2012-02-04 22:20:45,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/747,b'DataFrame.get_dtype_counts confused by object dtypes of different byteorder',"b'observed:\r\n\r\n```\r\ndtypes: float64(18), object(1), object(6)\r\n```\r\n\r\nIn reality first block is `<O8` (unpickled pickle from 32-bit platform) and second block is `|O8` (originated from 64-bit platform)'"
743,3092813,ruidc,adamklein,2012-02-04 09:25:29,2012-02-05 22:06:27,2012-02-04 21:37:43,closed,adamklein,0.7.0,18,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/743,b'Empty DataFrame returned when grouping on datetime column',"b""\r\nimport datetime\r\nimport numpy\r\nimport pandas\r\n\r\ndef simple_test(use_date=True):\r\n    data = [\r\n            [1, '2012-01-01', 1.0],\r\n            [2, '2012-01-02', 2.0],\r\n            [3, None, 3.0]\r\n          ]\r\n    if use_date:\r\n        data = [[row[0], datetime.datetime.strptime(row[1], '%Y-%m-\r\n%d').date() if row[1] else None, row[2]] for row in data]\r\n    df = pandas.DataFrame({'key': [x[0] for x in data], 'date': [x[1]\r\nfor x in data], 'value': [x[2] for x in data]})\r\n    df['weights'] = df['value']/df['value'].sum()\r\n    gb = df.groupby('date').aggregate(numpy.sum)\r\n\r\n    print 'II) Using date objects' if use_date else 'I) Using date\r\nstrings'\r\n    print df\r\n    print ''\r\n    print gb\r\n\r\nif __name__ == '__main__':\r\n    simple_test(use_date=False)\r\n    print '*' * 40\r\n    simple_test(use_date=True)"""
742,3089951,james18,wesm,2012-02-03 22:42:02,2012-02-05 23:44:13,2012-02-05 23:44:13,closed,adamklein,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/742,b'DataFrame.rank() does not work for dtype object',"b""from pandas import DataFrame\r\n\r\ndf = DataFrame([['b','c','a'],['a','c','b']])\r\n\r\nranked = df..rank(1)\r\n\r\nCurrently:\r\nranked = Empty DataFrame\r\n\r\nDesired\r\nranked  = a ranked DataFrame (by row) as in the case of DataFrame.rank(1) of dtype = float/int/etc"""
741,3084243,lodagro,lodagro,2012-02-03 15:18:06,2012-03-01 19:40:11,2012-03-01 19:40:10,closed,lodagro,,2,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/741,b'plot/hist/boxplot with non numeric/date Index and MultiIndex',"b'Plotting functions work fine if the index holds numeric or date data.\r\nBut error is generated when Index holds e.g strings, or when MultiIndex is used.'"
738,3072538,adamklein,wesm,2012-02-02 18:58:56,2012-02-05 22:52:55,2012-02-05 22:52:54,closed,adamklein,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/738,"b'test_to_csv_unicode fails on python 3.1, 3.2 windows'","b'======================================================================\r\nERROR: test_to_csv_unicode (pandas.tests.test_frame.TestDataFrame)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python32\\lib\\site-packages\\pandas-0.7.0.dev-py3.2-win-amd64.egg\\panda\r\ns\\tests\\test_frame.py"", line 2488, in test_to_csv_unicode\r\n    df.to_csv(path, encoding=\'UTF-8\')\r\n  File ""c:\\python32\\lib\\site-packages\\pandas-0.7.0.dev-py3.2-win-amd64.egg\\panda\r\ns\\core\\frame.py"", line 914, in to_csv\r\n    csvout.writerow(encoded_labels + encoded_cols)\r\n  File ""c:\\python32\\lib\\encodings\\cp1252.py"", line 19, in encode\r\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\nUnicodeEncodeError: \'charmap\' codec can\'t encode character \'\\u03c3\' in position\r\n8: character maps to <undefined>\r\n'"
733,3061194,wesm,wesm,2012-02-02 00:21:27,2012-02-05 20:50:51,2012-02-05 20:50:51,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/733,b'Improper handling of join keys in MergeOperation',"b'Merge keys are ""added back"" post facto. This is problematic both from a performance perspective and also causes the keys to be in the wrong order in the joined result (the join keys are inserted in the beginning rather than in their natural places). Consolidation of the DataFrame is being improperly deferred until after the join result is returned. '"
730,3057035,rowanu,wesm,2012-02-01 19:14:42,2012-02-05 15:44:28,2012-02-05 04:07:11,closed,,0.7.0,6,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/730,"b'SparseSeries.dropna() returns ""NaN""s for values'","b'A SparseSeries object I have called "".dropna()"" on will output the indexes which are **not** NaN (which is correct), but shows ""NaN"" in the value column.\r\n\r\nIf I run SparseSeries.to_dense().dropna() I get the expected output ie. non-NaN indexes and their values.'"
726,3048523,lodagro,wesm,2012-02-01 07:56:43,2012-02-07 22:46:06,2012-02-07 21:41:40,closed,,0.7.0,8,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/726,b'float formatting issue',"b""see first value\r\n\r\n```python\r\nIn [1]: import pandas\r\n\r\nIn [2]: pandas.__version__\r\nOut[2]: '0.7.0.dev-e3df4e2'\r\n\r\nIn [3]: df = pandas.DataFrame({'A': [746.03, 0.00, 5620.00, 1592.36]})\r\n\r\nIn [4]: df\r\nOut[4]:\r\n   A\r\n0  746.\r\n1  0.00\r\n2  5620\r\n3  1592\r\n\r\nIn [5]:                  \r\n```"""
722,3032633,philbigdog,philbigdog,2012-01-31 08:35:47,2012-01-31 20:49:47,2012-01-31 20:49:47,closed,,0.7.0,5,Bug,https://api.github.com/repos/pydata/pandas/issues/722,b'Exception from Series.mean()',"b'In certain circumstances I found the following exception was raised when calling series.mean():\r\nI\'m running pandas 0.6.0. (I also checked the latest build on GitHub just in case!)\r\n\r\nFile ""C:/Work/PyCharm/EPD-7.2.1/python/bin/heatmap.py"", line 166, in zscore\r\n    logger.debug(\'b:{}\'.format(series.mean()))\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 650, in mean\r\n    return nanops.nanmean(self.values, skipna=skipna)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\nanops.py"", line 30, in nanmean\r\n    the_mean = values.sum(axis) / float(values.shape[axis])\r\nTypeError: tuple indices must be integers, not NoneType\r\n\r\nThe axis parameter is not being handed to nanops.nanmean(), where it is used for indexing.\r\nSo I changed (series.py line 650):\r\nreturn nanops.nanmean(self.values, skipna=skipna)\r\nto:\r\nreturn nanops.nanmean(self.values, skipna=skipna, axis=axis)\r\nto fix my code.'"
719,3029387,wesm,wesm,2012-01-31 00:35:11,2012-02-01 18:55:23,2012-02-01 18:55:23,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/719,b'DataFrame.to_panel needs to check the sortedness of the MultiIndex',
717,3026081,adamklein,adamklein,2012-01-30 20:19:17,2012-01-31 15:42:31,2012-01-31 15:42:31,closed,,,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/717,b'read_csv performance regression',"b'3x performance regression, likely b/c unicode changes'"
714,3018971,kieranholland,wesm,2012-01-30 11:08:22,2012-02-04 22:40:26,2012-02-04 22:40:26,closed,,0.7.0,5,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/714,b'Series.unique() dies with many NaNs',"b""Series.unique() dies with many NaNs:\r\n\r\n    import time\r\n\r\n    def test_unique(obj):\r\n        for n in range(6):\r\n            objs = Series([obj] * 10 ** n)\r\n            start = time.time()\r\n            objs.unique()\r\n            stop = time.time()\r\n            print('%6.0f %s' % (len(objs), stop - start))\r\n\r\n    test_unique('a')\r\n\r\n           1 4.98294830322e-05\r\n          10 2.40802764893e-05\r\n         100 4.10079956055e-05\r\n        1000 6.91413879395e-05\r\n       10000 0.000164985656738\r\n      100000 0.0013279914856\r\n\r\n    test_unique(float('nan'))\r\n\r\n           1 3.91006469727e-05\r\n          10 3.60012054443e-05\r\n         100 0.000331163406372\r\n        1000 0.0283088684082\r\n       10000 2.71325206757\r\n      100000 Boom!     """
709,3007313,wesm,adamklein,2012-01-28 20:54:38,2012-01-30 18:14:20,2012-01-30 18:14:20,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/709,b'Unhelpful exception / bug in ix + MultiIndex use case',"b""This should work IMHO\r\n\r\n```\r\nIn [20]: df\r\nOut[20]: \r\n       value\r\n   id       \r\nt1 a   1    \r\n   b   2    \r\n   c   3    \r\nt2 a   7    \r\n   b   8    \r\n\r\nIn [21]: df.ix[:, 'value']\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/home/wesm/Dropbox/book/svn/<ipython-input-21-7d089958dc90> in <module>()\r\n----> 1 df.ix[:, 'value']\r\n\r\n/home/wesm/code/pandas/pandas/core/indexing.pyc in __getitem__(self, key)\r\n     28                 pass\r\n     29 \r\n---> 30             return self._getitem_tuple(key)\r\n     31         else:\r\n     32             return self._getitem_axis(key, axis=0)\r\n\r\n/home/wesm/code/pandas/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\r\n     97     def _getitem_tuple(self, tup):\r\n     98         try:\r\n---> 99             return self._getitem_lowerdim(tup)\r\n    100         except IndexingError:\r\n    101             pass\r\n\r\n/home/wesm/code/pandas/pandas/core/indexing.pyc in _getitem_lowerdim(self, tup)\r\n    126                 pass\r\n    127             except Exception:\r\n--> 128                 if tup[0] not in ax0:\r\n    129                     raise\r\n    130 \r\n\r\n/home/wesm/code/pandas/pandas/core/index.pyc in __contains__(self, key)\r\n    209 \r\n    210     def __contains__(self, key):\r\n--> 211         return key in self._engine\r\n    212 \r\n    213     def __hash__(self):\r\n\r\n/home/wesm/code/pandas/pandas/_engines.so in pandas._engines.DictIndexEngine.__contains__ (pandas/src/engines.c:1958)()\r\n\r\nTypeError: unhashable type\r\n```"""
708,3007287,wesm,wesm,2012-01-28 20:49:43,2012-01-29 23:43:02,2012-01-29 23:43:02,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/708,b'concat DataFrames bug with keys on axis=1',"b""```\r\n\r\nIn [11]: t1\r\nOut[11]: \r\n    value\r\nid       \r\na   1    \r\nb   2    \r\nc   3    \r\n\r\nIn [12]: t2\r\nOut[12]: \r\n    value\r\nid       \r\na   7    \r\nb   8    \r\n\r\nIn [13]: concat([t1, t2], axis=1, keys=['t1', 't2'])\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/home/wesm/Dropbox/book/svn/<ipython-input-13-8c00deb927ee> in <module>()\r\n----> 1 concat([t1, t2], axis=1, keys=['t1', 't2'])\r\n\r\n/home/wesm/code/pandas/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity)\r\n    649                        keys=keys, levels=levels, names=names,\r\n    650                        verify_integrity=verify_integrity)\r\n--> 651     return op.get_result()\r\n    652 \r\n    653 \r\n\r\n/home/wesm/code/pandas/pandas/tools/merge.pyc in get_result(self)\r\n    715             return Series(new_data, index=self.new_axes[0], name=name)\r\n    716         else:\r\n--> 717             new_data = self._get_concatenated_data()\r\n    718             return self.objs[0]._from_axes(new_data, self.new_axes)\r\n    719 \r\n\r\n/home/wesm/code/pandas/pandas/tools/merge.pyc in _get_concatenated_data(self)\r\n    735                 stacked_block = self._concat_blocks(klass_blocks)\r\n    736                 new_blocks.append(stacked_block)\r\n--> 737             new_data = BlockManager(new_blocks, self.new_axes)\r\n    738         except Exception:  # EAFP\r\n    739             # should not be possible to fail here for the expected reason with\r\n\r\n\r\n/home/wesm/code/pandas/pandas/core/internals.pyc in __init__(self, blocks, axes, do_integrity_check)\r\n    283 \r\n    284         if do_integrity_check:\r\n--> 285             self._verify_integrity()\r\n    286 \r\n    287     def __nonzero__(self):\r\n\r\n/home/wesm/code/pandas/pandas/core/internals.pyc in _verify_integrity(self)\r\n    362 \r\n    363     def _verify_integrity(self):\r\n--> 364         _union_block_items(self.blocks)\r\n    365         mgr_shape = self.shape\r\n    366         for block in self.blocks:\r\n\r\n/home/wesm/code/pandas/pandas/core/internals.pyc in _union_block_items(blocks)\r\n   1071 \r\n   1072     if tot_len > len(the_union):\r\n-> 1073         raise Exception('item names overlap')\r\n   1074     return the_union\r\n   1075 \r\n\r\nException: item names overlap\r\n```"""
707,3001490,james18,adamklein,2012-01-27 23:10:52,2012-01-29 20:29:56,2012-01-29 20:29:56,closed,adamklein,0.7.0,5,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/707,b'DataFrame.ix[n] for numerical (float) indices has an error',"b""So my understanding for a DataFrame with numerical indices, DataFrame.ix[i] indexes to the position where index=i, and not the ith row of the DataFrame.  Although I find this to be confusing, i accept that this was the decision that was made.\r\n\r\nHowever, the getting and setting with .ix is not consistent\r\n\r\nfrom pandas import DataFrame\r\ndf = DataFrame(['a','b','c'],index=[0,0.5,1])\r\ndf.ix[1] = 'zoo'\r\nprint df.ix[1]\r\n\r\nCurrent:\r\n'b'\r\n\r\nExpected:\r\n'zoo'\r\n\r\nBasically, df.ix[1] returns 'b', of index 0.5, instead of 'zoo' of index 1\r\n\r\n\r\n"""
700,2988661,wesm,wesm,2012-01-27 00:31:34,2012-01-27 03:40:15,2012-01-27 03:40:15,closed,,0.7.0,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/700,b'Not doing integer slicing with MultiIndexed Series via __getitem__',b'cc @craustin per e-mail discussion'
697,2987857,wesm,adamklein,2012-01-26 23:13:17,2012-01-27 14:56:51,2012-01-27 14:56:50,closed,adamklein,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/697,b'GroupBy by level bug',"b""reported by user, almost certainly a corner case introduced by the list changes recently\r\n\r\n```\r\nIn [11]: df\r\nOut[11]: \r\n               a       b  c       d\r\none two three                      \r\nfoo bar baz   -0.2732  1 -0.2732  1\r\n\r\nIn [12]: df.groupby(level='three')\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/home/wesm/code/pandas/<ipython-input-12-17ba46f6aa67> in <module>()\r\n----> 1 df.groupby(level='three')\r\n\r\n/home/wesm/code/pandas/pandas/core/generic.pyc in groupby(self, by, axis, level, as_index, sort)\r\n    120         from pandas.core.groupby import groupby\r\n    121         return groupby(self, by, axis=axis, level=level, as_index=as_index,\r\n--> 122                        sort=sort)\r\n    123                                                                                                   \r\n    124     def select(self, crit, axis=0):\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in groupby(obj, by, **kwds)\r\n    520         raise TypeError('invalid type: %s' % type(obj))\r\n    521 \r\n--> 522     return klass(obj, by, **kwds)\r\n    523 \r\n    524 def _get_axes(group):\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in __init__(self, obj, grouper, axis, level, groupings, exclusions, column, as_index, sort)                                                                          \r\n    112         if groupings is None:\r\n    113             groupings, exclusions = _get_groupings(obj, grouper, axis=axis,\r\n--> 114                                                    level=level, sort=sort)\r\n    115                                                                                                   \r\n    116         self.groupings = groupings\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _get_groupings(obj, grouper, axis, level, sort)\r\n    720             name = gpr\r\n    721             gpr = obj[gpr]\r\n--> 722         ping = Grouping(group_axis, gpr, name=name, level=level, sort=sort)\r\n    723         if ping.name is None:\r\n    724             ping.name = 'key_%d' % i\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in __init__(self, index, grouper, name, level, sort)\r\n    589             level_values = index.levels[level].take(inds)\r\n    590             if grouper is not None:\r\n--> 591                 self.grouper = level_values.map(self.grouper)\r\n    592             else:\r\n    593                 self._was_factor = True\r\n\r\n/home/wesm/code/pandas/pandas/core/index.pyc in map(self, mapper)\r\n    580 \r\n    581     def map(self, mapper):\r\n--> 582         return self._arrmap(self.values, mapper)\r\n    583 \r\n    584     def _get_method(self, method):\r\n\r\n/home/wesm/code/pandas/pandas/_tseries.so in pandas._tseries.arrmap_object (pandas/src/tseries.c:41721)()\r\n\r\nTypeError: 'numpy.ndarray' object is not callable\r\n```"""
691,2983742,jseabold,adamklein,2012-01-26 18:34:16,2012-01-26 20:14:37,2012-01-26 20:14:37,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/691,b'assignment with ix and mixed dtypes',"b'data = {""title"" : [\'foobar\',\'bar\',\'foobar\'] + [\'foobar\'] * 17 , ""cruft"" : np.random.random(20)}\r\n\r\ndf = pandas.DataFrame(data)\r\n\r\nix = df[df[\'title\'] == \'bar\'].index\r\n\r\n# this doesn\'t work with a list of variables\r\ndf.ix[ix, [\'title\']] = \'foobar\'\r\ndf.ix[ix, [\'cruft\']] = 0\r\n\r\nprint df\r\n\r\n# but this does\r\ndf.ix[ix, \'title\'] = \'foobar\'\r\ndf.ix[ix, \'cruft\'] = 0\r\n\r\nprint df'"
690,2981506,craustin,wesm,2012-01-26 16:04:43,2012-01-26 16:51:35,2012-01-26 16:43:05,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/690,"b""Series that used to be typed numeric is now typed 'object'""","b""import numpy as np\r\nfrom pandas import Series\r\ns = Series(range(3))\r\ns2 = s.map(lambda x: np.where(x == 0, 0, 1))\r\nprint s2.dtype\r\n\r\nIn pandas 0.6.0, this returned 'float64', but now it returns 'object'.  This change requires us to explicitly specify some dtypes to pandas constructors as a workaround.  If this is the desired behavior, we can live with it - but wanted to alert you guys."""
687,2972235,creeson,wesm,2012-01-25 22:05:17,2012-01-25 22:25:56,2012-01-25 22:25:56,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/687,b'SparseSeries.combine_first(Series) fails when combining with a Series',"b'Contrary to the documentation, the SparseSeries code, when doing a combine_first, actually assumes the other series is a SparseSeries. It performs a other.to_dense(), which fails on a Series.\r\n\r\n    def combine_first(self, other):\r\n        """"""\r\n        Combine Series values, choosing the calling Series\'s values\r\n        first. Result index will be the union of the two indexes\r\n\r\n        Parameters\r\n        ----------\r\n        other : Series\r\n\r\n        Returns\r\n        -------\r\n        y : Series\r\n        """"""\r\n        dense_combined = self.to_dense().combine_first(other.to_dense())\r\n        return dense_combined.to_sparse(fill_value=self.fill_value)\r\n\r\nI changed my code locally to the following, which seems to work (though I\'m sure there is a more elegant way):\r\n \r\n    def combine_first(self, other):\r\n        """"""\r\n        Combine Series values, choosing the calling Series\'s values\r\n        first. Result index will be the union of the two indexes\r\n\r\n        Parameters\r\n        ----------\r\n        other : Series\r\n\r\n        Returns\r\n        -------\r\n        y : Series\r\n        """"""\r\n        if isinstance(other, SparseSeries):\r\n            dense_combined = self.to_dense().combine_first(other.to_dense())\r\n        elif isinstance(other, Series):\r\n            dense_combined = self.to_dense().combine_first(other)\r\n        return dense_combined.to_sparse(fill_value=self.fill_value)\r\n'"
683,2966978,james18,wesm,2012-01-25 16:07:20,2012-01-26 17:43:59,2012-01-26 16:59:27,closed,,0.7.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/683,b'DataFrame.asfreq(MonthEnd(0)) currently goes into infinite loop',"b'I understand that 0 should not be passed into any DateOffset subclasses, but:\r\n\r\nCurrently:\r\nDataFrame.asfreq(MonthEnd(0)) goes into infinite loop.  I believe this is the case for all the Date *End subclasses\r\n\r\nDesired:\r\nException should be raised'"
680,2959355,craustin,wesm,2012-01-25 00:14:20,2012-01-27 00:23:56,2012-01-25 23:21:02,closed,,0.7.0,29,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/680,b'Cannot print DataFrame with unicode columns in IPython',"b""from pandas import DataFrame\r\ndf = DataFrame({u'c/\\u03c3':[1,2,3]})\r\n\r\nTry typing 'df' in IPython:\r\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03c3' in position 5: ordinal not in range(128)"""
679,2957549,wesm,wesm,2012-01-24 21:55:59,2012-01-24 23:00:30,2012-01-24 23:00:30,closed,,0.7.0,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/679,b'GroupBy with dict bug',"b""This used to work (cc @craustin):\r\n\r\n```\r\nfrom pandas import Series\r\ns = Series({'T1': 5})\r\nprint s.groupby({'T1': 'T2'}).agg(sum)\r\n```"""
678,2954103,dieterv77,wesm,2012-01-24 17:50:13,2012-01-24 23:24:33,2012-01-24 23:24:33,closed,,0.7.0,2,Bug;Prio-medium,https://api.github.com/repos/pydata/pandas/issues/678,b'Int64Index and pivot_table',"b""I found an issue related to having an Int64Index and using pivot_table.\r\n\r\nHere's the code that illustrates the problem:\r\n#Start code\r\nimport itertools\r\nimport datetime \r\nimport pandas\r\n\r\nd = datetime.date.min\r\ndata = list(itertools.product(['foo', 'bar'], ['A', 'B', 'C'],['x1', 'x2'],[d + datetime.timedelta(i) for i in xrange(20)], [1.0]))\r\n\r\nprint data[0]\r\ndf = pandas.DataFrame(data)\r\n\r\ndf2 = pandas.pivot_table(df, values=4, rows=[0,1,3],cols=[2])\r\nprint df2.columns\r\n\r\ndf = df.rename(columns=lambda x: str(x))\r\ndf2 = pandas.pivot_table(df, values='4', rows=['0','1','3'],cols=['2'])\r\nprint df2.columns\r\n#end code\r\n\r\nI would expect the columns of the pivoted table to be the same regardless of whether the column index of the original table\r\nis ints or strings.  I'm finding that this is not the case.\r\n\r\nthanks in advance for looking into it."""
676,2951523,craustin,wesm,2012-01-24 15:03:39,2012-01-27 21:34:42,2012-01-24 17:26:15,closed,,0.7.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/676,"b""Mean is incorrectly NaN in 'object' DataFrame""","b'from pandas import DataFrame\r\ndf = DataFrame({0: [np.nan, 2], 1: [np.nan, 3], 2: [np.nan, 4]}, dtype=object)\r\ndf.mean(1)\r\n\r\nExpected:\r\nIn [33]: df.mean(1)\r\nOut[33]:\r\n0    NaN\r\n1    3.0\r\n\r\nActual:\r\nIn [31]: df.mean(1)\r\nOut[31]:\r\n0    NaN\r\n1    NaN'"
672,2945374,bshanks,wesm,2012-01-24 02:55:33,2012-01-24 17:36:07,2012-01-24 17:36:07,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/672,b'DataFrame.mul on vector gives inconsistent results depending on combination of axis and whether argument is boxed',"b""When argument is boxed in a TimeSeries, the 'axis' argument is respected:\r\n\r\nIn [108]: a = DataFrame(reshape(range(9),(3,3)))\r\n\r\nIn [109]: a\r\nOut[109]: \r\n   0  1  2\r\n0  0  1  2\r\n1  3  4  5\r\n2  6  7  8\r\n\r\nIn [110]: a.mul(TimeSeries(range(3)))\r\nOut[110]: \r\n   0  1  2 \r\n0  0  1  4 \r\n1  0  4  10\r\n2  0  7  16\r\n\r\nIn [111]: a.mul(TimeSeries(range(3)), axis='index')\r\nOut[111]: \r\n   0   1   2 \r\n0  0   0   0 \r\n1  3   4   5 \r\n2  12  14  16\r\n\r\nbut when it is unboxed, the axis argument is silently ignored:\r\n\r\nIn [112]: a.mul(range(3))\r\nOut[112]: \r\n   0  1  2 \r\n0  0  1  4 \r\n1  0  4  10\r\n2  0  7  16\r\n\r\nIn [113]: a.mul(range(3), axis='index')\r\nOut[113]: \r\n   0  1  2 \r\n0  0  1  4 \r\n1  0  4  10\r\n2  0  7  16\r\n"""
671,2945066,adamklein,wesm,2012-01-24 02:15:27,2012-01-24 03:08:05,2012-01-24 03:08:05,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/671,"b'Buglets in MultiIndex, tuple indexing?'","b'bugs, or expected?\r\n\r\n          Q          E           A        B        C        D       \r\n    F  G                                                            \r\n    0  0  x51CohYbqL aJBXfDaxvl -0.21474 -0.07802  0.37657  0.519663\r\n    1  2  Q07yrbfCLo RvNdHB9dl3 -0.16727 -1.26608  1.21477 -1.559552\r\n    2  4  uihKvB6NBA 5DjiMCJ8Sw -0.43737  0.10384  0.50846  1.413200\r\n    3  6  1qgFB8QlJS 2fUK7jD4Vb  1.06883  0.39375 -0.90719 -1.280366\r\n    4  8  9cmUdqYSor 93UZeCZOG8  0.28849  0.20656  0.14076 -1.350762\r\n    5  10 upb4NIS9Im V2v36EBTbW  1.42492 -0.81992 -0.58840  1.297900\r\n    6  12 EI1mcWNhQc lOi1LZoa9d -0.43579 -0.55900 -0.38126 -1.996946\r\n    7  14 P4svFjVstF KRdANeSKc1 -0.06271 -0.73894 -1.61319  0.015665\r\n    8  16 8TnT3assPy 8GMHH7cwIM -0.16219 -1.37595  0.26222  0.387206\r\n    9  18 ZpQAdGJGqn aOyyiwsKe5 -1.48009  0.20925  1.66069 -0.165002\r\n    10 20 GsgxBj1X6H IjKfhf7s0t -0.90167  0.42439  0.15507  1.423653\r\n    11 22 HvPp46Lsw7 6eRfG5cO5i  0.44674  0.35359 -1.03449 -1.224809\r\n    12 24 KZFZq4N2x6 WoZU0DDsZt -1.04287  0.50613 -0.37356  0.412353\r\n    13 26 BeXkOEK1vp KisdEjCbGD -0.68967  1.75701  0.83935  2.449815\r\n    14 28 xIaBEjA3KJ cYu48PoJcE  2.33759  0.80017 -0.49848  0.184022\r\n    15 30 SLC9Xq9WJr aAZeWu4Cuz -1.12675 -1.43130  0.67064 -0.004682\r\n    16 32 3L68qvWgWr JicCNHSE0W  1.39660  0.43552  0.42691 -0.576866\r\n    17 34 d5WumlvFj0 dvyPF9hbpr -1.44074  0.71225 -0.31398 -0.332095\r\n    18 36 s7JvczBxzY tdBT7QIsea -0.08437  0.37257 -0.96486  0.598225\r\n    19 38 G4vFpFXvRC NiOLw70EzR  1.53051  0.11435 -0.88396  0.765056\r\n    20 40 wW6VWMgyVo Yar4UkKxFU  0.16342 -0.22157 -0.28934 -0.563170\r\n    21 42 pGSClBDzVm mnfNHAv4rO  0.43092  1.20629 -0.43644  0.381052\r\n    22 44 I72WAD7jmE LLxBVrAYSJ -0.12854 -0.08114 -0.51555  0.030052\r\n    23 46 8CZLWGObmb EJwNMl0JGu -0.81029 -0.91152 -0.13883  0.195795\r\n    24 48 gMEDr2vJpl MEFjWFHtAA -1.10904  0.23615  0.15525 -0.187997\r\n    25 50 lzhkv8AZih kQYVmuHnBR  0.11009 -2.23447 -0.08848 -0.354509\r\n    26 52 YzPLrcENot XYpjlEAPaO -0.04009 -1.95039  0.45737 -2.303422\r\n    27 54 VltHl29NXr NpveZNu6dQ  0.77100  0.09994 -0.36891  0.643359\r\n    28 56 Xdt4lbmTDV YFPDKFbdrL  0.65604 -0.41013  2.08534  1.749054\r\n    29 58 J6SbXUfABj 0r1IxqyAZj -1.64618 -0.33196 -0.91242 -0.527971\r\n\r\n\r\n    In [68]: df.ix[(0,0), :]\r\n    ERROR: An unexpected error occurred while tokenizing input\r\n\r\n    In [69]: df.ix[[(0,0)], :]\r\n    Out[69]: \r\n        Q          E           A       B        C       D     \r\n    F G                                                       \r\n    0 0 x51CohYbqL aJBXfDaxvl -0.2147 -0.07802  0.3766  0.5197\r\n\r\n\r\n    In [71]: df.ix[[0,0], :]\r\n        Q          E           A       B        C       D     \r\n    F G                                                       \r\n    0 0 x51CohYbqL aJBXfDaxvl -0.2147 -0.07802  0.3766  0.5197\r\n      0 x51CohYbqL aJBXfDaxvl -0.2147 -0.07802  0.3766  0.5197\r\n'"
670,2943747,wesm,wesm,2012-01-23 23:42:14,2012-01-24 02:28:20,2012-01-24 02:28:20,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/670,b'Buglet indexing Series with integer as float',"b'Was raised by user, need to find the e-mail. but:\r\n\r\nupdate: was actually slicing\r\n\r\n```\r\ns[:5.0]\r\n```\r\n\r\nmay fail in some cases (or it might have been fixed, need to test)'"
669,2943660,wesm,wesm,2012-01-23 23:34:56,2012-01-24 19:06:12,2012-01-24 19:06:12,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/669,b'Should setting an integer into a DataFrame float column cause a type cast?',"b""Not sure about this\r\n\r\n```\r\nIn [4]: df = tm.makeTimeDataFrame()\r\n\r\nIn [5]: df\r\nOut[5]: \r\n            A       B        C         D      \r\n2000-01-03  0.5694  1.15202  1.269872 -0.08819\r\n2000-01-04 -0.3278 -0.39915 -0.591330  0.81658\r\n2000-01-05  0.8175  0.15806  0.751025  0.61545\r\n2000-01-06  0.2541  2.25491 -0.536671 -0.43782\r\n2000-01-07 -1.4400 -0.12882  0.077768 -0.40887\r\n2000-01-10 -0.3598  1.74354 -1.388002  0.49269\r\n2000-01-11 -0.1260 -0.86097  1.028045 -0.15667\r\n2000-01-12  1.9286  1.42076  1.780311  1.78580\r\n2000-01-13  0.6148  0.28743  0.660499  0.94072\r\n2000-01-14 -0.7055  0.34959  0.538657  0.90766\r\n2000-01-17 -0.4395  0.19145 -0.413462  1.35715\r\n2000-01-18  0.6851 -1.04204  1.721110  0.67047\r\n2000-01-19  1.2581 -1.36096 -0.854228  0.02768\r\n2000-01-20  0.8928 -0.93538 -0.810424  1.64063\r\n2000-01-21  0.5252 -1.09177 -1.060109  0.44210\r\n2000-01-24  0.2277 -0.06904 -0.029423  0.36855\r\n2000-01-25  0.3063 -0.38531  0.597502 -0.24924\r\n2000-01-26 -1.5342 -0.17552  0.033193  0.88376\r\n2000-01-27  1.3528 -1.28998  0.378057 -1.15563\r\n2000-01-28  0.7619 -0.18432  0.476671 -0.08385\r\n2000-01-31  0.3166  0.17006 -0.470700 -0.22658\r\n2000-02-01 -0.4439  0.94540 -0.090201  0.97781\r\n2000-02-02 -0.1776  2.30224  0.225779 -0.52249\r\n2000-02-03 -1.6860  0.10163  0.009537 -0.44277\r\n2000-02-04 -0.7501  1.19757 -2.191548 -0.96064\r\n2000-02-07 -0.5917  0.81859  0.140187  1.42304\r\n2000-02-08  1.4421  0.15792 -0.085060  0.39537\r\n2000-02-09  0.6675 -0.59396 -1.012737 -1.67261\r\n2000-02-10 -0.2199  0.77754 -0.319754  0.24299\r\n2000-02-11 -0.4806 -0.84326  0.664512  0.91148\r\n\r\nIn [6]: df.dtypes\r\nOut[6]: \r\nA    float64\r\nB    float64\r\nC    float64\r\nD    float64\r\n\r\nIn [7]: df['B'] = 0\r\n\r\nIn [8]: df.dtypes\r\nOut[8]: \r\nA    float64\r\nB    int64\r\nC    float64\r\nD    float64\r\n```"""
668,2942471,craustin,wesm,2012-01-23 22:01:59,2012-01-23 23:13:45,2012-01-23 23:13:45,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/668,"b""'kind' and 'order' are ignored by Series.sort""",b'Is this desired? Is Series.sort needed when we have Series.order?'
666,2928500,wesm,wesm,2012-01-22 20:18:09,2013-12-04 00:43:44,2012-01-23 21:30:34,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/666,b'groupby not friendly to SparseDataFrame',b'Issue referenced in\r\n\r\nhttp://stackoverflow.com/questions/8957175/dataframe-to-panel-indexed-by-nonunique-column-with-pandas'
663,2923935,wesm,wesm,2012-01-21 23:00:12,2012-01-23 21:41:45,2012-01-23 21:41:45,closed,,0.7.0,1,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/663,b'Potential sparse memory leak',"b""from SO:\r\n\r\n```\r\n# In [4]: type(pd)\r\n# Out[4]: pandas.sparse.frame.SparseDataFrame\r\nmemid = unique(pd.Member)\r\npan = {}\r\nfor mem in memid:\r\n    pan[mem] = pd[pd.Member==mem]\r\ngoal = pandas.Panel(pan)\r\n```\r\n\r\ncomments\r\n\r\n```\r\n\t\r\nWhat are the dimensions (pd.shape) and density (pd.density) of the SparseDataFrame? Any change you can e-mail me a pickle of the object (pd.save(file_path)) for me to have a look to try to diagnose what's up? BTW these questions would be better posed on the mailing list than SO. \xa8C wesm 29 mins ago \r\n \t\r\n \t\r\npd.shape = (2668990, 232) \xa8C user1162837 23 mins ago\r\n \t\r\n \t\r\npd.density = 0.12814551216649045 The file is too large to send through email. \xa8C user1162837 22 mins ago\r\n \t\r\n \t\r\nAlso, pandas is a great library. :) \xa8C user1162837 14 mins ago\r\n```"""
662,2923226,wesm,wesm,2012-01-21 20:22:31,2012-01-21 21:46:20,2012-01-21 21:46:20,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/662,b'Multi column selection through DataFrame.__getitem__ drops index name',
660,2917190,wesm,wesm,2012-01-20 21:35:16,2012-01-21 17:28:21,2012-01-21 17:28:21,closed,,0.7.0,0,Bug;Prio-high,https://api.github.com/repos/pydata/pandas/issues/660,b'Handle case where MultiIndex level contains tuples',
641,2873675,aman-thakral,wesm,2012-01-17 19:55:35,2012-01-18 22:08:04,2012-01-17 20:33:37,closed,,0.7.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/641,b'print fails in current dev version under Windows 32-bit',"b'I get the following Traceback:\r\n\r\nTraceback (most recent call last):\r\n  File ""C:\\src\\OSR\\OSR\\flowering_start.py"", line 171, in <module>\r\n    df_x = execute(x,df,conn)\r\n  File ""C:\\src\\OSR\\OSR\\flowering_start.py"", line 92, in execute\r\n    print result[\'Date\']\r\n  File ""C:\\src\\pandas\\pandas\\core\\series.py"", line 566, in __str__\r\n    return repr(self)\r\n  File ""C:\\src\\pandas\\pandas\\core\\series.py"", line 521, in __repr__\r\n    result = self._tidy_repr(min(30, max_rows - 4))\r\n  File ""C:\\src\\pandas\\pandas\\core\\series.py"", line 533, in _tidy_repr\r\n    head = self[:num]._get_repr(print_header=True, length=False,\r\n  File ""C:\\src\\pandas\\pandas\\core\\series.py"", line 431, in __getslice__\r\n    return self.__getitem__(slobj)\r\n  File ""C:\\src\\pandas\\pandas\\core\\series.py"", line 299, in __getitem__\r\n    return self._get_with(key)\r\n  File ""C:\\src\\pandas\\pandas\\core\\series.py"", line 304, in _get_with\r\n    indexer = self.ix._convert_to_indexer(key, axis=0)\r\n  File ""C:\\src\\pandas\\pandas\\core\\indexing.py"", line 230, in _convert_to_indexer\r\n    i, j = labels.slice_locs(obj.start, obj.stop)\r\n  File ""C:\\src\\pandas\\pandas\\core\\index.py"", line 782, in slice_locs\r\n    beg_slice = self.get_loc(start)\r\n  File ""C:\\src\\pandas\\pandas\\core\\index.py"", line 478, in get_loc\r\n    return self._engine.get_loc(key)\r\n  File ""engines.pyx"", line 100, in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2328)\r\n  File ""engines.pyx"", line 107, in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2291)\r\nKeyError: 0'"
640,2872897,wesm,wesm,2012-01-17 18:58:03,2012-01-17 21:24:27,2012-01-17 21:24:27,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/640,b'DataFrame picklability issue',"b""User-reported crash\r\n\r\n```\r\nimport cPickle\r\nfrom pandas import DataFrame\r\ndf = DataFrame()\r\n\r\nwith open('df', 'wb') as f:\r\n   cPickle.dump(df, f)\r\n\r\nwith open('df', 'rb') as f:\r\n   df2 = cPickle.load(f)\r\n\r\nprint df2\r\n```"""
638,2868567,elpres,wesm,2012-01-17 13:43:08,2012-01-18 20:27:49,2012-01-18 19:32:12,closed,,0.7.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/638,b'NaN in a join-result despite know value',"b""When performing a join on a data frame with a data series, the last entry of the new column is sometimes NaN. It only seems to happen when the last item in the column the join is done on is found in more than one row. Here is a minimal example:\r\n\r\n    In [1]: df = DataFrame({'a': [1, 1]})\r\n\r\n    In [2]: ds = Series([2], index=[1], name='b')\r\n\r\n    In [3]: df.join(ds, on='a')\r\n    Out[3]: \r\n       a  b  \r\n    0  1  2  \r\n    1  1  NaN\r\n\r\nTested with 0.6.1 and 0.5.0 with identical results."""
636,2857998,wesm,wesm,2012-01-16 17:44:30,2012-01-17 18:39:38,2012-01-17 18:39:38,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/636,b'MultiIndex.reindex(Int64Index) -> unhelpful Cython error',"b'```\r\n\r\ntest_crosstab_margins (__main__.TestCrosstab) ... > /home/wesm/code/pandas/generated.pyx(132)pandas._tseries.merge_indexer_object (pandas/src/tseries.c:28324)()\r\n(Pdb) u\r\n> /home/wesm/code/pandas/pandas/core/index.py(1548)get_indexer()\r\n-> indexer = self._merge_indexer(target_index, self_index.indexMap)\r\n(Pdb) target_index\r\nInt64Index([  4,   4,   1,   2,   4,   3,   7,   5,   4,   3,   1,   1,   0,\r\n         6,   5,   2,   2,   3,   2,   5,   5,   1,   4,   3,   5,   4,\r\n         4,   3,   4,   3, 100])\r\n(Pdb) self\r\nMultiIndex([(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7),\r\n       (0, 8), (0, 9), (1, 0), (1, 1), (1, 3), (1, 4), (1, 5), (1, 6),\r\n       (1, 7), (1, 8), (1, 9), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4),\r\n       (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (\'All\', \'\')], dtype=object)\r\n(Pdb) c\r\nERROR\r\n\r\n======================================================================\r\nERROR: test_crosstab_margins (__main__.TestCrosstab)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""pandas/tools/tests/test_pivot.py"", line 201, in test_crosstab_margins\r\n    exp_rows = exp_rows.reindex(all_rows).fillna(0)\r\n  File ""/home/wesm/code/pandas/pandas/core/series.py"", line 1585, in reindex\r\n    level=level)\r\n  File ""/home/wesm/code/pandas/pandas/core/index.py"", line 1569, in reindex\r\n    indexer = self.get_indexer(target, method=method)\r\n  File ""/home/wesm/code/pandas/pandas/core/index.py"", line 1548, in get_indexer\r\n    indexer = self._merge_indexer(target_index, self_index.indexMap)\r\n  File ""generated.pyx"", line 132, in pandas._tseries.merge_indexer_object (pandas/src/tseries.c:28324)\r\nValueError: Buffer dtype mismatch, expected \'Python object\' but got \'long\'\r\n\r\n----------------------------------------------------------------------\r\n```'"
626,2837494,wesm,wesm,2012-01-13 22:38:10,2014-01-08 01:48:18,2012-05-07 15:01:12,closed,changhiskhan,0.8.0,1,Bug;Unicode,https://api.github.com/repos/pydata/pandas/issues/626,b'Unicode index handling in HDFStore',
623,2833082,wesm,wesm,2012-01-13 16:13:34,2012-01-13 22:44:35,2012-01-13 22:44:35,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/623,b'More float formatting bugs',b'```\r\n2000-01-03    NaN\r\n2000-01-04     0.6095\r\n2000-01-05    NaN\r\n2000-01-06     1.24\r\n2000-01-07    NaN\r\n2000-01-10     1.43\r\n2000-01-11    NaN\r\n2000-01-12     0.6033\r\n2000-01-13    NaN\r\n2000-01-14    -0.2555\r\n2000-01-17    NaN\r\n2000-01-18     0.4684\r\n2000-01-19    NaN\r\n2000-01-20    -1.825\r\n2000-01-21    NaN\r\n2000-01-24     1.023\r\n2000-01-25    NaN\r\n2000-01-26     0.09094\r\n2000-01-27    NaN\r\n2000-01-28     0.218\r\n2000-01-31    NaN\r\n2000-02-01    -1.741\r\n2000-02-02    NaN\r\n2000-02-03     1.437\r\n2000-02-04    NaN\r\n2000-02-07    -2.42\r\n2000-02-08    NaN\r\n2000-02-09     0.2374\r\n2000-02-10    NaN\r\n2000-02-11     0.06525\r\nName: ts\r\n```'
622,2831503,gdementen,wesm,2012-01-13 14:11:51,2013-02-10 16:23:57,2013-02-10 16:23:57,closed,jreback,0.11,14,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/622,b'pandas converts int32 to int64',"b""Is this intended? I had hoped no copying at all would happen in that case.\r\n\r\nIn [65]: a = np.random.randint(10, size=1e6)\r\n\r\nIn [66]: a.dtype\r\nOut[66]: dtype('int32')\r\n\r\nIn [67]: b = np.random.randint(2, size=1e6)\r\n\r\nIn [68]: df = pandas.DataFrame({'a': a, 'b': b})\r\n\r\nIn [69]: df.dtypes\r\nOut[69]:\r\na    int64\r\nb    int64"""
621,2823711,dieterv77,wesm,2012-01-12 21:13:05,2012-01-12 21:52:54,2012-01-12 21:52:54,closed,,0.7.0,2,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/621,b'DataFrame constructor speed',"b""Hi, i was playing around with constructing DataFrame's from nested dicts, and noticed that things have gotten a bit slower since v0.6.1.\r\n\r\nHere's some sample code i was playing with:\r\n\r\nimport time\r\nimport pandas\r\n\r\nprint pandas.__version__\r\n\r\ndata = dict((i,dict((j,float(j)) for j in xrange(100))) for i in xrange(5000))\r\nt0 = time.time(); df = pandas.DataFrame(data); t1 = time.time(); print t1 - t0\r\n\r\nWith version 0.6.1, the printed time is about 0.21s on my machine, with a little help from git bisect,\r\ni found that:\r\n\r\ncommit f3ca67dc371ce46589e6a1598168cddcc1b8cb6b takes it from 0.21s to 0.44s\r\ncommit 9d65e8e77b675ce4774b6f394acd8f74bd40af78 takes it from 0.44s to 0.54s\r\n\r\nIt's possible that some of these were unavoidable considering they may have been necessary bugfixes, but i wanted to\r\nsee if anyone else is seeing this too.\r\n\r\nenvironment info: 64bit ubuntu 11.10, python2.7, numpy 1.6.1, cython 0.15.1\r\n\r\n"""
618,2820867,yarikoptic,wesm,2012-01-12 17:28:30,2012-01-17 20:34:00,2012-01-17 20:34:00,closed,,0.7.0,6,Bug,https://api.github.com/repos/pydata/pandas/issues/618,b'Pure Python GroupBy bug',"b'I have tried to find related issue but failed... so pardon me if it is a duplicate:\r\n\r\nATM if groupping doesn\'t result in actually all possible combinations, the pandas spits out non-informative\r\n\r\n```\r\n/home/yoh/deb/gits/pkg-exppsy/pandas/pandas/core/groupby.py in _aggregate_series_pure_python(self, obj, func, ngroups)\r\n    431                     raise ValueError(\'function does not reduce\')\r\n    432 \r\n--> 433             counts[label] = group.shape[0]\r\n    434             result[label] = res\r\n    435 \r\n\r\nIndexError: index out of bounds\r\n> /home/yoh/deb/gits/pkg-exppsy/pandas/pandas/core/groupby.py(433)_aggregate_series_pure_python()\r\n    432 \r\n--> 433             counts[label] = group.shape[0]\r\n    434             result[label] = res\r\n```\r\n\r\nimho there could be an option to still handle those but place NaNs for those entries, OR at least spit out an informative exception something like ""combination f1=\'x\', f2=\'y\' doesn\'t have data entries in the original data, or smth like that'"
616,2814191,wesm,wesm,2012-01-12 05:10:01,2012-01-12 05:38:31,2012-01-12 05:38:31,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/616,b'Float formatting of mixed-type Series is incorrect',b'```\r\nIn [3]: data.ix[4]\r\nOut[3]: \r\nA    bar\r\nB    one\r\nC    NaN\r\nD     0.1949\r\nE     0.8223\r\nF     1.335\r\nName: 4\r\n```'
615,2813962,wesm,wesm,2012-01-12 04:19:47,2012-01-12 20:35:58,2012-01-12 20:35:57,closed,wesm,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/615,"b'Test concat with MultiIndex, found a buglet'",b'Just a note to myself to not forget about this'
612,2812325,wesm,wesm,2012-01-12 00:23:58,2012-01-12 00:46:30,2012-01-12 00:46:30,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/612,"b""Pure python multi-key groupby can't handle non-numeric results""","b'reported by @yarikoptic, could occur fairly easily in 0.6.1, now very pathological to reproduce\r\n\r\n```\r\ndata = DataFrame({\'A\' : [\'foo\', \'foo\', \'foo\', \'foo\',\r\n                                      \'bar\', \'bar\', \'bar\', \'bar\',\r\n                                      \'foo\', \'foo\', \'foo\'],\r\n                               \'B\' : [\'one\', \'one\', \'one\', \'two\',\r\n                                      \'one\', \'one\', \'one\', \'two\',\r\n                                      \'two\', \'two\', \'one\'],\r\n                               \'C\' : [\'dull\', \'dull\', \'shiny\', \'dull\',\r\n                                      \'dull\', \'shiny\', \'shiny\', \'dull\',\r\n                                      \'shiny\', \'shiny\', \'shiny\'],\r\n                               \'D\' : np.random.randn(11),\r\n                               \'E\' : np.random.randn(11),\r\n                               \'F\' : np.random.randn(11)})\r\n\r\ndef bad(x):\r\n    assert(len(x.base) == len(x))\r\n    return \'foo\'\r\n\r\ndata.groupby([\'A\', \'B\']).agg(bad)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/wesm/code/pandas/vb_suite/<ipython-input-23-745606b0da15> in <module>()\r\n----> 1 data.groupby([\'A\', \'B\']).agg(bad)\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\r\n    283         See docstring for aggregate\r\n    284         """"""\r\n--> 285         return self.aggregate(func, *args, **kwargs)\r\n    286 \r\n    287     def _iterate_slices(self):\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\r\n    963         else:\r\n    964             if len(self.groupings) > 1:\r\n--> 965                 return self._python_agg_general(arg, *args, **kwargs)\r\n    966             else:\r\n    967                 result = self._aggregate_generic(arg, *args, **kwargs)\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _python_agg_general(self, func, *args, **kwargs)\r\n    393             try:\r\n    394                 result, counts = self._aggregate_series(obj, agg_func,\r\n--> 395                                                         comp_ids, max_group)\r\n    396                 output[name] = result                                                                                                 \r\n    397             except TypeError:\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _aggregate_series(self, obj, func, group_index, ngroups)\r\n    411             return _aggregate_series_fast(obj, func, group_index, ngroups)\r\n    412         except Exception:\r\n--> 413             return self._aggregate_series_pure_python(obj, func, ngroups)\r\n    414 \r\n    415     def _aggregate_series_pure_python(self, obj, func, ngroups):\r\n\r\n/home/wesm/code/pandas/pandas/core/groupby.pyc in _aggregate_series_pure_python(self, obj, func, ngroups)\r\n    422                 continue\r\n    423             counts[label] = group.shape[0]\r\n--> 424             result[label] = func(group)\r\n    425 \r\n    426         return result, counts\r\n\r\nValueError: could not convert string to float: foo\r\n\r\n```'"
611,2810288,wesm,wesm,2012-01-11 21:25:49,2012-01-12 18:50:33,2012-01-12 18:50:33,closed,adamklein,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/611,b'TypeError in Cython with tuple subclasses in DataFrame.from_records',"b'From user email\r\n\r\nHi there,\r\n\r\nJust a quick bug report. I\'m using psycopg2 with a NamedTupleCursor, which returns namedtuples for rows. I\'m trying to use DataFrame.from_records like so:\r\n\r\ndf = DataFrame.from_records(rows, rows[0]._fields)\r\n\r\nI get the following error:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n/Users/kdevens/source/(etc.) <ipython-input-46-3e705b917abc> in <module>()\r\n----> 1 df = DataFrame.from_records(rows, rows[0]._fields)\r\n\r\n/Library/Python/2.6/site-packages/pandas-0.6.1-py2.6-macosx-10.6-universal.egg/pandas/core/frame.pyc in from_records(cls, data, index, exclude, columns, names)\r\n    533             columns, sdict = _rec_to_dict(data)\r\n    534         else:\r\n--> 535             sdict, columns = _list_to_sdict(data, columns)\r\n    536 \r\n    537         if exclude is None:\r\n\r\n/Library/Python/2.6/site-packages/pandas-0.6.1-py2.6-macosx-10.6-universal.egg/pandas/core/frame.pyc in _list_to_sdict(data, columns)\r\n   3547 def _list_to_sdict(data, columns):\r\n   3548     if isinstance(data[0], tuple):\r\n-> 3549         content = list(lib.to_object_array_tuples(data).T)\r\n   3550     else:\r\n   3551         # list of lists\r\n\r\n\r\n/Library/Python/2.6/site-packages/pandas-0.6.1-py2.6-macosx-10.6-universal.egg/pandas/_tseries.so in pandas._tseries.to_object_array_tuples (pandas/src/tseries.c:52160)()\r\n\r\nTypeError: Expected tuple, got Record\r\n```\r\n\r\nSo it looks like in tseries.c it\'s doing something like ""type(row) == tuple"" instead of ""isinstance(row, tuple)"". For what it\'s worth, ""isinstance(rows[0], tuple)"" is true, so the tuples returned by psycopg2 are definitely real namedtuples. Additionally, the following works fine:\r\n\r\n```\r\nIn [49]: dictrows = [r._asdict() for r in rows]\r\n\r\nIn [50]: df = DataFrame.from_dict(dictrows)\r\n```\r\n\r\nSo there\'s no problem with the data, or with DataFrame, just the type check in tseries.c.\r\n\r\nLet me know if I can provide any more information! Thanks.'"
601,2778822,xdong,wesm,2012-01-10 02:32:50,2012-01-10 03:52:04,2012-01-10 03:52:04,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/601,b'DataFrame.groupby bug',"b""pandas latest commit: When groupby using multiple columns, I get a TypeError. Furthermore, the TypeError is different on 64bit and 32bit Linux machines.\r\n\r\nIn [1]: import pandas\r\n\r\nIn [2]: a = pandas.DataFrame(randn(2, 3))\r\n\r\nIn [3]: a\r\nOut[3]: \r\n   0       1       2     \r\n0 -0.7262  0.9849  0.1685\r\n1 -1.0241 -2.3102 -1.1348\r\n\r\nIn [4]: g = a.groupby([0, 1])\r\n\r\nIn [5]: g.sum()\r\nOut[5]: \r\n\r\n#on Ubuntu64 11.10; numpy 1.6.1\r\nTypeError: object of type 'int' has no len()\r\n\r\n#on sles10 32bit; numpy 1.5.1\r\nTypeError: array cannot be safely cast to required type\r\n\r\n--------------------------------------\r\n\r\n#The following works fine on the 64bit machine, but not on the 32bit:\r\n\r\nIn [6]: a = pandas.DataFrame(randn(2, 3), columns=['a', 'b', 'c'])\r\n\r\nIn [7]: a\r\nOut[7]: \r\n   a        b       c     \r\n0  0.04682  0.1323  0.5046\r\n1 -0.15613  0.4555  0.4553\r\n\r\nIn [8]: g = a.groupby(['a', 'b'])\r\n\r\nIn [9]: g.sum()\r\nOut[9]: \r\n\r\n#on sles10 32bit; numpy 1.5.1\r\nTypeError: array cannot be safely cast to required type\r\n\r\n-------------------------------------\r\n\r\nThis bug was not present in the pypi version of pandas. However, that version has a Series index overflow bug on the 32bit machine:\r\n\r\nIn [10]: a = pandas.Series(randn(2))\r\n\r\nIn [11]: a\r\nOut[11]: \r\n0     0.08557\r\n1    -0.3659\r\n\r\nIn [12]: a[100]\r\nOut[12]: 8.318223500663e-312"""
592,2758651,wesm,wesm,2012-01-08 00:00:33,2012-01-13 15:59:34,2012-01-13 15:59:34,closed,,0.7.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/592,b'Disable integer-based slicing with integer indexes',
591,2758581,wesm,wesm,2012-01-07 23:45:04,2012-01-13 03:01:01,2012-01-13 03:00:57,closed,,0.7.0,5,Bug,https://api.github.com/repos/pydata/pandas/issues/591,"b'Docstring decorators confuse pdb, any way to fix?'",
590,2757546,wesm,wesm,2012-01-07 19:50:07,2012-01-08 02:46:00,2012-01-07 23:59:43,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/590,b'NA float format bug',b'Need to put a leading space on NaN by default...I will fix this weekend if no one else gets to it @adamklein \r\n\r\n```\r\n       D        E         F      \r\ncount  3.00000  3.000000  3.00000\r\nmean   0.45674  0.522317  0.15975\r\nstd    1.41366  1.268866  0.95044\r\nmin   -1.02552 -0.414997 -0.77343\r\n25%   -0.20989 -0.199634 -0.32366\r\n50%    0.60573  0.015730  0.12611\r\n75%    1.19786  0.990974  0.62634\r\nmax    1.78999  1.966219  1.12657\r\ncount  1.00000  1.000000  1.00000\r\nmean  -1.17800 -0.124118  0.19188\r\nstd   NaN      NaN       NaN     \r\nmin   -1.17800 -0.124118  0.19188\r\n25%   -1.17800 -0.124118  0.19188\r\n50%   -1.17800 -0.124118  0.19188\r\n75%   -1.17800 -0.124118  0.19188\r\nmax   -1.17800 -0.124118  0.19188\r\ncount  4.00000  4.000000  4.00000\r\nmean  -0.09697 -0.723308  0.18293\r\nstd    1.07386  0.870480  0.39080\r\nmin   -1.07445 -1.923554 -0.15451\r\n25%   -0.55308 -1.080760 -0.03626\r\n50%   -0.37468 -0.480612  0.07309\r\n75%    0.08143 -0.123160  0.29228\r\nmax    1.43593 -0.008454  0.74007\r\ncount  3.00000  3.000000  3.00000\r\nmean   0.39914 -0.110401 -0.82227\r\nstd    0.22223  1.146990  1.42853\r\nmin    0.17519 -1.022600 -1.78633\r\n25%    0.28890 -0.754235 -1.64286\r\n50%    0.40260 -0.485871 -1.49940\r\n75%    0.51111  0.345698 -0.34025\r\nmax    0.61961  1.177267  0.81891\r\n```'
583,2751462,wesm,wesm,2012-01-06 20:13:10,2012-01-06 20:22:30,2012-01-06 20:22:30,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/583,b'Converter returning string passed to read_csv causes exception',"b'for example:\r\n\r\n```\r\n        data = """"""Id;Number1;Number2;Text1;Text2;Number3\r\n1;1521,1541;187101,9543;ABC;poi;4,738797819\r\n2;121,12;14897,76;DEF;uyt;0,377320872\r\n3;878,158;108013,434;GHI;rez;2,735694704""""""\r\n        f = lambda x : x.replace("","", ""."")\r\n        converter = {\'Number1\':f,\'Number2\':f, \'Number3\':f}\r\n        df2 = read_csv(StringIO(data), sep=\';\',converters=converter)\r\n```'"
581,2748039,lodagro,wesm,2012-01-06 15:19:23,2012-01-09 22:55:11,2012-01-09 22:55:06,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/581,b'New numeric display adds sometimes unwanted trailing zeros',"b""New numeric display adds sometimes unwanted trailing zeros.\r\n\r\nOne should also be careful when adding trailing zeros. In the example below, pi is represented up to the default precision of three and trailing zeros are added. I'm not in favor of this. To write pi=3.142 is ok, pi=3.1420000 is not ok.\r\n\r\n```python\r\nIn [3]: df = pandas.DataFrame({'A': [np.pi, np.pi*100000]})\r\n\r\nIn [4]: df\r\nOut[4]: \r\n   A        \r\n0  3.1420000\r\n1  3.142e+05\r\n\r\nIn [5]: pandas.set_printoptions(precision=5)\r\nIn [6]: df\r\nOut[6]: \r\n   A        \r\n0 3.14160000\r\n1 3.1416e+05\r\n```\r\n\r\nIn this example -1 and 1 become -10000 and 10000.\r\n\r\n```python\r\nIn [5]: df = pandas.DataFrame({'A': [-1, 1, -1e-1, 1e-1, -1e-2, 1e-2, -1e-3, 1e-3]})\r\n\r\nIn [6]: df\r\nOut[6]: \r\n   A    \r\n0 -10000\r\n1  10000\r\n2 -0.100\r\n3  0.100\r\n4 -0.010\r\n5  0.010\r\n6 -0.001\r\n7  0.001\r\n\r\nIn [7]: df['A'][0]\r\nOut[7]: -1.0\r\n```\r\n\r\nWhen enabling the engineering float formatter, things get messy.\r\n\r\n```python\r\nIn [8]: pandas.set_eng_float_format(use_eng_prefix=True)\r\n\r\nIn [9]: df\r\nOut[9]: \r\n   A       \r\n0 -1.000000\r\n1 1.0000000\r\n2 -100.000m\r\n3 100.000m0\r\n4 -10.000m0\r\n5 10.000m00\r\n6 -1.000m00\r\n7 1.000m000\r\n\r\nIn [10]: pandas.set_eng_float_format(precision=1, use_eng_prefix=True)\r\n\r\nIn [11]: df\r\nOut[11]: \r\n   A     \r\n0 -1.0000\r\n1 1.00000\r\n2 -100.0m\r\n3 100.0m0\r\n4 -10.0m0\r\n5 10.0m00\r\n6 -1.0m00\r\n7 1.0m000\r\n```\r\n\r\nMaybe easier to replace added trailing zeros with white  space?\r\n"""
571,2720703,lodagro,wesm,2012-01-04 08:50:43,2012-01-08 00:17:36,2012-01-08 00:14:59,closed,adamklein,0.7.0,4,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/571,b'DataFrame.to_string() remove extra white space between columns',"b'```DataFrame.to_string()``` uses one extra white space between columns for sign alignment.\r\nRemove it, for user who need exact column width, how to do this nicely?\r\n\r\nsee also mailing list [discussion](https://groups.google.com/d/topic/pystatsmodels/XPuDEfFKU8U/discussion)'"
564,2704118,wesm,wesm,2012-01-02 19:09:40,2012-01-03 23:20:25,2012-01-03 23:20:25,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/564,b'Mixing Series/list + ndarray should not generate exception if everything right length in DataFrame constructor',
560,2686109,wesm,wesm,2011-12-30 03:37:29,2012-05-19 20:09:35,2012-05-19 20:09:35,closed,,0.8.0,2,Bug;Build;Docs,https://api.github.com/repos/pydata/pandas/issues/560,"b""Code blocks don't work right in ipython_directive""",
557,2684530,wesm,wesm,2011-12-30 01:00:49,2012-05-19 17:17:27,2012-05-19 17:17:27,closed,,0.8.0,3,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/557,b'DataFrame.drop_duplicates is unreliable in the presence of NA values',
556,2683725,adamklein,wesm,2011-12-29 23:09:47,2012-01-03 19:56:11,2012-01-03 19:56:11,closed,,0.7.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/556,b'Series constructor ought to take MaskedArray as data argument',b'For consistency with DataFrame constructor'
551,2671442,wesm,wesm,2011-12-28 14:14:20,2011-12-29 05:15:50,2011-12-29 05:15:50,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/551,b'Setting elements with MultiIndex through ix can fail',"b""reported by @CRP\r\n\r\n```\r\nHi, I am currently trying to access and change elements of a multiindexed DataFrame with .ix. This works on read but not on write:\r\n\r\nIn [342]: arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\r\n\r\nIn [343]: tuples = zip(*arrays)\r\n\r\nIn [344]: index = MultiIndex.from_tuples(tuples, names=['first', 'second'])\r\n\r\nIn [345]: a=DataFrame(randn(6, 3), index=index[:6])\r\n\r\nIn [346]: a.ix[('bar','two'),1]\r\nOut[346]: -0.574310975217078\r\n\r\nIn [347]: a.ix[('bar','two'),1]=999\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/Users/c.prinoth/<ipython-input-347-755b7b16a7da> in <module>()\r\n----> 1 a.ix[('bar','two'),1]=999\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/indexing.pyc in __setitem__(self, key, value)\r\n     56                 raise IndexingError('only tuples of length <= %d supported',\r\n     57                                     self.ndim)\r\n---> 58             indexer = self._convert_tuple(key)\r\n     59         else:\r\n     60             indexer = self._convert_to_indexer(key)\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/indexing.pyc in _convert_tuple(self, key)\r\n     65         keyidx = []\r\n     66         for i, k in enumerate(key):\r\n---> 67             idx = self._convert_to_indexer(k, axis=i)\r\n     68             keyidx.append(idx)\r\n     69         return _maybe_convert_ix(*keyidx)\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/indexing.pyc in _convert_to_indexer(self, obj, axis)\r\n    239                 mask = indexer == -1\r\n    240                 if mask.any():\r\n--> 241                     raise KeyError('%s not in index' % objarr[mask])\r\n    242 \r\n    243                 return indexer\r\n\r\nKeyError: '[bar two] not in index'\r\n\r\nIs this a bug or am I using the index incorrectly?\r\n```"""
548,2666204,wesm,wesm,2011-12-27 20:57:13,2011-12-28 22:29:44,2011-12-28 22:29:44,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/548,b'Series.corr failure',"b'The zero-length check in nancorr is in the wrong place, see below:\r\n\r\n```\r\nfrom pandas import Series\r\nimport numpy as np\r\ns1 = Series( index = range(3) , data = [np.NaN , np.NaN , np.NaN] )\r\ns2 = Series( index = range(3) , data = [np.NaN , np.NaN , np.NaN] )\r\ns2.corr(s1)\r\n ```'"
546,2665200,mattharrison,wesm,2011-12-27 18:21:23,2011-12-29 05:33:52,2011-12-29 04:57:47,closed,,0.7.0,7,Bug,https://api.github.com/repos/pydata/pandas/issues/546,"b'read_csv converters ""Buffer dtyle mismatch""'","b'Am trying to convert a string column to seconds in a dataframe. It looks like there is some sort of off by one error.\r\n\r\nHere\'s my csv\r\n\r\n```\r\n""id"",""Time to Calculate""\r\n1,""19:31:15""\r\n2,""19:18:17""\r\n3,""19:31:15""\r\n4,""19:27:42""\r\n5,""19:27:42""\r\n6,""19:28:25""\r\n7,""19:28:25""\r\n8,""19:28:25""\r\n9,""19:28:04""\r\n10,""19:28:04""\r\n11,""19:28:04""\r\n12,""19:25:56""\r\n13,""19:25:56""\r\n14,""19:25:57""\r\n15,""19:25:57""\r\n16,""19:26:41""\r\n17,""19:26:41""\r\n18,""19:26:08""\r\n19,""19:26:08""\r\n20,""1 day, 1:04:33""\r\n21,""1 day, 1:04:33""\r\n22,""1 day, 1:04:33""\r\n23,""2 days, 2:14:33""\r\n```\r\n\r\nHere\'s my code\r\n\r\n```\r\nfrom pandas import read_csv\r\n\r\n\r\ndef sec_to_calc(time_to_calc):\r\n    print ""TIMETD"", time_to_calc, type(time_to_calc)\r\n    total = 0\r\n    days = 0\r\n    if \'days\' in time_to_calc:\r\n        days, time_to_calc = time_to_calc.split("" days, "")\r\n        days = int(days)\r\n    elif \'day\' in time_to_calc:\r\n        days = 1\r\n        _, time_to_calc = time_to_calc.split("" day, "")\r\n    hours, min, sec = [int(x) for x in time_to_calc.split(\':\')]\r\n    return days + 24 * 60 * 60 + hours *60 * 60 + min * 60 + sec\r\n\r\n\r\nfilename = \'/tmp/test.csv\'\r\ndf = read_csv(filename, converters={\'Time to Calculate\': sec_to_calc})\r\n```\r\n\r\nHere\'s my error:\r\n\r\n```\r\n  File ""/tmp/test.py"", line 19, in <module>\r\n    df = read_csv(filename, converters={\'Time to Calculate\': sec_to_calc})\r\n  File ""/home/mharrison/work/pandas/env/lib/python2.6/site-packages/pandas/io/parsers.py"", line 64, in read_csv\r\n    return parser.get_chunk()\r\n  File ""/home/mharrison/work/pandas/env/lib/python2.6/site-packages/pandas/io/parsers.py"", line 418, in get_chunk\r\n    data = _convert_to_ndarrays(data, self.na_values)\r\n  File ""/home/mharrison/work/pandas/env/lib/python2.6/site-packages/pandas/io/parsers.py"", line 462, in _convert_to_ndarrays\r\n    result[c] = _convert_types(values, na_values)\r\n  File ""/home/mharrison/work/pandas/env/lib/python2.6/site-packages/pandas/io/parsers.py"", line 470, in _convert_types\r\n    lib.sanitize_objects(values, na_values)\r\n  File ""parsing.pyx"", line 220, in pandas._tseries.sanitize_objects (pandas/src/tseries.c:54380)\r\nValueError: Buffer dtype mismatch, expected \'Python object\' but got \'long\'\r\n```'"
545,2660468,wesm,wesm,2011-12-26 23:41:12,2011-12-29 03:58:07,2011-12-29 03:58:07,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/545,"b'Aggregate by multiple levels fails in DataFrame.sum, others'",
538,2649394,wesm,wesm,2011-12-23 18:35:24,2012-05-11 15:43:13,2012-05-11 15:43:13,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/538,b'Address problems surrounding mutability of index.name field (e.g. sharing of objects)',
529,2641738,wesm,wesm,2011-12-22 20:54:42,2011-12-23 18:27:39,2011-12-23 18:27:39,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/529,b'Handling of mis-matched columns in DataFrame incorrect except for combine_first',b'Probably should align on both rows and columns and it will solve the problem'
527,2641607,wesm,wesm,2011-12-22 20:41:46,2011-12-23 23:35:04,2011-12-23 23:35:04,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/527,"b'DataFrame.delevel drops the column names, is this right?'",
525,2640948,wesm,wesm,2011-12-22 19:33:49,2011-12-29 03:06:17,2011-12-29 03:06:17,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/525,b'Panel.to_frame does not preserve index names',
522,2630575,wesm,wesm,2011-12-21 22:33:55,2012-05-07 16:42:33,2012-05-07 16:42:33,closed,,0.8.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/522,b'Misbehaved reindexing with NaN labels',"b""from user report\r\n\r\n```\r\nIn [116]: df = pandas.DataFrame([[1,2], [3,4], [numpy.nan,numpy.nan], [7,8], [9,10]], columns=['a', 'b'], index=[100.0, 101.0, numpy.nan, 102.0, 103.0])                 \r\n\r\nIn [117]: print df\r\n\r\n--------> print(df)\r\n\r\n       a    b\r\n\r\n100.0  1    2\r\n\r\n101.0  3    4\r\n\r\nnan    NaN  NaN\r\n\r\n102.0  7    8\r\n\r\n103.0  9    10\r\n\r\n \r\n\r\n#referencing anything after the nan in the index results in getting nans!\r\n\r\nIn [118]: df.reindex(index=[101.0, 102.0, 103.0])\r\n\r\nOut[118]:\r\n\r\n       a    b\r\n\r\n101.0  3    4\r\n\r\n102.0  NaN  NaN\r\n\r\n103.0  NaN  NaN\r\n\r\n \r\n\r\n#after nan = NaN again!\r\n\r\nIn [119]: df.reindex(index=[103.0])\r\n\r\nOut[119]:\r\n\r\n       a    b\r\n\r\n103.0  NaN  NaN\r\n\r\n \r\n\r\n#before nan looks good though\r\n\r\nIn [120]: df.reindex(index=[101.0])\r\n\r\nOut[119]:\r\n\r\n       a    b\r\n\r\n101.0  3    4\r\n```"""
512,2616710,bshanks,wesm,2011-12-20 19:02:14,2012-12-28 15:08:05,2012-12-28 15:08:05,closed,,0.10.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/512,"b""HDFStore silently coerces number to string in 'where' clause but not in put""","b'Thanks for pandas! Once written, tables with numerical values in the index are unselectable. Perhaps either they should be coerced to string upon store.put, or alternately they should not be coerced in store.select:\r\n\r\n```python\r\nstore = HDFStore(\'test.h5\')\r\nstore.put(\'test\', DataFrame([0, 1, 2], [10, 11, 12], [\'col1\']), table=True)\r\nstore.select(\'test\', where=[{\'field\' : \'index\',\'op\'    : \'>=\',\'value\' : 11}])\r\n\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n\r\n/home/bshanks/prog/atr/<ipython console> in <module>()\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc in select(self, key, where)\r\n    235             raise Exception(\'can only select on objects written as tables\')\r\n    236         if group is not None:\r\n--> 237             return self._read_group(group, where)\r\n    238 \r\n    239     def put(self, key, value, table=False, append=False,\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc in _read_group(self, group, where)\r\n    619         kind = _LEGACY_MAP.get(kind, kind)\r\n    620         handler = self._get_handler(op=\'read\', kind=kind)\r\n--> 621         return handler(group, where)\r\n    622 \r\n    623     def _read_series(self, group, where=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc in _read_frame_table(self, group, where)\r\n    646 \r\n    647     def _read_frame_table(self, group, where=None):\r\n--> 648         return self._read_panel_table(group, where)[\'value\']\r\n    649 \r\n    650     def _read_panel_table(self, group, where=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc in _read_panel_table(self, group, where)\r\n    655         # create the selection\r\n\r\n    656         sel = Selection(table, where)\r\n--> 657         sel.select()\r\n    658         fields = table._v_attrs.fields\r\n    659 \r\n\r\n/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc in select(self)\r\n    861         """"""\r\n    862         if self.the_condition:\r\n--> 863             self.values = self.table.readWhere(self.the_condition)\r\n    864 \r\n    865         else:\r\n\r\n/usr/lib/python2.7/dist-packages/tables/table.pyc in readWhere(self, condition, condvars, field, start, stop, step)\r\n   1271 \r\n   1272         coords = [ p.nrow for p in\r\n-> 1273                    self._where(condition, condvars, start, stop, step) ]\r\n   1274         self._whereCondition = None  # reset the conditions\r\n   1275         return self.readCoordinates(coords, field)\r\n\r\n/usr/lib/python2.7/dist-packages/tables/table.pyc in _where(self, condition, condvars, start, stop, step)\r\n   1225         # Compile the condition and extract usable index conditions.\r\n\r\n   1226         condvars = self._requiredExprVars(condition, condvars, depth=3)\r\n-> 1227         compiled = self._compileCondition(condition, condvars)\r\n   1228 \r\n   1229         # Can we use indexes?\r\n\r\n\r\n/usr/lib/python2.7/dist-packages/tables/table.pyc in _compileCondition(self, condition, condvars)\r\n   1101         indexedcols = frozenset(indexedcols)\r\n   1102         # Now let ``compile_condition()`` do the Numexpr-related job.\r\n\r\n-> 1103         compiled = compile_condition(condition, typemap, indexedcols, copycols)\r\n   1104 \r\n   1105         # Check that there actually are columns in the condition.\r\n\r\n\r\n/usr/lib/python2.7/dist-packages/tables/conditions.pyc in compile_condition(condition, typemap, indexedcols, copycols)\r\n    154     except NotImplementedError, nie:\r\n    155         # Try to make this Numexpr error less cryptic.\r\n\r\n--> 156         raise _unsupported_operation_error(nie)\r\n    157     params = varnames\r\n    158 \r\n\r\nNotImplementedError: unsupported operand types for *ge*: long, str\r\n```'"
511,2615376,craustin,wesm,2011-12-20 17:19:47,2012-01-09 19:33:57,2012-01-09 19:33:57,closed,,0.7.0,9,Bug,https://api.github.com/repos/pydata/pandas/issues/511,b'Certain DataFrame cannot be pickled to string',"b""import cPickle\r\nimport numpy as np\r\nfrom pandas import Series\r\ns = Series({'a' : None})\r\ns['a'] = np.inf\r\ncPickle.loads(cPickle.dumps(s))\r\n\r\nValueError: could not convert string to float"""
510,2614924,craustin,wesm,2011-12-20 16:47:22,2011-12-21 15:46:57,2011-12-21 03:57:15,closed,,0.7.0,5,Bug,https://api.github.com/repos/pydata/pandas/issues/510,b'Series no longer returns float64',"b""Is this desired?\r\n\r\nimport numpy as np\r\nfrom pandas import Series\r\ns2 = Series({'A': np.float64(5.0), 'B': np.float64(0.0)})\r\nprint type(s2['A'])\r\n\r\n(type 'float')\r\n\r\nIn 0.4.0, this returned numpy.float64.  That seems more expected."""
505,2605282,solomon-negusse,wesm,2011-12-19 20:32:56,2013-12-04 00:43:44,2011-12-21 04:49:42,closed,,0.7.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/505,b'skiprows in read_csv not working?',"b""Hi Wes, \r\nI'm trying to read in a csv file with few header lines at the beginning of the file starting w/ a '#'. I tried to use skiprows specifying a list of the first n rows to skip  as [0,1,2,3,..,n] but I keep getting Index has duplicates ['#'] error.  Is this a bug or I'm doing something wrong?  Also, will this be the best place to post such issues? \r\nI'm using pandas v 0.6.0 with python 2.7.2\r\nThanks for the great work you are doing w/ pandas. I'm finding it very useful and saves me a lot of time analyzing time series data. \r\n\r\n-Solomon"""
501,2589627,wesm,wesm,2011-12-17 17:38:00,2011-12-17 18:01:28,2011-12-17 18:01:28,closed,,0.7.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/501,b'Handle passing Index.name in union',
500,2585098,craustin,wesm,2011-12-16 22:20:41,2011-12-17 18:03:57,2011-12-17 18:03:57,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/500,b'Adding DataFrames with offsets yields naive index',"b""Adding two DataFrames with a  DateRange index (with BusinessMonthEnd offset) yields a DataFrame with a simple 'Index'."""
495,2568345,adamklein,wesm,2011-12-15 15:46:25,2011-12-15 17:04:54,2011-12-15 17:04:54,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/495,b'invalid dtype access crashes interpreter (segfault)',"b""In [1]: from pandas import *\r\n\r\nIn [2]: df = DataFrame([1,2,3,4], index=['a','b','c','d'])\r\n\r\nIn [3]: df\r\nOut[3]: \r\n   0\r\na  1\r\nb  2\r\nc  3\r\nd  4\r\n\r\nIn [4]: df.dtypes\r\nOut[4]: 0    int64\r\n\r\nIn [5]: df.dtypes[0]\r\nOut[5]: dtype('int64')\r\n\r\nIn [6]: df.dtypes[1]\r\n*** glibc detected *** /home/adam/.virtualenvs/py27/bin/python: free(): corrupted unsorted chunks: 0x00000000025db8e0 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x78a96)[0x7f8751957a96]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x6c)[0x7f875195bd7c]\r\n/home/adam/.virtualenvs/py27/bin/python[0x4354d8]\r\n/home/adam/.virtualenvs/py27/bin/python[0x447a2c]\r\n/home/adam/.virtualenvs/py27/bin/python[0x4e4fe3]\r\n/home/adam/.virtualenvs/py27/bin/python[0x4e500d]\r\n======= Memory map: ========\r\n00400000-00633000 r-xp 00000000 08:05 5373988                            /home/adam/.virtualenvs/py27/bin/python\r\n00832000-00833000 r--p 00232000 08:05 5373988                            /home/adam/.virtualenvs/py27/bin/python\r\n00833000-0089c000 rw-p 00233000 08:05 5373988                            /home/adam/.virtualenvs/py27/bin/python\r\n0089c000-008ae000 rw-p 00000000 00:00 0 \r\n016f0000-0278e000 rw-p 00000000 00:00 0                                  [heap]\r\n"""
492,2564334,andreas-h,wesm,2011-12-15 08:52:27,2011-12-15 17:53:24,2011-12-15 17:53:24,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/492,b'Saving DataFrame with tuple index to HDFStore fails',"b'The following code fails on pandas 0.6.1\r\n\r\n```\r\nimport numpy as np\r\nimport pandas\r\ncol = np.arange(10)\r\nidx = [(0.,1.), (2., 3.), (4., 5.)]\r\ndata = np.random.randn(30).reshape((3, 10))\r\nDF = pandas.DataFrame(data, index=idx, columns=col)\r\nH = pandas.HDFStore(""test.h5"", ""w"")\r\nH[\'data\'] = DF\r\nH.close()\r\n```\r\n\r\nThe traceback is as follows:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home2/hilboll/<ipython-input-15-a313de079f58> in <module>()\r\n----> 1 H[\'data\'] = DF\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in __setitem__(self, key, value)\r\n    121 \r\n    122     def __setitem__(self, key, value):\r\n--> 123         self.put(key, value)\r\n    124 \r\n    125     def __len__(self):\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in put(self, key, value, table, append, compression)\r\n    259         """"""\r\n    260         self._write_to_group(key, value, table=table, append=append,\r\n--> 261                              comp=compression)\r\n    262 \r\n    263     def _get_handler(self, op, kind):\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in _write_to_group(self, key, value, table, append, comp)\r\n    327             wrapper = lambda value: handler(group, value)\r\n    328 \r\n--> 329         wrapper(value)\r\n    330         group._v_attrs.pandas_type = kind\r\n    331 \r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in <lambda>(value)\r\n    325 \r\n    326             handler = self._get_handler(op=\'write\', kind=kind)\r\n--> 327             wrapper = lambda value: handler(group, value)\r\n    328 \r\n    329         wrapper(value)\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in _write_frame(self, group, df)\r\n    336 \r\n    337     def _write_frame(self, group, df):\r\n--> 338         self._write_block_manager(group, df._data)\r\n    339 \r\n    340     def _read_frame(self, group, where=None):\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in _write_block_manager(self, group, data)\r\n    347         group._v_attrs.ndim = data.ndim\r\n    348         for i, ax in enumerate(data.axes):\r\n--> 349             self._write_index(group, \'axis%d\' % i, ax)\r\n    350 \r\n    351         # Supporting mixed-type DataFrame objects...nontrivial\r\n\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in _write_index(self, group, key, index)\r\n    439         else:\r\n    440             setattr(group._v_attrs, \'%s_variety\' % key, \'regular\')\r\n--> 441             converted, kind, _ = _convert_index(index)\r\n    442             self._write_array(group, key, converted)\r\n    443             node = getattr(group, key)\r\n\r\n/home2/hilboll/.virtualenvs/pydoas/lib/python2.7/site-packages/pandas/io/pytables.pyc in _convert_index(index)\r\n    734         return np.asarray(values, dtype=np.float64), \'float\', atom\r\n    735     else: # pragma: no cover\r\n--> 736         raise ValueError(\'unrecognized index type %s\' % type(values[0]))\r\n    737 \r\n    738 def _read_array(group, key):\r\n\r\nValueError: unrecognized index type <type \'tuple\'>\r\n```\r\n'"
491,2560859,craustin,wesm,2011-12-14 23:13:22,2013-12-04 00:43:45,2011-12-15 04:05:07,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/491,b'Cannot instantiate DataFrame with',"b'from pandas import DataFrame\r\nDataFrame([],[]) # DataFrame([]) does the same thing\r\n\r\npandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\r\n    208                                          copy=copy)\r\n    209         elif isinstance(data, list):\r\n--> 210             if isinstance(data[0], (list, tuple)):\r\n    211                 data, columns = _list_to_sdict(data, columns)\r\n    212                 mgr = self._init_dict(data, index, columns, dtype=dtype)\r\n\r\nIndexError: list index out of range'"
490,2557160,nbecker,wesm,2011-12-14 19:49:16,2011-12-15 20:13:46,2011-12-15 20:13:46,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/490,"b""can't print embedded ndarray""","b'Just playing with pandas.  I was using structured array.  I tried converting my structured array to pandas.  It was OK, until I tried with a field that was itself a (variable length) ndarray.\r\n\r\narr = np.empty ((len(results),), dtype=[(... various fields ommitted,\r\n\r\n\t\t\t\t\t(\'err\', object),\r\n                                        (\'bursts\', int),\r\n\t\t\t\t\t(\'per\', float), ...\r\n\t\t\t\t\t])\r\n\r\nNext the array is filled with data.\r\nThe field \'err\' is then filled in with a ndarray (vector) whose size is variable per instance.\r\n\r\nNow convert to df:\r\n\r\nfrom pandas import *\r\ndf = DataFrame (arr)\r\n\r\nAny attempt to print anything with \'err\' field gives an exception\r\n\r\n/home/nbecker/hn-8psk/<ipython-input-81-68bace0c3dbe> in <module>()\r\n----> 1 df[\'err\']\r\n\r\n/usr/lib/python2.7/site-packages/IPython/core/displayhook.pyc in __call__(self, result)\r\n    300             self.start_displayhook()\r\n    301             self.write_output_prompt()\r\n--> 302             format_dict = self.compute_format_data(result)\r\n    303             self.write_format_data(format_dict)\r\n    304             self.update_user_ns(result)\r\n\r\n/usr/lib/python2.7/site-packages/IPython/core/displayhook.pyc in compute_format_data(self, result)\r\n    213             MIME type representation of the object.\r\n    214         """"""\r\n--> 215         return self.shell.display_formatter.format(result)\r\n    216 \r\n    217     def write_format_data(self, format_dict):\r\n\r\n/usr/lib/python2.7/site-packages/IPython/core/formatters.pyc in format(self, obj, include, exclude)\r\n    120                     continue\r\n    121             try:\r\n--> 122                 data = formatter(obj)\r\n    123             except:\r\n    124                 # FIXME: log the exception\r\n\r\n\r\n/usr/lib/python2.7/site-packages/IPython/core/formatters.pyc in __call__(self, obj)\r\n    441                 type_pprinters=self.type_printers,\r\n    442                 deferred_pprinters=self.deferred_printers)\r\n--> 443             printer.pretty(obj)\r\n    444             printer.flush()\r\n    445             return stream.getvalue()\r\n\r\n/usr/lib/python2.7/site-packages/IPython/lib/pretty.pyc in pretty(self, obj)\r\n    349             if hasattr(obj_class, \'_repr_pretty_\'):\r\n    350                 return obj_class._repr_pretty_(obj, self, cycle)\r\n--> 351             return _default_pprint(obj, self, cycle)\r\n    352         finally:\r\n    353             self.end_group()\r\n\r\n/usr/lib/python2.7/site-packages/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)\r\n    467     if getattr(klass, \'__repr__\', None) not in _baseclass_reprs:\r\n    468         # A user-provided repr.\r\n\r\n--> 469         p.text(repr(obj))\r\n    470         return\r\n    471     p.begin_group(1, \'<\')\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.pyc in __repr__(self)\r\n    435             result = self._get_repr(print_header=True,\r\n    436                                     length=len(self) > 50,\r\n--> 437                                     name=True)\r\n    438         else:\r\n    439             result = \'%s\' % ndarray.__repr__(self)\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.pyc in _get_repr(self, name, print_header, length, na_rep, float_format)\r\n    494             return \'%s    %s\' % (str(k).ljust(padSpace), v)\r\n    495 \r\n--> 496         it = [_format(idx, v) for idx, v in izip(string_index, vals)]\r\n    497 \r\n    498         if print_header and have_header:\r\n\r\n/usr/lib64/python2.7/site-packages/pandas/core/series.pyc in _format(k, v)\r\n    488 \r\n    489         def _format(k, v):\r\n--> 490             if isnull(v):\r\n    491                 v = na_rep\r\n    492             if isinstance(v, (float, np.floating)):\r\n\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\nIn [82]: \r\n'"
489,2556418,craustin,wesm,2011-12-14 18:59:52,2011-12-15 14:03:33,2011-12-15 04:40:17,closed,,0.7.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/489,b'Series.to_string Always Prints Length',"b'We like to use to_string to print the contents of a Series to a buffer.  As of 0.6.1, this has started including ""\\nLength:..."" at the end.  Could this be an option?  It leaves us with no option but to rewrite to_string if we don\'t want this included.'"
488,2556375,craustin,wesm,2011-12-14 18:56:00,2013-12-04 00:43:45,2011-12-15 17:38:22,closed,,0.7.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/488,b'Exception calling to_string on empty Series',"b'from pandas import Series\r\ns = Series({})\r\ns.to_string()\r\n\r\npandas\\core\\series.pyc in to_string(self, buf,\r\nna_rep, float_format, nanRep)\r\n    460             na_rep = nanRep\r\n    461\r\n--> 462         the_repr = self._get_repr(float_format=float_format, na_rep=na_rep)\r\n    463         if buf is None:\r\n    464             return the_repr\r\n\r\npandas\\core\\series.pyc in _get_repr(self, name, print_header, length, na_rep, float_format)\r\n    481             string_index = index.format()\r\n    482\r\n--> 483         maxlen = max(len(x) for x in string_index)\r\n    484         padSpace = min(maxlen, 60)\r\n    485\r\n\r\nValueError: max() arg is an empty sequence'"
487,2554685,creeson,wesm,2011-12-14 16:48:19,2011-12-15 04:23:28,2011-12-15 04:23:28,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/487,b'nanops on datetime objects failing',"b'Hi,\r\n\r\nIt looks like apply_along_axis does not play well with objects that have no length? If I go comment out the numpy 1.6.1 workaround in Python 3.x portion, this goes back to working fine.\r\n\r\nThanks!\r\n\r\nFor example:\r\n>>> import pandas as ps\r\n>>> x = ps.Series([ps.datetime(2011,4,3)])\r\n>>> x.min()\r\n\r\nTraceback (most recent call last):\r\n  File ""<pyshell#3>"", line 1, in <module>\r\n    x.min()\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 722, in min\r\n    return nanops.nanmin(self.values, skipna=skipna, copy=True)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\nanops.py"", line 118, in nanmin\r\n    result = np.apply_along_axis(__builtin__.min, apply_ax, values)\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\lib\\shape_base.py"", line 104, in apply_along_axis\r\n    outshape[axis] = len(res)\r\nTypeError: object of type \'datetime.datetime\' has no len()'"
486,2549380,CRP,wesm,2011-12-14 09:48:57,2011-12-15 17:22:57,2011-12-15 17:22:57,closed,,0.7.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/486,b'exception joining dataframes with different datatypes',"b""In [71]: a=DataFrame(randn(10,2),columns=['a','b'])\r\n\r\nIn [72]: b=DataFrame(randn(10,1),columns=['c']).astype(np.float32)\r\n\r\nIn [73]: a.join(b)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/Users/c.prinoth/<ipython-input-73-3163f462d6f9> in <module>()\r\n----> 1 a.join(b)\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/frame.pyc in join(self, other, on, how, lsuffix, rsuffix)\r\n   2614             return self._join_on(other, on, how, lsuffix, rsuffix)\r\n   2615         else:\r\n-> 2616             return self._join_index(other, how, lsuffix, rsuffix)\r\n   2617 \r\n   2618     def _join_on(self, other, on, how, lsuffix, rsuffix):\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/frame.pyc in _join_index(self, other, how, lsuffix, rsuffix)\r\n   2649 \r\n   2650         # this will always ensure copied data\r\n\r\n-> 2651         merged_data = join_managers(thisdata, otherdata, axis=1, how=how)\r\n   2652         return self._constructor(merged_data)\r\n   2653 \r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/internals.pyc in join_managers(left, right, axis, how, copy)\r\n   1032 def join_managers(left, right, axis=1, how='left', copy=True):\r\n   1033     op = _JoinOperation(left, right, axis=axis, how=how)\r\n-> 1034     return op.get_result(copy=copy)\r\n   1035 \r\n   1036 class _JoinOperation(object):\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/internals.pyc in get_result(self, copy)\r\n   1090             if lblk and rblk:\r\n   1091                 # true merge, do not produce intermediate copy\r\n\r\n-> 1092                 res_blk = self._merge_blocks(lblk, rblk)\r\n   1093             elif lblk:\r\n   1094                 res_blk = self._reindex_block(lblk, side='left')\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/internals.pyc in _merge_blocks(self, lblk, rblk)\r\n   1206         else:\r\n   1207             common.take_fast(rblk.values, ridx, None, False,\r\n-> 1208                              axis=self.axis, out=out[lk:])\r\n   1209 \r\n   1210         # does not sort\r\n\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/common.pyc in take_fast(arr, indexer, mask, needs_masking, axis, out)\r\n    247         return take_2d(arr, indexer, out=out, mask=mask,\r\n    248                        needs_masking=needs_masking,\r\n--> 249                        axis=axis)\r\n    250 \r\n    251     result = arr.take(indexer, axis=axis, out=out)\r\n\r\n/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pandas/core/common.pyc in take_2d(arr, indexer, out, mask, needs_masking, axis)\r\n    232             needs_masking = mask.any()\r\n    233 \r\n--> 234         result = arr.take(indexer, axis=axis, out=out)\r\n    235         result = _maybe_mask(result, mask, needs_masking, axis=axis,\r\n    236                              out_passed=out is not None)\r\n\r\nTypeError: array cannot be safely cast to required type\r\n\r\n\r\nIn [77]: a.dtypes\r\nOut[77]: \r\na    float64\r\nb    float64\r\n\r\nIn [78]: b.dtypes\r\nOut[78]: c    float32\r\n"""
484,2540806,wesm,wesm,2011-12-13 18:08:37,2011-12-13 19:36:02,2011-12-13 19:36:02,closed,,0.6.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/484,b'Type inference with list of lists in DataFrame constructor',"b'per @aristotle137\r\n\r\n```\r\nIn [1216]: l = [[1, \'a\'], [2, \'b\']]\r\n     ...: df = DataFrame(data = l, columns = [""num"", ""str""])\r\n     ...:\r\n\r\nIn [1217]: df.dtypes\r\nOut[1217]: num    object\r\n               str    object\r\n```'"
481,2531946,wesm,wesm,2011-12-13 02:11:04,2011-12-13 20:00:31,2011-12-13 20:00:31,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/481,b'Bug: handle MultiIndex names in concatenation step of GroupBy.apply',
477,2529890,wesm,wesm,2011-12-12 22:56:57,2012-01-09 22:40:39,2012-01-09 22:40:39,closed,adamklein,0.7.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/477,b'Enable unstack to create Series (currently raises Exception)',
476,2529145,wesm,wesm,2011-12-12 21:56:47,2011-12-29 09:02:10,2011-12-29 03:21:12,closed,,0.7.0,2,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/476,b'DataFrame.delevel improvements',"b'from @lodagro\r\n\r\n```\r\nConcerning the method name, DataFrame.delevel() is maybe too close to a MultiIndex. \r\nDataFrame.deindex() is more general, both are fine for me. And i have no problem to change either.\r\n\r\nOne little remark, for a MultiIndex default column names are inserted when needed, whereas if no MultiIndex is used, and no name - an exception is raised. Would it not be cleaner to do the same for both, either raise an exception if no names are set or enter default column names?\r\n```'"
475,2528289,wesm,wesm,2011-12-12 20:45:34,2011-12-12 20:59:22,2011-12-12 20:59:22,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/475,b'DataFrame constructor bug from single Series with different Index',"b'reported by a user\r\n\r\n```\r\n\r\nIn [7]: DataFrame(Series(np.arange(10)), index=range(5))\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n/home/wesm/code/pandas/<ipython-input-7-0a85de6b44f0> in <module>()\r\n----> 1 DataFrame(Series(np.arange(10)), index=range(5))\r\n\r\n/home/wesm/code/pandas/pandas/core/frame.pyc in __init__(self, data, index, columns, dtype, copy)\r\n    206             else:\r\n    207                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n--> 208                                          copy=copy)\r\n    209         elif isinstance(data, list):                                                     \r\n    210             mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n\r\n/home/wesm/code/pandas/pandas/core/frame.pyc in _init_ndarray(self, values, index, columns, dtype, copy)                                                                                          \r\n    287         columns = _ensure_index(columns)\r\n    288         block = make_block(values.T, columns, columns)\r\n--> 289         return BlockManager([block], [columns, index])\r\n    290 \r\n    291     def _wrap_array(self, arr, axes, copy=False):\r\n\r\n/home/wesm/code/pandas/pandas/core/internals.pyc in __init__(self, blocks, axes, do_integrity_check)                                                                                              \r\n    274 \r\n    275         if do_integrity_check:\r\n--> 276             self._verify_integrity()\r\n    277 \r\n    278     @property\r\n\r\n/home/wesm/code/pandas/pandas/core/internals.pyc in _verify_integrity(self)\r\n    353         mgr_shape = self.shape\r\n    354         for block in self.blocks:\r\n--> 355             assert(block.values.shape[1:] == mgr_shape[1:])\r\n    356         tot_items = sum(len(x.items) for x in self.blocks)\r\n    357         assert(len(self.items) == tot_items)\r\n\r\nAssertionError: \r\n```'"
473,2527156,wesm,wesm,2011-12-12 19:14:18,2011-12-12 19:56:19,2011-12-12 19:56:19,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/473,b'Issue modifying data in object arrays in DataFrame',"b""reported by user\r\n\r\n```\r\nimport pandas\r\npandas\r\nimport numpy as np\r\nfrom pandas import DataMatrix\r\nY = DataMatrix( np.random.random( (4,4) ) , index = ('a','b','c','d') , columns = ('e','f','g','h' ))\r\nY\r\nY['e'] = Y['e'].astype('object')\r\nY['g']['c'] = np.NaN\r\nY\r\nY.sum()\r\nY['g'].sum()\r\n```\r\n\r\nAppears to be caused by a stale reference to a block"""
469,2518357,wesm,wesm,2011-12-11 23:47:42,2011-12-12 00:52:38,2011-12-12 00:52:25,closed,,0.6.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/469,b'Need to exclude NA values with dtype=object in stat methods',"b'reported by @npinger, regression from 0.5.0 I think. \r\n\r\nimport pandas\r\nlis = [1, 3, 5, None, 7]\r\nindex = [1, 2, 3, 4, 5]\r\nser = pandas.core.series.Series(data=lis, index=index)\r\nser.mean()\r\n\r\nGives this error:\r\n\r\nTraceback (most recent call last):\r\n File ""<console>"", line 1, in <module>\r\n File "".../pandas/core/series.py"", line 650, in mean\r\n   return nanops.nanmean(self.values, skipna=skipna)\r\n File "".../pandas/core/nanops.py"", line 30, in nanmean\r\n   the_mean = values.sum(axis) / float(values.shape[axis])\r\nTypeError: unsupported operand type(s) for +: \'int\' and \'NoneType\'\r\n\r\nimport pandas\r\nimport numpy as np\r\nlis = [1, 3, 5, np.nan, 7]\r\nindex =  [1, 2, 3, 4, 5]\r\nser = pandas.core.series.Series(data=lis, index=index)\r\nser.mean()\r\n(gives no error)\r\n'"
467,2508486,wesm,wesm,2011-12-09 23:30:29,2011-12-10 20:26:45,2011-12-10 20:26:45,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/467,b'Memory leak in 0.6.0',b'The following code results in huge memory usage. Reported by a user\r\n\r\n```\r\ndf = DataFrame(index=np.arange(175))\r\nfor i in range(8000):\r\n    df[i] = 5\r\n```'
465,2500548,lodagro,wesm,2011-12-09 11:13:43,2013-12-04 00:48:07,2011-12-12 02:58:34,closed,,0.6.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/465,b'DataFrame.applymap(func) issue when func returns tuple',"b""```python\r\nIn [82]: def func_tuple(x):\r\n   ....:     return (x,x)\r\n   ....:\r\n\r\nIn [83]: series = pandas.Series(['a', 'b', 'c', 'd'])\r\n\r\nIn [84]: df = pandas.DataFrame({'A': series})\r\n\r\nIn [85]: df\r\nOut[85]:\r\n   A\r\n0  a\r\n1  b\r\n2  c\r\n3  d\r\n\r\nIn [86]: df.applymap(func_tuple)\r\nOut[86]:\r\n   A\r\n0  None\r\n1  None\r\n2  None\r\n3  None\r\n```\r\n\r\nThis works fine\r\n\r\n```python\r\nIn [87]: map(func_tuple, series)\r\nOut[87]: [('a', 'a'), ('b', 'b'), ('c', 'c'), ('d', 'd')]\r\n```\r\n\r\nLet`s try with a list:\r\n\r\n```python\r\nIn [88]: def func_list(x):\r\n   ....:     return [x,x]\r\n\r\nIn [89]: df.applymap(func_list)\r\nOut[89]:\r\n   A\r\n0  ['a', 'a']\r\n1  ['b', 'b']\r\n2  ['c', 'c']\r\n3  ['d', 'd']\r\n```\r\n\r\nHad a look at the pandas.DataFrame.applymap() code and this behavior is related to numpy.\r\nI`m using numpy 1.5.1, i know that for other reason numpy 1.6.1 is preferred. Would more recent numpy version fix this?\r\nI tried to install more recent version of numpy, but so far unsuccessful (various compilation errors).\r\n\r\n```python\r\nIn [103]: np.frompyfunc(func_tuple, 1, 1)(series)\r\nOut[103]:\r\n0    NaN\r\n1    NaN\r\n2    NaN\r\n3    NaN\r\n\r\nIn [104]: np.frompyfunc(func_list, 1, 1)(series)\r\nOut[104]:\r\n0    ['a', 'a']\r\n1    ['b', 'b']\r\n2    ['c', 'c']\r\n3    ['d', 'd']\r\n```\r\n\r\n"""
456,2466718,wesm,wesm,2011-12-06 20:04:32,2011-12-08 03:48:17,2011-12-08 03:48:17,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/456,b'DateRange union bug caused by bad DateOffset comparison',"b""Reported by user\r\n\r\n```\r\nIn [35]: rng1 = DateRange('12/5/2011', '12/5/2011')\r\n\r\nIn [36]: rng2 = DateRange('12/2/2011', '12/5/2011')\r\n\r\nIn [37]: rng1.union(rng2)\r\nOut[37]: \r\n<class 'pandas.core.daterange.DateRange'>\r\noffset: <1 BusinessDay>, tzinfo: None\r\n[2011-12-02 00:00:00, ..., 2011-12-05 00:00:00]\r\nlength: 2\r\n\r\nIn [38]: rng2.offset = datetools.BDay()\r\n\r\nIn [39]: rng1.union(rng2)\r\nOut[39]: Index([2011-12-02 00:00:00, 2011-12-05 00:00:00], dtype=object)\r\n```"""
454,2464176,andreas-h,wesm,2011-12-06 16:30:34,2011-12-12 04:09:23,2011-12-12 04:09:23,closed,,0.6.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/454,b'Saving Panel with float index to HDFStore fails',"b'Hi,\r\n\r\nthe following code produces a `ValueError: unrecognized index type <type \'float\'>`:\r\n\r\n    import datetime\r\n    import pandas\r\n    timeidx = pandas.DateRange(start=datetime.datetime(2009,1,1),\r\n                       end=datetime.datetime(2009,12,31),\r\n                       offset=pandas.datetools.MonthEnd())\r\n    lons_coarse = np.linspace(-177.5, 177.5, 72)\r\n    lats_coarse = np.linspace(-87.5, 87.5, 36)\r\n    P = pandas.Panel(items=timeidx, major_axis=lons_coarse, minor_axis=lats_coarse)\r\n    D = pandas.DataFrame(data=np.random.randn(72*36).reshape((72,36)),index=lons_coarse, columns=lats_coarse)\r\n    P[datetime.datetime(2009,2,28)] = D\r\n    H = pandas.HDFStore(""test.h5"", complevel=9, complib=\'zlib\')\r\n    H[\'test\'] = P\r\n    H.close()\r\n'"
452,2463416,andreas-h,wesm,2011-12-06 15:32:31,2011-12-08 19:50:19,2011-12-08 19:50:19,closed,,0.6.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/452,"b'UnboundLocalError when setting data to a Panel: ""local variable \'mat\' referenced before assignment""'","b""Hi,\r\n\r\nthe following code produces a UnboundLocalError when executed with pandas 0.6.0 / Python 2.7:\r\n\r\n    import datetime\r\n    import pandas\r\n    timeidx = pandas.DateRange(start=datetime.datetime(2009,1,1),\r\n                           end=datetime.datetime(2009,12,31),\r\n                           offset=pandas.datetools.MonthEnd())\r\n    lons_coarse = np.linspace(-177.5, 177.5, 72)\r\n    lats_coarse = np.linspace(-87.5, 87.5, 36)\r\n    P = pandas.Panel(items=timeidx, major_axis=lons_coarse, minor_axis=lats_coarse)\r\n    P[datetime.datetime(2009,2,28)] = np.random.randn(72*36).reshape((72,36))\r\n\r\nMaybe this is to be expected, but it's definitely not intuitive. Since both `P[datetime.datetime(2009,2,28)]` and `np.random.randn(72*36).reshape((72,36))` have shape `(72,36)`, I would expect this to work.\r\n\r\nUntil it does, what would be the way to do this?\r\n\r\n\r\n\r\n"""
451,2462901,lodagro,wesm,2011-12-06 14:52:41,2011-12-08 05:06:54,2011-12-08 05:06:53,closed,,0.6.1,1,Bug,https://api.github.com/repos/pydata/pandas/issues/451,b'DataFrame.unstack() issue when unstacking multiple levels.',"b""Let`s create a dataframe first.\r\n\r\n```python\r\nIn [170]: import pandas\r\n\r\nIn [171]: import numpy as np\r\n\r\nIn [172]: import itertools\r\n\r\nIn [173]: index = pandas.MultiIndex.from_tuples([t for t in itertools.product(['foo', 'bar'], [0, 1] , ['a', 'b'])])\r\n\r\nIn [174]: df = pandas.DataFrame(np.random.randn(8,2), columns=['A', 'B'], index=index)\r\n\r\nIn [175]: df\r\nOut[175]:\r\n         A         B\r\nfoo 0 a -1.627     1.301\r\n      b -494.049m  120.371m\r\nfoo 1 a  955.212m  580.450m\r\n      b -55.806m   169.247m\r\nbar 0 a  266.666m  246.100m\r\n      b  806.406m  894.707m\r\nbar 1 a -1.319     475.675m\r\n      b  638.371m  409.483m\r\n```\r\n\r\nUnstacking a single level works fine.\r\n\r\n```python\r\nIn [176]: df.unstack(level=[1])\r\nOut[176]:\r\n       A                   B\r\n       0         1         0         1\r\nbar a  266.666m -1.319     246.100m  475.675m\r\n    b  806.406m  638.371m  894.707m  409.483m\r\nfoo a -1.627     955.212m  1.301     580.450m\r\n    b -494.049m -55.806m   120.371m  169.247m\r\n```\r\n\r\nTrying to unstack multiple level does not work as expected.\r\nLooks like when handling multiple levels the unstack are applied consecutively, ok for the first unstack, but following ones should have level adapted internally, which does not seem to happen.\r\nFirst example tries to unstack level 0 and 1 but level 0 and 2 get unstacked.\r\nSecond example, idea is to unstack level 1 and 2 but this gives an error, since after level 1 is unstacked, there is no level 2 any more this became level 1 - see also last example.\r\n\r\n```python\r\nIn [180]: df.unstack(level=[0,1])\r\nOut[180]:\r\n   A                   A                   B                   B\r\n   bar                 foo                 bar                 foo\r\n   a         b         a         b         a         b         a         b\r\n0  266.666m  806.406m -1.627    -494.049m  246.100m  894.707m  1.301     120.371m\r\n1 -1.319     638.371m  955.212m -55.806m   475.675m  409.483m  580.450m  169.247m\r\n\r\n\r\nIn [177]: df.unstack(level=[1,2])\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n...\r\nIndexError: pop index out of range\r\n\r\nIn [178]: df.unstack(level=[1,1])\r\nOut[178]:\r\n     A                   A                   B                   B\r\n     0                   1                   0                   1\r\n     a         b         a         b         a         b         a         b\r\nbar  266.666m  806.406m -1.319     638.371m  246.100m  894.707m  475.675m  409.483m\r\nfoo -1.627    -494.049m  955.212m -55.806m   1.301     120.371m  580.450m  169.247m\r\n```"""
434,2432464,wesm,wesm,2011-12-02 20:09:31,2011-12-02 20:23:30,2011-12-02 20:23:30,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/434,b'Empty series corner case arith bug',"b""This works in 0.4 and crashes in 0.6:\r\n\r\n``` \r\nimport numpy as np\r\nfrom pandas import Series\r\ns1 = Series([], [], dtype=np.int32)\r\ns2 = Series({'x' : 0.})\r\n \r\nprint s1 * s2\r\n```"""
432,2430095,wesm,wesm,2011-12-02 16:43:57,2011-12-02 20:46:17,2011-12-02 20:46:17,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/432,b'Element-wise map does not produce DataFrame in apply',"b'```\r\ncompareOp = lambda s1, s2 : map(lambda x, y: x if y is None else (y if x is None else min(x, y)), s1, s2)\r\n\r\nprint m2.apply(lambda x : compareOp(x, s))\r\n```'"
431,2429764,wesm,wesm,2011-12-02 16:17:55,2011-12-02 21:07:30,2011-12-02 21:02:23,closed,,0.6.1,2,Bug,https://api.github.com/repos/pydata/pandas/issues/431,"b'.ix[row, col] should preserve type'","b'In the case of float/int data, ints currently getting upcasted to float. @lodagro, @sjev'"
423,2373028,wesm,wesm,2011-11-28 20:31:16,2011-11-29 00:10:45,2011-11-29 00:10:45,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/423,b'DataFrame.count corner case with empty columns',b'Consider changing behavior of:\r\n\r\n`DataMatrix(index=range(10)).count(1)`\r\n\r\nreturning zeros should not be very harmful. '
421,2366051,wesm,wesm,2011-11-28 10:45:38,2011-12-02 22:01:06,2011-12-02 22:01:06,closed,,0.6.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/421,b'Buglet in groupby when passing function like len with as_index False',"b'```\r\nimport numpy as np\r\nimport pandas\r\n\r\ndef webuse(data, baseurl=\'http://www.stata-press.com/data/r11/\'):\r\n    """"""\r\n    Parameters\r\n    ----------\r\n    data : str\r\n        Name of dataset to fetch.\r\n\r\n    Examples\r\n    --------\r\n    >>> dta = webuse(\'auto\')\r\n\r\n    Notes\r\n    -----\r\n    Make sure baseurl has trailing forward slash. Doesn\'t do any \r\n    error checking in response URLs.\r\n    """"""\r\n    # lazy imports\r\n    from scikits.statsmodels.iolib import genfromdta\r\n    from urllib2 import urlopen\r\n    from urlparse import urljoin\r\n    from StringIO import StringIO\r\n\r\n    url = urljoin(baseurl, data+\'.dta\')\r\n    dta = urlopen(url)\r\n    dta = StringIO(dta.read()) # make it truly file-like\r\n    return genfromdta(dta)\r\n\r\ndta = webuse(\'auto\')\r\ndf = pandas.DataFrame.from_records(dta)\r\n```\r\n\r\nthen \r\n\r\n`df.groupby([\'foreign\', \'rep78\'], as_index=False)[\'mpg\'].agg(len)`\r\n\r\nraises an exception'"
419,2362969,algotr8der,wesm,2011-11-28 00:25:18,2013-12-04 00:43:49,2011-12-02 21:03:13,closed,,0.6.1,4,Bug,https://api.github.com/repos/pydata/pandas/issues/419,b'DataReader - request to truncate retrieved data to specified dates',"b'Pandas version 0.6.0\r\n\r\n\r\nfrom pandas.io.data import DataReader\r\nimport datetime\r\nimport matplotlib.pyplot as plt\r\n\r\nsp500 = DataReader(""SPY"", ""yahoo"", start=datetime.datetime(2005, 1,1))\r\nsp500.info()\r\nsp500[""Adj Close""].plot()\r\nplt.show()\r\n\r\n<class \'pandas.core.frame.DataFrame\'>\r\nIndex: 4744 entries, 1993-01-29 00:00:00 to 2011-11-25 00:00:00\r\nData columns:\r\nOpen         4744  non-null values\r\nHigh         4744  non-null values\r\nLow          4744  non-null values\r\nClose        4744  non-null values\r\nVolume       4744  non-null values\r\nAdj Close    4744  non-null values\r\ndtypes: int64(1), float64(5)\r\n\r\nAs you can see the index begins at 1993-01-29 despite a user specified start date of 2005-1-1. You can specify any ticker and the start date specified in the DataReader function seems to be ignored.\r\n\r\nComment from Wes below:\r\n\r\nThe Yahoo API is very unreliable in my experience-- the code currently returns whatever data is returned by the query, so something must have changed recently there. I guess it should truncate to the passed date\r\nrange regardless of what is returned.\r\n\r\n'"
416,2351937,wesm,wesm,2011-11-25 20:06:03,2011-11-26 03:09:18,2011-11-26 03:09:18,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/416,b'GroupBy level should pass on level name to result index',
412,2343842,wesm,wesm,2011-11-24 17:58:54,2011-12-03 17:51:16,2011-12-03 17:51:12,closed,,0.6.1,2,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/412,b'Override Index.astype to prevent bad things from happening',"b""For example, this should be possible: `index.astype('i8')`"""
411,2340150,CRP,wesm,2011-11-24 11:27:35,2011-11-25 05:50:26,2011-11-25 05:50:26,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/411,b'Panel constructor ignores dtype',"b'In [11]: Panel(items=arange(3),major_axis=arange(3),minor_axis=arange(3),dtype=np.object).ix[1].dtypes\r\nOut[11]: \r\n0    float64\r\n1    float64\r\n2    float64\r\n'"
409,2336706,wesm,wesm,2011-11-24 01:52:48,2012-04-11 03:21:36,2012-04-11 03:21:30,closed,,0.7.3,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/409,"b""A lot of partial setting stuff doesn't work with .ix, needs testing/work""",
408,2336417,wesm,wesm,2011-11-24 00:53:07,2011-11-24 02:12:48,2011-11-24 02:12:48,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/408,b'Index name is not properly pickled/unpickled',
405,2331232,wesm,wesm,2011-11-23 16:11:30,2011-11-23 17:07:42,2011-11-23 17:07:42,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/405,b'GroupBy does not handle non-string column labels passed',b'cc @dhrod5'
403,2320152,jseabold,wesm,2011-11-22 17:07:04,2013-07-29 11:43:14,2011-11-23 06:12:46,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/403,b'Unstack with an object variable overwrites BlockManager',"b'Consider the (long) code snippet here. Unless you remove ccode from the df beforehand, you end up with all object dtype for df after unstack.\r\n\r\nhttp://www.pastie.org/2904328'"
397,2314538,wesm,wesm,2011-11-22 05:00:20,2011-11-23 03:40:24,2011-11-23 03:40:24,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/397,b'Partial setting (with more than 1 level specified) with .ix raises exception',
394,2308025,wesm,wesm,2011-11-21 18:56:54,2011-11-22 05:50:21,2011-11-22 05:50:21,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/394,b'.ix[val] discards index names',
390,2283732,wesm,wesm,2011-11-18 19:38:38,2011-11-22 05:14:27,2011-11-22 05:14:27,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/390,b'Result of asfreq should have DateRange index',b'fyi @craustin'
389,2283388,craustin,wesm,2011-11-18 18:59:34,2011-11-22 18:38:03,2011-11-22 18:38:03,closed,,0.6.0,5,Bug,https://api.github.com/repos/pydata/pandas/issues/389,b'DataFrame.apply returns Series',"b""from pandas import DataFrame, DateRange\r\ndr = DateRange('2011-11-01','2011-11-15')\r\ndf = DataFrame(index=dr)\r\ndf_or_so_youd_think = df.apply(lambda x:x)\r\nprint df_or_so_youd_think\r\n\r\nThis returns an empty Series."""
384,2277604,wesm,wesm,2011-11-18 07:33:01,2011-11-23 03:40:33,2011-11-23 03:40:33,closed,,0.6.0,0,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/384,b'Test all NA case in min/max functions',
382,2275467,wesm,wesm,2011-11-18 00:07:02,2011-11-23 03:41:21,2011-11-23 03:41:21,closed,,0.6.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/382,b'DataFrame should be more aggressive about trying to aggregate numeric data stored in dtype object',"b""```\r\ndata = dict()\r\ndata['a'] =     [-0.00049987540199591344, -0.0016467257772919831, 0.00067695870775883013]\r\ndata['b'] =     [-0, -0, 0.0]\r\ndata['c'] =     [0.00031111847529610595, 0.0014902627951905339, -0.00094099200035979691]\r\nAA = DataFrame( data , index = ['20110801', '20110802', '20110803'] , dtype = \xa1\xaeobject\xa1\xaf )\r\n```"""
379,2274311,wesm,wesm,2011-11-17 22:03:21,2011-11-22 05:38:32,2011-11-22 05:38:32,closed,,0.6.0,0,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/379,b'Allow user to pass level names to swaplevel',
378,2272263,craustin,wesm,2011-11-17 18:52:42,2011-11-18 06:48:05,2011-11-18 06:48:05,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/378,b'Exception Upon Pivoting an Empty DataFrame',"b'from pandas import DataFrame\r\ndf = DataFrame({},columns=[\'time\',\'item\',\'value\'])\r\ndf.pivot(\'time\',\'item\',\'value\')\r\n\r\npandas\\core\\frame.pyc in pivot(self, index, columns, values)\r\n   1953         """"""\r\n   1954         from pandas.core.reshape import pivot\r\n-> 1955         return pivot(self, index=index, columns=columns, values=values)\r\n   1956\r\n   1957     def stack(self, level=-1, dropna=True):\r\n\r\npandas\\core\\reshape.pyc in pivot(self, index, columns, values)\r\n    220         stacked = stacked.sortlevel(level=0)\r\n    221\r\n--> 222     unstacked = stacked.unstack()\r\n    223     if values is not None:\r\n    224         unstacked.columns = unstacked.columns.droplevel(0)\r\n\r\npandas\\core\\frame.pyc in unstack(self, level)\r\n   2017             return result\r\n   2018         else:\r\n-> 2019             return unstack(self, level)\r\n   2020\r\n   2021     def delevel(self):\r\n\r\npandas\\core\\reshape.pyc in unstack(obj, level)\r\n    289     unstacker = _Unstacker(obj.values, obj.index, level=level,\r\n    290                            value_columns=columns)\r\n--> 291     return unstacker.get_result()\r\n    292\r\n    293 def stack(frame, level=-1, dropna=True):\r\n\r\npandas\\core\\reshape.pyc in get_result(self)\r\n    120\r\n    121         # filter out missing levels\r\n\r\n--> 122         values = values[:, mask]\r\n    123         columns = columns[mask]\r\n    124\r\n\r\nIndexError: invalid index'"
377,2271402,CRP,wesm,2011-11-17 17:34:22,2011-11-17 17:57:48,2011-11-17 17:57:48,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/377,b'mixed type dataframe loses type info',"b""I have a situation where joining objects of different types creates object where info() and dtypes disagree, and generally all datatypes revert to object:\r\n\r\nIn [630]: a=DataFrame(['aa','bb','cc','dd','ee'],index=[1,2,3,4,5],columns=['a'])\r\n\r\nIn [631]: b=DataFrame(randn(5,1),index=[1,2,3,4,5],columns=['b'])\r\n\r\nIn [632]: a.dtypes\r\nOut[632]: a    object\r\n\r\nIn [633]: b.dtypes\r\nOut[633]: b    float64\r\n\r\nIn [636]: a.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 5 entries, 1 to 5\r\nData columns:\r\na    5  non-null values\r\ndtypes: object(1)\r\n\r\nIn [637]: b.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 5 entries, 1 to 5\r\nData columns:\r\nb    5  non-null values\r\ndtypes: float64(1)\r\n\r\nIn [638]: b['b']\r\nOut[638]: \r\n1    0.182455296775\r\n2    0.341883955748\r\n3    -1.47860506557\r\n4    0.227912752553\r\n5    -0.393452694936\r\nName: b\r\n\r\nIn [639]: b['b'].dtype\r\nOut[639]: dtype('float64')\r\n\r\nIn [640]: a['b']=b['b']\r\n\r\nIn [641]: a\r\nOut[641]: \r\n   a   b     \r\n1  aa  0.1825\r\n2  bb  0.3419\r\n3  cc -1.479 \r\n4  dd  0.2279\r\n5  ee -0.3935\r\n\r\nIn [642]: a.dtypes\r\nOut[642]: \r\na    object\r\nb    object\r\n\r\nIn [643]: a.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 5 entries, 1 to 5\r\nData columns:\r\na    5  non-null values\r\nb    5  non-null values\r\ndtypes: float64(1), object(1)\r\nIn [644]: \r\n"""
375,2268125,ukch,wesm,2011-11-17 12:21:36,2011-11-18 06:08:34,2011-11-18 06:08:34,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/375,b'Pivoting on multiple columns causes data loss',"b'Seen on HEAD. This problem is new since 0.5.0.\r\n\r\n    In [3]: data\r\n    Out[3]: \r\n       foo   bar   baz   spam   data\r\n    0  foo1  bar1  baz1  spam2  20  \r\n    1  foo1  bar2  baz1  spam3  30  \r\n    2  foo2  bar2  baz1  spam2  40  \r\n    3  foo1  bar1  baz2  spam1  50  \r\n    4  foo3  bar1  baz2  spam1  60  \r\n\r\n    In [4]: pandas.pivot_table(data, values=""data"", rows=[""foo"", ""bar""], cols=[""baz"", ""spam""])\r\n    Out[4]: \r\n      baz      baz1   baz2 \r\n      spam     spam1  spam1\r\n    foo  bar               \r\n    foo1 bar1  20     50   \r\n         bar2  30     NaN  \r\n    foo2 bar2  40     NaN  \r\n    foo3 bar1  NaN    60   \r\n\r\nAs you can see, the `(""baz1"", ""spam2"")`, `(""baz2"", ""spam2"")`, `(""baz1"", ""spam3"")` and `(""baz2"", ""spam3"")` columns have disappeared and their contents have been aggregated into the remaining two columns.'"
368,2249840,wesm,wesm,2011-11-15 21:56:19,2013-09-29 00:03:46,2011-11-23 17:41:20,closed,,0.6.0,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/368,b'DataFrame.boxplot fixes',b'per mailing list discussion @lodagro\r\n\r\n- Return ax or axes\r\n- Handling of [col] and multiple columns\r\n- Plot when only single column passed not correct'
367,2245578,CRP,wesm,2011-11-15 15:53:22,2011-11-16 08:50:45,2011-11-16 08:50:45,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/367,b'Exception Joining DataFrames and Series',"b""\r\nIn [23]: a=DataFrame(randn(5,2),index=[1,2,3,6,7],columns=['a','b'])\r\n\r\nIn [24]: b=Series([6,7,8,9],index=[0,1,2,3],name='c')\r\n\r\nIn [25]: c=Series([6,7,8,9],name='c')\r\n\r\nIn [26]: a\r\nOut[26]: \r\n   a       b     \r\n1 -0.268  -0.1305\r\n2  0.2243  0.4644\r\n3  0.4767 -0.5677\r\n6  0.1806  0.6276\r\n7  0.9767  0.3687\r\n\r\nIn [27]: b\r\nOut[27]: \r\n0    6\r\n1    7\r\n2    8\r\n3    9\r\nName: c\r\n\r\nIn [28]: c\r\nOut[28]: \r\n0    6\r\n1    7\r\n2    8\r\n3    9\r\nName: c\r\n\r\nIn [29]: a.join(b,how='outer')\r\nOut[29]: \r\n   a       b       c  \r\n0  NaN     NaN     6  \r\n1 -0.268  -0.1305  7  \r\n2  0.2243  0.4644  8  \r\n3  0.4767 -0.5677  9  \r\n6  0.1806  0.6276  NaN\r\n7  0.9767  0.3687  NaN\r\n\r\nIn [30]: a.join(c,how='outer')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/Users/c.prinoth/<ipython-input-30-ec7cd102d8a5> in <module>()\r\n----> 1 a.join(c,how='outer')\r\n\r\n/Library/Python/2.6/site-packages/pandas/core/frame.pyc in join(self, other, on, how, lsuffix, rsuffix)\r\n   2303             return self._join_on(other, on, how, lsuffix, rsuffix)\r\n   2304         else:\r\n-> 2305             return self._join_index(other, how, lsuffix, rsuffix)\r\n   2306 \r\n   2307     def _join_on(self, other, on, how, lsuffix, rsuffix):\r\n\r\n/Library/Python/2.6/site-packages/pandas/core/frame.pyc in _join_index(self, other, how, lsuffix, rsuffix)\r\n   2340 \r\n   2341         # this will always ensure copied data\r\n\r\n-> 2342         merged_data = join_managers(thisdata, otherdata, axis=1, how=how)\r\n   2343         return self._constructor(merged_data)\r\n   2344 \r\n\r\n/Library/Python/2.6/site-packages/pandas/core/internals.pyc in join_managers(left, right, axis, how, copy)\r\n    991 \r\n    992 def join_managers(left, right, axis=1, how='left', copy=True):\r\n--> 993     op = _JoinOperation(left, right, axis=axis, how=how)\r\n    994     return op.get_result(copy=copy)\r\n    995 \r\n\r\n/Library/Python/2.6/site-packages/pandas/core/internals.pyc in __init__(self, left, right, axis, how)\r\n   1015         (self.join_index,\r\n   1016          self.lindexer,\r\n-> 1017          self.rindexer) = laxis.join(raxis, how=how, return_indexers=True)\r\n   1018 \r\n   1019         # do NOT sort\r\n\r\n\r\n/Library/Python/2.6/site-packages/pandas/core/index.pyc in join(self, other, how, return_indexers)\r\n    491         if self.is_monotonic and other.is_monotonic:\r\n    492             return self._join_monotonic(other, how=how,\r\n--> 493                                         return_indexers=return_indexers)\r\n    494 \r\n    495         if how == 'left':\r\n\r\n/Library/Python/2.6/site-packages/pandas/core/index.pyc in _join_monotonic(self, other, how, return_indexers)\r\n    530             join_index = Index(join_index)\r\n    531         elif how == 'outer':\r\n--> 532             join_index, lidx, ridx = lib.outer_join_indexer_object(self, other)\r\n    533             join_index = Index(join_index)\r\n    534         else:  # pragma: no cover\r\n\r\n/Library/Python/2.6/site-packages/pandas/_tseries.so in pandas._tseries.outer_join_indexer_object (pandas/src/tseries.c:43677)()\r\n\r\nValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\r\n"""
366,2242259,ukch,wesm,2011-11-15 11:09:50,2011-11-16 08:35:13,2011-11-16 08:35:13,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/366,b'Traceback when calling df.as_matrix()',"b'I have a DataFrame object that was created by importing some data in list form, pivoting it, then adding totals columns using `df.append(pandas.DataFrame(some_matrix, columns=df.columns, index=[some_tuple])` and `df[some_other_tuple] = some_array` respectively. The resulting DataFrame object looks like this:\r\n\r\n    >>> df\r\n                 baz1                 baz2                 Totals\r\n                 spam1  spam2  spam3  spam1  spam2  spam3        \r\n    foo    bar                                                   \r\n    foo1   bar1  NaN    20     NaN    50     NaN    NaN    NaN   \r\n           bar2  NaN    NaN    30     NaN    NaN    NaN    NaN   \r\n    foo2   bar2  NaN    40     NaN    NaN    NaN    NaN    NaN   \r\n    foo3   bar1  NaN    NaN    NaN    60     NaN    NaN    NaN   \r\n    Totals       NaN    NaN    NaN    NaN    NaN    NaN    NaN   \r\n\r\nCalling `df.as_matrix()` fails with the below traceback:\r\n\r\n    >>> df.as_matrix()\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/frame.py"", line 671, in as_matrix\r\n        self._consolidate_inplace()\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/generic.py"", line 267, in _consolidate_inplace\r\n        self._data = self._data.consolidate()\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 516, in consolidate\r\n        new_blocks = _consolidate(self.blocks, self.items)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 938, in _consolidate\r\n        new_block = _merge_blocks(list(group_blocks), items)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 951, in _merge_blocks\r\n        do_integrity_check=True)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 211, in make_block\r\n        do_integrity_check=do_integrity_check)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 34, in __init__\r\n        self._check_integrity()\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 40, in _check_integrity\r\n        return (self.ref_locs[1:] > self.ref_locs[:-1]).all()\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/pandas/core/internals.py"", line 47, in ref_locs\r\n        assert((indexer != -1).all())\r\n    AssertionError\r\n'"
358,2203207,wesm,wesm,2011-11-10 21:50:56,2011-11-11 18:37:45,2011-11-11 18:37:45,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/358,b'Single column groupby does not set name',"b""```\r\nIn [2]: paste\r\ndf = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\r\n                                    'foo', 'bar', 'foo', 'foo'],\r\n                             'B' : ['one', 'one', 'two', 'three',\r\n                                    'two', 'two', 'one', 'three'],\r\n                             'C' : np.random.randn(8),\r\n                             'D' : np.random.randn(8)})\r\n## -- End pasted text --\r\n\r\nIn [3]: df\r\nOut[3]: \r\n   A    B      C        D     \r\n0  foo  one   -2.096    0.4961\r\n1  bar  one   -0.8738  -1.074 \r\n2  foo  two   -0.01376 -0.956 \r\n3  bar  three  0.996    0.368 \r\n4  foo  two   -1.396   -0.3736\r\n5  bar  two    0.7779   0.4091\r\n6  foo  one   -0.1221   1.26  \r\n7  foo  three -0.9718  -0.4734\r\n\r\nIn [4]: df.groupby('A').mean()\r\nOut[4]: \r\n     C       D      \r\nbar  0.3001 -0.09914\r\nfoo -0.9199 -0.00939\r\n\r\nIn [5]: df.groupby(['A', 'B']).mean()\r\nOut[5]: \r\n           C       D     \r\nA   B                    \r\nbar one   -0.8738 -1.074 \r\n    three  0.996   0.368 \r\n    two    0.7779  0.4091\r\nfoo one   -1.109   0.878 \r\n    three -0.9718 -0.4734\r\n    two   -0.7048 -0.6648\r\n```"""
353,2191784,wesm,wesm,2011-11-09 21:49:54,2011-11-10 02:09:06,2011-11-10 02:09:06,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/353,b'str + Series does not yield expected result',b'dispatch order is wrong'
351,2179642,rsamson,wesm,2011-11-08 21:34:02,2011-11-10 03:19:38,2011-11-10 03:17:05,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/351,b'.join works incorrectly with MultiIndex (data is not actually alligned)',"b""- When you use .join with a MultiIndex, it combines the DataFrames improperly.  \r\n- When you use .join(how='outer'), the index is destroyed (turned into integers).\r\n\r\nExample below.\r\nThanks,\r\nRyan\r\n\r\n\r\nindex1=MultiIndex.from_arrays([['a','a','a','b','b','b'], [1,2,3,1,2,3]])\r\n\r\nindex2=MultiIndex.from_arrays([['b','b','b','c','c','c'], [1,2,3,1,2,3]])\r\n\r\nindex1\r\n\r\nMultiIndex([('a', 1), ('a', 2), ('a', 3), ('b', 1), ('b', 2), ('b', 3)], dtype=object)\r\n\r\nindex2\r\n\r\nMultiIndex([('b', 1), ('b', 2), ('b', 3), ('c', 1), ('c', 2), ('c', 3)], dtype=object)\r\n\r\ndf1 = DataFrame(data=numpy.random.randn(6), index=index1, columns=['var X'])\r\n\r\ndf2 = DataFrame(data=numpy.random.randn(6), index=index2, columns=['var Y'])\r\n\r\ndf1=df1.sortlevel(0)\r\n\r\ndf2=df2.sortlevel(0)\r\n\r\ndf1\r\n\r\n         var X \r\na 1     0.8658\r\n   2    -0.9211\r\n   3    -0.6389\r\nb 1    -0.6716\r\n   2    -0.6799\r\n   3     1.98  \r\n\r\ndf2\r\n\r\n     var Y \r\nb 1 -0.6978\r\n  2 -0.6186\r\n  3  0.117 \r\nc 1 -0.4577\r\n  2  0.7812\r\n  3 -1.691 \r\n\r\ndf1.join(df2)  #(a1, a2, a3 should be Nan for 'var Y', the values shown under a1, a2, a3 should appear under b1, b2, b3)\r\n\r\n     var X   var Y \r\na 1  0.8658 -0.6978\r\n  2 -0.9211 -0.6186\r\n  3 -0.6389  0.117 \r\nb 1 -0.6716 -0.4577\r\n  2 -0.6799  0.7812\r\n  3  1.98   -1.691 \r\n\r\ndf1.join(df2, how='outer')  #The index is destroyed, the index is not expanded, and the data is incorrectly alligned\r\n\r\n   var X   var Y \r\n0  0.8658 -0.6978\r\n1 -0.9211 -0.6186\r\n2 -0.6389  0.117 \r\n3 -0.6716 -0.4577\r\n4 -0.6799  0.7812\r\n5  1.98   -1.691 \r\n"""
349,2171078,wesm,wesm,2011-11-08 04:57:10,2011-11-08 05:22:20,2011-11-08 05:22:20,closed,,0.6.0,0,Bug;Testing,https://api.github.com/repos/pydata/pandas/issues/349,b'index_col argument in read_csv may not respect order given',b'User reported on mailing list'
337,2150636,wesm,wesm,2011-11-05 03:59:53,2011-11-06 23:03:50,2011-11-06 23:03:50,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/337,b'DataFrame.min/max do not support mixed-type frames currently',
331,2145767,wesm,wesm,2011-11-04 16:36:19,2011-11-04 16:43:36,2011-11-04 16:43:36,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/331,b'DataFrame.join fails with unconsolidated inputs',"b""An oversight in the join refactoring recently\r\n\r\n```\r\ncreate two objects:\r\na=DataFrame(randn(30,2),columns=['a','b'])\r\nc=Series(randn(30))\r\n\r\nnow add c as column of a:\r\na['c']=c\r\n\r\ncreate a third object:\r\nd=DataFrame(randn(30,1),columns=['d'])\r\n\r\ntry to join with a:\r\na.join(d)\r\n```"""
329,2137852,carljv,wesm,2011-11-03 21:25:02,2011-11-05 01:08:44,2011-11-05 01:08:44,closed,,0.6.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/329,b'[Bug] pandas.io.data.DataReader Yahoo dates bug',"b""Yahoo finance seems to set its historical data URLs with the month variables set back minus one, and DataReader doesn't seem to accommodate this.\r\n\r\nFor example, running:\r\nticker = '^GSPC'\r\nstart_dt = datetime(2008, 6, 30)\r\nend_dt = datetime(2010, 12, 31)\r\ndata = DataReader(ticker, 'yahoo', start=start_dt, end=end_dt)\r\n\r\nThen:\r\n\r\nIn[27]: data.index[0]\r\nOut[27]: datetime.datetime(2008, 7, 30, 0, 0)\r\n\r\ndata.index[-1]\r\nOut[28]: datetime.datetime(2011, 11, 2, 0, 0)\r\n\r\nShows that the pull started in July, not June as intended, and ended on the latest available day instead of December 31, 2010.\r\n\r\nLooking at the source, DataReader looks like it constructs the following URL for the download:\r\nhttp://ichart.yahoo.com/table.csv?s=^GSPC&a=6&b=30&c=2008&d=12&e=31&f=2010&g=d&ignore=.csv\r\n\r\nIf I go get this data from Yahoo manually, I get:\r\nhttp://ichart.finance.yahoo.com/table.csv?s=^GSPC&a=05&b=30&c=2008&d=11&e=31&f=2010&g=d&ignore=.csv\r\n\r\n(Whether you use .finance or not doesn't seem to matter).\r\n\r\nThe hack to get around this isn't straightforward, since you have to pass a datetime to DataReader, and you can't create datetime(2010, 11, 31).  I'm assuming Yahoo is giving me all data to today because there is no month where d=12 in the URL (December is 11, January is 0).\r\n\r\n\r\n"""
328,2132016,mcobzarenco,wesm,2011-11-03 12:44:21,2012-01-07 01:29:06,2012-01-07 01:29:06,closed,,0.7.0,3,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/328,b'Fundamental issue with ix',"b'Using square bracket indexing [x] on a Series where x is an integer \r\nseems to return: \r\n - the item at the index x if such index is present \r\n - the x-th item if the index does not exist \r\nTo be precise: \r\nIn [190]: ss = pandas.Series(range(10), index = range(0, 20, 2)) \r\n     ...: print ss \r\n     ...: \r\n0     0 \r\n2     1 \r\n4     2 \r\n6     3 \r\n8     4 \r\n10    5 \r\n12    6 \r\n14    7 \r\n16    8 \r\n18    9 \r\nName: None, Length: 10 \r\nIn [191]: ss[0] \r\nOut[191]: 0 \r\nIn [192]: ss[1] \r\nOut[192]: 1 \r\nIn [193]: ss[2] \r\nOut[193]: 1 \r\nIn [194]: ss[3] \r\nOut[194]: 3 \r\nIn [195]: ss[4] \r\nOut[195]: 2 \r\nIn [196]: ss[17] \r\n\r\n--------------------------------------------------------------------------- \r\n\r\nKeyError                                  Traceback (most recent call \r\nlast) \r\n/home/marius/Code/ccapital/<ipython-input-196-065c9192fad1> in \r\n<module>() \r\n----> 1 ss[17] \r\n/usr/lib64/python2.7/site-packages/pandas/core/series.pyc in \r\n__getitem__(self, key) \r\n    267                     except Exception, _: \r\n    268                         pass \r\n--> 269                     raise e1 \r\n    270         except TypeError: \r\n    271             pass \r\nKeyError: 17 \r\nIn [197]: ss[18] \r\nOut[197]: 9 \r\n\r\n================= \r\n\r\nIs this behaviour intended? To me it seems like a fundamental bug. \r\n\r\nFrom the documentation: \r\nSelect row by location (int):   df.ix[loc]      Series \r\nWhich is not what ix seems to be doing. \r\n\r\nIf the labels are made strings, the behaviour is utterly different \r\n(correct according to the documentation): \r\nss = pandas.Series(range(10), index = map(str, range(0, 20, 2))) \r\nprint ss \r\n0     0 \r\n2     1 \r\n4     2 \r\n6     3 \r\n8     4 \r\n10    5 \r\n12    6 \r\n14    7 \r\n16    8 \r\n18    9 \r\nName: None, Length: 10 \r\nss[0] \r\nOut[220]: 0 \r\nss[1] \r\nOut[221]: 1 \r\nss[2] \r\nOut[222]: 2 \r\n\r\n\r\nMany thanks,\r\n    -- Marius \r\n\r\n'"
326,2127081,wesm,wesm,2011-11-02 21:57:40,2011-11-03 21:03:07,2011-11-03 21:03:07,closed,,0.6.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/326,b'DataFrame.plot with legend fails when the columns are integers',b'Fails at least with matplotlib 1.1. Should always stringify the columns'
325,2126661,craustin,wesm,2011-11-02 21:17:27,2011-11-05 00:32:56,2011-11-05 00:32:56,closed,,0.6.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/325,b'to_string on DataFrame may print column labels in the wrong order',"b""d = {'a': [1,2,3], 'b':[4,5,6]}\r\nfrom pandas import DataFrame\r\ndf = DataFrame(d)\r\n\r\ndf.to_string()\r\n\r\nReturns:\r\n    a  b\r\n0  1  4\r\n1  2  5\r\n2  3  6\r\n\r\ndf.to_string(columns=['b','a'])\r\n\r\nReturns:\r\n    a  b\r\n0  4  1\r\n1  5  2\r\n2  6  3"""
323,2124754,mcobzarenco,wesm,2011-11-02 18:34:47,2011-11-05 00:00:46,2011-11-05 00:00:46,closed,,0.6.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/323,b'[BUG] DataFrame.describe fails if no numeric columns',"b""DataFrame.describe function fails with the following when called on a \r\nData Frame with no numeric columns: \r\n\r\n  /usr/lib64/python2.7/site-packages/pandas/core/ \r\nframe.py(2324)describe() \r\n   2322                        '25%', '50%', '75%', 'max'] \r\n   2323 \r\n-> 2324         data = [tmp.count(), tmp.mean(), tmp.std(), tmp.min(), \r\n   2325                 tmp.quantile(.25), tmp.median(), \r\n   2326                 tmp.quantile(.75), tmp.max()] \r\n  /usr/lib64/python2.7/site-packages/pandas/core/frame.py(2361)count() \r\n   2359             frame = self \r\n   2360 \r\n-> 2361         result = DataFrame.apply(frame, Series.count, \r\naxis=axis) \r\n   2362 \r\n   2363         # what happens with empty DataFrame \r\n  /usr/lib64/python2.7/site-packages/pandas/core/frame.py(1984)apply() \r\n   1982         else: \r\n   1983             if not broadcast: \r\n-> 1984                 return self._apply_standard(func, axis) \r\n   1985             else: \r\n   1986                 return self._apply_broadcast(func, axis) \r\n> /usr/lib64/python2.7/site-packages/pandas/core/frame.py(2000)_apply_standar d() \r\n\r\n   1998             results[k] = func(target[k]) \r\n   1999 \r\n-> 2000         if hasattr(results.values()[0], '__iter__'): \r\n   2001             result = self._constructor(data=results, \r\nindex=target.index, \r\n   2002 \r\n\r\n===========================================\r\n \r\nThe problem seems to be that in the function DataFrame.describe, tmp \r\nis empty if there are no numeric columns: \r\n\r\ncols = self._get_numeric_columns() \r\ntmp = self.reindex(columns=cols) \r\n\r\nNonetheless, Series.describe itself works on non-numeric values.\r\n\r\n -- Marius """
322,2124458,davclark,wesm,2011-11-02 18:05:41,2012-04-10 05:07:43,2012-04-10 05:07:38,closed,,,4,Bug,https://api.github.com/repos/pydata/pandas/issues/322,b'Round trip from CSV to CSV should leave data types unaltered',"b""I love where pandas is going. A use-case for which R's csv capabilities still win, though, is in editing a large csv file that's used as a poor-mans database.\r\n\r\nR will keep a column of integers as integers, pandas changes some of them to floats, some to strings! I'm not sure why there's inconsistency there, but I figure I'll kick the general issue your way and I'm happy to clarify as needed."""
316,2115148,wesm,wesm,2011-11-01 21:10:07,2011-12-24 21:37:14,2011-12-24 21:37:14,closed,,0.7.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/316,b'Raise warning or exception when calling Series.sort when do not own data',"b'For example, when part of a DataFrame, can result in the source array being modified in place'"
315,2114312,wesm,wesm,2011-11-01 19:56:01,2011-11-03 21:19:25,2011-11-03 21:19:25,closed,,0.6.0,3,Bug,https://api.github.com/repos/pydata/pandas/issues/315,b'Support boolean data in Cythonized groupby',"b'reported by @bburan on the mailing list\r\n\r\n```\r\n\r\nIn [146]: frame = DataFrame({\'a\': np.random.randint(0, 5, 10), \'b\': np.random.ra\r\nndint(0, 2, 10).astype(\'bool\')})\r\n\r\nIn [147]: print frame\r\n  a  b\r\n0  1  False\r\n1  2  True\r\n2  2  True\r\n3  1  True\r\n4  3  True\r\n5  3  True\r\n6  3  True\r\n7  1  True\r\n8  2  True\r\n9  1  False\r\n\r\nIn [150]: frame.groupby(\'a\')[\'b\'].mean()\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\nc:\\users\\brad\\projects\\sane\\src\\sane\\scripts\\<ipython-input-150-fceb5b21892e> in\r\n <module>()\r\n----> 1 frame.groupby(\'a\')[\'b\'].mean()\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.pyc in mean(self)\r\n   297         For multiple groupings, the result index will be a MultiIndex\r\n   298         """"""\r\n--> 299         return self._cython_agg_general(\'mean\')\r\n   300\r\n   301     def size(self):\r\n\r\nC:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.pyc in _cython_agg_general(sel\r\nf, how)\r\n   347             output[name] = result[mask]\r\n   348\r\n--> 349         return self._wrap_aggregated_output(output, mask)\r\n   350\r\n   351     def _get_multi_index(self, mask):\r\n\r\nUnboundLocalError: local variable \'mask\' referenced before assignment\r\n\r\nNote that agg() does work:\r\n\r\nIn [153]: frame.groupby(\'a\')[\'b\'].agg(np.mean)\r\nOut[153]:\r\n1    0.5\r\n2    1.0\r\n3    1.0\r\nName: None, Length: 3\r\n```'"
314,2113737,creeson,wesm,2011-11-01 18:56:52,2011-11-03 21:31:38,2011-11-03 21:31:38,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/314,b'Series.apply() does not always return a Series',"b'Example below of it returning a NumPy array. This seems unexpected, given the documentation in the function.\r\n\r\nThanks!\r\n\r\nExample:\r\nCode:\r\nimport pandas as ps\r\nimport numpy as np\r\n\r\ntest = ps.Series(np.linspace(start=0, stop=1, num=20))\r\nprint test\r\nprint test.apply(lambda x: np.round(x, 1))\r\n\r\nNote : this is equivalent to np.round(test, 1)\r\n\r\nOutput:\r\n>>> \r\n0     0.0\r\n1     0.0526315789474\r\n2     0.105263157895\r\n3     0.157894736842\r\n4     0.210526315789\r\n5     0.263157894737\r\n6     0.315789473684\r\n7     0.368421052632\r\n8     0.421052631579\r\n9     0.473684210526\r\n10    0.526315789474\r\n11    0.578947368421\r\n12    0.631578947368\r\n13    0.684210526316\r\n14    0.736842105263\r\n15    0.789473684211\r\n16    0.842105263158\r\n17    0.894736842105\r\n18    0.947368421053\r\n19    1.0\r\nName: None, Length: 20\r\n[ 0.   0.1  0.1  0.2  0.2  0.3  0.3  0.4  0.4  0.5  0.5  0.6  0.6  0.7  0.7\r\n  0.8  0.8  0.9  0.9  1. ]\r\n\r\n'"
311,2103787,wesm,wesm,2011-11-01 00:42:23,2011-11-04 22:19:15,2011-11-04 22:19:15,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/311,b'aggregate_item_by_item does not handle as_index=False in GroupBy',
304,2068646,Komnomnomnom,wesm,2011-10-27 14:45:53,2011-10-28 18:08:43,2011-10-28 15:13:43,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/304,b'Cannot fill inserted column of NaNs',"b""After inserting a column of NaNs into a preexisting DataFrame its contents cannot be filled using advanced indexing. See test case below.\r\n\r\nThis only appears to happen if the existing DataFrame is filled with floats, i.e. it works fine for ints.\r\n\r\nTested with pandas 0.5 and latest github snapshot.\r\n\r\n```python\r\nfrom pandas import DataFrame\r\nimport numpy as np\r\n\r\nprint '\\n***RESULT\\n'\r\n\r\ndf = DataFrame({'x': [1.1, 2.1, 3.1, 4.1], 'y': [5.1, 6.1, 7.1, 8.1]}, index=[0,1,2,3])\r\ndf.insert(2, 'z', np.nan)\r\n\r\nprint df\r\n\r\ndf.ix[2:, 'z'] = 42\r\n\r\nprint df\r\n\r\nprint '\\n***EXPECTED\\n'\r\n\r\ndf = DataFrame({'x': [1.1, 2.1, 3.1, 4.1], 'y': [5.1, 6.1, 7.1, 8.1], 'z':[np.nan]*4}, index=[0,1,2,3])\r\n\r\nprint df\r\n\r\ndf.ix[2:, 'z'] = 42\r\n\r\nprint df\r\n```\r\n\r\nThe output I get is \r\n\r\n```\r\n\r\n***RESULT\r\n\r\n   x    y    z  \r\n0  1.1  5.1  NaN\r\n1  2.1  6.1  NaN\r\n2  3.1  7.1  NaN\r\n3  4.1  8.1  NaN\r\n   x    y    z  \r\n0  1.1  5.1  NaN\r\n1  2.1  6.1  NaN\r\n2  3.1  7.1  NaN\r\n3  4.1  8.1  NaN\r\n\r\n***EXPECTED\r\n\r\n   x    y    z  \r\n0  1.1  5.1  NaN\r\n1  2.1  6.1  NaN\r\n2  3.1  7.1  NaN\r\n3  4.1  8.1  NaN\r\n   x    y    z   \r\n0  1.1  5.1  NaN \r\n1  2.1  6.1  NaN \r\n2  3.1  7.1  42\r\n3  4.1  8.1  42\r\n```"""
293,2056828,lodagro,wesm,2011-10-26 13:52:34,2011-11-04 20:04:21,2011-11-04 20:04:21,closed,,0.6.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/293,b'DataFrame constructor fails on list of tuples',"b""In [93]: pandas.__version__\r\nOut[0]: '0.5.0'\r\n\r\nIn [94]: df = pandas.DataFrame({'A': [(1, 2)]})\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n\r\n< .. snip ... >\r\n\r\nThe latter fails, however the following works fine:\r\n\r\nIn [95]: s = pandas.Series([(1, 2)])         ---> this was fixed in issue #270\r\n\r\nIn [96]: df = pandas.DataFrame({'A': s})\r\n\r\nIn [97]: s\r\nOut[0]:\r\n0    (1, 2)\r\nName: None, Length: 1\r\n\r\nIn [98]: df\r\nOut[0]:\r\n     A\r\n0  (1, 2)\r\n"""
290,2052174,dieterv77,wesm,2011-10-26 00:56:05,2011-10-26 10:24:27,2011-10-26 01:14:10,closed,,0.6.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/290,b'DataFrame.to_csv throws exception',"b'Hi, i ran into a problem with the current master on github.  Here\'s \r\n\r\nimport cStringIO as StringIO\r\nimport pandas\r\nf1 = StringIO.StringIO(\'a,1.0\\nb,2.0\')\r\ndf = pandas.DataFrame.from_csv(f1,header=None)\r\nnewdf = pandas.DataFrame({\'t\': df[df.columns[0]]})\r\nnewdf.to_csv(\'/tmp/test.csv\')\r\n\r\nThe last line gives me an exception like this:\r\n\r\nTraceback (most recent call last):\r\n  File ""/tmp/test.py"", line 6, in <module>\r\n    newdf.to_csv(\'/tmp/test.csv\')\r\n  File ""/usr/lib/python2.7/dist-packages/pandas/core/frame.py"", line 531, in to_csv\r\n    csvout.writerow(list(index_label) + list(cols))\r\nTypeError: \'NoneType\' object is not iterable\r\n\r\nCode like this was working until quite recently.  What i\'m really trying to do is to read a bunch of csv files, each of which contains two columns of data, the first being the index, the second some values.  The index is the same across files, so\r\ni\'m trying to combine these files into one DataFrame.\r\n\r\nthanks\r\ndieter'"
285,2037201,wesm,wesm,2011-10-24 19:00:45,2011-10-25 01:35:04,2011-10-25 01:35:04,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/285,b'Investigate erroneous int conversion in MultiIndex',"b""```\r\n\r\n>> I was just using this feature and noticed a curious behaviour -- if you have dates in the ISO8601 YYYYMMDD format then importing from csv with a MultiIndex turns them into ints, whereas importing with a regular index parses them to dates.\r\n>>\r\n>> Looking at pandas/io/parsers.py (https://github.com/wesm/pandas/commit/1a5a252d97cb3f691a6bf93412b921b085f1be76#L1L113) it seems that the _maybe_convert_int_mindex() method you added prefers ints over dates -- is this intentional or were you just not expecting any overlap between ints and dates?\r\n>>\r\n>\r\n> Most likely the latter.\r\n>\r\n>> You also no longer call _maybe_convert_int, which was previously being called but is now unused, and instead do the int parsing with a map().\r\n>>\r\n>> Why not just implement _maybe_convert_int_mindex() along the lines of what was originally there, something like this:\r\n>>  for i in range(len(index)):\r\n>>    if parse_dates:\r\n>>      index = _try_parse_dates(index[i], parser=date_parser)\r\n>>    index[i] = _maybe_convert_int(np.array(index[i], dtype=object))\r\n>>\r\n>> I would just do it and submit a patch but I just wanted to check with you as you're more familiar with this code since you wrote it.\r\n```"""
270,2016922,wesm,wesm,2011-10-21 16:02:13,2011-10-21 17:13:58,2011-10-21 17:13:58,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/270,b'Series constructor fails on list of tuples',"b'example:\r\n\r\n```\r\nSeries([(1,2)])\r\n```'"
262,2005758,wesm,wesm,2011-10-20 15:11:40,2011-10-20 22:54:13,2011-10-20 22:54:13,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/262,b'Index names lost when calling sort_index',"b""\r\n\r\n```\r\nMultiIndex names get lost in the returned dataframe after sort_index is used.\r\nHere is an example:\r\n\r\nIn [1]: import pandas\r\n\r\nIn [2]: pandas.__version__\r\nOut[2]: '0.4.3'\r\n\r\nIn [3]: index = pandas.MultiIndex.from_tuples([('x','y','z'), ('a','b','c'), ('p','q','r')], names=['one','two','three'])\r\n\r\nIn [4]: df = pandas.DataFrame(randn(3, 5), index=index)\r\n\r\nIn [5]: df\r\nOut[5]:\r\n         0       1       2       3       4\r\nx  y  z -1.098  -0.3314 -0.5526  0.3791  1.12\r\na  b  c  1.141   0.3309 -0.6509 -0.561  -1.086\r\np  q  r  0.3399 -0.3135  1.677   0.533   0.1158\r\n\r\n\r\nIn [6]: df.index.names\r\nOut[6]: ['one', 'two', 'three']\r\n\r\nIn [7]: dfs = df.sort_index()\r\n\r\nIn [8]: dfs\r\nOut[8]:\r\n         0       1       2       3       4\r\na  b  c  1.141   0.3309 -0.6509 -0.561  -1.086\r\np  q  r  0.3399 -0.3135  1.677   0.533   0.1158\r\nx  y  z -1.098  -0.3314 -0.5526  0.3791  1.12\r\n\r\n\r\nIn [9]: dfs.index.names\r\nOut[9]: [None, None, None]\r\n```"""
260,2004114,asqui,wesm,2011-10-20 12:19:18,2011-10-21 04:32:09,2011-10-21 04:32:09,closed,,0.5.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/260,b'MultiIndex.diff not implemented and drops through to Index.diff',"b'MultiIndex implements custom union() and intersection() behavior, but diff() is not implemented and drops through to Index.diff, giving incorrect behavior.'"
258,1995451,wesm,wesm,2011-10-19 16:06:23,2011-10-19 17:27:37,2011-10-19 17:27:37,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/258,b'read_table will modify passed names if index_col is not None',
257,1995179,wesm,wesm,2011-10-19 15:40:45,2011-10-19 17:27:28,2011-10-19 17:27:28,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/257,b'Specifying names should make header=None by default in read_table',
252,1967281,wesm,wesm,2011-10-18 19:36:14,2011-10-19 21:07:19,2011-10-19 21:07:19,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/252,b'Casting float column to int does not work',"b""```\r\n\r\nIn [1]: df = DataFrame(np.random.randint(0, 5, 10), dtype=float)\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   0\r\n0  1\r\n1  4\r\n2  0\r\n3  0\r\n4  1\r\n5  1\r\n6  2\r\n7  4\r\n8  3\r\n9  3\r\n\r\n\r\nIn [3]: df[0] = df[0].astype('i8')\r\n\r\nIn [4]: df\r\nOut[4]: \r\n   0\r\n0  1\r\n1  4\r\n2  0\r\n3  0\r\n4  1\r\n5  1\r\n6  2\r\n7  4\r\n8  3\r\n9  3\r\n\r\n\r\nIn [5]: df.dtypes\r\nOut[5]: \r\n0    float64\r\nName: None, Length: 1\r\n```"""
251,1965603,aman-thakral,aman-thakral,2011-10-18 19:12:20,2011-10-18 20:59:19,2011-10-18 20:59:19,closed,,0.5.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/251,b'Subplots error when DataFrame column contains all null values',b'This crashes the plot() command in matplotlib.  Simple fix would be to just check that all only columns containing non-null values are plotted.'
246,1962049,wesm,wesm,2011-10-18 18:22:23,2011-10-18 18:31:50,2011-10-18 18:31:50,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/246,b'DataFrame.join on key with single key failure',"b""e.g.\r\n\r\n```\r\ndf.join(df2, on=['key'])\r\n```\r\n\r\n"""
239,1918892,wesm,wesm,2011-10-15 19:36:07,2011-10-16 01:11:43,2011-10-15 19:52:37,closed,,0.5.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/239,b'Enable creation of empty Panel objects',b'@creeson reported that this is not possible'
238,1914732,creeson,wesm,2011-10-14 22:28:24,2011-10-15 19:23:26,2011-10-15 18:53:42,closed,,0.5.0,4,Bug,https://api.github.com/repos/pydata/pandas/issues/238,b'Joining with empty DataFrame on outer breaks',"b'In some problems it is convenient to create an empty dataframe, and then append to it in a loop. The following code now breaks:\r\n\r\nimport pandas as ps\r\nx = ps.DataFrame()\r\nx.join(ps.DataFrame([3], index=[0], columns=[\'A\']), how=\'outer\')\r\nx.join(ps.DataFrame([3], index=[0], columns=[\'A\']))\r\n\r\nPandas 0.4.1\r\n\r\n>>> import pandas as ps\r\n>>> x = ps.DataFrame()\r\n>>> x.join(ps.DataFrame([3], index=[0], columns=[\'A\']), how=\'outer\')\r\n   A\r\n0  3\r\n\r\n>>> x.join(ps.DataFrame([3], index=[0], columns=[\'A\']))\r\nEmpty DataFrame\r\nIndex([], dtype=object)\r\n\r\nPandas 0.4.3\r\n\r\n>>> import pandas as ps\r\n>>> x = ps.DataFrame()\r\n>>> x.join(ps.DataFrame([3], index=[0], columns=[\'A\']), how=\'outer\')\r\n\r\nTraceback (most recent call last):\r\n  File ""<pyshell#1197>"", line 1, in <module>\r\n    x.join(ps.DataFrame([3], index=[0], columns=[\'A\']), how=\'outer\')\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 2168, in join\r\n    return self._join_index(other, how, lsuffix, rsuffix)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 2193, in _join_index\r\n    merged_data = join_managers(thisdata, otherdata, axis=1, how=how)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 958, in join_managers\r\n    lblocks = _maybe_upcast_blocks(left.blocks, lneed_masking)\r\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py"", line 1029, in _maybe_upcast_blocks\r\n    return _consolidate(new_blocks, newb.ref_items)\r\nUnboundLocalError: local variable \'newb\' referenced before assignment\r\n\r\n>>> x.join(ps.DataFrame([3], index=[0], columns=[\'A\']))\r\nEmpty DataFrame\r\nIndex([], dtype=object)\r\n'"
237,1914126,wesm,wesm,2011-10-14 21:01:49,2011-10-14 21:15:38,2011-10-14 21:15:38,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/237,b'Groupby apply mixed-type casting problem',"b""the following groupby transform results in things getting casted to object...\r\n\r\n```python\r\ndf = DataFrame({'d' : [1.,1.,1.,2.,2.,2.],\r\n                'c' : np.tile(['a','b','c'], 2),\r\n                'v' : np.arange(1., 7.)})\r\n# in IPython\r\nIn [34]: df\r\nOut[34]: \r\n   c  d  v\r\n0  a  1  1\r\n1  b  1  2\r\n2  c  1  3\r\n3  a  2  4\r\n4  b  2  5\r\n5  c  2  6\r\n```\r\n\r\nNow write a small transform function:\r\n\r\n```python\r\ndef f(group):\r\n    v = group['v']\r\n    group['v2'] = (v - v.min()) / (v.max() - v.min())\r\n    return group\r\n```\r\n\r\nNote that this also handles NAs since the v variable is a pandas Series object.\r\n\r\nNow group by the d column and apply f:\r\n\r\n```python\r\nIn [36]: df.groupby('d').apply(f)\r\nOut[36]: \r\n   c  d  v  v2 \r\n0  a  1  1  0  \r\n1  b  1  2  0.5\r\n2  c  1  3  1  \r\n3  a  2  4  0  \r\n4  b  2  5  0.5\r\n5  c  2  6  1  \r\n```"""
231,1900225,wesm,wesm,2011-10-13 13:45:18,2011-10-15 19:20:41,2011-10-15 19:20:41,closed,,0.5.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/231,"b""Objects indexed by datetime.date can't be stored in HDFStore""",
225,1887760,takluyver,wesm,2011-10-12 14:19:49,2011-10-12 20:21:48,2011-10-12 18:54:16,closed,,0.5.0,6,Bug,https://api.github.com/repos/pydata/pandas/issues/225,b'read_csv interprets index column as dates',"b'I loaded a dataset with an index column going from 1 to 304. Then I try to get the repr of the dataframe, but it raises `ValueError: year=100 is before 1900; the datetime strftime() methods require year >= 1900`. Looking at `df.index` shows that the numeric indices have been transformed into datetimes (the first 31 to days of this month, 32-60 as years 20xx, 61-99 as years 19xx, and higher numbers as a raw year).'"
224,1886696,takluyver,takluyver,2011-10-12 12:03:52,2011-10-12 15:04:10,2011-10-12 15:04:10,closed,,0.5.0,6,Bug,https://api.github.com/repos/pydata/pandas/issues/224,b'Plotting series (columns) fails',"b""Calling plot from matplotlib with series (e.g. columns of a DataFrame) fails, because it tries to add an axis to the series objects. A traceback can be found here: https://gist.github.com/1281034 .\r\n\r\nOne way to solve this could be, if we try to index a series to multiple dimensions, simply return a plain numpy array of the data. Or maybe it makes more sense to make changes on the matplotlib side?\r\n\r\n`Series.plot()` and `DataFrame.plot()` are designed to plot columns against the index, but don't appear to help with plotting one column against another."""
205,1853753,wesm,wesm,2011-10-08 20:25:44,2011-10-09 14:25:43,2011-10-09 14:25:43,closed,,0.4.3,0,Bug,https://api.github.com/repos/pydata/pandas/issues/205,b'Sparse speed optimization',"b'Ops in intindex form are much faster than in block form, for some reason'"
204,1847077,wesm,wesm,2011-10-07 18:30:04,2011-10-07 19:18:07,2011-10-07 19:18:07,closed,,0.4.3,1,Bug,https://api.github.com/repos/pydata/pandas/issues/204,b'HDFStore chokes on writing empty LongPanel',
202,1837475,rsamson,wesm,2011-10-06 20:26:31,2011-10-06 20:49:08,2011-10-06 20:46:52,closed,,0.4.3,2,Bug,https://api.github.com/repos/pydata/pandas/issues/202,b'.sortlevel does not retain .names in MultiIndex',b'.sortlevel does not retain .names in MultiIndex.  For example:\r\n\r\n>>> options\r\n                        ForwardPrice  Strike  CallPut  Premium  \r\n105169  57.2024993896   15.03         15.03   C        2.206\r\n        -41.0596008301  15.03         15.03   P        2.231\r\n...\r\n\r\n>>> options.delevel()\r\n   SecurityID  Delta  ForwardPrice  Strike  CallPut  Premium \r\n0  105169      57.2   15.03         15.03   C        2.206          \r\n1  105169     -41.06  15.03         15.03   P        2.231         \r\n...\r\n\r\n>>> options = options.sortlevel(0)\r\n>>> options.delevel()\r\n   level_0  level_1  ForwardPrice  Strike  CallPut  Premium  \r\n0  105169  -41.06    15.03         15.03   P        2.231                \r\n1  105169   57.2     15.03         15.03   C        2.206        \r\n...'
201,1836676,rsamson,wesm,2011-10-06 18:57:04,2011-10-06 21:32:22,2011-10-06 21:24:33,closed,,0.4.3,2,Bug,https://api.github.com/repos/pydata/pandas/issues/201,"b'.append drops MultiIndex, .join not supported for MultiIndex'","b'You are probably already aware, but .append and .join are not yet fully functional with MultiIndex.  \r\n\r\n1. .append turns the index into numbers\r\n\r\n>>> s1=s[:4]\r\n>>> s2=s[4:]\r\n>>> s1\r\nbar  one    0.229167564165\r\n     two    -1.12529475186\r\nbaz  one    1.03310069555\r\n     two    -0.859600695252\r\n>>> s2\r\nfoo  one    1.75545035115\r\n     two    -1.44073899127\r\nqux  one    -0.239883854241\r\n     two    -1.22027857654\r\n>>> s1.append(s2)\r\n0    0.229167564165\r\n1    -1.12529475186\r\n...\r\n6    -0.239883854241\r\n7    -1.22027857654\r\n\r\n\r\n>>> df1=df[:4]\r\n>>> df2=df[4:]\r\n>>> df1\r\n          A       B       C     \r\nbar  one -1.165   0.6214 -0.4865\r\n     two -0.8845 -0.3048 -0.1469\r\nbaz  one  1.533  -0.7665  1.133 \r\n     two -0.2581  2.067  -0.4839\r\n\r\n>>> df2\r\n          A       B        C      \r\nfoo  one -1.321  -0.08075 -1.135  \r\n     two -0.3081  0.8832   0.09399\r\nqux  one -0.7685  0.8034  -1.032  \r\n     two  0.3256  1.158    0.163  \r\n\r\n>>> df1.append(df2)\r\n   A       B        C      \r\n0 -1.165   0.6214  -0.4865 \r\n1 -0.8845 -0.3048  -0.1469 \r\n...\r\n6 -0.7685  0.8034  -1.032  \r\n7  0.3256  1.158    0.163  \r\n\r\n\r\n2. .join errors out\r\n\r\n>>> df1t=df1.T\r\n>>> df2t=df2.T\r\n>>> df1t\r\n   bar             baz           \r\n   one     two     one     two   \r\nA -1.165  -0.8845  1.533  -0.2581\r\nB  0.6214 -0.3048 -0.7665  2.067 \r\nC -0.4865 -0.1469  1.133  -0.4839\r\n\r\n>>> df2t\r\n   foo               qux           \r\n   one      two      one     two   \r\nA -1.321   -0.3081  -0.7685  0.3256\r\nB -0.08075  0.8832   0.8034  1.158 \r\nC -1.135    0.09399 -1.032   0.163 \r\n\r\n>>> df1t.join(df2t)\r\n...\r\nIndexError: list index out of range\r\n\r\n\r\n3. both .join and .append are fine when you are lining the data up with MultiIndex, rather than creating an expanded MultiIndex.\r\n>>> dfa\r\n          A       B     \r\nbar  one  0.5341  0.5299\r\n     two -0.3412  0.8686\r\nbaz  one  0.6352  1.835 \r\n     two -0.4022 -0.2686\r\n\r\n>>> dfb\r\n          C       D      \r\nbar  one -1.066  -1.029  \r\n     two -0.8317 -0.9012 \r\nbaz  one -0.4223  0.9805 \r\n     two -1.348   1.204  \r\n\r\n>>> dfa.join(dfb)\r\n          A       B       C       D      \r\nbar  one  0.5341  0.5299 -1.066  -1.029  \r\n     two -0.3412  0.8686 -0.8317 -0.9012 \r\nbaz  one  0.6352  1.835  -0.4223  0.9805 \r\n     two -0.4022 -0.2686 -1.348   1.204  \r\n\r\n>>> (dfa.T.append(dfb.T)).T\r\n          A       B       C       D      \r\nbar  one  0.5341  0.5299 -1.066  -1.029  \r\n     two -0.3412  0.8686 -0.8317 -0.9012 \r\nbaz  one  0.6352  1.835  -0.4223  0.9805 \r\n     two -0.4022 -0.2686 -1.348   1.204  \r\n'"
199,1836426,wesm,wesm,2011-10-06 18:25:09,2011-10-09 17:36:26,2011-10-09 17:36:26,closed,,0.4.3,1,Bug,https://api.github.com/repos/pydata/pandas/issues/199,b'MultiIndex with single level does not work',"b'`.ix` based indexing fails, for example'"
185,1771030,wesm,wesm,2011-09-29 01:31:48,2011-09-30 01:05:12,2011-09-30 01:05:08,closed,,0.4.2,0,Bug,https://api.github.com/repos/pydata/pandas/issues/185,b'Advanced indexing .ix does not handle step with integer labels',"b'Should raise exception:\r\n\r\n```python\r\n\r\nIn [14]: import pandas.util.testing as tm\r\n\r\nIn [15]: df = tm.makeTimeDataFrame()\r\n\r\nIn [16]: df.index = np.arange(len(df))\r\n\r\nIn [17]: df.ix[:20:2, :]\r\nOut[17]: \r\n    A       B        C         D     \r\n0   0.7272  2.299    0.6004   -1.295 \r\n1   0.1992 -0.6554  -0.6646    2.679 \r\n2  -2.888  -0.8418   0.9162   -0.423 \r\n3   0.1537  2.135    0.2564    0.7554\r\n4   2.036   1.728   -1.055     0.59  \r\n5  -0.1152 -1.131    0.1782    0.89  \r\n6  -0.1432 -0.5045  -1.04     -1.235 \r\n7   0.4615  0.6217   0.652     0.7405\r\n8   0.9663 -0.8796   1.295    -1.333 \r\n9  -0.2225 -0.6098  -0.1984   -0.534 \r\n10  0.3549  2.245    1.554     1.43  \r\n11 -1.241  -1.069   -1.85     -1.263 \r\n12 -1.484  -0.01338  0.4773   -1.221 \r\n13  0.8658  1.43    -0.005297  0.5183\r\n14 -0.4009 -0.5202  -0.6209   -1.438 \r\n15  0.756   0.2559   0.4617    0.3081\r\n16  0.7314  1.809    1.922    -0.1331\r\n17 -1.633  -1.026   -1.327    -0.3797\r\n18  0.731  -0.1977   1.26      0.9189\r\n19  0.1858 -0.9953   0.8936    1.237 \r\n20 -1.195  -0.7265  -0.7423   -0.8248\r\n```'"
182,1765150,scottza,wesm,2011-09-28 15:03:45,2011-10-02 11:56:59,2011-10-01 19:24:10,closed,,0.4.2,2,Bug,https://api.github.com/repos/pydata/pandas/issues/182,b'DataFrame to_csv() ignores nanRep argument for float32 input',"b""import numpy as np\r\nimport pandas\r\n\r\nd = np.random.randn(8, 4)\r\ndf = pandas.DataFrame(d)\r\ndf[1][:] = np.nan\r\ndf.to_csv('pandas-success.csv', nanRep='999')\r\n\r\nd = np.asarray(np.random.randn(8, 4), dtype=np.float32)\r\ndf = pandas.DataFrame(d)\r\ndf[1][:] = np.nan\r\ndf.to_csv('pandas-fail.csv', nanRep='999')\r\n\r\nAfter running this with pandas 0.4.1 the file pandas-success.csv\r\ncontains the nanRep string '999' as defined, but the file\r\npandas-fail.csv contains the string 'nan' instead."""
181,1758697,wesm,wesm,2011-09-28 02:32:20,2011-10-01 19:56:43,2011-10-01 19:56:43,closed,,0.4.2,1,Bug,https://api.github.com/repos/pydata/pandas/issues/181,b'as_index in GroupBy still not fully implemented',"b""```python\r\ndf = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\r\n                          'foo', 'bar', 'foo', 'foo'],\r\n                   'B' : ['one', 'one', 'two', 'three',\r\n                          'two', 'two', 'one', 'three'],\r\n                   'C' : randn(8), 'D' : randn(8)})\r\ndf\r\n   grouped = df.groupby('A')\r\ngrouped = df.groupby('A')\r\ngrouped.agg(np.sum)\r\ngrouped = df.groupby('A', as_index=False)\r\ngrouped.agg(np.sum)\r\ngrouped.sum()\r\n```\r\n\r\nAdding @laserson so notifications go through"""
179,1749992,salestial,wesm,2011-09-27 08:19:17,2012-04-22 02:11:51,2012-04-22 02:11:51,closed,,0.8.0,7,Bug;Enhancement;Timeseries,https://api.github.com/repos/pydata/pandas/issues/179,b'Pre-epoch dates giving OverflowError in mktime with HDFStore',"b'I am having trouble with pre-epoch dates working with HDFStore, getting an OverflowError in mktime, as shown below.\r\n\r\n\r\nSample code is:\r\n\r\n        import numpy as np\r\n        from pandas import *\r\n        \r\n        \r\n        def panda_test():\r\n            \r\n            # generate some data\r\n            data = np.random.rand(50,5)    \r\n            # generate some dates\r\n            dates = DateRange(\'1/1/1969\',periods=50)    \r\n            # generate column headings\r\n            cols = [\'A\',\'B\',\'C\',\'D\',\'E\']\r\n            \r\n            df = DataFrame(data,index=dates,columns=cols)\r\n            \r\n            # save to HDF5Store\r\n            store = HDFStore(\'bugzilla.h5\', mode=\'w\')\r\n            store[\'df\'] = df # This gives: OverflowError: mktime argument out of range\r\n            store.close()\r\n        \r\n        \r\n        if __name__ == \'__main__\':\r\n            panda_test()\r\n\r\n\r\nThe error I get is:\r\n\r\n        Traceback (most recent call last):\r\n          File ""C:\\Users\\Salman Ansari\\Documents\\Python\\VAR\\src\\bugzilla.py"", line 27, in <module>\r\n            panda_test()\r\n          File ""C:\\Users\\Salman Ansari\\Documents\\Python\\VAR\\src\\bugzilla.py"", line 22, in panda_test\r\n            store[\'df\'] = df\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 122, in __setitem__\r\n            self.put(key, value)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 260, in put\r\n            comp=compression)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 327, in _write_to_group\r\n            wrapper(value)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 325, in <lambda>\r\n            wrapper = lambda value: handler(group, value)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 335, in _write_frame\r\n            self._write_block_manager(group, df._data)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 346, in _write_block_manager\r\n            self._write_index(group, \'axis%d\' % i, ax)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 432, in _write_index\r\n            converted, kind, _ = _convert_index(index)\r\n          File ""C:\\Python27\\lib\\site-packages\\pandas-0.4.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 698, in _convert_index\r\n            for v in values], dtype=np.int64)\r\n        OverflowError: mktime argument out of range\r\n        Closing remaining open files: bugzilla.h5... done\r\n\r\n\r\nAny ideas, please?'"
169,1725826,dlovell,wesm,2011-09-23 18:56:20,2011-09-25 04:55:27,2011-09-25 04:55:27,closed,,0.4.1,3,Bug,https://api.github.com/repos/pydata/pandas/issues/169,b'issue moving from 0.3.0 to v0.4.0',"b""0.4.0 seems to use np.add.reduceat() where 0.3.0 doesn't which is causing issues with some code I run during the evaluation of the beta in a PanelOLS.  In particular, during the evaluation of self._y_trans.count() in _time_obs_count() there is a seg fault.\r\n\r\nI'm not familiar with the internals of numpy or pandas but it looks like 0.4.0 passes in a zero length array for the output.  The debugger won't go into the np.add.reduceat() code but it segfaults as soon as I make the next step\r\n\r\npython -m pdb valModel1.py\r\nb /usr/local/lib/python2.7/site-packages/pandas/core/series.py:505\r\n\r\n> /usr/local/lib/python2.7/site-packages/pandas/core/series.py(505)_count_level()\r\n-> np.add.reduceat(mask, locs[start:end], out=out)\r\n(Pdb) np.shape(mask)\r\n(0,)\r\n(Pdb) np.shape(locs)\r\n(271,)\r\n(Pdb) np.shape(out)\r\n(0,)\r\n\r\nAnd then seg faults\r\n\r\nIts possible this is an underlying numpy issue but everything works fine on the same system with 0.3.0.\r\n\r\nIf this is of interest to you I can give you more info.\r\n\r\nI'm running Freebsd 8.1 with the following packages and pandas compiled from a git pull\r\n\r\npython27-2.7.1_1\r\ncython-0.14.1\r\npy27-numpy-1.5.1,1\r\npy27-matplotlib-1.0.1\r\npy27-scipy-0.8.0_1\r\npy27-dateutil-1.5\r\npy27-setuptools-0.6c11_1\r\npy27-nose-1.0.0\r\npy27-pytz-2011c\r\n\r\nBest\r\nDan"""
168,1717291,wesm,wesm,2011-09-22 19:41:56,2011-09-25 05:16:37,2011-09-25 05:16:37,closed,,0.4.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/168,b'DateRange.copy bug',b'tzinfo attribute does not make it into the copy'
160,1684051,wesm,wesm,2011-09-20 01:05:12,2011-09-22 15:22:03,2011-09-22 15:22:03,closed,,0.4.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/160,b'as_index in GroupBy does not work properly with a single key',
155,1671380,wesm,wesm,2011-09-18 01:44:00,2011-10-14 04:39:36,2011-10-14 04:39:36,closed,,0.5.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/155,b'Weights option should be removed in OLS classes',
154,1670786,wesm,wesm,2011-09-17 21:37:51,2011-09-20 15:52:21,2011-09-20 15:52:17,closed,,0.4.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/154,b'Series.shift fails on integer arrays due to NaN handling',
147,1667354,wesm,wesm,2011-09-17 01:19:26,2011-09-20 15:38:52,2011-09-20 15:38:48,closed,,0.4.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/147,b'Sortlevel / pivot may silently misalign data with duplicate labels',
144,1659106,wesm,wesm,2011-09-16 03:21:33,2011-09-22 15:59:55,2011-09-22 15:59:55,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/144,b'DataFrame.corrwith fails with object columns',
138,1645096,wesm,wesm,2011-09-14 16:26:08,2011-09-22 13:45:44,2011-09-22 13:45:44,closed,,0.4.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/138,b'DataFrame repr fails if a column contains tuples',
135,1629620,wesm,wesm,2011-09-13 01:30:09,2011-09-20 16:30:48,2011-09-20 16:30:48,closed,,0.4.1,0,Bug,https://api.github.com/repos/pydata/pandas/issues/135,b'Setting single column in mixed type frame throws exception',"b""This raises an exception\r\n\r\n    df = DataFrame(randn(5, 3), index=labels,\r\n                   columns=['foo', 'bar', 'baz'])\r\n    df['str'] = 'value'\r\n    df.ix[::2, 'str'] = nan\r\n"""
133,1604584,wesm,wesm,2011-09-09 02:27:47,2011-09-09 03:06:57,2011-09-09 02:55:30,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/133,b'Multi-key groupby fails with multiple functions',"b""Chris Jordan-Squire said:\r\n\r\nIf I had some DataFrame with a bunch of columns, including, say\r\n'char1', 'char2', and 'float' then\r\n\r\ndf.groupby('char1')['float'].agg([np.mean, np.std])\r\n\r\nworks but\r\n\r\ndf.groupby(['char1', 'char2'])['float'].agg([np.mean, np.std])\r\n\r\nthrows TypeError: 'list' object is not callable."""
131,1603412,wesm,wesm,2011-09-08 22:32:28,2011-09-09 03:11:08,2011-09-09 03:07:28,closed,,0.4.0,2,Bug,https://api.github.com/repos/pydata/pandas/issues/131,"b""GroupBy.apply can't yield Series with DataFrame input""",b'    In [23]: import pandas.util.testing as tm\r\n\r\n    In [24]: df = tm.makeTimeDataFrame()\r\n\r\n    In [25]: df.groupby(lambda x: x.month).count()\r\n    Out[25]: \r\n       A   B   C   D \r\n    1  21  21  21  21\r\n    2  9   9   9   9 \r\n\r\n\r\n    In [26]: df.groupby(lambda x: x.month).apply(len)\r\n    Exception\r\n\r\n    you want to get a Series like:\r\n\r\n    1  21\r\n    2  9\r\n'
128,1581128,wesm,wesm,2011-09-06 17:23:12,2011-09-08 03:07:23,2011-09-08 03:07:23,closed,,0.4.0,0,Bug,https://api.github.com/repos/pydata/pandas/issues/128,b'Objects with hierarchical index cannot be saved using HDFStore',b'This really should get done for 0.4'
120,1554718,wesm,wesm,2011-09-02 20:11:06,2014-07-01 02:55:23,2011-09-05 20:39:52,closed,,0.4.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/120,b'Sortedness problems with MultiIndex',"b""Take this example:\r\n\r\n   arrays = [['bar', 'bar', 'baz', 'baz', 'qux', 'qux', 'foo', 'foo'],\r\n             ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\r\n   tuples = zip(*arrays)\r\n   tuples\r\n   index = MultiIndex.from_tuples(tuples)\r\n   s = Series(randn(8), index=index)\r\n\r\nThis does not yield the expected result:\r\n\r\n   s['qux']\r\n\r\n"""
112,1485006,wesm,wesm,2011-08-25 18:48:29,2012-02-09 21:20:29,2011-09-06 04:14:42,closed,,0.4.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/112,b'.stack() does not yet work when MultiIndex already present',b'This will not be dealt with in 0.4 unless I get a lot of energy'
107,1454623,wesm,wesm,2011-08-22 00:39:43,2011-09-05 20:23:29,2011-09-05 20:23:29,closed,,0.4.0,1,Bug,https://api.github.com/repos/pydata/pandas/issues/107,b'Partial fancy indexing using ints does not work on MultiIndex',"b""    In [44]: import pandas.rpy.common as com\r\n\r\n    In [45]: cw = com.load_data('ChickWeight')\r\n\r\n             Chick  weight\r\n    1  0.0   10.5   41.4  \r\n       2.0   10.5   47.25 \r\n       4.0   10.11  56.47 \r\n       6.0   10.11  66.79 \r\n       8.0   10.11  79.68 \r\n       10.0  10.11  93.05 \r\n       12.0  10.11  108.5 \r\n       14.0  9.778  123.4 \r\n       16.0  9.471  144.6 \r\n       18.0  9.471  158.9 \r\n       20.0  9.471  170.4 \r\n       21.0  9.562  177.8 \r\n    2  0.0   25.5   40.7  \r\n       2.0   25.5   49.4  \r\n       4.0   25.5   59.8  \r\n       6.0   25.5   75.4  \r\n       8.0   25.5   91.7  \r\n\r\n    In [47]: cw.groupby(['Diet', 'Time']).mean().ix[1]\r\n    Out[47]: \r\n    Chick     10.5\r\n    weight    47.25\r\n"""
106,1445136,craustin,wesm,2011-08-19 17:04:42,2011-08-23 02:20:04,2011-08-23 02:20:04,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/106,b'ols should handle a Series (w/ MultiIndex) + LongPanel combo.',b'The Series (w/ MultiIndex) is returned by __getitem__ on a LongPanel.'
100,1376703,wesm,wesm,2011-08-10 00:51:55,2011-08-19 20:12:50,2011-08-19 20:12:50,closed,,,1,Bug;Enhancement,https://api.github.com/repos/pydata/pandas/issues/100,b'Remove AmbiguousIndexError in fancy indexing',"b'.ix field should prefer labels over integers, while __getitem__ should prefer integers\r\n'"
98,1375374,wesm,wesm,2011-08-09 20:53:42,2011-08-09 22:54:57,2011-08-09 22:54:57,closed,,,0,Bug,https://api.github.com/repos/pydata/pandas/issues/98,b'Calling xs on DataFrame with index but no columns raises Exception',"b""example:\r\n\r\n    In [1]: df = DataFrame(index=['a', 'b', 'c'])\r\n\r\n    In [2]: df.ix['a']\r\n    ERROR: An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: ('EOF in multi-line statement', (2, 0))\r\n\r\n    ---------------------------------------------------------------------------\r\n    IndexError                                Traceback (most recent call last)\r\n    /home/wesm/code/pandas/<ipython-input-2-6181dcf1ffea> in <module>()\r\n    ----> 1 df.ix['a']\r\n\r\n    /home/wesm/code/pandas/pandas/core/indexing.pyc in __getitem__(self, key)\r\n        102             return self._fancy_getitem(key, axis=0)\r\n        103         else:\r\n    --> 104             return self._fancy_getitem_axis(key, axis=0)\r\n        105 \r\n        106     def __setitem__(self, key, value):\r\n\r\n    /home/wesm/code/pandas/pandas/core/indexing.pyc in _fancy_getitem_axis(self, key, axis)\r\n        200                 return self.frame.xs(idx)\r\n        201             else:\r\n    --> 202                 return self.frame.xs(idx, copy=False)\r\n        203         else:\r\n        204             col = key\r\n\r\n    /home/wesm/code/pandas/pandas/core/frame.pyc in xs(self, key, copy)\r\n        839 \r\n        840         self._consolidate_inplace()\r\n    --> 841         values = self._data.xs(key, axis=1, copy=copy)\r\n        842         return Series(values.as_matrix(), index=self.columns)\r\n        843 \r\n\r\n    /home/wesm/code/pandas/pandas/core/internals.pyc in xs(self, key, axis, copy)\r\n        451                 new_blocks.append(newb)\r\n        452         else:\r\n    --> 453             vals = self.blocks[0].values[slicer]\r\n        454             if copy:\r\n        455                 vals = vals.copy()\r\n\r\n    IndexError: list index out of range"""
81,1299665,changhiskhan,wesm,2011-07-27 23:02:15,2011-07-31 16:23:24,2011-07-31 16:23:24,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/81,b'bug in DataFrame/DataMatrix toCSV method when index=False',"b""When index=False an extra column is added because the leading comma is never removed.\r\n\r\n    def toCSV(self, path, nanRep='', cols=None, header=True, index=True, mode='wb'):\r\n\r\n        *<snip>*\r\n\r\n        for idx in self.index:\r\n\r\n            if index:\r\n                f.write(str(idx))\r\n            for col in cols:\r\n                *<snip>*\r\n                f.write(',%s' % val)\r\n\r\n"""
76,1245597,wesm,wesm,2011-07-18 21:00:39,2011-07-20 03:52:38,2011-07-20 03:52:38,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/76,b'Sparse cumsum functions do not work',b'e.g. SparseSeries.cumsum'
66,1242473,wesm,wesm,2011-07-18 15:45:19,2011-07-30 18:02:16,2011-07-30 18:02:16,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/66,b'Fancy indexing on single column of DataFrame',"b""Things like this does not work\r\n\r\n    df.ix[boolean_vec, 'B'] = val"""
61,1239398,wesm,wesm,2011-07-18 02:15:11,2011-07-20 19:46:26,2011-07-20 19:46:25,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/61,b'DataFrame.__ne__ not implemented',"b""See\r\n   \r\n    dm = DataFrame({'col1':[1,2],'col2':[1,2]}) \r\n    dm[dm!=0] = 5"""
48,997165,wesm,wesm,2011-06-03 09:08:28,2011-06-14 14:06:41,2011-06-14 14:06:41,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/48,"b'""Proper"" boolean array with NA handling in DataMatrix'",b'Currently booleans are getting casted to floats in some circumstances in order to handle NAs. Need to devise a workable scheme for boolean data possibly containing NAs.'
45,934142,surbas,wesm,2011-05-20 22:13:31,2011-06-23 04:17:27,2011-06-23 04:17:27,closed,,,2,Bug,https://api.github.com/repos/pydata/pandas/issues/45,"b'Importing data using HDFStore with pre-epoch dates; ""ValueError: timestamp out of range for platform localtime()/gmtime() function""'","b'I have data with a DataFrame that goes back to 1949. I imported it from a csv into a hdf5 using HDFStore. That went fine, but when reading from the HDFStore to get a DF back, I get the below stack trace. When looking at the data in the store I see that the index has negative values for preepoch times... \n\n\nValueError: timestamp out of range for platform localtime()/gmtime() function\nFile ""C:\\dev\\MktDB\\test_continuation.py"", line 59, in <module>\n  main()\nFile ""C:\\Python27\\lib\\site-packages\\pandas-0.3.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 157, in _read_group\nFile ""C:\\Python27\\lib\\site-packages\\pandas-0.3.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 173, in _read_frame\nFile ""C:\\Python27\\lib\\site-packages\\pandas-0.3.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 210, in _read_index\nFile ""C:\\Python27\\lib\\site-packages\\pandas-0.3.0-py2.7-win32.egg\\pandas\\io\\pytables.py"", line 227, in _unconvert_index\nFile ""C:\\Users\\Shon\\AppData\\Roaming\\Python-Eggs\\pandas-0.3.0-py2.7-win32.egg-tmp\\pandas\\lib\\tseries.pyd"", line 45, in tseries.array_to_datetime (pandas\\lib\\src\\tseries.c:14378)\nFile ""C:\\Users\\Shon\\AppData\\Roaming\\Python-Eggs\\pandas-0.3.0-py2.7-win32.egg-tmp\\pandas\\lib\\tseries.pyd"", line 20, in tseries.to_datetime (pandas\\lib\\src\\tseries.c:13910)\n\nAny guidance would be most appreciated. \nShon'"
18,358952,wesm,wesm,2010-10-12 16:15:10,2011-12-31 16:53:33,2011-09-25 05:17:23,closed,,,1,Bug,https://api.github.com/repos/pydata/pandas/issues/18,b'weights option may not be working in pandas.stats.ols.OLS',b'Need to investigate (user notified)'
8,339355,andylei,wesm,2010-09-29 19:45:47,2014-02-11 20:40:01,2011-06-23 04:50:05,closed,,,3,Bug,https://api.github.com/repos/pydata/pandas/issues/8,"b""np.fix doesn't work""","b'    In [28]: np.fix(Series([1,2,3], range(3)))\r\n    ---------------------------------------------------------------------------\r\n    Exception                                 Traceback (most recent call last)\r\n    \r\n    C:\\alei1\\basic_mktneutral\\<ipython console> in <module>()\r\n    \r\n    C:\\Python26\\lib\\site-packages\\numpy\\lib\\ufunclike.pyc in fix(x, y)\r\n         43     x = nx.asanyarray(x)\r\n         44     if y is None:\r\n    ---> 45         y = nx.zeros_like(x)\r\n         46     y1 = nx.floor(x)\r\n         47     y2 = nx.ceil(x)\r\n    \r\n    C:\\Python26\\lib\\site-packages\\numpy\\core\\numeric.pyc in zeros_like(a)\r\n         92     if isinstance(a, ndarray):\r\n         93         res = ndarray.__new__(type(a), a.shape, a.dtype, order=a.flags.fnc)\r\n    ---> 94         res.fill(0)\r\n         95         return res\r\n         96     try:\r\n    \r\n    C:\\Python26\\lib\\site-packages\\pandas\\core\\series.pyc in fill(self, value, method)\r\n        822         """"""\r\n        823         if value is not None:\r\n    --> 824             newSeries = self.copy()\r\n        825             newSeries[isnull(newSeries)] = value\r\n        826             return newSeries\r\n    \r\n    C:\\Python26\\lib\\site-packages\\pandas\\core\\series.pyc in copy(self)\r\n        342\r\n        343     def copy(self):\r\n    --> 344         return Series(self.values.copy(), index=self.index)\r\n        345\r\n        346 #-------------------------------------------------------------------------------\r\n    \r\n    \r\n    C:\\Python26\\lib\\site-packages\\pandas\\core\\series.pyc in __new__(cls, data, index, dtype, copy)\r\n        136\r\n        137         if index is None:\r\n    --> 138             raise Exception(\'Index cannot be None!\')\r\n        139\r\n        140         # This is to prevent mixed-type Series getting all casted to\r\n\r\ni think the bigger problem is overriding the default behavior of the fill() method, which may contribute to other numpy / scipy functions not behaving correctly.'"
