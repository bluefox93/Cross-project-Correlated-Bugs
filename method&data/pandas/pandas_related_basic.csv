issue,id,reporter,closed_by,created_at,updated_at,closed_at,state,assignee,milestone,comments,label_name,url,title,body
dask/dask#1393,166696658,jcrist,jcrist,2016-07-20 22:25:07,2016-07-23 06:20:43,2016-07-21 17:26:14,closed,,,10,,https://api.github.com/repos/dask/dask/issues/1393,b'A few df fixes',"b""A few fixes for bugs discovered during the scipy sprints.\r\n\r\n- Use fsync in `partd.append`. This was causing issues in some scenarios due to data not being consistently written to disk. This shouldn't have any significant performance impact, and covers some periodic edge cases.\r\n- A few fixes to make things work with numeric column names. This was causing problems with `groupby` and `dir` specifically."""
numpy/numpy#7854,166682036,flatberg,,2016-07-20 21:06:01,2016-07-24 09:38:34,None,open,,,12,,https://api.github.com/repos/numpy/numpy/issues/7854,b'Memory (segfault) issues with np.empty_like',b'import numpy as np\r\nimport pandas as pd\r\npd.DataFrame(np.empty_like([None]))\r\n\r\nDetails at: pydata/pandas/issues/13717'
numpy/numpy#7857,166752365,gfyoung,gfyoung,2016-07-21 07:12:28,2016-07-23 19:51:04,2016-07-23 08:54:55,closed,,,18,,https://api.github.com/repos/numpy/numpy/issues/7857,b'BUG: Explicitly fill in None for np.empty_like',"b'Explicitly fills in returned array with `Py_None` if the resulting `dtype` is `object` to avoid data\r\ncorruption, which led to segfaults in `pandas`.\r\n\r\nCloses #7854.\r\nCloses <a href=""https://github.com/pydata/pandas/issues/13717"">#13717</a> (`pandas`).\r\n\r\ncc @bashtage @jreback'"
dask/dask#1174,156365730,davclark,jcrist,2016-05-23 21:00:08,2016-07-20 20:49:45,2016-07-20 20:49:45,closed,,,11,dataframe;io,https://api.github.com/repos/dask/dask/issues/1174,b'read_hdf fails in dask even though it works in pandas',"b'dask version 0.9.0, pandas 0.18.1 (most recent from conda as of posting)\r\n\r\nGrab a 1 MB fake TAQ file from [here](https://github.com/dlab-projects/marketflow/raw/master/test-data/small_test_data_public.h5). (aside: same data is ~1/4 MB in zipped fixed width - chunk sizes are probably dumb for this data.)\r\n\r\n`pd.read_hdf(\'small_test_data_public.h5\', \'/IXQAJE/no_suffix\')` works, `dask.dataframe.read_hdf(\'small_test_data_public.h5\', \'/IXQAJE/no_suffix\')` fails with the following stack trace. I think it may be due to the attempt to read an empty dataframe of 0-length. If I read the intent correctly, it would probably make more sense to retrieve a pytables or h5py object which would provide the desired metadata without the weirdness around a 0-length read.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-110-8ad7ff0d4733> in <module>()\r\n----> 1 spy_dd = dd.read_hdf(fname, max_sym)\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in read_hdf(pattern, key, start, stop, columns, chunksize, lock)\r\n    559                                     columns=columns, chunksize=chunksize,\r\n    560                                     lock=lock)\r\n--> 561                    for path in paths])\r\n    562 \r\n    563 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in <listcomp>(.0)\r\n    559                                     columns=columns, chunksize=chunksize,\r\n    560                                     lock=lock)\r\n--> 561                    for path in paths])\r\n    562 \r\n    563 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in _read_single_hdf(path, key, start, stop, columns, chunksize, lock)\r\n    499     from .multi import concat\r\n    500     return concat([one_path_one_key(path, k, start, s, columns, chunksize, lock)\r\n--> 501                    for k, s in zip(keys, stops)])\r\n    502 \r\n    503 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in <listcomp>(.0)\r\n    499     from .multi import concat\r\n    500     return concat([one_path_one_key(path, k, start, s, columns, chunksize, lock)\r\n--> 501                    for k, s in zip(keys, stops)])\r\n    502 \r\n    503 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in one_path_one_key(path, key, start, stop, columns, chunksize, lock)\r\n    474         not contain any wildcards).\r\n    475         """"""\r\n--> 476         empty = pd.read_hdf(path, key, stop=0)\r\n    477         if columns is not None:\r\n    478             empty = empty[columns]\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in read_hdf(path_or_buf, key, **kwargs)\r\n    328                                  \'multiple datasets.\')\r\n    329             key = keys[0]\r\n--> 330         return store.select(key, auto_close=auto_close, **kwargs)\r\n    331     except:\r\n    332         # if there is an error, close the store\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\r\n    678                            chunksize=chunksize, auto_close=auto_close)\r\n    679 \r\n--> 680         return it.get_result()\r\n    681 \r\n    682     def select_as_coordinates(\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in get_result(self, coordinates)\r\n   1362 \r\n   1363         # directly return the result\r\n-> 1364         results = self.func(self.start, self.stop, where)\r\n   1365         self.close()\r\n   1366         return results\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in func(_start, _stop, _where)\r\n    671             return s.read(start=_start, stop=_stop,\r\n    672                           where=_where,\r\n--> 673                           columns=columns, **kwargs)\r\n    674 \r\n    675         # create the iterator\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in read(self, where, columns, **kwargs)\r\n   4052 \r\n   4053             block = make_block(values, placement=np.arange(len(cols_)))\r\n-> 4054             mgr = BlockManager([block], [cols_, index_])\r\n   4055             frames.append(DataFrame(mgr))\r\n   4056 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/core/internals.py in __init__(self, blocks, axes, do_integrity_check, fastpath)\r\n   2592 \r\n   2593         if do_integrity_check:\r\n-> 2594             self._verify_integrity()\r\n   2595 \r\n   2596         self._consolidate_check()\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/core/internals.py in _verify_integrity(self)\r\n   2802         for block in self.blocks:\r\n   2803             if block._verify_integrity and block.shape[1:] != mgr_shape[1:]:\r\n-> 2804                 construction_error(tot_items, block.shape[1:], self.axes)\r\n   2805         if len(self.items) != tot_items:\r\n   2806             raise AssertionError(\'Number of manager items must equal union of \'\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/core/internals.py in construction_error(tot_items, block_shape, axes, e)\r\n   3966         raise e\r\n   3967     if block_shape[0] == 0:\r\n-> 3968         raise ValueError(""Empty data passed with indices specified."")\r\n   3969     raise ValueError(""Shape of passed values is {0}, indices imply {1}"".format(\r\n   3970         passed, implied))\r\n\r\nValueError: Empty data passed with indices specified.\r\n```'"
matplotlib/matplotlib#6365,153009738,Puggie,tacaswell,2016-05-04 13:19:19,2016-05-23 19:16:12,2016-05-23 19:16:12,closed,,2.0 (style change major release),12,date handling,https://api.github.com/repos/matplotlib/matplotlib/issues/6365,b'Default format time series xtick labels changed',"b'The default format to display time series xtick labels in time series plots seems to have been changed between matplotlib 1.4 and 1.5. When using time stamps in a datetime format with a 1 second resolution and plotting a minute worth of data, the major xtick labels are now formatted as ""HH:MM:SS.mmmmm"" as opposed to ""HH:MM:SS"" in matplotlib 1.4. This new behaviour doesn\'t match the significany of the timestamps and results in overlapping xtick labels as well.\r\n\r\nA simple example to illustrate the issue:\r\n\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport datetime\r\n\r\n# generating time stamps with a resolution of 1s\r\nx = np.array([datetime.datetime(2016, 5, 4, 12, 0, i) for i in range(60)])\r\n# generating some random y-values\r\ny = np.random.randn(x.shape[0]).cumsum()\r\n\r\n# plot\r\nplt.plot(x, y)\r\nplt.show()\r\n```\r\n This results in the following figure:\r\n![image](https://cloud.githubusercontent.com/assets/3796187/15014847/8f482f6a-1208-11e6-8152-4999e6a7b013.png)\r\n\r\nOf course, the xtick label format can be manually adjusted:\r\n```\r\nimport matplotlib.dates as dates\r\n\r\n# create plot\r\nplt.plot(x, y)\r\n# get reference to x-axis\r\nxax = plt.gca().get_xaxis()\r\n# format major xtick label\r\nxax.set_major_formatter(dates.DateFormatter(\'%H:%M:%S\'))\r\n# show plot\r\nplt.show()\r\n```\r\nBut I found the old behavior more elegant.\r\n\r\nI\'m using matplotlib 1.5.1 on Python 3.5.1 32-bit as part of the Anaconda3 4.0.0 Windows x86 distrubtion. The OS is Windows 7 64 bit.'"
matplotlib/matplotlib#6392,153882063,tacaswell,WeatherGod,2016-05-09 22:13:13,2016-05-11 14:27:56,2016-05-11 14:27:52,closed,,1.5.2 (Critical bug fix release),3,,https://api.github.com/repos/matplotlib/matplotlib/issues/6392,b'FIX: do not assume we can [] error inputs',b'In errorbar do not assume that [0] or [1] will work correctly\r\n(due to issues with pandas).\r\n\r\nCloses https://github.com/pydata/pandas/issues/11858'
dask/dask#1063,143794271,thequackdaddy,jcrist,2016-03-27 09:54:29,2016-03-29 21:11:34,2016-03-29 21:11:34,closed,,,5,,https://api.github.com/repos/dask/dask/issues/1063,"b""BUG: dask.dataframe fill_value doesn't seem to do anything...""","b""Try the following code... it should result in a dask dataframe with no `NaN` values, but alas, it still shows up. I guess `fill_value` isn't working. \r\n\r\n\r\n```\r\nimport dask.dataframe as ddf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nx = np.arange(1, 10, dtype=float)\r\nx[5] = np.nan\r\n\r\nx = pd.DataFrame(x, columns=['foo'])\r\nx = ddf.from_pandas(x, chunksize=3)\r\nx.pow(2, fill_value=1).compute()\r\n```"""
numpy/numpy#7163,130466335,jreback,jreback,2016-02-01 19:55:48,2016-02-01 21:21:06,2016-02-01 21:21:06,closed,,,2,,https://api.github.com/repos/numpy/numpy/issues/7163,"b""BUG: np.percentile(..., interpolation='midpoint') seems to have just broken""","b""These matched here: ``1.12.0.dev0+0bba665``\r\n\r\nnow\r\n```\r\nIn [22]: np.__version__\r\nOut[22]: '1.12.0.dev0+d1dada1'\r\n\r\nIn [19]: arr = np.array([0,2,4])\r\n\r\nIn [20]: np.percentile(arr,[25,50],interpolation='linear')\r\nOut[20]: array([ 1.,  2.])\r\n\r\nIn [21]: np.percentile(arr,[25,50],interpolation='midpoint')\r\nOut[21]: array([ 1.,  2.])\r\n```\r\n\r\nfor comparison to 10.2\r\n```\r\nIn [5]: np.__version__\r\nOut[5]: '1.10.2'\r\n\r\nIn [1]: In [19]: arr = np.array([0,2,4])\r\n\r\nIn [2]: In [20]: np.percentile(arr,[25,50],interpolation='linear')\r\nOut[2]: array([ 1.,  2.])\r\n\r\nIn [4]: In [21]: np.percentile(arr,[25,50],interpolation='midpoint')\r\nOut[4]: array([ 1.,  3.])\r\n```"""
nilmtk/nilmtk#471,127137955,jhyun0919,JackKelly,2016-01-18 01:12:58,2016-01-27 13:06:05,2016-01-27 13:06:05,closed,,,19,,https://api.github.com/repos/nilmtk/nilmtk/issues/471,b'proportion_of_energy_submetered() raises an IndexError',"b'\r\nStats for MeterGroups\r\n-----------------------------------------\r\nProportion of energy submetered\r\n\r\n----------------------------------------------\r\n```python\r\nIn [10]: elec.proportion_of_energy_submetered()\r\n\r\n---------------------------------------------\r\nRunning MeterGroup.proportion_of_energy_submetered...\r\nnilmtk/metergroup.py:901: UserWarning: As a quick implementation we only get Good Sections from the first meter in the meter group.  We should really return the intersection of the good sections for all meters.  This will be fixed...\r\n  warn(""As a quick implementation we only get Good Sections from""\r\n/root/anaconda2/envs/nilmtk-env/lib/python2.7/site-packages/pandas/tseries/base.py:100: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\r\n  val = getitem(key)\r\n\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-10-3b14c6037fc4> in <module>()\r\n----> 1 elec.proportion_of_energy_submetered()\r\n\r\n/root/nilmtk/nilmtk/metergroup.pyc in proportion_of_energy_submetered(self, **loader_kwargs)\r\n   1050             if verbose:\r\n   1051                 print(""Calculating proportion for"", m)\r\n-> 1052             prop = m.proportion_of_energy(mains, **loader_kwargs)\r\n   1053             if not np.isnan(prop):\r\n   1054                 proportion += prop\r\n\r\n/root/nilmtk/nilmtk/electric.pyc in proportion_of_energy(self, other, **loader_kwargs)\r\n    282         float [0,1] or NaN if other.total_energy == 0\r\n    283         """"""\r\n--> 284         good_other_sections = other.good_sections(**loader_kwargs)\r\n    285         loader_kwargs.setdefault(\'sections\', good_other_sections)\r\n    286 \r\n\r\n/root/nilmtk/nilmtk/metergroup.pyc in good_sections(self, **kwargs)\r\n    903                      "" return the intersection of the good sections for all""\r\n    904                      "" meters.  This will be fixed..."")\r\n--> 905             return self.meters[0].good_sections(**kwargs)\r\n    906         else:\r\n    907             return []\r\n\r\n/root/nilmtk/nilmtk/elecmeter.pyc in good_sections(self, **loader_kwargs)\r\n    630         results_obj = GoodSections.results_class(self.device[\'max_sample_period\'])\r\n    631         return self._get_stat_from_cache_or_compute(\r\n--> 632             nodes, results_obj, loader_kwargs)        \r\n    633 \r\n    634     def _get_stat_from_cache_or_compute(self, nodes, results_obj, loader_kwargs):\r\n\r\n/root/nilmtk/nilmtk/elecmeter.pyc in _get_stat_from_cache_or_compute(self, nodes, results_obj, loader_kwargs)\r\n    684         if loader_kwargs.get(\'preprocessing\') is None:\r\n    685             cached_stat = self.get_cached_stat(key_for_cached_stat)\r\n--> 686             results_obj.import_from_cache(cached_stat, sections)\r\n    687 \r\n    688             def find_sections_to_compute():\r\n\r\n/root/nilmtk/nilmtk/stats/goodsectionsresults.pyc in import_from_cache(self, cached_stat, sections)\r\n    116                     timeframes = []\r\n    117                     for _, row in sections_df.iterrows():\r\n--> 118                         section_start = tz_localize_naive(row[\'section_start\'], tz)\r\n    119                         section_end = tz_localize_naive(row[\'section_end\'], tz)\r\n    120                         timeframes.append(TimeFrame(section_start, section_end))\r\n\r\n/root/anaconda2/envs/nilmtk-env/lib/python2.7/site-packages/pandas/core/series.pyc in __getitem__(self, key)\r\n    555     def __getitem__(self, key):\r\n    556         try:\r\n--> 557             result = self.index.get_value(self, key)\r\n    558 \r\n    559             if not np.isscalar(result):\r\n\r\n/root/anaconda2/envs/nilmtk-env/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)\r\n   1778         s = getattr(series,\'_values\',None)\r\n   1779         if isinstance(s, Index) and lib.isscalar(key):\r\n-> 1780             return s[key]\r\n   1781 \r\n   1782         s = _values_from_object(series)\r\n\r\n/root/anaconda2/envs/nilmtk-env/lib/python2.7/site-packages/pandas/tseries/base.pyc in __getitem__(self, key)\r\n     98         getitem = self._data.__getitem__\r\n     99         if np.isscalar(key):\r\n--> 100             val = getitem(key)\r\n    101             return self._box_func(val)\r\n    102         else:\r\n\r\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\r\n```'"
matplotlib/matplotlib#6392,153882063,tacaswell,WeatherGod,2016-05-09 22:13:13,2016-05-11 14:27:56,2016-05-11 14:27:52,closed,,1.5.2 (Critical bug fix release),3,,https://api.github.com/repos/matplotlib/matplotlib/issues/6392,b'FIX: do not assume we can [] error inputs',b'In errorbar do not assume that [0] or [1] will work correctly\r\n(due to issues with pandas).\r\n\r\nCloses https://github.com/pydata/pandas/issues/11858'
matplotlib/matplotlib#5550,118458770,arc-jim,tacaswell,2015-11-23 19:52:20,2016-01-01 21:57:36,2016-01-01 21:57:32,closed,tacaswell,v1.5.1,16,,https://api.github.com/repos/matplotlib/matplotlib/issues/5550,b'Plotting datetime values from Pandas dataframe',"b""This appears to be a new issue in 1.5.0.\r\n\r\nThe script below attempts to plot two 2-D graphs whose X and Y values are Pandas series.  The issue seems to occur when pyplot is passed a datetime column which doesn't contain an index of value 0 - note how the second dataframe contains only odd indices (1, 3, 5, etc.)\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# create sample dataframe with preset dates and values columns\r\ndates = np.arange('2005-02', '2005-03', dtype='datetime64[D]')\r\nvalues = np.sin(np.array(range(len(dates))))\r\ndf = pd.DataFrame({'dates': dates, 'values': values})\r\n\r\n# matplotlib figure + two subplots for comparison\r\nfig, axes = plt.subplots(1, 2)\r\n\r\n# create two dataframes for comparison - one with all indices, including 0, and one with only odd indices\r\nwith_zero_index = df.copy()\r\nwithout_zero_index = df[np.array(df.index) % 2 == 1].copy()\r\n\r\n# plot both - note how second plot fails without a 0 index\r\naxes[0].plot(with_zero_index['dates'], with_zero_index['values'])\r\naxes[1].plot(without_zero_index['dates'], without_zero_index['values'])\r\n```\r\n\r\nStack trace:\r\n```\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-29-28e878247c17> in <module>()\r\n     17 # plot both - note how second plot fails without a 0 index\r\n     18 axes[0].plot(with_zero_index['dates'], with_zero_index['values'])\r\n---> 19 axes[1].plot(without_zero_index['dates'], without_zero_index['values'])\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/__init__.pyc in inner(ax, *args, **kwargs)\r\n   1809                     warnings.warn(msg % (label_namer, func.__name__),\r\n   1810                                   RuntimeWarning, stacklevel=2)\r\n-> 1811             return func(ax, *args, **kwargs)\r\n   1812         pre_doc = inner.__doc__\r\n   1813         if pre_doc is None:\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc in plot(self, *args, **kwargs)\r\n   1425             kwargs['color'] = c\r\n   1426 \r\n-> 1427         for line in self._get_lines(*args, **kwargs):\r\n   1428             self.add_line(line)\r\n   1429             lines.append(line)\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/axes/_base.pyc in _grab_next_args(self, *args, **kwargs)\r\n    384                 return\r\n    385             if len(remaining) <= 3:\r\n--> 386                 for seg in self._plot_args(remaining, kwargs):\r\n    387                     yield seg\r\n    388                 return\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/axes/_base.pyc in _plot_args(self, tup, kwargs)\r\n    362             x, y = index_of(tup[-1])\r\n    363 \r\n--> 364         x, y = self._xy_from_xy(x, y)\r\n    365 \r\n    366         if self.command == 'plot':\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/axes/_base.pyc in _xy_from_xy(self, x, y)\r\n    195     def _xy_from_xy(self, x, y):\r\n    196         if self.axes.xaxis is not None and self.axes.yaxis is not None:\r\n--> 197             bx = self.axes.xaxis.update_units(x)\r\n    198             by = self.axes.yaxis.update_units(y)\r\n    199 \r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/axis.pyc in update_units(self, data)\r\n   1387         neednew = self.converter != converter\r\n   1388         self.converter = converter\r\n-> 1389         default = self.converter.default_units(data, self)\r\n   1390         if default is not None and self.units is None:\r\n   1391             self.set_units(default)\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/matplotlib/dates.pyc in default_units(x, axis)\r\n   1562 \r\n   1563         try:\r\n-> 1564             x = x[0]\r\n   1565         except (TypeError, IndexError):\r\n   1566             pass\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/pandas/core/series.pyc in __getitem__(self, key)\r\n    519     def __getitem__(self, key):\r\n    520         try:\r\n--> 521             result = self.index.get_value(self, key)\r\n    522 \r\n    523             if not np.isscalar(result):\r\n\r\n/home/jim/arcemweb/venv/local/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)\r\n   1593 \r\n   1594         try:\r\n-> 1595             return self._engine.get_value(s, k)\r\n   1596         except KeyError as e1:\r\n   1597             if len(self) > 0 and self.inferred_type in ['integer','boolean']:\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_value (pandas/index.c:3113)()\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_value (pandas/index.c:2844)()\r\n\r\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:3704)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7224)()\r\n\r\npandas/hashtable.pyx in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7162)()\r\n\r\nKeyError: 0\r\n```\r\n\r\nLooks like matplotlib's dates.py module is attempting to access the first value in the datetime series - but when passed as a pandas series, x[0] represents the value at index 0, and a) might not exist, and b) isn't necessarily the first value in the series!  Possible fix might be to catch IndexErrors and attempt an x.iloc[0] call instead.\r\n\r\nScript above worked fine in 1.4.3, but fails specifically in 1.5.0."""
dask/dask#954,131375210,user32000,user32000,2016-02-04 15:05:12,2016-02-04 15:25:35,2016-02-04 15:25:35,closed,,,3,,https://api.github.com/repos/dask/dask/issues/954,b'Fatal error with read_csv for large file on OS X on both ',"b'I\'m attempting to read a 4.5GB csv file with dask.  read_csv reproducibly crashes Python, as per below.  This seems different to [Fatal error when running read_csv #841](https://github.com/blaze/dask/issues/841)?\r\n\r\n\r\n```\r\nPython 2.7.11 |Continuum Analytics, Inc.| (default, Dec  6 2015, 18:57:58) \r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\nIPython 4.0.3 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python\'s own help system.\r\nobject?   -> Details about \'object\', use \'object??\' for extra details.\r\n\r\nIn [1]: pdb\r\nAutomatic pdb calling has been turned ON\r\n\r\nIn [2]: import dask.dataframe as dd\r\n\r\nIn [3]: from dask.diagnostics import Pro\r\nProfiler     ProgressBar  \r\n\r\nIn [3]: from dask.diagnostics import ProgressBar\r\n\r\nIn [4]: df = dd. read_csv(\'bigcsv_utf_dot_comma.csv\')\r\n\r\nIn [5]: ProgressBar().register()\r\n\r\nIn [6]: df.compute()\r\n[                                        ] | 0% Completed |  3.4sFatal Python error: GC object already tracked\r\n/Applications/anaconda/envs/python2/bin/python.app: line 3:  5977 Abort trap: 6           /Applications/anaconda/envs/python2/python.app/Contents/MacOS/python ""$@""\r\n```'"
dask/dask#841,118105776,mrocklin,mrocklin,2015-11-20 19:05:40,2016-04-21 01:35:23,2016-01-22 16:07:27,closed,,,21,,https://api.github.com/repos/dask/dask/issues/841,b'Fatal error when running read_csv',"b'Running `read_csv` on https://jakevdp.github.io/blog/2015/08/14/out-of-core-dataframes-in-python/ failed hard, both in single threaded with some parsing exceptions and in multi-threaded with a hard kernel failure.  This occurred both in pandas master (which releases the GIL) and in pandas 0.17.0.\r\n\r\nConversation at https://gitter.im/blaze/dask?at=564f686d06a214f1080b06f7\r\n\r\nSome things to consider:\r\n\r\n*  Are we sending the right blocks of data to `read_csv`?  Is `dask.utils.textblock` running properly (cc @cowlicks).  E.g. does this run correctly before 3ce1de85 ?\r\n*  Do recent changes to Pandas in releasing the GIL cause thread safety issues?'"
dask/dask#860,120127664,drdangersimon,mrocklin,2015-12-03 09:33:11,2016-02-02 22:07:41,2016-02-02 22:07:41,closed,,,13,,https://api.github.com/repos/dask/dask/issues/860,b'Fatal Python error: GC object already tracked',"b""Running python 2.7.10, dask 0.7.5 and pandas 0.17.1.\r\n\r\nWhen I load a csv using dask.DataFrame.read_csv and try to do an apply on the array it gives me that error and stops python.\r\n\r\nexpample:\r\n import dask.dataframe as dd\r\nchunks = dd.read_csv('path to csv', sep='\\t')\r\ndata = chunks.apply(somefunction, axis=1, columns='somecolumn').compute()\r\nFatal Python error: GC object already tracked"""
dask/dask#888,123580009,sscondie,mrocklin,2015-12-23 00:16:56,2016-03-08 23:00:11,2016-01-22 16:03:34,closed,,,14,,https://api.github.com/repos/dask/dask/issues/888,b'Unstable dask._Frame.map_partitions behavior',"b'Consider the following script operating on 15 csv files each about 16MB in size (each csv is an array of floats that is 10 columns and 100,000 rows).  \r\n\r\n```python\r\nimport dask.dataframe as dd\r\ndf = dd.read_csv(""test_data_small/*.csv"")\r\nprint(df.npartitions)\r\nprint(df.mean().compute())\r\nprint(df.map_partitions(len,columns=df.columns).compute())\r\n```\r\n\r\nRunning this script on my machine (OS X (10.11.1), Python 2.7.11 [Anaconda 2.3.0 (x86_64)], pandas 0.17.1) produces (in approximately the same frequencies and in random order) one of the following results (for each result I give first a description and then stdout)\r\n\r\n\r\n**1. npartitions completes, mean completes, map_partitions seg faults** \r\n\r\n```text\r\n15\r\n0                -0.000135\r\n1                -0.000011\r\n10               -0.000939\r\n2                 0.000771\r\n3                 0.000309\r\n4                 0.000573\r\n5                 0.000635\r\n6                 0.000770\r\n7                 0.000713\r\n8                 0.001237\r\n9                -0.000440\r\nUnnamed: 0    49999.500000\r\ndtype: float64\r\nSegmentation fault: 11\r\n```\r\n**2. npartitions completes, Fatal Python Error at mean**\r\n\r\n```text\r\n15\r\nFatal Python error: GC object already tracked\r\nAbort trap: 6\r\n```\r\n\r\n**3. npartitions completes, mean completes, Fatal Python Error at map_partitions**\r\n \r\n```text\r\n15\r\n0                -0.000135\r\n1                -0.000011\r\n10               -0.000939\r\n2                 0.000771\r\n3                 0.000309\r\n4                 0.000573\r\n5                 0.000635\r\n6                 0.000770\r\n7                 0.000713\r\n8                 0.001237\r\n9                -0.000440\r\nUnnamed: 0    49999.500000\r\ndtype: float64\r\nFatal Python error: GC object already tracked\r\nAbort trap: 6\r\n```\r\n\r\n**4.  npartitions completes, seg fault at mean**\r\n\r\n```text\r\n15\r\nSegmentation fault: 11\r\n```\r\n\r\n**5.  Everything completes**\r\n\r\n```text\r\n15\r\n0                -0.000135\r\n1                -0.000011\r\n10               -0.000939\r\n2                 0.000771\r\n3                 0.000309\r\n4                 0.000573\r\n5                 0.000635\r\n6                 0.000770\r\n7                 0.000713\r\n8                 0.001237\r\n9                -0.000440\r\nUnnamed: 0    49999.500000\r\ndtype: float64\r\n(100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001)\r\n```\r\n\r\nI reverted pandas back to 0.17.0 based on a suggestion from Matt [here] http://stackoverflow.com/questions/34128540/why-is-running-a-compute-in-dask-causing-fatal-python-error-gc-object-alrea  but that hasn\'t noticeably changed the results.\r\n\r\nI\'m happy to debug this in dask, but there is at least some chance that this could be a pandas problem.  Has anyone else seen this kind of behavior?  If so, were you able to find an answer?'"
dask/dask#860,120127664,drdangersimon,mrocklin,2015-12-03 09:33:11,2016-02-02 22:07:41,2016-02-02 22:07:41,closed,,,13,,https://api.github.com/repos/dask/dask/issues/860,b'Fatal Python error: GC object already tracked',"b""Running python 2.7.10, dask 0.7.5 and pandas 0.17.1.\r\n\r\nWhen I load a csv using dask.DataFrame.read_csv and try to do an apply on the array it gives me that error and stops python.\r\n\r\nexpample:\r\n import dask.dataframe as dd\r\nchunks = dd.read_csv('path to csv', sep='\\t')\r\ndata = chunks.apply(somefunction, axis=1, columns='somecolumn').compute()\r\nFatal Python error: GC object already tracked"""
dask/dask#888,123580009,sscondie,mrocklin,2015-12-23 00:16:56,2016-03-08 23:00:11,2016-01-22 16:03:34,closed,,,14,,https://api.github.com/repos/dask/dask/issues/888,b'Unstable dask._Frame.map_partitions behavior',"b'Consider the following script operating on 15 csv files each about 16MB in size (each csv is an array of floats that is 10 columns and 100,000 rows).  \r\n\r\n```python\r\nimport dask.dataframe as dd\r\ndf = dd.read_csv(""test_data_small/*.csv"")\r\nprint(df.npartitions)\r\nprint(df.mean().compute())\r\nprint(df.map_partitions(len,columns=df.columns).compute())\r\n```\r\n\r\nRunning this script on my machine (OS X (10.11.1), Python 2.7.11 [Anaconda 2.3.0 (x86_64)], pandas 0.17.1) produces (in approximately the same frequencies and in random order) one of the following results (for each result I give first a description and then stdout)\r\n\r\n\r\n**1. npartitions completes, mean completes, map_partitions seg faults** \r\n\r\n```text\r\n15\r\n0                -0.000135\r\n1                -0.000011\r\n10               -0.000939\r\n2                 0.000771\r\n3                 0.000309\r\n4                 0.000573\r\n5                 0.000635\r\n6                 0.000770\r\n7                 0.000713\r\n8                 0.001237\r\n9                -0.000440\r\nUnnamed: 0    49999.500000\r\ndtype: float64\r\nSegmentation fault: 11\r\n```\r\n**2. npartitions completes, Fatal Python Error at mean**\r\n\r\n```text\r\n15\r\nFatal Python error: GC object already tracked\r\nAbort trap: 6\r\n```\r\n\r\n**3. npartitions completes, mean completes, Fatal Python Error at map_partitions**\r\n \r\n```text\r\n15\r\n0                -0.000135\r\n1                -0.000011\r\n10               -0.000939\r\n2                 0.000771\r\n3                 0.000309\r\n4                 0.000573\r\n5                 0.000635\r\n6                 0.000770\r\n7                 0.000713\r\n8                 0.001237\r\n9                -0.000440\r\nUnnamed: 0    49999.500000\r\ndtype: float64\r\nFatal Python error: GC object already tracked\r\nAbort trap: 6\r\n```\r\n\r\n**4.  npartitions completes, seg fault at mean**\r\n\r\n```text\r\n15\r\nSegmentation fault: 11\r\n```\r\n\r\n**5.  Everything completes**\r\n\r\n```text\r\n15\r\n0                -0.000135\r\n1                -0.000011\r\n10               -0.000939\r\n2                 0.000771\r\n3                 0.000309\r\n4                 0.000573\r\n5                 0.000635\r\n6                 0.000770\r\n7                 0.000713\r\n8                 0.001237\r\n9                -0.000440\r\nUnnamed: 0    49999.500000\r\ndtype: float64\r\n(100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001, 100001)\r\n```\r\n\r\nI reverted pandas back to 0.17.0 based on a suggestion from Matt [here] http://stackoverflow.com/questions/34128540/why-is-running-a-compute-in-dask-causing-fatal-python-error-gc-object-alrea  but that hasn\'t noticeably changed the results.\r\n\r\nI\'m happy to debug this in dask, but there is at least some chance that this could be a pandas problem.  Has anyone else seen this kind of behavior?  If so, were you able to find an answer?'"
jvns/neural-nets-are-weird#6,124603456,DEKHTIARJonathan,,2016-01-02 18:14:29,2016-06-17 22:51:36,None,open,,,0,,https://api.github.com/repos/jvns/neural-nets-are-weird/issues/6,"b'From IPython to Jupyter, HTTPS and Password support, Modifications on Readme.md and Notebook'","b'Everything is good and running by now. Github says ""Can\'t automatically merge"" but it\'s good for overwrite ;)\r\n\r\n**Notebook Updates:** \r\n\r\nProxy Support Added with global var. \xa1\xad\r\nPanda Issue fixed : pydata/pandas#11727 && pydata/pandas#11783\r\nNotebook Checked & Working Perfectly. Executed from started in a ""Run All"" way.\r\n\r\n**Readme Updates:**\r\n\r\nReadme Edited. Automated Build added \xa1\xad\r\nDetached Container commands added\r\n\r\nOSX instructions generalized for Windows.\r\n\r\nNew instructions related to HTTPS and Password protection\r\n\r\n**Docker Container Updates:**\r\n\r\nChanged from IPython (deprecated) to Jupyter Notebook.\r\nConfig File added to simplify container launch command.\r\nHTTPS Support added and SSL/TLS Encryption => Default certificate added to repo.\r\nServer protection by password added : Default password : **neuralnet**.'"
nilmtk/nilmtk#450,116913602,athenawisdoms,nipunbatra,2015-11-14 09:21:56,2016-01-09 13:25:04,2016-01-09 13:25:04,closed,,,11,,https://api.github.com/repos/nilmtk/nilmtk/issues/450,b'AmbiguousTimeError: Cannot infer dst time from Timestamp',"b""Hi all, I tried importing my own data into a `DataSet` using `convert_redd` to output the HDF5 file which is read using `DataSet()`. My data set consists of 2 columns without headers, the first column is the timestamp in unix time in seconds, the second column is the power, making this similar to `channel_1.dat` in the REDD data set.\r\n\r\n    1325376600 52.1147\r\n    1325376900 50.9517\r\n    1325377200 49.8164\r\n    1325377500 49.1795\r\n    1325377800 47.6288\r\n\r\nHowever when trying to create a `pandas.DataFrame`\r\n\r\n    test = DataSet('./data/test/test.h5')\r\n    mains = test.buildings[1].elec.mains()\r\n    mains_df = mains.load().next()\r\n\r\nI'm getting the error\r\n\r\n    pandas/tslib.pyx in pandas.tslib.tz_localize_to_utc (pandas/tslib.c:64516)()\r\n\r\n    AmbiguousTimeError: Cannot infer dst time from Timestamp('2012-11-04 01:00:00'), try using the 'ambiguous' argument\r\n\r\nWhy is this happening? Did something go wrong in the creating of our own `DataSet`?"""
,,,,,,,,,,,,,,
blaze/odo#331,109862339,llllllllll,llllllllll,2015-10-05 18:53:52,2015-10-05 20:02:51,2015-10-05 19:52:18,closed,,,0,backend;bug;pandas,https://api.github.com/repos/blaze/odo/issues/331,b'ENH: Adds nan->pd.NaT edge',"b""Currently reductions on series of dtype datetime64 which are empty result in a `nan` instead of ` pd.NaT` which causes things like:\r\n\r\n```python\r\nodo(pd.Series(dtype='datetime64[ns]').max(), pd.Timestamp)\r\n```\r\n\r\nto fail by not finding a route from `float -> pd.Timestamp`\r\n\r\nI have a pr for pandas (https://github.com/pydata/pandas/pull/11245) that changes return to nat in pandas land but this solves the immediate issue."""
numpy/numpy#4366,28302504,jaimefrio,,2014-02-26 01:51:55,2015-11-28 21:44:40,None,open,,,16,10 - Maintenance;component: numpy.lib,https://api.github.com/repos/numpy/numpy/issues/4366,b'BUG: Allow any integer type in bincount (fixes #823)',"b'Gives array-likes that cannot be cast to np.intp a second chance, by\r\ncomparing its entries one by one to NPY_INTP_MAX and forcing the cast if\r\nnone exceeds it. Fixes #823.'"
numpy/numpy#6238,102624307,kawochen,,2015-08-23 14:14:20,2015-08-24 03:13:10,None,open,,,3,,https://api.github.com/repos/numpy/numpy/issues/6238,b'numpy.arange miscounts when step is large relative to value past stop',"b'```\r\n In [14]: len(numpy.arange(0, 10000000000000001, 100000000000000))\r\nOut[14]: 101\r\n\r\nIn [15]: len(numpy.arange(0, 100000000000000000001, 1000000000000000000))\r\nOut[15]: 100\r\n```'"
dask/dask#587,100982976,sinhrks,mrocklin,2015-08-14 10:28:58,2015-09-04 15:01:41,2015-08-25 14:52:18,closed,,0.7.1,4,,https://api.github.com/repos/dask/dask/issues/587,b'Add DataFrame.merge and DataFrame.join',"b""Add following methods which exists in ``pandas``. \r\n\r\n- ``DataFrame.merge``: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\r\n- ``DataFrame.join``: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html\r\n\r\nAlso, fixed a bug ``dd.merge`` when column name is specified like ``left_on='x'`` and ``right_index=True``.\r\n\r\n```\r\nA = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [1, 1, 2, 2, 3, 4]})\r\na = dd.repartition(A, [0, 4, 5])\r\n\r\nB = pd.DataFrame({'y': [1, 3, 4, 4, 5, 6], 'z': [6, 5, 4, 3, 2, 1]})\r\nb = dd.repartition(B, [0, 2, 5])\r\n\r\ndd.merge(a, b, left_on='x', right_index=True).compute()\r\n# stalls...\r\n```\r\n\r\ndd.merge(a, b, left_on='x', right_index=True).compute()\r\n\r\n### ToDo:\r\n\r\nCurrently, ``DataFrame.join`` using ``on`` kw doesn't work as expected. I'm digging in further. """
dask/dask#558,99897858,sinhrks,sinhrks,2015-08-09 13:38:49,2015-08-09 22:12:12,2015-08-09 22:12:10,closed,,,1,,https://api.github.com/repos/dask/dask/issues/558,b'BUG: Series aggregation may output incorrect results',"b""Found some cases ``dask.Series`` aggregation results are different from ``pandas``.\r\n\r\n#### 1. handiling nan\r\n\r\nIf any partition's aggregation result contains ``nan``, reduction using normal numpy function (like ``np.sum``) results in ``nan``. We can't use ``np.sum``, and also ``np.nansum`` because input can be non-numerics.\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\n# pandas\r\ns = pd.Series([1] + [np.nan] * 10)\r\ns.sum()\r\n# 1.0\r\n\r\n# dask\r\ndd.from_pandas(s, 3).sum().compute()\r\n# nan\r\n```\r\n\r\n#### 2. non-numeric dtypes\r\n\r\n``pandas`` may aggregate non-numerics, but ``dask`` doesn't.\r\n\r\n```\r\n# pandas\r\ns = pd.Series(['a', 'b', 'c', 'd', 'e'])\r\ns.sum()\r\n# 'abcde'\r\n\r\n# dask\r\ndd.from_pandas(s, 3).sum().compute()\r\n# TypeError: Exception occurred in remote worker.\r\n```\r\n\r\nThis PR fixes both issues, and adds test cases to confirm ``dask`` and ``pandas`` outputs the same result (or same error if aggregation is not supported).\r\n\r\nNOTE: ``.std`` and ``.mean`` for ``timedelta`` dtype are not supported yet.\r\n"""
dask/dask#760,108618899,rabernat,mrocklin,2015-09-28 09:20:01,2015-11-03 04:39:40,2015-10-01 14:38:59,closed,,0.7.4,6,,https://api.github.com/repos/dask/dask/issues/760,b'segmentation fault with dask.dataframe.read_hdf',"b""I continue in my quest to use dask with a large hdf file.\r\n\r\nAfter learning that my original pytables-generated HDF files might not be able to be read by dask.dataframe (because dask uses subselection: pydata/pandas#11188), I tried re-generating the files using the pandas HDFStore interface. (First I had to updated pytables overcome a different bug: pydata/pandas#10672.) (Also, in the meantime, a workaround was found for reading the pytables file into dask: #747, #759.)\r\n\r\nAnyway, now I have a pandas-generated HDF file. I can read it with pandas, but with dask I am getting a segfault.\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\nfname = '/data/scratch/rpa/global_lagrangian/run/pandas_floats.h5'\r\nkey = '/floats/trajectories'\r\ndf = pd.read_hdf(fname, key)\r\nddf = dd.read_hdf(fname, key)\r\nprint(len(df))\r\nprint(len(ddf))\r\n```\r\n\r\nThe first print statement gives ``753664023``. The second generates a segfault via the ``len`` call.\r\n\r\nI can still call ``ddf.head()`` and ``ddf.tail()``, but other more complex operations (e.g. ``groupby``) fail, presumably due to the same underlying issue.\r\n\r\nI am using all the latest masters of dask, pandas, and pytables."""
biocore/scikit-bio#1023,98508913,RNAer,ebolyen,2015-08-01 05:11:06,2015-09-30 20:34:17,2015-09-14 17:14:52,closed,,,41,,https://api.github.com/repos/biocore/scikit-bio/issues/1023,b'add skeleton for genbank support',b'This is to fix #986. not for merge for the time being. Just for code review and feedback. Will be constantly updated.'
biocore/scikit-bio#1023,98508913,RNAer,ebolyen,2015-08-01 05:11:06,2015-09-30 20:34:17,2015-09-14 17:14:52,closed,,,41,,https://api.github.com/repos/biocore/scikit-bio/issues/1023,b'add skeleton for genbank support',b'This is to fix #986. not for merge for the time being. Just for code review and feedback. Will be constantly updated.'
etal/cnvkit#108,154640747,mpschr,mpschr,2016-05-13 06:11:04,2016-05-20 07:40:12,2016-05-20 07:40:11,closed,,,2,,https://api.github.com/repos/etal/cnvkit/issues/108,b'Fix genes not split correctly for gainloss report',"b'This commit should fix #107 \r\n\r\n\r\na couple of details:\r\n * `gary.py` now has a function called `_get_gene_map` which returns an OrderedDict of the gene names (all split) and their respective indices in the data. The order of the map is the same as of occurrence\r\n\r\n* if `gainloss` is performed with a `called.cns` the additional columns such as `cn`, `baf` are being maintained as the `segment` is being updated with `gene`, `start` and `end`, whereas all other values are being maintaine - including `log2` and `probes` are from the segmented information. Particularly the `probes` are important to come from the segmented file, since filtering is being done upon that value.\r\n\r\n* There is a pandas bug which affects the function `gary.by_chromosome` upon grouping of a categorical. I had to inject a `astype(str)` - otherwise it will not work. Can be removed in the future.'"
bokeh/bokeh#3772,129439866,chbrandt,,2016-01-28 12:40:02,2016-04-13 14:27:27,None,open,,short-term,13,tag: API: charts;type: bug,https://api.github.com/repos/bokeh/bokeh/issues/3772,b'BoxPlot chart crashing with pandas.cut() categorical data',"b""bokeh.__version__ : '0.11.0'\r\npandas.__version__ : '0.17.1'\r\n\r\n When I try to do `BoxPlot` using categorical data generated by Pandas 'cut' method, the code crashes with `ValueError: items in new_categories are not the same as in old categories`. The exception is raised by `pandas/core/categorical.py` code, but I suspect the source of the error comes from Bokeh. The reason I think it starts at Bokeh (`bokeh/charts/data_source.groupby()`) is because using the pandas.DataFrame builtin `boxplot()` function everything runs properly.\r\n\r\nThe following code should reproduce the error:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nsignal = np.random.normal(0.5,0.1,900) \r\ninstr = np.random.poisson(0.5,100)\r\nsample = np.concatenate((signal,instr),axis=0)\r\nnp.random.shuffle(sample)\r\ndf = pd.DataFrame({'sample':sample})\r\nnbins = 10\r\nbins = np.linspace(0,1,nbins)\r\ndf['bins'] = pd.cut(df['sample'],bins)\r\nfrom bokeh.charts import BoxPlot\r\np = BoxPlot(df,values='sample',label='bins')\r\n```\r\nIf instead of `bokeh.charts.BoxPlot` we use Pandas' boxplot, everything works fine:\r\n```\r\ndf.boxplot(column='sample',by='bins')\r\n```\r\nAm I doing something wrong?\r\nIf it's a bug, possibly related to pydata/pandas#10505?\r\n\r\nps: there is a notebook I made available for if it helps. The notebook has more steps then the strictly necessary ones (above), though: http://nbviewer.jupyter.org/github/chbrandt/pynotes/blob/master/issues/bokeh/boxplot_report.ipynb\r\n\r\nTnx"""
etal/cnvkit#108,154640747,mpschr,mpschr,2016-05-13 06:11:04,2016-05-20 07:40:12,2016-05-20 07:40:11,closed,,,2,,https://api.github.com/repos/etal/cnvkit/issues/108,b'Fix genes not split correctly for gainloss report',"b'This commit should fix #107 \r\n\r\n\r\na couple of details:\r\n * `gary.py` now has a function called `_get_gene_map` which returns an OrderedDict of the gene names (all split) and their respective indices in the data. The order of the map is the same as of occurrence\r\n\r\n* if `gainloss` is performed with a `called.cns` the additional columns such as `cn`, `baf` are being maintained as the `segment` is being updated with `gene`, `start` and `end`, whereas all other values are being maintaine - including `log2` and `probes` are from the segmented information. Particularly the `probes` are important to come from the segmented file, since filtering is being done upon that value.\r\n\r\n* There is a pandas bug which affects the function `gary.by_chromosome` upon grouping of a categorical. I had to inject a `astype(str)` - otherwise it will not work. Can be removed in the future.'"
adamhajari/spyre#24,87850422,sinhrks,adamhajari,2015-06-12 20:58:24,2015-06-29 15:43:11,2015-06-29 15:43:11,closed,,,4,,https://api.github.com/repos/adamhajari/spyre/issues/24,b'Add Options to display DataFrame Index ',"b'Thanks for the great package!\r\n\r\nCurrently, ``spyre`` doesn\'t render ``DataFrame`` index by default. Is it reasonable to add an option to switch the behaviour?\r\n\r\n- https://github.com/adamhajari/spyre/blob/master/spyre/server.py#L117\r\n\r\nAlso, there is a bug when a ``DataFrame`` has a named ``Index``. Pls see the below image which has unnecessary row.\r\n\r\n```\r\nfrom spyre import server\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nidx = pd.Index(list(\'ABCDE\'), name=\'index\')\r\ndf = pd.DataFrame(np.random.randn(5, 5), index=idx)\r\n\r\nclass Example(server.App):\r\n\r\n    tabs = [\'example\']\r\n    outputs = [{""output_type"" : ""table"",\r\n                ""output_id"" : ""table_id"",\r\n                ""tab"" : ""example"",\r\n                ""on_page_load"" : True }]\r\n\r\n    def getData(self, params):\r\n        return df\r\n\r\napp = Example()\r\napp.launch(port=9093)\r\n```\r\n\r\n![2015-06-13 5 55 22](https://cloud.githubusercontent.com/assets/1696302/8139718/d6089a12-1190-11e5-98c0-5a12cc3e61c6.png)\r\n'"
,,,,,,,,,,,,,,
PyTables/PyTables#409,51364776,guziy,FrancescAlted,2014-12-08 22:59:13,2015-04-19 06:43:02,2015-04-18 18:55:31,closed,,3.2,5,defect,https://api.github.com/repos/PyTables/PyTables/issues/409,b'table.where query does not seem to be able to find rows...',"b'Hi:\r\n\r\nI am creating a big table but I cannot find the rows which are supposed to be there using ```table.where```. But ```table.get_where_list()``` does find the rows in question (see the screenshot, **Note that there is no output as a result of the loop in table.where**). \r\n\r\n![screen shot 2014-12-08 at 17 58 36](https://cloud.githubusercontent.com/assets/900941/5349065/df8c3dca-7f03-11e4-8c41-1b14faf5a40b.png)\r\n\r\n\r\nI was trying to reproduce it with smaller dummy files but no luck... Even when I select a table using ptrepack, the problem goes away... \r\n\r\nNot indexing columns also seems to fix the problem, except in this case querying is very slow....\r\n'"
PyTables/PyTables#319,25509942,jreback,FrancescAlted,2014-01-13 16:41:42,2015-04-19 06:42:02,2015-04-18 18:54:56,closed,,3.2,10,defect,https://api.github.com/repos/PyTables/PyTables/issues/319,b'BUG: possible indexing/selection bug',"b'```\r\nIn [10]: tables.print_versions()\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\nPyTables version:  3.0.0\r\nHDF5 version:      1.8.4-patch1\r\nNumPy version:     1.7.1\r\nNumexpr version:   2.1 (not using Intel\'s VML/MKL)\r\nZlib version:      1.2.3.4 (in Python interpreter)\r\nLZO version:       2.03 (Apr 30 2008)\r\nBlosc version:     1.2.3 (2013-05-17)\r\nCython version:    0.17.2\r\nPython version:    2.7.3 (default, Jun 21 2012, 07:50:29) \r\n[GCC 4.4.5]\r\nPlatform:          linux2-x86_64\r\nByte-ordering:     little\r\nDetected cores:    12\r\nDefault encoding:  ascii\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\n\r\n```\r\n\r\nsee here: https://github.com/pydata/pandas/issues/5913\r\n\r\nNarrowed it down to this:\r\n\r\ncreate a table with a larger ``expectedrows`` that actually storing\r\ncreate an index on a column\r\nselect via a pretty small start/stop range (e.g. in the below example if you use a chunksize of 1M, then it doesn\'t show up, but 500k makes it fail).\r\n\r\nIf I don\'t pass ``expectedrows``, then this works as expected!\r\n\r\n\r\nCode to reproduce:\r\n```\r\nimport numpy as np\r\nimport tables\r\n\r\ndef pr(result):\r\n    print np.unique(result[\'o\'])\r\n\r\nn = 7000000\r\narr = np.zeros((n,),dtype=[(\'index\', \'i8\'), (\'o\', \'i8\'), (\'value\', \'f8\')])\r\narr[\'index\'] = np.arange(n)\r\narr[\'o\'] = np.random.randint(-20000,-15000,size=n)\r\narr[\'value\'] = value = np.random.randn(n)\r\n\r\nhandle = tables.openFile(\'test.h5\',\'w\',filters=tables.Filters(complevel=9,complib=\'blosc\'))\r\nnode   = handle.createGroup(handle.root, \'foo\')\r\ntable  = handle.createTable(node, \'table\', dict(\r\n    index   = tables.Int64Col(),\r\n    o   = tables.Int64Col(),\r\n    value  = tables.FloatCol(shape=())),\r\n                            expectedrows=10000000)\r\n\r\ntable.cols.index.createIndex()\r\ntable.append(arr)\r\nhandle.close()\r\n\r\nv1 = np.unique(arr[\'o\'])[0]\r\nv2 = np.unique(arr[\'o\'])[1]\r\nselector = \'((o == %s) | (o == %s))\' % (v1, v2)\r\nprint ""selecting values: %s"" % selector\r\n\r\nhandle = tables.openFile(\'test.h5\',\'a\')\r\ntable  = handle.root.foo.table\r\n\r\nprint ""select entire table""\r\npr(table.readWhere(selector))\r\n\r\nprint ""index the column o""\r\ntable.cols.o.createIndex()\r\n\r\nprint ""select via chunks""\r\ncs = 500000\r\nchunks = n / cs\r\nfor i in range(chunks):\r\n    pr(table.readWhere(selector, start=i*cs,stop=(i+1)*cs))\r\n\r\nhandle.close()\r\n```\r\n\r\nOutput; the output for each  chunk should be ``[-20000, -19999]``; extraneous values are being selected that\r\nare not in the selection spec\r\n```\r\n[-20000 -19999]\r\n[sheep-jreback-~/pandas] python test.py\r\nselecting values: ((o == -20000) | (o == -19999))\r\nselect entire table\r\n[-20000 -19999]\r\nindex the column o\r\nselect via chunks\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999 -18154]\r\n[-20000 -19999]\r\n[-20000 -19999 -15413]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n```'"
PyTables/PyTables#441,71946428,FrancescAlted,FrancescAlted,2015-04-29 18:09:47,2015-04-30 17:41:19,2015-04-29 18:27:15,closed,,,3,,https://api.github.com/repos/PyTables/PyTables/issues/441,b'Another bug in an indexed query',"b'This is an example based in [pandas\' issue #9676](https://github.com/pydata/pandas/issues/9676#issuecomment-96771578):\r\n\r\n```\r\nimport numpy as np\r\nimport tables\r\n\r\ntables.copy_file(""bug-idx.h5"", ""bug-idx-copy.h5"", overwrite=True)\r\nh5 = tables.open_file(""bug-idx-copy.h5"", ""a"")\r\no = h5.root.table\r\nvals = o.cols.path[:]\r\nnpvals = set(np.where(vals == 6)[0])\r\n\r\n# Setting the chunkshape is critical for reproducing the bug\r\nt = o.copy(newname=""table2"", chunkshape=2730)\r\nt.cols.path.create_index()\r\n\r\nindexed = set(r.nrow for r in t.where(\'path == 6\'))\r\nprint ""numpy/indexed query:"", len(npvals), len(indexed)\r\ndiffs = sorted(npvals - indexed)\r\nprint ""ndiff:"", len(diffs), diffs\r\n```\r\n\r\nwhere the data file is in https://github.com/PyTables/PyTables/blob/release-3.2.0/tables/tests/bug-idx.h5\r\n\r\nThe output for the above example is:\r\n\r\n```\r\nnumpy/indexed query: 2972 2892\r\nndiff: 80 [46424, 46425, 46426, 46427, 46824, 46825, 46826, 46827, 47224, 47225, 47226, 47227, 47624, 47625, 47626, 47627, 48024, 48025, 48026, 48027, 48424, 48425, 48426, 48427, 48824, 48825, 48826, 48827, 49224, 49225, 49226, 49227, 49624, 49625, 49626, 49627, 50024, 50025, 50026, 50027, 50424, 50425, 50426, 50427, 50824, 50825, 50826, 50827, 51224, 51225, 51226, 51227, 51624, 51625, 51626, 51627, 139624, 139625, 139626, 139627, 140024, 140025, 140026, 140027, 140424, 140425, 140426, 140427, 140824, 140825, 140826, 140827, 141224, 141225, 141226, 141227, 141624, 141625, 141626, 141627]\r\n```\r\n\r\nbut the `ndiff` should be 0.'"
numpy/numpy#1063,7725178,numpy-gitbot,numpy-gitbot,2012-10-19 18:53:03,2014-05-13 01:53:02,2012-10-19 18:53:04,closed,,1.0.2 Release,2,11 - Bug;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/1063,"b""ndarray's mean method should be computed using double precision (Trac #465)""","b'_Original ticket http://projects.scipy.org/numpy/ticket/465 on 2007-03-07 by @chanley, assigned to unknown._\n\nThe default data type for the accumulator variable in the mean method should be double precision.  The problem can best be illustrated with the following example:\n\n\n\n    Python 2.4.3 (#2, Dec  7 2006, 11:01:45) \n    [GCC 4.0.1 (Apple Computer, Inc. build 5367)] on darwin\n    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n    >>> import numpy as n\n    >>> n.__version__\n    \'1.0.2.dev3571\'\n    >>> a = n.ones((1000,1000),dtype=n.float32)*132.00005\n    >>> print a\n    [[ 132.00004578  132.00004578  132.00004578 ...,  132.00004578\n       132.00004578  132.00004578]\n     [ 132.00004578  132.00004578  132.00004578 ...,  132.00004578\n       132.00004578  132.00004578]\n     [ 132.00004578  132.00004578  132.00004578 ...,  132.00004578\n       132.00004578  132.00004578]\n     ..., \n     [ 132.00004578  132.00004578  132.00004578 ...,  132.00004578\n       132.00004578  132.00004578]\n     [ 132.00004578  132.00004578  132.00004578 ...,  132.00004578\n       132.00004578  132.00004578]\n     [ 132.00004578  132.00004578  132.00004578 ...,  132.00004578\n       132.00004578  132.00004578]]\n    >>> a.min()\n    132.000045776\n    >>> a.max()\n    132.000045776\n    >>> a.mean()\n    133.96639999999999\n\n\n\nHaving the mean be greater than the maximum is a tad odd.\n\nThe calculation of the mean is occurring with a single precision accumulator variable.  A user can force a double precision calculation with the following command and receive a correct result:\n\n\n\n    >>> a.mean(dtype=n.float64)\n    132.00004577636719\n    >>> \n\n\n\nHowever, this is not going to be obvious to the casual user and will appear to be an error.\n\nI realize that one reason for not doing all calculations as double precision is performance.  However, it is probably better to always receive the correct answer than to quickly arrive at the wrong one.\n\nThe current default behavior needs to be changed.  All calculations should be done in double precision.  If performance is needed the ""expert user"" can go back and start setting data types after having shown that their application arrives at a correct result.\n\nNot having to worry about overflow problems in the accumulator variable would also make numpy consistent with numarray\'s behavior.\n\n'"
numpy/numpy#4694,33239237,Lysovenko,njsmith,2014-05-10 14:44:20,2014-05-13 03:21:07,2014-05-10 14:50:08,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/4694,b'Numerical stability',"b""It is no numerical stability in the NumPy (version1.6.2 but perhaps also in future fersions ).\r\n<pre>\r\n\r\n>>> d=array([ 253.,  253.,  253.,  253.,  253.,  253.,  252.,  253.,  252.,\r\n        252.,  253.,  253.,  252.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  252.,  253.,  253.,  252.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  254.,  253.,  253.,  253.,  252.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  252.,  251.,  254.,  254.,  254.,\r\n        252.,  252.,  253.,  253.,  254.,  253.,  253.,  253.,  254.,\r\n        253.,  252.,  253.,  254.,  253.,  252.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  252.,  253.,  253.,  253.,  252.,  253.,\r\n        254.,  252.,  252.,  253.,  253.,  253.,  253.,  253.,  252.,\r\n        253.,  253.,  253.,  252.,  253.,  253.,  253.,  252.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  254.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  252.,  253.,\r\n        253.,  253.,  252.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  252.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  252.,  253.,  253.,  253.,  253.,  253.,  253.,  254.,\r\n        253.,  253.,  253.,  253.,  253.,  254.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  252.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        254.,  253.,  253.,  253.,  253.,  254.,  253.,  253.,  253.,\r\n        253.,  254.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  254.,  254.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        254.,  254.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  254.,  254.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  254.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  254.,  253.,  253.,  253.,\r\n        253.,  252.,  252.,  253.,  253.,  253.,  253.,  254.,  253.,\r\n        253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,  253.,\r\n        252.,  253.,  253.,  253.,  253.,  252.,  253.,  253.,  253.,\r\n        253.,  252.,  253.,  253.,  252.,  252.,  252.,  252.,  252.,\r\n        253.,  252.,  253.,  252.,  253.,  253.,  252.,  253.,  253.,\r\n        252.,  252.,  252.,  252.,  252.,  253.,  253.,  253.,  252.,\r\n        252.,  251.,  252.,  252.,  252.,  252.,  252.,  252.,  253.,\r\n        252.,  252.,  252.,  252.,  252.,  252.,  253.,  253.,  253.,\r\n        252.,  252.,  252.,  253.,  253.,  252.], dtype=float32)\r\n>>> d.sum() / len(d) == d.mean()\r\nTrue\r\n# it is wrong: when used huge number of numbers last digits can be lost\r\n>>> (d ** 2).mean() - d.mean() ** 2\r\n-0.072070897236699238\r\n# unreal bullshit happens: dispersion can't be negative\r\n>>> x = 0.\r\n>>> for i, v in enumerate(d, 1):\r\n...  x += (v - x) / i\r\n... \r\n>>> x\r\n252.90442890442881\r\n>>> d.mean()\r\n252.9044289044289\r\n'# x is the numerically stable d.mean()\r\n>>> y = 0.\r\n>>> for i, v in enumerate(d ** 2, 1):\r\n...  y += (v - y) / i\r\n... \r\n>>> y\r\n63960.853146853144\r\n>>> (d ** 2).mean()\r\n63960.578088578091\r\n>>> y - x ** 2\r\n0.20298737785924459\r\n>>> d.std() ** 2\r\n0.20298681725988854\r\n# at least d.std() have some stability\r\n</pre>"""
numpy/numpy#2448,7731472,numpy-gitbot,charris,2012-10-19 22:27:26,2014-07-01 16:38:39,2014-02-20 18:53:23,closed,,,10,06 - Task;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2448,b'Numerical-stable sum (similar to math.fsum) (Trac #1855)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/1855 on 2011-06-02 by trac user ling, assigned to unknown._\n\nFor some applications, having a numerical-stable sum is critical.  What I mean can be well illustrated in the examples below:\n\n>>> math.fsum([1, 1e-10] * 1000), numpy.sum([1, 1e-10] * 1000)\n(1000.0000001, 1000.0000001000171)\n>>> math.fsum([1, 1e100, 1, -1e100] * 10000), numpy.sum([1, 1e100, 1, -1e100] * 10000)\n(20000.0, 0.0)\n\nHere math.fsum is a numerical stable sum introduced in Python 2.6, which uses the Shewchuk algorithm (seems to be described in http://code.activestate.com/recipes/393090/).\n\nOther complaints about the plain vanilla numpy.sum could be found in a recent thread: http://article.gmane.org/gmane.comp.python.numeric.general/42756.\n\nI hope numpy.sum can also implement something similar (e.g., Shewchuk algorithm or Kahan summation algorithm or partial sum).  We could add an optional argument ""stable=True/False"" to numpy.sum and let the user decides whether they want a more accuracy (but potentially slower) version.'"
numpy/numpy#1033,7725114,numpy-gitbot,numpy-gitbot,2012-10-19 18:51:34,2014-05-13 01:53:02,2012-10-19 18:51:35,closed,,,1,11 - Bug;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/1033,b'numpy.mean(): accumulator default type should not be single precision (Trac #435)',"b""_Original ticket http://projects.scipy.org/numpy/ticket/435 on 2007-01-24 by @chanley, assigned to unknown._\n\nThe accumulator used in the mean algorithm should not be single precision by default.  This default can cause unexpected results.  Please see the following example:\n\n\n\n    In [5]: a.dtype\n    Out[5]: dtype('>f4')\n    \n    In [6]: print a\n    [[ 132.  132.  132. ...,  132.  132.  132.]\n     [ 132.  132.  132. ...,  132.  132.  132.]\n     [ 132.  132.  132. ...,  132.  132.  132.]\n     ..., \n     [ 132.  132.  132. ...,  132.  132.  132.]\n     [ 132.  132.  132. ...,  132.  132.  132.]\n     [ 132.  132.  132. ...,  132.  132.  132.]]\n    \n    In [7]: a.min()\n    Out[7]: 132.0\n    \n    In [8]: a.max()\n    Out[8]: 389.0\n    \n    In [9]: a.mean()\n    Out[9]: 129.742439153\n\n\n\nHowever, if you recast the array as float64 you get the correct result:\n\n\n    In [11]: a.astype(numpy.float64).mean()\n    Out[11]: 132.062805059\n\n\n\nI believe that double precision would be a more appropriate default type for the accumulator.\n"""
numpy/numpy#5562,57415755,shoyer,,2015-02-12 04:23:38,2015-02-19 13:37:18,None,open,,,1,,https://api.github.com/repos/numpy/numpy/issues/5562,b'NumPy scalars are not collections.Hashable on Python 3',"b'xref: https://github.com/pydata/pandas/issues/9276\r\n\r\nFor example:\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import collections\r\n\r\nIn [3]: isinstance(3.14, collections.Hashable)\r\nOut[3]: True\r\n\r\nIn [4]: isinstance(np.float64(3.14), collections.Hashable)\r\nOut[4]: False\r\n```\r\n\r\nOn Python 2.7, the last output here is `True` (which is the correct value)'"
jmcnamara/XlsxWriter#204,52805337,drorata,jmcnamara,2014-12-24 08:50:18,2015-01-06 06:54:24,2014-12-30 16:50:13,closed,jmcnamara,,7,question;ready to close,https://api.github.com/repos/jmcnamara/XlsxWriter/issues/204,b'Formatting a column',"b""I fail to format a column when exporting from pandas.DataFrame. Consider the following minimal example:\r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas import ExcelWriter\r\ndf =pd.DataFrame([[123456,2],[3,4]], columns=['First column', 'Second column'])\r\nwriter = ExcelWriter('test.xlsx', engine = 'xlsxwriter')\r\ndf.to_excel(writer)\r\nformater = writer.book.add_format({'num_format': '#,##0'})\r\nsheet = writer.book.worksheets()[0]\r\nsheet.write('F5',1234.8888, formater)\r\nsheet.set_column(1,1, 10, formater)\r\nwriter.save()\r\n```\r\n\r\nAs you can note, the number in cell F5 is well-formatted, but the number in B2 (in column 1) is not formatted (and the width of the column is not changed).\r\n\r\nIn contrast, a similar approach works when it doesn't come from Pandas:\r\n\r\n```python\r\nimport xlsxwriter\r\n\r\n# Create a workbook and add a worksheet.\r\nworkbook = xlsxwriter.Workbook('Expenses01.xlsx')\r\nworksheet = workbook.add_worksheet()\r\n\r\n# Some data we want to write to the worksheet.\r\nexpenses = (\r\n    ['Rent', 1000],\r\n    ['Gas',   100],\r\n    ['Food',  300],\r\n    ['Gym',    50],\r\n)\r\n\r\n# Start from the first cell. Rows and columns are zero indexed.\r\nrow = 0\r\ncol = 0\r\n\r\n# Iterate over the data and write it out row by row.\r\nfor item, cost in (expenses):\r\n    worksheet.write(row, col,     item)\r\n    worksheet.write(row, col + 1, cost)\r\n    row += 1\r\n\r\nworksheet.set_column(1,1,10,workbook.add_format({'bold': True}))\r\nworkbook.close()\r\n```\r\nWhat am I doing wrong?\r\n\r\nThanks in advance,\r\nDror"""
jmcnamara/XlsxWriter#207,53299797,rschell,jmcnamara,2015-01-03 09:55:02,2015-01-21 17:00:02,2015-01-21 17:00:02,closed,jmcnamara,,3,question;ready to close,https://api.github.com/repos/jmcnamara/XlsxWriter/issues/207,b'Datetime export not working correctly from pandas',"b""Using version 0.6.5 with Pandas 0.15.2 I'm using to_excel to export some dataframes which have a datatime variable as the first column.  I noticed the the exported spreadsheet values only contained daily values when I was expecting hourly values.  I traced to problem to the utility.py module and the  datetime_to_excel_datetime function.  Please check the equation at line 630 it appears to me to be missing delta.hours (and delta.minutes?) portion of the datetime value.  \r\n\r\nThanks for providing this excel interface,  I have tried others and xlsxwriter fulfills my requirements the best."""
marians/openweather#9,52591006,scls19fr,,2014-12-21 09:41:15,2014-12-24 09:27:36,None,open,,,3,,https://api.github.com/repos/marians/openweather/issues/9,b'Python Pandas DataFrame output',"b""Hello,\r\n\r\nIt will be nice if `get_historic_weather` could output Pandas DataFrame:\r\n\r\nHere is a very basic code\r\n\r\n    import pandas as pd\r\n\r\n    data = ow.get_historic_weather(station_id, start_date, end_date)\r\n    df = pd.DataFrame(data)\r\n    df['dt'] = pd.to_datetime(df['dt'], unit='s') # convert unix timestamp to datetime\r\n    df = df.set_index('dt') # dt is now index of DataFrame\r\n\r\nSo plotting temperature is now very easy:\r\n\r\n    import matplotlib.pyplot as plt\r\n    df['temp'].map(lambda d: d['ma'] - 273.15).plot()\r\n    plt.show()\r\n\r\nBut that not enough because ideally each column of this dataframe should also be split according dict keys.\r\n\r\nMoreover using Pandas Daframe it will be very easy to output CSV or anything else (database table, HDF5, Excel file...)\r\nsee http://pandas.pydata.org/pandas-docs/dev/io.html\r\n\r\nKind regards"""
bokeh/bokeh#1556,52036869,damianavila,damianavila,2014-12-15 20:50:05,2014-12-16 02:37:24,2014-12-16 02:37:19,closed,damianavila,0.7.1,6,reso: completed;type: bug,https://api.github.com/repos/bokeh/bokeh/issues/1556,b'Travis CI failures',"b'We have two problems today hitting us at the same time:\r\n\r\nThe first one, after update to python 2.7.9, we have a problem in certificates trying to open url from s3 to download the sample data:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""bokeh/sampledata/__init__.py"", line 72, in download\r\n    _getfile(base_url, file_name, data_dir, progress=progress)\r\n  File ""bokeh/sampledata/__init__.py"", line 78, in _getfile\r\n    url = urlopen(file_url)\r\n  File ""/home/travis/miniconda/lib/python2.7/urllib2.py"", line 154, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File ""/home/travis/miniconda/lib/python2.7/urllib2.py"", line 431, in open\r\n    response = self._open(req, data)\r\n  File ""/home/travis/miniconda/lib/python2.7/urllib2.py"", line 449, in _open\r\n    \'_open\', req)\r\n  File ""/home/travis/miniconda/lib/python2.7/urllib2.py"", line 409, in _call_chain\r\n    result = func(*args)\r\n  File ""/home/travis/miniconda/lib/python2.7/urllib2.py"", line 1240, in https_open\r\n    context=self._context)\r\n  File ""/home/travis/miniconda/lib/python2.7/urllib2.py"", line 1197, in do_open\r\n    raise URLError(err)\r\nurllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)>\r\n```\r\n\r\nThe second problem is with the update of pandas to 0.15.2, throwing an TypeError in the burtin example:\r\n\r\n```python\r\n>>> Testing /home/travis/build/bokeh/bokeh/examples/plotting/file/burtin.py ...\r\nTraceback (most recent call last):\r\n  File ""/home/travis/miniconda/lib/python3.3/site-packages/pandas/core/ops.py"", line 468, in na_op\r\n    raise_on_error=True, **eval_kwargs)\r\n  File ""/home/travis/miniconda/lib/python3.3/site-packages/pandas/computation/expressions.py"", line 218, in evaluate\r\n    **eval_kwargs)\r\n  File ""/home/travis/miniconda/lib/python3.3/site-packages/pandas/computation/expressions.py"", line 71, in _evaluate_standard\r\n    return op(a, b)\r\nTypeError: can\'t multiply sequence by non-int of type \'float\'\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 10, in <module>\r\n  File ""burtin.py"", line 73, in <module>\r\n    angles = np.pi/2 - big_angle/2 - df.index.to_series()*big_angle\r\n  File ""/home/travis/miniconda/lib/python3.3/site-packages/pandas/core/ops.py"", line 531, in wrapper\r\n    return left._constructor(wrap_results(na_op(lvalues, rvalues)),\r\n  File ""/home/travis/miniconda/lib/python3.3/site-packages/pandas/core/ops.py"", line 478, in na_op\r\n    result[mask] = op(x[mask], y)\r\nTypeError: can\'t multiply sequence by non-int of type \'float\'\r\n[FAIL] /home/travis/build/bokeh/bokeh/examples/plotting/file/burtin.py\r\n```\r\n\r\nThe second seems a problem reading the initial atb data because the resulting index is now a multi-index...\r\n\r\nAbout the first one, any idea about how to attack the problem? (maybe related with the prolem detailed in the main FD channel?) pinging @mattpap because he know about the s3 data... '"
neurokernel/neurokernel#28,51929602,lebedov,lebedov,2014-12-14 19:56:21,2014-12-15 15:41:21,2014-12-15 15:41:21,closed,,,1,bug,https://api.github.com/repos/neurokernel/neurokernel/issues/28,b'Possible regression in Pandas 0.15.2 affects validation of connections in Pattern class',b'See https://github.com/pydata/pandas/issues/9075; Pattern validation should be modified to avoid using the MultiIndex.has_duplicates property for the time being.'
dateutil/dateutil#5,51526679,jarondl,,2014-12-10 07:21:21,2016-03-08 23:47:07,None,open,,,5,bug;documentation;time zones,https://api.github.com/repos/dateutil/dateutil/issues/5,b'pandas claim we broke the API',"b""Pandas claim we broke the API in 2.3,\r\nsee:  pydata/pandas#8639 pydata/pandas#9021 pydata/pandas#9036 pydata/pandas#9047\r\nand: https://travis-ci.org/jreback/pandas/jobs/43281972\r\n\r\n1. We should understand exactly what's broken. I am still not sure.\r\n2. We should fix in the next release.\r\n3. We should write specific tests to avoid this in the future\r\n\r\n/CC @jreback @jlec @jorisvandenbossche \r\n\r\n"""
,,,,,,,,,,,,,,
nilmtk/nilmtk#246,50411421,nipunbatra,JackKelly,2014-11-29 01:33:10,2014-12-01 13:56:49,2014-12-01 13:08:06,closed,,v0.3,5,bug,https://api.github.com/repos/nilmtk/nilmtk/issues/246,b'`fraction_per_meter` gives error when run second time or afterwards (after caching done)',"b'Code\r\n```python\r\nfraction = elec.submeters().fraction_per_meter().dropna()\r\nlabels = elec.get_appliance_labels(fraction.index)\r\nplt.figure(figsize=(8,8))\r\nfraction.plot(kind=\'pie\', labels=labels);\r\n```\r\n\r\n\r\nTraceback\r\n\r\n```python\r\n1/10 ElecMeter(instance=2, building=11, dataset=\'WikiEnergy\', appliances=[Appliance(type=\'air conditioner\', instance=1)])Using cached result.\r\nCalculating energy for column (\'power\', \'active\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-683776b415f6> in <module>()\r\n----> 1 fraction = elec.submeters().fraction_per_meter().dropna()\r\n      2 labels = elec.get_appliance_labels(fraction.index)\r\n      3 plt.figure(figsize=(8,8))\r\n      4 fraction.plot(kind=\'pie\', labels=labels);\r\n\r\n/home/nipun/git/nilmtk/nilmtk/metergroup.pyc in fraction_per_meter(self, **load_kwargs)\r\n    895         Each value is a float in the range [0,1].\r\n    896         """"""\r\n--> 897         energy_per_meter = self.energy_per_meter(**load_kwargs).max()\r\n    898         total_energy = energy_per_meter.sum()\r\n    899         return energy_per_meter / total_energy\r\n\r\n/home/nipun/git/nilmtk/nilmtk/metergroup.pyc in energy_per_meter(self, **load_kwargs)\r\n    885             print(\'\\r{:d}/{:d} {}\'.format(i+1, n_meters, meter), end=\'\')\r\n    886             stdout.flush()\r\n--> 887             meter_energy = meter.total_energy(**load_kwargs)\r\n    888             energy_per_meter[meter.identifier] = meter_energy\r\n    889         return energy_per_meter.dropna(how=\'all\')\r\n\r\n/home/nipun/git/nilmtk/nilmtk/elecmeter.pyc in total_energy(self, **loader_kwargs)\r\n    446         nodes = [Clip, TotalEnergy]\r\n    447         return self._get_stat_from_cache_or_compute(\r\n--> 448             nodes, TotalEnergy.results_class(), loader_kwargs)\r\n    449 \r\n    450     def dropout_rate(self, **loader_kwargs):\r\n\r\n/home/nipun/git/nilmtk/nilmtk/elecmeter.pyc in _get_stat_from_cache_or_compute(self, nodes, results_obj, loader_kwargs)\r\n    545 \r\n    546             # Merge cached results with newly computed\r\n--> 547             results_obj.update(computed_result.results)\r\n    548 \r\n    549             # Save to disk newly computed stats\r\n\r\n/home/nipun/git/nilmtk/nilmtk/results.pyc in update(self, new_result)\r\n    102                             .format(self.__class__))\r\n    103 \r\n--> 104         self._data = self._data.append(new_result._data, verify_integrity=True)\r\n    105         self._data.sort_index(inplace=True)\r\n    106         self.check_for_overlap()\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc in append(self, other, ignore_index, verify_integrity)\r\n   3780             to_concat = [self, other]\r\n   3781         return concat(to_concat, ignore_index=ignore_index,\r\n-> 3782                       verify_integrity=verify_integrity)\r\n   3783 \r\n   3784     def join(self, other, on=None, how=\'left\', lsuffix=\'\', rsuffix=\'\',\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\r\n    720                        keys=keys, levels=levels, names=names,\r\n    721                        verify_integrity=verify_integrity,\r\n--> 722                        copy=copy)\r\n    723     return op.get_result()\r\n    724 \r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\r\n    852         self.copy = copy\r\n    853 \r\n--> 854         self.new_axes = self._get_new_axes()\r\n    855 \r\n    856     def get_result(self):\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_new_axes(self)\r\n    917                 new_axes[i] = ax\r\n    918 \r\n--> 919         new_axes[self.axis] = self._get_concat_axis()\r\n    920         return new_axes\r\n    921 \r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_concat_axis(self)\r\n    974                                                   self.levels, self.names)\r\n    975 \r\n--> 976         self._maybe_check_integrity(concat_axis)\r\n    977 \r\n    978         return concat_axis\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _maybe_check_integrity(self, concat_index)\r\n    983                 overlap = concat_index.get_duplicates()\r\n    984                 raise ValueError(\'Indexes have overlapping values: %s\'\r\n--> 985                                 % str(overlap))\r\n    986 \r\n    987 \r\n\r\nValueError: Indexes have overlapping values: <class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2014-04-01 05:00:00]\r\nLength: 1, Freq: None, Timezone: None\r\n```\r\n\r\nThe code runs correctly the first time. Subsequent runs throw this issue. I suspect something related to cache may be the issue?'"
numpy/numpy#5326,50458929,cournape,charris,2014-11-30 14:13:15,2014-11-30 17:14:23,2014-11-30 17:14:13,closed,,,1,,https://api.github.com/repos/numpy/numpy/issues/5326,b'BUG: fix collections.Hashable behaviour for numpy arrays.',"b'While `hash(a)` raises an exception as expected for an array `a`, `a.__hash__()` does not, and `isinstance(a, collections.IsHashable)` returns True.\r\n\r\nThis fixes the issuew/ one trivial test.'"
nilmtk/nilmtk#246,50411421,nipunbatra,JackKelly,2014-11-29 01:33:10,2014-12-01 13:56:49,2014-12-01 13:08:06,closed,,v0.3,5,bug,https://api.github.com/repos/nilmtk/nilmtk/issues/246,b'`fraction_per_meter` gives error when run second time or afterwards (after caching done)',"b'Code\r\n```python\r\nfraction = elec.submeters().fraction_per_meter().dropna()\r\nlabels = elec.get_appliance_labels(fraction.index)\r\nplt.figure(figsize=(8,8))\r\nfraction.plot(kind=\'pie\', labels=labels);\r\n```\r\n\r\n\r\nTraceback\r\n\r\n```python\r\n1/10 ElecMeter(instance=2, building=11, dataset=\'WikiEnergy\', appliances=[Appliance(type=\'air conditioner\', instance=1)])Using cached result.\r\nCalculating energy for column (\'power\', \'active\')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-683776b415f6> in <module>()\r\n----> 1 fraction = elec.submeters().fraction_per_meter().dropna()\r\n      2 labels = elec.get_appliance_labels(fraction.index)\r\n      3 plt.figure(figsize=(8,8))\r\n      4 fraction.plot(kind=\'pie\', labels=labels);\r\n\r\n/home/nipun/git/nilmtk/nilmtk/metergroup.pyc in fraction_per_meter(self, **load_kwargs)\r\n    895         Each value is a float in the range [0,1].\r\n    896         """"""\r\n--> 897         energy_per_meter = self.energy_per_meter(**load_kwargs).max()\r\n    898         total_energy = energy_per_meter.sum()\r\n    899         return energy_per_meter / total_energy\r\n\r\n/home/nipun/git/nilmtk/nilmtk/metergroup.pyc in energy_per_meter(self, **load_kwargs)\r\n    885             print(\'\\r{:d}/{:d} {}\'.format(i+1, n_meters, meter), end=\'\')\r\n    886             stdout.flush()\r\n--> 887             meter_energy = meter.total_energy(**load_kwargs)\r\n    888             energy_per_meter[meter.identifier] = meter_energy\r\n    889         return energy_per_meter.dropna(how=\'all\')\r\n\r\n/home/nipun/git/nilmtk/nilmtk/elecmeter.pyc in total_energy(self, **loader_kwargs)\r\n    446         nodes = [Clip, TotalEnergy]\r\n    447         return self._get_stat_from_cache_or_compute(\r\n--> 448             nodes, TotalEnergy.results_class(), loader_kwargs)\r\n    449 \r\n    450     def dropout_rate(self, **loader_kwargs):\r\n\r\n/home/nipun/git/nilmtk/nilmtk/elecmeter.pyc in _get_stat_from_cache_or_compute(self, nodes, results_obj, loader_kwargs)\r\n    545 \r\n    546             # Merge cached results with newly computed\r\n--> 547             results_obj.update(computed_result.results)\r\n    548 \r\n    549             # Save to disk newly computed stats\r\n\r\n/home/nipun/git/nilmtk/nilmtk/results.pyc in update(self, new_result)\r\n    102                             .format(self.__class__))\r\n    103 \r\n--> 104         self._data = self._data.append(new_result._data, verify_integrity=True)\r\n    105         self._data.sort_index(inplace=True)\r\n    106         self.check_for_overlap()\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc in append(self, other, ignore_index, verify_integrity)\r\n   3780             to_concat = [self, other]\r\n   3781         return concat(to_concat, ignore_index=ignore_index,\r\n-> 3782                       verify_integrity=verify_integrity)\r\n   3783 \r\n   3784     def join(self, other, on=None, how=\'left\', lsuffix=\'\', rsuffix=\'\',\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\r\n    720                        keys=keys, levels=levels, names=names,\r\n    721                        verify_integrity=verify_integrity,\r\n--> 722                        copy=copy)\r\n    723     return op.get_result()\r\n    724 \r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\r\n    852         self.copy = copy\r\n    853 \r\n--> 854         self.new_axes = self._get_new_axes()\r\n    855 \r\n    856     def get_result(self):\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_new_axes(self)\r\n    917                 new_axes[i] = ax\r\n    918 \r\n--> 919         new_axes[self.axis] = self._get_concat_axis()\r\n    920         return new_axes\r\n    921 \r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_concat_axis(self)\r\n    974                                                   self.levels, self.names)\r\n    975 \r\n--> 976         self._maybe_check_integrity(concat_axis)\r\n    977 \r\n    978         return concat_axis\r\n\r\n/home/nipun/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _maybe_check_integrity(self, concat_index)\r\n    983                 overlap = concat_index.get_duplicates()\r\n    984                 raise ValueError(\'Indexes have overlapping values: %s\'\r\n--> 985                                 % str(overlap))\r\n    986 \r\n    987 \r\n\r\nValueError: Indexes have overlapping values: <class \'pandas.tseries.index.DatetimeIndex\'>\r\n[2014-04-01 05:00:00]\r\nLength: 1, Freq: None, Timezone: None\r\n```\r\n\r\nThe code runs correctly the first time. Subsequent runs throw this issue. I suspect something related to cache may be the issue?'"
numpy/numpy#4102,23784339,seberg,charris,2013-12-05 12:38:44,2014-10-30 14:11:13,2013-12-20 23:16:06,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/4102,b'FIX: Array split should not hack empty array shape away',"b'Also fixes that it hacked the dtype away (""closes gh-612"").\r\nI really don\'t see *any* point for that kludge comment, the\r\nshape should stay there for concatenation, etc. to work.\r\n\r\n--\r\n\r\nI guess this probably needs a futurewarning, so anyone agrees I will do that first.'"
numpy/numpy#5246,47126712,immerrr,,2014-10-29 09:32:16,2014-10-29 17:16:11,None,open,,,2,,https://api.github.com/repos/numpy/numpy/issues/5246,b'BUG?: priority inversion when comparing scalar to non-ndarray casts it to rank-0 array',"b'Here\'s how it can be reproduced:\r\n```python\r\nIn [1]:        \r\nclass A(object):\r\n    __array_priority__ = 1000\r\n    def __radd__(self, other):\r\n        print ""__radd__"", type(other), other\r\n    def __gt__(self, other):\r\n        print ""__gt__"", type(other), other\r\n   ...:         \r\n\r\nIn [2]: a = A()\r\n```\r\n\r\nComparison (type == `numpy.ndarray`):\r\n```python\r\nIn [3]: np.int64(1) < a\r\n__gt__ <type \'numpy.ndarray\'> 1\r\n```\r\n\r\nAddition (type is `numpy.int64`):\r\n```python\r\nIn [4]: np.int64(1) + a\r\n__radd__ <type \'numpy.int64\'> 1\r\n```\r\n\r\nThis happens on current master:\r\n```python\r\nIn [5]: np.__version__\r\nOut[5]: \'1.10.0.dev-3b22d87\'\r\n```\r\n\r\nIs this a bug?'"
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
dask/dask#1174,156365730,davclark,jcrist,2016-05-23 21:00:08,2016-07-20 20:49:45,2016-07-20 20:49:45,closed,,,11,dataframe;io,https://api.github.com/repos/dask/dask/issues/1174,b'read_hdf fails in dask even though it works in pandas',"b'dask version 0.9.0, pandas 0.18.1 (most recent from conda as of posting)\r\n\r\nGrab a 1 MB fake TAQ file from [here](https://github.com/dlab-projects/marketflow/raw/master/test-data/small_test_data_public.h5). (aside: same data is ~1/4 MB in zipped fixed width - chunk sizes are probably dumb for this data.)\r\n\r\n`pd.read_hdf(\'small_test_data_public.h5\', \'/IXQAJE/no_suffix\')` works, `dask.dataframe.read_hdf(\'small_test_data_public.h5\', \'/IXQAJE/no_suffix\')` fails with the following stack trace. I think it may be due to the attempt to read an empty dataframe of 0-length. If I read the intent correctly, it would probably make more sense to retrieve a pytables or h5py object which would provide the desired metadata without the weirdness around a 0-length read.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-110-8ad7ff0d4733> in <module>()\r\n----> 1 spy_dd = dd.read_hdf(fname, max_sym)\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in read_hdf(pattern, key, start, stop, columns, chunksize, lock)\r\n    559                                     columns=columns, chunksize=chunksize,\r\n    560                                     lock=lock)\r\n--> 561                    for path in paths])\r\n    562 \r\n    563 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in <listcomp>(.0)\r\n    559                                     columns=columns, chunksize=chunksize,\r\n    560                                     lock=lock)\r\n--> 561                    for path in paths])\r\n    562 \r\n    563 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in _read_single_hdf(path, key, start, stop, columns, chunksize, lock)\r\n    499     from .multi import concat\r\n    500     return concat([one_path_one_key(path, k, start, s, columns, chunksize, lock)\r\n--> 501                    for k, s in zip(keys, stops)])\r\n    502 \r\n    503 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in <listcomp>(.0)\r\n    499     from .multi import concat\r\n    500     return concat([one_path_one_key(path, k, start, s, columns, chunksize, lock)\r\n--> 501                    for k, s in zip(keys, stops)])\r\n    502 \r\n    503 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/dask/dataframe/io.py in one_path_one_key(path, key, start, stop, columns, chunksize, lock)\r\n    474         not contain any wildcards).\r\n    475         """"""\r\n--> 476         empty = pd.read_hdf(path, key, stop=0)\r\n    477         if columns is not None:\r\n    478             empty = empty[columns]\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in read_hdf(path_or_buf, key, **kwargs)\r\n    328                                  \'multiple datasets.\')\r\n    329             key = keys[0]\r\n--> 330         return store.select(key, auto_close=auto_close, **kwargs)\r\n    331     except:\r\n    332         # if there is an error, close the store\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\r\n    678                            chunksize=chunksize, auto_close=auto_close)\r\n    679 \r\n--> 680         return it.get_result()\r\n    681 \r\n    682     def select_as_coordinates(\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in get_result(self, coordinates)\r\n   1362 \r\n   1363         # directly return the result\r\n-> 1364         results = self.func(self.start, self.stop, where)\r\n   1365         self.close()\r\n   1366         return results\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in func(_start, _stop, _where)\r\n    671             return s.read(start=_start, stop=_stop,\r\n    672                           where=_where,\r\n--> 673                           columns=columns, **kwargs)\r\n    674 \r\n    675         # create the iterator\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/io/pytables.py in read(self, where, columns, **kwargs)\r\n   4052 \r\n   4053             block = make_block(values, placement=np.arange(len(cols_)))\r\n-> 4054             mgr = BlockManager([block], [cols_, index_])\r\n   4055             frames.append(DataFrame(mgr))\r\n   4056 \r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/core/internals.py in __init__(self, blocks, axes, do_integrity_check, fastpath)\r\n   2592 \r\n   2593         if do_integrity_check:\r\n-> 2594             self._verify_integrity()\r\n   2595 \r\n   2596         self._consolidate_check()\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/core/internals.py in _verify_integrity(self)\r\n   2802         for block in self.blocks:\r\n   2803             if block._verify_integrity and block.shape[1:] != mgr_shape[1:]:\r\n-> 2804                 construction_error(tot_items, block.shape[1:], self.axes)\r\n   2805         if len(self.items) != tot_items:\r\n   2806             raise AssertionError(\'Number of manager items must equal union of \'\r\n\r\n/home/dav/miniconda3/envs/TAQ/lib/python3.5/site-packages/pandas/core/internals.py in construction_error(tot_items, block_shape, axes, e)\r\n   3966         raise e\r\n   3967     if block_shape[0] == 0:\r\n-> 3968         raise ValueError(""Empty data passed with indices specified."")\r\n   3969     raise ValueError(""Shape of passed values is {0}, indices imply {1}"".format(\r\n   3970         passed, implied))\r\n\r\nValueError: Empty data passed with indices specified.\r\n```'"
PyTables/PyTables#409,51364776,guziy,FrancescAlted,2014-12-08 22:59:13,2015-04-19 06:43:02,2015-04-18 18:55:31,closed,,3.2,5,defect,https://api.github.com/repos/PyTables/PyTables/issues/409,b'table.where query does not seem to be able to find rows...',"b'Hi:\r\n\r\nI am creating a big table but I cannot find the rows which are supposed to be there using ```table.where```. But ```table.get_where_list()``` does find the rows in question (see the screenshot, **Note that there is no output as a result of the loop in table.where**). \r\n\r\n![screen shot 2014-12-08 at 17 58 36](https://cloud.githubusercontent.com/assets/900941/5349065/df8c3dca-7f03-11e4-8c41-1b14faf5a40b.png)\r\n\r\n\r\nI was trying to reproduce it with smaller dummy files but no luck... Even when I select a table using ptrepack, the problem goes away... \r\n\r\nNot indexing columns also seems to fix the problem, except in this case querying is very slow....\r\n'"
PyTables/PyTables#319,25509942,jreback,FrancescAlted,2014-01-13 16:41:42,2015-04-19 06:42:02,2015-04-18 18:54:56,closed,,3.2,10,defect,https://api.github.com/repos/PyTables/PyTables/issues/319,b'BUG: possible indexing/selection bug',"b'```\r\nIn [10]: tables.print_versions()\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\nPyTables version:  3.0.0\r\nHDF5 version:      1.8.4-patch1\r\nNumPy version:     1.7.1\r\nNumexpr version:   2.1 (not using Intel\'s VML/MKL)\r\nZlib version:      1.2.3.4 (in Python interpreter)\r\nLZO version:       2.03 (Apr 30 2008)\r\nBlosc version:     1.2.3 (2013-05-17)\r\nCython version:    0.17.2\r\nPython version:    2.7.3 (default, Jun 21 2012, 07:50:29) \r\n[GCC 4.4.5]\r\nPlatform:          linux2-x86_64\r\nByte-ordering:     little\r\nDetected cores:    12\r\nDefault encoding:  ascii\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\n\r\n```\r\n\r\nsee here: https://github.com/pydata/pandas/issues/5913\r\n\r\nNarrowed it down to this:\r\n\r\ncreate a table with a larger ``expectedrows`` that actually storing\r\ncreate an index on a column\r\nselect via a pretty small start/stop range (e.g. in the below example if you use a chunksize of 1M, then it doesn\'t show up, but 500k makes it fail).\r\n\r\nIf I don\'t pass ``expectedrows``, then this works as expected!\r\n\r\n\r\nCode to reproduce:\r\n```\r\nimport numpy as np\r\nimport tables\r\n\r\ndef pr(result):\r\n    print np.unique(result[\'o\'])\r\n\r\nn = 7000000\r\narr = np.zeros((n,),dtype=[(\'index\', \'i8\'), (\'o\', \'i8\'), (\'value\', \'f8\')])\r\narr[\'index\'] = np.arange(n)\r\narr[\'o\'] = np.random.randint(-20000,-15000,size=n)\r\narr[\'value\'] = value = np.random.randn(n)\r\n\r\nhandle = tables.openFile(\'test.h5\',\'w\',filters=tables.Filters(complevel=9,complib=\'blosc\'))\r\nnode   = handle.createGroup(handle.root, \'foo\')\r\ntable  = handle.createTable(node, \'table\', dict(\r\n    index   = tables.Int64Col(),\r\n    o   = tables.Int64Col(),\r\n    value  = tables.FloatCol(shape=())),\r\n                            expectedrows=10000000)\r\n\r\ntable.cols.index.createIndex()\r\ntable.append(arr)\r\nhandle.close()\r\n\r\nv1 = np.unique(arr[\'o\'])[0]\r\nv2 = np.unique(arr[\'o\'])[1]\r\nselector = \'((o == %s) | (o == %s))\' % (v1, v2)\r\nprint ""selecting values: %s"" % selector\r\n\r\nhandle = tables.openFile(\'test.h5\',\'a\')\r\ntable  = handle.root.foo.table\r\n\r\nprint ""select entire table""\r\npr(table.readWhere(selector))\r\n\r\nprint ""index the column o""\r\ntable.cols.o.createIndex()\r\n\r\nprint ""select via chunks""\r\ncs = 500000\r\nchunks = n / cs\r\nfor i in range(chunks):\r\n    pr(table.readWhere(selector, start=i*cs,stop=(i+1)*cs))\r\n\r\nhandle.close()\r\n```\r\n\r\nOutput; the output for each  chunk should be ``[-20000, -19999]``; extraneous values are being selected that\r\nare not in the selection spec\r\n```\r\n[-20000 -19999]\r\n[sheep-jreback-~/pandas] python test.py\r\nselecting values: ((o == -20000) | (o == -19999))\r\nselect entire table\r\n[-20000 -19999]\r\nindex the column o\r\nselect via chunks\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999 -18154]\r\n[-20000 -19999]\r\n[-20000 -19999 -15413]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n```'"
,,,,,,,,,,,,,,
matplotlib/matplotlib#3569,43947313,tacaswell,,2014-09-25 17:53:23,2014-09-27 20:37:08,2014-09-27 20:37:08,closed,,v1.4.1,3,Release critical,https://api.github.com/repos/matplotlib/matplotlib/issues/3569,b'boxplot stats regression on empty data',b'See https://github.com/pydata/pandas/issues/8382'
matplotlib/matplotlib#3571,44132401,tacaswell,mdboom,2014-09-27 03:10:41,2014-09-27 20:09:28,2014-09-27 19:47:44,closed,,v1.4.1,0,needs_review;needs_revision;Release critical,https://api.github.com/repos/matplotlib/matplotlib/issues/3571,b'BUG : deal with empty list passed to boxplot',"b""If a data list is empty, return a dict full of np.nan.\r\n\r\nCloses #3569 and addresses part of https://github.com/pydata/pandas/issues/8382\r\n\r\n4th time's the charm on this commit.\r\n\r\nNeeds testing."""
biocore/scikit-bio#700,45050574,jairideout,gregcaporaso,2014-10-06 23:33:54,2014-10-07 00:40:59,2014-10-07 00:40:59,closed,gregcaporaso,0.2.1: IO subpackage,1,,https://api.github.com/repos/biocore/scikit-bio/issues/700,b'Support newest matplotlib release',"b""The newest matplotlib release (1.4.0) had its boxplot functionality largely rewritten/refactored. There were a few unintended API/behavior changes that affect the boxplotting functionality in skbio (`skbio.draw.boxplots`). The most noticeable change is that a `ValueError` is raised if an empty distribution is provided; in the past, this type of input was supported.\r\n\r\nThis bug has been fixed in the latest development version of matplotlib (https://github.com/matplotlib/matplotlib/pull/3571). It was first noticed in pandas (https://github.com/pydata/pandas/issues/8382) and they worked around this issue by passing `np.array([np.nan])` instead of an empty sequence (https://github.com/pydata/pandas/pull/8240). We use their approach here in order support plotting empty distributions in pre- and post-1.4.0 matplotlib releases. If we decide to only support matplotlib > 1.4.0 at some point in the future, this NaN hack can be removed.\r\n\r\nSome of the existing boxplot test code had to be updated to work in either matplotlib version; this was due to changes in the fliers returned by `pyplot.boxplot` (https://github.com/matplotlib/matplotlib/issues/3544). I also added (very ugly) test code to test that the NaN hack appears to be working; this will hopefully catch changes to matplotlib's boxplot functionality in the future if the hack stops working as intended.\r\n\r\nI updated Travis to test against matplotlib < 1.4.0 and whatever the latest release is, in order to make sure that our code works with either version.\r\n\r\nFixes #649."""
quantopian/pyfolio#119,100966794,twiecki,twiecki,2015-08-14 09:03:23,2015-08-14 18:32:30,2015-08-14 18:32:04,closed,,,4,,https://api.github.com/repos/quantopian/pyfolio/issues/119,b'MAINT Change format of cached SPY and Fama-French factors to csv inst\xa1\xad',b'\xa1\xadead of hdf. Also fixes some more UTC issues.'
pydata/patsy#47,40253165,janschulz,janschulz,2014-08-14 13:21:30,2015-03-21 19:54:49,2015-03-21 19:54:49,closed,,,28,,https://api.github.com/repos/pydata/patsy/issues/47,b'Fix: Add compat code for pd.Categorical in pandas>=0.15',"b""pandas renamed pd.Categorical.labels to pd.Categorical.codes. It's\r\nalso now possible to have Categoricals as blocks, so Series can contain\r\nCategoricals."""
statsmodels/statsmodels#1884,40275357,janschulz,josef-pkt,2014-08-14 16:59:08,2014-08-24 18:41:29,2014-08-14 19:57:43,closed,,,3,,https://api.github.com/repos/statsmodels/statsmodels/issues/1884,b'Fix: Add compat code for pd.Categorical in pandas>=0.15',
blaze/blaze#458,40065750,mrocklin,mrocklin,2014-08-12 15:18:24,2014-09-24 01:19:12,2014-09-24 01:19:12,closed,,,20,question,https://api.github.com/repos/blaze/blaze/issues/458,b'Consistent column naming scheme',"b'What should the following return?\r\n\r\n    by(t, t.amount > 0, t.id.count()).columns\r\n\r\nSo two questions:\r\n\r\n1.  What is the name of `t.amount > 0`\r\n2.  What is the name of `t.id.count()`\r\n\r\nCurrently `t.amount > 0` has no name, this is an issue.  Pandas would name it as `0`, `1`, ....\r\n\r\nCurrently `t.id.count()` has the name of `id_count`\r\n\r\n'"
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
numpy/numpy#4779,34992167,jreback,,2014-06-04 19:09:23,2014-06-05 13:42:48,None,open,,,2,,https://api.github.com/repos/numpy/numpy/issues/4779,b'arr.dtype.type has different hashes',"b""Not sure if this is supposed to be like this.\r\n\r\noriginal issue here: https://github.com/pydata/pandas/issues/7332\r\ncross post to numexpr here: https://code.google.com/p/numexpr/issues/detail?id=126\r\n\r\nessentially in pandas were looking up a ``dtype,type`` in a cython dictionary\r\n\r\nturns out that for ``int64`` (and ``int32`` but NOT ``int64`` on 32-bit platforms), the hashes are DIFFERENT, but same for other dtypes (including ``float64``).\r\n\r\nIs this maybe an implementation detail on ``numexpr`` and/or incorrect usage of ``dtype.type`` and/or invalid guarantees on this object?\r\n\r\nFYI, we switched to using ``dtype.name`` for the lookup and no issues.\r\n\r\n```\r\nIn [20]: import numexpr as ne\r\nIn [21]: import numpy as np\r\nIn [22]: ne.__version__\r\nOut[22]: '2.4'\r\nIn [23]: np.__version__\r\nOut[23]: '1.8.1'\r\n\r\nIn [24]: a = np.arange(10,dtype='int64')\r\nIn [25]: b = np.arange(10,dtype='int64')\r\n\r\nIn [26]: result_ne = ne.evaluate('a+b')\r\nIn [27]: result_numpy = a+b\r\n\r\nIn [28]: (result_ne == result_numpy).all()\r\nOut[28]: True\r\n\r\nIn [29]: result_ne.dtype.type\r\nOut[29]: numpy.int64\r\n\r\nIn [30]: result_numpy.dtype.type\r\nOut[30]: numpy.int64\r\n\r\nIn [31]: hash(result_ne.dtype.type)\r\nOut[31]: 8768103730016\r\n\r\nIn [32]: hash(result_numpy.dtype.type)\r\nOut[32]: 8768103729990\r\n```\r\n\r\nFor the floats the same though\r\n```\r\nIn [1]: a = np.arange(10.)\r\n\r\nIn [2]: b = np.arange(10.)\r\n\r\nn [4]: hash(ne.evaluate('a+b').dtype.type)\r\nOut[4]: 8751212391216\r\n\r\nIn [5]: hash((a+b).dtype.type)\r\nOut[5]: 8751212391216\r\n```"""
ulmo-dev/ulmo#88,34967487,wilsaj,wilsaj,2014-06-04 15:06:34,2015-01-05 20:37:49,2015-01-05 20:37:49,closed,,,5,,https://api.github.com/repos/ulmo-dev/ulmo/issues/88,b'pandas 0.14 support',"b""Right now we're limiting to <0.14 due to https://github.com/pydata/pandas/issues/7284\r\n\r\nI'm inclined to wait until 0.14.1 since the issue is already fixed in pandas."""
statsmodels/statsmodels#1762,35773395,detzom,josef-pkt,2014-06-16 07:29:00,2014-06-16 12:34:56,2014-06-16 12:34:43,closed,,,2,type-invalid,https://api.github.com/repos/statsmodels/statsmodels/issues/1762,b'Unable to build',"b'statsmodels is needed by another github project that I use ""etsy/skyline"", and I\'m still unable to build your project.\r\n\r\nI have the same error either using PIP, EASY_INSTALL, python setup.py build, from 0.5 release and actual git release.\r\n\r\n\r\nThere is the output : \r\n\r\n\r\n\r\npython2.6 setup.py build\r\nTraceback (most recent call last):\r\n  File ""setup.py"", line 388, in <module>\r\n    check_dependency_versions(min_versions)\r\n  File ""setup.py"", line 117, in check_dependency_versions\r\n    from pandas.version import short_version as pversion\r\n  File ""/usr/lib64/python2.6/site-packages/pandas/__init__.py"", line 45, in <module>\r\n    from pandas.io.api import *\r\n  File ""/usr/lib64/python2.6/site-packages/pandas/io/api.py"", line 7, in <module>\r\n    from pandas.io.excel import ExcelFile, ExcelWriter, read_excel\r\n  File ""/usr/lib64/python2.6/site-packages/pandas/io/excel.py"", line 626, in <module>\r\n    .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))\r\nValueError: zero length field name in format'"
kwgoodman/bottleneck#83,32152225,jreback,kwgoodman,2014-04-24 14:17:47,2014-07-08 22:04:36,2014-07-08 22:04:36,closed,,,6,,https://api.github.com/repos/kwgoodman/bottleneck/issues/83,b'BUG: nansum platform overflow',"b""this is on 32-bit linux\r\non 64-bit the the int dtypes work correctly\r\nstems from here\r\n\r\nhttps://github.com/numpy/numpy/issues/4638\r\nhttps://github.com/pydata/pandas/issues/6915\r\n\r\nworkaround with numpy is to do arithmetic in highest dtype, e.g. ``values.sum(dtype='float64')`` then cast back\r\n```\r\n>>> import numpy as np\r\n>>> import bottleneck as bn\r\n>>> bn.__version__\r\n'0.8.0'\r\n>>> np.__version__\r\n'1.8.1'\r\n\r\n>>> float(bn.nansum(np.arange(5000000,dtype='float32')))\r\n12499997949952.0\r\n>>> float(bn.nansum(np.arange(5000000,dtype='float64')))\r\n12499997500000.0\r\n>>> int(bn.nansum(np.arange(5000000,dtype='int32')))\r\n1642668640\r\n>>> int(bn.nansum(np.arange(5000000,dtype='int64')))\r\n12499997500000L\r\n\r\n```"""
numpy/numpy#4638,32147261,jreback,charris,2014-04-24 13:17:07,2014-04-24 14:32:54,2014-04-24 14:32:54,closed,,,6,,https://api.github.com/repos/numpy/numpy/issues/4638,b'BUG: overflow on integer ops on 32-bit',"b""numpy 1.8.1\r\n\r\n32-bit linux\r\n```\r\n(Pdb) np.arange(5249571,dtype='int64').sum()\r\n13778995217235\r\n(Pdb) np.arange(5249571,dtype='int32').sum()\r\n740131667\r\n(Pdb) np.arange(5249571,dtype='int32').mean()\r\n2624785.0\r\n(Pdb) np.arange(5249571,dtype='int64').mean()\r\n2624785.0\r\n```\r\n\r\n64-bit linux\r\n```\r\nIn [2]: np.arange(5249571,dtype='int64').sum()\r\nOut[2]: 13778995217235\r\n\r\nIn [3]: np.arange(5249571,dtype='int32').sum()\r\nOut[3]: 13778995217235\r\n\r\nIn [4]: np.arange(5249571,dtype='int32').mean()\r\nOut[4]: 2624785.0\r\n\r\nIn [5]: np.arange(5249571,dtype='int64').mean()\r\nOut[5]: 2624785.0\r\n```\r\n\r\nSo it seems that when the ``int32`` overflows it should then upcast to ``int64`` if needed; interesting that this works on 64-bit.\r\n\r\nrelated issue: https://github.com/pydata/pandas/issues/6915"""
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
numpy/numpy#4328,27881912,jreback,juliantaylor,2014-02-19 15:14:24,2014-02-27 16:32:44,2014-02-19 17:41:42,closed,seberg,,13,,https://api.github.com/repos/numpy/numpy/issues/4328,b'REGR: indexing issue with numpy scalars on 32-bit linux',"b""On numpy master\r\n\r\nONLY on 32-linux versions!\r\n\r\nworks fine on  numpy 1.8.0\r\nfound originally here: https://groups.google.com/forum/#!topic/pydata/90QNz8EY74Q\r\npandas issue: https://github.com/pydata/pandas/issues/6410\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\npython: 2.7.3.final.0\r\npython-bits: 32\r\nOS: Linux\r\nOS-release: 3.2.0-23-generic-pae\r\nmachine: i686\r\nprocessor: i686\r\nbyteorder: little\r\nLC_ALL: en_US\r\nLANG: en_US\r\n```\r\n```\r\nIn [21]: import numpy as np\r\n\r\nIn [22]: np.__version__\r\nOut[22]: '1.9.0.dev-8997167'\r\n\r\nIn [23]: values = np.array([946684800000000000, 946684800000000000], dtype=np.int64)\r\n\r\nIn [24]: indexer = np.array([0], dtype=np.int64)\r\n\r\nIn [25]: value = np.int64(946684800000000000)\r\n\r\nIn [26]: values[indexer] = value\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-26-bb506aca2290> in <module>()\r\n----> 1 values[indexer] = value\r\n\r\nTypeError: Iterator operand 0 dtype could not be cast from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n\r\n```"""
,,,,,,,,,,,,,,
ContinuumIO/ArrayManagement#7,31030281,hhuuggoo,quasiben,2014-04-07 23:06:32,2014-06-13 06:05:51,2014-04-09 14:13:53,closed,,,3,,https://api.github.com/repos/ContinuumIO/ArrayManagement/issues/7,b'added backwards compatability for pandas term parsing so we can work wit...',"b'@quasiben @spearsem \r\n\r\npandas 0.13 breaks old style pytables query syntax, I ripped out one of their backwards compat functions so we could use it\r\n\r\nfew changes\r\n- it\'s an instance method, I made it a non-instance method\r\n- for some reason ""{0}"".format(x) does not work for numpy datetimes, so I had to call str(x) on it first\r\n\r\nThe reason this is necessary, is that pandas fixed their backwards compatibility logic with the old query syntax in 0.14, but 0.13 is out and is broken, pydata/pandas#6313\r\n\r\n'"
jreback/pandas#8,26931991,TomAugspurger,jreback,2014-02-05 01:38:21,2014-06-16 02:27:42,2014-02-05 01:39:14,closed,,,1,,https://api.github.com/repos/jreback/pandas/issues/8,b'extra test for GH5684',b'additional test for https://github.com/pydata/pandas/issues/5684'
statsmodels/statsmodels#1377,27203275,juliantaylor,jseabold,2014-02-08 16:25:29,2014-05-22 16:58:01,2014-02-20 21:27:52,closed,,,2,,https://api.github.com/repos/statsmodels/statsmodels/issues/1377,b'TestAnova2.test_results fails with pandas 0.13.1',"b'with pandas 0.13.1, cython 0.20, numpy 1.7.1 and 1.8.0\r\n\r\nit does not seem to happen with pandas 0.12, maybe they changed how NaN are handled as the element which is different originates from NaN inputs.\r\n\r\n```\r\n======================================================================\r\nFAIL: __main__.TestAnova2.test_results\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""statsmodels/stats/tests/test_anova.py"", line 149, in test_results\r\n    np.testing.assert_equal(results[\'df\'].values, Df)\r\n  File ""/usr/lib/pymodules/python2.7/numpy/testing/utils.py"", line 257, in assert_equal\r\n    return assert_array_equal(actual, desired, err_msg, verbose)\r\n  File ""/usr/lib/pymodules/python2.7/numpy/testing/utils.py"", line 719, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""/usr/lib/pymodules/python2.7/numpy/testing/utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 25.0%)\r\n x: array([ 1.,  2.,  2.,  0.])\r\n y: array([ 1,  2,  2, 51])\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 8.314s\r\n```'"
wabu/pandas#1,27562810,jreback,,2014-02-13 23:45:21,2014-06-27 18:37:08,None,open,,,0,,https://api.github.com/repos/wabu/pandas/issues/1,b'HDFStore unconvert string fix',b'https://github.com/pydata/pandas/pull/6166\r\n\r\ncherry-pick these 2 (or you can just copy its not long)\r\nhttps://github.com/jreback/pandas/commit/6c268de1be38a239e9a114aaf1723a85e44146f2\r\nhttps://github.com/jreback/pandas/commit/33cb7fb8f1db54597e4e9b9653a78f611e954dac\r\n\r\nand update your PR\r\n\r\nThis works on windows 3.3 now...see if it passes travis'
numpy/numpy#2485,7731526,numpy-gitbot,seberg,2012-10-19 22:28:57,2014-01-29 11:27:42,2013-12-16 23:22:58,closed,,,10,11 - Bug;component: Other;priority: normal,https://api.github.com/repos/numpy/numpy/issues/2485,"b""object arrays converted to string arrays of 'S' dtype have default length. (Trac #1892)""","b""_Original ticket http://projects.scipy.org/numpy/ticket/1892 on 2011-07-04 by trac user ehiggs, assigned to unknown._\n\nI'm sure this must have been reported before, but I can't find it in the bug tracker. The bug is that calling a vectorized function on a string type truncates the string to '|S8' regardless of the original length.\n\nExample:\n\nIn [1]: import numpy\n\nIn [2]: def __f(x):\n\n   ...:     return x  \n\n   ...: \n\nIn [3]: f = numpy.vectorize(__f)\n\nIn [4]: s = '2011-07-04'\n\nIn [5]: y = f(s)\n\nIn [6]: y  \n\nOut[6]: \narray('2011-07-04', \n      dtype='|S10')\n\nIn [8]: z = f(numpy.array([s,s,s,s]))\n\nIn [9]: z\nOut[9]: \narray(['2011-07-', '2011-07-', '2011-07-', '2011-07-'], \n      dtype='|S8')\n\nIf this has been raised, please merge this in so I can track progress on the original."""
,,,,,,,,,,,,,,
numpy/numpy#4250,26718205,y-p,y-p,2014-01-31 21:49:46,2014-02-24 23:05:26,None,open,,,11,11 - Bug;component: numpy.core;priority: normal,https://api.github.com/repos/numpy/numpy/issues/4250,b'std() corner case behaves differently on different windows pythons',"b""xref https://github.com/pydata/pandas/issues/6136\r\n\r\na failing test led us to a case where using `.std(ddof=1)` on a singleton array \r\nreturns some infinity on py2.6.6/64/np1.8.0 but returns `nan` on other windows\r\npythons and and in linux (at least as far as we know).\r\n\r\nThis may just be a facts-of-life numerical corner case. Reporting just in case. \r\nI have data to reliably reproduce the issue.\r\n\r\nThe numpy binary is from cgohlke's collection."""
PyTables/PyTables#328,25886672,jreback,FrancescAlted,2014-01-19 19:41:13,2015-04-27 09:34:19,2015-04-27 09:34:18,closed,,3.2,8,defect,https://api.github.com/repos/PyTables/PyTables/issues/328,b'BUG: can fileno() return -1?',"b""https://github.com/pydata/pandas/issues/5999\r\n\r\nwe have this 'feature' where a user can manually call fsync\r\n\r\nI can't repro this but I think this is happening with 3.1rc1 (which Debian guys are testing with)\r\n\r\nI am working around but not sure if this is a valid return code?"""
python-excel/xlrd#78,25865191,jmcnamara,cjw296,2014-01-18 18:19:36,2014-06-21 17:12:12,2014-01-26 19:36:27,closed,,,1,,https://api.github.com/repos/python-excel/xlrd/issues/78,b'Added function to convert Excel date to datetime.',"b'This relates to issue #76 and the initial pull request #77.\r\n\r\nI\'ve added a function, `xldate.xldate_as_datetime()`, to return a `datetime.datetime` object from an Excel serial date/time.\r\n\r\nThis would be useful for anyone interested in preserving milli/microseconds from an Excel date and would allow us to close Pandas issue: pydata/pandas/issues/5945.\r\n\r\nThe PR includes tests based on dates and time extracted from Excel files. The function is very slightly slower than the `xldate.xldate_as_tuple()` function:\r\n\r\n```python\r\n$ python\r\nPython 2.7.2 (default, Feb 19 2013, 23:47:31)\r\n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>>\r\n>>> from xlrd import xldate\r\n>>> import timeit\r\n>>>\r\n>>> xldate.xldate_as_tuple(41657.75165, 0)\r\n(2014, 1, 18, 18, 2, 23)\r\n>>>\r\n>>> xldate.xldate_as_datetime(41657.75165, 0)\r\ndatetime.datetime(2014, 1, 18, 18, 2, 22, 560000)\r\n>>>\r\n>>> timeit.timeit(\'xldate.xldate_as_tuple(   41657.75165, 0)\', setup=\'from xlrd import xldate\', number=170000)\r\n1.045557975769043\r\n>>>\r\n>>> timeit.timeit(\'xldate.xldate_as_datetime(41657.75165, 0)\', setup=\'from xlrd import xldate\', number=170000)\r\n1.0717370510101318\r\n>>>\r\n```\r\nI didn\'t update the docs because I wasn\'t sure if they are auto-generated or not. If you need me to update them I can do that too.\r\n\r\nJohn'"
PyTables/PyTables#319,25509942,jreback,FrancescAlted,2014-01-13 16:41:42,2015-04-19 06:42:02,2015-04-18 18:54:56,closed,,3.2,10,defect,https://api.github.com/repos/PyTables/PyTables/issues/319,b'BUG: possible indexing/selection bug',"b'```\r\nIn [10]: tables.print_versions()\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\nPyTables version:  3.0.0\r\nHDF5 version:      1.8.4-patch1\r\nNumPy version:     1.7.1\r\nNumexpr version:   2.1 (not using Intel\'s VML/MKL)\r\nZlib version:      1.2.3.4 (in Python interpreter)\r\nLZO version:       2.03 (Apr 30 2008)\r\nBlosc version:     1.2.3 (2013-05-17)\r\nCython version:    0.17.2\r\nPython version:    2.7.3 (default, Jun 21 2012, 07:50:29) \r\n[GCC 4.4.5]\r\nPlatform:          linux2-x86_64\r\nByte-ordering:     little\r\nDetected cores:    12\r\nDefault encoding:  ascii\r\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\n\r\n```\r\n\r\nsee here: https://github.com/pydata/pandas/issues/5913\r\n\r\nNarrowed it down to this:\r\n\r\ncreate a table with a larger ``expectedrows`` that actually storing\r\ncreate an index on a column\r\nselect via a pretty small start/stop range (e.g. in the below example if you use a chunksize of 1M, then it doesn\'t show up, but 500k makes it fail).\r\n\r\nIf I don\'t pass ``expectedrows``, then this works as expected!\r\n\r\n\r\nCode to reproduce:\r\n```\r\nimport numpy as np\r\nimport tables\r\n\r\ndef pr(result):\r\n    print np.unique(result[\'o\'])\r\n\r\nn = 7000000\r\narr = np.zeros((n,),dtype=[(\'index\', \'i8\'), (\'o\', \'i8\'), (\'value\', \'f8\')])\r\narr[\'index\'] = np.arange(n)\r\narr[\'o\'] = np.random.randint(-20000,-15000,size=n)\r\narr[\'value\'] = value = np.random.randn(n)\r\n\r\nhandle = tables.openFile(\'test.h5\',\'w\',filters=tables.Filters(complevel=9,complib=\'blosc\'))\r\nnode   = handle.createGroup(handle.root, \'foo\')\r\ntable  = handle.createTable(node, \'table\', dict(\r\n    index   = tables.Int64Col(),\r\n    o   = tables.Int64Col(),\r\n    value  = tables.FloatCol(shape=())),\r\n                            expectedrows=10000000)\r\n\r\ntable.cols.index.createIndex()\r\ntable.append(arr)\r\nhandle.close()\r\n\r\nv1 = np.unique(arr[\'o\'])[0]\r\nv2 = np.unique(arr[\'o\'])[1]\r\nselector = \'((o == %s) | (o == %s))\' % (v1, v2)\r\nprint ""selecting values: %s"" % selector\r\n\r\nhandle = tables.openFile(\'test.h5\',\'a\')\r\ntable  = handle.root.foo.table\r\n\r\nprint ""select entire table""\r\npr(table.readWhere(selector))\r\n\r\nprint ""index the column o""\r\ntable.cols.o.createIndex()\r\n\r\nprint ""select via chunks""\r\ncs = 500000\r\nchunks = n / cs\r\nfor i in range(chunks):\r\n    pr(table.readWhere(selector, start=i*cs,stop=(i+1)*cs))\r\n\r\nhandle.close()\r\n```\r\n\r\nOutput; the output for each  chunk should be ``[-20000, -19999]``; extraneous values are being selected that\r\nare not in the selection spec\r\n```\r\n[-20000 -19999]\r\n[sheep-jreback-~/pandas] python test.py\r\nselecting values: ((o == -20000) | (o == -19999))\r\nselect entire table\r\n[-20000 -19999]\r\nindex the column o\r\nselect via chunks\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999 -18154]\r\n[-20000 -19999]\r\n[-20000 -19999 -15413]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n[-20000 -19999]\r\n```'"
,,,,,,,,,,,,,,
nilmtk/nilmtk#83,25106422,JackKelly,JackKelly,2014-01-06 15:05:56,2014-07-12 02:42:09,2014-07-12 02:42:09,closed,JackKelly,,1,Building;DataStore and format conversion;design;pre-processing;Statistics and correlations,https://api.github.com/repos/nilmtk/nilmtk/issues/83,b'Use a subclass of DataFrame to store metadata and basic methods',"b'Back in issue #12 ""How to represent a Building in memory?"" we discussed whether or not to subclass `pandas.DataFrame`.\r\n\r\n@jreback very kindly mentioned that [GeoPandas](https://github.com/kjordahl/geopandas) create a subclass of pd.DataFrame called [GeoDataFrame](https://github.com/kjordahl/geopandas/blob/master/geopandas/geodataframe.py#L27) which carries metadata; and that metadata is copied when  methods inherited from `Pandas.DataFrame` like `dropna()` are called.  More info [on the pandas pull request relating to metadata propagation](https://github.com/pydata/pandas/pull/5205); also see https://github.com/pydata/pandas/issues/2485\r\n\r\nThere\'s quite a lot of text below.  If you can\'t be bothered to read all that then here\'s a summary:\r\n\r\n### Summary\r\n\r\n* Subclassing pandas.DataFrame (and having metadata propagate after Pandas methods are called) appears to only work in Pandas-0.13-dev, not the current Pandas release (0.12).  Propagating metadata should work in Pandas 0.13\r\n* My suggestion would be that we don\'t worry about subclassing DataFrame for now.  Even if we did use it now (and hence require users to install Pandas-dev) then I don\'t think it would change our paper much, if at all.\r\n* After Pandas 0.13 is released, let\'s re-visit the idea of subclassing DataFrame.\r\n* Personally, I quite like the idea of storing metadata in a subclass of DataFrame, it feels tidy.\r\n\r\nSome details:\r\n\r\n### Why subclass DataFrame?\r\n\r\n* We currently use one DataFrame per meter (e.g. per appliance meter; or per mains meter) and we store metadata about that channel in a separate dict.  This feels a little fragile, because we have to write the code to ensure that the metadata stays in sync with each DataFrame.  It feels conceptually cleaner to store metadata *inside* the DataFrame object.  right now, in Pandas 0.12, we could do this just by adding a `dataframe._metadata` attribute but, in Pandas 0.12, that `._metadata` attribute will not propagate if we use any Pandas DataFrame method (e.g. `dropna()`).  In Pandas 0.13. that `._metadata` attribute will propagate.  GeoPandas stores metadata in their GeoSeries and GeoDataFrame classes.\r\n* If we kept metadata with the DataFrame then we could have a tidy mechanism for tracking what pre-processing has been applied to each DataFrame.  At the moment, our `preprocessing.electricity.single` functions take a DataFrame as an argument and return a new DataFrame but there is no tidy way to update the metadata associated with the DataFrame to record the fact that that preprocessing function has been run.  Recording the sequence of pre-processing steps done on each DataFrame feels like an important ""open science"" contribution ;)\r\n* We could also record which entries have some uncertainty associated with them (e.g. because these entries have been inserted during pre-processing).\r\n* We could have a tidy place to cache attributes like `sample_period`\r\n* In the Pandas API, Series and DataFrame both have lots of useful stats / pre-processing methods (like dropna(), resample(), max(), describe() etc).  It feels like some of our preprocessing / stats / plotting functions which act on a single DataFrame deserve to be class methods rather than separate functions.\r\n* I\'m not sure, but I think that `_metadata` might automatically be pulled into the HDFStore object.\r\n\r\n### Why did I bother to tinker with subclassing now, where there are other priorities?\r\n\r\nBecause, if we could get subclassing to work tidily now, I\'d advocate implementing subclassing now (which would have implications for some of the code I need to write over the next few days!)\r\n\r\n### Experiments\r\n\r\nI tried running this code on both Pandas 0.12 and Pandas 0.13-DEV:\r\n\r\nI\'ll print the code first, and then the results on Pandas 0.12 and 0.13-DEV:\r\n\r\n#### Code\r\n\r\n```python\r\nfrom __future__ import print_function, division\r\nimport pandas as pd\r\n\r\nclass ElectricDataFrame(pd.DataFrame):\r\n    def __init__(self, *args, **kwargs):\r\n        super(ElectricDataFrame, self).__init__(*args, **kwargs)\r\n        self._metadata = {\'test\': \'TEST\'}\r\n\r\n    def foo(self):\r\n        print(""FOOOO!"")\r\n\r\n    #\r\n    # Implement pandas methods\r\n    #\r\n    @property\r\n    def _constructor(self):\r\n        # Borrowed from GeoPandas.GeoDataFrame._constructor\r\n        print(""_constructor called"")\r\n        return ElectricDataFrame\r\n\r\n    def __finalize__(self, other, method=None, **kwargs):\r\n        """""" propagate metadata from other to self """"""\r\n        # Borrowed from GeoPandas.GeoDataFrame.__finalize__\r\n        # NOTE: backported from pandas master (upcoming v0.13)\r\n        print(""__finalize__ called"")\r\n        for name in self._metadata:\r\n            object.__setattr__(self, name, getattr(other, name, None))\r\n        return self\r\n\r\n    def copy(self, deep=True):\r\n        """"""\r\n        Make a copy of this ElectricDataFrame object\r\n\r\n        Parameters\r\n        ----------\r\n        deep : boolean, default True\r\n            Make a deep copy, i.e. also copy data\r\n\r\n        Returns\r\n        -------\r\n        copy : ElectricDataFrame\r\n        """"""\r\n        # Borrowed from GeoPandas.GeoDataFrame.copy\r\n        # FIXME: this will likely be unnecessary in pandas >= 0.13\r\n        print(""copy called"")\r\n        data = self._data\r\n        if deep:\r\n            data = data.copy()\r\n        return ElectricDataFrame(data).__finalize__(self)\r\n\r\nprint(\'pandas.__version__ =\', pd.__version__)\r\n\r\ndef try_to_get_metadata(df):\r\n    try:\r\n        print(df._metadata)\r\n    except Exception as e:\r\n        print(\'EXCEPTION:\', e)\r\n\r\nedf = ElectricDataFrame([1,2,3,4,5], pd.date_range(\'2010\', freq=\'D\', periods=5))\r\nprint (\'edf._metadata:\', edf._metadata)\r\nprint()\r\nprint (\'edf.resample(rule=\\\'D\\\', how=\\\'max\\\')._metadata:\')\r\ntry_to_get_metadata(edf.resample(rule=\'D\', how=\'max\'))\r\nprint()\r\nprint (\'edf.resample(rule=\\\'D\\\')._metadata:\')\r\ntry_to_get_metadata(edf.resample(rule=\'D\'))\r\nprint()\r\nprint(\'edf.dropna()._metadata:\')\r\ntry_to_get_metadata(edf.dropna())\r\n```\r\n\r\n#### Run on pandas 0.12:\r\n\r\n```python\r\nIn [14]: pandas.__version__ = 0.12.0\r\nedf._metadata: {\'test\': \'TEST\'}\r\n\r\nedf.resample(rule=\'D\', how=\'max\')._metadata:\r\nEXCEPTION: \'DataFrame\' object has no attribute \'_metadata\'\r\n\r\nedf.resample(rule=\'D\')._metadata:\r\nEXCEPTION: \'DataFrame\' object has no attribute \'_metadata\'\r\n\r\nedf.dropna()._metadata:\r\nEXCEPTION: \'DataFrame\' object has no attribute \'_metadata\'\r\n```\r\n\r\n#### Run on pandas 0.13-dev:\r\n\r\n```python\r\nIn [4]: pandas.__version__ = 0.13.0-75-g7d9e9fa\r\nedf._metadata: {\'test\': \'TEST\'}\r\n\r\nedf.resample(rule=\'D\', how=\'max\')._metadata:\r\n[]\r\n\r\nedf.resample(rule=\'D\')._metadata:\r\ncopy called\r\n__finalize__ called\r\n{\'test\': \'TEST\'}\r\n\r\nedf.dropna()._metadata:\r\n_constructor called\r\n__finalize__ called\r\n{\'test\': \'TEST\'}\r\n```'"
numpy/numpy#3243,13163321,seberg,charris,2013-04-13 21:38:19,2014-06-12 13:03:41,2013-06-09 15:11:02,closed,,,16,,https://api.github.com/repos/numpy/numpy/issues/3243,b'Deprecate non integer arguments',"b'This is gh-2891 rebased (with slight modifications in mapping.c). So basically all things said there still apply. To wrap it up, this deprecates non-integer usage almost everywhere (which means strides, shapes, axes, indices, slices), by deprecating it in PyArray_PyIntAsIntp.\r\n\r\nAlso deprecates `__index__` for ndim > 0.\r\n\r\nI did go a bit further and deprecated python bools, too. If this starts bugging someone, that should maybe be made indexing specific.\r\n\r\nI guess this might still need some love. The deprecation tests are not too great, but when errors get replaced it needs to test both warning and error. And they change, because previously they raised DeprecationWarnings (as errors) and now these are usually replaced.'"
,,,,,,,,,,,,,,
yhat/ggplot#118,23789316,ericchiang,glamp,2013-12-05 14:17:44,2013-12-09 23:21:29,2013-12-09 23:21:29,closed,,,11,,https://api.github.com/repos/yhat/ggplot/issues/118,b'Issues generating baseline images',"b""I'm currently working on geom_boxplot 00702da406b4e4268267b4aacb68d08aa7a42feb (see image below for example) and am having difficulties generating the baseline images. I've been attempting the same process as my last geom b2d89eb36384a018e4a73a8409cf0955750b5ecd, but <code>python tests.py -v -d</code> hasn't worked.\r\n\r\nI realize there have been some changes to the test cases (2b64837f84fdfa3a6474d03ed3494c4177a684b5) so my first instinct was to attempt to revert those, to no avail.\r\n\r\nIs anyone else having problems generating baseline images or is it just an issue with my local environment?\r\n\r\n![geom_boxplot_ex](https://f.cloud.github.com/assets/2342749/1683072/bfcc9e76-5db7-11e3-85d2-e35f9708083b.png)"""
quantopian/zipline#243,23670410,jikamens,,2013-12-03 20:14:39,2013-12-03 20:34:23,None,open,,,2,,https://api.github.com/repos/quantopian/zipline/issues/243,"b""get_trading_days doesn't return Friday trading day in tight range""","b""If you pass get_trading_days a time range that includes exactly a Friday, it doesn't return it in the result:\r\n\r\n    (Pdb) get_trading_days(datetime.datetime(2013,11,22,5,0,0,tzinfo=pytz.utc), datetime.datetime(2013,11,23,4,59,59,tzinfo=pytz.utc))\r\n    Index([], dtype=object)\r\n\r\nShift both times exactly a day earlier, and it works just fine:\r\n\r\n    (Pdb) get_trading_days(datetime.datetime(2013,11,21,5,0,0,tzinfo=pytz.utc), datetime.datetime(2013,11,22,4,59,59,tzinfo=pytz.utc))\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2013-11-21 05:00:00]\r\n    Length: 1, Freq: None, Timezone: UTC\r\n\r\nIt also works the day before a non-trading-day that is a holiday rather than a Saturday, so this appears to be somehow unique to how weekends are handled:\r\n\r\n    (Pdb) get_trading_days(datetime.datetime(2013,11,27,5,0,0,tzinfo=pytz.utc), datetime.datetime(2013,11,28,4,59,59,tzinfo=pytz.utc))\r\n    <class 'pandas.tseries.index.DatetimeIndex'>\r\n    [2013-11-27 05:00:00]\r\n    Length: 1, Freq: None, Timezone: UTC\r\n"""
PyTables/PyTables#282,19589791,jreback,scopatz,2013-09-17 00:59:10,2014-01-19 19:27:44,2014-01-06 02:19:33,closed,andreabedini,3.1,11,defect,https://api.github.com/repos/PyTables/PyTables/issues/282,b'BUG: selection on Float64Column gives incorrect results with index and np.nan in 0th row',"b'This is very subtle. It requires a ``np.nan`` in the 0th row of a table AND an index.\r\nIf those conditions are met then a ``readWhere`` selector does not return any results.\r\n\r\nOutput of a small tester program (with the code below)\r\n\r\nThis shows various cases of no nans, nans in the 1st row, 0th row, and last row\r\nboth with and without an index on that column.\r\n\r\nSelection is ``(values >=0.0)`` so all non-nan rows should be selected (e.g. 4 is the correct result with 1 nan).\r\n\r\nPresent in 2.3.1 and 3.0.0\r\nhdf version: 1.8.4-patch\r\nlinux x64 (debian and ubuntu)\r\nnumpy 1.7.1\r\n\r\n```\r\nno_index_no_nan      (  5): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nindex_no_nan         (  5): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nno_index_1st_nan     (  4): [(0, 0.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nindex_1st_nan        (  4): [(0, 0.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nno_index_0th_nan     (  4): [(1, 1.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nindex_0th_nan        (  0): []\r\nno_index_last_nan    (  4): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0)]\r\nindex_last_nan       (  4): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0)]\r\n```\r\n\r\ncode to reproduce\r\n```\r\n#!/usr/local/bin/python\r\n\r\nimport tables\r\nimport numpy as np\r\n\r\ntest_file = \'test_select.h5\'\r\n\r\nhandle = tables.openFile(test_file, ""w"")\r\n\r\ndef test(name, n=None, index=True):\r\n\r\n    node   = handle.createGroup(handle.root, name)\r\n    table  = handle.createTable(node, \'table\', dict(\r\n        index   = tables.Int64Col(),\r\n        values  = tables.FloatCol(shape=()),\r\n        ))\r\n\r\n    r = table.row\r\n    for i in range(5):\r\n        r[\'index\'] = i\r\n        r[\'values\'] = np.nan if n is not None and n == i else i\r\n        r.append()\r\n    table.flush()\r\n\r\n    if index:\r\n        table.cols.values.createIndex()\r\n\r\n    # retrieve\r\n    result = table.readWhere(\'(values >= 0)\')\r\n    print ""%-20.20s (%3d): %s"" % (name,len(result), result)\r\n\r\n# no index, no nan\r\ntest(\'no_index_no_nan\',index=False)\r\ntest(\'index_no_nan\',index=True)\r\ntest(\'no_index_1st_nan\',n=1,index=False)\r\ntest(\'index_1st_nan\',n=1,index=True)\r\ntest(\'no_index_0th_nan\',n=0,index=False)\r\ntest(\'index_0th_nan\',n=0,index=True)\r\ntest(\'no_index_last_nan\',n=4,index=False)\r\ntest(\'index_last_nan\',n=4,index=True)\r\n\r\nhandle.close()\r\n```\r\n'"
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
matplotlib/matplotlib#2356,18766369,ChrisBeaumont,dmcdougall,2013-08-30 01:02:13,2013-11-15 10:32:13,2013-08-31 22:56:25,closed,,,1,,https://api.github.com/repos/matplotlib/matplotlib/issues/2356,b'Bad xlim/ylim when using shared axes subplots and an empty subplot',"b'The following code\r\n```\r\nfig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True)\r\naxes[0].plot([1,2,3])\r\nprint axes[0].get_xlim(), axes[0].get_ylim()\r\nplt.show()\r\n```\r\n\r\noutputs\r\n```\r\n(-0.059999999999999998, 0.059999999999999998) (-0.059999999999999998, 0.059999999999999998)\r\n```\r\n\r\nThe axis limits are only updated to sane values once the second axes is populated with something.\r\n\r\nIf a subplot with shared axes is empty, I expect the view limits to be set according to the non-empty plots'"
matplotlib/matplotlib#2357,18767243,ChrisBeaumont,dmcdougall,2013-08-30 01:39:33,2014-06-13 12:44:33,2013-08-31 22:56:05,closed,,,3,,https://api.github.com/repos/matplotlib/matplotlib/issues/2357,b'Better axis limits when using shared axes and empty subplots',b'This is one possible solution to #2356'
PyTables/PyTables#282,19589791,jreback,scopatz,2013-09-17 00:59:10,2014-01-19 19:27:44,2014-01-06 02:19:33,closed,andreabedini,3.1,11,defect,https://api.github.com/repos/PyTables/PyTables/issues/282,b'BUG: selection on Float64Column gives incorrect results with index and np.nan in 0th row',"b'This is very subtle. It requires a ``np.nan`` in the 0th row of a table AND an index.\r\nIf those conditions are met then a ``readWhere`` selector does not return any results.\r\n\r\nOutput of a small tester program (with the code below)\r\n\r\nThis shows various cases of no nans, nans in the 1st row, 0th row, and last row\r\nboth with and without an index on that column.\r\n\r\nSelection is ``(values >=0.0)`` so all non-nan rows should be selected (e.g. 4 is the correct result with 1 nan).\r\n\r\nPresent in 2.3.1 and 3.0.0\r\nhdf version: 1.8.4-patch\r\nlinux x64 (debian and ubuntu)\r\nnumpy 1.7.1\r\n\r\n```\r\nno_index_no_nan      (  5): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nindex_no_nan         (  5): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nno_index_1st_nan     (  4): [(0, 0.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nindex_1st_nan        (  4): [(0, 0.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nno_index_0th_nan     (  4): [(1, 1.0) (2, 2.0) (3, 3.0) (4, 4.0)]\r\nindex_0th_nan        (  0): []\r\nno_index_last_nan    (  4): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0)]\r\nindex_last_nan       (  4): [(0, 0.0) (1, 1.0) (2, 2.0) (3, 3.0)]\r\n```\r\n\r\ncode to reproduce\r\n```\r\n#!/usr/local/bin/python\r\n\r\nimport tables\r\nimport numpy as np\r\n\r\ntest_file = \'test_select.h5\'\r\n\r\nhandle = tables.openFile(test_file, ""w"")\r\n\r\ndef test(name, n=None, index=True):\r\n\r\n    node   = handle.createGroup(handle.root, name)\r\n    table  = handle.createTable(node, \'table\', dict(\r\n        index   = tables.Int64Col(),\r\n        values  = tables.FloatCol(shape=()),\r\n        ))\r\n\r\n    r = table.row\r\n    for i in range(5):\r\n        r[\'index\'] = i\r\n        r[\'values\'] = np.nan if n is not None and n == i else i\r\n        r.append()\r\n    table.flush()\r\n\r\n    if index:\r\n        table.cols.values.createIndex()\r\n\r\n    # retrieve\r\n    result = table.readWhere(\'(values >= 0)\')\r\n    print ""%-20.20s (%3d): %s"" % (name,len(result), result)\r\n\r\n# no index, no nan\r\ntest(\'no_index_no_nan\',index=False)\r\ntest(\'index_no_nan\',index=True)\r\ntest(\'no_index_1st_nan\',n=1,index=False)\r\ntest(\'index_1st_nan\',n=1,index=True)\r\ntest(\'no_index_0th_nan\',n=0,index=False)\r\ntest(\'index_0th_nan\',n=0,index=True)\r\ntest(\'no_index_last_nan\',n=4,index=False)\r\ntest(\'index_last_nan\',n=4,index=True)\r\n\r\nhandle.close()\r\n```\r\n'"
,,,,,,,,,,,,,,
matplotlib/matplotlib#2419,19427099,cpcloud,cpcloud,2013-09-13 02:21:48,2013-09-13 12:30:27,2013-09-13 12:30:27,closed,,,2,,https://api.github.com/repos/matplotlib/matplotlib/issues/2419,b'extra ticklocs and ticklabels when plotting with bar(log=True) in matplotlib >= 1.3',"b""with matplotlib >= 1.3 i see the following behavior:\r\n\r\n```\r\nIn [6]: s = [200, 500]\r\n\r\nIn [7]: fig = figure()\r\n\r\nIn [8]: ax = fig.add_subplot(111)\r\n\r\nIn [10]: ax.bar([0, 1], s, log=True, bottom=1)\r\nOut[10]: <Container object of 2 artists>\r\n\r\nIn [12]: ax.yaxis.get_ticklocs()\r\nOut[12]: array([     0.1,      1. ,     10. ,    100. ,   1000. ,  10000. ])\r\n\r\nIn [15]: [x.get_text() for x in ax.yaxis.get_ticklabels()]\r\nOut[15]:\r\n['',\r\n '$\\\\mathdefault{10^{0}}$',\r\n '$\\\\mathdefault{10^{1}}$',\r\n '$\\\\mathdefault{10^{2}}$',\r\n '$\\\\mathdefault{10^{3}}$',\r\n '']\r\n```\r\n\r\nThe corresponding plot is:\r\n\r\n![i-can-haz-lawg-plawt](https://f.cloud.github.com/assets/417981/1135621/f3f9e80e-1c1a-11e3-8bcf-c397d99f472d.png)\r\n\r\nWhy are there empty tick labels and tick locs that aren't actually in the plot? Am I missing something? I don't observe this behavior in 1.2.1\r\n\r\n```\r\nIn [7]: [x.get_text() for x in ax.yaxis.get_ticklabels()]\r\nOut[7]:\r\n['$\\\\mathdefault{10^{0}}$',\r\n '$\\\\mathdefault{10^{1}}$',\r\n '$\\\\mathdefault{10^{2}}$',\r\n '$\\\\mathdefault{10^{3}}$']\r\n\r\nIn [8]: ax.yaxis.get_ticklocs()\r\nOut[8]: array([    1.,    10.,   100.,  1000.])\r\n\r\nIn [10]: matplotlib.__version__\r\nOut[10]: '1.2.1'\r\n```"""
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
joblib/joblib#183,55608010,mangecoeur,GaelVaroquaux,2015-01-27 11:48:05,2015-03-17 08:29:06,2015-03-17 07:57:40,closed,,,3,,https://api.github.com/repos/joblib/joblib/issues/183,"b""cannot include dtype 'M' in a buffer""","b""Using memory cache with pandas dataframes with datelike columns results in error `cannot include dtype 'M' in a buffer`\r\n\r\nSuspect it is related to this issue:\r\n\r\nhttps://github.com/pydata/pandas/issues/4533"""
,,,,,,,,,,,,,,
numpy/numpy#3270,13621065,jayvius,charris,2013-04-25 02:46:06,2014-06-17 18:10:27,2013-04-25 23:23:33,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/3270,"b""Fix for astype('S') string truncate issue""","b""Calling astype('S') for an array of string objects results in a string array where dtype='S64', even if the original string objects are longer than 64 characters. Add call to GetParamsFromObject() to determine maximum string object length, and use that as string dtype size. Fixes #2485"""
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
numpy/numpy#1611,7728334,thouis,,2012-10-19 20:43:31,2014-09-30 17:55:16,None,open,,,11,11 - Bug;component: Other;priority: low,https://api.github.com/repos/numpy/numpy/issues/1611,b'Ufunc generates unintuitive AttributeError on object arrays (Trac #1013)',"b'_Original ticket http://projects.scipy.org/numpy/ticket/1013 on 2009-02-20 by @wesm, assigned to unknown._\n\nThis error is very unintuitive for end-users, arrays formed from SQL query results can frequently end up as object arrays by accident.\n\nIn [15]: arr = np.random.randn(100).astype(object)\n\nIn [16]: np.log(arr)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\nH:\\workspace\\Python\\src\\<ipython console> in <module>()\n\nAttributeError: log\n\nSame AttributeError is raised for other ufuncs'"
PyTables/PyTables#265,15149546,jreback,avalentino,2013-06-05 01:14:47,2014-01-22 10:38:17,2013-06-29 11:30:33,closed,avalentino,3.1,16,defect,https://api.github.com/repos/PyTables/PyTables/issues/265,b'BUG/ENH: readWhere failing in 3.0.0',"b'In py3, using 3.0.0, readWhere is raising when passed a condition that touches encoded data in a Table\r\n\r\nfrom the mailing list, reproduced here:\r\n\r\nhttp://sourceforge.net/mailarchive/forum.php?thread_name=CAPk-6T6F%2Bou_u2ozmLHXJf4%3D2cY4wyB3NBm10f1%2B_oWcZB1Hcg%40mail.gmail.com&forum_name=pytables-users\r\n\r\nversions\r\n```\r\nIn [2]: import sys\r\n\r\nIn [3]: sys.version\r\nOut[3]: \'3.3.0 (default, Feb  1 2013, 08:25:35) \\n[GCC 4.4.5]\'\r\n\r\nIn [4]: import numpy as np\r\n\r\nIn [5]: np.__version__\r\nOut[5]: \'1.7.1\'\r\n\r\nIn [6]: import numexpr as ne\r\n\r\nIn [7]: ne.__version__\r\nOut[7]: \'2.1\'\r\n\r\nIn [8]: import tables\r\n\r\nIn [9]: tables.__version__\r\nOut[9]: \'3.0.0\'\r\n```\r\n\r\nSetup\r\n```\r\nIn [10]: encoding = \'UTF-8\'\r\n\r\nIn [11]: test_file = \'test_select.h5\'\r\n\r\nIn [13]: handle = tables.openFile(test_file, ""w"")\r\n\r\nIn [14]: node   = handle.createGroup(handle.root, \'test\')\r\n\r\nIn [15]: table  = handle.createTable(node, \'table\', dict(\r\n   ....:         index   = tables.Int64Col(),\r\n   ....:         column  = tables.StringCol(25),\r\n   ....:         values  = tables.FloatCol(shape=(3)),\r\n   ....:         ))\r\n\r\nIn [17]: r = table.row\r\n\r\nIn [18]: for i in range(10):\r\n   ....:             r[\'index\'] = i\r\n   ....:             r[\'column\'] = (""str-%d"" % (i % 5)).encode(encoding)\r\n   ....:             r[\'values\'] = np.arange(3)\r\n   ....:             r.append()\r\n   ....:     \r\n\r\nIn [19]: table.flush()\r\n\r\nIn [20]: handle.close()\r\n```\r\n\r\nregular read succeeds\r\n```\r\nIn [22]: handle = tables.openFile(test_file,""r"")\r\n\r\nIn [23]: result = handle.root.test.table.read()\r\n\r\nIn [24]: result\r\nOut[24]: \r\narray([(b\'str-0\', 0, [0.0, 1.0, 2.0]), (b\'str-1\', 1, [0.0, 1.0, 2.0]),\r\n       (b\'str-2\', 2, [0.0, 1.0, 2.0]), (b\'str-3\', 3, [0.0, 1.0, 2.0]),\r\n       (b\'str-4\', 4, [0.0, 1.0, 2.0]), (b\'str-0\', 5, [0.0, 1.0, 2.0]),\r\n       (b\'str-1\', 6, [0.0, 1.0, 2.0]), (b\'str-2\', 7, [0.0, 1.0, 2.0]),\r\n       (b\'str-3\', 8, [0.0, 1.0, 2.0]), (b\'str-4\', 9, [0.0, 1.0, 2.0])], \r\n      dtype=[(\'column\', \'S25\'), (\'index\', \'<i8\'), (\'values\', \'<f8\', (3,))])\r\n\r\nIn [25]: result[0]\r\nOut[25]: (b\'str-0\', 0, [0.0, 1.0, 2.0])\r\n\r\nIn [26]: result[0][0]\r\nOut[26]: b\'str-0\'\r\n\r\nIn [27]: type(result[0][0])\r\nOut[27]: numpy.bytes_\r\n```\r\n\r\ntrying a readWhere (fails with a string, or encoded string),\r\nraising in ne\r\n```\r\nIn [28]: selector = ""(column == \'str-2\')""\r\n\r\nIn [29]: type(selector)\r\nOut[29]: builtins.str\r\n\r\nIn [30]: handle.root.test.table.readWhere(selector)\r\nTypeError: string argument without an encoding\r\n\r\nIn [31]: handle.root.test.table.readWhere(selector.encode(encoding))\r\nTypeError: string argument without an encoding\r\n```\r\n\r\ntest with numexpr against in memory data\r\n```\r\nIn [32]: arr = result[\'column\']\r\n\r\nIn [33]: arr\r\nOut[33]: \r\narray([b\'str-0\', b\'str-1\', b\'str-2\', b\'str-3\', b\'str-4\', b\'str-0\',\r\n       b\'str-1\', b\'str-2\', b\'str-3\', b\'str-4\'], \r\n      dtype=\'|S25\')\r\n\r\nIn [34]: ne.evaluate(\'arr==""str-2""\')\r\nTypeError: string argument without an encoding\r\n\r\nIn [35]: ne.evaluate(\'arr==""str-2""\'.encode(encoding))\r\nValueError: must specify expression as a string\r\n```\r\n\r\nworkaround!\r\n```\r\nIn [36]: arg = arr[2]\r\n\r\nIn [37]: arg\r\nOut[37]: b\'str-2\'\r\n\r\nIn [38]: type(arg)\r\nOut[38]: numpy.bytes_\r\n\r\nIn [39]: ne.evaluate(\'arr==arg\')\r\nOut[39]: array([False, False,  True, False, False, False, False,  True, False, False], dtype=bool)\r\n\r\nIn [41]: selector2 = ""(column==arg)""\r\n\r\nIn [42]: handle.root.test.table.readWhere(selector2,condvars={ \'arg\' : arg})\r\nOut[42]: \r\narray([(b\'str-2\', 2, [0.0, 1.0, 2.0]), (b\'str-2\', 7, [0.0, 1.0, 2.0])], \r\n      dtype=[(\'column\', \'S25\'), (\'index\', \'<i8\'), (\'values\', \'<f8\', (3,))])\r\n```'"
,,,,,,,,,,,,,,
eike-welk/clair#45,14293082,eike-welk,,2013-05-14 04:07:41,2013-05-14 04:07:41,None,open,,,0,enhancement,https://api.github.com/repos/eike-welk/clair/issues/45,b'Use `pd.Timestamp` for parsing and representing points in time.',"b""Currently times are represented by `datetime.datetime` objects; while parsing string representations of times is done with the library `dateutil`. However Pandas converts `datetime.datetime` to `pd.Timestamp`, and represents times internally as integers. Furthermore `pd.Timestamp` can automatically and comfortably parse string representations of times.\n\nTherefore port all date computations to `pd.Timestamp`. \n\nEmpty series should be filled with `NaN` or `numpy.datetime64('NaT')`, not with None.\n\nhttp://pandas.pydata.org/pandas-docs/dev/missing_data.html#datetimes\n\nhttp://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html\n\nand discussion at end:\n\nhttps://github.com/pydata/pandas/issues/3593#issuecomment-17850876"""
eike-welk/clair#43,14254219,eike-welk,eike-welk,2013-05-13 10:26:45,2013-05-13 23:45:48,2013-05-13 23:45:48,closed,,,0,bug,https://api.github.com/repos/eike-welk/clair/issues/43,b'Pandas converts dates to integers.',"b'The new version of Pandas converts dates to integers. This makes the GUI more or less unusable. It will also most probably confuse other parts of the program, especially the file IO will be confused.\r\n\r\nThe automatic conversion occurs only in some algorithms. It happens when the ""expected products"" field is updated. \r\n\r\nRelated issue for Pandas: https://github.com/pydata/pandas/issues/3593'"
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
numpy/numpy#3001,11130462,stephenwlin,charris,2013-02-18 20:42:43,2013-02-25 02:01:03,2013-02-25 02:01:03,closed,,,2,,https://api.github.com/repos/numpy/numpy/issues/3001,b'BUG: take on object array (sometimes?) leaves self in inconsistent state when IndexError is raised',"b""Hello,\r\n\r\npydata/pandas#2892 is apparently due to the following, with Python 3.3 and latest dev numpy (on 32-bit ubuntu):\r\n\r\n```python \r\nIn [1]: import numpy as np\r\n\r\nIn [2]: u = np.asarray([ 0, 6, 12, 1, 7, 13, 2, 8, 14])\r\n\r\nIn [3]: i = np.asarray([1.0, 2.0, 3.0]).astype(object)\r\n\r\nIn [4]: i\r\nOut[4]: array([1.0, 2.0, 3.0], dtype=object)\r\n\r\nIn [5]: i.take(u)\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-5-8293b97d15cc> in <module>()\r\n----> 1 i.take(u)\r\n\r\nIndexError: index 6 is out of bounds for axis 0 with size 3\r\n\r\nIn [6]: i\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nNo segfault occurs if `astype` is omitted above.\r\n\r\nI'm not familiar with the numpy code, so I'm not sure if my description of the issue in the subject line is 100% correct, but it seems to be the gist of it from testing.\r\n\r\nThanks in advance for any help with this!"""
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
luca-dex/pyTSA#18,24870663,luca-dex,luca-dex,2013-12-30 00:52:32,2013-12-30 00:56:14,2013-12-30 00:56:14,closed,,,0,,https://api.github.com/repos/luca-dex/pyTSA/issues/18,"b""usecol not working with engine 'python' or delimiter different from ','""","b""It's a known bug of Pandas https://github.com/pydata/pandas/issues/2733 \r\n\r\nA workaround is needed..."""
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
dask/dask#739,107600268,cowlicks,cowlicks,2015-09-21 21:17:24,2015-09-25 19:33:08,2015-09-24 20:00:09,closed,,0.7.2,7,,https://api.github.com/repos/dask/dask/issues/739,b'Fixing issues with UTF-16/32 encodings and dataframe.read_csv',"b""UTF 16 and UTF 32 data usually starts with a 2 byte Byte Order Mark (BOM) that indicates encoding and its endianess.\r\n\r\nThis is a problem if we break a file into chunks because only the first chunk will have the BOM.\r\n\r\nThis is what causes problems when `dask.dataframe.read_csv` tries to read a `utf-16` file.\r\n \r\nThe encoding of a file can be specified with the endianess: `utf-16 -> utf-16-be, utf-16-le`. So I tried\r\nto fix this by reading the BOM and changing the encoding passed to `pandas.read_csv` to include the endianess.  See [here](https://github.com/blaze/dask/compare/blaze:master...cowlicks:bom?expand=1#diff-2a4c3ed91854a1a70080d69581e762b3R51).\r\n\r\nThis doesn't work. I'm not sure why."""
numpy/numpy#2704,8073847,wesm,seberg,2012-11-02 22:52:38,2013-05-11 15:43:56,2013-05-11 15:43:56,closed,,,7,,https://api.github.com/repos/numpy/numpy/issues/2704,b'Boolean indexing edge case',"b'This looks like a bug to me:\r\n\r\n```\r\narr = np.array([[]]).T\r\nk = np.array([], dtype=bool)\r\narr[k]\r\narr[k] = 0\r\n```\r\n\r\n(NumPy 1.6.1, have not checked git master yet)'"
numpy/numpy#459,7136613,jseabold,njsmith,2012-09-25 22:45:09,2014-06-17 07:25:23,2012-10-03 15:29:59,closed,,,5,,https://api.github.com/repos/numpy/numpy/issues/459,b'Fix unicode repr object',"b'If you try to print an array with unicode in it and an object dtype, it currently fails. This happens often for me working with pandas because it defaults to the object dtype for non-numerical arrays. This is because the fallback for the formatting is str. Is there any reason not to use the numpystr function here that can handle unicode?\n\nReproduce\n\n```python\nimport numpy as np\na = np.array([u""\\xe9""], dtype=object)\nprint a\n```'"
kwgoodman/bottleneck#50,6642493,erg,kwgoodman,2012-09-04 18:37:35,2012-09-06 19:28:19,2012-09-06 19:28:19,closed,,,21,,https://api.github.com/repos/kwgoodman/bottleneck/issues/50,"b""bottleneck.move_std() produces nans, doesn't match pandas.rolling_std()""",b'https://gist.github.com/3624548\n\nhttps://github.com/pydata/pandas/issues/1840'
,,,,,,,,,,,,,,
statsmodels/statsmodels#226,4120053,cdeil,jseabold,2012-04-14 21:04:50,2012-06-19 02:00:27,2012-06-07 02:25:48,closed,,0.4.x,5,comp-base,https://api.github.com/repos/statsmodels/statsmodels/issues/226,b'test_datetools.test_infer_freq fails',"b'After updating pandas to https://github.com/pydata/pandas/commit/9385b62d07c12ea361112b167ae811b3da62d3f2 and statsmodels to fdd16e64b562b91fdfff091fa97c6f82604ee4d3 I now see one statsmodels test failure that wasn\'t there before.\r\n\r\n```\r\n======================================================================\r\nERROR: statsmodels.tsa.base.tests.test_datetools.test_infer_freq\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/deil/Library/Python/2.7/lib/python/site-packages/statsmodels-0.4.0-py2.7-macosx-10.7-x86_64.egg/statsmodels/tsa/base/tests/test_datetools.py"", line 63, in test_infer_freq\r\n    npt.assert_string_equal(_infer_freq(b), \'B\')\r\n  File ""/Users/deil/Library/Python/2.7/lib/python/site-packages/statsmodels-0.4.0-py2.7-macosx-10.7-x86_64.egg/statsmodels/tsa/base/datetools.py"", line 234, in _infer_freq\r\n    elif delta >= timedelta(28*nobs) and delta <= timedelta(31*nobs):\r\nTypeError: ufunc \'greater_equal\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'safe\'\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nThis is on Mac OS X Lion with XCode 4.3 and numpy 1.6.1. installed via Macports.\r\n'"
statsmodels/statsmodels#259,4503872,wesm,jseabold,2012-05-09 23:34:09,2014-07-13 21:02:23,2012-05-10 12:55:05,closed,,0.4.x,1,,https://api.github.com/repos/statsmodels/statsmodels/issues/259,b'TST: support new pandas 0.8.0 frequency inference API',"b""With these changes all the TSA tests pass. We'll need to do more work this summer to better integrate the new pandas timeseries APIs prior to 0.5.0. Should close #226"""
statsmodels/statsmodels#168,3509147,jseabold,jseabold,2012-03-05 16:29:12,2012-06-19 02:09:03,2012-06-07 02:31:10,closed,,0.4.x,3,comp-tsa;design;pandas-integration,https://api.github.com/repos/statsmodels/statsmodels/issues/168,b'Pandas dates and TSA models',"b'We may have to change some of the forecast dates code when pandas switches over to the ordinal timestamp representation. Right now, our current implementation is broken with the new datetime branch installed. To replicate, install pandas datetime branch and run examples/tsa/ex_dates.py. Make sure to write tests for this whenever the pandas code settles down.'"
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
PyTables/PyTables#151,4455663,wesm,,2012-05-07 15:00:32,2014-09-11 21:07:37,None,open,,,8,enhancement,https://api.github.com/repos/PyTables/PyTables/issues/151,b'UnicodeAtom / UnicodeString types',b'Are there plans for these / are they pretty simple to do? Cross linking from https://github.com/pydata/pandas/issues/626'
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
,,,,,,,,,,,,,,
