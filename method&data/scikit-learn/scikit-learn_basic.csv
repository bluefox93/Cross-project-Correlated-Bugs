issue,id,reporter,closed_by,created_at,updated_at,closed_at,state,assignee,milestone,comments,label_name,url,title,body
6902,160855909,LeonieBorne,agramfort,2016-06-17 10:13:31,2016-07-23 20:29:29,2016-07-23 20:29:28,closed,,,5,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6902,b'RadiusNeighborsClassifier: possible problem with outliers class?',"b'Hi,\r\n\r\nWhen I use the RadiusNeighborsClassifier with an outlier class, I get this message : \r\n\r\n```ruby\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-238-0224ec628811>"", line 17, in <module>\r\n    scores.append(clf.score(X_test, y_test))\r\n\r\n  File ""/volatile/anaconda2/lib/python2.7/site-packages/sklearn/base.py"", line 310, in score\r\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\r\n\r\n  File ""/volatile/anaconda2/lib/python2.7/site-packages/sklearn/neighbors/classification.py"", line 379, in predict\r\n    in zip(pred_labels[inliers], weights)],\r\n\r\n  File ""/volatile/anaconda2/lib/python2.7/site-packages/sklearn/utils/extmath.py"", line 404, in weighted_mode\r\n    w = np.zeros(a.shape, dtype=w.dtype) + w\r\n\r\nValueError: operands could not be broadcast together with shapes (101,) (82,) \r\n```\r\n\r\nI\'m wondering if the line `in zip(pred_labels[inliers], weights)] `needs to be replaced with `in zip(pred_labels[inliers], weights[inliers])]`. \r\nI\'m using the version 0.17.1 of sklearn.\r\n\r\n\r\n'"
6795,155538483,gioelelm,jnothman,2016-05-18 15:55:07,2016-05-25 13:07:38,2016-05-25 13:07:38,closed,,,1,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6795,"b""LogisticRegressionCV with class_weights='balanced'""","b'When calling LogisticRegressionCV fit method after setting class_weight as \'balanced\', I get the following error:\r\n\r\n> /Users/admin/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.py in logistic_regression_path(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\r\n>     606                 class_weight = temp.copy()\r\n>     607             else:\r\n> --> 608                 raise ValueError(""In LogisticRegressionCV the lib linear ""\r\n>     609                                  ""solver cannot handle multiclass with ""\r\n>     610                                  ""class_weight of type dict. Use the lbfgs, ""\r\n> \r\n> ValueError: In LogisticRegressionCV the liblinear solver cannot handle multiclass with class_weight of type dict. Use the lbfgs, newton-cg or sag solvers or set class_weight=\'balanced\'\r\n\r\nThe code below should replicate the issue:\r\n\r\n```python\r\nLR = LogisticRegressionCV( solver=\'liblinear\', multi_class=\'ovr\',class_weight=\'balanced\',)\r\nLR.fit(np.random.normal(0,1,(1000,2000)), np.random.randint(0,5,1000))\r\n```\r\n\r\nA quick look at the source suggests me that the problem is that the class_weight get passed to logistic_regression_path after being transformed into a dictionary, therefore the line 600 at logistic.py will always evaluate as True:\r\n\r\n`if isinstance(class_weight, dict) or multi_class == \'multinomial\':`\r\n\r\nHowever making minor changes (e.g passing self.class_weight instead of class_weight) to the code did not fix the issue and resulted in other errors so I pass the issue to the experts.'"
6793,155463670,mbatchkarov,ogrisel,2016-05-18 10:07:35,2016-05-26 11:51:28,2016-05-26 11:51:18,closed,,0.18,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6793,b'NMF incorrectly raises ValueError',"b'<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://lists.sourceforge.net/lists/listinfo/scikit-learn-general\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nNMF raises the following error: `ValueError: Number of components must be positive; got (n_components=10)`.\r\n\r\nThere are two problem that I can see:\r\n\r\n- [this line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/nmf.py#L750) checks the type of `n_components`, but ignores the possibility that it might be something like `np.int64`. The next two checks (`tol` and `max_iter`) are correct.\r\n- the error message is uninformative. The problem has nothing to do with `n_components` not being positive\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.naive_bayes import MultinomialNB\r\nfrom sklearn.decomposition import NMF\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.learning_curve import validation_curve\r\n\r\nimport platform; print(""Platform"", platform.platform())\r\nimport sys; print(""Python"", sys.version)\r\nimport numpy; print(""NumPy"", numpy.__version__)\r\nimport scipy; print(""SciPy"", scipy.__version__)\r\nimport sklearn; print(""Scikit-Learn"", sklearn.__version__)\r\n\r\nX = np.random.random((100, 100))\r\ny = [0] * 50 + [1] * 50\r\n\r\npipe = Pipeline([\r\n    (\'nmf\', NMF(n_components=10)),\r\n    (\'clf\', MultinomialNB())\r\n])\r\n\r\n# runs fine because nmf__n_components is a Python int\r\ntrain_scores, valid_scores = validation_curve(pipe, X, y, ""nmf__n_components"",range(10, 21, 10))\r\n\r\n# ValueError: Number of components must be positive; got (n_components=10)\r\n# because nmf__n_components is np.int64\r\ntrain_scores, valid_scores = validation_curve(pipe, X, y, ""nmf__n_components"", np.arange(10, 21, 10))\r\n```\r\n\r\n#### Expected Results\r\nNo error is thrown.\r\n\r\n#### Actual Results\r\n`ValueError: Number of components must be positive; got (n_components=10)`\r\n\r\n#### Versions\r\nPlatform Darwin-14.5.0-x86_64-i386-64bit\r\nPython 3.4.4 |Anaconda 2.5.0 (x86_64)| (default, Jan  9 2016, 17:30:09) \r\n[GCC 4.2.1 (Apple Inc. build 5577)]\r\nNumPy 1.10.4\r\nSciPy 0.17.0\r\nScikit-Learn 0.17.1'"
6764,153193326,geekoala,ogrisel,2016-05-05 08:56:18,2016-05-10 07:32:15,2016-05-10 07:32:15,closed,,0.18,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6764,b'[MRG+1] Fix bug in SquaredEpsilonInsensitive loss function #6728',"b'<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests\r\n-->\r\n#### Reference Issue\r\nThis PR comes to fix #6728 .\r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\n+ Condition of if statement is incorrect in linear_model.SquaredEpsilonInsensitive(loss function) when p - y > epsilon. It should be z < -epsilon where z = y - p.\r\n+ Added tests for each Cython class on the gradient values in SGD.\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n\r\n'"
6728,151536528,perhapszzy,ogrisel,2016-04-28 02:42:30,2016-05-10 07:32:15,2016-05-10 07:32:15,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6728,b'Bug in loss function in SquaredEpsilonInsensitive?',"b'https://github.com/scikit-learn/scikit-learn/blob/acdaeaf5dd2577855b09a23862489d7b10faeaa3/sklearn/linear_model/sgd_fast.pyx#L322\r\n\r\nShouldn\'t this line be ```z < -self.epsilon``` (add the ""-"" before self.epsilon)'"
6724,151269688,jnothman,jnothman,2016-04-27 00:23:41,2016-05-14 13:55:24,2016-05-14 13:55:24,closed,,0.18,6,Bug;Needs Review,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6724,b'[MRG+2] FIX bug where expected_mutual_information may miscalculate',"b""Fixes #6718 ~~but I don't understand why, hence have no unit test :/~~"""
6718,151135761,jnothman,jnothman,2016-04-26 13:30:09,2016-05-14 13:55:24,2016-05-14 13:55:24,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6718,b'Cythonized expected_mutual_information disagrees with former version',"b""[This SO post](http://stackoverflow.com/questions/36865542/adjusted-mutual-information-scikit-learn) appears to highlight a long-standing bug introduced in 9cf5f00eb20cefbd9160ed8ac6be76ec1c8ea51e, shortly after `expected_mutual_information` was rewritten in cython. See #1334\r\n\r\nUsing the SO poster's data:\r\n```python\r\nimport urllib  # py2\r\nimport ast\r\nimport numpy as np\r\nfrom sklearn.metrics import cluster\r\npred = np.array(ast.literal_eval(urllib.urlopen('http://pastebin.com/raw/hJz1M4sf').read()))\r\ntrue = np.array(ast.literal_eval(urllib.urlopen('http://pastebin.com/raw/9Y5TE6b7').read()))\r\nprint(cluster.expected_mutual_information(cluster.contingency_matrix(pred,true), len(pred)))\r\n```\r\n\r\noutput prior to 9cf5f00eb20cefbd9160ed8ac6be76ec1c8ea51e: 0.000307845374016.\r\n\r\noutput since 9cf5f00eb20cefbd9160ed8ac6be76ec1c8ea51e: 1.53843820095.\r\n"""
6499,138884898,MechCoder,amueller,2016-03-07 04:32:23,2016-03-07 17:23:10,2016-03-07 17:23:10,closed,,,2,API;Blocker;Bug;Documentation;Easy;Enhancement;Large Scale;Moderate;Need Contributor;Needs Review;New Feature,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6499,b'10000 Stars !!',b':+1: :pizza: :cake: :beer: :beers: :wine_glass: :curry: :sake: :fireworks: :rice_cracker: :100: '
6443,136011440,dsquareindia,dsquareindia,2016-02-24 09:57:38,2016-02-29 21:49:01,2016-02-29 21:49:01,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6443,b'nosetests -v sklearn/ giving errors on master',"b'Versions used:\r\n - Python 2.7.11\r\n - Sklearn 0.18.dev0\r\n - Scipy 0.16.1\r\n - Numpy 1.10.2\r\n\r\nJust to add, my master is up to date with upstream/master\r\n\r\nFollowing is the traceback:\r\n```\r\n======================================================================\r\nERROR: sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pickle(\'wminkowski\', {\'p\': 1, \'w\': array([ 0.31179588,  0.69634349,  0.37775184,  0.17960368])})\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/devashish/miniconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/devashish/EXPERIMENTATION/scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py"", line 128, in check_pickle\r\n    D2 = dm2.pairwise(self.X1)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 390, in sklearn.neighbors.dist_metrics.DistanceMetric.pairwise (sklearn/neighbors/dist_metrics.c:5395)\r\n    self.pdist(get_memview_DTYPE_2D(Xarr),\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 319, in sklearn.neighbors.dist_metrics.DistanceMetric.pdist (sklearn/neighbors/dist_metrics.c:4754)\r\n    D[i1, i2] = self.dist(&X[i1, 0], &X[i2, 0], X.shape[1])\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 612, in sklearn.neighbors.dist_metrics.WMinkowskiDistance.dist (sklearn/neighbors/dist_metrics.c:7880)\r\n    return pow(self.rdist(x1, x2, size), 1. / self.p)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 602, in sklearn.neighbors.dist_metrics.WMinkowskiDistance.rdist (sklearn/neighbors/dist_metrics.c:7744)\r\n    raise ValueError(\'WMinkowskiDistance dist: \'\r\nValueError: WMinkowskiDistance dist: size of w does not match\r\n\r\n======================================================================\r\nERROR: sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pickle(\'wminkowski\', {\'p\': 1.5, \'w\': array([ 0.31179588,  0.69634349,  0.37775184,  0.17960368])})\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/devashish/miniconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/devashish/EXPERIMENTATION/scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py"", line 128, in check_pickle\r\n    D2 = dm2.pairwise(self.X1)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 390, in sklearn.neighbors.dist_metrics.DistanceMetric.pairwise (sklearn/neighbors/dist_metrics.c:5395)\r\n    self.pdist(get_memview_DTYPE_2D(Xarr),\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 319, in sklearn.neighbors.dist_metrics.DistanceMetric.pdist (sklearn/neighbors/dist_metrics.c:4754)\r\n    D[i1, i2] = self.dist(&X[i1, 0], &X[i2, 0], X.shape[1])\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 612, in sklearn.neighbors.dist_metrics.WMinkowskiDistance.dist (sklearn/neighbors/dist_metrics.c:7880)\r\n    return pow(self.rdist(x1, x2, size), 1. / self.p)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 602, in sklearn.neighbors.dist_metrics.WMinkowskiDistance.rdist (sklearn/neighbors/dist_metrics.c:7744)\r\n    raise ValueError(\'WMinkowskiDistance dist: \'\r\nValueError: WMinkowskiDistance dist: size of w does not match\r\n\r\n======================================================================\r\nERROR: sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pickle(\'wminkowski\', {\'p\': 3, \'w\': array([ 0.31179588,  0.69634349,  0.37775184,  0.17960368])})\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/devashish/miniconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/devashish/EXPERIMENTATION/scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py"", line 128, in check_pickle\r\n    D2 = dm2.pairwise(self.X1)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 390, in sklearn.neighbors.dist_metrics.DistanceMetric.pairwise (sklearn/neighbors/dist_metrics.c:5395)\r\n    self.pdist(get_memview_DTYPE_2D(Xarr),\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 319, in sklearn.neighbors.dist_metrics.DistanceMetric.pdist (sklearn/neighbors/dist_metrics.c:4754)\r\n    D[i1, i2] = self.dist(&X[i1, 0], &X[i2, 0], X.shape[1])\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 612, in sklearn.neighbors.dist_metrics.WMinkowskiDistance.dist (sklearn/neighbors/dist_metrics.c:7880)\r\n    return pow(self.rdist(x1, x2, size), 1. / self.p)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 602, in sklearn.neighbors.dist_metrics.WMinkowskiDistance.rdist (sklearn/neighbors/dist_metrics.c:7744)\r\n    raise ValueError(\'WMinkowskiDistance dist: \'\r\nValueError: WMinkowskiDistance dist: size of w does not match\r\n\r\n======================================================================\r\nERROR: sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pickle(\'seuclidean\', {\'V\': array([ 0.22741463,  0.25435648,  0.05802916,  0.43441663])})\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/devashish/miniconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/devashish/EXPERIMENTATION/scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py"", line 128, in check_pickle\r\n    D2 = dm2.pairwise(self.X1)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 390, in sklearn.neighbors.dist_metrics.DistanceMetric.pairwise (sklearn/neighbors/dist_metrics.c:5395)\r\n    self.pdist(get_memview_DTYPE_2D(Xarr),\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 319, in sklearn.neighbors.dist_metrics.DistanceMetric.pdist (sklearn/neighbors/dist_metrics.c:4754)\r\n    D[i1, i2] = self.dist(&X[i1, 0], &X[i2, 0], X.shape[1])\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 464, in sklearn.neighbors.dist_metrics.SEuclideanDistance.dist (sklearn/neighbors/dist_metrics.c:6384)\r\n    return sqrt(self.rdist(x1, x2, size))\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 454, in sklearn.neighbors.dist_metrics.SEuclideanDistance.rdist (sklearn/neighbors/dist_metrics.c:6239)\r\n    raise ValueError(\'SEuclidean dist: size of V does not match\')\r\nValueError: SEuclidean dist: size of V does not match\r\n\r\n======================================================================\r\nERROR: sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pickle(\'mahalanobis\', {\'VI\': array([[ 0.80314541,  0.98494408,  0.59389881,  1.16113843],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/devashish/miniconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/devashish/EXPERIMENTATION/scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py"", line 128, in check_pickle\r\n    D2 = dm2.pairwise(self.X1)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 390, in sklearn.neighbors.dist_metrics.DistanceMetric.pairwise (sklearn/neighbors/dist_metrics.c:5395)\r\n    self.pdist(get_memview_DTYPE_2D(Xarr),\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 319, in sklearn.neighbors.dist_metrics.DistanceMetric.pdist (sklearn/neighbors/dist_metrics.c:4754)\r\n    D[i1, i2] = self.dist(&X[i1, 0], &X[i2, 0], X.shape[1])\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 682, in sklearn.neighbors.dist_metrics.MahalanobisDistance.dist (sklearn/neighbors/dist_metrics.c:8708)\r\n  File ""sklearn/neighbors/dist_metrics.pyx"", line 664, in sklearn.neighbors.dist_metrics.MahalanobisDistance.rdist (sklearn/neighbors/dist_metrics.c:8521)\r\n    ITYPE_t size) nogil except -1:\r\nValueError: Mahalanobis dist: size of V does not match\r\n\r\n----------------------------------------------------------------------\r\nRan 6811 tests in 216.465s\r\n\r\nFAILED (SKIP=14, errors=5)\r\n```'"
6343,133275440,lesteve,GaelVaroquaux,2016-02-12 15:55:26,2016-02-16 12:13:07,2016-02-16 12:13:07,closed,,0.17.1,5,Bug;Needs Review,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6343,"b'[MRG+1] Finish ""Fix memory leak in Barnes-Hut SNE"" PR'","b'A minor tidying up of #5983, as requested by @ogrisel and because @AlexanderFabisch did not have the bandwith to tackle it.\r\n\r\nCloses #5983 and fixes #5916.\r\n\r\nI used the following snippet (from @ogrisel in #5983) to make sure the memory leak was fixed.\r\n\r\n```python\r\n\r\nimport psutil\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\n\r\nX = np.random.rand(10, 10)\r\np = psutil.Process()\r\n\r\nfor i in range(100000):\r\n    TSNE().fit(X)\r\n    if i % 100 == 0:\r\n        print(p.memory_info().rss / 1e6)\r\n```'"
6335,132961385,ogrisel,ogrisel,2016-02-11 12:35:23,2016-02-18 07:59:31,2016-02-18 07:59:31,closed,,,18,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6335,b'IndexError: invalid slice in test_non_meta_estimators called with GenericUnivariateSelect',"b'When running the tests with Python 3.5 against numpy and scipy dev from:\r\n\r\nhttp://travis-dev-wheels.scipy.org\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_non_meta_estimators(\'GenericUnivariateSelect\', <class \'sklearn.feature_selection.univariate_selection.GenericUnivariateSelect\'>)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/venv/lib/python3.5/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/code/sklearn/utils/testing.py"", line 319, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File ""/code/sklearn/utils/estimator_checks.py"", line 703, in check_estimators_dtypes\r\n    getattr(estimator, method)(X_train)\r\n  File ""/code/sklearn/feature_selection/base.py"", line 76, in transform\r\n    mask = self.get_support()\r\n  File ""/code/sklearn/feature_selection/base.py"", line 47, in get_support\r\n    mask = self._get_support_mask()\r\n  File ""/code/sklearn/feature_selection/univariate_selection.py"", line 724, in _get_support_mask\r\n    return selector._get_support_mask()\r\n  File ""/code/sklearn/feature_selection/univariate_selection.py"", line 414, in _get_support_mask\r\n    kept_ties = ties[:max_feats - mask.sum()]\r\nIndexError: invalid slice\r\n\r\n```'"
6320,132468947,larsmans,larsmans,2016-02-09 16:42:30,2016-02-13 17:28:45,2016-02-13 17:28:45,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6320,"b""LDA doesn't produce probabilities""","b""Not sure if this is a bug or a documentation issue, but `LatentDirichletAllocation` doesn't produce normalized probabilities from `transform` and doesn't explain how to get these either:\r\n\r\n```py\r\n>>> from scipy.sparse import rand\r\n>>> X = rand(10, 54, random_state=42)\r\n>>> X.data = np.abs(X.data)\r\n>>> lda = LatentDirichletAllocation(n_topics=3).fit(X)\r\n>>> lda.transform(X)\r\narray([[ 0.33333333,  0.33333333,  0.33333333],\r\n       [ 0.33333333,  0.33333333,  0.33333333],\r\n       [ 0.34945295,  0.35770751,  0.35092315],\r\n       [ 1.88534135,  0.34398327,  0.3449241 ],\r\n       [ 0.3930671 ,  0.39372527,  0.36920215],\r\n       [ 0.89679679,  0.35476694,  0.34955128],\r\n       [ 0.33333333,  0.33333333,  0.33333333],\r\n       [ 0.33333333,  0.33333333,  0.33333333],\r\n       [ 0.33333333,  0.33333333,  0.33333333],\r\n       [ 0.33333333,  0.33333333,  0.33333333]])\r\n```"""
6280,131365910,ogrisel,TomDLT,2016-02-04 14:34:11,2016-02-17 12:59:42,2016-02-17 12:59:42,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6280,b'test_label_binarizer failure under Windows',"b'This failure is revealed by enabling missing tests in #6274. I occurs under Python 3.5, either 32 bit or 64 bit:\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.preprocessing.tests.test_label.test_label_binarizer\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python35\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python35\\lib\\site-packages\\sklearn\\preprocessing\\tests\\test_label.py"", line 46, in test_label_binarizer\r\n    assert_array_equal(lb.inverse_transform(got), inp)\r\n  File ""C:\\Python35\\lib\\site-packages\\numpy\\testing\\utils.py"", line 739, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""C:\\Python35\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n \r\n(mismatch 100.0%)\r\n x: [repr failed]\r\n y: array([\'pos\', \'pos\', \'pos\', \'pos\'], \r\n      dtype=\'<U3\')\r\n```'"
6279,131364957,ogrisel,TomDLT,2016-02-04 14:30:51,2016-05-03 14:29:55,2016-05-03 14:29:55,closed,,,10,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6279,b'test_pls.test_scale_and_stability failure',"b'This is a bug revealed by enabling some missing tests as part of #6274. It occurs under Windows with Python 2.7.8 (64 bit) and numpy 1.9.3 with MKL.\r\n\r\nhttps://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.5133\r\n```\r\n======================================================================\r\nFAIL: sklearn.cross_decomposition.tests.test_pls.test_scale_and_stability\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27-x64\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27-x64\\lib\\site-packages\\sklearn\\cross_decomposition\\tests\\test_pls.py"", line 347, in test_scale_and_stability\r\n    assert_array_almost_equal(X_s_score, X_score)\r\n  File ""C:\\Python27-x64\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""C:\\Python27-x64\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 6 decimals\r\n \r\n(mismatch 50.0%)\r\n x: array([[-1.337317, -0.041705],\r\n       [-1.108472,  0.098154],\r\n       [ 0.407632, -0.103084],\r\n       [ 2.038158,  0.046634]])\r\n y: array([[-1.337317,  0.      ],\r\n       [-1.108472,  0.      ],\r\n       [ 0.407632,  0.      ],\r\n       [ 2.038158,  0.      ]])\r\n```'"
6272,131040232,jakevdp,agramfort,2016-02-03 14:22:01,2016-02-04 13:33:13,2016-02-04 08:36:14,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6272,b'[MRG] BUG: fix pickling of DistanceMetric instances (fixes #6269)',"b'This fixes an issue when pickling & unpickling DistanceMetric objects (and by extension, nearest neighbors methods that use them).'"
6268,130673444,ogrisel,ogrisel,2016-02-02 12:48:35,2016-02-06 18:32:18,2016-02-06 18:31:01,closed,,,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6268,b'[MRG] FIX performance issue in _graph_connected_component',b'This is a reworked version of the fix in #5713 with a new test.'
6226,128615809,MechCoder,amueller,2016-01-25 19:22:40,2016-01-25 22:42:20,2016-01-25 22:42:20,closed,,,1,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6226,b'LabelEncoder transform should raise helpful error for 0-D arrays',"b'    from sklearn.preprocessing import LabelEncoder\r\n    le = LabelEncoder()\r\n    le.fit([""apple"", ""orange""]\r\n    \r\nIn old versions of NumPy\r\n\r\n    le.transform(""apple"")\r\n    ""y contains new labels: [a p l e]"" % str(diff)\r\n    \r\nIn new versions of NumPy\r\n\r\n    le.transform(""apple"")\r\n    0\r\n\r\nThe fix should be as simple as calling `column_or_1d` as the start of transform'"
6202,127800364,MechCoder,MechCoder,2016-01-20 22:35:50,2016-03-21 13:51:39,2016-03-21 13:51:39,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6202,b'LabelBinarizer returns dense output when input has only a single label even when sparse_output is set to True',"b'    lb = LabelBinarizer(sparse_output=True)\r\n    y = [1, 1, 1, 1, 1]\r\n    lb.fit_transform(y)\r\n    array([[0],\r\n       [0],\r\n       [0],\r\n       [0],\r\n       [0]])\r\n'"
6196,127708199,boskaiolo,MechCoder,2016-01-20 15:33:28,2016-01-28 06:21:57,2016-01-28 06:21:57,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6196,"b""fetch_20newsgroups 'remove' argument (Python 2.7.11)""","b'On Python 2.7.11, the remove argument of the fetch_20newsgroups method doesn\'t work.\r\nHere\'s an example (you can change \'10\' with another index, the problem appear again):\r\n\r\n````\r\nfrom sklearn.datasets import fetch_20newsgroups\r\nprint fetch_20newsgroups(shuffle=False, remove=(\'headers\', \'footers\', \'quotes\')).data[10]\r\n````\r\nAlthough the removal of headers, footers and quotes is set, this is the output:\r\n\r\n````\r\nFrom: maler@vercors.imag.fr (Oded Maler)\r\nSubject: Re: FLAME and a Jewish home in Palestine\r\nNntp-Posting-Host: pelvoux\r\nOrganization: IMAG, University of Grenoble, France\r\nLines: 40\r\n\r\nIn article <C5HJBC.1HC@bony1.bony.com>, jake@bony1.bony.com (Jake Livni) writes:\r\n|> In article <1993Apr13.172422.2407@newshub.ariel.yorku.ca> nabil@ariel.yorku.ca (Nabil Gangi) writes:\r\n|>\r\n|> >According to Exodus, there were 600,000 Jews that marched out of Egypt.\r\n|>\r\n|> This is only the number of adult males.  The total number of Jewish\r\n|> slaves leaving Egypt was much larger.\r\n|>\r\n|> >The number which could have arrived to the Holy Lands must have been\r\n|> >substantially less ude to the harsh desert and the killings between the\r\n|> >Jewish tribes on the way..\r\n|> >\r\n|> >NABIL\r\n|>\r\n|> Typical Arabic thinking.  If we are guilty of something, so is\r\n|> everyone else.  Unfortunately for you, Nabil, Jewish tribes are not\r\n|> nearly as susceptible to the fratricidal murdering that is still so\r\n|> common among Arabs in the Middle East.  There were no "" killings\r\n|> between the Jewish tribes on the way.""\r\n\r\nI don\'t like this comment about ""Typical"" thinking. You could state\r\nyour interpretation of Exodus without it. As I read Exodus I can see\r\na lot of killing there, which is painted by the author of the bible\r\nin ideological/religious colors. The history in the desert can be seen\r\nas an ethos of any nomadic people occupying a land. That\'s why I think\r\nit is a great book with which descendants Arabs, Turks and Mongols can\r\nunify as well.\r\n\r\n\r\n|> Jake\r\n|> --\r\n|> Jake Livni  jake@bony1.bony.com           Ten years from now, George Bush will\r\n|> American-Occupied New York                   have replaced Jimmy Carter as the\r\n|> My opinions only - employer has no opinions.    standard of a failed President.\r\n\r\n--\r\n===============================================================\r\nOded Maler, LGI-IMAG, Bat D, B.P. 53x, 38041 Grenoble, France\r\nPhone:  76635846     Fax: 76446675      e-mail: maler@imag.fr\r\n===============================================================\r\n````\r\n\r\nProblem doesn\'t appear in Python 3.5.1'"
6195,127699569,GaelVaroquaux,ogrisel,2016-01-20 14:55:08,2016-03-03 09:01:18,2016-01-27 10:12:02,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6195,b'Bug in LedoitWolf Shrinkage',"b'The estimate of the shrinkage in the Ledoit is pretty broken:\r\n\r\n<pre>\r\nimport numpy as np\r\nfrom sklearn import covariance\r\nnp.random.seed(42)\r\nsignals = np.random.random(size=(75, 4))\r\nprint(covariance.ledoit_wolf(signals))\r\n</pre>\r\n\r\nThis outputs:\r\n<pre>\r\n(array([[ 0.08626827,  0.        , -0.        , -0.        ],\r\n       [ 0.        ,  0.08626827,  0.        ,  0.        ],\r\n       [-0.        ,  0.        ,  0.08626827, -0.        ],\r\n       [-0.        ,  0.        , -0.        ,  0.08626827]]), 1.0)\r\n</pre>\r\n\r\nIn other words, the estimator has deduced that their should be a shrinkage of 1: it\'s taking something proportional to the identity.\r\n\r\nThat shrinkage is given by ""m_n"" in lemma 3.2 of ""A well-conditioned estimator for large-dimensional covariance matrices"", Olivier Ledoit and Michael Wolf: ""m_n = <S_n, I_n>"" where ""<.,.>"" is the canonical matrix inner product, I_n is the identity, and S_n the data scatter matrix. As can be seen from this equation, m_n == 1 is possible only if the scatter matrix is 1. Hence this result is false. Not that I believed it at all.\r\n\r\nI know where the bug is (n_splits == 0). I just need to find a robust test so that these things don\'t happen again.\r\n\r\nThis is quite bad: we have had a broken Ledoit Wolf for a few releases :(. Ledoit Wolf is the most useful covariance estimator.'"
6193,127530721,amueller,ogrisel,2016-01-19 20:25:52,2016-01-28 09:43:03,2016-01-28 09:43:03,closed,,,0,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6193,"b'clustering documentation ""input space"" note outdated'","b'The note at the beginning of the clustering docs talks about input in data space or similarity space.\r\nCurrently all algorithms assume X to be the data, possibly by choosing some measure of similarity.\r\nAlso, birch and DBSCAN are not mentioned in the note:\r\nhttp://scikit-learn.org/dev/modules/clustering.html#clustering'"
6147,125766835,armgilles,ogrisel,2016-01-09 16:03:21,2016-04-15 12:12:43,2016-01-29 10:26:40,closed,,0.17.1,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6147,b'n_jobs in GridSearchCV issue',"b'Hi,\r\n\r\nFirst thanks for your awesome work !\r\n\r\nI have an issue with GridSearchCV and n_jobs for a ExtraTreesClassifier model.\r\n\r\n- `platform.platform()` : Linux-3.13.0-74-generic-x86_64-with-debian-jessie-sid\r\n- `cpu_count()` : 8\r\n- RAM : 32 GB (never exceeds 6 GO during exec)\r\n- Python 2.7.11 :: Anaconda 2.4.1 (64-bit)\r\n- `sklearn.__version__` : \'0.17\'\r\n- `numpy.__version__` : \'1.10.4\'\r\n- `scipy.__version__` : \'0.16.1\'\r\n- `pandas.__version__` : \'0.17.1\'\r\n- `joblib.__version__` : \'0.9.3\'\r\n\r\n**Code KO :** \r\n\r\n```\r\nmodel = ExtraTreesClassifier(class_weight=\'balanced\')\r\nparameters = {\'criterion\': [\'gini\', \'entropy\'],\r\n                       \'max_depth\': [4, 10, 20],\r\n                       \'min_samples_split\' : [2, 4, 8],\r\n                       \'max_depth\' : [3, 10, 20]}\r\n\r\nclf = GridSearchCV(model, parameters, verbose=3, scoring=\'roc_auc\',\r\n                        cv=StratifiedKFold(y_train, n_folds=5, shuffle=True),  \r\n                        n_jobs=4)\r\n\r\nclf.fit(X_train.values, y_train.values)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""create_extratrees.py"", line 305, in <module>\r\n    clf.fit(X_train.values, y_train.values)\r\n  File ""/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py"", line 804, in fit\r\n    return self._fit(X, y, ParameterGrid(self.param_grid))\r\n  File ""/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py"", line 553, in _fit\r\n    for parameters in parameter_iterable\r\n  File ""/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 812, in __call__\r\n    self.retrieve()\r\n  File ""/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 762, in retrieve\r\n    raise exception\r\nsklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueError\r\n___________________________________________________________________________\r\nMultiprocessing exception:\r\n...........................................................................\r\n/home/gillesa/github/mailling/create_extratrees.py in <module>()\r\n    300                 \'max_depth\' : [3, 10, 20]}\r\n    301\r\n    302 clf = GridSearchCV(model, parameters,\r\n    303                    cv=StratifiedKFold(y_train, n_folds=5, shuffle=True), verbose=3, scoring=\'roc_auc\', n_jobs=4)\r\n    304\r\n--> 305 clf.fit(X_train.values, y_train.values)\r\n    306\r\n    307 best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\r\n    308 print(clf.scoring  + \' score : \', score)\r\n    309 for param_name in sorted(best_parameters.keys()):\r\n\r\n...........................................................................\r\n/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=sklearn.cross_validation.Stratif..._jobs\', refit=True, scoring=\'roc_auc\', verbose=3), X=array([[  0.,   9.,  56., ...,   1.,   0.,   0.]...      [  0.,   7.,  68., ...,   0.,   0.,   0.]]), y=array([0, 0, 0, ..., 1, 0, 0]))\r\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\r\n    800             Target relative to X for classification or regression;\r\n    801             None for unsupervised learning.\r\n    802\r\n    803         """"""\r\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\r\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...jobs\', refit=True, scoring=\'roc_auc\', verbose=3)>\r\n        X = array([[  0.,   9.,  56., ...,   1.,   0.,   0.]...      [  0.,   7.,  68., ...,   0.,   0.,   0.]])\r\n        y = array([0, 0, 0, ..., 1, 0, 0])\r\n        self.param_grid = {\'criterion\': [\'gini\', \'entropy\'], \'max_depth\': [3, 10, 20], \'min_samples_split\': [2, 4, 8]}\r\n    805\r\n    806\r\n    807 class RandomizedSearchCV(BaseSearchCV):\r\n    808     """"""Randomized search on hyper parameters.\r\n\r\n...........................................................................\r\n/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=sklearn.cross_validation.Stratif..._jobs\', refit=True, scoring=\'roc_auc\', verbose=3), X=array([[  0.,   9.,  56., ...,   1.,   0.,   0.]...      [  0.,   7.,  68., ...,   0.,   0.,   0.]]), y=array([0, 0, 0, ..., 1, 0, 0]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\r\n    548         )(\r\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\r\n    550                                     train, test, self.verbose, parameters,\r\n    551                                     self.fit_params, return_parameters=True,\r\n    552                                     error_score=self.error_score)\r\n--> 553                 for parameters in parameter_iterable\r\n        parameters = undefined\r\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\r\n    554                 for train, test in cv)\r\n    555\r\n    556         # Out is a list of triplet: score, estimator, n_test_samples\r\n    557         n_fits = len(out)\r\n\r\n...........................................................................\r\n/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=4), iterable=<generator object <genexpr>>)\r\n    807             if pre_dispatch == ""all"" or n_jobs == 1:\r\n    808                 # The iterable was consumed all at once by the above for loop.\r\n    809                 # No need to wait for async callbacks to trigger to\r\n    810                 # consumption.\r\n    811                 self._iterating = False\r\n--> 812             self.retrieve()\r\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=4)>\r\n    813             # Make sure that we get a last message telling us we are done\r\n    814             elapsed_time = time.time() - self._start_time\r\n    815             self._print(\'Done %3i out of %3i | elapsed: %s finished\',\r\n    816                         (len(self._output), len(self._output),\r\n\r\n---------------------------------------------------------------------------\r\n```\r\n\r\n\r\nSub-process traceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                         Sat Jan  9 16:42:09 2016\r\nPID: 18076                Python 2.7.11: /home/gillesa/anaconda2/bin/python\r\n...........................................................................\r\n/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\r\n     67     def __init__(self, iterator_slice):\r\n     68         self.items = list(iterator_slice)\r\n     69         self._size = len(self.items)\r\n     70\r\n     71     def __call__(self):\r\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\r\n     73\r\n     74     def __len__(self):\r\n     75         return self._size\r\n     76\r\n\r\n...........................................................................\r\n/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=ExtraTreesClassifier(bootstrap=False, class_weig..., random_state=None, verbose=0, warm_start=False), X=memmap([[  0.,   9.,  56., ...,   1.,   0.,   0....      [  0.,   7.,  68., ...,   0.,   0.,   0.]]), y=memmap([0, 0, 0, ..., 1, 0, 0]), scorer=make_scorer(roc_auc_score, needs_threshold=True), train=memmap([      0,       1,       2, ..., 1217841, 1217842, 1217843]), test=memmap([      0,       2,       3, ..., 1217824, 1217825, 1217833]), verbose=3, parameters={\'criterion\': \'gini\', \'max_depth\': 3, \'min_samples_split\': 4}, fit_params={}, return_train_score=False, return_parameters=True, error_score=\'raise\')\r\n   1545                              "" numeric value. (Hint: if using \'raise\', please""\r\n   1546                              "" make sure that it has been spelled correctly.)""\r\n   1547                              )\r\n   1548\r\n   1549     else:\r\n-> 1550         test_score = _score(estimator, X_test, y_test, scorer)\r\n   1551         if return_train_score:\r\n   1552             train_score = _score(estimator, X_train, y_train, scorer)\r\n   1553\r\n   1554     scoring_time = time.time() - start_time\r\n\r\n...........................................................................\r\n/home/gillesa/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _score(estimator=ExtraTreesClassifier(bootstrap=False, class_weig..., random_state=None, verbose=0, warm_start=False), X_test=memmap([[  0.,   9.,  56., ...,   1.,   0.,   0....      [  0.,   6.,  57., ...,   1.,   0.,   0.]]), y_test=memmap([0, 0, 0, ..., 0, 0, 0]), scorer=make_scorer(roc_auc_score, needs_threshold=True))\r\n   1604         score = scorer(estimator, X_test)\r\n   1605     else:\r\n   1606         score = scorer(estimator, X_test, y_test)\r\n   1607     if not isinstance(score, numbers.Number):\r\n   1608         raise ValueError(""scoring must return a number, got %s (%s) instead.""\r\n-> 1609                          % (str(score), type(score)))\r\n   1610     return score\r\n   1611\r\n   1612\r\n   1613 def _permutation_test_score(estimator, X, y, cv, scorer):\r\n\r\nValueError: scoring must return a number, got 0.671095795498 (<class \'numpy.core.memmap.memmap\'>) instead.\r\n```\r\n----------------------\r\n\r\nIf I set my n_jobs model to 8 and n_jobs GridSearchCV to 1, **it\'s OK**\r\n```\r\nmodel = ExtraTreesClassifier(class_weight=\'balanced\', n_jobs=8)\r\nparameters = {\'criterion\': [\'gini\', \'entropy\'],\r\n                       \'max_depth\': [4, 10, 20],\r\n                       \'min_samples_split\' : [2, 4, 8],\r\n                       \'max_depth\' : [3, 10, 20]}\r\n\r\nclf = GridSearchCV(model, parameters, verbose=3, scoring=\'roc_auc\',\r\n                        cv=StratifiedKFold(y_train, n_folds=5, shuffle=True),  \r\n                        n_jobs=1)\r\n\r\nclf.fit(X_train.values, y_train.values)\r\n```\r\n\r\nI try different setup but if GridSearchCV n_jobs > 1 it fails.\r\n\r\nI would like to optimize my CPU and i think n_jobs > 1 on GridSearchCV it better than n_jobs on your model. Maybe someone has feedback ?\r\n\r\n\r\nPossible relation with #6023 '"
6078,123472272,duboism,jnothman,2015-12-22 12:18:10,2015-12-28 22:27:33,2015-12-28 22:27:33,closed,,,4,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/6078,b'Bug for some scorer',"b'Some scorers (the ones derived from the confusion matrix: `recall_macro`, `precision_macro`, etc.) are defined with `functools.partial`. Partial functions have no `__name__` which creates problems: \r\n\r\n    >>> from sklearn.metrics.scorer import get_scorer\r\n    >>> recall_macro = get_scorer(\'recall_macro\')\r\n    >>> recall_macro\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/usr/lib/python2.7/dist-packages/sklearn/metrics/scorer.py"", line 49, in __repr__\r\n         % (self._score_func.__name__,\r\n    AttributeError: \'functools.partial\' object has no attribute \'__name__\'\r\n\r\nThe `__name__` attribute can be set after creating the partial function. If I\'m correct, the most correct option would be to use the name of the original function (`metric.__name__`).\r\n'"
5808,116669188,ktrnka,glouppe,2015-11-12 23:53:21,2016-01-15 23:48:31,2015-11-22 17:15:56,closed,,,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5808,b' GradientBoostingClassifier 40-50% slower in 0.17 vs 0.16.1',"b""I've run several tests with 0.17 and found that GradientBoostingClassifier takes 40-50% longer to run.\r\n\r\n50k x 61 dataset:\r\n\r\n* 0.16.1: 7.6 min\r\n* 0.17: 11.0 min, 11.6 min\r\n\r\n200k x 61 dataset:\r\n\r\n* 0.16.1: 43.9 min\r\n* 0.17: 62.3 min\r\n\r\nI tried adding feature scaling but that doesn't help. I tried disabling presort but it runs slower (good sign at least). \r\n\r\nAnother strange part is that my accuracy improved slightly with gradient boosting in 0.17. Could just be randomness and small sample but thought I should note it just in case. On the 200k test I got 67.66% in 0.16.1 and 67.75% in 0.17. On the 50k tests I got 66.08% in 0.16.1 and 66.17% and 66.34% in 0.17. The latter surprises me; I've seen variation due to randomness but usually under 0.05 not 0.17.\r\n\r\nI also tested RandomForestClassifier but found that it was slightly faster in 0.17.\r\n\r\nI've prepared a [fork of my repo](https://github.com/ktrnka/league-match-predictor) if you'd like to take a look. There's a dropbox link to the data in the readme. The main experimental script is src/exploration/train_test and gradient boosting is in the function gradient_boosting_exp. (Great apologies if you have to read my code... I've been working on this solo for a while. If you want code explanations let me know)\r\n\r\nI'm running on a Mid 2014 MacBook Pro 2.6ghz i5 8gb ram, integrated graphics"""
5738,115355077,amueller,agramfort,2015-11-05 19:24:53,2015-12-10 20:31:11,2015-12-10 20:31:11,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5738,b'Python3.3 compatibility',b'There is still a bug in #5429 :-/\r\nThere should be an ``else`` before the import.'
5725,115161961,amueller,TomDLT,2015-11-04 22:17:17,2016-01-27 11:42:07,2016-01-04 15:51:10,closed,,0.17.1,6,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5725,b'String to array comparison in RandomizedLasso',"b""here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/randomized_l1.py#L333\r\n\r\n```python\r\nif alpha in ('aic', 'bic')\r\n```\r\nThis popped up in the examples, but not in the tests as far as I can tell. Which probably means it is not covered in the tests.\r\nping @agramfort ;)"""
5677,114582382,glouppe,glouppe,2015-11-02 12:12:02,2015-11-02 21:30:08,2015-11-02 12:28:12,closed,,,9,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5677,b'Linear models crash on non-integer values for y',"b'This crashes, while it properly works for estimators outside of `sklearn.linear_model`. \r\n\r\n```\r\nX = [[0, 1], [1, 1]] \r\ny = [""apple"", ""banana""]\r\n\r\nfrom sklearn.linear_model import RidgeCV\r\nclf = RidgeCV().fit(X, y)\r\n\r\n>>> TypeError: cannot perform reduce with flexible type\r\n```'"
5671,114518671,amueller,GaelVaroquaux,2015-11-02 03:39:45,2015-11-03 18:26:55,2015-11-03 18:26:55,closed,,0.17,2,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5671,b'OneHotEncoder uses misshapen boolean mask',"b""There are some concerning warnings about shape mismatch of ``X[~mask]`` in the preprocessing tests of ``OneHotEncoder``.\r\nping @vighneshbirodkar if you have any time.\r\nOtherwise I'll investigate tomorrow."""
5627,114301706,ogrisel,ogrisel,2015-10-30 16:15:16,2015-10-30 19:39:48,2015-10-30 17:02:51,closed,,0.17,7,Blocker;Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5627,b'Appveyor failure on 0.17.X branch on Python 2 only',"b'Here is a failure we currently have on the 0.17.X branch:\r\n\r\nhttps://ci.appveyor.com/project/ogrisel/scikit-learn/build/job/5x1jj8tdg7w30ay2\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.metrics.tests.test_pairwise.test_pairwise_parallel(<function pairwise_kernels at 0x0CDB9EF0>, <function callable_rbf_kernel at 0x0D7ED470>, {\'gamma\': 0.1})\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\tests\\test_pairwise.py"", line 163, in check_pairwise_parallel\r\n    n_jobs=2, **kwds)\r\n  File ""C:\\Python27\\lib\\unittest\\case.py"", line 473, in assertRaises\r\n    callableObj(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\pairwise.py"", line 1342, in pairwise_kernels\r\n    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\pairwise.py"", line 1055, in _parallel_pairwise\r\n    for s in gen_even_slices(Y.shape[0], n_jobs))\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 813, in __call__\r\n    self.retrieve()\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 732, in retrieve\r\n    self._output.extend(job.get())\r\n  File ""C:\\Python27\\lib\\multiprocessing\\pool.py"", line 558, in get\r\n    raise self._value\r\nTokenError: (\'EOF in multi-line statement\', (105, 0))\r\n \r\n----------------------------------------------------------------------\r\n```\r\n\r\nThis test seems to be old. I don\'t understand what has changed beween 0.17b1 (where it worked) and the current branch that could trigger this issue.'"
5625,114294346,ogrisel,amueller,2015-10-30 15:39:36,2015-11-01 23:06:37,2015-11-01 23:06:37,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5625,b'Test failure in test_logistic_regression_class_weights undex OS X with Accelerate',"b'Under OS X 10.11:\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.linear_model.tests.test_logistic.test_logistic_regression_class_weights\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/venvs/py35/lib/python3.5/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/linear_model/tests/test_logistic.py"", line 674, in test_logistic_regression_class_weights\r\n    assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)\r\n  File ""/Users/ogrisel/venvs/py35/lib/python3.5/site-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/Users/ogrisel/venvs/py35/lib/python3.5/site-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 50.0%)\r\n x: array([[-0.454107,  0.830293, -2.263768, -0.966612],\r\n       [ 0.474448, -0.33929 , -0.168293, -0.797186],\r\n       [-0.02034 , -0.491003,  2.432062,  1.763798]])\r\n y: array([[-0.454113,  0.83029 , -2.26377 , -0.96661 ],\r\n       [ 0.474451, -0.339288, -0.168292, -0.797188],\r\n       [-0.020338, -0.491002,  2.432062,  1.763798]])\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 6 decimals\\n\\n(mismatch 50.0%)\\n x: array([[-0.454107,  0.830293, -2.263768, -0.966612],\\n       [ 0.474448, -0.33929 , -0.168293, -0.797186],\\n       [-0.02034 , -0.491003,  2.432062,  1.763798]])\\n y: array([[-0.454113,  0.83029 , -2.26377 , -0.96661 ],\\n       [ 0.474451, -0.339288, -0.168292, -0.797188],\\n       [-0.020338, -0.491002,  2.432062,  1.763798]])\')\r\n```\r\n\r\nI can make them pass by changing the decimals precision to 4:\r\n\r\n```\r\ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\r\nindex 13e5527..da19d80 100644\r\n--- a/sklearn/linear_model/tests/test_logistic.py\r\n+++ b/sklearn/linear_model/tests/test_logistic.py\r\n@@ -671,7 +671,7 @@ def test_logistic_regression_class_weights():\r\n                                   class_weight=class_weight_dict)\r\n         clf1.fit(X, y)\r\n         clf2.fit(X, y)\r\n-        assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)\r\n+        assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=4)\r\n\r\n     # Binary case: remove 90% of class 0 and 100% of class 2\r\n     X = iris.data[45:100, :]\r\n```\r\n\r\nWill open a PR.'"
5623,114232723,ogrisel,ogrisel,2015-10-30 09:33:11,2015-10-30 18:19:16,2015-10-30 18:19:16,closed,,0.17,1,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5623,b'joblib new forkserver default mode fails on interactively defined callables',b'Reported initially in joblib/joblib#263.\r\n\r\nThe long term fix is to use dill in joblib to be able to pickle functions by value instead of by reference (with `recurse=True`).\r\n\r\nI think we should revert to the default fork server mode for now. We need to release 0.9.3 with that stopgap and re-sync into scikit-learn and backport to 0.17.X before the release.'
5608,113772871,lesteve,ogrisel,2015-10-28 09:03:45,2016-02-08 16:15:18,2016-02-08 16:15:18,closed,,,5,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5608,b'randomized_svd flip sign according to v and not u when transpose=True',"b""From the randomized_svd doc:\r\n```\r\n   flip_sign: boolean, (True by default)\r\n        The output of a singular value decomposition is only unique up to a\r\n        permutation of the signs of the singular vectors. If `flip_sign` is\r\n        set to `True`, the sign ambiguity is resolved by making the largest\r\n        loadings for each component in the left singular vectors positive.\r\n```\r\n\r\nLooking at svd_flip the sign flip is done by column so that the maximum of the column is ensured to be positive (u_based_decision is True by default).\r\n\r\nHere is an example that shows that it is not the case when transpose=True (I guess in this case the svd_flip is according to v and not u):\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport sklearn.utils.extmath as extmath\r\n\r\nmat = np.arange(10 * 8).reshape(10, -1)\r\n\r\n\r\ndef max_loading_is_positive(u, v):\r\n    return {\r\n        'u_based': (np.abs(u).max(axis=0) == u.max(axis=0)).all(),\r\n        'v_based': (np.abs(v).max(axis=1) == v.max(axis=1)).all()\r\n    }\r\n\r\n\r\nu_flipped, _, v_flipped = extmath.randomized_svd(mat, 3, flip_sign=True)\r\nu_flipped_with_transpose, _, v_flipped_with_transpose = extmath.randomized_svd(\r\n    mat, 3, flip_sign=True, transpose=True)\r\n\r\nprint('flipped:', max_loading_is_positive(u_flipped, v_flipped))\r\nprint('flipped_with_transpose:',\r\n      max_loading_is_positive(\r\n          u_flipped_with_transpose, v_flipped_with_transpose))\r\n```\r\n\r\nOutput:\r\n```\r\nflipped: {'u_based': True, 'v_based': False}\r\nflipped_with_transpose: {'u_based': False, 'v_based': True}\r\n```"""
5604,113639217,yarikoptic,glouppe,2015-10-27 17:34:10,2015-10-30 19:25:12,2015-10-28 08:02:23,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5604,b'a collection of test failures on 32bit of 0.17.0~b1+git14-g4e6829c',"b'all full logs for the rebuilds across debian/ubuntus are at \r\nhttp://neuro.debian.net/_files/_buildlogs/scikit-learn/0.17.0~b1+git14-g4e6829c\r\n\r\nbut major problem seems to be failures on 32bit:\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_forest.test_importances(array([[-1.96100428,  2.19591399,  1.24476387, ...,  0.87672677,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_forest.py"", line 229, in check_importances\r\n    assert_less(np.abs(importances - importances_bis).mean(), 0.001)\r\nAssertionError: 0.0020379702810459388 not less than 0.001\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_forest.test_importances(array([[-1.96100428,  2.19591399,  1.24476387, ...,  0.87672677,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_forest.py"", line 229, in check_importances\r\n    assert_less(np.abs(importances - importances_bis).mean(), 0.001)\r\nAssertionError: 0.0018216795210648632 not less than 0.001\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_forest.test_importances(array([[-1.96100428,  2.19591399,  1.24476387, ...,  0.87672677,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_forest.py"", line 229, in check_importances\r\n    assert_less(np.abs(importances - importances_bis).mean(), 0.001)\r\nAssertionError: 0.0043495526382049685 not less than 0.001\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_forest.test_importances(array([[-1.96100428,  2.19591399,  1.24476387, ...,  0.87672677,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_forest.py"", line 229, in check_importances\r\n    assert_less(np.abs(importances - importances_bis).mean(), 0.001)\r\nAssertionError: 0.0011367665927937837 not less than 0.001\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_forest.test_importances(array([[-1.96100428,  2.19591399,  1.24476387, ...,  0.87672677,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_forest.py"", line 229, in check_importances\r\n    assert_less(np.abs(importances - importances_bis).mean(), 0.001)\r\nAssertionError: 0.0014722643948262063 not less than 0.001\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_forest.test_importances(array([[-1.96100428,  2.19591399,  1.24476387, ...,  0.87672677,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_forest.py"", line 229, in check_importances\r\n    assert_less(np.abs(importances - importances_bis).mean(), 0.001)\r\nAssertionError: 0.0010797068491393968 not less than 0.001\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_gradient_boosting.test_sparse_input(<class \'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier\'>, array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_gradient_boosting.py"", line 1041, in check_sparse_input\r\n    dense.feature_importances_)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 35.0%)\r\n x: array([ 0.089538,  0.086273,  0.036033,  0.084526,  0.046756,  0.072195,\r\n        0.030241,  0.142401,  0.096477,  0.040223,  0.037448,  0.027445,\r\n        0.025698,  0.026856,  0.037866,  0.      ,  0.      ,  0.046634,\r\n        0.039335,  0.034056])\r\n y: array([ 0.078375,  0.086273,  0.036033,  0.084526,  0.046756,  0.065001,\r\n        0.030241,  0.142401,  0.093059,  0.037069,  0.035723,  0.030442,\r\n        0.025698,  0.026856,  0.037866,  0.      ,  0.      ,  0.046634,\r\n        0.039335,  0.057713])\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_gradient_boosting.test_sparse_input(<class \'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier\'>, array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_gradient_boosting.py"", line 1041, in check_sparse_input\r\n    dense.feature_importances_)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 35.0%)\r\n x: array([ 0.089538,  0.086273,  0.036033,  0.084526,  0.046756,  0.072195,\r\n        0.030241,  0.142401,  0.096477,  0.040223,  0.037448,  0.027445,\r\n        0.025698,  0.026856,  0.037866,  0.      ,  0.      ,  0.046634,\r\n        0.039335,  0.034056])\r\n y: array([ 0.078375,  0.086273,  0.036033,  0.084526,  0.046756,  0.065001,\r\n        0.030241,  0.142401,  0.093059,  0.037069,  0.035723,  0.030442,\r\n        0.025698,  0.026856,  0.037866,  0.      ,  0.      ,  0.046634,\r\n        0.039335,  0.057713])\r\n\r\n======================================================================\r\nFAIL: sklearn.ensemble.tests.test_gradient_boosting.test_sparse_input(<class \'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier\'>, array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/ensemble/tests/test_gradient_boosting.py"", line 1041, in check_sparse_input\r\n    dense.feature_importances_)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 35.0%)\r\n x: array([ 0.089538,  0.086273,  0.036033,  0.084526,  0.046756,  0.072195,\r\n        0.030241,  0.142401,  0.096477,  0.040223,  0.037448,  0.027445,\r\n        0.025698,  0.026856,  0.037866,  0.      ,  0.      ,  0.046634,\r\n        0.039335,  0.034056])\r\n y: array([ 0.078375,  0.086273,  0.036033,  0.084526,  0.046756,  0.065001,\r\n        0.030241,  0.142401,  0.093059,  0.037069,  0.035723,  0.030442,\r\n        0.025698,  0.026856,  0.037866,  0.      ,  0.      ,  0.046634,\r\n        0.039335,  0.057713])\r\n\r\n======================================================================\r\nFAIL: sklearn.feature_selection.tests.test_from_model.test_feature_importances\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/feature_selection/tests/test_from_model.py"", line 93, in test_feature_importances\r\n    assert_almost_equal(importances, importances_bis)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 474, in assert_almost_equal\r\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 7 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.2539375,  0.2189745,  0.126979 ,  0.0607117,  0.0535292,\r\n        0.0625085,  0.0579722,  0.058656 ,  0.0500583,  0.0566731])\r\n y: array([ 0.2599767,  0.2110881,  0.1350195,  0.0588613,  0.0591183,\r\n        0.0590995,  0.055685 ,  0.056484 ,  0.0493844,  0.0552833])\r\n\r\n======================================================================\r\nFAIL: sklearn.tree.tests.test_export.test_graphviz_toy\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/build/scikit-learn-0.17.0~b1+git14-g4e6829c/debian/tmp/usr/lib/python2.7/dist-packages/sklearn/tree/tests/test_export.py"", line 182, in test_graphviz_toy\r\n    assert_equal(contents1, contents2)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 334, in assert_equal\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nItems are not equal:\r\n ACTUAL: \'digraph Tree {\\nnode [shape=box, style=""filled"", color=""black""] ;\\n0 [label=""X[1] <= 0.0\\\\nsamples = 6\\\\nvalue = [[3.0, 1.5, 0.0]\\\\n[1.5, 1.5, 1.5]]"", fillcolor=""#e5813900""] ;\\n1 [label=""X[1] <= -1.5\\\\nsamples = 3\\\\nvalue = [[3, 0, 0]\\\\n[1, 1, 1]]"", fillcolor=""#e5813966""] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=""True""] ;\\n2 [label=""samples = 1\\\\nvalue = [[1, 0, 0]\\\\n[0, 0, 1]]"", fillcolor=""#e58139ff""] ;\\n1 -> 2 ;\\n3 [label=""samples = 2\\\\nvalue = [[2, 0, 0]\\\\n[1, 1, 0]]"", fillcolor=""#e581398c""] ;\\n1 -> 3 ;\\n4 [label=""X[0] <= 1.5\\\\nsamples = 3\\\\nvalue = [[0.0, 1.5, 0.0]\\\\n[0.5, 0.5, 0.5]]"", fillcolor=""#e5813966""] ;\\n0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=""False""] ;\\n5 [label=""samples = 2\\\\nvalue = [[0.0, 1.0, 0.0]\\\\n[0.5, 0.5, 0.0]]"", fillcolor=""#e581398c""] ;\\n4 -> 5 ;\\n6 [label=""samples = 1\\\\nvalue = [[0.0, 0.5, 0.0]\\\\n[0.0, 0.0, 0.5]]"", fillcolor=""#e58139ff""] ;\\n4 -> 6 ;\\n}\'\r\n DESIRED: \'digraph Tree {\\nnode [shape=box, style=""filled"", color=""black""] ;\\n0 [label=""X[0] <= 0.0\\\\nsamples = 6\\\\nvalue = [[3.0, 1.5, 0.0]\\\\n[1.5, 1.5, 1.5]]"", fillcolor=""#e5813900""] ;\\n1 [label=""X[1] <= -1.5\\\\nsamples = 3\\\\nvalue = [[3, 0, 0]\\\\n[1, 1, 1]]"", fillcolor=""#e5813965""] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=""True""] ;\\n2 [label=""samples = 1\\\\nvalue = [[1, 0, 0]\\\\n[0, 0, 1]]"", fillcolor=""#e58139ff""] ;\\n1 -> 2 ;\\n3 [label=""samples = 2\\\\nvalue = [[2, 0, 0]\\\\n[1, 1, 0]]"", fillcolor=""#e581398c""] ;\\n1 -> 3 ;\\n4 [label=""X[0] <= 1.5\\\\nsamples = 3\\\\nvalue = [[0.0, 1.5, 0.0]\\\\n[0.5, 0.5, 0.5]]"", fillcolor=""#e5813965""] ;\\n0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=""False""] ;\\n5 [label=""samples = 2\\\\nvalue = [[0.0, 1.0, 0.0]\\\\n[0.5, 0.5, 0.0]]"", fillcolor=""#e581398c""] ;\\n4 -> 5 ;\\n6 [label=""samples = 1\\\\nvalue = [[0.0, 0.5, 0.0]\\\\n[0.0, 0.0, 0.5]]"", fillcolor=""#e58139ff""] ;\\n4 -> 6 ;\\n}\'\r\n\r\n----------------------------------------------------------------------\r\nRan 5332 tests in 112.213s\r\n\r\nFAILED (SKIP=21, failures=11)\r\n```'"
5598,113471475,TomDLT,amueller,2015-10-26 23:22:09,2015-11-01 23:05:30,2015-11-01 23:05:30,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5598,"b'App_veyor failure on master, in test_logistic_regression_sample_weights'","b'Appveyor seems to fail on master (ex: [here](https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.3299), [here](https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.3302), or [here](https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.3206)), with always the same error:\r\n```\r\nFAIL: sklearn.linear_model.tests.test_logistic.test_logistic_regression_sample_weights\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python35\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python35\\lib\\site-packages\\sklearn\\linear_model\\tests\\test_logistic.py"", line 615, in test_logistic_regression_sample_weights\r\n    clf_sw_lbfgs.coef_, clf_sw_sag.coef_, decimal=4)\r\n  File ""C:\\Python35\\lib\\site-packages\\numpy\\testing\\utils.py"", line 842, in assert_array_almost_equal\r\n    precision=decimal)\r\n  File ""C:\\Python35\\lib\\site-packages\\numpy\\testing\\utils.py"", line 665, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 4 decimals\r\n \r\n(mismatch 100.0%)\r\n x: array([[-0.2551, -0.0118, -0.0619,  0.1805,  0.028 ]])\r\n y: array([[ -9.1461e-04,  -6.9190e-05,  -2.2578e-04,   6.5820e-04,\r\n          1.0460e-04]])\r\n```\r\nI couldn\'t reproduce the error on the following environment:\r\nWindows 7 (64 bit) Python 2.7.10 (32 bit)\r\nNumpy 1.9.2 and Scipy 0.15.1\r\n\r\nThe regression probably comes from #5274 or #5008 which seems to have avoided appveyor attention during the sprint.'"
5536,112835768,Bordbonen,amueller,2015-10-22 16:01:19,2015-11-02 02:58:49,2015-11-02 02:58:49,closed,,0.17,14,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5536,b'sklearn.preprocessing.LabelEncoder transform wrong output',"b'Python 3.4\r\nWindows 7\r\nMetadata-Version: 2.0\r\nName: scikit-learn\r\nVersion: 0.16.1\r\n\r\nTrying to use labelEncoder() to label mulltiple classes. But, when using le.transform(\'something\'), often the wrong values are received.\r\nThouhgt that it maybe has to do with my encoding, but you can see it doesnt matter.\r\n\r\nThanks in Advance!\r\n\r\nExample code:\r\n\r\nPython 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n\r\n    >>> import sys\r\n\t>>> sys.stdin.encoding\r\n\t\'utf-8\'\r\n\t>>> from sklearn import preprocessing\r\n\t>>> le = preprocessing.LabelEncoder()\r\n\t>>> list = [\'PLACE\', \'JOBTITLE\', \'CONCEPT\', \'ORGANISATION\', \'PERSON\', \'EVENT\', \'WORK\']\r\n\t>>> le.fit(list)\r\n\tLabelEncoder()\r\n\t>>> fit.classes_\r\n\tTraceback (most recent call last):\r\n\t  File ""<stdin>"", line 1, in <module>\r\n\tNameError: name \'fit\' is not defined\r\n\t>>> le.classes_\r\n\tarray([\'PLACE\', \'JOBTITLE\', \'CONCEPT\', \'ORGANISATION\', \'PERSON\', \'EVENT\',\r\n\t       \'WORK\'],\r\n\t      dtype=\'<U12\')\r\n\t>>> le.transform(\'PLACE\')\r\n\t6\r\n\t>>> import codecs\r\n\t>>> codecs.register(lambda name: codecs.lookup(\'utf-8\') if name == \'cp65001\' else None)\r\n\t>>> le.transform(\'PLACE\')\r\n\t6\r\n\t>>> le.transform(\'JOBTITLE\')\r\n\t0\r\n\t>>> le.transform(\'CONCEPT\')\r\n\t0\r\n\t>>> le.transform(\'ORGANISATION\')\r\n\t3\r\n        ------------------------\r\n\r\nAnd using inverser_transform on these values will give the wrong categories\r\nPlace becomes: Work\r\nJobtitle, Concept become: Place\r\n\r\nEtc\r\n\r\n'"
5502,112539500,amueller,amueller,2015-10-21 08:47:20,2015-11-03 19:06:26,2015-11-03 19:06:26,closed,,0.17,8,Bug;Moderate;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5502,b'RobustScaler scaling one row (and scaling sparse matrix)',"b""Scaling a single row in RobustScaler doesn't work as expected.\r\nSee #5433 and #5449.\r\nping @untom @Jeffrey04\r\n\r\nI think also we should raise a ``ValueError`` in ``transform`` if ``issparse(X) and self.with_centering``."""
5495,112400800,amueller,amueller,2015-10-20 15:53:51,2015-10-23 14:31:26,2015-10-23 14:31:26,closed,,0.17,10,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5495,"b'OVR(SVC()) fails with decision_function_shape=""ovr""'","b""This fails in ``predict``:\r\n\r\n```python\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.multiclass import OneVsRestClassifier\r\n\r\nfrom sklearn.datasets import make_blobs\r\n\r\nX, y = make_blobs()\r\n\r\nclf = OneVsRestClassifier(SVC(decision_function_shape='ovr')).fit(X, y)\r\nclf.predict(X)\r\n```"""
5474,112293080,GaelVaroquaux,ogrisel,2015-10-20 05:58:51,2015-10-20 11:36:54,2015-10-20 11:36:54,closed,,,8,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5474,b'Gaussian Process examples require too recent version of matplotlib',"b'Looks like the example are dependent on a very recent version of matplotlib, and that breaks CircleCI (our new CI infrastructure that is meant for testing the examples:):\r\n\r\nhttps://circleci.com/gh/scikit-learn/scikit-learn/51\r\n\r\nSpecifically, in plot_gpc.py, there is, on line 95, plt.colorbar(label=""Log-marginal Likelihood"")\r\n\r\nAnd the version of matplotlib that we have doesn\'t like the \'label\' argument to colorbar.'"
5450,112127397,TomDLT,amueller,2015-10-19 11:41:34,2015-10-22 10:44:37,2015-10-22 10:44:37,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5450,b'LogisticRegressionCV handles class weights differently',"b'These two classifiers are not equivalent:\r\n```python\r\nclass_weights = compute_class_weight(""balanced"", np.unique(y), y)\r\nclf1 = LogisticRegressionCV(class_weight=""balanced"")\r\nclf2 = LogisticRegressionCV(class_weight=class_weights)\r\n```\r\nIndeed, the first one computes different `class_weights` for every folds, whereas the second one uses the same `class_weights` for every folds. That is why have [this line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tests/test_common.py#L116)\r\n\r\nI am working on it'"
5449,112126531,Jeffrey04,ogrisel,2015-10-19 11:34:50,2015-10-21 11:10:45,2015-10-21 09:42:27,closed,,0.17,31,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5449,b'[MRG + 1] max abs scaler 1 row csr fix',b'an attempt to fix #5433'
5433,112074655,Jeffrey04,ogrisel,2015-10-19 05:31:25,2015-10-21 09:42:27,2015-10-21 09:42:27,closed,,,6,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5433,b'MaxAbsScaler is unable to scale a 1-row sparse matrix',"b'Suppose I have my collection of data scaled with MinMaxScaler. Then I need to transform a new sparse matrix with one row (sparse vector?), eg.\r\n\r\n```\r\n  (0, 56839)\t0.462743526481\r\n  (0, 55421)\t0.469655562306\r\n  (0, 54368)\t0.203714596644\r\n  (0, 54060)\t0.0962621236939\r\n  (0, 51441)\t0.540495850676\r\n  (0, 48518)\t0.056152354043\r\n  (0, 45181)\t0.0652388777274\r\n  (0, 38682)\t0.230776053348\r\n  (0, 31876)\t0.199738544715\r\n  (0, 14641)\t0.280892719445\r\n  (0, 434)\t0.207189026352\r\n```\r\n\r\nthe shape is (1, 58188)\r\n\r\nHowever, if I attempt to transform the 1-row sparse matrix above (so I can do comparison with the training data), I get this assertion error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""./address-query.py"", line 41, in <module>\r\n    main()\r\n  File ""./address-query.py"", line 28, in main\r\n    query = scaler.transform(query_)[0].toarray()\r\n  File ""/Users/jeffrey04/.local/lib/python3.5/site-packages/sklearn/preprocessing/data.py"", line 792, in transform\r\n    inplace_row_scale(X, 1.0 / self.scale_)\r\n  File ""/Users/jeffrey04/.local/lib/python3.5/site-packages/sklearn/utils/sparsefuncs.py"", line 200, in inplace_row_scale\r\n    inplace_csr_row_scale(X, scale)\r\n  File ""/Users/jeffrey04/.local/lib/python3.5/site-packages/sklearn/utils/sparsefuncs.py"", line 61, in inplace_csr_row_scale\r\n    assert scale.shape[0] == X.shape[0]\r\n```\r\n\r\nthe value for scaler._scale.shape is (58188,)\r\n\r\nThe workaround to the problem is to use a matrix that has more than 1 row, due to this part of the code (if I am not mistaken)\r\n\r\n```\r\n        if sparse.issparse(X):\r\n            if X.shape[0] == 1:\r\n                inplace_row_scale(X, self.scale_)\r\n            else:\r\n                inplace_column_scale(X, self.scale_)\r\n```\r\n\r\nI am using 0.17b1 of scikit-learn with python 3.5, on os x 10.10 yosemite (installed by homebrew)'"
5422,111859825,amueller,amueller,2015-10-16 15:57:01,2015-10-16 16:05:56,2015-10-16 16:05:56,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5422,b'logistic regression with class weight balanced',"b'There is some special treatment for OVR and multinomial in logistic regression for class-weight ``""auto""`` but not for ``""balanced""``. I assume that\'s a bug.'"
5415,111744685,luoq,ogrisel,2015-10-16 02:16:33,2015-10-30 13:30:45,2015-10-30 13:30:45,closed,,0.17,11,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5415,"b""use class_weight='auto' and multi_class='multinomial' in LogisticRegression cause UnboundLocalError in line 637 of logistic.py""","b'code to reproduce\r\n```python\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.linear_model import LogisticRegression\r\nX, y = make_classification(n_classes=5, n_informative=5, random_state=42)\r\nmodel = LogisticRegression(multi_class=\'multinomial\', class_weight=\'auto\', solver=\'lbfgs\')\r\nmodel.fit(X, y)\r\n```\r\n\r\ninnermost exception\r\n```\r\n/home/qiang.luo/.local/lib/python3.4/site-packages/sklearn/linear_model/logistic.py in logistic_regression_path(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\r\n    635     if class_weight == ""auto"":\r\n    636         class_weight_ = compute_class_weight(class_weight, mask_classes,\r\n--> 637                                              y_bin)\r\n    638         sample_weight *= class_weight_[le.fit_transform(y_bin)]\r\n    639 \r\n\r\nUnboundLocalError: local variable \'y_bin\' referenced before assignment\r\n```'"
5412,111696491,amueller,amueller,2015-10-15 20:02:49,2015-10-22 13:09:22,2015-10-22 13:09:22,closed,,,16,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5412,b'odd Joblib error in test suite (not sure if we talked about that already)',"b""Running the unit tests outputs\r\n```\r\nsklearn.metrics.tests.test_pairwise.test_pairwise_parallel(<function pairwise_kernels at 0x7f0b2fcdba28>, <function callable_rbf_kernel at 0x7f0b27bf5de8>, {'gamma': 0.1}) ... An unexpected error occurred wh\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: ('EOF in multi-line statement', (105, 0))\r\n\r\nAn unexpected error occurred while tokenizing input file /home/andy/checkout/scikit-learn/sklearn/metrics/pairwise.pyc\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: ('EOF in multi-line statement', (105, 0))\r\n\r\nok\r\n```\r\nat some point.\r\nThat seems odd."""
5408,111624662,lesteve,amueller,2015-10-15 14:08:54,2015-10-16 15:47:27,2015-10-16 15:47:27,closed,,0.17,3,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5408,b'Broken example: examples/svm/plot_rbf_parameters.py',"b'```\r\n--------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/le243287/dev/scikit-learn/examples/svm/plot_rbf_parameters.py in <module>()\r\n    117 scaler = StandardScaler()\r\n    118 X = scaler.fit_transform(X)\r\n--> 119 X_2d = scaler.fit_transform(X_2d)\r\n    120 \r\n    121 ##############################################################################\r\n\r\n/home/le243287/dev/scikit-learn/sklearn/base.pyc in fit_transform(self, X, y, **fit_params)\r\n    453         if y is None:\r\n    454             # fit method of arity 1 (unsupervised transformation)\r\n--> 455             return self.fit(X, **fit_params).transform(X)\r\n    456         else:\r\n    457             # fit method of arity 2 (supervised transformation)\r\n\r\n/home/le243287/dev/scikit-learn/sklearn/preprocessing/data.pyc in fit(self, X, y)\r\n    501         y: Passthrough for ``Pipeline`` compatibility.\r\n    502         """"""\r\n--> 503         return self.partial_fit(X, y)\r\n    504 \r\n    505     def partial_fit(self, X, y=None):\r\n\r\n/home/le243287/dev/scikit-learn/sklearn/preprocessing/data.pyc in partial_fit(self, X, y)\r\n    565             self.mean_, self.var_, self.n_samples_seen_ = \\\r\n    566                 _incremental_mean_and_var(X, self.mean_, self.var_,\r\n--> 567                                           self.n_samples_seen_)\r\n    568 \r\n    569         if self.with_std:\r\n\r\n/home/le243287/dev/scikit-learn/sklearn/utils/extmath.pyc in _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count)\r\n    730     updated_sample_count = last_sample_count + new_sample_count\r\n    731 \r\n--> 732     updated_mean = (last_sum + new_sum) / updated_sample_count\r\n    733 \r\n    734     if last_variance is None:\r\n\r\nValueError: operands could not be broadcast together with shapes (4,) (2,)\r\n```'"
5407,111624561,lesteve,amueller,2015-10-15 14:08:25,2015-10-21 08:58:04,2015-10-21 08:58:04,closed,,0.17,6,Blocker;Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5407,b'Broken example: examples/model_selection/plot_roc.py',"b'```\r\n--------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/le243287/dev/scikit-learn/examples/model_selection/plot_roc.py in <module>()\r\n    100 \r\n    101 # Compute macro-average ROC curve and ROC area\r\n--> 102 fpr[""macro""] = np.mean([fpr[i] for i in range(n_classes)], axis=0)\r\n    103 tpr[""macro""] = np.mean([tpr[i] for i in range(n_classes)], axis=0)\r\n    104 roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\r\n\r\n/volatile/le243287/miniconda3/envs/py27/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc in mean(a, axis, dtype, out, keepdims)\r\n   2733 \r\n   2734     return _methods._mean(a, axis=axis, dtype=dtype,\r\n-> 2735                             out=out, keepdims=keepdims)\r\n   2736 \r\n   2737 def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\r\n\r\n/volatile/le243287/miniconda3/envs/py27/lib/python2.7/site-packages/numpy/core/_methods.pyc in _mean(a, axis, dtype, out, keepdims)\r\n     64         dtype = mu.dtype(\'f8\')\r\n     65 \r\n---> 66     ret = umr_sum(arr, axis, dtype, out, keepdims)\r\n     67     if isinstance(ret, mu.ndarray):\r\n     68         ret = um.true_divide(\r\n\r\nValueError: operands could not be broadcast together with shapes (21,) (35,) \r\n```'"
5406,111624435,lesteve,amueller,2015-10-15 14:07:44,2015-10-15 19:18:53,2015-10-15 19:18:53,closed,,0.17,0,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5406,b'Broken example: examples/manifold/plot_lle_digits.py',"b'```\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n/home/le243287/dev/scikit-learn/examples/manifold/plot_lle_digits.py in <module>()\r\n    112 X2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\r\n    113 t0 = time()\r\n--> 114 X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y)\r\n    115 plot_embedding(X_lda,\r\n    116                ""Linear Discriminant projection of the digits (time %.2fs)"" %\r\n\r\nNameError: name \'discriminant_analysis\' is not defined\r\n```'"
5405,111624156,lesteve,glouppe,2015-10-15 14:06:15,2015-10-17 07:24:17,2015-10-17 07:24:17,closed,,0.17,4,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5405,b'Broken example: examples/ensemble/plot_random_forest_embedding.py',"b'```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/le243287/dev/scikit-learn/examples/ensemble/plot_random_forest_embedding.py in <module>()\r\n     45 # Visualize result using PCA\r\n     46 pca = TruncatedSVD(n_components=2)\r\n---> 47 X_reduced = pca.fit_transform(X_transformed)\r\n     48 \r\n     49 # Learn a Naive Bayes classifier on the transformed data\r\n\r\n/home/le243287/dev/scikit-learn/sklearn/decomposition/truncated_svd.pyc in fit_transform(self, X, y)\r\n    165             if k >= n_features:\r\n    166                 raise ValueError(""n_components must be < n_features;""\r\n--> 167                                  "" got %d >= %d"" % (k, n_features))\r\n    168             U, Sigma, VT = randomized_svd(X, self.n_components,\r\n    169                                           n_iter=self.n_iter,\r\n\r\nValueError: n_components must be < n_features; got 2 >= 1\r\n```'"
5404,111624013,lesteve,ogrisel,2015-10-15 14:05:27,2015-10-16 14:57:38,2015-10-16 14:57:38,closed,,0.17,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5404,b'Broken example: examples/applications/plot_tomography_l1_reconstruction.py',"b""Only numpy 1.10, error is:\r\n\r\n`TypeError: Cannot cast ufunc add output from dtype('float64') to dtype('int64') with casting rule 'same_kind'`\r\n"""
5397,111287518,amueller,amueller,2015-10-13 22:48:04,2016-02-29 19:54:28,2015-10-15 16:45:43,closed,,0.17,6,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5397,b'Numpy 1.10.1 compatibility',"b""I get a lot of errors when I just upgraded to numpy 1.10.1 using conda.\r\nMostly they are about casting:\r\n```\r\nTypeError: Cannot cast ufunc subtract output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\r\n```\r\n\r\nDid I do something odd or are we just not numpy 1.10 compatible?"""
5389,111058861,l3link,amueller,2015-10-12 21:23:13,2015-11-02 18:04:22,2015-11-02 18:04:22,closed,,0.17,5,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5389,b'Missing Documentation for Pandas DataFrame Integration with sklearn.cross_validation.train_test_split',"b'The current documentation (0.16.1) does not mention that you can pass in (and will get as output) pandas data frames to the train_test_split method.  This was especially unfortunate given that the change between 0.15* to 0.16 resulted in a different output when a pandas dataframe is used as input (.15 returns numpy arrays, and .16 returns a list of pandas dataframes).'"
5371,110550137,ylow,jnothman,2015-10-08 22:00:32,2015-10-10 10:06:37,2015-10-10 10:06:37,closed,,0.17,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5371,b'[MRG + 1] Increase length of array indexing type in ArrayDataset',"b'int (32-bit on all platforms) will loop around once the number of elements exceed 2^31-1. While ints are used elsewhere for row index, this is particularly significant here, since this int is used as an index into a dense matrix. i.e. as long as #rows * #cols is greater than 2^31, this will fail. This is probably the root cause of #2393.'"
5358,110242037,arthurmensch,amueller,2015-10-07 14:47:44,2015-11-02 03:00:59,2015-11-02 03:00:25,closed,,0.17,15,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5358,"b'[MRG+1] Fix fit_transform, stability issue and scale issue in PLS'","b'This PR fixes stability issue and sign indeterminacy in PLS and CCA (see bug #2821).\r\n\r\nThree issues were adressed:\r\n- `fit_transform` did not work for obvious reason. I fixed this and change estimator_checks so that it do not overlook Transformer unable to perform `fit_transform` without a previous `fit`\r\n- scipy.linalg.pinv is based on `lstsq` and is subject to more numerical instability than `scipy.linalg.pinv2`, which, quite amusingly, is the same as `numpy.linalg.pinv`, and is based on SVD decomposition\r\n- PLS is subject to sign indeterminacy (like any matrix decomposition method). Similar to `svd_flip`, we fix this within PLS code.\r\n\r\nThe signs of `x_loadings_`, `x_score_`, `x_weights_`, `x_rotations_` can differ columnwise from R implementation, as there is a sign indeterminacy that we seek to raise (cf SVD with `svd_flip`).\r\n\r\n- [x] Since `test_pls` is based on R output, I still need to change it so that it does not fail because of sign differences.\r\n\r\nping @twiecki, @Fenugreek for PLS proficient reviews'"
5351,110152494,cel4,ogrisel,2015-10-07 05:00:53,2015-10-12 11:32:27,2015-10-12 11:32:27,closed,,0.17,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5351,b'LASSO: fit_intercept - conversion logic missing?',"b'```python\r\nfrom sklearn import linear_model\r\nimport numpy as np\r\nnp.random.seed(42)\r\n\r\nX = np.random.randint(0, 9, size=(4,4))\r\ny = np.random.randint(0, 9, size=4)\r\n\r\n# works\r\nmodel = linear_model.Lasso(alpha=0.1, fit_intercept=True)\r\nmodel.fit(X, y)\r\n\r\n# ou, oh :(\r\nmodel = linear_model.Lasso(alpha=0.1, fit_intercept=False)\r\nmodel.fit(X, y)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-57f2c63422ba> in <module>()\r\n     12 # ou, oh :(\r\n     13 model = linear_model.Lasso(alpha=0.1, fit_intercept=False)\r\n---> 14 model.fit(X, y)\r\n\r\n/Users/ch/miniconda/envs/sci34/lib/python3.4/site-packages/sklearn/linear_model/coordinate_descent.py in fit(self, X, y)\r\n    671                           coef_init=coef_[k], max_iter=self.max_iter,\r\n    672                           random_state=self.random_state,\r\n--> 673                           selection=self.selection)\r\n    674             coef_[k] = this_coef[:, 0]\r\n    675             dual_gaps_[k] = this_dual_gap[0]\r\n\r\n/Users/ch/miniconda/envs/sci34/lib/python3.4/site-packages/sklearn/linear_model/coordinate_descent.py in enet_path(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, **params)\r\n    430             model = cd_fast.enet_coordinate_descent(\r\n    431                 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random,\r\n--> 432                 positive)\r\n    433         else:\r\n    434             raise ValueError(""Precompute should be one of True, False, ""\r\n\r\nsklearn/linear_model/cd_fast.pyx in sklearn.linear_model.cd_fast.enet_coordinate_descent (sklearn/linear_model/cd_fast.c:2839)()\r\n\r\nValueError: Buffer dtype mismatch, expected \'DOUBLE\' but got \'long\'\r\n```'"
5340,109661779,DmitriR,DmitriR,2015-10-04 04:07:37,2015-10-27 23:06:16,2015-10-27 23:06:16,closed,,,21,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5340,b'AffinityPropagation bug? RuntimeWarning: Mean of empty slice',"b'Slight modification of the parameters (cluster_std=0.7) in the demo code: http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html\r\n\r\n    X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.7, random_state=0)\r\n\r\nproduces a blown-up result (fig below) and a warning:\r\n\r\n`/anaconda/anaconda3/anaconda/lib/python3.4/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\r\n  warnings.warn(""Mean of empty slice."", RuntimeWarning)`\r\n\r\nversion: \'0.16.1\', OSX, Jupyter notebook\r\n\r\ncluster_std=0.7 looks like a \'magic value\'; e.g. cluster_std=0.69 and cluster_std=0.71 produce a reasonably-looking result. \r\n\r\n(orange dots: the intended centers, added to the plot by me)\r\n\r\n![t](https://cloud.githubusercontent.com/assets/13791275/10266366/9ba33bb4-6a2a-11e5-9c86-0d39b95278bb.png)\r\n'"
5324,108622324,cosmos2006,jnothman,2015-09-28 09:42:26,2015-10-15 20:47:14,2015-10-15 20:47:14,closed,,0.17,4,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5324,b'FAIL: Doctest: sklearn.neighbors.approximate.LSHForest',"b'while buidling the scikit.learn, I encountered following error. Please have a look at it.\r\n\r\nRegards\r\ncosmos\r\n```\r\nFAIL: Doctest: sklearn.neighbors.approximate.LSHForest\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib64/python2.7/doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.neighbors.approximate.LSHForest\r\n  File ""/usr/local/scikit-learn-master/sklearn/neighbors/approximate.py"", line 110, in LSHForest\r\n\r\n----------------------------------------------------------------------\r\nFile ""/usr/local/scikit-learn-master/sklearn/neighbors/approximate.py"", line 198, in sklearn.neighbors.approximate.LSHForest\r\nFailed example:\r\n    distances                                        # doctest: +ELLIPSIS\r\nExpected:\r\n    array([[ 0.069...,  0.149...],\r\n           [ 0.229...,  0.481...],\r\n           [ 0.004...,  0.014...]])\r\nGot:\r\n    array([[ 0.0693931 ,  0.14960959],\r\n           [ 0.22932536,  0.49225586],\r\n           [ 0.00489444,  0.01481913]])\r\n----------------------------------------------------------------------\r\nFile ""/usr/local/scikit-learn-master/sklearn/neighbors/approximate.py"", line 202, in sklearn.neighbors.approximate.LSHForest\r\nFailed example:\r\n```'"
5267,106339018,myflyinggip,ogrisel,2015-09-14 13:19:39,2015-10-16 08:33:51,2015-10-16 07:18:56,closed,,0.17,21,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5267,b'Problem in Using LogisticRegressionCV',"b'Hi guys, \r\n\r\nNot sure if anyone has experienced this, but LogisticRegressionCV seems to have some problems when the predictor matrix for training (e.g., X_train) is sliced from some bigger matrix. Apologies that I might be using some incorrect code markdown below but I\'m trying to attach my example code. The code uses the digits data from sklearn. I put three possible ways of getting X_train and feed it into LogisticRegressionCV, labled as \'Safe\', \'Dodgy 1\' and \'Dodgy 2\'. In \'Dodgy 2\', for example, I slice X_train from a bigger matrix and then change a different part of the bigger matrix that isn\'t a part of the sliced result, the resultant AUC changes (from 0.5 to below 0.5). \r\n\r\nI suspect this is because some inappropriate use of underlying memory. Does anyone have any comments? Thanks a lot! \r\n\r\nMy example code: \r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\nfrom sklearn import datasets\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\r\n\r\nimport timeit\r\nimport os\r\nimport time\r\nimport datetime\r\n\r\n\r\n\r\ndef main():\r\n\r\n    digits = datasets.load_digits()\r\n    X, y = digits.data, digits.target\r\n    X = StandardScaler().fit_transform(X)\r\n    data = np.hstack((np.reshape(y.astype(float), (y.size,1)), X))\r\n\r\n    X = data[:, 1:]\r\n    y = data[:, 0]\r\n\r\n    \r\n\r\n\r\n    ts = time.time()\r\n    st = datetime.datetime.fromtimestamp(ts).strftime(\'%Y%m%d_%H%M%S\')\r\n    result_dir = ""../Results/"" + st + ""/""\r\n    if not os.path.exists(result_dir):\r\n        os.makedirs(result_dir)\r\n\r\n\r\n    # classify small against large digits\r\n    y = (y > 4).astype(np.int)\r\n\r\n\r\n\r\n\r\n    train_percent = 0.5\r\n    nTrains = train_percent * y.size\r\n    X_train = X[:nTrains, :]\r\n    y_train = y[:nTrains]\r\n    X_test = X[nTrains:, :]\r\n    y_test = y[nTrains:]\r\n\r\n\r\n\r\n    # This has problems if using \'liblinear\' as the solver. See below.\r\n    logistic_reg = LogisticRegressionCV(cv=10, penalty=\'l2\', tol=0.01, solver=""liblinear"")\r\n\r\n\r\n\r\n    # Three different choices below\r\n\r\n    # Safe: use a copy of the sliced data. This produces a good (as expected) AUC.\r\n    model = logistic_reg.fit(np.copy(X_train), y_train)\r\n\r\n    # # Dodgy 1. use the data sliced from the data matrix directly (reference). This produces an AUC = 0.5.\r\n    # model = logistic_reg.fit(X_train, y_train)\r\n\r\n    # # Dodgy 2. make a copy of the original data matrix, slice X_train from the copy,\r\n    # # but also change the other part in the copy that\'s not sliced.\r\n    # # This makes the result AUC < 0.5\r\n    # data_copy = np.copy(data)\r\n    # X_train = data_copy[:nTrains,1:]\r\n    # data_copy[:,0].fill(0)\r\n    # model = logistic_reg.fit(X_train, y_train)\r\n\r\n\r\n\r\n\r\n    # prediction\r\n\r\n    y_pred = model.predict_proba(X_test)\r\n\r\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred[:,1])\r\n    roc_auc = auc(fpr, tpr)\r\n\r\n    accuracy = 1 - float(np.sum(np.abs(y_pred[:,1] - y_test))) / y_pred[:,1].size\r\n    print ""accuracy:"", accuracy\r\n\r\n\r\n\r\n    plt.figure()\r\n    plt.plot(fpr, tpr, label=\'ROC curve (area = %0.2f)\' % roc_auc)\r\n    plt.plot([0, 1], [0, 1], \'k--\')\r\n    plt.xlim([0.0, 1.0])\r\n    plt.ylim([0.0, 1.05])\r\n    plt.xlabel(\'False Positive Rate\')\r\n    plt.ylabel(\'True Positive Rate\')\r\n    plt.title(\'Receiver operating characteristic example\')\r\n    plt.legend(loc=""lower right"")\r\n\r\n    plt.savefig(result_dir + ""auc.png"")\r\n    plt.show()\r\n\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    start_time = timeit.default_timer()\r\n\r\n    main()\r\n\r\n    elapsed = timeit.default_timer() - start_time\r\n    print ""Time elapsed:"",elapsed\r\n```\r\n'"
5246,105814552,aesuli,ogrisel,2015-09-10 14:12:12,2015-10-07 09:43:48,2015-10-07 09:43:48,closed,,0.17,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5246,b'Second call to SGD partial_fit fails in multiclass setup with averaging turned on',"b'Test code:\r\n\r\n```\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\n\r\nbinclasses = [\'a\',\'b\']\r\nmulticlasses = [\'a\',\'b\',\'c\']\r\n\r\n\r\n#ok\r\nclf = SGDClassifier(average=False)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],binclasses)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],binclasses)\r\n\r\n#ok\r\nclf = SGDClassifier(average=False)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],multiclasses)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],multiclasses)\r\n\r\n#ok\r\nclf = SGDClassifier(average=10)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],binclasses)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],binclasses)\r\n\r\n#the second partial_fit fails\r\nclf = SGDClassifier(average=10)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],multiclasses)\r\nclf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],multiclasses)\r\n```\r\n\r\nthis last call fails with exception\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""multiclass_average_bug.py"", line 32, in <module>\r\n    clf.partial_fit([[1,0,0],[0,1,0]],[\'a\',\'b\'],multiclasses)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py"", line 526, in partial_fit\r\n    coef_init=None, intercept_init=None)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py"", line 387, in _partial_fit\r\n    sample_weight=sample_weight, n_iter=n_iter)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py"", line 471, in _fit_multiclass\r\n    for i in range(len(self.classes_)))\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 659, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 406, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 140, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py"", line 272, in fit_binary\r\n    _prepare_fit_binary(est, y, i)\r\n  File ""C:\\Programs\\Python3.4\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py"", line 256, in _prepare_fit_binary\r\n    intercept = est.standard_intercept_[i]\r\nIndexError: index 1 is out of bounds for axis 0 with size 1\r\n```\r\n\r\nThis happens because est.standard_intercept_ is of size one instead of size three, as needed by the multiclass setup of the test case.\r\n\r\nI have found the issue originates from [_fit_multiclass function in stochastic_gradient.py, line 485](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradient.py#L485):\r\n\r\n```\r\nself.standard_intercept_ = np.atleast_1d(intercept)\r\n```\r\n\r\nI fixed it by replacing the above code with\r\n\r\n```\r\nself.standard_intercept_ = np.atleast_1d(self.intercept_)\r\n```\r\n\r\nbecause self.intercept_ is properly created using the intercept values from the various OvA jobs in lines 474-475; also, line 486 can be removed.\r\n\r\nChanging the code as I suggest solves the issue, but the role of self.standard_intercept_ and its relation with self.intercept_ is not 100% clear to me, so I would like a check from people with more experience on sklearn code before calling it a bugfix.'"
5235,105675377,amueller,ogrisel,2015-09-09 20:12:02,2015-10-07 09:26:04,2015-10-07 09:26:04,closed,,0.17,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5235,b'Ridge class weight failure on appveyor',b'https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.2018/job/8tf9xbl5i9j2dcyp'
5177,103666309,FRidh,amueller,2015-08-28 08:00:52,2016-01-20 15:10:27,2016-01-20 15:10:27,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5177,b'0.16.1: Failing doctests on i686',"b'When using scikit-learn 0.16.1 with numpy 1.9.2 on i686 system certain doctests fail. They don\'t seem to fail on x86-64 systems though. I haven\'t tested with master.\r\n\r\nIn all cases what\'s missing is the dtype of the array. Here\'s an example:\r\n```py\r\nFile ""/tmp/nix-build-python3.4-scikit-learn-0.16.1.drv-0/scikit-learn-0.16.1/sklearn/preprocessing/data.py"", line 1034, in sklearn.preprocessing.data.OneHotEncoder\r\nFailed example:\r\n    enc.feature_indices_\r\nExpected:\r\n    array([0, 2, 5, 9])\r\nGot:\r\n    array([0, 2, 5, 9], dtype=int32)\r\n\r\n>>  raise self.failureException(self.format_failure(<_io.StringIO object at 0xf0915b6c>.getvalue()))\r\n```\r\nand here\'s the log\r\nhttp://hydra.nixos.org/build/25129295/nixlog/1/raw\r\n'"
5164,103312114,kevin-coder,amueller,2015-08-26 16:28:55,2015-11-02 01:02:59,2015-11-02 01:00:30,closed,,0.17,13,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5164,"b'After build and install scikit-learn, test will fali'","b'Pre-requisitions:\r\n1. Ubuntu 15.10, Python 2.7.10, gcc version 5.2.1 20150808 (Ubuntu 5.2.1-15ubuntu2)\r\nReproduce steps:\r\n1. Git clone latest version of code from github.\r\n2. Install dependencies\r\n3. Build and install the scikit-learn from latest code.\r\n4. Execute test via command: nosetests -v sklearn\r\n\r\nOutput Result:\r\n```\r\n======================================================================\r\nFAIL: sklearn.tree.tests.test_export.test_graphviz_toy\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/tree/tests/test_export.py"", line 182, in test_graphviz_toy\r\n    assert_equal(contents1, contents2)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 317, in assert_equal\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nItems are not equal:\r\n ACTUAL: \'digraph Tree {\\nnode [shape=box, style=""filled"", color=""black""] ;\\n0 [label=""X[0] <= 0.0\\\\nsamples = 6\\\\nvalue = [[3.0, 1.5, 0.0]\\\\n[1.5, 1.5, 1.5]]"", fillcolor=""#e5813900""] ;\\n1 [label=""X[1] <= -1.5\\\\nsamples = 3\\\\nvalue = [[3, 0, 0]\\\\n[1, 1, 1]]"", fillcolor=""#e5813966""] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=""True""] ;\\n2 [label=""samples = 1\\\\nvalue = [[1, 0, 0]\\\\n[0, 0, 1]]"", fillcolor=""#e58139ff""] ;\\n1 -> 2 ;\\n3 [label=""samples = 2\\\\nvalue = [[2, 0, 0]\\\\n[1, 1, 0]]"", fillcolor=""#e581398c""] ;\\n1 -> 3 ;\\n4 [label=""X[0] <= 1.5\\\\nsamples = 3\\\\nvalue = [[0.0, 1.5, 0.0]\\\\n[0.5, 0.5, 0.5]]"", fillcolor=""#e5813966""] ;\\n0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=""False""] ;\\n5 [label=""samples = 2\\\\nvalue = [[0.0, 1.0, 0.0]\\\\n[0.5, 0.5, 0.0]]"", fillcolor=""#e581398c""] ;\\n4 -> 5 ;\\n6 [label=""samples = 1\\\\nvalue = [[0.0, 0.5, 0.0]\\\\n[0.0, 0.0, 0.5]]"", fillcolor=""#e58139ff""] ;\\n4 -> 6 ;\\n}\'\r\n DESIRED: \'digraph Tree {\\nnode [shape=box, style=""filled"", color=""black""] ;\\n0 [label=""X[0] <= 0.0\\\\nsamples = 6\\\\nvalue = [[3.0, 1.5, 0.0]\\\\n[1.5, 1.5, 1.5]]"", fillcolor=""#e5813900""] ;\\n1 [label=""X[1] <= -1.5\\\\nsamples = 3\\\\nvalue = [[3, 0, 0]\\\\n[1, 1, 1]]"", fillcolor=""#e5813965""] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=""True""] ;\\n2 [label=""samples = 1\\\\nvalue = [[1, 0, 0]\\\\n[0, 0, 1]]"", fillcolor=""#e58139ff""] ;\\n1 -> 2 ;\\n3 [label=""samples = 2\\\\nvalue = [[2, 0, 0]\\\\n[1, 1, 0]]"", fillcolor=""#e581398c""] ;\\n1 -> 3 ;\\n4 [label=""X[0] <= 1.5\\\\nsamples = 3\\\\nvalue = [[0.0, 1.5, 0.0]\\\\n[0.5, 0.5, 0.5]]"", fillcolor=""#e5813965""] ;\\n0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=""False""] ;\\n5 [label=""samples = 2\\\\nvalue = [[0.0, 1.0, 0.0]\\\\n[0.5, 0.5, 0.0]]"", fillcolor=""#e581398c""] ;\\n4 -> 5 ;\\n6 [label=""samples = 1\\\\nvalue = [[0.0, 0.5, 0.0]\\\\n[0.0, 0.0, 0.5]]"", fillcolor=""#e58139ff""] ;\\n4 -> 6 ;\\n}\'\r\n\r\n----------------------------------------------------------------------\r\nRan 4486 tests in 958.378s\r\n\r\nFAILED (SKIP=17, failures=1)\r\n```'"
5136,101857201,sebconort,amueller,2015-08-19 10:22:43,2015-10-22 07:59:18,2015-10-13 19:40:18,closed,,0.17,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5136,b'Bug in predict_proba of MultinomialNB when fitted with .fit but not when fitted with .partial_fit',"b'Hello,\r\n\r\nI can provide a IPython notebook where I test the accuracy calculation via three methods for a MultinomialNB.\r\n1/ .score method \r\n2/ .predict method and then computing the mean of predictions being equal to y\r\n3/.predict_proba and the computing of preditcions with argmax and then computing the mean of predictions being equal to y\r\n\r\nAll three are equals (as expected!) when I fitted the MultinomialNB model with .partial_fit method.\r\nThe 3/ seems to be inconsistent when I fitted the MultinomialNB model with .fit method.\r\n\r\nBest regards\r\n\r\n'"
5135,101749881,soufanom,amueller,2015-08-18 21:10:04,2015-09-10 14:19:18,2015-09-10 14:19:18,closed,,0.17,5,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5135,b'Suggestion to Have multiclass.py allow prediction over one sample only !',"b'Greetings Guys,\r\n\r\nI came through the contributed implementation to multiclass.py in Scikit-learn. I just have a suggestion for you to consider the case when only one testing sample is passed to decision_function ""Decision function for the OneVsOneClassifier"". As for the current implementation, an undesirable output comes since  n_samples = X.shape[0] will take a number larger than one when X is only a single list vector with some values. I may suggest you check the shape of X before parsing it in a particular way, or update the documentation to advise the user on a suggested way to get the prediction for one testing sample.\r\n\r\nIn a sense, it is true to say that usually, there is a testing set of many samples but in a specific case of mine, it was preferable to predict sample by sample. I overcome this by using X[0:1,:] instead of X[0,:] where X is a testing set of several samples.\r\n\r\nThe sklearn version I have installed is 0.16.1\r\n\r\nI did not get an error when inputing 1d X and what I receive back are predictions as many as the length of this 1d list.\r\n\r\nFor example:\r\n\r\n>>> from sklearn import datasets\r\n>>> from sklearn.multiclass import OneVsOneClassifier\r\n>>> from sklearn.svm import LinearSVC\r\n>>> iris = datasets.load_iris()\r\n>>> X, y = iris.data, iris.target\r\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X[1,:])\r\nOut[1]: array([0, 1, 1, 1])\r\n\r\nAnd by replacing X[1,:] to be X[1:2,:] which in terms of values are the same:\r\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X[1:2,:])\r\nOut[2]: array([0]) # Proper output\r\n\r\nRegards,\r\nOthman'"
5134,101694080,jagapiou,MechCoder,2015-08-18 16:23:33,2015-09-08 17:51:20,2015-09-08 17:51:20,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5134,"b""LogisticRegression.predict_proba is incorrect when multiclass='multinomial'.""","b""Looking at the code for LogisticRegression.predict_proba it seems to assume that multiclass='ovr': it does sigmoid(wTx+b) and then normalizes over the classes. I think that when multiclass='multinomial' is used it should use softmax instead."""
5132,101573799,jnothman,ogrisel,2015-08-18 05:23:09,2015-08-27 13:22:38,2015-08-27 13:22:38,closed,,,7,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5132,b'cross_val_predict should work for sparse `y`',"b'Currently it uses `np.concatenate` to merge predictions, but predictions could be sparse matrices.'"
5118,100790698,larsmans,ogrisel,2015-08-13 14:50:15,2015-09-28 15:32:55,2015-09-28 15:18:13,closed,,0.17,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5118,b'Multicore LatentDirichletAllocation is dead slow',"b'On my box, fitting LDA to all of 20news with `n_jobs=-1` or `n_jobs=4` is twice as slow as doing it single-core. I see one Python process taking up 49% of one core, and a few more doing next to nothing. Will investigate further.'"
5107,100327443,larsmans,larsmans,2015-08-11 14:37:42,2015-08-13 21:11:31,2015-08-13 21:11:31,closed,,,7,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5107,"b""LatentDirichletAllocation's number of iterations""","b""When I amend `examples/applications/topics_extraction_with_nmf_lda.py` to print `lda.n_iter_` after fitting, it reports 81. That's clearly more than `max_iter=5`."""
5095,99658689,olologin,olologin,2015-08-07 14:30:31,2015-08-08 17:45:03,2015-08-07 16:11:26,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5095,b'OneClassSvm sparse matrix return different result than dense',"b""I tried to solve this problem: http://stackoverflow.com/q/31856501/1030820\r\nAnd seems it's a bug, i'm not sure but maybe somewhere in svm_csr_train. Can somebody confirm this?"""
5073,98489596,adamzjw,jnothman,2015-07-31 23:43:32,2015-08-17 01:32:20,2015-08-17 01:32:20,closed,,0.17,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5073,b'Bugs in metrics.ranking.precision_recall_curve',"b""Current code for precision_recall_curve assumes the curve always passes through (recall=0, precision=1). However this is not the case.\r\n\r\nFor instance,\r\npred_proba = [0.8, 0.8, 0.8, 0.2, 0.2]\r\ntrue_value = [0, 0, 1, 1, 0]\r\nmetrics.precision_recall_curve(true_value, pred_proba) will return\r\nprecision = [ 0.4         0.33333333  1.        ]\r\nrecall = [ 1.   0.5  0. ]\r\n![index](https://cloud.githubusercontent.com/assets/1704511/9019531/34e97af6-37a2-11e5-88e4-4cb2b703874d.png)\r\n\r\nthe result's not correct and actually in favor of the poor model (the model misclassified points with high-score will have more area under the curve).\r\n\r\nBe careful when using auc based on metrics.ranking.precision_recall_curve before the bug is solved"""
5045,97816491,amueller,amueller,2015-07-28 22:38:45,2015-07-31 23:08:40,2015-07-31 23:07:19,closed,,,5,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5045,b'master fails with scipy 0.16.0',"b""See https://travis-ci.org/scikit-learn/scikit-learn/jobs/72439348 (passing with 0.15.1)\r\nand https://travis-ci.org/scikit-learn/scikit-learn/jobs/72957365 (failing with 0.16)\r\n\r\nI'm looking into it, but help would be nice ;)"""
5043,97772313,amueller,larsmans,2015-07-28 18:40:51,2015-08-14 16:45:50,2015-08-14 16:45:50,closed,,,1,Bug;Documentation;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5043,b'Documentation of callable kernel in SVM bad',"b'The callable kernel in the SVM needs to work on arrays of shape ``(n_samples, n_features)``. This is not clear from the docs and should be made explicit here:\r\nhttp://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\r\n\r\nAlso the example here uses lowercase letters for the parameters, suggesting vectors: http://scikit-learn.org/dev/auto_examples/svm/plot_custom_kernel.html#example-svm-plot-custom-kernel-py\r\n\r\nSame here:\r\nhttp://scikit-learn.org/dev/modules/svm.html#custom-kernels\r\n\r\nThe docs do mention matrices, but are not explicit about the shapes. Again I think we should use upper case letters for things that are ``(n_samples, n_features)`` and mention the shapes explicitly.\r\n\r\nVia http://stackoverflow.com/questions/31599624/user-defined-svm-kernel-with-scikit-learn\r\n\r\n'"
5040,97683740,mgabel-stratoscale,amueller,2015-07-28 11:40:00,2015-08-03 15:43:07,2015-08-03 15:43:07,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5040,b'KernelPCA causes future (seemingly unrelated) matplotlib plot call to fail (!?)',"b""A simple `KernelPCA.fit_transform()` call when `n_components == None` causes a later matlotlib `plot()` to fail, raising `ValueError: cannot convert float NaN to integer`.\r\n```python\r\nfrom sklearn.decomposition import KernelPCA\r\nimport matplotlib.pylab as pl\r\nimport numpy as np\r\n\r\nx = np.random.randn(30,18)\r\nkpca = KernelPCA(kernel='rbf')\r\nz = kpca.fit_transform(x)\r\npl.plot(np.random.rand(10))\r\npl.show()\r\n```\r\nThe problem disappears when setting `n_components` to say, 4: `KernelPCA(4, kernel='rbf')`.\r\nI am using sklearn 0.16.1 and matplotlib 1.4.3 as bundled in WinPython-64bit-2.7.10.1 .\r\n"""
5033,97300438,yuriShafet,jnothman,2015-07-26 09:08:44,2015-08-27 02:35:49,2015-08-27 02:35:49,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5033,b'BaggingClassifier support for sparse matrices problem',"b""estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\r\n\r\nthrowing error when using coo_matrices:\r\n\r\nTypeError: 'coo_matrix' object has no attribute '__getitem__'"""
5029,97052066,joewreschnig,amueller,2015-07-24 13:22:46,2015-12-10 21:51:15,2015-12-10 21:51:15,closed,,,8,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5029,b'Pipeline inverse_transform works oddly for 1D arrays',"b'When a 1D array is passed to Pipeline\'s `inverse_transform` method it wraps it in another array before processing it ([pipeline.py, line 306](https://github.com/scikit-learn/scikit-learn/blob/0650d5502e01e6b4245ce99729fc8e7a71aacff3/sklearn/pipeline.py#L306)). This means, e.g., an inverse_transform on a pipeline containing just one element can produce a different result than an inverse_transform on only that element, which confused me greatly when replacing an explicit set of transform calls with a Pipeline.\r\n\r\nThe following script shows the problem, using a trivial transformer.\r\n\r\n```python\r\nimport numpy as np\r\n\r\nfrom sklearn.base import BaseEstimator, TransformerMixin\r\nfrom sklearn.pipeline import Pipeline\r\n\r\n\r\nclass Add1Transform(BaseEstimator, TransformerMixin):\r\n    def fit(self, X, y=None):\r\n        return self\r\n\r\n    def transform(self, X):\r\n        return X + 1\r\n\r\n    def inverse_transform(self, X):\r\n        return X - 1\r\n\r\n\r\ndef test_pipeline_transform_1d():\r\n    X = np.array(range(10))\r\n    xfrm = Add1Transform()\r\n    pipeline = Pipeline([(\'xfrm\', xfrm)])\r\n\r\n    X_back1 = xfrm.inverse_transform(xfrm.transform(X))\r\n    X_back2 = pipeline.inverse_transform(pipeline.transform(X))\r\n    print ""Transform:"", X_back1\r\n    print ""Pipelined:"", X_back2\r\n    if not np.array_equal(X_back1, X_back2):\r\n        raise SystemExit(""Results do not match."")\r\n\r\nif __name__ == ""__main__"":\r\n    test_pipeline_transform_1d()\r\n```\r\n\r\nWhen I run it, it says:\r\n\r\n    Transform: [0 1 2 3 4 5 6 7 8 9]\r\n    Pipelined: [[0 1 2 3 4 5 6 7 8 9]]\r\n    Results do not match.\r\n\r\nI am not well-versed in numpy/sklearn, so maybe there is a reason for this wrapping. But I suspect it\'s a bug, and even if it\'s not I think it should be documented more loudly.'"
5013,96493070,ogrisel,ogrisel,2015-07-22 07:21:06,2015-10-05 08:50:36,2015-10-05 08:50:36,closed,,0.17,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5013,b'Random segfault under windows in sklearn.decomposition.tests.test_sparse_pca.test_fit_transform',b'This problem can be seen from time to time on appveyor builds.'
5012,96405050,yanlend,GaelVaroquaux,2015-07-21 20:04:12,2016-01-27 15:00:13,2015-10-23 07:26:42,closed,,,36,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5012,b'[MRG+1] Initialize ARPACK eigsh',"b'`v0 = random_state.rand(M.shape[0])` leads to an initial residual vector in ARPACK which is all positive. However, this is not the absolute or squared residual, but a true difference. Thus, it is better to initialize with `randn` to have an equally distributed sign.\r\n\r\nThe effect of the previous initialization is that eigsh frequently does not converge to the correct eigenvalues, e.g. negative eigenvalues for s.p.d. matrix, which leads to an incorrect null-space.'"
5008,96071072,TomDLT,amueller,2015-07-20 14:02:20,2015-10-30 13:31:11,2015-10-22 10:44:37,closed,,0.17,12,Blocker;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/5008,b'[MRG+1] fix logistic regression class weights',"b'-This PR tests that these two classifiers are equivalent:\r\n```python\r\nclass_weights = compute_class_weight(""balanced"", np.unique(y), y)\r\nclf1 = LogisticRegression(multi_class=""multinomial"", class_weight=""balanced"")\r\nclf2 = LogisticRegression(multi_class=""multinomial"", class_weight=class_weights)\r\n```\r\n-[EDIT]This PR also tests that these two classifiers are equivalent: (fix #5450)\r\n```python\r\nclass_weights = compute_class_weight(""balanced"", np.unique(y), y)\r\nclf1 = LogisticRegressionCV(class_weight=""balanced"")\r\nclf2 = LogisticRegressionCV(class_weight=class_weights)\r\n```\r\nThat is why we can remove [this line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tests/test_common.py#L116)\r\n\r\nI also : \r\n- updated some remaining `""auto""` (deprecated) into `""balanced""`.\r\n- added the test from #5420, which tests the bug detailed in #5415.\r\n- changed the variable names to distinguish `y_bin` and `Y_bin`'"
4996,95800061,yangj1e,jnothman,2015-07-18 07:29:30,2015-08-15 13:40:32,2015-08-15 13:39:59,closed,,,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4996,b'The shape of threshold returned by precision_recall_curve',"b'In the `sklearn.metrics.precision_recall_curve` documentation,\r\n\r\n> thresholds : array, shape = [n_thresholds := len(np.unique(probas_pred))]\r\n\r\nBut as the example below shows,\r\n\r\n    >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\r\n    ...\r\n    >>> thresholds\r\n    array([ 0.35,  0.4 ,  0.8 ])\r\n\r\nthe shape of threshold is not `len(np.unique(probas_pred))`.'"
4995,95755483,yanlend,GaelVaroquaux,2015-07-17 22:15:50,2016-01-28 07:51:46,2015-10-19 13:49:09,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4995,b'[MRG + 1] Bug fix for unnormalized laplacian',"b'So far, _set_diag() always set the diagonal of the Laplacian to 1. This is only valid for the normalized Laplacian. For the unnormalized Laplacian, the diagonal should not be changed.'"
4985,95441749,andrewww,larsmans,2015-07-16 14:11:00,2015-07-31 19:32:15,2015-07-31 09:00:32,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4985,"b'Issue with sparse matrices in MiniBatchKMeans, possible issue with assign_rows_csr'","b'I apologize for not debugging this further, but I do not and cannot have a development environment on my machine. In trying to figure out [this StackOverflow question][http://stackoverflow.com/questions/31337217/scikit-learn-minibatch-kmeans-sparse-vs-dense-matrix-clustering/31456290#31456290]. I determined that (at least on my/our platforms) assign_rows_csr is zeroing the output matrix before copying rows, i.e., it successfully copies the new rows. This code replicates the error.\r\n\r\nI am on Win7 64, Anaconda 2.1.0, Python 2.7, with sklearn 0.15.2 (but so far as I can tell the subject function has not been modified since then) and numpy 1.9.0.\r\n\r\n```\r\nfrom sklearn.utils.sparsefuncs_fast import assign_rows_csr\r\nfrom scipy.sparse import csr_matrix\r\nimport numpy as np\r\n\r\n\r\na = np.ones( [5,5] )\r\nd = a.copy()\r\nprint a\r\nb = np.zeros( [10,5] )\r\nb[ 5,: ] = 2\r\nc = csr_matrix( b )\r\nprint c\r\nassign_rows_csr( c, np.array([5]).astype(np.intp), np.array([3]).astype(np.intp), a )\r\nprint a\r\n\r\nd[3,:] = b[5,:]\r\nprint d\r\nassert np.all( d==a )\r\n```\r\n\r\nOutput\r\n```\r\n[[ 1.  1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.  1.]]\r\n  (5, 0)        2.0\r\n  (5, 1)        2.0\r\n  (5, 2)        2.0\r\n  (5, 3)        2.0\r\n  (5, 4)        2.0\r\n[[ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]\r\n [ 2.  2.  2.  2.  2.]\r\n [ 0.  0.  0.  0.  0.]]\r\n[[ 1.  1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.  1.]\r\n [ 2.  2.  2.  2.  2.]\r\n [ 1.  1.  1.  1.  1.]]\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n.... in <module>()\r\n     17 print d\r\n     18\r\n---> 19 assert np.all( d==a )\r\n\r\nAssertionError:\r\n```'"
4976,95047438,poldrack,GaelVaroquaux,2015-07-14 21:35:04,2015-10-19 21:31:52,2015-10-19 21:31:52,closed,,,6,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4976,b'metrics.mutual_info_score hangs when given real vectors',"b'I accidentally passed two real vectors to this function, and it ended up completely hanging my mac, requiring several hard reboots until I discovered the issue (version 0.16.1, mac os x 10.10).  Should probably validate the inputs to prevent this.'"
4949,94316080,lukauskas,jnothman,2015-07-10 14:21:15,2015-08-04 00:43:27,2015-08-04 00:43:27,closed,,,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4949,b'GridSearchCV should accept factory functions for estimators',"b'According to the current [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html), `GridSearchCV` accepts  *object type that implements the \xa1\xb0fit\xa1\xb1 and \xa1\xb0predict\xa1\xb1 methods* as the `estimator` parameter.\r\nWhile fine for most, certain use cases are made quite unintuitive by this API.\r\n\r\nFor instance, consider the [`AdaBoostClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) API. Essentially this classifier just wraps the boosting around whatever classifier is provided by `base_estimator` parameter. Most of the parameter tuning therefore happens in this `base_estimator` rather than the booster itself. If I were to use grid search for parameter tuning, I would probably do something among the lines of:\r\n\r\n```python\r\nbase_estimators = [DecisionTreeClassifier(max_depth=d) for d in range(1, 11)]\r\ngrid = GridSearchCV(AdaBoostClassifier(), dict(base_estimator=base_estimators))\r\n```\r\nWhich is already quite ugly, and I am only tuning the `max_depth` parameter. Imagine if I also wanted to tune some other parameter in `DecisionTreeClassifier` class. \r\n\r\nOne way to fix this is to make `GridSearchCV` accept factory functions for classifiers, and not only the classifiers themselves. \r\n\r\nParticularly, something among the lines could make things a bit easier:\r\n\r\n```python\r\ndef ada_factory(*args, **kwargs):\r\n     return AdaBoostClassifier(DecisionTreeClassifier(*args, **kwargs))\r\ngrid = GridSearchCV(ada_factory, dict(max_depth=range(1,11))\r\n```\r\nObviously, the contract where the objects returned from the factory function contain `fit`, and `predict` methods should remain in place. \r\n\r\nNot only does this solve this particular problem, it would also allow one to test multiple estimators within the same grid search -- just add a parameter to your factory function. '"
4944,94072258,JefferyRPrice,glouppe,2015-07-09 15:02:29,2015-07-24 05:12:34,2015-07-24 05:12:34,closed,,,14,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4944,b'Thresholding in `_samme_proba` in AdaBoostClassifier',"b'In the subject line function, log probability is calculated as follows (lines 287-88):\r\n\r\n    proba[proba <= 0] = 1e-5\r\n    log_proba = np.log(proba)\r\n\r\nOftentimes one encounters probabilities significantly smaller than 1.0e-5 that are still nonzero. In the current implementation, for example, the log prob of 1.0e-6 will actually evaluate higher than log prob of 0.0. It seems to me that a better implementation of this would be a floor-like function as follows:\r\n\r\n    proba[proba < 1e-9] = 1.e-9'"
4942,94013437,eyaler,amueller,2015-07-09 10:38:14,2015-08-08 15:38:14,2015-08-08 15:38:14,closed,,,21,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4942,b'oneclasssvm crash',"b'using 0.16.1, i took the example in: http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html\r\nand changed:\r\nX = 0.3 * np.random.randn(100, 2)  ---->  X = 0.3 * np.random.randn(100000, 2)\r\nthis causes ""Process finished with exit code -1073741819 (0xC0000005)"" on the oneclasssvm fit() call'"
4940,93870732,amaurits,amueller,2015-07-08 19:30:51,2015-07-11 16:58:19,2015-07-11 16:58:19,closed,,,2,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4940,"b""Bug in changed parameter values (from 'l2' to 'squared_hinge') for linearSVC""","b""The 'loss' parameter for linearSVC used to be 'l1' or 'l2'. This was recently changed to 'hinge' or 'squared_hinge', according to the documentation. However, supplying those as parameters gives a very unhelpful error (which I believe misidentifies the loss parameter as the penalty parameter, since I'm not even touching the latter):\r\n\r\nValueError: Unsupported set of arguments: penalty='l1' is only supported when dual='false'., Parameters: penalty='l2', loss='squared_hinge', dual=True\r\n\r\nIf, instead, I change back to loss='l2' it appears to work just fine. \r\nI believe this recent Stackoverflow question concerns the same issue:\r\nhttp://stackoverflow.com/questions/29902190/value-error-happens-when-using-gridsearchcv\r\n\r\n"""
4930,93409382,chiatungkuo,amueller,2015-07-07 00:46:21,2015-09-22 16:27:59,2015-09-22 16:27:59,closed,,,7,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4930,"b'Suggestion on NMF initialization of W, H'","b""Below is something I found troublesome but not exactly a bug.\r\n\r\nWhen setting init='random', NMF draws initial W, H from standard normal distribution and then take absolute values while the default init='nndsvd' takes values from results of randomized SVD (with small manipulations). The magnitudes of the initial W and H from the former (i.e. random) are irrespective of the data (entry-wise, but linear to size of X, of course) whereas the latter (nndsvd) tends to give initial W and H whose magnitudes are highly dependent on the data X.\r\nSince the termination criterion during fitting is set based on the norms of the initial W and H, this often requires the user to supply very different values of 'tol' when using 'nndsvd' and 'random' to initialize. My experience was that using the default tol=0.0001 and init='random' while fitting a row-normalized X of size 10000x1000 results in immediate termination since the effective tolerance during the update step in fit() is too large.\r\nAccordingly I suggest the author add a normalization step when init='random' such that the initial W and H have norms comparable to that of data X (or similar to what would result from nndsvd)."""
4921,92734656,davidgbe,MechCoder,2015-07-02 21:01:12,2015-09-14 17:56:54,2015-09-14 17:56:54,closed,,0.17,5,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4921,b'Class specified in class_weight is missing in classes error',"b'Hi all!\r\n\r\nI\'ve been using sklearn\'s SVM for a bit, and I\'ve encountered a small edge case in the compute_class_weight function in sklearn/utils/class_weight.py. The issue occurs at line 60. Here\'s some context:\r\n\r\n```python\r\n        for c in class_weight:\r\n            i = np.searchsorted(classes, c)\r\n            if classes[i] != c:\r\n                raise ValueError(""Class label %d not present."" % c)\r\n            else:\r\n                weight[i] = class_weight[c]\r\n```\r\n\r\nBasically, we are affirming that each key (class) in the class_weight dict actually exists in the classes. The issue occurs when a class in class_weight is greater than all the values in the classes array. Instead of raising the specified value error, you simply get an unhelpful IndexError. Here is the proposed solution:\r\n\r\n```python\r\n            if i >= len(classes) or classes[i] != c:\r\n                raise ValueError(""Class label %d not present."" % c)\r\n```\r\n\r\nThanks for all you do guys! This library is a huge help to everyone!'"
4907,91905929,wfbradley,larsmans,2015-06-29 20:45:07,2015-07-12 12:18:30,2015-07-12 12:18:30,closed,,,3,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4907,b'RANSAC and residual_threshold==0 (feature request)',"b'If the ""residual_threshold"" argument in RANSACRegressor is set to zero, the regression will almost certainly fail.  However, figuring out what is going on based on the error messages is not obvious:\r\n```\r\nValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.\r\n```\r\n(I believe that when residual_threshold==0.0, one of the cross-validation classes is empty, and things break.)\r\n\r\nBecause (I think?) there is no legitimate reason to set residual_threshold==0.0, and to save some future soul the debugging journey I just went through, it might be helpful to throw an exception (or at least a clear warning) if residual_threshold==0.0.'"
4895,90805937,ahurriyetoglu,amueller,2015-06-24 23:33:15,2015-09-24 20:41:39,2015-09-24 20:41:39,closed,,0.17,13,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4895,b'pickle a TfidfVectorizer with a given set of features',"b'Dear all,\r\n\r\nI would like to inform scikit-learn community about my experience in pickling a vectorizer and a classifier. Each time I read a pickled vectorizer and a pickled classifier, which are a TfidfVectorizer and a Multinomial Naive Bayes objects, I was getting a different result for a test document. After two days of analysis, I discovered that assigning a set() object to the vocabulary parameter of the TfidfVectorizer object makes the features to get random order and let the classifier generate random estimations. After I convert my set of features to a list(), everything become normal.\r\n\r\nYou may want to take that into consideration in the documentation or code improvements.\r\n\r\nThanks for the great effort you spend for that great project!\r\n\r\nAli'"
4893,90670527,jmschrei,agramfort,2015-06-24 13:04:41,2015-06-26 06:12:55,2015-06-25 21:47:13,closed,,0.17,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4893,b'[MRG+1] CCA Stability',"b""This pull request should address #4888.\r\n\r\nOn a Windows machine with 64bit python, the unit tests for CCA would fail because of the presence of NaN. This issue was traced to the function `_nipals_twoblocks_inner_loop`. \r\n\r\nConceptually, the function takes in a data matrix X and a multivariate response Y, and tries to find a lower dimensional representation of both, one component at a time. However, if earlier components can reconstruct the data well, later components may have their weights shrunk to quantities below machine precision. In the case of the unit test, the values are ~5e-16, which can be represented on Ubuntu machines but not Windows machines, where they would be rounded to 0. \r\n\r\nPractically, this caused an issue because on line 47 and 60 of pls_.py the weights are normalized by dividing by some function of themselves. Instead of explicitly raising an error, numpy would fill in nan in those positions, which would cause math issues for the remainder of the function but return successfully.  It also wouldn't be caught in lines 300-302 because the numbers were not small, they were nan. It isn't until line 336-338 where the pseudoinverse is calculated that the array is checked to make sure no nan elements are present; and since they exist, the error above is raised.\r\n\r\nThis PR fixes the issue by adding an epsilon close to machine precision to the weights before they are divided, so that there is never a divide by zero issue. Unit tests run successfully on both Ubuntu and Windows 64bit machines. """
4888,90314375,jmschrei,amueller,2015-06-23 07:37:41,2015-09-11 19:25:23,2015-09-11 19:25:23,closed,,0.17,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4888,b'Test failure in sklearn.tests.test_common.test_transformer_n_iter',"b'I cloned sklearn v0.17.dev0, built it on Windows for 64bit Python, and ran the nosetests. I get one error, in sklearn.tests.test_common.test_transformer_n_iter.  I am using numpy v1.9.2 and scipy v0.15.1. \r\n\r\nRunning `C:\\Anaconda\\Python -c ""import nose; nose.main()"" -v sklearn.tests.test_common:test_transformer_n_iter` yields the following logs:\r\n\r\n```\r\nc:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\metrics\\metrics.py:4: DeprecationWarning: sklearn.metrics.metrics is deprecated and will be removed in 0.18. Please import from sklearn.metrics\r\n  DeprecationWarning)\r\nsklearn.tests.test_common.test_transformer_n_iter(\'CCA\', CCA(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06)) ... c:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\cross_decomposition\\pls_.py:46: RuntimeWarning: invalid value encountered in divide\r\n  x_weights /= np.sqrt(np.dot(x_weights.T, x_weights))\r\nc:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\cross_decomposition\\pls_.py:64: RuntimeWarning: invalid value encountered in less\r\n  if np.dot(x_weights_diff.T, x_weights_diff) < tol or Y.shape[1] == 1:\r\nc:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\cross_decomposition\\pls_.py:67: UserWarning: Maximum number of iterations reached\r\n  warnings.warn(\'Maximum number of iterations reached\')\r\nc:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\cross_decomposition\\pls_.py:297: RuntimeWarning: invalid value encountered in less\r\n  if np.dot(x_scores.T, x_scores) < np.finfo(np.double).eps:\r\nERROR\r\nsklearn.tests.test_common.test_transformer_n_iter(\'DictionaryLearning\', DictionaryLearning(alpha=1, code_init=None, dict_init=None, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'FactorAnalysis\', FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'FastICA\', FastICA(algorithm=\'parallel\', fun=\'logcosh\', fun_args=None, max_iter=200, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'KMeans\', KMeans(copy_x=True, init=\'k-means++\', max_iter=300, n_clusters=8, n_init=10, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'LinearSVC\', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'LogisticRegression\', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'MiniBatchKMeans\', MiniBatchKMeans(batch_size=100, compute_labels=True, init=\'k-means++\', ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'NMF\', NMF(beta=1, eta=0.1, init=None, max_iter=200, n_components=None, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'PLSCanonical\', PLSCanonical(algorithm=\'nipals\', copy=True, max_iter=500, n_components=2, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'PLSRegression\', PLSRegression(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06)) ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'ProjectedGradientNMF\', ProjectedGradientNMF(beta=1, eta=0.1, init=None, max_iter=200, ... ok\r\nsklearn.tests.test_common.test_transformer_n_iter(\'SparsePCA\', SparsePCA(U_init=None, V_init=None, alpha=1, max_iter=1000, method=\'lars\', ... ok\r\n\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_transformer_n_iter(\'CCA\', CCA(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06))\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Anaconda\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\utils\\estimator_checks.py"", line 1306, in check_transformer_n_iter\r\n    estimator.fit(X, y_)\r\n  File ""c:\\users\\jacob\\documents\\github\\scikit-learn\\sklearn\\cross_decomposition\\pls_.py"", line 335, in fit\r\n    linalg.pinv(np.dot(self.x_loadings_.T, self.x_weights_)))\r\n  File ""C:\\Anaconda\\lib\\site-packages\\scipy\\linalg\\basic.py"", line 602, in pinv\r\n    a = np.asarray_chkfinite(a)\r\n  File ""C:\\Anaconda\\lib\\site-packages\\numpy\\lib\\function_base.py"", line 613, in asarray_chkfinite\r\n    ""array must not contain infs or NaNs"")\r\nValueError: array must not contain infs or NaNs\r\n\r\n----------------------------------------------------------------------\r\nRan 13 tests in 0.344s\r\n\r\nFAILED (errors=1)\r\n```\r\n\r\nI will begin looking deeper into this issue.'"
4880,89850450,wlattner,mblondel,2015-06-21 02:15:20,2015-06-24 11:08:02,2015-06-24 11:08:02,closed,,0.16.2,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4880,b'LogisticRegressionCV: best model with l1 penalty and refit=True is not sparse',"b""When `LogisticRegressionCV` is run with an l1 penalty and `refit=True`, the final model does not seem to be doing any variable selection, the final coefficients are all non-zero. Models fit separately (using `LogisticRegression`) on the same values of C in the grid passed to `LogisticRegressionCV` result in mostly zero coefficients.\r\n\r\n```python\r\nimport numpy as np\r\nimport sklearn\r\n\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\r\n\r\nprint(sklearn.__version__)  # 0.16.1\r\n\r\nx, y = make_classification(n_samples=10000, random_state=133)\r\n\r\nlr_01 = LogisticRegression(penalty='l1', solver='liblinear', C=0.01)\r\nlr_01 = lr_01.fit(x, y)\r\nprint(np.count_nonzero(lr_01.coef_))  # 2 non-zero coefficients\r\n\r\nlr_001 = LogisticRegression(penalty='l1', solver='liblinear', C=0.001)\r\nlr_001 = lr_001.fit(x, y)\r\nprint(np.count_nonzero(lr_001.coef_))  # 1 non-zero coefficients\r\n\r\n# use LogisticRegressionCV on a grid of [0.01, 0.001] for C\r\nlr_cv = LogisticRegressionCV(penalty='l1', solver='liblinear', Cs=[0.01, 0.001], refit=True)\r\nlr_cv = lr_cv.fit(x, y)\r\nprint(np.count_nonzero(lr_cv.coef_))  # 20 non-zero coefficients\r\n\r\n# check which value of C was selected\r\nprint(lr_cv.C_)  # [ 0.01]\r\n\r\n# compare lr_cv.coef_ to lr_01.coef_\r\nprint(lr_cv.coef_)\r\n# [[ 0.00378572 -0.0382404   0.03526494  0.04258872  0.01447892 -0.00853596\r\n#    0.76508529 -0.99786672 -0.01364018 -0.02769111  0.00275433 -0.04077918\r\n#    0.03579957 -0.01425979  0.00437261 -0.01483434  0.02060494 -0.01298574\r\n#    2.32994756 -0.01440317]]\r\nprint(lr_01.coef_)\r\n# [[ 0.                  0.          0.          0.          0.          0.          0.\r\n#   -0.61909139  0.          0.          0.          0.          0.          0.\r\n#    0.                  0.          0.          0.          3.12856295       0.        ]]\r\n\r\n# re-run LogisticRegressionCV w/ refit=False\r\nlr_cv_no_refit = LogisticRegressionCV(penalty='l1', solver='liblinear', Cs=[0.01, 0.001],\r\n                                                              refit=False)\r\nlr_cv_no_refit = lr_cv_no_refit.fit(x, y)\r\nprint(np.count_nonzero(lr_cv_no_refit.coef_))  # 2 non-zero coefficients\r\n```"""
4879,89825596,mbatchkarov,agramfort,2015-06-20 22:16:55,2015-06-24 21:33:10,2015-06-24 21:33:10,closed,,0.16.2,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4879,b'RandomForestClassifier changes label values',"b""Consider the following example:\r\n```python\r\nimport numpy as np\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\nCLASSES = 15\r\nX = np.arange(CLASSES).reshape(-1, 1)\r\ny = [ch for ch in 'ABCDEFGHIJKLMNOPQRSTU'[:CLASSES]]\r\n\r\nprint(RandomForestClassifier().fit(X, y).predict(X))\r\n```\r\n\r\nThe output is\r\n\r\n`['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'B' 'B' 'B' 'B' 'B']`\r\n\r\nwhereas I expect it to be\r\n\r\n`['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O']`\r\n\r\nI traced the cause of the error to [here](https://github.com/scikit-learn/scikit-learn/blob/733629d256abaf9a31d6e6305f859768432907ed/sklearn/ensemble/forest.py#L418). I'm happy to put in a pull request, but can others please verify they get the same unexpected behaviour first?\r\n\r\nSystem information:\r\n``\r\nCPython 3.3.5\r\nIPython 3.1.0\r\n\r\nnumpy 1.9.2\r\nscipy 0.14.0\r\nscikit-learn 0.16.1(conda)\r\n\r\ncompiler   : GCC 4.2.1 (Apple Inc. build 5577)\r\nsystem     : Darwin\r\nrelease    : 13.4.0\r\nmachine    : x86_64\r\nprocessor  : i386\r\nCPU cores  : 4\r\ninterpreter: 64bit\r\n```"""
4846,87249677,mblondel,mblondel,2015-06-11 07:49:22,2015-06-22 15:38:09,2015-06-22 15:38:09,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4846,b'RidgeClassifier triggers data copy',b'RidgeClassifier always triggers a data copy even when not using sample weights.\r\n\r\nRegression introduced in #4838.\r\n\r\nSee:\r\nhttps://github.com/scikit-learn/scikit-learn/pull/4838#discussion_r32090535'
4845,87123840,Carldeboer,amueller,2015-06-10 22:06:43,2015-09-09 15:00:25,2015-09-09 15:00:25,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4845,b'Added error messages in case user provides one dimensional data',"b'When trying to learn a GMM on one dimensional data, both weights and the data can be represented as 1-d vectors.  However, this breaks GMM.fit() and it used to die with cryptic error messages.  This addition specifically checks for one dimensional input and issues an error message accordingly.'"
4809,84524375,verne91,verne91,2015-06-03 12:06:20,2015-07-01 23:16:23,2015-07-01 23:16:23,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4809,b'cross_validation.cross_val_score : Type Error',"b'Hi,\r\n\r\nWhen I use cross_validation.cross_val_score with parameter ""n_jobs = 10"", I came across TypeError below.\r\n\r\nProcess PoolWorker-1:\r\nTraceback (most recent call last):\r\n  File ""/usr/lib64/python2.6/multiprocessing/process.py"", line 232, in _bootstrap\r\n    self.run()\r\n  File ""/usr/lib64/python2.6/multiprocessing/process.py"", line 88, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""/usr/lib64/python2.6/multiprocessing/pool.py"", line 57, in worker\r\n    task = get()\r\n  File ""/usr/lib64/python2.6/site-packages/sklearn/externals/joblib/pool.py"", line 363, in get\r\n    return recv()\r\nTypeError: type \'partial\' takes at least one argument\r\n\r\nProcess PoolWorker-2:\r\n...'"
4793,82877506,sergey-salnikov,amueller,2015-05-30 18:18:13,2015-06-01 17:16:31,2015-06-01 17:16:31,closed,,,5,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4793,b'Nearest neighbor query fails on array of dimension > 2',"b'Examples, on scikit-learn 0.16.1:\r\n\r\n    >>> import numpy as np\r\n    >>> from sklearn.neighbors import NearestNeighbors, KDTree\r\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\r\n    >>> nbrs = NearestNeighbors(n_neighbors=2, algorithm=\'kd_tree\').fit(X)\r\n    >>> nbrs.kneighbors([[[0,0], [0,1]], [[1,0], [1,1]]])\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/neighbors/base.py"", line 327, in kneighbors\r\n        X = check_array(X, accept_sparse=\'csr\')\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 350, in check_array\r\n        array.ndim)\r\n    ValueError: Found array with dim 3. Expected <= 2\r\n    >>> tree = KDTree(X)\r\n    >>> tree.query([[[0,0], [0,1]], [[1,0], [1,1]]])\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""binary_tree.pxi"", line 1287, in sklearn.neighbors.kd_tree.BinaryTree.query (sklearn/neighbors/kd_tree.c:10407)\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 350, in check_array\r\n        array.ndim)\r\n    ValueError: Found array with dim 3. Expected <= 2\r\n\r\nDocstrings still say ""X : array-like, last dimension same as that of fit data"", don\'t say that X must be 2d. With scikit-learn 0.15.2, the same code works just fine.\r\n\r\nOn the other hand, I don\'t know, maybe this is a documentation bug? At least now this error is consistent and does not depend on algorithm, while on 0.15.2 algorithm=\'brute\' failed with a less clear message:\r\n\r\n    >>> nbrs = NearestNeighbors(n_neighbors=2, algorithm=\'brute\').fit(X)\r\n    >>> nbrs.kneighbors([[[0,0], [0,1]], [[1,0], [1,1]]])\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/neighbors/base.py"", line 312, in kneighbors\r\n        squared=True)\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/metrics/pairwise.py"", line 1073, in pairwise_distances\r\n        return func(X, Y, **kwds)\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/metrics/pairwise.py"", line 212, in euclidean_distances\r\n        XX = row_norms(X, squared=True)[:, np.newaxis]\r\n      File ""/usr/lib/python2.7/site-packages/sklearn/utils/extmath.py"", line 65, in row_norms\r\n        norms = np.einsum(\'ij,ij->i\', X, X)\r\n    ValueError: operand has more dimensions than subscripts given in einstein sum, but no \'...\' ellipsis provided to broadcast the extra dimensions.\r\n'"
4786,82107211,amueller,amueller,2015-05-28 20:18:10,2015-06-01 19:57:05,2015-06-01 19:56:58,closed,,0.17,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4786,b'[MRG] Fix RFE / RFECV estimator tags',"b'Pretty bad regression from introducing ``_estimator_tags``. I forgot to define them for RFE, which made accuracies on iris be 0 using default cross-validation. ouch.'"
4784,81841367,vicpara,amueller,2015-05-28 09:14:55,2015-07-11 15:05:33,2015-06-01 20:30:48,closed,,0.16.2,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4784,"b'DBSCAN: Buffer has wrong number of dimensions (expected 1, got 2)'","b'I\'m running this code:\r\n```\r\ngms = [10, 0.0001, 0.000001]\r\nfor g in gms:\r\n    printMsg(""Running RBF kernel with Gamma: %.7f""%g)\r\n    kDistMat = pairwise_kernels(tfidf[0:40000], Y=None, metric=""rbf"", filter_params=False, n_jobs=-1, gamma =g)\r\n\r\n    printMsg(""\\t Computing DBSCAN\\t"")\r\n    db = DBSCAN(eps=0.000001, min_samples=35, leaf_size=300, metric=\'precomputed\', algorithm=""auto"")\r\n    labels = db.fit_predict(kDistMat)\r\n\r\n```\r\n\r\nThat produces this error when running with gamma = 0.0001. When gamma is 10, it runs without a problem. With gamma 10 it only requires about 40Gb of ram. When gamma is 0.0001 124GB ram seem insufficient.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-9ebec25e4fc3> in <module>()\r\n     18     printMsg(""\\t Computing DBSCAN\\t"")\r\n     19     db = DBSCAN(eps=0.000001, min_samples=35, leaf_size=300, metric=\'precomputed\', algorithm=""auto"")\r\n---> 20     labels = db.fit_predict(kDistMat)\r\n     21     infoClusters(labels)\r\n     22 \r\n\r\n/home/paraschv/anaconda/lib/python2.7/site-packages/sklearn/cluster/dbscan_.pyc in fit_predict(self, X, y, sample_weight)\r\n    260             cluster labels\r\n    261         """"""\r\n--> 262         self.fit(X, sample_weight=sample_weight)\r\n    263         return self.labels_\r\n\r\n/home/paraschv/anaconda/lib/python2.7/site-packages/sklearn/cluster/dbscan_.pyc in fit(self, X, y, sample_weight)\r\n    230         """"""\r\n    231         X = check_array(X, accept_sparse=\'csr\')\r\n--> 232         clust = dbscan(X, sample_weight=sample_weight, **self.get_params())\r\n    233         self.core_sample_indices_, self.labels_ = clust\r\n    234         if len(self.core_sample_indices_):\r\n\r\n/home/paraschv/anaconda/lib/python2.7/site-packages/sklearn/cluster/dbscan_.pyc in dbscan(X, eps, min_samples, metric, algorithm, leaf_size, p, sample_weight, random_state)\r\n    137     # A list of all core samples found.\r\n    138     core_samples = np.asarray(n_neighbors >= min_samples, dtype=np.uint8)\r\n--> 139     dbscan_inner(core_samples, neighborhoods, labels)\r\n    140     return np.where(core_samples)[0], labels\r\n    141 \r\n\r\nsklearn/cluster/_dbscan_inner.pyx in sklearn.cluster._dbscan_inner.dbscan_inner (sklearn/cluster/_dbscan_inner.cpp:1411)()\r\n\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n```'"
4756,79063938,tiagofrepereira2012,amueller,2015-05-21 16:12:29,2015-05-27 16:50:40,2015-05-27 16:50:40,closed,,,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4756,b'It is not possible to set the regularization parameter',"b'Hi,\r\n\r\nI was using the LocallyLinearEmbbeding implementation from this package and I found a bug regarding to the regularization parameter. \r\nBasically, setting different regularization values leads to the same embedding, which is not correct.\r\nThe code below reproduce the issue:\r\n\r\nIn the class LocallyLinearEmbedding (scikit-learn/scikit-learn/blob/master/sklearn/manifold/locally_linear.py) method ""_fit_transform"" the ""reg"" parameter is not set with the value defined in the constructor ""self.reg"".\r\n\r\n#############################\r\nimport sklearn.manifold\r\nimport numpy\r\n\r\nX = numpy.random.rand(10,5)\r\n\r\nimport sklearn.manifold\r\nimport numpy\r\n\r\nX = numpy.random.rand(10,5)\r\n\r\n#Setting different regularizers\r\nreg_1 = 1E-3\r\nreg_2 = 1E-5\r\n\r\n#Training one manifold\r\nlle_1 = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=3, n_components=4, eigen_solver=\'dense\', neighbors_algorithm=\'brute\', reg=reg_1)\r\nlle_1.fit(X)\r\n\r\n#Training another manifold\r\nlle_2 = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=3, n_components=4, eigen_solver=\'dense\', neighbors_algorithm=\'brute\', reg=reg_2)\r\nlle_2.fit(X)\r\n\r\nprint lle_1.embedding_ - lle_2.embedding_\r\n###########################\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
4755,79036603,amueller,agramfort,2015-05-21 15:06:45,2015-06-09 08:55:19,2015-06-09 08:55:19,closed,,,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4755,b'RidgeCV ignores sample_weights if cv != None',"b'There is a fixme in RidgeCV to also slice sample weights, currently they are silently ignored if ``cv != None``. Since then we fixed GridSearchCV so this is a really easy change.\r\n'"
4751,78663150,amueller,ogrisel,2015-05-20 19:30:18,2015-05-22 11:55:18,2015-05-22 11:55:18,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4751,b'[MRG+1] FIX n_jobs slicing bug in dict learning',b'Also add input validation to gen_even_slices.\r\nFixes #4746.'
4746,78495139,arthurmensch,arthurmensch,2015-05-20 11:56:49,2015-05-22 13:30:48,2015-05-22 13:30:48,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4746,b'Dictionary learning is slower and fail with n_jobs > 1',"b'dict_learning_online fails with n_jobs > 1. Bug can be reproduced using denoising examples available in the doc : \r\n\r\nhttps://gist.github.com/arthurmensch/d85f9efb3716fe090937\r\n\r\nObserved images are not correct, and dictionary learning take much longer time.\r\n\r\nIncriminated code can be located in function sparse_encode in sklearn.decomposition.dict_learning, line 248.\r\n\r\n```python\r\n    code_views = Parallel(n_jobs=n_jobs)(\r\n        delayed(_sparse_encode)(\r\n            X[this_slice], dictionary, gram, cov[:, this_slice], algorithm,\r\n            regularization=regularization, copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter)\r\n        for this_slice in slices)\r\n```'"
4744,78459108,y0ast,glouppe,2015-05-20 10:00:43,2015-07-19 17:26:25,2015-07-19 17:26:25,closed,,,2,Bug;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4744,b'Bug with using TreeClassifier with OOB score and sparse matrices',"b'When using the ExtraTreesClassifier (and likely other classes that are derived from BaseTreeClassifier), there is a problem when using sparsematrices: `ValueError: X should be in csr_matrix format, got <class \'scipy.sparse.csc.csc_matrix\'>`.\r\n\r\nI tracked the issue down to the following lines:\r\n\r\nOn line 195 of forest.py the sparse matrix is changed to a csc matrix:\r\n`X = check_array(X, dtype=DTYPE, accept_sparse=""csc"")`\r\n\r\nHowever on line 369 of forest.py, the following is call is made with `check_input=false`:\r\n`p_estimator = estimator.predict_proba(X[mask_indices, :], check_input=False)`\r\n\r\nThis leads to a ValueError in predict `ValueError: X should be in csr_matrix format, got <class \'scipy.sparse.csc.csc_matrix\'>`.\r\n\r\nChanging check_input to True seems to fix the issue. It\'s probably best to also include a test case for this problem, I just made a quick PR with only the False -> True fix.\r\n'"
4737,77782149,amueller,amueller,2015-05-18 19:49:09,2015-12-08 23:27:33,2015-12-08 23:27:33,closed,,,6,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4737,b'Sphinx 1.3.1 breaks API docs',"b""Something changed in autodoc, and many of the sections in the api overview are not populated. I haven't investigated further."""
4731,76816038,amueller,mblondel,2015-05-15 18:36:09,2015-05-16 00:02:52,2015-05-16 00:02:52,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4731,b'Heisenfailure in Dummy code',b'Travis failed on dummy tests:\r\nhttps://travis-ci.org/scikit-learn/scikit-learn/jobs/62616416\r\n\r\nI saw this only once so far. Weird. Maybe @arjoly knows?'
4721,76317109,StevenLOL,amueller,2015-05-14 10:40:09,2015-05-15 03:30:44,2015-05-14 14:04:50,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4721,"b'GradientBoostingClassifier np.nan_to_num(np.exp(pred[:, k]  IndexError: too many indices for array'","b'scikit-learn 0.16.1 or 0.17dev\r\nnumpy   1.80 or 1.9.2\r\nscipy   0.14.0\r\n\r\n```\r\nfrom sklearn.datasets import make_blobs\r\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\n\r\ndatax,datay=make_blobs(n_samples=500)\r\nbaseSVC=RandomForestClassifier() \r\nclf=GradientBoostingClassifier(init=baseSVC,verbose=1)\r\nclf.fit(datax,datay)\r\ny_pred=clf.predict(datax)\r\n\r\n```\r\n\r\nAnd get these:\r\nIter       Train Loss   Remaining Time \r\nTraceback (most recent call last):\r\n  File ""/media/internal4tb/steven/research_lre_ivector_2015/utils/bugTest.py"", line 16, in <module>\r\n    clf.fit(datax,datay)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.py"", line 980, in fit\r\n    begin_at_stage, monitor)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.py"", line 1040, in _fit_stages\r\n    random_state)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.py"", line 747, in _fit_stage\r\n    sample_weight=sample_weight)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.py"", line 550, in negative_gradient\r\n    return y - np.nan_to_num(np.exp(pred[:, k] -\r\nIndexError: too many indices for array\r\n\r\n\r\nSimilar to #2233 #2691 not only the same error for RF and SVC\r\n'"
4719,76067561,amueller,amueller,2015-05-13 17:46:08,2015-06-01 15:21:37,2015-06-01 15:21:37,closed,,0.16.2,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4719,b'OMP travis error',"b""A new heisenfailure in OMP:\r\nhttps://travis-ci.org/scikit-learn/scikit-learn/jobs/62182759\r\nSaw it a couple of times now. Haven't investigated yet."""
4717,75988478,rebeccaroisin,rebeccaroisin,2015-05-13 13:41:39,2016-05-31 01:38:28,2015-05-14 09:21:03,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4717,b'EM algorithm in GMM fails for one-dimensional datasets using 0.16.1 (but fine with 0.15.2) ',"b'Fitting a one-dimensional gaussian distribution using GMM.fit() produces a Runtime error using scikit-learn version 0.16.1, but produces appropriate parameters using 0.15.2.\r\n\r\nA short example to demonstrate the problem:\r\n\r\n```python\r\nimport sklearn\r\nfrom sklearn import mixture\r\nimport numpy as np\r\nfrom scipy import stats\r\nimport sys\r\n\r\n# the version info \r\nprint(""Python version: %s.%s"" %(sys.version_info.major, sys.version_info.minor))\r\nprint(""scikit-learn version: %s"" %(sklearn.__version__))\r\n\r\n# some pretend data\r\nnp.random.seed(seed=0)\r\ndata = stats.norm.rvs(loc=100, scale=1, size=1000)\r\nprint(""Data mean = %s, Data std dev = %s"" %(np.mean(data), np.std(data)))\r\n\r\n# Fitting using a GMM with a single component\r\nclf = mixture.GMM(n_components=1)\r\nclf.fit(data)\r\nprint(clf.means_, clf.weights_, clf.covars_)\r\n```\r\n\r\nRunning this example code with scikit-learn 0.15.2 produces correct output:\r\n```shell\r\nPython version: 3.4\r\nscikit-learn version: 0.15.2\r\nData mean = 99.9547432925, Data std dev = 0.987033158669\r\n[[ 99.95474329]] [ 1.] [[ 0.97523446]]\r\n```\r\nHowever, exactly the same code using scikit-learn 0.16.1 gives this traceback:\r\n```shell\r\nPython version: 3.4\r\nscikit-learn version: 0.16.1\r\nData mean = 99.9547432925, Data std dev = 0.987033158669\r\n/home/rebecca/anaconda/envs/new_sklearn/lib/python3.4/site-packages/numpy/lib/function_base.py:1890: RuntimeWarning: Degrees of freedom <= 0 for slice\r\n  warnings.warn(""Degrees of freedom <= 0 for slice"", RuntimeWarning)\r\n/home/rebecca/anaconda/envs/new_sklearn/lib/python3.4/site-packages/numpy/lib/function_base.py:1901: RuntimeWarning: invalid value encountered in true_divide\r\n  return (dot(X, X.T.conj()) / fact).squeeze()\r\nTraceback (most recent call last):\r\n  File ""test_sklearn.py"", line 18, in <module>\r\n    clf.fit(data)\r\n  File ""/home/rebecca/anaconda/envs/new_sklearn/lib/python3.4/site-packages/sklearn/mixture/gmm.py"", line 498, in fit\r\n    ""(or increasing n_init) or check for degenerate data."")\r\nRuntimeError: EM algorithm was never able to compute a valid likelihood given initial parameters. Try different init parameters (or increasing n_init) or check for degenerate data.\r\n```\r\nI\'ve tried various different values of the n_init, n_iter and covariance_type parameters. I\'ve also tried a range of different datasets. All of these result in this error or similar using 0.16.1, but there are no issues at all using 0.15.2. The problem seems to be related to the initial parameters used in the expectation maximisation, so it\'s possible that this is related to this issue: #4429\r\n\r\nIn case this is useful info, I was using an anaconda virtual environment with a clean install of scikit-learn, set up as follows (for version 0.16.1):\r\n\r\n```shell\r\nconda create -n new_sklearn python=3.4\r\nsource activate new_sklearn\r\nconda install sklearn\r\n```'"
4705,75264185,amueller,ogrisel,2015-05-11 15:41:16,2015-05-20 12:22:17,2015-05-20 11:42:23,closed,,0.16.2,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4705,b'[MRG+2] make cross_val_predict work on lists',b'Also test compatibility with various input types and that they are passed through.\r\n\r\nFixes \r\n#4700.'
4701,75077329,Renzhh,amueller,2015-05-11 04:11:07,2015-09-09 20:20:23,2015-09-09 20:20:23,closed,,,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4701,b'python crashed when computing silhouette_score/ silhouette_samples of KMeans on large amounts of data',"b'Firstly, show the code:\r\n\r\n    km = joblib.load(filename)\r\n    cluster_labels = km.predict(X)\r\n    silhouette_avg = silhouette_score(X, cluster_labels)\r\n    # Compute the silhouette scores for each sample\r\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\r\n\xa1\xaekm\xa1\xaf is the model that I trained using training data. see details:\r\n\r\n    km = KMeans(n_clusters=num_k, init=\'k-means++\', max_iter=300, n_init=1, verbose=False)\r\n    km.fit(X)\r\nWhen the amount of X less than 30 thousands rows, both of silhouette_score and silhouette_samples are OK and can get expected results. But when the amount of X more than 100 thousands, the program crashed and get ""Segmentation fault (core dumped)"".  See the detail error information:\r\n\r\n    Traceback (most recent call last):\r\n      File ""test19_statistic_silhouette_score.py"", line 87, in <module>  out()\r\n      File ""test19_statistic_silhouette_score.py"", line 63, in out sample_silhouette_values = silhouette_samples(X, cluster_labels)\r\n      File ""/home/supermicro/.local/lib/python2.7/site-packages/sklearn/metrics/cluster/unsupervised.py"", line 153, in silhouette_samples\r\n        distances = pairwise_distances(X, metric=metric, **kwds)\r\n      File ""/home/supermicro/.local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py"", line 1112, in pairwise_distances\r\n        return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\r\n      File ""/home/supermicro/.local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py"", line 962, in _parallel_pairwise\r\n        return func(X, Y, **kwds)\r\n      File ""/home/supermicro/.local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py"", line 207, in euclidean_distances\r\n        distances = safe_sparse_dot(X, Y.T, dense_output=True)\r\n      File ""/home/supermicro/.local/lib/python2.7/site-packages/sklearn/utils/extmath.py"", line 178, in safe_sparse_dot\r\n        ret = a * b\r\n      File ""/usr/lib/python2.7/dist-packages/scipy/sparse/base.py"", line 303, in __mul__\r\n        return self._mul_sparse_matrix(other)\r\n      File ""/usr/lib/python2.7/dist-packages/scipy/sparse/compressed.py"", line 528, in _mul_sparse_matrix\r\n        return self.__class__((data,indices,indptr),shape=(M,N))\r\n      File ""/usr/lib/python2.7/dist-packages/scipy/sparse/compressed.py"", line 84, in __init__\r\n        self.check_format(full_check=False)\r\n      File ""/usr/lib/python2.7/dist-packages/scipy/sparse/compressed.py"", line 144, in check_format raise ValueError(""Last value of index pointer should be less than ""\r\n    ValueError: Last value of index pointer should be less than the size of index and data arrays\r\n    *** Error in `python\': munmap_chunk(): invalid pointer: 0x00007f9249d68010 ***\r\n    Aborted (core dumped)'"
4700,75054929,Jonfor,ogrisel,2015-05-11 02:11:33,2015-05-20 12:21:05,2015-05-20 12:21:05,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4700,b'cross_val_predict AttributeError with lists',"b""When calling the cross_val_predict with an X parameter that is a list type, an AttributeError is raised on line 1209. This is because it is checking for the shape of the X parameter, but a list does not have the shape attribute.\r\n\r\nThe documentation says that this function supports lists so I am supposing that it isn't intended behavior. Commenting out that line also makes the rest of the function work perfectly fine.\r\n\r\nAlso not that the cross_val_score function, that takes the same arguments, works fine.\r\n\r\nI can provide the dataset I used if necessary."""
4684,74011421,ogrisel,amueller,2015-05-07 14:44:48,2015-05-20 15:06:56,2015-05-20 15:06:56,closed,,0.16.2,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4684,b'[MRG + 1] FIX for LassoLarsCV on with readonly folds',b'FIX #4597'
4666,72843798,amueller,amueller,2015-05-03 18:19:32,2015-05-04 15:25:24,2015-05-04 13:48:32,closed,,,12,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4666,b'Ridge regression docs say alpha = 1 / C',"b'The ridge regression docs say ``alpha  = 1 / C`` in several places. IIRC that is not true, and it should be ``alpha = n_samples / C``, see #759 (or save yourself the pain and confirm it yourself).\r\n\r\nCan someone confirm?'"
4654,72243054,amueller,ogrisel,2015-04-30 18:32:05,2015-05-07 16:59:39,2015-05-07 10:21:16,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4654,b'[MRG+1] FIX pass percentiles to partial_dependence in plotting.',"b""Fixes #4625. Ping @pprett.\r\nI don't know how to unit-test for this, and I'm not sure it is worth the hassle."""
4646,71997936,amueller,amueller,2015-04-29 21:40:39,2015-09-09 20:21:25,2015-09-09 20:21:25,closed,,,3,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4646,b'OneClassSVM tests hang',"b'On both of my boxes, the common tests on the OneClassSVM sometimes run indefinitely. I have not found any way to reproduce that, but it happened a couple of times now. Does anyone else observe this?'"
4641,71503304,volsky,ogrisel,2015-04-28 06:53:41,2015-05-04 08:55:50,2015-05-04 08:55:50,closed,,,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4641,b'ValueError in dbscan_inner in case of precomputed metric ',"b'In case metric is precomputed neighborhoods array can be 2-dimensional array which causes\r\n```\r\nValueError in dbscan_inner call:\r\n File ""C:\\Python27\\lib\\site-packages\\sklearn\\cluster\\dbscan_.py"", line 232, in fit\r\n    clust = dbscan(X, sample_weight=sample_weight, **self.get_params())\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\cluster\\dbscan_.py"", line 139, in dbscan\r\n    dbscan_inner(core_samples, neighborhoods, labels)\r\n  File ""sklearn/cluster/_dbscan_inner.pyx"", line 19, in sklearn.cluster._dbscan_inner.dbscan_inner (sklearn\\cluster\\_dbscan_inner.cpp:1411)\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n```'"
4634,71095286,adjgiulio,amueller,2015-04-26 17:23:31,2015-06-08 15:19:47,2015-06-08 15:19:47,closed,,,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4634,b'CalibratedClassifierCV in conjunction with SVC and multiclass results in index out of bounds error',"b'Reported this on StackOverflow:\r\nhttp://stackoverflow.com/questions/29873981/error-with-sklearn-calibratedclassifiercv-and-svm\r\n\r\n```\r\nkf = StratifiedShuffleSplit(y, n_iter=1, test_size=0.2)\r\nclf = svm.SVC(C=1,probability=True)            \r\nsig_clf = CalibratedClassifierCV(clf, method=""isotonic"", cv=kf)\r\nsig_clf.fit(X, y)\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/g/anaconda/lib/python2.7/site-packages/sklearn/calibration.py"", line 166, in fit\r\n    calibrated_classifier.fit(X[test], y[test])\r\n  File ""/home/g/anaconda/lib/python2.7/site-packages/sklearn/calibration.py"", line 309, in fit\r\n    calibrator.fit(this_df, Y[:, k], sample_weight)\r\nIndexError: index 9 is out of bounds for axis 1 with size 9\r\n```\r\n\r\nThis is not an issue with any other classifiers I have used, including `RandomForestClassifier` and `LogisticRegression`.\r\n'"
4625,69884708,cnglen,ogrisel,2015-04-21 16:00:32,2015-05-07 10:21:16,2015-05-07 10:21:16,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4625,"b'plot_partial_dependence() ingores ""percentiles""'","b'    pd_result = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(partial_dependence)(gbrt, fxs, X=X,\r\n                                    grid_resolution=grid_resolution)\r\n        for fxs in features)\r\n\r\nshall changed to:\r\n\r\n    pd_result = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(partial_dependence)(gbrt, fxs, X=X,\r\n                                    grid_resolution=grid_resolution, percentiles=percentiles)\r\n        for fxs in features)\r\n\r\n'"
4620,69403498,rabby410002,amueller,2015-04-19 09:14:46,2015-06-15 19:05:34,2015-06-15 19:05:34,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4620,b'ImportError: cannot import name mkdtemp',"b'while using cython to compile ""seq_dataset.pyx""\r\nI met this error :\r\n```\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\testing\\utils.py"", line\r\n15, in <module>\r\n    from tempfile import mkdtemp\r\nImportError: cannot import name mkdtemp\r\n```\r\ni tried to put tempfile.py just under the same filefolder but still not work\r\nis it because there are some conflicts of redundant name or...?\r\ndoes anyone know how to solve this problem? \r\n\r\n\r\nthis is the whole bug message:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\Main.py"", line\r\n 633, in main\r\n    result = compile(sources, options)\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\Main.py"", line\r\n 608, in compile\r\n    return compile_multiple(source, options)\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\Main.py"", line\r\n 581, in compile_multiple\r\n    context = options.create_context()\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\Main.py"", line\r\n 506, in create_context\r\n    self.cplus, self.language_level, options=self)\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\Main.py"", line\r\n 61, in __init__\r\n    import Builtin, CythonScope\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\CythonScope.py\r\n"", line 3, in <module>\r\n    from UtilityCode import CythonUtilityCode\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\UtilityCode.py\r\n"", line 1, in <module>\r\n    from TreeFragment import parse_from_strings, StringParseContext\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\TreeFragment.p\r\ny"", line 10, in <module>\r\n    from Visitor import VisitorTransform\r\n  File ""Visitor.py"", line 11, in init Cython.Compiler.Visitor (Cython\\Compiler\\V\r\nisitor.c:16378)\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\ExprNodes.py"",\r\n line 3755, in <module>\r\n    class SliceIndexNode(ExprNode):\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Compiler\\ExprNodes.py"",\r\n line 3892, in SliceIndexNode\r\n    ""SliceObject"", ""ObjectHandling.c"", context={\'access\': \'Get\'})\r\n  File ""Code.py"", line 275, in Cython.Compiler.Code.UtilityCodeBase.load (Cython\r\n\\Compiler\\Code.c:7470)\r\n\r\n  File ""Code.py"", line 462, in Cython.Compiler.Code.TempitaUtilityCode.__init__\r\n(Cython\\Compiler\\Code.c:11410)\r\n\r\n  File ""Code.py"", line 455, in Cython.Compiler.Code.sub_tempita (Cython\\Compiler\r\n\\Code.c:11151)\r\n\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\Cython\\Tempita\\__init__.py"", l\r\nine 4, in <module>\r\n    from _tempita import *\r\n  File ""_tempita.py"", line 34, in init Cython.Tempita._tempita (Cython\\Tempita\\_\r\ntempita.c:27816)\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\cgi.py"", line 50, in <module>\r\n    import mimetools\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\mimetools.py"", line 6, in <module>\r\n    import tempfile\r\n  File ""tempfile.py"", line 35, in <module>\r\n    from random import Random as _Random\r\n  File ""random.py"", line 5, in <module>\r\n    import numpy as np\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\__init__.py"", line 185,\r\nin <module>\r\n    from . import add_newdocs\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\add_newdocs.py"", line 13\r\n, in <module>\r\n    from numpy.lib import add_newdoc\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\lib\\__init__.py"", line 8\r\n, in <module>\r\n    from .type_check import *\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\lib\\type_check.py"", line\r\n 11, in <module>\r\n    import numpy.core.numeric as _nx\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\core\\__init__.py"", line\r\n46, in <module>\r\n    from numpy.testing import Tester\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\testing\\__init__.py"", li\r\nne 13, in <module>\r\n    from .utils import *\r\n  File ""C:\\Users\\Hsuan\\Anaconda\\lib\\site-packages\\numpy\\testing\\utils.py"", line\r\n15, in <module>\r\n    from tempfile import mkdtemp\r\nImportError: cannot import name mkdtemp\r\n\r\n```'"
4615,69324177,AlexanderFabisch,amueller,2015-04-18 16:27:15,2015-04-20 06:29:13,2015-04-19 19:53:02,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4615,b'TSNE.fit_transform([[]]) does not raise an error',"b'This issue has been mentioned on [stackoverflow](http://stackoverflow.com/questions/28054958).\r\n\r\nThis does not produce an error:\r\n\r\n    In [1]: from sklearn.manifold import TSNE\r\n\r\n    In [2]: TSNE(random_state=0).fit_transform([[]])\r\n    Out[2]: array([[  1.76405235e-04,   4.00157208e-05]])\r\n\r\nI think it should raise an error or return [[]].\r\n\r\nBut this one fails because PCA raises an error (maybe too late?)\r\n```\r\nIn [3]: TSNE(random_state=0, init=""pca"").fit_transform([[]])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-f1099f0b1c29> in <module>()\r\n----> 1 TSNE(random_state=0, init=""pca"").fit_transform([[]])\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/manifold/t_sne.pyc in fit_transform(self, X)\r\n    522             Embedding of the training data in low-dimensional space.\r\n    523         """"""\r\n--> 524         self._fit(X)\r\n    525         return self.embedding_\r\n\r\n...\r\n\r\n/usr/lib/python2.7/dist-packages/scipy/linalg/decomp_qr.pyc in safecall(f, name, *args, **kwargs)\r\n     19     if lwork is None:\r\n     20         kwargs[\'lwork\'] = -1\r\n---> 21         ret = f(*args, **kwargs)\r\n     22         kwargs[\'lwork\'] = ret[-2][0].real.astype(numpy.int)\r\n     23     ret = f(*args, **kwargs)\r\n\r\nValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)\r\n```'"
4597,68665973,DonBeo,amueller,2015-04-15 12:16:38,2015-05-20 15:06:56,2015-05-20 15:06:56,closed,,0.17,34,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4597,b'LassoLarsCV error if n_jobs > 1',"b'```python\r\nn,p = 2194, 12277\r\nX = np.random.normal(0, 1, (n, p))\r\ny = X[:,1] + X[:,2] + np.random.normal(0,1, n)\r\n\r\n\r\nl1 = LassoLarsCV(n_jobs=1).fit(X,y)\r\nprint(l1.coef_)\r\nl = LassoLarsCV(n_jobs=-1).fit(X,y)\r\nprint(l.coef_)\r\n\r\nOUTPUT\r\n\r\n>>> \r\n>>> \r\n[ 0.          0.91122441  0.88051206 ...,  0.          0.          0.        ]\r\nmultiprocessing.pool.RemoteTraceback: \r\n""""""\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py"", line 94, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/least_angle.py"", line 853, in _lars_path_residues\r\n    X_train -= X_mean\r\nValueError: output array is read-only\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py"", line 104, in __call__\r\n    raise TransportableException(text, e_type)\r\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\r\n___________________________________________________________________________\r\nValueError                                         Wed Apr 15 14:14:46 2015\r\nPID: 4123                                    Python 3.4.0: /usr/bin/python3\r\n...........................................................................\r\n/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/least_angle.py in _lars_path_residues(X_train=memmap([[ 1.00577915, -1.05126903,  1.30304231, ... -0.35100295,\r\n        -0.9267396 ,  0.63928884]]), y_train=array([ 1.3232542 ,  0.84262663, -2.62905025, ...,  1.62472897,\r\n       -1.45902494, -5.00504709]), X_test=array([[-0.31256648, -0.8413869 ,  0.46347277, ....  1.21977537,\r\n        -1.12781956, -1.12695671]]), y_test=array([ -1.91413301e-02,  -7.90822410e-01,  -2.1...2352138e-01,  -2.06750561e+00,  -3.79778359e-01]), Gram=\'auto\', copy=False, method=\'lasso\', verbose=0, fit_intercept=True, normalize=True, max_iter=500, eps=2.2204460492503131e-16)\r\n    848         X_test = X_test.copy()\r\n    849         y_test = y_test.copy()\r\n    850 \r\n    851     if fit_intercept:\r\n    852         X_mean = X_train.mean(axis=0)\r\n--> 853         X_train -= X_mean\r\n        X_train = memmap([[ 1.00577915, -1.05126903,  1.30304231, ... -0.35100295,\r\n        -0.9267396 ,  0.63928884]])\r\n        X_mean = memmap([-0.01531417,  0.02889734, -0.01592266, ...,  0.00315228,\r\n       -0.00060194,  0.01170645])\r\n    854         X_test -= X_mean\r\n    855         y_mean = y_train.mean(axis=0)\r\n    856         y_train = as_float_array(y_train, copy=False)\r\n    857         y_train -= y_mean\r\n\r\nValueError: output array is read-only\r\n___________________________________________________________________________\r\n""""""\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py"", line 518, in retrieve\r\n    self._output.append(job.get())\r\n  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 599, in get\r\n    raise self._value\r\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\r\n___________________________________________________________________________\r\nValueError                                         Wed Apr 15 14:14:46 2015\r\nPID: 4123                                    Python 3.4.0: /usr/bin/python3\r\n...........................................................................\r\n/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/least_angle.py in _lars_path_residues(X_train=memmap([[ 1.00577915, -1.05126903,  1.30304231, ... -0.35100295,\r\n        -0.9267396 ,  0.63928884]]), y_train=array([ 1.3232542 ,  0.84262663, -2.62905025, ...,  1.62472897,\r\n       -1.45902494, -5.00504709]), X_test=array([[-0.31256648, -0.8413869 ,  0.46347277, ....  1.21977537,\r\n        -1.12781956, -1.12695671]]), y_test=array([ -1.91413301e-02,  -7.90822410e-01,  -2.1...2352138e-01,  -2.06750561e+00,  -3.79778359e-01]), Gram=\'auto\', copy=False, method=\'lasso\', verbose=0, fit_intercept=True, normalize=True, max_iter=500, eps=2.2204460492503131e-16)\r\n    848         X_test = X_test.copy()\r\n    849         y_test = y_test.copy()\r\n    850 \r\n    851     if fit_intercept:\r\n    852         X_mean = X_train.mean(axis=0)\r\n--> 853         X_train -= X_mean\r\n        X_train = memmap([[ 1.00577915, -1.05126903,  1.30304231, ... -0.35100295,\r\n        -0.9267396 ,  0.63928884]])\r\n        X_mean = memmap([-0.01531417,  0.02889734, -0.01592266, ...,  0.00315228,\r\n       -0.00060194,  0.01170645])\r\n    854         X_test -= X_mean\r\n    855         y_mean = y_train.mean(axis=0)\r\n    856         y_train = as_float_array(y_train, copy=False)\r\n    857         y_train -= y_mean\r\n\r\nValueError: output array is read-only\r\n___________________________________________________________________________\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/donbeo/Desktop/prova.py"", line 31, in <module>\r\n    l = LassoLarsCV(n_jobs=-1).fit(X,y)\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/least_angle.py"", line 999, in fit\r\n    for train, test in cv)\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py"", line 666, in __call__\r\n    self.retrieve()\r\n  File ""/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py"", line 549, in retrieve\r\n    raise exception_type(report)\r\nsklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueError\r\n___________________________________________________________________________\r\nMultiprocessing exception:\r\n    ...........................................................................\r\n/home/donbeo/Desktop/<stdin> in <module>()\r\n----> 1 \r\n      2 \r\n      3 \r\n      4 \r\n      5 \r\n      6 \r\n      7 \r\n      8 \r\n      9 \r\n     10 \r\n\r\n...........................................................................\r\n/home/donbeo/Desktop/prova.py in <module>()\r\n     26 y = X[:,1] + X[:,2] + np.random.normal(0,1, n)\r\n     27 \r\n     28 \r\n     29 l1 = LassoLarsCV(n_jobs=1).fit(X,y)\r\n     30 print(l1.coef_)\r\n---> 31 l = LassoLarsCV(n_jobs=-1).fit(X,y)\r\n     32 print(l.coef_)\r\n     33 \r\n     34 \r\n     35 \r\n\r\n...........................................................................\r\n/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/least_angle.py in fit(self=LassoLarsCV(copy_X=True, cv=None, eps=2.22044604...normalize=True, precompute=\'auto\', verbose=False), X=array([[-0.31256648, -0.8413869 ,  0.46347277, .... -0.35100295,\r\n        -0.9267396 ,  0.63928884]]), y=array([-0.01914133, -0.79082241, -2.15777265, ...,  1.62472897,\r\n       -1.45902494, -5.00504709]))\r\n    994             delayed(_lars_path_residues)(\r\n    995                 X[train], y[train], X[test], y[test], Gram=Gram, copy=False,\r\n    996                 method=self.method, verbose=max(0, self.verbose - 1),\r\n    997                 normalize=self.normalize, fit_intercept=self.fit_intercept,\r\n    998                 max_iter=self.max_iter, eps=self.eps)\r\n--> 999             for train, test in cv)\r\n        cv = sklearn.cross_validation.KFold(n=2194, n_folds=3, shuffle=False, random_state=None)\r\n   1000         all_alphas = np.concatenate(list(zip(*cv_paths))[0])\r\n   1001         # Unique also sorts\r\n   1002         all_alphas = np.unique(all_alphas)\r\n   1003         # Take at most max_n_alphas values\r\n\r\n...........................................................................\r\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\r\n    661             if pre_dispatch == ""all"" or n_jobs == 1:\r\n    662                 # The iterable was consumed all at once by the above for loop.\r\n    663                 # No need to wait for async callbacks to trigger to\r\n    664                 # consumption.\r\n    665                 self._iterating = False\r\n--> 666             self.retrieve()\r\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\r\n    667             # Make sure that we get a last message telling us we are done\r\n    668             elapsed_time = time.time() - self._start_time\r\n    669             self._print(\'Done %3i out of %3i | elapsed: %s finished\',\r\n    670                         (len(self._output),\r\n\r\n    ---------------------------------------------------------------------------\r\n    Sub-process traceback:\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                         Wed Apr 15 14:14:46 2015\r\nPID: 4123                                    Python 3.4.0: /usr/bin/python3\r\n...........................................................................\r\n/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/least_angle.py in _lars_path_residues(X_train=memmap([[ 1.00577915, -1.05126903,  1.30304231, ... -0.35100295,\r\n        -0.9267396 ,  0.63928884]]), y_train=array([ 1.3232542 ,  0.84262663, -2.62905025, ...,  1.62472897,\r\n       -1.45902494, -5.00504709]), X_test=array([[-0.31256648, -0.8413869 ,  0.46347277, ....  1.21977537,\r\n        -1.12781956, -1.12695671]]), y_test=array([ -1.91413301e-02,  -7.90822410e-01,  -2.1...2352138e-01,  -2.06750561e+00,  -3.79778359e-01]), Gram=\'auto\', copy=False, method=\'lasso\', verbose=0, fit_intercept=True, normalize=True, max_iter=500, eps=2.2204460492503131e-16)\r\n    848         X_test = X_test.copy()\r\n    849         y_test = y_test.copy()\r\n    850 \r\n    851     if fit_intercept:\r\n    852         X_mean = X_train.mean(axis=0)\r\n--> 853         X_train -= X_mean\r\n        X_train = memmap([[ 1.00577915, -1.05126903,  1.30304231, ... -0.35100295,\r\n        -0.9267396 ,  0.63928884]])\r\n        X_mean = memmap([-0.01531417,  0.02889734, -0.01592266, ...,  0.00315228,\r\n       -0.00060194,  0.01170645])\r\n    854         X_test -= X_mean\r\n    855         y_mean = y_train.mean(axis=0)\r\n    856         y_train = as_float_array(y_train, copy=False)\r\n    857         y_train -= y_mean\r\n\r\nValueError: output array is read-only\r\n___________________________________________________________________________\r\n>>> >>> \r\n\r\n```'"
4591,68466529,yw652,amueller,2015-04-14 18:45:59,2015-04-15 15:26:17,2015-04-15 15:26:17,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4591,"b""ValueError: Buffer dtype mismatch, expected 'int' but got 'long'""","b""I have X_training data looking like the following:\r\n['this is the first document',\r\n 'this is the second document',\r\n 'this is the third document'] -- essentially a list of string\r\ny_train data like the following:\r\n[['1409108', '1501079', '1335609', '1335703', '1342230', '1335530', '1335602', '1342114', '1335851', '1355623', '1342299', '1342278', '1335804', '1342189', '1351217'], ['1404708', '5101079', '1335609', '1335703', '1342230', '1335530', '1453602', '13422214', '1335851', '1355623', '1345299', '1342278', '1335804', '1342189', '1351217']] -- essentially a list of list, where each list within is a list of topic\r\n\r\nX_train and y_train has the same length, each string in X correpsonds to a list of topic in y. \r\n\r\nI am trying to use the pipeline [('tfidf', TfidfVectorizer()), ('clf', OneVsRestClassifier(LinearSVC()))] to develop a model on the X_train and y_train data, while the system prompts the above error. \r\n\r\nlb = preprocessing.MultiLabelBinarizer()\r\n    y_indicator = lb.fit_transform(y_train)\r\n\r\nclf = Pipeline([\r\n        ('vectorizer', CountVectorizer()),\r\n        ('tfidf', TfidfTransformer()),\r\n        ('clf', OneVsRestClassifier(LinearSVC()))\r\n    ])\r\n\r\nclf.fit(X_train, y_indicator)\r\n\r\nI've seen similar issues posted here and I'm not sure if this is the version issue, but I'll post them here anyway...\r\n\r\nscipy/intel/0.13.3\r\nscikit-learn/intel/0.15.1\r\nscikit-learn/intel/0.15.2\r\n\r\nI'm running my program on nyu hpc server(mercer) so i'm not really sure what's the platform version here. I used to run my program on anaconda locally (python 2.7.9, with every packages updated to date) and they turned out to be completely fine. \r\n\r\nI really appreciate your help!\r\nThanks!\r\n\r\n"""
4564,67517081,shenchong721,amueller,2015-04-10 05:59:38,2015-04-14 15:46:23,2015-04-14 15:46:23,closed,,,3,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4564,b'build failed',"b'error: Command ""gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/shenchong/usr/lib/python2.7/site-packages/numpy/core/include -I/home/shenchong/usr/lib/python2.7/site-packages/numpy/core/include -I/home/shenchong/usr/include/python2.7 -c sklearn/utils/sparsetools/_graph_tools.c -o build/temp.linux-x86_64-2.7/sklearn/utils/sparsetools/_graph_tools.o""'"
4562,67449840,hsuantien,agramfort,2015-04-09 20:58:14,2015-07-13 06:29:32,2015-06-24 21:30:38,closed,,0.17,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4562,b'[MRG + 1] Emptylabelset',"b'When using sklearn/datasets/svmlight_format.py to load multi-label data sets, it is possible that the label set is empty. For instance, line 10 of \r\n\r\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel/mediamill/train-exp1.svm.bz2\r\n\r\nIn that case, the original code would wrongly try to parse the first feature ""index:feature"" as the label-set list (which should be a string with comma as delimiters), causing parsing exceptions.\r\n\r\nThe fix is to detect "":"" when parsing the ""label-set string"". If detected, then the label-set should be empty and the string containing "":"" should belong to features instead.\r\n\r\nThe fix also improve earlier code that copies line_parts (the parsed tokens) to features (the features to be parsed) but almost never uses the latter. Now all the feature-parsing part uses the copied list variable ""features"" instead of ""line_parts"".\r\n\r\nThanks for reviewing.\r\n'"
4559,67388683,amueller,ogrisel,2015-04-09 15:38:35,2015-04-16 14:46:10,2015-04-16 14:46:10,closed,,,0,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4559,b'Bug in assert_raise_message',"b'If the function passed to ``assert_raise_message`` raises a different kind of error than expected, this error is not caught, i.e. the original error is raised.\r\nExample\r\n\r\n```python\r\ndef f():\r\n    raise ValueError(""That\'s how you get ants"")\r\nassert_raise_message(TypeError, ""wrong type"", f)\r\n```\r\nwill raise ""ValueError \'That\'s how you get ants\'"" when it should raise ""AssertionError, f didn\'t raise TypeError"".\r\n\r\nOn the other hand, ``assert_raises_regex`` behaves correctly. So I think instead of fixing the bug, we should just trash ""assert_raise_message"" and use the standard function instead.'"
4556,67354932,adrien-pain-01,adrien-pain-01,2015-04-09 12:52:52,2015-04-15 14:03:25,2015-04-13 23:42:19,closed,,0.16.1,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4556,b'Scikit-learn 0.16.0 performance drop in RandomForests',"b""\r\n![Uploading scikit-learn-0.15.2 - profiling.png . . .]()\r\n![Uploading scikit-learn-0.16.0 - profiling.png . . .]()\r\n\r\nUpgrading scikit-learn from 0.15.2 to 0.16.0 (using Anaconda3 under ubuntu 14.04 64-bits) leads in a performance drop on a project i'm working on, using RandomForests.\r\n\r\n`sklearn-0.15.2` : running in 44.17 sec.  **20.235 sec** spent in `sklearn/ensemble/forest.py:477(predict_proba)`\r\n`sklearn-0.16.0` : running in 80.27 sec. **49.902 sec** spent in `sklearn/ensemble/forest.py:477(predict_proba)`\r\n\r\nBoth runs used same input-data and same sklearn pipeline :\r\n-> MinMaxScaler(copy=True, feature_range=(-1, 1))\r\n-> RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None,\r\n\t max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\r\n\t min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=500,\r\n\t n_jobs=1, oob_score=False, random_state=42, verbose=False,\r\n\t warm_start=False)\r\n}\r\n\r\nPlease note that I specified `n_jobs=1` for both runs.\r\nThe times shown above are given by a predict() loop over all test-datas (820 call to pipeline.predict_proba(), each using the same RandomForest with 500 estimators).\r\n\r\nIt seems that the performance drop comes from multiprocessing (externals/joblib ?) but i'm not sure.\r\n\r\nI've cProfiled the runs, but it seems I can only attach image files (PNG, JPG, ...)\r\nHow can I join the binary dumps from cProfile ? Is it relevant for you to investigate ?\r\n\r\nThanks.\r\n"""
4540,66913363,vkuznet,GaelVaroquaux,2015-04-07 15:17:47,2015-04-14 15:45:22,2015-04-14 15:45:22,closed,,,31,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4540,b'Pandas Validation failure',"b'Hi, I downloaded latest pandas (version 0.16.0) and found that I can\'t longer fit my models. I tried different classifiers and all of them fails with the same error:\r\n\r\n```\r\n  # this is my code where I apply the scaler to train dataset\r\n  x_train = getattr(preprocessing, scaler)().fit_transform(x_train)\r\n\r\n  # this is last part of traceback of sklearn failure\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py"", line 433, in fit_transform\r\n    return self.fit(X, **fit_params).transform(X)\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py"", line 348, in fit\r\n    ensure_2d=False)\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 336, in check_array\r\n    if hasattr(array, ""dtype"") and array.dtype.kind == ""O"":\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/generic.py"", line 1978, in __getattr__\r\n    (type(self).__name__, name))\r\nAttributeError: \'Series\' object has no attribute \'kind\'\r\n```\r\n\r\nThe same code worked fine in previous version of pandas (version 0.15.1) and sklearn (version 0.15.2). I was able to fix the problem by changing sklearn/utils/validation.py file line 336 from\r\n```if hasattr(array, ""dtype"") and array.dtype.kind == ""O"":```\r\nto\r\n```if hasattr(array, ""dtype""):```\r\n\r\nFor reference this is direct URL to the failed line:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/validation.py#L336\r\n\r\nSince I don\'t know the logic of sklearn in details I\'ll leave it up to developers to investigate and apply proper change. But removing array.dtype.kind check fixed the problem for me. \r\nThanks,\r\nValentin.'"
4538,66788854,Quadrocube,amueller,2015-04-07 06:11:43,2015-04-09 15:30:52,2015-04-09 15:30:52,closed,,,4,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4538,b'Something wrong with some of *.cpython file encodings',"b""E.g. trying to calll predict_proba method over the instance of SGDClassifier with loss-type that doesn't support it brings up SyntaxError: invalid or missing encoding declaration for '/usr/lib/python3.4/site-packages/sklearn/linear_model/sgd_fast.cpython-34m.so'."""
4536,66711451,amueller,amueller,2015-04-06 21:36:40,2015-04-15 19:52:39,2015-04-15 19:52:33,closed,,,5,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4536,"b'CalibratedClassifierCV non-deterministic by default, and can not be made deterministic'","b""By default ``CalibratedClassifierCV`` uses a ``LinearSVC`` as ``base_estimator``. ``LinearSVC`` is non-deterministic, which makes testing annoying. I think the default value is not that important, and I'd vote to just fix the random_state in the ``LinearSVC()`` that is constructed."""
4528,66654495,amueller,agramfort,2015-04-06 17:02:34,2015-05-01 21:01:01,2015-05-01 21:00:53,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4528,b'[MRG+1] FIX make rfe feature importance test deterministic.',b'Fixes issues in #4496 which I merged to fast.'
4524,66480112,amueller,amueller,2015-04-05 20:40:03,2015-04-30 18:17:58,2015-04-30 18:17:58,closed,,,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4524,b'utils.testing.if_matplotlib wrong',"b'The utils.testing.if_matplotlib check is too generic and catches all errors. I think it should only catch ""ImportErrror"".\r\nIn particular, it caches errors in test_partial_dependence.py that look like actual errors to me.'"
4523,66478994,amueller,lesteve,2015-04-05 20:28:02,2016-06-29 15:06:00,2016-06-29 15:06:00,closed,,0.18,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4523,b'Jaccard distance in trees very different from pairwise_distances jaccard distance.',"b'As observed in #4522, BallTree and ``pairwise_distances`` have very different results for ``metric=""jaccard""``:\r\n```python\r\nfrom sklearn.neighbors import NearestNeighbors\r\nX = np.random.uniform(size=(6, 5))\r\n\r\nnn = NearestNeighbors(metric=""jaccard"", algorithm=\'brute\').fit(X)\r\nprint(nn.kneighbors(X)[0])\r\nnn = NearestNeighbors(metric=""jaccard"", algorithm=\'ball_tree\').fit(X)\r\nprint(nn.kneighbors(X)[0])\r\n```\r\n\r\n>[[ 0.  1.  1.  1.  1.]\r\n [ 0.  1.  1.  1.  1.]\r\n [ 0.  1.  1.  1.  1.]\r\n [ 0.  1.  1.  1.  1.]\r\n [ 0.  1.  1.  1.  1.]\r\n [ 0.  1.  1.  1.  1.]]\r\n[[ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]\r\n [ 0.  0.  0.  0.  0.]]\r\n\r\n'"
4512,66246175,ThatGeoGuy,ogrisel,2015-04-03 23:18:23,2015-10-12 11:34:56,2015-10-12 11:34:56,closed,,0.17,6,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4512,b'Array dimension issue when using sklearn.covariance.fast_mcd',"b'Lines 362-368 in `sklearn/covariance/robust_covariance.py` specify the following:\r\n\r\n```python\r\n    X = np.asarray(X)\r\n    if X.ndim == 1:\r\n        X = np.reshape(X, (1, -1))\r\n        warnings.warn(""Only one sample available. ""\r\n                      ""You may want to reshape your data array"")\r\n    n_samples, n_features = X.shape\r\n```\r\n\r\nThe problem with this is that if you pass in a 1D array of shape (n_samples,), you typically want the univariate estimate of the MCD for all those samples (hence you actually wanted to pass in an array of shape (n_samples, 1)). However, the code above assumes you really wanted to pass in a 1D array of shape (1, n_features), which has the following problems:\r\n\r\n1. This is backwards to the assumption made by LIBRA (http://wis.kuleuven.be/stat/robust.html\r\n), which contains the reference implementation of FastMCD by Van Dreissen and Rombouts. This can be found in the `mcdcov.m` file, lines 213-215.\r\n2. Nobody in their right mind would try to find the covariance amongst n_features using only a single sample. I\'m half-kidding, but at the very least, the behaviour appears unconventional to me. Perhaps I am missing some justification for this? \r\n3. The current behaviour raises an exception stating that the covariance matrix is singular, when in reality it is non-singular, just univariate. If you for example pass in a matrix object instead of an array with the appropriate dimensions, then the function will compute without error.\r\n\r\nBecause of these reasons, I\'m going to assume this is a bug, which appears similar to the bugs found in #4509 and #4466. The fix is simple, just change the above lines to the following:\r\n\r\n```python\r\n    X = np.asarray(X)\r\n    if X.ndim == 1:\r\n        X = np.reshape(X, (-1, 1))\r\n        warnings.warn(""1D array passed in. "" \r\n                ""Assuming the array contains samples, not features. ""\r\n                ""You may wish to reshape your data."")\r\n    n_samples, n_features = X.shape\r\n```\r\n\r\nWith this fix, finding univariate estimates of the MCD becomes much easier. I have made the above changes to my fork at https://github.com/ThatGeoGuy/scikit-learn and can submit a pull request at any time. However, while running nosetests, I could not correctly get the tests to complete. I also could not find any documentation mentioning how to run the tests so `sklearn.__check_build` will run appropriately. \r\n\r\nAny advice is appreciated. I am examining the `MinCovDet` / `fast_mcd` so that I can hopefully fix issue #3367, which is currently preventing me from completing a project I am working on. '"
4509,66218606,natoromano,amueller,2015-04-03 19:59:10,2015-09-09 22:32:47,2015-09-09 22:32:47,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4509,b'GMM with a population : array shape issue',"b""I'm using GMM on a singe population (list), with version 0.16.\r\n\r\nI get the following error, after calling the score_sample function:\r\n\r\n    'The shape of X  is not compatible with self'\r\n\r\nWhen digging a little bit, I found that what used to be, in the previous versions:\r\n\r\n        X = np.asarray(X)\r\n        if X.ndim == 1:\r\n            X = X[:, np.newaxis]\r\n        if X.size == 0:\r\n            return np.array([]), np.empty((0, self.n_components))\r\n        if X.shape[1] != self.means_.shape[1]:\r\n            raise ValueError('The shape of X  is not compatible with self')\r\n\r\nIs now:\r\n\r\n        X = check_array(X)\r\n        if X.ndim == 1:\r\n            X = X[:, np.newaxis]\r\n        if X.size == 0:\r\n            return np.array([]), np.empty((0, self.n_components))\r\n        if X.shape[1] != self.means_.shape[1]:\r\n            raise ValueError('The shape of X  is not compatible with self')\r\n\r\nThis changes everything, as when you use it on a list, np.asarray returns a 1d array, which is then turned to a 2d array of shape (n,1) by the next lines, and the array is in the correct format. Whereas check_array turns our list into a (1,n) array, which leads to the error.\r\n\r\nAm I getting this wrong or is this indeed what the problem is ? And in this case it's probably a bug in the latest release."""
4507,66153979,zhaipro,amueller,2015-04-03 14:12:58,2015-04-14 15:32:06,2015-04-14 15:32:06,closed,,0.16.1,17,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4507,"b'Fix a bug in _k_means.pyx, when KMeans.fit(X.T)'","b'bad case:\r\n``` Python\r\nX = np.array([[0, 0, 0], [0, 1, 1]])\r\nestimator = KMeans(n_clusters=2, n_init=1, max_iter=20, \\\r\n                   precompute_distances=False)\r\nestimator.fit(X.T)\r\nprint X.T\r\nprint estimator.labels_\r\nprint estimator.cluster_centers_\r\nprint estimator.n_iter_\r\n```'"
4479,65687182,arjoly,amueller,2015-04-01 13:42:42,2015-04-01 17:33:25,2015-04-01 17:33:25,closed,,,2,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4479,b'References are missing in stable version of the doc',"b'For instance, metrics are listed in the dev version, but not in the stable version \r\nof the references \r\n\r\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.cluster\r\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.cluster\r\n\r\nThis is weird.'"
4461,65014339,samzhang111,amueller,2015-03-29 02:37:53,2015-04-19 19:57:43,2015-04-19 19:57:43,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4461,b'Searching across FeatureUnion transformer_weights in a GridSearchCV',"b""I have a Pipeline set up very similar to the FeatureUnion with heterogeneous data sources example (http://scikit-learn.org/dev/auto_examples/hetero_feature_union.html), but I'm attempting to grid-search over the different ways of weighting the transformers in the FeatureUnion:\r\n\r\n```python\r\nparameters = {\r\n    'features__transformer_weights': [{'summary_stats': 1.0, 'vocab': 0.5},\r\n                                      {'summary_stats': 1.0, 'vocab': 1.0},\r\n                                      {'summary_stats': 1.0, 'vocab': 0.1}]\r\n}\r\n\r\ngrid_search = GridSearchCV(full_clf_with_length, parameters, n_jobs=1, verbose=1, scoring='roc_auc')\r\n```\r\n\r\nHowever, it looks like parameters of FeatureUnion objects aren't accessible from within a GridSearch. This would be a useful feature for me, and I'd be happy to contribute a PR if someone could point me in the right direction."""
4460,64998019,amueller,amueller,2015-03-28 22:38:38,2015-04-03 17:41:24,2015-04-03 17:41:24,closed,,0.17,16,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4460,b'Isotonic Calibration example weird / broken',"b""On the GaussianNB plot the scores contradict the description: http://scikit-learn.org/dev/auto_examples/calibration/plot_calibration_curve.html\r\nisotonic calibration actually make things worse.\r\n\r\nPing @agramfort @jmetzen\r\nI have a vague memory of this being discussed before, but I didn't find it.\r\nCurrently there is clearly a conflict of result and description."""
4455,64824386,aishnogah,ogrisel,2015-03-27 18:31:12,2015-10-15 11:39:47,2015-10-15 11:39:47,closed,,,6,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4455,"b""inconsistency in randomized_svd regarding transpose='auto' (default option)""","b'When ```transpose=\'auto\'```, the behavior of ```randomized_svd``` is the opposite of the docstring and code comments:\r\n\r\nfrom the doc string:\r\n```python\r\ndef randomized_svd(M, n_components, n_oversamples=10, n_iter=0,\r\n                   transpose=\'auto\', flip_sign=True, random_state=0):\r\n    """"""    \r\n    ...\r\n    transpose: True, False or \'auto\' (default)\r\n        Whether the algorithm should be applied to M.T instead of M. The\r\n        result should approximately be the same. The \'auto\' mode will\r\n        trigger the transposition if M.shape[1] > M.shape[0] since this\r\n        implementation of randomized SVD tend to be a little faster in that\r\n        case).\r\n    ...\r\n    """"""\r\n```\r\nfrom the code:\r\n```python\r\n    n_samples, n_features = M.shape\r\n\r\n    if transpose == \'auto\' and n_samples > n_features:\r\n        transpose = True\r\n    if transpose:\r\n        # this implementation is a bit faster with smaller shape[1]\r\n        M = M.T\r\n```\r\n\r\nI am unsure which behavior is actually intended or optimal.\r\n\r\n\r\n\r\n'"
4448,64267360,pprett,pprett,2015-03-25 13:23:59,2015-03-26 12:58:16,2015-03-26 12:58:16,closed,,,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4448,b'Fix gbrt min leaf weight',b'Fixes https://github.com/scikit-learn/scikit-learn/issues/4447\r\n\r\ncc @arjoly '
4447,64258791,arjoly,pprett,2015-03-25 12:37:41,2015-03-26 12:58:16,2015-03-26 12:58:16,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4447,"b'Gradient boosting estimator : presort-best splitter expects min_weight_leaf, but is given min_weight_fraction_leaf'","b""Bugs is at the initialization of splitter\r\nhttps://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L1007\r\n\r\nNote that currently this is working partially, because the condition is also verified in the tree growing procedure. Otherwise said, we might accept a split that violate the pre-pruning condition, but won't grow further a node that violates the condition."""
4446,64253438,ematvey,amueller,2015-03-25 12:04:29,2015-03-25 23:38:30,2015-03-25 22:17:07,closed,,0.16,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4446,b'SGD: sample_weight broken when shuffle=True',"b'```python\r\nx, y, w = make_data()\r\nSGDClassifier(shuffle=True).fit(x, y, sample_weight=w)\r\n```\r\nWhen data-derived weights are supplied, training with shuffle=True leads to higly non-stable results (very different on each run). As far as I understand, shuffle is applied to training data `(x, y)` only, not the weights `w`.'"
4442,63983268,dartdog,ogrisel,2015-03-24 12:22:31,2015-03-26 14:32:57,2015-03-26 14:32:57,closed,,0.16,15,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4442,b'Test failing after new install Ubuntu 14.04 and Scikit learn dev 17+',"b'After some issues from a prior install which I finally removed the folders from dist packages and ran make  from the cloned repo I get the following which I see had been an issue prior but it seems that my python-joblib is up to date?\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/tests/test_lfw.py"", line 120, in test_load_fake_lfw_people\r\n    min_faces_per_person=3)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 370, in load_lfw_people\r\n    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 301, in fetch_lfw_people\r\n    min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)\r\n  File ""/home/tom/scikit-learn/sklearn/externals/joblib/memory.py"", line 481, in __call__\r\n    return self._cached_call(args, kwargs)[0]\r\n  File ""/home/tom/scikit-learn/sklearn/externals/joblib/memory.py"", line 428, in _cached_call\r\n    out, metadata = self.call(*args, **kwargs)\r\n  File ""/home/tom/scikit-learn/sklearn/externals/joblib/memory.py"", line 673, in call\r\n    output = self.func(*args, **kwargs)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 202, in _fetch_lfw_people\r\n    faces = _load_imgs(file_paths, slice_, color, resize)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 159, in _load_imgs\r\n    face = imresize(face, resize)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py"", line 420, in imresize\r\n    imnew = im.resize(size, resample=func[interp])\r\n  File ""/usr/local/lib/python2.7/dist-packages/PIL/Image.py"", line 1533, in resize\r\n    if self.size == size:\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading LFW people faces from /tmp/scikit_learn_lfw_test_appTcR/lfw_home\r\nsklearn.datasets.lfw: INFO: Loading face #00001 / 00010\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_pairs\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/tests/test_lfw.py"", line 159, in test_load_fake_lfw_pairs\r\n    lfw_pairs_train = load_lfw_pairs(data_home=SCIKIT_LEARN_DATA)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 492, in load_lfw_pairs\r\n    return fetch_lfw_pairs(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 479, in fetch_lfw_pairs\r\n    slice_=slice_)\r\n  File ""/home/tom/scikit-learn/sklearn/externals/joblib/memory.py"", line 481, in __call__\r\n    return self._cached_call(args, kwargs)[0]\r\n  File ""/home/tom/scikit-learn/sklearn/externals/joblib/memory.py"", line 428, in _cached_call\r\n    out, metadata = self.call(*args, **kwargs)\r\n  File ""/home/tom/scikit-learn/sklearn/externals/joblib/memory.py"", line 673, in call\r\n    output = self.func(*args, **kwargs)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 355, in _fetch_lfw_pairs\r\n    pairs = _load_imgs(file_paths, slice_, color, resize)\r\n  File ""/home/tom/scikit-learn/sklearn/datasets/lfw.py"", line 159, in _load_imgs\r\n    face = imresize(face, resize)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py"", line 420, in imresize\r\n    imnew = im.resize(size, resample=func[interp])\r\n  File ""/usr/local/lib/python2.7/dist-packages/PIL/Image.py"", line 1533, in resize\r\n    if self.size == size:\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading train LFW pairs from /tmp/scikit_learn_lfw_test_appTcR/lfw_home\r\nsklearn.datasets.lfw: INFO: Loading face #00001 / 00020\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n----------------------------------------------------------------------\r\nRan 4949 tests in 101.385s\r\n\r\nFAILED (SKIP=14, errors=2)\r\nmake: *** [test-code] Error 1\r\n\r\ntom@tom-sam:~$ apt-cache policy python-joblib\r\npython-joblib:\r\n  Installed: 0.7.1-1\r\n  Candidate: 0.7.1-1\r\n  Version table:\r\n *** 0.7.1-1 0\r\n        500 http://us.archive.ubuntu.com/ubuntu/ trusty/universe amd64 Packages\r\n        100 /var/lib/dpkg/status\r\ntom@tom-sam:~$ \r\nsklearn.__version__\r\n\'0.17.dev0\'\r\n```'"
4436,63728329,ogrisel,amueller,2015-03-23 13:54:38,2015-03-24 10:32:13,2015-03-23 16:12:32,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4436,b'[MRG+2] FIX make StandardScaler & scale more numerically stable',b'This is a rebased and fixed version of #3747. I will merge if travis is still green. '
4435,63683439,hhchen1105,amueller,2015-03-23 10:04:28,2015-04-17 18:23:21,2015-04-17 18:23:21,closed,,0.17,14,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4435,"b'When fetching both training and test set of the 20newsgroup dataset,'","b""When fetching both training and test set of the 20newsgroup dataset, the data['data'], data['target'], data['filenames'] should be updated."""
4430,63298447,dave1g,amueller,2015-03-20 20:09:23,2016-02-25 10:15:28,2015-03-20 20:26:04,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4430,b'joblib numpy arrays not cleaned up from /dev/shm',"b'I ran a large gridsearch (base don the code here http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py)  and the process locked up, I killed it and tried to rerun and got this error. I later figured out it is because /dev/shm was full of old files from numpy arrays \r\n\r\n```\r\n$ ls -lrt /dev/shm/*joblib*\r\n/dev/shm/joblib_memmaping_pool_64060_189405392:\r\ntotal 13528188\r\n-rw------- 1 general 13852862624 Mar 20 09:09 64060-190150864-190138048-0.pkl_01.npy\r\n-rw------- 1 general         152 Mar 20 09:09 64060-190150864-190138048-0.pkl\r\n```\r\n\r\n4 or 5 similar files/directories were present\r\n\r\nobviously its not possible to cleanup when i killed the task (which was probably stuck because of the lack of space in the first place. (which should have triggered an exception that when caught  would cleanup this space) \r\n\r\nBut when I try to rerun the script these files should be cleaned up in an intelligent manner instead of also crashing, Or should at least fail with a better message as to which directory is full.  (Ionly was able to figure it out form reading this: https://github.com/scikit-learn/scikit-learn/issues/3313 )\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File "".../python/linux/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 240, in save\r\n    obj, filename = self._write_array(obj, filename)\r\n  File "".../python/linux/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 203, in _write_array\r\n    self.np.save(filename, array)\r\n  File "".../python/linux/lib/python2.6/site-packages/numpy/lib/npyio.py"", line 453, in save\r\n    format.write_array(fid, arr)\r\n  File "".../python2.6/site-packages/numpy/lib/format.py"", line 521, in write_array\r\n    array.tofile(fp)\r\nIOError: 1731607818 requested and 502 written\r\n```\r\n'"
4427,63258901,amueller,ogrisel,2015-03-20 16:28:15,2015-03-23 18:16:37,2015-03-23 17:59:00,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4427,"b'[MRG+1] FIX LDA(solver=""lsqr""): make sure the right error is raised on transform'",b'Fixes part B of #4415.'
4422,63154985,trevorstephens,ogrisel,2015-03-20 04:05:43,2015-03-23 13:45:47,2015-03-23 09:49:52,closed,,0.16,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4422,b'[MRG + 1] [FIX] LarsCV and LassoLarsCV fails for numpy 1.8.0',"b""fixes #4399\r\n\r\n@amueller \r\n\r\nAlso, should I also update `y_numeric` for https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/kernel_ridge.py#L144 and https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradient.py#L868 ? Both are explicitly regressors. There's a couple others that share `fit` between classifiers and regressors too: gbm, adaboost, bagging... with no `y_numeric=True` on `check_X_y`. \r\n\r\nPresumably all of the above get through tests because no call to the offending numpy function though...\r\n\r\nCan confirm all tests pass locally on this branch, and 2 fails on an up to date master, with np 1.8.0 installed on python 2.7.9, ubuntu 14.04. :feelsgood: """
4415,62902149,StevenLOL,amueller,2015-03-19 07:25:13,2015-09-09 22:56:32,2015-09-09 22:56:32,closed,,0.16,15,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4415,"b'MemoryError: LW: n_features is too large , when using shrinkage LDA with lsqr solver'","b'Test with the latest 0.17dev \r\n\r\nFollow the example [here] (http://scikit-learn.org/dev/auto_examples/classification/plot_lda.html), but with different size of training data, in my case size of traindata =895x2048   (samples*dim)\r\n\r\n##Error A\r\n\r\n\r\n        lda=LDA(solver=\'lsqr\', shrinkage=\'auto\');\r\n        lda.fit(traindata,trainlabel)\r\n        traindatalda=lda.transform(traindata)\r\n        testdatalda=lda.transform(testdata)\r\n\r\n\r\n\r\nThe following errors are printed:\r\n\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/lda.py"", line 428, in fit\r\n    self._solve_lsqr(X, y, shrinkage=self.shrinkage)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/lda.py"", line 271, in _solve_lsqr\r\n    self.covariance_ = _class_cov(X, y, self.priors_, shrinkage)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/lda.py"", line 123, in _class_cov\r\n    covs.append(np.atleast_2d(_cov(Xg, shrinkage)))\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/lda.py"", line 56, in _cov\r\n    s = sc.std_ * ledoit_wolf(X)[0] * sc.std_  # scale back\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/covariance/shrunk_covariance_.py"", line 291, in ledoit_wolf\r\n    ""try increasing block_size"")\r\nMemoryError: LW: n_features is too large, try increasing block_size\r\n\r\n\r\n##Error B\r\n\r\n        lda=LDA(solver=\'lsqr\');\r\n        lda.fit(traindata,trainlabel)\r\n        traindatalda=lda.transform(traindata)\r\n        testdatalda=lda.transform(testdata)\r\n\r\n\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/lda.py"", line 453, in transform\r\n    check_is_fitted(self, [\'xbar_\', \'scalings_\'], all_or_any=any)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py"", line 622, in check_is_fitted\r\n    raise NotFittedError(msg % {\'name\': type(estimator).__name__})\r\nsklearn.utils.validation.NotFittedError: This LDA instance is not fitted yet. Call \'fit\' with appropriate arguments before using this method.'"
4404,62556069,AdamSLevy,AdamSLevy,2015-03-18 01:21:50,2015-03-23 22:14:00,2015-03-23 20:26:14,closed,,,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4404,"b""AssertionError: dtype('O') != 'uint8'""","b'Using the conda distribution of scikit. Just uninstalled and reinstalled using \r\n    conda remove scikit\r\n    conda install scikit\r\n\r\nto resolve a previous test error.\r\n\r\nNow my test fails with this message:\r\n\r\n    FAIL: sklearn.datasets.tests.test_base.test_load_sample_image\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""//anaconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n        self.test(*self.arg)\r\n      File ""//anaconda/lib/python2.7/site-packages/sklearn/datasets/tests/test_base.py"", line 136, in test_load_sample_image\r\n        assert_equal(china.dtype, \'uint8\')\r\n    AssertionError: dtype(\'O\') != \'uint8\'\r\n    \r\n    ----------------------------------------------------------------------\r\n    Ran 3342 tests in 96.693s\r\n    \r\n    FAILED (SKIP=20, failures=1)'"
4402,62515681,amueller,ogrisel,2015-03-17 20:52:09,2015-03-18 19:13:08,2015-03-18 19:11:45,closed,,0.16,16,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4402,b'[MRG+1] fix ompcv on old scipy versions',"b'Fixes #4387.\r\nWe have an ""if old scipy version ignore error"" which made this error hard to track down ([here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/omp.py#L23) )\r\nSolves #3190 [once and for all](http://i.imgur.com/gHcGr.jpg).'"
4399,62270186,trevorstephens,ogrisel,2015-03-17 03:08:03,2015-03-23 09:49:52,2015-03-23 09:49:52,closed,,0.16,53,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4399,b'Test fails in least_angle on master',"b'Just updated my local branch to master and am running into some fails during `make` that I have not encountered before.\r\n\r\n    Ubuntu 14.04 LTS 64 bit\r\n    Python 2.7.9\r\n    numpy==1.8.0\r\n    scikit-learn==0.17.dev0\r\n    scipy==0.13.3\r\n\r\n    ======================================================================\r\n    ERROR: sklearn.tests.test_common.test_non_meta_estimators(\'LarsCV\', <class \'sklearn.linear_model.least_angle.LarsCV\'>)\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/opt/anaconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n        self.test(*self.arg)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/utils/estimator_checks.py"", line 178, in check_dtype_object\r\n        estimator.fit(X, y.astype(object))\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/linear_model/least_angle.py"", line 999, in fit\r\n        for train, test in cv)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/externals/joblib/parallel.py"", line 659, in __call__\r\n        self.dispatch(function, args, kwargs)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/externals/joblib/parallel.py"", line 406, in dispatch\r\n        job = ImmediateApply(func, args, kwargs)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/externals/joblib/parallel.py"", line 140, in __init__\r\n        self.results = func(*args, **kwargs)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/linear_model/least_angle.py"", line 855, in _lars_path_residues\r\n        y_mean = y_train.mean(axis=0)\r\n      File ""/opt/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py"", line 67, in _mean\r\n        ret = ret.dtype.type(ret / rcount)\r\n    AttributeError: \'int\' object has no attribute \'dtype\'\r\n\r\n    ======================================================================\r\n    ERROR: sklearn.tests.test_common.test_non_meta_estimators(\'LassoLarsCV\', <class \'sklearn.linear_model.least_angle.LassoLarsCV\'>)\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/opt/anaconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n        self.test(*self.arg)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/utils/estimator_checks.py"", line 178, in check_dtype_object\r\n        estimator.fit(X, y.astype(object))\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/linear_model/least_angle.py"", line 999, in fit\r\n        for train, test in cv)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/externals/joblib/parallel.py"", line 659, in __call__\r\n        self.dispatch(function, args, kwargs)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/externals/joblib/parallel.py"", line 406, in dispatch\r\n        job = ImmediateApply(func, args, kwargs)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/externals/joblib/parallel.py"", line 140, in __init__\r\n        self.results = func(*args, **kwargs)\r\n      File ""/bigdrive/Git/scikit-learn/sklearn/linear_model/least_angle.py"", line 855, in _lars_path_residues\r\n        y_mean = y_train.mean(axis=0)\r\n      File ""/opt/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py"", line 67, in _mean\r\n        ret = ret.dtype.type(ret / rcount)\r\n    AttributeError: \'int\' object has no attribute \'dtype\''"
4390,61140916,Barmaley-exe,amueller,2015-03-13 17:08:56,2015-03-16 22:16:01,2015-03-16 19:59:38,closed,,0.16,6,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4390,b'[MRG+1] Replace assert_array_equal with assert_almost_equal to compare arrays of floats',b'Fixes #4386'
4387,60883744,yarikoptic,ogrisel,2015-03-12 19:28:38,2015-03-18 19:11:45,2015-03-18 19:11:45,closed,,0.16,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4387,"b""test_non_meta_estimators 'OrthogonalMatchingPursuitCV' ValueError: array must not contain infs or NaNs""","b'on older systems (such as Debian wheezy) seems to puke (twice)\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_non_meta_estimators(\'OrthogonalMatchingPursuitCV\', <class \'sklearn.linear_model.omp.OrthogonalMatchingPursuitCV\'>)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/tmp/buildd/scikit-learn-0.16.0~b1+git1-gab4d07d/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/utils/estimator_checks.py"", line 178, in check_dtype_object\r\n    estimator.fit(X, y.astype(object))\r\n  File ""/tmp/buildd/scikit-learn-0.16.0~b1+git1-gab4d07d/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/linear_model/omp.py"", line 817, in fit\r\n    for train, test in cv)\r\n  File ""/usr/lib/python2.7/dist-packages/joblib/parallel.py"", line 653, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""/usr/lib/python2.7/dist-packages/joblib/parallel.py"", line 400, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""/usr/lib/python2.7/dist-packages/joblib/parallel.py"", line 138, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""/tmp/buildd/scikit-learn-0.16.0~b1+git1-gab4d07d/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/linear_model/omp.py"", line 710, in _omp_path_residues\r\n    return_path=True)\r\n  File ""/tmp/buildd/scikit-learn-0.16.0~b1+git1-gab4d07d/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/linear_model/omp.py"", line 376, in orthogonal_mp\r\n    copy_X=copy_X, return_path=return_path\r\n  File ""/tmp/buildd/scikit-learn-0.16.0~b1+git1-gab4d07d/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/linear_model/omp.py"", line 110, in _cholesky_omp\r\n    **solve_triangular_args)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/linalg/basic.py"", line 116, in solve_triangular\r\n    a1, b1 = map(np.asarray_chkfinite,(a,b))\r\n  File ""/usr/lib/pymodules/python2.7/numpy/lib/function_base.py"", line 590, in asarray_chkfinite\r\n    ""array must not contain infs or NaNs"")\r\nValueError: array must not contain infs or NaNs\r\n```'"
4386,60882827,yarikoptic,amueller,2015-03-12 19:25:04,2015-03-16 19:59:38,2015-03-16 19:59:38,closed,,0.16,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4386,b'Little precision issue with test_weights_multiplied  on 32bit systems',"b'seems to happen across the board:\r\n```\r\n======================================================================\r\nFAIL: Tests that class_weight and sample_weight are multiplicative\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/tmp/buildd/scikit-learn-0.16.0~b1+git1-gab4d07d/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/linear_model/tests/test_sgd.py"", line 633, in test_weights_multiplied\r\n    assert_array_equal(clf1.coef_, clf2.coef_)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 718, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 644, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 16.6666666667%)\r\n x: array([[-0.34681637, -0.32240913, -0.31335029,  0.34920789,  0.3658206 ,\r\n         0.38752489]])\r\n y: array([[-0.34681637, -0.32240913, -0.31335029,  0.34920789,  0.3658206 ,\r\n         0.38752489]])\r\n\r\n```\r\n```\r\n~/deb/builds/scikit-learn/0.16.0~b1+git1-gab4d07d-1$ grep FAIL: *build\r\nscikit-learn_0.16.0~b1+git1-gab4d07d-1~nd14.04+1_i386.build:FAIL: Tests that class_weight and sample_weight are multiplicative\r\nscikit-learn_0.16.0~b1+git1-gab4d07d-1~nd14.10+1_i386.build:FAIL: Tests that class_weight and sample_weight are multiplicative\r\nscikit-learn_0.16.0~b1+git1-gab4d07d-1~nd+1_i386.build:FAIL: Tests that class_weight and sample_weight are multiplicative\r\nscikit-learn_0.16.0~b1+git1-gab4d07d-1~nd70+1_i386.build:FAIL: Tests that class_weight and sample_weight are multiplicative\r\nscikit-learn_0.16.0~b1+git1-gab4d07d-1~nd70+1_i386.build:FAIL: sklearn.tests.test_common.test_non_meta_estimators(\'LocallyLinearEmbedding\', <class \'sklearn.manifold.locally_linear.LocallyLinearEmbedding\'>)\r\nscikit-learn_0.16.0~b1+git1-gab4d07d-1~nd80+1_i386.build:FAIL: Tests that class_weight and sample_weight are multiplicative\r\n```\r\n'"
4384,60794614,mblondel,mblondel,2015-03-12 09:57:39,2015-03-12 14:09:33,2015-03-12 14:09:33,closed,,,0,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4384,"b""KernelRidge doesn't work with sparse matrices""",b'The problem is just in the input validation.'
4377,60495293,vinayak-mehta,ogrisel,2015-03-10 12:59:07,2015-03-20 22:59:20,2015-03-20 11:56:11,closed,,0.16,29,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4377,b'[MRG + 1] Fixes #4374: LinearSVC(intercept_scaling=0) breaks',b'#4374'
4375,60467460,koszpe,glouppe,2015-03-10 08:47:49,2015-10-19 09:48:19,2015-10-19 09:48:19,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4375,"b""RandomForestClassifier with class_weight='subsample' exit with Access Violation""","b""When I run the RandomForestClassifier from the master branch with class_weight='subsample' and n_jobs=4 parameters, sometimes it ends with exit code -1073741819 (0xC0000005). If I know well, it means Access Violation on Windows.\r\nI guess the problem is the _parallel_build_trees(...) function calls the compute_sample_weight('auto', y, indices) (https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/forest.py line number 92) which uses np.choose(...) (https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/class_weight.py#L141 line number 141). As I know, it is not thread-safe.\r\n\r\nSorry, if I report this bug in wrong place.\r\n\r\n\r\n"""
4374,60443323,amueller,ogrisel,2015-03-10 02:52:38,2015-03-20 11:56:40,2015-03-20 11:56:40,closed,,0.16,10,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4374,b'LinearSVC(intercept_scaling=0) breaks',"b""See [SO](http://stackoverflow.com/questions/28888070/sklearn-gridsearchcv-valueerror-x-has-21-features-per-sample-expecting-19/28955322#28955322).\r\nSetting ``intercept_scaling=0`` changes the shape of ``coef_`` so the estimator can not be used.\r\n\r\nI am a bit surprised that the ``coef_`` is reduced by *two* features, but that seems to be the case. One fore each class? I'm not sure what is happening in liblinear here. I guess the easiest would be to raise on our side, as the combination with ``fit_intercept=True`` is nonsensical."""
4358,60205247,ogrisel,ogrisel,2015-03-07 12:55:16,2015-03-11 18:43:05,2015-03-11 18:43:05,closed,,0.16,6,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4358,b'Broken test_score_objects under windows',"b'```\r\n[00:15:09] ======================================================================\r\n[00:15:09] FAIL: Test that scorers support sample_weight or raise sensible errors\r\n[00:15:09] ----------------------------------------------------------------------\r\n[00:15:09] Traceback (most recent call last):\r\n[00:15:09]   File ""C:\\Python34-x64\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n[00:15:09]     self.test(*self.arg)\r\n[00:15:09]   File ""C:\\Python34-x64\\lib\\site-packages\\sklearn\\utils\\testing.py"", line 300, in wrapper\r\n[00:15:09]     return fn(*args, **kwargs)\r\n[00:15:09]   File ""C:\\Python34-x64\\lib\\site-packages\\sklearn\\metrics\\tests\\test_score_objects.py"", line 343, in test_scorer_sample_weight\r\n[00:15:09]     ""{2}"".format(name, weighted, unweighted))\r\n[00:15:09] AssertionError: 0.29999999999999999 == 0.29999999999999999 : scorer recall_samples behaves identically when called with sample weights: 0.3 vs 0.3\r\n[00:15:09] \r\n[00:15:09] ----------------------------------------------------------------------\r\n```'"
4327,59590844,trevorstephens,glouppe,2015-03-03 03:41:45,2016-02-11 10:42:47,2016-02-11 10:42:47,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4327,b'compute_class_weight() class param behaviour',"b'Not sure if it\'s relevant to the motivation behind the implementation as discussed in #4324 , but a two-class `y` array with two classes present in the `classes` param proceeds with the sum of the weights being equal to the number of classes:\r\n\r\n    compute_class_weight(\'auto\', [0, 1], iris.target[0:100])\r\n    array([ 1.,  1.])\r\n\r\nWhile a three-class y array with only two of the classes present in the `classes` param does something different altogether:\r\n\r\n    compute_class_weight(\'auto\', [0, 1], iris.target[0:120])\r\n    array([ 0.66666667,  0.66666667])\r\n\r\nI had sidestepped this in `compute_sample_weight` in #4190 by determining the present classes from `y` itself. I\'m happy to open a PR to remove the param, and was going to, but while the function is somewhat private, it is exposed in `partial_fit` in `BaseSGDClassifier`: \r\n\r\n> ""In order to use \'auto\' weights, use compute_class_weight(\'auto\', classes, y).""\r\n\r\nSo does this need a deprecation warning? Some more discussion?'"
4324,59569683,hannawallach,amueller,2015-03-02 23:31:42,2015-10-14 16:17:32,2015-10-14 16:17:32,closed,,,24,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4324,b'Possible bug in compute_class_weight()',"b'Hi, \r\n\r\nI think there might be a bug in the compute_class_weight() method in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/class_weight.py. Lines 50 and 51 are as follows:\r\n\r\n        recip_freq = 1. / bincount(y_ind)\r\n        weight = recip_freq[le.transform(classes)] / np.mean(recip_freq)\r\n\r\nFor a two class problem, where there are N0 data points belonging to class 0 and N1 data points belonging to class 1, this is equivalent to saying weight[0] = w0 =  (1 / N0) / (0.5 * (1 / N0 + 1 / N1)) = 2 * N1 / (N0 + N1) and weight[1] = w1 = (1 / N1) / (0.5 * (1 / N0 + 1 / N1)). \r\n\r\nThese expressions for w0 and w1 do not correspond with my intuition or with the expressions given in <a href=""http://gking.harvard.edu/files/0s.pdf"">this paper</a> by King and Zeng.\r\n\r\nIntuitively, assuming that I am trying to ""balance"" my classes using my class weights, I would expect my class weights (w0 and w1, respectively) to satisfy w0 * N0 + w1 * N1 = N0 + N1 and w0 * N0 = w1 * N1. Solving for w0 gives (N0 + N1) / (2 * N0) while solving for w1 gives (N0 + N1) / (2 * N1).\r\n\r\nThis intuition is backed up by King and Zeng\'s paper, which says (on pages 144--145) that if tau is the fraction of ones in the population (i.e., 0.5 for a balanced population, which is what we\'re assuming here) and y is the fraction of ones in the sample (i.e., N1 / (N0 + N1)) then w1 = tau / y = 0.5 / (N1 / (N0 + N1)) = (N0 + N1) / (2 * N1) and similarly w0 = (N0 + N1) / (2 * N0).\r\n\r\nIs this what you were intending to compute in lines 50 and 51? If so, then I think there is a bug.'"
4322,59537714,amueller,ogrisel,2015-03-02 19:51:45,2015-03-20 12:05:11,2015-03-20 12:05:11,closed,,0.16,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4322,b'[MRG+2] Pass include_self=True to kneighbors_graph',b'Fixes #4235.'
4319,59506704,amueller,amueller,2015-03-02 16:41:51,2015-03-02 19:49:48,2015-03-02 19:49:48,closed,,0.16,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4319,b'unexpected keyword average in accuracy_score',"b'In this example:\r\n```\r\nFile ""./text/document_classification_20newsgroups.py"", line 211, in benchmark\r\n    score = metrics.accuracy_score(y_test, pred, average=\'micro\')\r\nTypeError: accuracy_score() got an unexpected keyword argument \'average\'\r\n```\r\n\r\n@jnothman @arjoly can you confirm when this was removed / deprecated?'"
4311,59412013,amueller,ogrisel,2015-03-01 18:55:10,2015-03-06 19:56:29,2015-03-02 19:35:33,closed,,0.16,8,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4311,b'Non-deterministic OMP failures on travis.',b'they make me unhappy\r\n\r\nhttps://travis-ci.org/scikit-learn/scikit-learn/builds/52648279'
4304,59305759,amueller,ogrisel,2015-02-27 22:46:10,2015-03-06 14:07:26,2015-03-06 14:07:26,closed,,0.16,6,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4304,b'SpectralClustering should be explicit about include_self',"b'Currently it raises a deprecation warning because it doesn\'t pass ""include_self"" to ``kneighbors_graph``'"
4303,59282933,ogrisel,ogrisel,2015-02-27 19:33:52,2015-03-06 14:05:36,2015-03-06 14:05:36,closed,,0.16,6,API;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4303,b'The datastructure returned by LSHForest.radius_neighbors should be consistent with the output of exact methods',b'LSHForest has not been released yet. We should therefore fix this API consistency bug before the 0.16 release to avoid introducing a backward incompatible fix in the future.'
4302,59282389,amueller,ogrisel,2015-02-27 19:29:28,2015-03-06 16:30:16,2015-03-06 16:29:46,closed,,0.16,35,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4302,b'[MRG+1] Isotonic regression duplicate fixes',"b'Fixes #4184. With tests from #4185.\r\nThis is a stupid pure-python version. I\'m not sure if there is an easy way to vectorize.\r\nIt now implements the ""secondary"" method, which basically replaces duplicate points with weighted averages. This is the only method that makes ``fit_transform`` behave identical to ``fit().transform()``. I used a naive implementation of ``fit_transform()``.'"
4297,59084912,ogrisel,GaelVaroquaux,2015-02-26 15:03:36,2015-03-06 21:46:24,2015-03-06 21:46:24,closed,,0.16,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4297,b'Infinite loop when running isotonic regression with some zero-valued weights',"b'I extract the following bug from the discussion in https://github.com/scikit-learn/scikit-learn/issues/2507#issuecomment-72048443 :\r\n\r\n```\r\nimport numpy as np\r\nimport sklearn.isotonic\r\n\r\nregression = sklearn.isotonic.IsotonicRegression()\r\nn_samples = 60\r\n\r\nx = np.linspace(-3, 3, n_samples)\r\ny = x + np.random.uniform(size=n_samples)\r\nw = np.random.uniform(size=n_samples)\r\nw[5:8] = 0\r\nregression.fit(x, y, sample_weight=w)\r\n```\r\n\r\nThis bug alone should probably be considered a release critical bug for 0.16.'"
4268,58219440,hannawallach,ogrisel,2015-02-19 15:04:23,2015-02-27 10:35:30,2015-02-27 10:35:30,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4268,b'Bug in BernoulliBN',"b'Hi,\r\n\r\nI found a small bug in the _update_feature_log_prob() method of the BernoulliNB class (https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py). Line 706 currently reads\r\n\r\n    smoothed_cc = self.class_count_ + self.alpha * n_classes\r\n\r\nbut should instead read\r\n\r\n    smoothed_cc = self.class_count_ + self.alpha * 2 \r\n\r\nTo see why this is the case, check out line 8 in the TrainBernoulli() method on this page: http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html (Basically, because features take on the value of 0/1 (presence/absence), the class-conditional probability of the presence/absence of a feature must sum to one over 0/1. Since the numerator is (# data points in class c containing feature + alpha) for 1 and (# data points in class c not containing feature + alpha) for 0, the denominator must contain alpha * 2. Happy to explain more, if that would be useful.)'"
4262,57998064,amueller,ogrisel,2015-02-17 23:15:13,2015-03-03 21:08:22,2015-03-03 21:08:22,closed,,0.16,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4262,b'SVM decision function consistency',"b""Several people on SO and the mailing list have complained they can't reproduce the decision function of the kernel SVM, and I was not able to do so myself. Either something in the docs or in the math is wrong, and I'm afraid I messed up some sign at some point :-/\r\n\r\n\r\nSee:\r\nhttp://stackoverflow.com/questions/28503932/calculating-decision-function-of-svm-manually"""
4260,57916087,ogrisel,ogrisel,2015-02-17 12:02:53,2015-02-25 15:23:10,2015-02-25 15:23:10,closed,,0.16,0,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4260,b'Broken plot_svm_scale_c.py example',"b'Here is the exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""examples/svm/plot_svm_scale_c.py"", line 133, in <module>\r\n    grid.fit(X, y)\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/grid_search.py"", line 732, in fit\r\n    return self._fit(X, y, ParameterGrid(self.param_grid))\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/grid_search.py"", line 505, in _fit\r\n    for parameters in parameter_iterable\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/externals/joblib/parallel.py"", line 659, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/externals/joblib/parallel.py"", line 406, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/externals/joblib/parallel.py"", line 140, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/cross_validation.py"", line 1459, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/svm/classes.py"", line 198, in fit\r\n    loss\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/svm/base.py"", line 774, in _fit_liblinear\r\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\r\n  File ""/volatile/ogrisel/code/scikit-learn/sklearn/svm/base.py"", line 657, in _get_liblinear_solver_type\r\n    % (error_string, penalty, loss, dual))\r\nValueError: Unsupported set of arguments: Loss L2 is not supported, Parameters: penalty=\'L1\', loss=\'L2\', dual=False\r\n```\r\n\r\nI found this pbm by building the documentation.\r\n\r\nThis might have reveal a change that broke backward compatibility. Need investigation.'"
4252,57808792,ogrisel,ogrisel,2015-02-16 14:14:30,2015-09-10 07:26:54,2015-09-10 07:26:54,closed,,1.0,4,API;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4252,b'Ensure that fitting on 1D input data is consistent across estimators',"b'As noted by @amueller in issue #3440  ""Input validation refactoring"", fitting on 1D input is currently not consistent across estimators.\r\n\r\nDepending on the model the following case can either be treated as x is array of 100 samples and 1 feature (like `MinMaxScaler` does for instance) or as 1 sample with 100 features as most models currently do even if almost always counter-intuitive to do so.\r\n\r\n```python\r\nx = np.linspace(0, 1, 100)\r\nestimator.fit(x, y)\r\n```\r\n\r\nMost models use `check_X_y(X, y)` or `check_array(X)` and leave the `ensure_2d=True` kwarg to its default value. The current behavior of `ensure_2d` is to cast 1d array as row-vectors (1 sample with len(X) features). This was done so for backward compatibility reasons.\r\n\r\nI think this behavior is counter-intuitive and we should break backward compat for this edge-case to always treat 1d arrays as multi-sample collections of a single features rather than the opposite.\r\n\r\nI mark this issue as an API discussion. It should be tackled before version 1.0.'"
4245,57670006,ogrisel,ogrisel,2015-02-13 23:54:05,2015-02-26 09:50:01,2015-02-26 09:47:15,closed,,0.16,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4245,b'[MRG+2] Fix empty input data common checks',b'This PR includes #4214 but also additional common tests that currently fail on some estimators that do not have a consistent behavior and that probably need to be fixed on a case by case basis.'
4238,57253353,Thunder1989,amueller,2015-02-10 23:18:00,2015-02-11 00:34:41,2015-02-11 00:21:26,closed,,0.16,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4238,b'different results on differen versions of sklearn',"b'I reinstalled my sklearn to the latest 0.15 and re-ran the same code with the same input on same setting (k-fold splitting, random seed) using SVC, but the classification results are quite different (like ~75% vs 93%). Previously I was using 0.14. Any thoughts on why?'"
4235,57231627,amueller,ogrisel,2015-02-10 20:27:22,2015-03-20 12:05:11,2015-03-20 12:05:11,closed,,0.16,17,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4235,b'Check all usages of kneighbors_graph',"b""After @MechCoder fixed kneighbors_graph in #4046, we should check all uses in the code for whether we want ``include_self==True`` or not. I came across this in ``SpectralEmbedding`` where the current default makes no sense, I think.\r\nYou can simply ``git grep`` and should find a lot of occurrences.\r\nIf our test output wasn't so flooded with warnings, we would have probably detected that earlier :-/"""
4227,57014669,Treora,amueller,2015-02-09 10:46:53,2015-03-03 22:12:24,2015-03-03 22:12:24,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4227,b'Random state in RBM',"b""I have not scrutinised this properly, but the way random state is used in the `BernoulliRBM` looks dubious. If `random_state` is set with an integer, `check_random_state` will return a new random genenator with that state. If I'm not mistaken this could lead to state loops when trying to do many iterations of gibbs sampling, as the new state has become a deterministic function of the old state (see [line 214]).\r\n\r\n`partial_fit` seems to already sets a `self.random_state_` to circumvent problems with reusing random state ([line 236]). Perhaps a single random generator could be set in `__init__`? Like so:\r\n`self.random_state = check_random_state(random_state)`\r\n\r\n[line 214]: https://github.com/scikit-learn/scikit-learn/blob/a3283c6d6bf5e4163c2991ec4c4e25cb08ce6e44/sklearn/neural_network/rbm.py#L214\r\n[line 236]: https://github.com/scikit-learn/scikit-learn/blob/a3283c6d6bf5e4163c2991ec4c4e25cb08ce6e44/sklearn/neural_network/rbm.py#L236"""
4224,56992859,jamestwebber,MechCoder,2015-02-09 06:13:55,2015-02-12 16:44:25,2015-02-12 16:44:25,closed,,0.16,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4224,b' local variable referenced before assignment in LassoCV and ElasticNetCV',"b""Per @GaelVaroquaux's [comment](https://github.com/scikit-learn/scikit-learn/issues/1059#issuecomment-73461028) in #1059, I'm opening a new ticket for this.\r\n\r\nIf I try to use LassoCV or ElasticNetCV to fit a target of uniform values, it will fail to properly initialize and this leads to an unassigned variable in the fit() method. This seems to be happening because it tries to pick the alpha values based on the range of the target vector (see [this line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/coordinate_descent.py#L100)) and that leads to a bunch of NaNs, which leads to trouble in [this comparison](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/coordinate_descent.py#L1095) because NaN is not less than Inf and so best_l1_ratio, etc are never assigned.\r\n\r\nOne possible fix would be at [coordinate_descent.py#L100](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/coordinate_descent.py#L100), changing that so it doesn't run into numerical problems.\r\n\r\nCode to reproduce (this was with version 0.15.1 but it doesn't look like this has been fixed since then):\r\n\r\n```python\r\nimport numpy as np\r\n\r\nfrom sklearn.linear_model import ElasticNetCV, LassoCV\r\n\r\nenet = ElasticNetCV()\r\n\r\nX = np.random.random(size=(5,5))\r\n\r\ny = np.zeros(5)\r\n\r\nenet.fit(X, y)\r\n/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:100: RuntimeWarning: divide by zero encountered in log10\r\n  alphas = np.logspace(np.log10(alpha_max * eps), np.log10(alpha_max),\r\n/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/function_base.py:87: RuntimeWarning: invalid value encountered in double_scalars\r\n  step = (stop-start)/float((num-1))\r\n/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:479: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\r\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, positive)\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-6-290bc86de561> in <module>()\r\n----> 1 enet.fit(X, y)\r\n\r\n/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc in fit(self, X, y)\r\n   1113                 best_mse = this_best_mse\r\n   1114 \r\n-> 1115         self.l1_ratio_ = best_l1_ratio\r\n   1116         self.alpha_ = best_alpha\r\n   1117         if self.alphas is None:\r\n\r\nUnboundLocalError: local variable 'best_l1_ratio' referenced before assignment\r\n\r\nlasso = LassoCV()\r\n\r\nlasso.fit(X,y)\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-9-5cdf793e8cfd> in <module>()\r\n----> 1 lasso.fit(X,y)\r\n\r\n/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc in fit(self, X, y)\r\n   1113                 best_mse = this_best_mse\r\n   1114 \r\n-> 1115         self.l1_ratio_ = best_l1_ratio\r\n   1116         self.alpha_ = best_alpha\r\n   1117         if self.alphas is None:\r\n\r\nUnboundLocalError: local variable 'best_l1_ratio' referenced before assignment\r\n\r\ny = np.ones(5)\r\n\r\nlasso.fit(X,y)\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-11-5cdf793e8cfd> in <module>()\r\n----> 1 lasso.fit(X,y)\r\n\r\n/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc in fit(self, X, y)\r\n   1113                 best_mse = this_best_mse\r\n   1114 \r\n-> 1115         self.l1_ratio_ = best_l1_ratio\r\n   1116         self.alpha_ = best_alpha\r\n   1117         if self.alphas is None:\r\n\r\nUnboundLocalError: local variable 'best_l1_ratio' referenced before assignment\r\n```"""
4212,56810451,ogrisel,ogrisel,2015-02-06 13:16:41,2015-10-28 17:00:09,2015-10-28 17:00:09,closed,,,3,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4212,b'Heisen test failure in test_transformer_n_iter with CCA',"b'The following was caught by a 64 bit Windows build with Python 3.4:\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_transformer_n_iter(\'CCA\', CCA(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06))\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python34-x64\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python34-x64\\lib\\site-packages\\sklearn\\utils\\estimator_checks.py"", line 999, in check_transformer_n_iter\r\n    estimator.fit(X, y_)\r\n  File ""C:\\Python34-x64\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py"", line 329, in fit\r\n    linalg.pinv(np.dot(self.x_loadings_.T, self.x_weights_)))\r\n  File ""C:\\Python34-x64\\lib\\site-packages\\scipy\\linalg\\basic.py"", line 600, in pinv\r\n    a = np.asarray_chkfinite(a)\r\n  File ""C:\\Python34-x64\\lib\\site-packages\\numpy\\lib\\function_base.py"", line 595, in asarray_chkfinite\r\n    ""array must not contain infs or NaNs"")\r\nValueError: array must not contain infs or NaNs\r\n```\r\n\r\nIt\'s very rare so this might not be related to the platform nor the Python version.'"
4206,56692407,ogrisel,jnothman,2015-02-05 16:23:51,2015-02-07 10:54:45,2015-02-07 10:54:45,closed,,0.16,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4206,b'[MRG+1] explicit exception message for strict selectors',"b'This is a fix for #4059 to raise a `ValueError` at transform time with an explicit error message instead of crashing when calling `get_support()` with a cryptic error message.\r\n\r\nNote that for consistency, the behavior of `SelectKBest(k=0)` is also impacted by this change.'"
4185,55979995,mjbommar,mjbommar,2015-01-30 00:47:39,2015-02-28 13:00:14,2015-02-28 12:23:11,closed,,0.16,44,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4185,b'Adding unit test to cover ties/duplicate x values in Isotonic Regression...',b'Unit test to highlight regression in issue #4184 '
4184,55976235,mjbommar,ogrisel,2015-01-30 00:00:36,2015-03-06 16:29:46,2015-03-06 16:29:46,closed,,0.16,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4184,b'IsotonicRegression results differ between fit/transform and fit_transform with ties in X',"b'  Per conversation in issue #2507, IsotonicRegression appears to have regressed due to commit a9ea55f.\r\n\r\n  [This IPython notebook](http://nbviewer.ipython.org/urls/gist.githubusercontent.com/mjbommar/74fcefdcd0f2b1a5f708/raw/4742691db799101091598922cd0808f1eb5f07f2/isotonic_test_case_20150129.json) demonstrates the failure on HEAD.\r\n\r\n  I tested the following two commits with the notebook:\r\n* d255866: no difference, SUCCESS\r\n* a9ea55f: difference, FAILURE\r\n\r\n  In other words, I think we can blame the switch for ``interp1d`` from ""linear"" to ""slinear""; first thought is that 1-d spline ""slinear"" matrix formulation is ill-posed for x-ties, whereas the piecewise ""linear"" implementation is unaffected?\r\n\r\nSmall additional note: confirmed failure with test case where x-values are all non-zero,  e.g., ``[1, 1, 2, 3, 4]`` instead of ``[0, 0, 1, 2, 3]``, so ``x=0`` isn\'t part of the cause.\r\n'"
4182,55951212,amueller,amueller,2015-01-29 20:20:13,2016-06-22 12:14:54,2015-09-20 03:37:26,closed,,0.17,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4182,"b'[MRG+1] Give dpgmm a covars_ property, allows for sampling'",b'Fixes #1637.\r\n'
4176,55821030,amueller,amueller,2015-01-28 21:50:43,2015-02-07 14:33:41,2015-02-07 12:26:07,closed,,0.16,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4176,"b'[MRG + 1] Better error messages in MeanShift, slightly more robust to bad binning.'","b'Fixes #2356. (see discussion there)\r\nThis will still crash for some variant of the reported example, like this:\r\n```\r\nimport numpy as np\r\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\r\nfrom sklearn.datasets.samples_generator import make_blobs\r\n\r\n\r\n# Generate sample data\r\ncenters = [\r\n    np.ones(200),\r\n    -np.ones(200),\r\n]\r\nX, _ = make_blobs(n_samples=100, centers=centers, cluster_std=0.3, random_state=0)\r\n\r\nms = MeanShift(bin_seeding=True)\r\n\r\nms.fit(X)\r\n```\r\nbut it will tell you that "" No point was within bandwidth of any seed. Try a different seeding strategy.""\r\nTrying to put a grid on a 200d space is just not a good idea.'"
4175,55802664,amueller,amueller,2015-01-28 19:25:10,2015-09-20 03:37:21,2015-09-20 03:37:21,closed,,0.17,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4175,b'[MRG] Fix gamma update in DPGMM',"b""Fixes #1764.\r\nTests don't change, I have no idea how to test for this.\r\nIn the light of #2454 it seems unlikely we can currently test for this in a sensible way."""
4168,55653827,otakar-smrz,amueller,2015-01-27 18:14:52,2015-02-27 22:10:49,2015-02-27 22:10:49,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4168,b'Mahalanobis distance in covariance.EmpiricalCovariance is wrong',"b'The Mahalanobis distance is correctly defined in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/dist_metrics.pyx#L623. However, the mahalanobis method of covariance.EmpiricalCovariance https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/covariance/empirical_covariance_.py#L258 is wrong for two reasons:\r\n\r\n1. The current implementation returns the *square of the distance*. The np.sqrt function must be applied on the current result in order to get the true distance.\r\n\r\n2. The docs say ""the provided observations are assumed to be centered"". Yet, the opposite is true, since the observations *are being centered* inside the method, and the user *should not be centering* the observations.\r\n\r\nThanks for fixing these and making the relevant changes to the dependent examples.\r\n'"
4156,55387868,AlexanderFabisch,larsmans,2015-01-24 22:49:55,2015-01-27 21:35:43,2015-01-27 21:35:43,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4156,b'[MRG + 1] Fix issue #4154',
4154,55275767,bthirion,larsmans,2015-01-23 12:20:23,2015-01-27 21:49:00,2015-01-27 21:36:20,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4154,b'TSNE spits an error when n_components=1',"b""Sorry, I haven't taken time to investigate it in detail\r\n\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\nx = np.random.randn(100, 10)\r\nTSNE(n_components=2).fit_transform(x)\r\nTSNE(n_components=1).fit_transform(x)"""
4150,55243966,cathydeng,jnothman,2015-01-23 04:28:05,2015-01-24 11:37:31,2015-01-24 11:37:25,closed,,0.16,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4150,b'[MRG + 1] fix agglomerative clustering w/ precomputed distances & connectivity matrix',"b""linkage_tree doesn't calculate distances when affinity = 'precomputed' and connectivity is not None. PR includes fix & test case. let me know if there's anything I should change!"""
4146,55205835,amueller,amueller,2015-01-22 20:40:20,2015-02-24 22:15:19,2015-02-24 22:15:19,closed,,0.16,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4146,b'[MRG + 1] Fdr treshold bug',"b'Continues #2932. Fixes #2771.\r\nThese are some minor fixes on top of #2932, where @bthirion already gave his +1.\r\nMaybe @arjoly wants to have a look as he commented there.\r\nThis is a good bug fix that I think we should include asap.\r\n\r\nFYI tests take .5s.'"
4144,55137938,SylvainTakerkart,amueller,2015-01-22 10:38:20,2015-03-10 21:55:36,2015-03-10 21:43:45,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4144,b'problem with sample_weight in SVC...',"b""Hello, following Andy's advice(http://sourceforge.net/p/scikit-learn/mailman/message/33251002/), I'm opening this issue... Here is the description of the problem:\r\n\r\nI'm trying to use the instance weighting capability of the SVC class, and I\r\nencountered some weird behavior: when the C parameter is chosen very small\r\nand the weights are not very large, weighted SVM yields constant\r\nprediction...\r\n\r\nBelow, you'll find a  piece of code that demonstrates this. What it does:\r\n\r\n1. create a dummy dataset for binary classification, with a training and a\r\ntesting dataset\r\n2. create random weights that are very very very close to one (so close\r\nthat they should not influence what follows)\r\n3. run unweighted and weighted SVM (hereafter SVM and wSVM) with decreasing\r\nvalues of C\r\n\r\nWe expect SVM and wSVM to yield exactly the same predictions because the\r\ninstance weights are ridiculously close to one. This is exactly what\r\nhappens for large to small-ish values of C; but at some point when C gets\r\nsmaller, wSVM yields constant predictions (either all zeros, or all ones)\r\nwhile SVM still behaves normally...\r\n\r\nSylvain\r\n\r\n```python\r\nimport numpy as np\r\nfrom numpy.random import multivariate_normal\r\n\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n\r\n# create dummy training and testing dataset...\r\nn_samples_per_class = 100\r\n\r\nX_train_plus = multivariate_normal([0,2],np.identity(2),n_samples_per_class)\r\nX_train_minus = multivariate_normal([0,3],np.identity(2),n_samples_per_class)\r\nX_test_plus = multivariate_normal([0,2],np.identity(2),n_samples_per_class)\r\nX_test_minus = multivariate_normal([0,3],np.identity(2),n_samples_per_class)\r\n\r\nX_train = np.vstack([X_train_plus, X_train_minus])\r\nX_test = np.vstack([X_test_plus, X_test_minus])\r\n\r\n# create labels\r\ny_train = np.hstack([np.zeros(n_samples_per_class), np.ones(n_samples_per_class)])\r\ny_test = np.hstack([np.zeros(n_samples_per_class), np.ones(n_samples_per_class)])\r\n\r\nn_train_samples = len(y_train)\r\n\r\n# the list of decreasing C values\r\nC_list = 10. ** np.arange(2,-5,-1)\r\n\r\n# create random sample weights (very very very close to one)\r\nmu = 1.\r\nsigma = 0.001\r\nsample_weights = np.random.normal(mu,sigma,n_train_samples)\r\n\r\nfor C in C_list:\r\n    # run SVM\r\n    svc = SVC(C=C)\r\n    svc.fit(X_train,y_train)\r\n    y_svc = svc.predict(X_test)\r\n    svc_acc = accuracy_score(y_svc,y_test)\r\n\r\n    # run weighted SVM\r\n    wsvc = SVC(C=C)\r\n    wsvc.fit(X_train,y_train,sample_weight=sample_weights)\r\n    y_wsvc = wsvc.predict(X_test)\r\n\r\n    wsvc_acc = accuracy_score(y_wsvc,y_test)\r\n    print 'Accuracies of SVM and weighted-SVM for C = %f: %.2f %.2f' % (C,svc_acc,wsvc_acc)\r\n    print 'Predictions for weighted-SVM:'\r\n    print y_wsvc\r\n    print\r\n```\r\n\r\n\r\n"""
4141,55075577,amueller,larsmans,2015-01-21 20:54:19,2015-01-24 17:36:39,2015-01-24 14:24:18,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4141,b'[MRG] Bootstrap with test_size=1 fix',"b'Before ``test_size=1`` gave you the whole data.\r\n\r\n```python\r\nisinstance(1, numbers.real) == True\r\n```\r\nso we need to change the order of evaluation.\r\n\r\nFixes #4070'"
4137,54953531,amueller,ogrisel,2015-01-20 23:02:19,2015-02-26 09:54:27,2015-02-26 09:54:27,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4137,b'[MRG+1] Make BaggingClassifier use if_delegate_has_method in decision_function',
4136,54950104,amueller,amueller,2015-01-20 22:31:03,2015-02-13 23:36:22,2015-02-13 22:13:59,closed,,0.16,18,API;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4136,"b'[MRG+1] More robust input validation, more testing.'","b'Test a a lot more of the estimators for input validation etc.\r\nEnforce that they can accept other dtypes, in particular ``float32``.\r\nHack for parts of #4134, fixing parts of #4056, fixing #4124, fixing #4133 (debatable) and #4132.'"
4135,54949802,mathDR,amueller,2015-01-20 22:28:42,2015-01-22 18:36:40,2015-01-22 17:49:23,closed,,0.16,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4135,b'sklearn.naive_bayes error in variance',"b'Line 219, in function _update_mean_variance():\r\n\r\nreturn total_sum / n_total, total_ssd / n_total\r\n\r\nshould be\r\nreturn total_sum / n_total, total_ssd / (n_total-1)\r\n\r\nas you want the unbiased variance and not the biased version (if n_total == 1, variance = 0) \r\n'"
4130,54896130,ZhiNie,amueller,2015-01-20 15:28:09,2015-03-23 22:22:35,2015-03-23 22:22:35,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4130,b'test_tree failed after installing the package',"b'After I installed the package, I run the file ""test_tree.py"" under the directory ""/scikit-learn/sklearn/tree/tests"".  Here is the message I got: \r\nTraceback (most recent call last):\r\n  File ""test_tree.py"", line 39, in <module>\r\n    from sklearn.tree.tree import SPARSE_SPLITTERS\r\nImportError: cannot import name SPARSE_SPLITTERS\r\n\r\nI have no idea why this happens.\r\n'"
4127,54827845,wizeman,larsmans,2015-01-20 00:05:26,2015-02-07 21:36:46,2015-02-07 21:36:46,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4127,b'test_disk_used fails when filesystem is compressing data',"b'While building scikit-learn 0.15.2 with python 2.7.9, I have experienced the following test failure:\r\n\r\n```\r\nFAIL: sklearn.externals.joblib.test.test_disk.test_disk_used\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/nix/store/pq6rm5aj8n0z6ahlgglnjbj7kxby2mh1-python2.7-nose-1.3.4/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/tmp/nix-build-python2.7-scikit-learn-0.15.2.drv-0/scikit-learn-0.15.2/sklearn/externals/joblib/test/test_disk.py"", line 40, in test_disk_used\r\n    nose.tools.assert_true(disk_used(cachedir) >= target_size)\r\nAssertionError: False is not true\r\n    \'False is not true\' = self._formatMessage(\'False is not true\', ""%s is not true"" % safe_repr(False))\r\n>>  raise self.failureException(\'False is not true\')\r\n    \r\n\r\n----------------------------------------------------------------------\r\nRan 3496 tests in 150.553s\r\n\r\nFAILED (SKIP=18, failures=1)\r\n```\r\n\r\nLooking into the test and at the `disk_used()` function, I am guessing that the most likely reason for the failure is that the test doesn\'t take into account if the filesystem is compressing the data.\r\n\r\nSince I am using the ZFS filesystem and I have enabled compression by default, it will cause the on-disk size of the file(s) to become smaller than the size of the written data (unless the written data was random).\r\n'"
4124,54763095,h10r,amueller,2015-01-19 13:25:58,2015-02-25 18:56:55,2015-02-25 18:56:55,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4124,"b""sklearn.manifold.TSNE, ValueError: Buffer dtype mismatch, expected 'float_t' but got 'float'""","b'Thank you so much for adding sklearn.manifold.TSNE, I\'m very excited that it\'s now part of sklearn! However, when I try using it, I get the following error message (both with 0.15.2 and 0.16):\r\n\r\n```python\r\nTraceback (most recent call last):\r\nFile ""main.py"", line 104, in <module>\r\nvectors_tsne = do_tsne_on_vectors( vectors_in_file )\r\nFile ""main.py"", line 56, in do_tsne_on_vectors\r\nreturn tsne.fit_transform( vectors )\r\nFile ""(...)/sklearn/manifold/t_sne.py"", line 519, in fit_transform\r\nself._fit(X)\r\nFile ""(...)/sklearn/manifold/t_sne.py"", line 444, in _fit\r\nP = _joint_probabilities(distances, self.perplexity, self.verbose)\r\nFile ""(...)/sklearn/manifold/t_sne.py"", line 51, in _joint_probabilities\r\ndistances, desired_perplexity, verbose)\r\nFile ""_utils.pyx"", line 14, in sklearn.manifold._utils._binary_search_perplexity (sklearn/manifold/_utils.c:2023)\r\nValueError: Buffer dtype mismatch, expected \'float_t\' but got \'float\'\r\n```\r\n\r\nTo fix this, I did:\r\n\r\n```\r\nvectors = np.asfarray( vectors, dtype=\'float\' )\r\n```\r\n\r\nwhich seems to work for me. \r\n\r\nMy input is a list of vectors I get from gensim.models.word2vec.\r\n\r\nKind regards and thanks for the great library!\r\nHendrik'"
4121,54705955,seowyanyi,ogrisel,2015-01-18 18:55:57,2015-03-02 14:43:19,2015-02-28 17:25:10,closed,,0.16,20,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4121,b'[MRG] Fix boundary handling in radius_neighbors (issue #4072)',b'Fixes #4072 \r\n\r\nThis PR makes the `radius_neighbors` for brute force to include points lying on boundary of chosen radius. This is the same behavior as the KD Tree and Ball Tree `radius_neighbors`. \r\n\r\n'
4109,54627718,amueller,agramfort,2015-01-16 21:34:15,2015-02-04 19:04:35,2015-02-03 18:52:17,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4109,b'[MRG + 1] Gmm tied covariance fixes from #4039',"b'Following up on #4039, minor fixes.'"
4102,54386881,vineetp13,amueller,2015-01-14 22:34:17,2015-02-10 16:19:21,2015-02-10 16:19:21,closed,,0.16,27,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4102,b'nosetest failure:assertionerror',"b'```\r\n======================================================================\r\nFAIL: sklearn.feature_extraction.tests.test_image.test_connect_regions\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Library/Python/2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/tests/test_image.py"", line 63, in test_connect_regions\r\n    assert_equal(ndimage.label(mask)[1], connected_components(graph)[0])\r\nAssertionError: 777 != 767\r\n\r\n======================================================================\r\nFAIL: sklearn.feature_extraction.tests.test_image.test_connect_regions_with_grid\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Library/Python/2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Library/Python/2.7/site-packages/sklearn/feature_extraction/tests/test_image.py"", line 70, in test_connect_regions_with_grid\r\n    assert_equal(ndimage.label(mask)[1], connected_components(graph)[0])\r\nAssertionError: 777 != 767\r\n\r\n----------------------------------------------------------------------\r\nRan 3342 tests in 148.698s\r\n\r\nFAILED (SKIP=20, failures=2)\r\n```'"
4074,53973455,oeddyo,jnothman,2015-01-10 22:17:33,2015-01-14 10:58:20,2015-01-14 10:58:20,closed,,,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4074,b'nearest_centroid.py is iterating over all Y labels instead of classes',"b'In nearest_centroid function fit, it is looping over all Y\'s \r\n\r\n        for cur_class in y_ind:\r\n            center_mask = y_ind == cur_class\r\n            nk[cur_class] = np.sum(center_mask)\r\n            if is_X_sparse:\r\n                center_mask = np.where(center_mask)[0]\r\n\r\n But I think the idea is to compute the centroid for each possible classes. And by looping over ""y_ind"" it essentially goes over all Y\'s. Thus the complexity becomes O(N*N*M) where N is number of examples and M is number of features. My understanding is it should be O(N*M*K) where K is number of unique classes. Thus the code should really be\r\n\r\n        for cur_class in self.classes_:\r\n            center_mask = y_ind == cur_class\r\n            nk[cur_class] = np.sum(center_mask)\r\n            if is_X_sparse:\r\n                center_mask = np.where(center_mask)[0]\r\n'"
4072,53959895,jnothman,jnothman,2015-01-10 14:36:31,2015-03-03 20:02:25,2015-03-03 20:02:25,closed,,0.16,12,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4072,b'Handling of boundary in radius_neighbors inconsistent',"b'The handling of boundary case in `neghbors.*.radius_neighbors` is not properly documented or tested: BallTree/KDTree appear to include the boundary (i.e. where the distance between the query and target is equal to the radius, the target is not returned), but the brute method in nearest neighbors does not. `LSHForest` includes the boundary due to [this logic](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/approximate.py#L314).\r\n\r\nThis should be consistent, tested and better documented.'"
4064,53707356,amueller,ogrisel,2015-01-08 02:19:34,2015-02-07 10:43:31,2015-02-06 18:22:40,closed,,0.16,36,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4064,"b'[MRG+1] FIX Pipelined fitting of Clustering algorithms, scoring of K-Means in pipelines'","b""Fixes #4063.\r\n\r\nAlso fixes the embarrassing bug where clustering algorithms can't be used in pipelines."""
4063,53700719,pmaher86,ogrisel,2015-01-08 00:30:14,2015-02-06 18:22:40,2015-02-06 18:22:40,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4063,b'Pipeline score broken for unsupervised algorithms',"b""The score method for pipeline assigns `y=None` and always passes it to the score method of the final estimator, but some estimators (for instance KMeans) take only X. Is there a reason to use `y=None` instead of `*args`?\r\n\r\nReproducible by\r\n```python\r\nimport numpy as np,sklearn\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.cluster import KMeans\r\nX=np.random.rand(100,2)\r\np=Pipeline([('clf',KMeans(n_clusters=3))])\r\np.fit(X)\r\np.score(X)\r\n```"""
4059,53665414,amueller,jnothman,2015-01-07 18:37:09,2015-02-07 10:54:52,2015-02-07 10:54:52,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4059,b'Crash in univariate feature selection if no feature is selected.',"b'Univariate feature selection crashes if no feature is selected with an unhelpful message:\r\n```python\r\nfrom sklearn.feature_selection import SelectFdr\r\n\r\nrng = np.random.RandomState(0)\r\nX = rng.rand(40, 10) \r\ny = rng.randint(0, 4, size=40)\r\n\r\nfdr = SelectFdr()\r\nfdr.fit(X, y)\r\nfdr.transform(X)\r\n```\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-5cd77e510247> in <module>()\r\n----> 1 asdf.transform(X)\r\n\r\n/home/andy/checkout/scikit-learn/sklearn/feature_selection/base.pyc in transform(self, X)\r\n     73         """"""\r\n     74         X = check_array(X, accept_sparse=\'csr\')\r\n---> 75         mask = self.get_support()\r\n     76         if len(mask) != X.shape[1]:\r\n     77             raise ValueError(""X has a different shape than during fitting."")\r\n\r\n/home/andy/checkout/scikit-learn/sklearn/feature_selection/base.pyc in get_support(self, indices)\r\n     44             values are indices into the input feature vector.\r\n     45         """"""\r\n---> 46         mask = self._get_support_mask()\r\n     47         return mask if not indices else np.where(mask)[0]\r\n     48 \r\n\r\n/home/andy/checkout/scikit-learn/sklearn/feature_selection/univariate_selection.pyc in _get_support_mask(self)\r\n    488         alpha = self.alpha\r\n    489         sv = np.sort(self.pvalues_)\r\n--> 490         threshold = sv[sv < alpha * np.arange(len(self.pvalues_))].max()\r\n    491         return self.pvalues_ <= threshold\r\n    492 \r\n\r\n/usr/lib/python2.7/dist-packages/numpy/core/_methods.pyc in _amax(a, axis, out, keepdims)\r\n     15 def _amax(a, axis=None, out=None, keepdims=False):\r\n     16     return um.maximum.reduce(a, axis=axis,\r\n---> 17                             out=out, keepdims=keepdims)\r\n     18 \r\n     19 def _amin(a, axis=None, out=None, keepdims=False):\r\n\r\nValueError: zero-size array to reduction operation maximum which has no identity\r\n```'"
4055,53565307,amueller,amueller,2015-01-06 21:25:50,2015-02-24 21:39:25,2015-02-24 21:39:25,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4055,b'GMM bic crashes on dtype object',"b""GMM is not happy with ``dtype=object`` in some cases.\r\n\r\nI'm not sure if there is an open issue on handling dtype='object'.\r\nI think we should try to convert to float in ``check_array`` if ``dtype=object``.\r\nDoes anyone remember where the discussion about this was?\r\n\r\nI think #4006 is related (we should collect dtype object issues somewhere).\r\n\r\nto reproduce\r\n```python\r\nfrom sklearn import mixture\r\nfrom sklearn.datasets import make_blobs\r\n\r\nX, _ = make_blobs()\r\nX = X.astype(object)\r\ng = mixture.GMM(covariance_type='full',n_components=4) #for some reason only full works\r\ng.fit(X)\r\n#works\r\nprint g.bic(X)\r\ng = mixture.GMM(covariance_type='diag',n_components=4) #for some reason only full works\r\ng.fit(X)\r\n#fails:\r\nprint g.bic(X)\r\n```"""
4046,53308591,MechCoder,MechCoder,2015-01-03 16:37:55,2015-01-29 14:00:25,2015-01-29 09:06:10,closed,,0.16,80,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4046,b'[MRG + 2] Refactor kneighbors_graph and radius_neighbors_graph (et.al)',"b""Following the discussion in https://github.com/scikit-learn/scikit-learn/pull/4019 this PR\r\n1. Allows X to be None in `neighbors.kneighbors` which skips the first neighbor in both distance and connectivity mode.\r\n2. Do not do anything special in other cases.\r\n\r\n\r\n    In [2]: knn = NearestNeighbors(n_neighbors=1).fit([[0], [1]])\r\n    In [3]: knn.kneighbors_graph([[0], [1]]).toarray()\r\n    Out[3]: \r\n    array([[ 1.,  0.],\r\n           [ 0.,  1.]])\r\n    In [5]: knn.kneighbors_graph([[2], [1]]).toarray()\r\n    Out[5]: \r\n    array([[ 0.,  1.],\r\n           [ 0.,  1.]])\r\n    In [8]: knn.kneighbors_graph().toarray()\r\n    Out[8]: \r\n    array([[ 0.,  1.],\r\n           [ 1.,  0.]])\r\n    In [6]: knn.kneighbors_graph([[2], [1]], mode='distance').toarray()\r\n    Out[6]: \r\n    array([[ 0.,  1.],\r\n           [ 0.,  0.]])\r\n    In [7]: knn.kneighbors_graph([[0], [1]], mode='distance').toarray()\r\n    Out[7]: \r\n    array([[ 0.,  0.],\r\n           [ 0.,  0.]])\r\n    In [9]: knn.kneighbors_graph(mode='distance').toarray()\r\n    Out[9]: \r\n    array([[ 0.,  1.],\r\n           [ 1.,  0.]])\r\n\r\nAlso, a happy 2015 to everyone!"""
4039,53197841,wadawson,amueller,2014-12-31 18:53:59,2015-02-04 19:05:16,2015-02-04 19:04:45,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4039,"b""Corrected two bugs related to 'tied' covariance_type in mixture.GMM(), a...""","b""...dded test, closes #4036\r\n\r\nThe first bug was with the Gaussian log-density calculation for the 'tied' covariance_type.\r\nRather than fix this equation I reformatted the covars data structure so that it could be fed\r\ninto the 'full' covariance_type log-density calculation. It might be a tiny bit slower but I think\r\nit is better to have the least amount of potentially redundant code as this should minimize\r\nthe potential for such errors.\r\n\r\nThe second bug I fixed was related to the _covar_mstep_tied() function, only the first part\r\nof the equation should be divided by X.shape[0]."""
4036,53153051,wadawson,amueller,2014-12-30 22:35:01,2015-02-03 18:52:42,2015-02-03 18:52:42,closed,,0.16,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4036,"b""mixture.gmm() produces different BIC scores for 'tied' vs other covariance types, for 1 component fits to 1D data""","b""For 1 dimensional data fit with a one component GMM, I would expect that the BIC scores of the 'tied' and other covariance types ('diag', 'spherical', 'full') should be the same (as is the case when the data is run through the R package Mclust (note Mclust actually plots -BIC):\r\n![mclust](https://cloud.githubusercontent.com/assets/3067471/5583501/faffaf7e-902e-11e4-9ac5-6e944bf2bb7e.png)\r\nhowever this is not the case, as can be seen from the following result generated with sklearn.mixture.gmm() on the same 1D data:\r\n![test2](https://cloud.githubusercontent.com/assets/3067471/5583531/61f914ea-902f-11e4-8369-febd2c82e4c7.png)\r\nNote that the \\DeltaBIC score is just a given model's BIC score minus the best model's BIC score. So for 1 component, both the 'tied' and 'diag' point should be on top of one another.\r\n\r\nI have verified that the BIC difference (for the 1 component, 1D case) is only due to different results with self.score() results for 'tied' vs 'diag'. (i.e. not due to improper estimate of self._n_parameters())\r\n\r\nNote that 'diag', 'spherical', and 'full' all produce the same BIC score for the 1 component, 1D case. It is only 'tied' that differs."""
4015,52967154,MechCoder,MechCoder,2014-12-27 22:10:42,2015-01-30 20:19:42,2015-01-29 09:08:30,closed,,,16,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4015,b'Inconsistency between `mode=connectivity` and `mode=distance` for kneighbors_graph',"b""I can completely understand why this happens, but I'm not sure how to handle this. The k nearest neighbors for a sample when using `mode=connectivity` includes the current sample, but while using `mode=distance` uses `k` neighbors does not take into account the current sample.\r\n\r\n    >>> X = [[0], [3], [1]]\r\n    >>> from sklearn.neighbors import kneighbors_graph\r\n    >>> A = kneighbors_graph(X, 2)\r\n    >>> A.toarray()\r\n    array([[ 1.,  0.,  1.],\r\n           [ 0.,  1.,  1.],\r\n           [ 1.,  0.,  1.]])\r\n    >>> A = kneighbors_graph(X, 2, mode='distance')\r\n    >>> A.toarray()\r\n    array([[ 0.,  3.,  1.],\r\n           [ 3.,  0.,  2.],\r\n           [ 1.,  2.,  0.]]\r\n\r\nBackground: Using connected components using the distance and connectivity modes, give different results.\r\n\r\n\r\n\r\n"""
4006,52927606,MidoriYakumo,ogrisel,2014-12-26 15:47:50,2015-02-24 21:33:21,2015-02-24 21:33:21,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/4006,b'QDA may meet broken 2d array',"b'```\r\n>>> sklearn.__version__\r\n\'0.16-git\'\r\n```\r\nFor my dataset it gives:\r\n```\r\n  File ""/usr/lib64/python3.4/site-packages/sklearn/qda.py"", line 148, in _decision_function\r\n    return (-0.5 * (norm2 + np.sum(np.log(self.scalings_), 1))\r\nAttributeError: \'numpy.ndarray\' object has no attribute \'log\'\r\n```\r\nWhen\r\n```\r\nqda.py:134:         self.scalings_ = np.asarray(scalings)\r\n```\r\nIt shows that scalings is a 2d array in shape of (0..23, 0..23, 0..23, 0..6)'"
3984,52491179,davidbrough1,amueller,2014-12-19 14:57:59,2014-12-27 17:58:09,2014-12-27 17:58:09,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3984,b'Remove dimension requirement for GridSearchCV and train_test_split',"b'In version 0.15.2, GridSearchCV and train_test_split require the input arrays have the dimensions [n_samples, n_features]. Neither of this functions need to have a dimension requirement.'"
3981,52420076,andrewmwhite,jnothman,2014-12-18 21:43:14,2014-12-24 04:16:15,2014-12-24 04:16:15,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3981,b'MultinomialNB produces different predictions for same feature values but different *label* values',"b'The `MultinomialMB` class can produce different predictions for the same data depending on the values of the labels assigned to the classes. Given:\r\n\r\n    from sklearn.naive_bayes import MultinomialNB\r\n    X = [[0, 0], [0, 1], [1, 1], [1, 0]]\r\n\r\nthis code:\r\n\r\n    y_int = [0, 0, 1, 1]\r\n    print MultinomialNB().fit(X, y_int).predict(X)\r\n\r\nproduces the (expected) output `[0 0 1 1]`, while this code:\r\n\r\n    y_rev = [1, 1, 0, 0]\r\n    print MultinomialNB().fit(X, y_rev).predict(X)\r\n\r\nproduces `[0 1 0 0]`.'"
3977,52275292,amueller,amueller,2014-12-17 18:40:04,2014-12-17 18:42:37,2014-12-17 18:42:37,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3977,b'Regression in LinearSVC',"b'It looks like there is a regression in ``LinearSVC``. I ran the ``bench_mnist.py`` script in #3939 and compared it against ##3204, where the main difference is that #3939 tracks master.\r\nThe fourier_approx_svm results are much worse on master and take longer.\r\n\r\n```\r\nClassifier                  train-time       test-time      error-rate   \r\nfourier_approx_svm         139.248281956   1.81456899643      0.0487 (old)\r\nfourier_approx_svm         231.988574028   1.94094800949       0.072 (master)\r\n```\r\nOn the old branch I get\r\n```\r\nsklearn/svm/base.py:731: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\r\n  ""the number of iterations."", ConvergenceWarning)\r\n```\r\nwhich I don\'t get on master...'"
3975,52252580,MechCoder,amueller,2014-12-17 15:27:54,2015-02-25 04:15:17,2015-02-25 04:15:17,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3975,b'`n_components` parameter seems unused in AgglomerativeClustering et al.',"b'The n_components parameter is overwritten by the number of connected components found using `sparsetools.connected_components` over here, https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/hierarchical.py#L62 . Also the attribute `n_component_` is almost always equal to the parameter `n_component` over here (unless returned by `scipy.cluster.hierarchy`, https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/hierarchical.py#L248 . \r\n\r\nIf it is unused, I think we should just deprecate it.'"
3945,51316849,AlexisMignon,ogrisel,2014-12-08 15:53:22,2015-02-24 22:39:35,2015-02-24 22:39:35,closed,,0.16,17,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3945,b'[MRG + 1] Fix problem with non positive definite covariance matrices in GMM',"b""Due to round off errors, the formula used to compute the weighted covariance matrix may lead to matrices with large negative eigenvalues.\r\n\r\nThe formula is:\r\nC = \\sum_i w_i (x_i - \\mu)(x_i - \\mu)'                         (1)\r\nC = (\\sum_i w_i x_i x_i')  - \\mu \\mu'                           (2)\r\n\r\nassuming \\sum_i w_i = 1 and \\mu = \\sum_i w_i x_i\r\n\r\nThe second formula does not guarantee that C is positive definite.\r\n\r\nThe fix may have as a side effect to increase the memory consumption since a new array containing the difference (X - mu) is created and may take longer to compute."""
3935,51026634,larsmans,larsmans,2014-12-04 20:44:30,2014-12-04 21:21:39,2014-12-04 21:21:39,closed,,,3,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3935,b'Isomap has hidden randomness',"b'I just got this the first time I ran `nosetests sklearn/manifold/tests/test_isomap.py`. I couldn\'t reproduce it.\r\n\r\n```\r\n======================================================================\r\nFAIL: test_isomap.test_transform\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/lars/src/scikit-learn/sklearn/manifold/tests/test_isomap.py"", line 101, in test_transform\r\n    assert_less(np.sqrt(np.mean((X_iso - X_iso2) ** 2)), 2 * noise_scale)\r\nAssertionError: 0.051835189464888631 not less than 0.02\r\n    """"""Fail immediately, with the given message.""""""\r\n>>  raise self.failureException(\'0.051835189464888631 not less than 0.02\')\r\n```\r\n\r\nThere\'s no `random_state` on Isomap.'"
3932,50900241,dominiqu,amueller,2014-12-03 22:10:55,2015-04-03 17:56:10,2015-04-03 17:55:26,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3932,b'bug in PLSRegression() when one of the columns in X is constant',"b""In PLSRegression(), pls.transform(X,Y) on the training data should yield the same scores for X as returned by pls.x_scores_ (see example below). It does when the matrix X is full rank (none of the columns are of constant value). However, when adding a constant column to X, this stops being true, which appears to be a bug. Another way to think of this is that adding extraneous dimensions to X (basically embedding its data in a higher dimensional space while retaining its orginal true dimensionality) shouldn't affect the directions obtained by PLS (i.e. the coefficients in the original dimensions should remain the same). This is true in the PLS implementation in Matlab (I checked), but not in the implementation in ScikitLearn. \r\n\r\nThoughts/questions?\r\n\r\nI am copying an example below so you can reproduce it.\r\n\r\nThanks!\r\nDominique\r\n\r\n\r\nExample:\r\n\r\n```\r\n### generate data\r\nfrom pylab import *\r\nx = 2*linspace(0, 10, 100)\r\ny = 5*linspace(0, 10, 100)\r\nX,Y = meshgrid(y, x)\r\nZ = X+Y\r\n\r\n### transform data \r\nadd_extraneous_dimension_to_X=False\r\nif add_extraneous_dimension_to_X:\r\n    XX = hstack([X.reshape((prod(X.shape),1)), Y.reshape((prod(Y.shape),1)), np.zeros((prod(X.shape),1))])\r\nelse:\r\n    XX = hstack([X.reshape((prod(X.shape),1)), Y.reshape((prod(Y.shape),1))])\r\nYY = reshape(Z,[10000,1])\r\n\r\n### zero mean and normalize the data\r\nmeans = mean(XX,axis=0)\r\nstds = std(XX,axis=0)\r\n\r\nfor ix in range(XX.shape[1]):\r\n    XX[:,ix] = XX[:,ix] - means[ix]\r\n\r\n### normalize the data\r\nfor ix in range(XX.shape[1]):\r\n    stdDev = stds[ix]\r\n    if stdDev > 1e-10:\r\n        XX[:,ix] = XX[:,ix]/stdDev\r\n    else:\r\n        XX[:,ix] = np.zeros(XX[:,ix].shape)\r\n\r\n### now learn the pls axes\r\npls = PLSRegression(n_components=XX.shape[1])\r\npls.fit(XX, YY)\r\n\r\n### do a transform of sample rows in the training data \r\nXX_PLS, YY_PLS = pls.transform(XX[0:10,:],YY[:10])\r\n\r\n### compare the transform to the loadings\r\nprint XX_PLS[0:10,:] - pls.x_scores_[0:10,:]   # there should be no difference between these two\r\n```"""
3930,50857229,dengemann,MechCoder,2014-12-03 16:12:44,2016-03-16 20:09:28,2016-03-16 20:09:28,closed,,0.18,14,API;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3930,b'FIX/ENH?: PCA and RandomizedPCA now have inconsistent meanings of ``.components_``',"b'@eickenberg introduced a new API / meaning of the components attribute to PCA: https://github.com/scikit-learn/scikit-learn/commit/3f6c61fed71563cb91d9291d851af5fa5a662eb4\r\n\r\nThe RandomizedPCA code still stores whitened components instead of unit scale components. This should be changed, right?\r\n\r\ncc @agramfort @ogrisel '"
3928,50718107,trevorstephens,trevorstephens,2014-12-02 20:52:44,2014-12-12 15:44:37,2014-12-12 15:44:37,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3928,b'SGDClassifier -- class_weights & sample_weights',"b""Easy one first, there is an unused `class_weight` parameter in the `fit` method signature, `class_weight` flows in through the constructor:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradient.py#L527\r\n\r\nJust to prove it:\r\n\r\n    from sklearn.linear_model import SGDClassifier\r\n    from sklearn.datasets import make_classification\r\n    from sklearn.utils import compute_class_weight\r\n    import numpy as np\r\n    \r\n    X, y = make_classification(n_features=5, weights=[0.7, 0.3],\r\n                               n_clusters_per_class=1, random_state=415)\r\n    \r\n    # Baseline\r\n    clf = SGDClassifier()\r\n    clf.fit(X, y)\r\n    print clf.coef_\r\n    # [[ 2.13434174  1.8734288   2.12685039  5.08116123  2.89369872]]\r\n    \r\n    # With unused fit class_weight attribute\r\n    clf = SGDClassifier()\r\n    clf.fit(X, y, class_weight='auto')\r\n    print clf.coef_\r\n    # [[ 2.13434174  1.8734288   2.12685039  5.08116123  2.89369872]]\r\n\r\nNow weighting the samples in different (equivalent) ways:\r\n\r\n    # With auto-weights\r\n    clf = SGDClassifier(class_weight='auto')\r\n    clf.fit(X, y)\r\n    print clf.coef_\r\n    # [[ 10.10838607  -4.29529238  13.14026606  -5.99728163   7.65887541]]\r\n    \r\n    weights = compute_class_weight('auto', clf.classes_, y)\r\n    weights = dict(zip(clf.classes_, weights))\r\n    mapper = np.vectorize(lambda c: weights[c])\r\n    weights = mapper(y)\r\n    \r\n    # With manual auto-weights\r\n    clf = SGDClassifier()\r\n    clf.fit(X, y, sample_weight=weights)\r\n    print clf.coef_\r\n    # [[ 10.10838607  -4.29529238  13.14026606  -5.99728163   7.65887541]]\r\n    \r\n    # With manual auto-weights & unused fit class_weight attribute\r\n    clf = SGDClassifier()\r\n    clf.fit(X, y, sample_weight=weights, class_weight='auto')\r\n    print clf.coef_\r\n    # [[ 10.10838607  -4.29529238  13.14026606  -5.99728163   7.65887541]]\r\n\r\nAll fine so far, but if you do both `class_weight` in the constructor and `sample_weights` in the fitting, the resulting weights appear to be multiplicative. \r\n\r\n    # With manual auto-weights, squared\r\n    clf = SGDClassifier()\r\n    clf.fit(X, y, sample_weight=weights**2)\r\n    print clf.coef_\r\n    # [[  3.22495438  14.11510502   0.58504094   6.38631993   9.55338404]]\r\n    \r\n    # With auto-weights manual auto-weights -- multiplicative\r\n    clf = SGDClassifier(class_weight='auto')\r\n    clf.fit(X, y, sample_weight=weights)\r\n    print clf.coef_\r\n    # [[  3.22495438  14.11510502   0.58504094   6.38631993   9.55338404]]\r\n\r\nWhether this is desirable or not is one thing, but it does not appear to be documented anywhere, ie neither `class_weight` nor `sample_weight` refer to one another in their docstrings. I feel like perhaps a warning or error should be raised, or at least a mention of the interaction in the docstring."""
3872,49685306,ogrisel,ogrisel,2014-11-21 11:19:16,2015-02-25 18:55:47,2015-02-25 13:35:43,closed,,0.16,11,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3872,b'Fix broken examples reported when building the documentation',"b'When building the documentation of the current master we get the following failures.\r\n\r\n```\r\nplotting plot_compare_cross_decomposition.py\r\n\r\n===================================\r\nCompare cross decomposition methods\r\n===================================\r\n\r\nSimple usage of various cross decomposition algorithms:\r\n- PLSCanonical\r\n- PLSRegression, with multivariate response, a.k.a. PLS2\r\n- PLSRegression, with univariate response, a.k.a. PLS1\r\n- CCA\r\n\r\nGiven 2 multivariate covarying two-dimensional datasets, X, and Y,\r\nPLS extracts the \'directions of covariance\', i.e. the components of each\r\ndatasets that explain the most shared variance between both datasets.\r\nThis is apparent on the **scatterplot matrix** display: components 1 in\r\ndataset X and dataset Y are maximally correlated (points lie around the\r\nfirst diagonal). This is also true for components 2 in both dataset,\r\nhowever, the correlation across datasets for different components is\r\nweak: the point cloud is very spherical.\r\n\r\nCorr(X)\r\n[[ 1.    0.53 -0.01 -0.01]\r\n [ 0.53  1.   -0.06 -0.06]\r\n [-0.01 -0.06  1.    0.5 ]\r\n [-0.01 -0.06  0.5   1.  ]]\r\nCorr(Y)\r\n[[ 1.    0.45 -0.02 -0.02]\r\n [ 0.45  1.   -0.06  0.02]\r\n [-0.02 -0.06  1.    0.53]\r\n [-0.02  0.02  0.53  1.  ]]\r\nTrue B (such that: Y = XB + Err)\r\n[[1 1 1]\r\n [2 2 2]\r\n [0 0 0]\r\n [0 0 0]\r\n [0 0 0]\r\n [0 0 0]\r\n [0 0 0]\r\n [0 0 0]\r\n [0 0 0]\r\n [0 0 0]]\r\nEstimated B\r\n[[ 1.   1.   1. ]\r\n [ 2.   2.   2. ]\r\n [ 0.1  0.   0. ]\r\n [ 0.  -0.   0. ]\r\n [-0.   0.   0. ]\r\n [-0.  -0.1 -0. ]\r\n [-0.  -0.  -0. ]\r\n [-0.   0.   0. ]\r\n [-0.  -0.  -0. ]\r\n [-0.   0.  -0. ]]\r\n________________________________________________________________________________\r\nplot_compare_cross_decomposition.py is not compiling:\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/code/scikit-learn/doc/sphinxext/gen_rst.py"", line 861, in generate_file_rst\r\n    execfile(os.path.basename(src_file), my_globals)\r\n  File ""/Users/ogrisel/code/scikit-learn/doc/sphinxext/gen_rst.py"", line 47, in execfile\r\n    exec(code, global_vars, local_vars)\r\n  File ""plot_compare_cross_decomposition.py"", line 137, in <module>\r\n    pls1.fit(X, y)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/cross_decomposition/pls_.py"", line 247, in fit\r\n    \'has %s\' % (X.shape[0], Y.shape[0]))\r\nValueError: Incompatible shapes: X has 1000 samples, while Y has 1\r\n________________________________________________________________________________\r\n - time elapsed : 0 sec\r\nplotting plot_pca_3d.py\r\n\r\n=========================================================\r\nPrincipal components analysis (PCA)\r\n=========================================================\r\n\r\nThese figures aid in illustrating how a point cloud\r\ncan be very flat in one direction--which is where PCA\r\ncomes in to choose a direction that is not flat.\r\n\r\n\r\n________________________________________________________________________________\r\nplot_pca_3d.py is not compiling:\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/matplotlib/colors.py"", line 368, in to_rgba\r\n    \'length of rgba sequence should be either 3 or 4\')\r\nValueError: length of rgba sequence should be either 3 or 4\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/matplotlib/colors.py"", line 399, in to_rgba_array\r\n    return np.array([self.to_rgba(c, alpha)], dtype=np.float)\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/matplotlib/colors.py"", line 376, in to_rgba\r\n    \'to_rgba: Invalid rgba arg ""%s""\\n%s\' % (str(arg), exc))\r\nValueError: to_rgba: Invalid rgba arg ""[ 0.10633089  0.00265258  0.00217836 ...,  0.00257675  0.0280324\r\n  0.01485181]""\r\nlength of rgba sequence should be either 3 or 4\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/code/scikit-learn/doc/sphinxext/gen_rst.py"", line 861, in generate_file_rst\r\n    execfile(os.path.basename(src_file), my_globals)\r\n  File ""/Users/ogrisel/code/scikit-learn/doc/sphinxext/gen_rst.py"", line 47, in execfile\r\n    exec(code, global_vars, local_vars)\r\n  File ""plot_pca_3d.py"", line 93, in <module>\r\n    plot_figs(1, elev, azim)\r\n  File ""plot_pca_3d.py"", line 65, in plot_figs\r\n    ax.scatter(a[::10], b[::10], c[::10], c=density, marker=\'+\', alpha=.4)\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/mpl_toolkits/mplot3d/axes3d.py"", line 2240, in scatter\r\n    patches = Axes.scatter(self, xs, ys, s=s, c=c, *args, **kwargs)\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/matplotlib/axes/_axes.py"", line 3614, in scatter\r\n    colors = mcolors.colorConverter.to_rgba_array(c, alpha)\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/matplotlib/colors.py"", line 403, in to_rgba_array\r\n    raise ValueError(""Color array must be two-dimensional"")\r\nValueError: Color array must be two-dimensional\r\n________________________________________________________________________________\r\n```'"
3863,49294425,julian-ramos,larsmans,2014-11-18 21:17:22,2014-11-30 13:30:15,2014-11-30 13:30:15,closed,,,11,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3863,b'OneVsOneClassifier.fit returns error when y is list',"b""Hi all,\r\n\r\nIf you pass y as a list the next error appears\r\n\r\nTypeError: only integer arrays with one element can be converted to an index\r\n\r\nThe error goes away when you pass y as numpy array however shouldn't this be either taken care of in the code (check whether is numpy array or list) or display a warning. Just saying.\r\n\r\nThanks,\r\n"""
3848,48619440,fabianp,fabianp,2014-11-13 10:19:00,2015-01-06 10:32:07,2015-01-06 10:32:07,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3848,b'BUG: cross_val_score ignores scoring when estimator is a GridSearchCV object',"b""When I call cross_val_score with a GridSearchCV object (you can see this as nested cross-validation) the scoring value is systematically ignored. Take for example the following code:\r\n\r\n    X = np.random.randn(100, 10)\r\n    w = np.random.randn(10)\r\n    y = X.dot(w)\r\n    alphas=np.logspace(-3, 3, 10)\r\n    clf = grid_search.GridSearchCV(linear_model.ElasticNet(), {'alpha' : np.logspace(-3, 3, 8)})\r\n\r\n    scores = cross_validation.cross_val_score(clf, X, y, scoring='mean_absolute_error', n_jobs=-1)\r\n    print(scores)\r\n\r\nsince the scoring function is 'mean_absolute_error', the values should be negative. However, what I get is [ 0.99999806  0.99999758  0.99999911], which corresponds to the scoring function r2 (the default).\r\n\r\nThis happens because cross_validation_score is not able to find the 'predict' method in the GridSearchCV object. That is, the test hasattr(clf, 'predict') in check_scoring fails. The interesting thing is that the GridSearchCV object _does_ define a predict method in BaseSearchCV, only that as a property and thus the hasattr method fails."""
3843,48209407,conarchllc,glouppe,2014-11-09 18:44:55,2014-12-29 14:40:28,2014-12-29 14:40:28,closed,,,4,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3843,b'sklearn.tree.DecisionTreeClassifier documentation of max_depth parameter',b'The description of the max_depth parameter states that this parameter is Ignored if max_samples_leaf is not None. What is max_samples_leaf? Should this be min_samples_leaf?'
3830,47923503,falconair,GaelVaroquaux,2014-11-06 04:53:45,2014-11-29 14:45:55,2014-11-29 14:05:17,closed,,,10,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3830,b'Fix broken PDF generation for the documentation',"b""The documentation for this project looks fantastic. I'm new to ML but I'd love to have a PDF copy of the docs which I can read along with text books on the subject.\r\n\r\nI just spend half a day trying to do 'make latexpdf' only to find out that pdf functionality hasn't worked in over a year.\r\n\r\nCan you please just provide a pdf copy of the manual and save lots of people lots of time and frustration. Even the sourceforge email lists are full of people asking for the pdf."""
3777,45928522,nmayorov,ogrisel,2014-10-15 22:37:19,2015-09-01 04:46:49,2015-03-03 21:20:20,closed,,0.16,24,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3777,"b'[MRG] DOC, FIX: Support Python 2 and 3 in gen_rst.py'","b'The encoding (if not specified) used by `open` in Python is platform dependent. On my Windows machine it is `cp1251`, so I had troubles building docs with the example gallery because of that (some examples contain non-ASCII characters.) \r\n\r\nI think setting it explicitly to `utf-8` is a good thing.'"
3760,45574616,Garrett-R,amueller,2014-10-12 09:43:28,2015-02-25 06:26:04,2015-02-24 22:29:35,closed,,0.16,36,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3760,b'[MRG + 1] Fix KNeighborsRegressor and RadiusNeighborsRegressor returning NaN predi...',"b""A  `KNeighborsRegressor` with distance weighting will predict `NaN` if the data to predict on happens to match a data point from the training set.  (This actually happened to me with some data from a Kaggle competition)\r\n\r\nFor example,\r\n\r\n    from sklearn import neighbors\r\n    import numpy as np\r\n    \r\n    X = np.array([[1.0],[2.0],[3.0]])\r\n    y = np.array([1.0, 2.0, 3.0])\r\n    \r\n    clf = neighbors.KNeighborsRegressor(n_neighbors=3, weights='distance')\r\n    \r\n    clf.fit(X,y)\r\n    print(clf.predict([[1.0]]))\r\n\r\nThe output is `NaN`.  This seems like undesirable behavior.  I would have expected `pred` to be `1.0`.  In addition, `RadiusNeighborsRegressor` suffers from this problem.  \r\n\r\nNote that the infinities are already handled properly by `KNeighborsClassifier` and `RadiusNeighborsClassifiers`.  For example,\r\n\r\n    from sklearn import neighbors\r\n    import numpy as np\r\n    \r\n    X = np.array([[1.0],[2.0],[3.0]])\r\n    y = np.array([1, 0, 1])\r\n    \r\n    clf = neighbors.KNeighborsClassifier(n_neighbors=3, weights='distance')\r\n    \r\n    clf.fit(X,y)\r\n    print(clf.predict([[1.0]]))\r\n\r\nThis outputs `1` as expected.\r\n\r\nThis pull request is my proposed solution.  Note that if there are two points with distance=0, it chooses one of them, which is consistent with the warning given in the [KNeighborsRegressor docs](http://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsRegressor.html).\r\n\r\nThis is my first pull request for any project, so let me know if I'm doing something wrong!"""
3747,45400668,ngoix,amueller,2014-10-09 18:29:35,2015-03-23 16:12:57,2015-03-23 16:12:57,closed,,0.16,41,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3747,b'[MRG  + 1] Fix preprocessing.scale for arrays with near zero ratio variance/max',"b'Fix #3722, prolongating #3725.'"
3741,45200962,shoyer,amueller,2014-10-08 04:01:05,2015-02-24 21:28:54,2015-02-24 21:28:54,closed,,0.16,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3741,b'Should GradientBoostingRegressor.staged_predict really modify the array it yields in-place?',"b'I recently made the extremely frustrating discovery that this method yields references to a single array, which is modified in-place as each element is yielded. For example,\r\n```\r\npredictions = np.array(list(est.staged_predict(x[:, np.newaxis])))\r\n```\r\nreturns a 2D arrays with all elements fixed along the first axis (even if the staged predictions are different).\r\n\r\nI understand why mutation may be desirable for performance reasons, but from the perspective of someone trying to use staged prediction to understand my model, I found this very surprising. I thought there was something wrong with how I was using gradient boosting.\r\n\r\nAt a minimum, this should be clearly documented, but if for some good reason this behavior cannot be changed, my preferred resolution would be to add a backwards compatible keyword argument such as `copy=False` or `inplace=True` so that this unintuitive behavior is clearly called out in the method signature. Thoughts?'"
3736,44888513,glouppe,glouppe,2014-10-04 17:10:24,2014-10-05 11:46:36,2014-10-04 17:46:15,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3736,b'Bug: cross-validating a GridSearchCV object',"b'While preparing the slides for the defense of my thesis, I encountered what seems to be quite a serious bug in GridSearchCV. I basically wanted to do model selection inside a cross-validation loop in order to have an unbiased estimate of the generalization error of my selected model, so I did something like:\r\n\r\n```\r\nprint -cross_val_score(GridSearchCV(estimator=RandomForestRegressor(n_estimators=50, n_jobs=-1), \r\n                                    param_grid={""max_features"": range(1, X.shape[1]+1)},\r\n                                    scoring=""mean_squared_error""),\r\n                       X, y, scoring=""mean_squared_error"", cv=3)\r\n>>> [ 0.20720674 -0.28844932 -0.30089467]\r\n```\r\n\r\nThe first estimate on the first fold is correct, the following are obviously wrong, they should be positive. \r\n'"
3728,44541450,cosmos2006,amueller,2014-10-01 08:54:05,2015-09-09 22:45:39,2015-09-09 22:45:39,closed,,0.16,9,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3728,b'installation for scikit-learn-0.15.2 failing',"b'while installation, I get the following error during \r\n\r\n> python setup.py build\r\n\r\nError is:\r\n\r\n```\r\n/usr/bin/gfortran -Wall -g -L/usr/local/lib -L/sw/lib build/temp.linux-x86_64-2.7/sklearn/cluster/_k_means.o -L/usr/lib64 -L/usr/lib64 -Lbuild/temp.linux-x86_64-2.7 -lcblas -lm -lpython2.7 -lgfortran -o build/lib.linux-x86_64-2.7/sklearn/cluster/_k_means.so\r\n/usr/lib/gcc/x86_64-redhat-linux/4.7.2/../../../../lib64/crt1.o: In function `_start\':\r\n(.text+0x20): undefined reference to `main\'\r\ncollect2: error: ld returned 1 exit status\r\n/usr/lib/gcc/x86_64-redhat-linux/4.7.2/../../../../lib64/crt1.o: In function `_start\':\r\n(.text+0x20): undefined reference to `main\'\r\ncollect2: error: ld returned 1 exit status\r\nerror: Command ""/usr/bin/gfortran -Wall -g -L/usr/local/lib -L/sw/lib build/temp.linux-x86_64-2.7/sklearn/cluster/_k_means.o -L/usr/lib64 -L/usr/lib64 -Lbuild/temp.linux-x86_64-2.7 -lcblas -lm -lpython2.7 -lgfortran -o build/lib.linux-x86_64-2.7/sklearn/cluster/_k_means.so"" failed with exit status 1\r\n```\r\n\r\nMay I know what is the problem and how to sort it out?\r\n\r\nThanks\r\nMradul'"
3725,44487910,cocuh,ogrisel,2014-09-30 19:15:04,2015-03-03 09:10:34,2015-03-03 09:10:34,closed,,0.16,15,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3725,b'[MRG]Fix nearly zero division',"b'fixes #3722\r\n- separate ""reset zero to 1.0"" function from `_mean_and_std()`\r\n- add  `_replace_nearly_value()` function\r\n    - with` isclose()`, ""nealy"" zero values also replaced to 1.0\r\n- add floating point error testcase'"
3722,44379015,maximsch2,amueller,2014-09-30 01:38:51,2015-03-23 16:13:23,2015-03-23 16:13:23,closed,,,8,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3722,b'preprocessing.scale provides consistent results on arrays with zero variance',"b'I\'m using Python 2.7, NumPy 1.8.2 and scikit-learn 0.14.1 on x64 linux (all installed through Anaconda) and getting very inconsistent results for preprocessing.scale function:\r\n\r\n> print preprocessing.scale(np.zeros(6) + np.log(1e-5))\r\n[ 0.  0.  0.  0.  0.  0.]\r\n\r\n> print preprocessing.scale(np.zeros(8) + np.log(1e-5))\r\n[-1. -1. -1. -1. -1. -1. -1. -1.]\r\n\r\n> print preprocessing.scale(np.zeros(22) + np.log(1e-5))\r\n[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\r\n\r\nI would guess this is not is supposed to be happening. Quick investigation, points to the fact that np.std() of second and third array is not exactly zero, but very close to machine zero. sklearn still uses it to divide data (it doesn\'t go into the ""std == 0.0"" case in the code).\r\n\r\n\r\nNote that in the case of the array, this can be easily fixed by passing with_std=False, but when that happens for one of the many features in 2D matrix this is not an option.'"
3721,44350823,larsmans,amueller,2014-09-29 20:19:36,2015-01-23 21:37:58,2015-01-23 21:37:58,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3721,b'QDA crash due to NaNs (from SVD?)',"b""[Reported on SO](http://stackoverflow.com/q/26107930/166749):\r\n\r\n```\r\ndef test_cv():\r\n    from sklearn import cross_validation\r\n    X = np.random.randn(6, 4)\r\n    y = [0, 0, 0, 1, 1, 1]\r\n    cv = cross_validation.KFold(len(y), n_folds=3, shuffle=True)\r\n    clf = qda.QDA()\r\n    scores = cross_validation.cross_val_score(clf, X, y, cv=cv)\r\n```\r\n\r\ncan cause `QDA` to crash. The problems is in the `scalings_` attribute, which can take the form\r\n\r\n```\r\narray([array([ nan]),\r\n       array([  4.51776471e+00,   2.98684272e-01,   5.55011150e-32])], dtype=object)\r\n```\r\n\r\nThe bug doesn't show up every time, even when I fix the random seed, so it's probably randomness/instability inside `np.linalg.svd`."""
3714,44219637,fhyme,amueller,2014-09-28 07:12:21,2015-03-23 22:26:26,2015-03-23 22:26:26,closed,,,2,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3714,"b""'make latexpdf' Error""","b'when i run the command \'make latexpdf \' in the terminal of ubuntu 13.10, there are something wrong. I don\'t know how to deal it . Please give me some help .Thx ~\r\n\r\n```\r\n# make latexpdf\r\nsphinx-build -b latex -d _build/doctrees   . _build/latex\r\nMaking output directory...\r\nRunning Sphinx v1.2.3\r\n\r\nException occurred:\r\n  File ""/usr/lib/python2.7/subprocess.py"", line 1326, in _execute_child\r\n    raise child_exception\r\nOSError: [Errno 2] No such file or directory\r\nThe full traceback has been saved in /tmp/sphinx-err-uw5ejT.log, if you want to report the issue to the developers.\r\nPlease also report this if it was a user error, so that a better error message can be provided next time.\r\nA bug report can be filed in the tracker at <https://bitbucket.org/birkenfeld/sphinx/issues/>. Thanks!\r\nmake: *** [latexpdf] \xb4\xed\xce\xf3 1\r\n```'"
3709,44074869,pprett,larsmans,2014-09-26 15:39:31,2014-09-30 16:34:43,2014-09-30 06:41:22,closed,,,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3709,"b""Ridge solver='svd' is broken on sparse inputs""","b""Here is a test case that reproduces the problem:\r\n\r\nnumpy: 1.8.2\r\nscipy: 0.9.0\r\nsklearn: current master (556903597a0e64f94830b3f0153531ee7b7adeb3)\r\n\r\n<pre>\r\n>>> from scipy import sparse as sp\r\n>>> import numpy as np\r\n>>> from sklearn.linear_model import Ridge\r\n\r\n>>> X = sp.csc_matrix(np.random.rand(100, 10))\r\n>>> y = np.random.rand(100)\r\n>>> est = Ridge(solver='svd')\r\n>>> est.fit(X, y)\r\nLinAlgError: 0-dimensional array given. Array must be at least two-dimensional\r\n</pre>"""
3700,43848795,parthaghosh,larsmans,2014-09-25 03:27:27,2014-09-29 20:52:32,2014-09-29 20:52:32,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3700,b'OneVsRestClassifier predict_proba method does not return probability values for all the classes',"b'Sklearn\'s classifier\'s predict_proba method does not return the probabilities for all the classes. I am running Python 3.3.5 and scikit-learn v 0.15.2.   Consider the following code: \r\n\r\n```python\r\nimport numpy as np \r\nfrom sklearn import svm \r\nfrom sklearn.multiclass import OneVsRestClassifier\r\nfeat = [[1,2,3], [4,5,6]]\r\nlabels = [\'One\', \'Two\']\r\nnp_feat = np.array(feat)\r\nsvm_clf  = svm.SVC(probability=True)\r\novr = OneVsRestClassifier(svm_clf)\r\novr.fit(np_feat,labels)\r\novr.predict([[1,3,2]])\r\novr.predict_proba([[1,3,2]])\r\n```\r\n\r\nNow, run the following command sequence under Python Interpreter console.\r\n\r\n    >>> ovr.classes_\r\n\r\nYou get as output: \r\n\r\n    array([\'One\', \'Two\'], dtype=\'< U3\') \r\n\r\nNext execute:\r\n\r\n    >>>ovr.predict ([1,3,2]) \r\n\r\nYou get as output:\r\n\r\n    array([\'One\'], dtype=\'\'< U3\') \r\n\r\nNext execute: \r\n\r\n    >>> ovr.predict_proba([[1,3,2]])\r\n\r\nYou ONLY get as output: \r\n\r\n    array([[ 1.]])\r\n\r\nClearly the probability value for the class ""Two"" is missing from the output. This causes the problem for downstream code to know which classes are present in the output. We typically assume probability values [even if some are ""0""] for all the classes are present. \r\n\r\nWhat is the way out?'"
3694,43673285,HapeMask,larsmans,2014-09-23 19:35:33,2014-09-25 17:38:30,2014-09-25 17:38:30,closed,,,0,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3694,"b""sklearn.utils.shuffle can't shuffle arrays with ndim > 2""","b'```sklearn.utils.shuffle``` can no longer be used to shuffle arrays with ndim > 2, since it (more precisely, it and ```resample```, which it calls) provides no way to pass the ""allow_nd"" flag to ```check_arrays``` (sklearn/utils/\\_\\_init\\_\\_.py:245).'"
3693,43658341,drudd,drudd,2014-09-23 17:25:53,2015-04-22 15:11:19,2015-04-20 21:38:17,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3693,b'test_transformers numerically unstable tests fail on 64-bit python',"b'This bug has been reported in other cases (#3255, #1826, #1838) but reported as limited to 32-bit python (see specifically #3255 and justification for closure). \r\n\r\nScientificLinux 6.5 (64-bit, gcc 4.4.7)\r\npython 2.7.8\r\nnumpy 1.8.2 (built against MKL 10.3)\r\nscipy 0.14.0\r\nscikit-learn 0.15.2\r\n\r\n```\r\nFAIL: sklearn.tests.test_common.test_transformers(\'CCA\', <class \'sklearn.cross_decomposition.cca_.CCA\'>, array([[ 2.51189522,  2.6430893 ,  2.54847718],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/sklearn/tests/test_common.py"", line 287, in check_transformer\r\n    % Transformer)\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/numpy/testing/utils.py"", line 811, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/numpy/testing/utils.py"", line 644, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\nconsecutive fit_transform outcomes not consistent in <class \'sklearn.cross_decomposition.cca_.CCA\'>\r\n(mismatch 88.3333333333%)\r\n x: array([[  8.87962900e-01,   1.60346761e-15],\r\n       [ -9.23212796e-01,  -3.35912310e-16],\r\n       [ -9.23212796e-01,  -3.35912310e-16],...\r\n y: array([[ 0.8484875 ,  0.53649757],\r\n       [-0.8994568 , -0.5195078 ],\r\n       [-0.8994568 , -0.5195078 ],...\r\n\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_transformers(\'KernelPCA\', <class \'sklearn.decomposition.kernel_pca.KernelPCA\'>, array([[ 2.51189522,  2.6430893 ,  2.54847718],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/sklearn/tests/test_common.py"", line 296, in check_transformer\r\n    % Transformer)\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/numpy/testing/utils.py"", line 811, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/software/python-2.7-2014q3-el6-x86_64/lib/python2.7/site-packages/numpy/testing/utils.py"", line 599, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\nconsecutive fit_transform outcomes not consistent in <class \'sklearn.decomposition.kernel_pca.KernelPCA\'>\r\n(shapes (30, 17), (30, 16) mismatch)\r\n x: array([[  1.87664949e+00,   8.57398986e-02,   4.20312700e-02,\r\n          3.33090792e-08,   0.00000000e+00,   0.00000000e+00,\r\n          0.00000000e+00,   0.00000000e+00,   2.59520075e-08,...\r\n y: array([[  1.87664949e+00,   8.57398986e-02,   4.20312700e-02,\r\n          3.15985371e-08,   0.00000000e+00,   0.00000000e+00,\r\n          0.00000000e+00,   2.66040637e-08,   0.00000000e+00,...\r\n\r\n----------------------------------------------------------------------\r\n```'"
3691,43609747,ogrisel,ogrisel,2014-09-23 09:49:48,2014-10-02 09:40:42,2014-10-02 09:40:42,closed,,0.16,3,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3691,b'test_libsvm_iris fails on windows',"b'For instance in this build https://ci.appveyor.com/project/sklearn-ci/scikit-learn/build/1.0.151/job/xnjw15iyoun3r00w\r\n\r\n```\r\n======================================================================\r\nFAIL: Check consistency on dataset iris.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n File ""C:\\Python27_32\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n self.test(*self.arg)\r\n File ""C:\\Python27_32\\lib\\site-packages\\sklearn\\svm\\tests\\test_svm.py"", line 74, in test_libsvm_iris\r\n assert_greater(np.mean(pred == iris.target), .95)\r\nAssertionError: 0.94666666666666666 not greater than 0.95\r\n```\r\n\r\nThis is strange as this test used to pass on the same platform and the first commit that triggered the failure seems to be unrelated to libsvm:\r\n\r\nhttps://ci.appveyor.com/project/sklearn-ci/scikit-learn/history\r\nhttps://github.com/scikit-learn/scikit-learn/commit/d34e928059ca320f50d8c1ff7c372d5bfe514555'"
3673,43024744,arjoly,larsmans,2014-09-17 15:20:29,2014-10-13 07:51:25,2014-10-12 15:24:57,closed,,,6,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3673,"b'SVR documentation have ""classification"" parameters / attributes'","b'As far as I know, SVR is a regression model. Nevertheless, many parameters are about classification such as probability parameter, intercept_ attribute.\r\n\r\nThis is really confusing, especially for new user.'"
3670,43010266,nathanathan,larsmans,2014-09-17 13:25:31,2015-01-27 12:32:34,2015-01-27 12:32:34,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3670,b'Altering precision_recall_fscore_support docstring',"b'If the labels are unsorted and precision_recall_fscore_support is used with multilabel classifications it will raise an exception. E.g.:\r\n```\r\nsklearn.metrics.precision_recall_fscore_support(\r\n    [(1,2)],\r\n    [(3,4)],\r\n    labels=[4, 1, 2, 3]\r\n)\r\n```\r\nraises\r\n```\r\n/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\r\n    945     elif label_order is not None:\r\n    946         indices = np.searchsorted(labels, label_order)\r\n--> 947         precision = precision[indices]\r\n    948         recall = recall[indices]\r\n    949         f_score = f_score[indices]\r\n\r\nIndexError: index 4 is out of bounds for size 4\r\n```\r\nAlso, AFAICT the labels do not need to be integers.'"
3666,42859247,pprett,pprett,2014-09-16 09:05:44,2014-10-10 16:28:14,2014-10-10 16:28:14,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3666,b'IsotonicRegression is not pickleable',"b""<pre>\r\n>>> import pickle\r\n>>> from sklearn import isotonic\r\n>>> est = isotonic.IsotonicRegression()\r\n>>> X, y = np.random.rand(100,), np.random.rand(100)\r\n>>> est.fit(X, y)\r\n>>> pickle.dumps(est)\r\nTypeError: can't pickle instancemethod objects\r\n</pre>"""
3650,42324491,martin-hunt,glouppe,2014-09-09 16:18:12,2015-10-19 17:51:04,2015-10-19 17:51:04,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3650,b'MLE for theta in GaussianProcess unreliable',"b'The maximum likelihood extimate for theta sometimes produces results that look very unreasonable.\r\nEven trying to fit a very simple, smooth curve it fails sometimes.   For example,\r\nx* sin(x/divisor) where x is in [0,10000], changing the divisor from 636.06767 to 636.06768 changes the MLE for theta from 8 to 632.\r\n\r\nHere is the code to reproduce this\r\n\r\nimport numpy as np\r\nimport sklearn\r\nfrom sklearn.gaussian_process import GaussianProcess\r\n\r\ndef compute_fit(divisor):\r\n\r\n    x = np.linspace(0, 10000, 11)\r\n    y = x * np.sin(x/divisor)\r\n    x = np.atleast_2d(x).T\r\n\r\n    gp = GaussianProcess(theta0=2,\r\n                         thetaL=.0001,\r\n                         thetaU=1000)\r\n    gp.fit(x, y)\r\n    theta = gp.theta_[0][0]\r\n    return theta, gp.reduced_likelihood_function_value_\r\n\r\nprint ""sklearn version:"", sklearn.__version__\r\nfor n in [500, 636, 636.06767, 636.06768, 637, 1000]:\r\n    theta, like = compute_fit(n)\r\n    print \'%s:\\ttheta=%s, likelihood=%s\' % (n, theta, like)\r\n\r\n> \r\n> python prob.py \r\nsklearn version: 0.15.2\r\n500:\ttheta=632.455532034, likelihood=-1.0\r\n636:\ttheta=632.455532034, likelihood=-1.0\r\n636.06767:\ttheta=632.455532034, likelihood=-1.0\r\n636.06768:\ttheta=8.47584723676, likelihood=-0.983409372837\r\n637:\ttheta=8.18740741469, likelihood=-0.976101646893\r\n1000:\ttheta=0.873105480722, likelihood=-0.0844448086551\r\n\r\n\r\n'"
3648,42185133,sergiopasra,amueller,2014-09-08 11:05:38,2015-01-23 09:40:14,2015-01-22 23:03:03,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3648,b'Error in test sklearn.linear_model.tests.test_base  in 0.15.2',"b'I have found an error when running the tests of 0.15.2 \r\n\r\nIt happens both with python 3.4 and python 2.7. The problematic test is in `sklearn.linear_model.tests.test_base`\r\n\r\nI have the following packages:\r\nnumpy 1.9.0rc1\r\nscipy 0.14.0\r\n\r\nIn testing in a Fedora system, so numpy and scipy are both precompiled RPMs. sklearn compiled from the tarball from PyPI.\r\n\r\n    $ nosetests-v2.7 sklearn.linear_model.tests.test_base\r\n    ../usr/lib64/python2.7/site-packages/scipy/sparse/linalg/isolve/lsqr.py:435: RuntimeWarning: invalid value encountered in double_scalars\r\n      test2 = arnorm / (anorm * rnorm)\r\n    E.......\r\n    ======================================================================\r\n    ERROR: Test that linear regression also works with sparse data\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/usr/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n        self.test(*self.arg)\r\n      File ""/home/xxx/.local/lib/python2.7/site-packages/sklearn/linear_model/tests/test_base.py"", line 78, in test_linear_regression_sparse\r\n        ols.fit(X, y.ravel())\r\n      File ""/home/xxx/.local/lib/python2.7/site-packages/sklearn/linear_model/base.py"", line 359, in fit\r\n        out = lsqr(X, y)\r\n      File ""/usr/lib64/python2.7/site-packages/scipy/sparse/linalg/isolve/lsqr.py"", line 436, in lsqr\r\n        test3 = 1 / acond\r\n    ZeroDivisionError: float division by zero\r\n\r\n    ----------------------------------------------------------------------\r\n    Ran 10 tests in 0.052s\r\n\r\n    FAILED (errors=1)'"
3641,42077946,grodrigues3,amueller,2014-09-05 18:54:31,2015-09-09 22:46:18,2015-09-09 22:46:18,closed,,,7,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3641,b'Sparse Matrix Error for Adaboot',"b'The docs say that ""Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR."" for the fit function, but it appears that a type error is given when a lil_matrix is passed.'"
3616,41593611,grechkay,amueller,2014-08-31 19:13:51,2015-09-09 22:47:20,2015-09-09 22:47:20,closed,,,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3616,"b""cross_val_score can't handle type 'float' when a scoring parameter is given""","b'I was working with pandas and sklearn modules, and I tried using the following:\r\n\r\n```\r\nskcv.cross_val_score(clf_1_v3, feature_vec, result_vec.ix[:, 0] , cv=5, scoring=\'mean_squared_error\')\r\n```\r\n\r\nWith feature_vec and result_vec both being pandas DataFrames; I got the following error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2820, in run_code\r\n    exec code_obj in self.user_global_ns, self.user_ns\r\n  File ""<ipython-input-48-4e1db779e5e6>"", line 1, in <module>\r\n    skcv.cross_val_score(clf_1_v3,aa ,bb , cv=5, scoring=\'mean_squared_error\')\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/cross_validation.py"", line 1151, in cross_val_score\r\n    for train, test in cv)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 644, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 391, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 129, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/cross_validation.py"", line 1240, in _fit_and_score\r\n    test_score = _score(estimator, X_test, y_test, scorer)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/cross_validation.py"", line 1296, in _score\r\n    score = scorer(estimator, X_test, y_test)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/metrics/scorer.py"", line 80, in __call__\r\n    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\r\n  File ""/home/grechkay/.local/lib/python2.7/site-packages/sklearn/metrics/metrics.py"", line 2221, in mean_squared_error\r\n    weights=sample_weight)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py"", line 496, in average\r\n    scl = avg.dtype.type(a.size/avg.size)\r\nAttributeError: \'float\' object has no attribute \'dtype\'\r\n```\r\n\r\nAfter quite a few hours of debugging, I tried to convert the DataFrame to ndarray, but I got the same error, and then I looked at the type of individual datapoints and compared them to types that are known to function properly. \r\n\r\nTurns out that the type of my data was \'float\', while the type of data known to function properly was \'numpy.float64\'. So then I changed my command to: \r\n\r\n```\r\nskcv.cross_val_score(clf_1_v3, feature_vec.astype(\'float64\'), result_vec.ix[:, 0].astype(\'float64\'), cv=5, scoring=\'mean_squared_error\')\r\n```\r\n\r\nAnd that fixed the problem.\r\n\r\nIt seems that the cross_val_score should detect and fix this automatically.'"
3602,41458885,gatapia,amueller,2014-08-29 00:01:06,2015-01-23 21:35:27,2015-01-23 21:35:27,closed,,0.16,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3602,"b""OneHotEncoder dtype being ignored when n_values='auto'""","b'Easiest to explain in code:\r\n\r\n    na = preprocessing.OneHotEncoder(dtype=np.float16).\\\r\n      fit_transform(np.array([[1, 2, 3]]))\r\n    self.assertEqual(np.float16, na.astype(np.float16).dtype)\r\n    self.assertEqual(np.float16, na.dtype)\r\n    # Fails - na.dtype is float32\r\n'"
3597,41390209,ogrisel,GaelVaroquaux,2014-08-28 13:33:52,2014-08-28 14:54:31,2014-08-28 14:54:31,closed,,0.15.2,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3597,b'scikit-learn should not have a strong dependency on nose',"b'On a new virtualenv without nose installed I get with scikit-learn 0.15.1:\r\n\r\n```\r\n$ python -c \'from sklearn.svm import SVC\'\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/svm/__init__.py"", line 13, in <module>\r\n    from .classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/svm/classes.py"", line 1, in <module>\r\n    from .base import BaseLibLinear, BaseSVC, BaseLibSVM\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/svm/base.py"", line 9, in <module>\r\n    from . import libsvm_sparse\r\n  File ""libsvm_sparse.pyx"", line 5, in init sklearn.svm.libsvm_sparse (sklearn/svm/libsvm_sparse.c:6773)\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/utils/__init__.py"", line 11, in <module>\r\n    from .validation import (as_float_array, check_arrays, safe_asarray,\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 17, in <module>\r\n    from .fixes import safe_copy\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/utils/fixes.py"", line 18, in <module>\r\n    from .testing import ignore_warnings\r\n  File ""/Users/ogrisel/venvs/py27-test/lib/python2.7/site-packages/sklearn/utils/testing.py"", line 36, in <module>\r\n    from nose.tools import assert_equal\r\nImportError: No module named nose.tools\r\n```'"
3596,41301077,ogrisel,amueller,2014-08-27 16:09:49,2015-10-12 22:25:51,2015-10-12 22:25:51,closed,,0.17,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3596,b'Python 2 / 3 incompatibility when fetching joblib compressed datasets',"b'For instance when running a Python 2 script that loads the olivetti dataset when it has already been loaded with Python 3 in the past:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-d64d4cc01dcf> in <module>()\r\n      1 from sklearn import datasets\r\n----> 2 lfw_people = datasets.fetch_olivetti_faces()\r\n      3 lfw_people.data.shape\r\n\r\n/Users/ogrisel/venvs/py27/lib/python2.7/site-packages/sklearn/datasets/olivetti_faces.pyc in fetch_olivetti_faces(data_home, shuffle, random_state, download_if_missing)\r\n    116         del mfile\r\n    117     else:\r\n--> 118         faces = joblib.load(join(data_home, TARGET_FILENAME))\r\n    119     # We want floating point data, but float32 is enough (there is only\r\n    120     # one byte of precision in the original uint8s anyway)\r\n\r\n/Users/ogrisel/code/joblib/joblib/numpy_pickle.pyc in load(filename, mmap_mode)\r\n    417                               \'ignoring mmap_mode ""%(mmap_mode)s"" flag passed\'\r\n    418                               % locals(), Warning, stacklevel=2)\r\n--> 419             unpickler = ZipNumpyUnpickler(filename, file_handle=file_handle)\r\n    420         else:\r\n    421             unpickler = NumpyUnpickler(filename, file_handle=file_handle,\r\n\r\n/Users/ogrisel/code/joblib/joblib/numpy_pickle.pyc in __init__(self, filename, file_handle)\r\n    306         NumpyUnpickler.__init__(self, filename,\r\n    307                                 file_handle,\r\n--> 308                                 mmap_mode=None)\r\n    309\r\n    310     def _open_pickle(self, file_handle):\r\n\r\n/Users/ogrisel/code/joblib/joblib/numpy_pickle.pyc in __init__(self, filename, file_handle, mmap_mode)\r\n    264         self._dirname = os.path.dirname(filename)\r\n    265         self.mmap_mode = mmap_mode\r\n--> 266         self.file_handle = self._open_pickle(file_handle)\r\n    267         Unpickler.__init__(self, self.file_handle)\r\n    268         try:\r\n\r\n/Users/ogrisel/code/joblib/joblib/numpy_pickle.pyc in _open_pickle(self, file_handle)\r\n    309\r\n    310     def _open_pickle(self, file_handle):\r\n--> 311         return BytesIO(read_zfile(file_handle))\r\n    312\r\n    313\r\n\r\n/Users/ogrisel/code/joblib/joblib/numpy_pickle.pyc in read_zfile(file_handle)\r\n     63     length = file_handle.read(len(_ZFILE_PREFIX) + _MAX_LEN)\r\n     64     length = length[len(_ZFILE_PREFIX):]\r\n---> 65     length = int(length, 16)\r\n     66     # We use the known length of the data to tell Zlib the size of the\r\n     67     # buffer to allocate.\r\n\r\nValueError: invalid literal for int() with base 16: \'0x1900a2           x\'\r\n```\r\n\r\nThis should be fixed upstream in joblib but it\'s good to track the issue here as well for discoverability.'"
3594,41205954,jimrybarski,jnothman,2014-08-26 18:36:21,2015-12-09 00:28:22,2015-08-24 09:04:48,closed,,,16,Bug;Easy;Need Contributor,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3594,b'Face recognition example is broken',"b'When executed without alteration, the example code [here](http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html) throws an exception. I have confirmed that the images downloaded successfully. Using Python 2.7.6, sklearn 0.15.1, numpy 1.8.1, scipy 0.13.3\r\n\r\n    $ python face_recognition.py\r\n    \r\n    ===================================================\r\n    Faces recognition example using eigenfaces and SVMs\r\n    ===================================================\r\n    \r\n    The dataset used in this example is a preprocessed excerpt of the\r\n    ""Labeled Faces in the Wild"", aka LFW_:\r\n    \r\n      http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\r\n    \r\n    .. _LFW: http://vis-www.cs.umass.edu/lfw/\r\n    \r\n    Expected results for the top 5 most represented people in the dataset::\r\n    \r\n                         precision    recall  f1-score   support\r\n    \r\n      Gerhard_Schroeder       0.91      0.75      0.82        28\r\n        Donald_Rumsfeld       0.84      0.82      0.83        33\r\n             Tony_Blair       0.65      0.82      0.73        34\r\n           Colin_Powell       0.78      0.88      0.83        58\r\n          George_W_Bush       0.93      0.86      0.90       129\r\n    \r\n            avg / total       0.86      0.84      0.85       282\r\n    \r\n    \r\n    \r\n    \r\n    2014-08-26 13:30:49,451 Loading LFW people faces from /home/jim/scikit_learn_data/lfw_home\r\n    2014-08-26 13:30:49,454 Loading face #00001 / 01140\r\n    Traceback (most recent call last):\r\n      File ""eigenface.py"", line 52, in <module>\r\n        lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\r\n      File ""/usr/local/lib/python2.7/dist-packages/sklearn/datasets/lfw.py"", line 277, in fetch_lfw_people\r\n        min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)\r\n      File ""/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/memory.py"", line 481, in __call__\r\n        return self._cached_call(args, kwargs)[0]\r\n      File ""/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/memory.py"", line 428, in _cached_call\r\n    out, metadata = self.call(*args, **kwargs)\r\n      File ""/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/memory.py"", line 673, in call\r\n        output = self.func(*args, **kwargs)\r\n      File ""/usr/local/lib/python2.7/dist-packages/sklearn/datasets/lfw.py"", line 202, in _fetch_lfw_people\r\n        faces = _load_imgs(file_paths, slice_, color, resize)\r\n      File ""/usr/local/lib/python2.7/dist-packages/sklearn/datasets/lfw.py"", line 156, in _load_imgs\r\n        face = np.asarray(imread(file_path)[slice_], dtype=np.float32)\r\n    IndexError: 0-d arrays can only use a single () or a list of newaxes (and a single ...) as an index'"
3540,39722041,kastnerkyle,ogrisel,2014-08-07 12:53:17,2015-03-02 14:51:28,2015-03-02 14:51:28,closed,,,2,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3540,b'OrthogonalMatchingPursuitCV errors in test_regressors',"b'The errors in OrthogonalMatchingPursuitCV on Travis have returned in ```test_regressors```. I think the new ""filtering"" way to do the skip check must be lacking something... cc @amueller @ogrisel '"
3526,39370079,gchers,larsmans,2014-08-03 13:20:40,2014-10-21 16:39:35,2014-10-21 16:39:35,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3526,b'Bug in sklearn.manifold tsne',"b""It looks to me there's an issue when using some distance different from 'euclidean' in tsne implementation.\r\nExample:\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\nx=np.random.randn(10,10)\r\nt=TSNE(metric='chebyshev')\r\nz=t.fit_transform(x)\r\nProduces:\r\nTypeError: pdist() got an unexpected keyword argument 'squared'\r\n```\r\n\r\nIn fact at lines 438-439 of sklearn/manifold/t_sne.py function _fit() I read:\r\ndistances = pairwise_distances(X, metric=self.metric, squared=True)\r\nwhich means that it always provides 'squared' argument to pairwise_distances(). Now, not all the distances support this (see sklearn/metric/pairwise.py), which leads to an error.\r\n\r\nI report this as an issue because in t_sne.py from line 338 it says:\r\n\r\n> If metric is a string, it must be one of the options\r\n> allowed by scipy.spatial.distance.pdist for its metric parameter, or\r\n> a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\r\n\r\nI'd suggest to substitute line 438 with something like:\r\n\r\n```python\r\nif self.metric == 'euclidean':\r\n    distances = pairwise_distances(X, metric=self.metric, squared=True)\r\nelse:\r\n    distances = pairwise_distances(X, metric=self.metric)\r\n```\r\n\r\nbut I'm quite sure it's not as simple as that: I don't, for example, understand why the author wanted ```squared=True```.\r\n\r\nCheers"""
3513,39192636,arjoly,pprett,2014-07-31 13:11:50,2014-08-01 07:31:49,2014-07-31 21:49:08,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3513,b'GBRT failed if fitted twice with warm_start=True and constant n_estimators',"b""Here the script to reproduce. \r\n\r\n```python\r\nIn [1]: from sklearn.ensemble import GradientBoostingClassifier\r\n\r\nIn [2]: clf = GradientBoostingClassifier(warm_start=True)\r\n\r\nIn [3]: clf.fit([[0, 1], [2, 3]], [0, 1])\r\nOut[3]: \r\nGradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\r\n              max_depth=3, max_features=None, max_leaf_nodes=None,\r\n              min_samples_leaf=1, min_samples_split=2,\r\n              min_weight_fraction_leaf=0.0, n_estimators=100,\r\n              random_state=None, subsample=1.0, verbose=0, warm_start=True)\r\n\r\nIn [4]: clf.fit([[0, 1], [2, 3]], [0, 1])\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-4-03e3eff789e2> in <module>()\r\n----> 1 clf.fit([[0, 1], [2, 3]], [0, 1])\r\n\r\n/Users/ajoly/git/scikit-learn/sklearn/ensemble/gradient_boosting.pyc in fit(self, X, y, monitor)\r\n   1144         self.classes_, y = np.unique(y, return_inverse=True)\r\n   1145         self.n_classes_ = len(self.classes_)\r\n-> 1146         return super(GradientBoostingClassifier, self).fit(X, y, monitor)\r\n   1147 \r\n   1148     def _score_to_proba(self, score):\r\n\r\n/Users/ajoly/git/scikit-learn/sklearn/ensemble/gradient_boosting.pyc in fit(self, X, y, monitor)\r\n    783         # fit the boosting stages\r\n    784         n_stages = self._fit_stages(X, y, y_pred, random_state,\r\n--> 785                                     begin_at_stage, monitor)\r\n    786         # change shape of arrays after fit (early-stopping or additional ests)\r\n    787         if n_stages != self.estimators_.shape[0]:\r\n\r\n/Users/ajoly/git/scikit-learn/sklearn/ensemble/gradient_boosting.pyc in _fit_stages(self, X, y, y_pred, random_state, begin_at_stage, monitor)\r\n    855                 if early_stopping:\r\n    856                     break\r\n--> 857         return i + 1\r\n    858 \r\n    859     def _make_estimator(self, append=True):\r\n\r\nUnboundLocalError: local variable 'i' referenced before assignment\r\n\r\n```\r\n\r\nI would have expected a warning or nothing as in the random forest module. (ping @pprett) """
3507,39077249,amueller,arjoly,2014-07-30 10:00:03,2014-07-30 10:43:49,2014-07-30 10:43:49,closed,,,1,API;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3507,b'Inconsistent mulit-label predict_proba',"b'The output shape for multi-label predict_proba is inconsistent :-(\r\nThe OneVsRest classifier produces (n_samples, n_classes) (like multi-class)\r\nwhere for example RandomForestClassifier produces a list of probabilities per label, which are each (n_samples, 2) or (n_samples, 1).\r\n\r\nNot sure how to do a deprecation for that.\r\nBut we should do a common test for whether a classifier supports multi-label and what the output shapes are.'"
3503,39020484,ogrisel,ogrisel,2014-07-29 18:35:27,2014-11-05 15:59:41,2014-11-05 15:59:41,closed,,,4,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3503,b'Unstable CCA test',"b'The following failure was observed under Windows 32 bit with numpy 1.8.1, scipy 0.14.1 and MKL:\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_regressors(\'CCA\', <class \'sklearn.cross_decomposition.cca_.CCA\'>)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n File ""C:\\Python27_32\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n self.test(*self.arg)\r\n File ""C:\\Python27_32\\lib\\site-packages\\sklearn\\utils\\estimator_checks.py"", line 645, in check_regressors_train\r\n regressor.fit(X, y_)\r\n File ""C:\\Python27_32\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py"", line 334, in fit\r\n linalg.inv(np.dot(self.y_loadings_.T, self.y_weights_)))\r\n File ""C:\\Python27_32\\lib\\site-packages\\scipy\\linalg\\basic.py"", line 383, in inv\r\n raise LinAlgError(""singular matrix"")\r\nLinAlgError: singular matrix\r\n ```'"
3485,38744793,roseperrone,ogrisel,2014-07-25 15:56:23,2014-08-01 21:09:23,2014-08-01 13:06:05,closed,,0.15.1,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3485,b'SGDClassifier with class_weight=auto fails on scikit-learn 0.15 but not 0.14',b'See the error and workaround here:\r\n\r\nhttp://stackoverflow.com/questions/24808821/sgdclassifier-with-class-weight-auto-fails-on-scikit-learn-0-15-but-not-0-14'
3475,38524121,kastnerkyle,kastnerkyle,2014-07-23 12:28:56,2014-07-23 15:25:33,2014-07-23 15:25:33,closed,,0.15.1,14,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3475,b'Docbuild is broken in a fresh repo',"b'Docbuild is broken in fresh repositories which have not done a build before...\r\n\r\n```\r\nException occurred:\r\n  File ""/volatile/accounts/kkastner/scikit-learn/doc/sphinxext/gen_rst.py"", line 637, in generate_dir_rst\r\n    with open(include_path, \'a\' if seen else \'w\') as ex_file:\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/volatile/accounts/kkastner/scikit-learn/doc/auto_examples/../modules/generated/sklearn.pipeline.Pipeline.examples\'\r\nThe full traceback has been saved in /tmp/sphinx-err-rbzxm3fc.log, if you want to report the issue to the developers.\r\n```\r\n\r\nTo fix this, I had to do:\r\n```\r\nmkdir doc/modules/generated\r\nmkdir doc/generated\r\n```\r\n\r\nIt should be a simple fix but I am unsure how/where it should go.'"
3471,38384221,GaelVaroquaux,ogrisel,2014-07-22 09:45:20,2014-07-25 11:51:22,2014-07-25 11:51:22,closed,,0.15.1,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3471,b'[MRG+1] Fix np1.9',b'Some numpy 1.9 stuff I had forgotten to push :(.\r\n\r\nSorry working while traveling has side effects.'
3470,38380939,ogrisel,ogrisel,2014-07-22 08:59:44,2014-07-22 11:36:04,2014-07-22 09:28:13,closed,,0.15.1,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3470,b'[MRG] better RandomizedPCA sparse deprecation',b'Better deprecation warning for the problem reported on #3469. \r\n\r\nThis fix should be backported in the 0.15.X branch for inclusion in 0.15.1.'
3469,38376120,kastnerkyle,kastnerkyle,2014-07-22 07:41:39,2014-07-22 09:18:34,2014-07-22 09:18:34,closed,,0.15.1,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3469,b'Randomized PCA .explained_variance_ratio_ sums to greater than one in sklearn 0.15.0',"b'As reported here:\r\n\r\nhttp://stackoverflow.com/questions/24875838/randomized-pca-explained-variance-ratio-sums-to-greater-than-one-in-sklearn-0\r\n\r\nTrying to get version info for BLAS, etc. as well'"
3462,38302933,ogrisel,ogrisel,2014-07-21 13:34:51,2014-07-29 14:27:27,2014-07-29 14:27:27,closed,,0.15.1,15,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3462,b'LabelBinarizer regression between 0.14.1 and 0.15.0',"b""In 0.14.1 we have the following behavior:\r\n\r\n```\r\n>>> lb = LabelBinarizer()\r\n>>> lb.fit_transform(['a', 'b', 'c'])\r\narray([[1, 0, 0],\r\n       [0, 1, 0],\r\n       [0, 0, 1]])\r\n>>> lb.transform(['a', 'd', 'e'])\r\narray([[1, 0, 0],\r\n       [0, 0, 0],\r\n       [0, 0, 0]])\r\n```\r\n\r\nIn 0.15.0 the call to `transform` with unseen labels raises a `ValueError`. If we to change to a new behavior we should at least raise a deprecation warning and keep the old behavior by default while implementing the new behavior with a flag.\r\n"""
3431,38181732,argriffing,larsmans,2014-07-18 14:37:20,2014-08-07 17:00:08,2014-08-07 17:00:08,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3431,b'numpy version comparison',"b""The new numpy development versions `1.10.0.dev-whatever` are uncovering version comparison bugs, so I thought I'd check `scikit-learn`.  The comparison in line\r\nhttps://github.com/scikit-learn/scikit-learn/blob/0.15.X/sklearn/externals/joblib/numpy_pickle.py#L110\r\nlooks wrong, and maybe there are others.\r\n"""
3389,37892855,vene,larsmans,2014-07-15 15:07:44,2015-01-28 11:35:48,2015-01-28 11:35:48,closed,,,1,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3389,"b'Random doctest failure in statistical_inference tutorial, model_selection'","b'This was triggered on Travis by a completely unrelated commit.\r\nIt seems that the tutorial gets a better-than-expected result on digits :)\r\n\r\nThe Travis configuration that failed is `DISTRIB=""conda"" PYTHON_VERSION=""2.6"" INSTALL_MKL=""false"" NUMPY_VERSION=""1.6.2"" SCIPY_VERSION=""0.11.0""`\r\n\r\n```\r\nI: Seeding RNGs with 565480497\r\n[...]\r\n======================================================================\r\nFAIL: Doctest: model_selection.rst\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/travis/anaconda/envs/testenv/lib/python2.6/doctest.py"", line 2163, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for model_selection.rst\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/doc/tutorial/statistical_inference/model_selection.rst"", line 0\r\n\r\n----------------------------------------------------------------------\r\nFile ""/home/travis/build/scikit-learn/scikit-learn/doc/tutorial/statistical_inference/model_selection.rst"", line 148, in model_selection.rst\r\nFailed example:\r\n    clf.best_score_                                  # doctest: +ELLIPSIS\r\nExpected:\r\n    0.924...\r\nGot:\r\n    0.93561368209255535\r\n----------------------------------------------------------------------\r\nFile ""/home/travis/build/scikit-learn/scikit-learn/doc/tutorial/statistical_inference/model_selection.rst"", line 150, in model_selection.rst\r\nFailed example:\r\n    clf.best_estimator_.gamma == 1e-6\r\nExpected:\r\n    True\r\nGot:\r\n    False\r\n```'"
3372,37743415,ogrisel,ogrisel,2014-07-13 15:29:19,2014-07-13 17:40:20,2014-07-13 17:40:20,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3372,b'Unstable test_common.check_regressors_train for RANSACRegressor',"b'Heisen failure under Python 2.7 32-bit for Windows:\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_regressors_train(\'RANSACRegressor\', <class \'sklearn.linear_model.ransac.RANSACRegr\r\nessor\'>, array([[-0.44836249, -0.47282444, -1.20608008, ..., -0.75500806,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\tests\\test_common.py"", line 850, in check_regressors_train\r\n    assert_raises(ValueError, regressor.fit, X, y[:-1])\r\n  File ""C:\\Python27\\lib\\unittest\\case.py"", line 473, in assertRaises\r\n    callableObj(*args, **kwargs)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\ransac.py"", line 261, in fit\r\n    y_subset = y[subset_idxs]\r\nIndexError: index 199 is out of bounds for axis 0 with size 199\r\n```'"
3371,37743266,ogrisel,ogrisel,2014-07-13 15:22:10,2014-07-13 16:37:02,2014-07-13 16:37:02,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3371,b'Failing test_common.check_classifiers_input_shapes for RidgeClassifierCV under Windows',"b'Failure seen under windows with Python 2.7 32-bit:\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_classifiers_input_shapes(\'RidgeClassifierCV\', <class \'sklearn.linear_model.ridge.RidgeClassifierCV\'>, array([[ -5.25060772e-02,   2.18907205e+00,  -1.45500381e+00,\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\tests\\test_common.py"", line 730, in check_classifiers_input_shapes\r\n    assert_equal(len(w), 1)\r\nAssertionError: 2 != 1\r\n\r\n```\r\n\r\nNumpy 1.8.1, scipy 0.14.0 with MKL.'"
3370,37731734,ogrisel,ogrisel,2014-07-13 00:39:32,2014-09-11 21:00:17,2014-09-11 20:59:07,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3370,b'Heisen failure in # of alphas returned by lars_path on Windows with 32-bit Python',"b'There seems to be a randomly duplicated alpha from time to time:\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.linear_model.tests.test_least_angle.test_lasso_lars_path_length\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n File ""C:\\Python34\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n self.test(*self.arg)\r\n File ""C:\\Python34\\lib\\site-packages\\sklearn\\linear_model\\tests\\test_least_angle.py"", line 298, in test_lasso_lars_path_length\r\n assert_array_almost_equal(lasso.alphas_[:3], lasso2.alphas_)\r\n File ""C:\\Python34\\lib\\site-packages\\numpy\\testing\\utils.py"", line 811, in assert_array_almost_equal\r\n header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n File ""C:\\Python34\\lib\\site-packages\\numpy\\testing\\utils.py"", line 599, in assert_array_compare\r\n raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 6 decimals\r\n\r\n(shapes (3,), (4,) mismatch)\r\n x: array([ 2.14804358, 2.01202713, 1.02466283])\r\n y: array([ 2.14804358, 2.01202713, 1.02466283, 1.02466283])\r\n```'"
3277,35700554,agramfort,larsmans,2014-06-13 19:34:10,2014-07-15 16:29:46,2014-06-15 12:27:45,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3277,b'[MRG+1]: FIX : allow nd X for cross_val_score (was working in 0.14)',b'cc @ogrisel\r\n\r\nwould be great to get this in the release\r\n\r\nbreaks cross val in mne-python'
3269,35492964,mblondel,larsmans,2014-06-11 15:05:43,2014-07-06 14:21:46,2014-07-06 14:21:46,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3269,b'bug in KernelDensity.sample',"b'KernelDensity\'s sample method returns a scalar when the number of samples to generate is n_samples=1 and n_features=1. The expected result is an array of shape (1, 1). The bug only affects kernel=""gaussian"". Here\'s a minimal example to reproduce the problem:\r\n```python\r\nimport numpy as np\r\nfrom sklearn.neighbors import KernelDensity\r\n\r\nN = 100\r\nnp.random.seed(1)\r\nX = np.concatenate((np.random.normal(0, 1, 0.3 * N),\r\n                    np.random.normal(5, 1, 0.7 * N)))[:, np.newaxis]\r\n\r\nkde = KernelDensity(kernel=""gaussian"", bandwidth=0.5).fit(X)\r\n\r\nx = kde.sample(1)\r\nprint x\r\nprint\r\n\r\nx = kde.sample(2)\r\nprint x\r\nprint x.shape\r\n```\r\n\r\nOn a related note, this [line](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/kde.py#L203) seems to be using an undocumented functionality of rng.normal. The [documentation](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html#numpy.random.normal) says that ""loc"" should be a scalar but here a 2d array is passed. What does rng.normal do in this case? I guess it generates each feature independently (i.e. diagonal covariance matrix) but a comment would be definitely helpful.\r\n\r\nCC @jakevdp '"
3266,35415440,cbmeyer,cbmeyer,2014-06-10 19:20:09,2015-03-09 16:49:14,2015-01-30 21:30:31,closed,,0.16,45,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3266,b'Many redundant prediction probabilities for test instances with sparse SVM',"b""I\xa1\xafm having an issue using the prediction probabilities for sparse SVM, where many of the predictions come out the same for my test instances.  These probabilities are produced during cross validation, and when I plot an ROC curve for the folds, the results look very strange, as there are a handful of clustered points on the graph.  Here is my cross validation code, I based it off of the samples on the scikit website:\r\n\r\n    skf = StratifiedKFold(y, n_folds=numfolds)\r\n\r\n    for train_index, test_index in skf:\r\n             #split the training and testing sets\r\n             X_train, X_test = X_scaled[train_index], X_scaled[test_index]\r\n             y_train, y_test = y[train_index], y[test_index]\r\n\r\n             #train on the subset for this fold\r\n             print 'Training on fold ' + str(fold)\r\n             classifier = svm.SVC(C=C_val, kernel='rbf', gamma=gamma_val, probability=True)\r\n             probas_ = classifier.fit(X_train, y_train).predict_proba(X_test)\r\n\r\n             #Compute ROC curve and area the curve\r\n             fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\r\n             mean_tpr += interp(mean_fpr, fpr, tpr)\r\n             mean_tpr[0] = 0.0\r\n             roc_auc = auc(fpr, tpr)\r\n\r\nI\xa1\xafm just trying to figure out if there\xa1\xafs something I\xa1\xafm obviously missing here, since I used this same training set and SVM parameters with libsvm and got much better results.  When I used libsvm and printed out the distances from the hyperplane for the CV test instances and then plotted the ROC, it came out much more like I expected, and a much better AUC.  Since the decision_function() method is not supported for sparse matrices, I cannot recreate this functionality in scikit, and therefore have to rely on the prediction probabilities.\r\n\r\nThere are 20k instances total, 10k positive and 10k negative, and I'm using 5-fold cross-validation.  In the cross-validation results, there are several prediction values for which there are 1k-2k samples that all have the same prediction value, and there are only 3600 distinct prediction values over all of the folds for cross-validation.  The resulting ROC looks like five big stair steps, with some little bits of fuzziness around the inner corners.\r\n\r\nI have many sparse features, so I'm hashing those into index ranges for different types of feature subsets, so one feature subset will be in the index range 1 million to 2 million, the next will be in the range 2 million to 3 million, etc.\r\n\r\n"""
3255,35142930,kastnerkyle,ogrisel,2014-06-06 12:54:57,2014-09-23 17:25:53,2014-07-18 17:44:42,closed,,0.15.1,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3255,b'Unstable test_common.test_transformers under Windows with Python 32-bit for some estimators',"b'I am seeing failing tests with both python 2.7 and python 3.4 for Windows\r\n\r\nscipy 0.14\r\nnumpy 1.8.1\r\nall 32 bit\r\n\r\nCCA, LLE, and KernelPCA seem to be the primary culprits. Here is a sample traceback\r\n```\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_transformers(\'KernelPCA\', <class \'sklearn.decomposition.kernel_pca.KernelPCA\r\nay([[ 2.51189522,  2.6430893 ,  2.54847718],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\tests\\test_common.py"", line 269, in check_transformer\r\n    % Transformer)\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 811, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 599, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\nconsecutive fit_transform outcomes not consistent in <class \'sklearn.decomposition.kernel_pca.KernelPCA\'>\r\n(shapes (30, 15), (30, 14) mismatch)\r\n x: array([[  1.87664949e+00,   8.57398986e-02,   4.20312700e-02,\r\n          3.31837404e-08,   0.00000000e+00,   0.00000000e+00,\r\n          0.00000000e+00,   2.24505178e-08,   0.00000000e+00,...\r\n y: array([[  1.87664949e+00,   8.57398986e-02,   4.20312700e-02,\r\n          3.31844220e-08,   0.00000000e+00,   0.00000000e+00,\r\n          0.00000000e+00,   2.24490761e-08,   0.00000000e+00,...\r\n\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_transformers(\'LocallyLinearEmbedding\', <class \'sklearn.manifold.locally_line\r\nllyLinearEmbedding\'>, array([[ 2.51189522,  2.6430893 ,  2.54847718],\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\tests\\test_common.py"", line 269, in check_transformer\r\n    % Transformer)\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 811, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 644, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\nconsecutive fit_transform outcomes not consistent in <class \'sklearn.manifold.locally_linear.LocallyLinearEmbeddi\r\n(mismatch 25.0%)\r\n x: array([[ -2.27507872e-01,   2.98382398e-01],\r\n       [  1.22093549e-01,  -1.92026395e-11],\r\n       [  1.22093549e-01,  -2.04742612e-11],...\r\n y: array([[ -2.35941411e-01,   2.98382398e-01],\r\n       [  1.04872863e-01,  -1.23895338e-12],\r\n       [  1.04872863e-01,   1.95896077e-12],...\r\n\r\n----------------------------------------------------------------------\r\nRan 3257 tests in 279.742s\r\n```\r\n\r\nNames of estimators that cause the failure:\r\n\r\n- KernelPCA\r\n- LocallyLinearEmbedding\r\n- CCA'"
3252,35064101,loli,amueller,2014-06-05 14:49:08,2016-03-09 06:42:57,2015-09-09 22:50:31,closed,,,20,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3252,b'sklearn.cross_validation.cross_val_score and faulty parallelization?',"b""I've just been playing with the `cross_val_score` function and observed the following strange behaviour:\r\n\r\n```\r\niris = datasets.load_iris()\r\nclf = RandomForestClassifier(n_estimators=1)\r\nfor _ in range(10):\r\n    print cross_val_score(clf, iris.data, iris.target, n_jobs=1).mean()\r\n```\r\nresults, as expected, in\r\n```\r\n0.92\r\n0.953333333333\r\n...\r\n```\r\nif I know allow multiple processes / threads, the results are all exactly equal, i.e.\r\n```\r\niris = datasets.load_iris()\r\nclf = RandomForestClassifier(n_estimators=1)\r\nfor _ in range(10):\r\n    print cross_val_score(clf, iris.data, iris.target, n_jobs=2).mean()\r\n```\r\nleads to \r\n```\r\n0.933333333333\r\n0.933333333333\r\n0.933333333333\r\n...\r\n```\r\n\r\nRerunning the last script changes the result itself, but each iteration still yields always the same result as the first.\r\n\r\nI could reproduce the behaviour with sklearn `0.14.1` and `0.15-git`.\r\n\r\nAny ideas what might cause this? And how to overcome it? Probably some difficulties with `joblib.Parallel` and random seeds? Especially, since both the 3-fold cross validation and the RandomForestClassifier introduce some measures of randomness. \r\n\r\nBest,\r\nloli"""
3226,34699124,bertilhatt,larsmans,2014-05-31 07:32:55,2014-07-06 14:00:24,2014-07-06 14:00:24,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3226,b'Ambiguity on submatrix values with anaconda/python2.7',"b'Copy-pasted from nose report\r\n```\r\n======================================================================\r\nERROR: sklearn.cluster.bicluster.tests.test_utils.test_get_submatrix\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""//anaconda/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""//anaconda/lib/python2.7/site-packages/sklearn/cluster/bicluster/tests/test_utils.py"", line 43, in test_get_submatrix\r\n    assert_true(np.all(X != -1))\r\n  File ""//anaconda/lib/python2.7/unittest/case.py"", line 422, in assertTrue\r\n    if not expr:\r\n  File ""//anaconda/lib/python2.7/site-packages/scipy/sparse/base.py"", line 183, in __bool__\r\n    raise ValueError(""The truth value of an array with more than one ""\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().\r\n\r\n----------------------------------------------------------------------\r\n```'"
3209,34407105,colincsl,ogrisel,2014-05-27 19:56:56,2016-07-06 07:03:37,2016-07-06 07:03:37,closed,,,5,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3209,b'Build errors with Clang',"b'I had an issue compiling sklearn on my new mac. The issue has to do with clang and is documented elsewhere (http://stackoverflow.com/questions/22313407/clang-error-unknown-argument-mno-fused-madd-python-package-installation-fa)\r\n\r\nThe issue is that clang doesn\'t like the ""shorten-64-to-32"" cpp flag. The build crashes when trying to compile libsvm.\r\n\r\n//In order to compile I had to do:\r\n//export ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future\r\n//sudo -E python setup.py install\r\nedit: It finished building with the previous fix but it crashes when I use sklearn in a python terminal.\r\n'"
3190,34172870,arjoly,GaelVaroquaux,2014-05-23 13:05:49,2014-07-15 08:19:59,2014-07-15 08:19:59,closed,,,9,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3190,b'Heisen-bug with omp_cv',"b'Got a  heisen travis failure while working on #3173.\r\nThe entire travis log is at https://travis-ci.org/scikit-learn/scikit-learn/jobs/25868444\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.linear_model.tests.test_omp.test_omp_cv\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_omp.py"", line 195, in test_omp_cv\r\n    ompcv.fit(X, y_)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/omp.py"", line 867, in fit\r\n    for train, test in cv)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py"", line 644, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py"", line 391, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py"", line 129, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/omp.py"", line 764, in _omp_path_residues\r\n    return_path=True)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/omp.py"", line 371, in orthogonal_mp\r\n    copy_X=copy_X, return_path=return_path)\r\n  File ""/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/omp.py"", line 109, in _cholesky_omp\r\n    **solve_triangular_args)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/linalg/basic.py"", line 115, in solve_triangular\r\n    a1, b1 = map(asarray_chkfinite,(a,b))\r\n  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 595, in asarray_chkfinite\r\n    ""array must not contain infs or NaNs"")\r\nValueError: array must not contain infs or NaNs\r\n```\r\n\r\nThe test doesn\'t seem to be always stable.'"
3186,34111829,randomrandom,amueller,2014-05-22 19:03:28,2015-01-26 15:46:11,2015-01-26 15:46:11,closed,,0.16,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3186,b'class_prior has no effect when using partial_fit on MultinomialNB',"b""I tried to use class_prior when doing an out-of-core learning using the partial_fit method on a MultinomialNB classifier, but it seems like the prior probabilities are not taken into account at all. \r\n\r\nSo I checked the code of the BaseDiscreteNB and it really seems like the class_prior is never taken into account when using the partial_fit method. Is this done intentionally or is it a bug?\r\n\r\nIf it is not a bug, can you please document why class_prior shouldn't and wouldn't be taken into account when using partial_fit?\r\n\r\nThanks a lot!"""
3185,34098655,t-pfaff,amueller,2014-05-22 16:26:32,2014-07-18 15:43:57,2014-07-18 15:43:57,closed,,0.15.1,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3185,b'DOC score() of `BaseSearchCV` used different scorer than defined in `scoring` of `GridSearchCV`',"b""I'm not sure if this is intended, but I was confused that the ```score``` function after grid search uses a different scorer than the scorer defined in ```GridSearchCV```. \r\n\r\nExample:\r\n```python\r\nfrom sklearn import datasets\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.grid_search import GridSearchCV\r\nfrom sklearn.cross_validation import train_test_split\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom sklearn.metrics import r2_score\r\n\r\ndigits = datasets.load_digits()\r\nX = digits.data\r\ny = digits.target\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\r\n\r\nhyperparams = [{'fit_intercept':[True, False]}]\r\nalgo = LinearRegression()\r\n\r\ngrid = GridSearchCV(algo, hyperparams, cv=5, scoring='mean_squared_error')\r\ngrid.fit(X_train, y_train)\r\nprint grid.score(X_test, y_test)\r\nprint mean_squared_error(y_test, grid.best_estimator_.predict(X_test))\r\nprint r2_score(y_test, grid.best_estimator_.predict(X_test))\r\n```\r\nI expected that ```grid.score()``` would use the mean_squared_error because this was previously defined in the ```scoring``` option. It took me some time to find out that the number is actually the r2_score which seems to be the ```score``` function of ```LinearRegression```.\r\nThe documentation of ```score()``` in ```BaseSearchCV``` says:\r\n*The ``score`` function of the best estimator is used, or the ``scoring`` parameter where unavailable.*\r\nMaybe the documentation of ```score()``` in ```BaseSearchCV``` could be adapted to make it clear that the calculated score is not necessarily the same as the one defined in the ```scoring``` parameter of ```GridSearchCV```."""
3183,34081093,dlangenk,larsmans,2014-05-22 13:25:46,2014-05-23 09:42:05,2014-05-23 09:42:05,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3183,"b'multiclass OVO, estimator must implement predict_proba'","b'In the documentation (http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html)\r\nthe following is stated:\r\n\r\nestimator : estimator object\r\nAn estimator object implementing fit and predict.\r\n\r\nBut in fact the estimator must implement predict_proba.\r\n\r\nCall graph:\r\n```\r\nclf=sklearn.multiclass.OneVsOneClassifier(estimator)\r\nclf.predict(X)\r\n    predict_ovo(self.estimators_, self.classes_, X)\r\n           score = _predict_binary(estimators[k], X)\r\n                   score = estimator.predict_proba(X)[:, 1]\r\n```'"
3179,34072110,larsmans,larsmans,2014-05-22 11:17:28,2014-05-23 11:07:12,2014-05-23 11:07:12,closed,,,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3179,"b""TfidfVectorizer doesn't export `idf_`""",b'Reported on [SO](http://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer). Easy fix: introduce a property that refers to `_tfifd.idf_`.'
3174,33985031,arjoly,arjoly,2014-05-21 14:03:01,2014-05-22 12:20:05,2014-05-22 12:20:05,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3174,"b""Bug LassoCV and ElasticNetCV doesn't handle input type in np.float32""","b'Here a script to reproduce, it pops from #3173 whenever I clean the input type checking.\r\n\r\n```\r\nimport numpy as np\r\nfrom scipy.sparse import csr_matrix\r\nfrom sklearn.utils.validation import safe_asarray\r\nfrom sklearn.datasets import make_multilabel_classification\r\n\r\nfrom sklearn.linear_model import LassoCV\r\nfrom sklearn.linear_model import ElasticNetCV\r\n\r\nX, y = make_multilabel_classification(n_classes=1, random_state=0,\r\n                                      return_indicator=True)\r\nX = safe_asarray(csr_matrix(X), dtype=np.float32)\r\ny = y.ravel()\r\n\r\nfor Estimator in [LassoCV, ElasticNetCV]:\r\n    try:\r\n        estimator = Estimator()\r\n        estimator.fit(X, y)\r\n    except ValueError as e:\r\n        print(""\\nFailed with %s"" % estimator)\r\n        print(e)\r\n\r\n```\r\n\r\nThe output is\r\n```\r\nFailed with LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\r\n    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\r\n    precompute=\'auto\', tol=0.0001, verbose=False)\r\nBuffer dtype mismatch, expected \'DOUBLE\' but got \'float\'\r\n\r\nFailed with ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\r\n       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,\r\n       normalize=False, positive=False, precompute=\'auto\', tol=0.0001,\r\n       verbose=0)\r\nBuffer dtype mismatch, expected \'DOUBLE\' but got \'float\'\r\n```\r\n\r\nShould it coerce its input type to np.float64 (aka DOUBLE)?'"
3160,33756947,DominikJaskiewicz,arjoly,2014-05-18 18:15:56,2014-06-04 13:47:12,2014-05-19 07:05:08,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3160,b'predict_ovo wrongly calculates scores for tie-break',"b""SVC's and LinearSVC's (I didn't check other classifiers) decision_function in binary classification case returns values lower than 0 when predict returns 0, and >0 when predict returns 1. \r\nSo this part of code is wrong:\r\n\r\npred = estimators[k].predict(X)\r\nscore = _predict_binary(estimators[k], X)\r\nscores[:, i] += score\r\nscores[:, j] -= score\r\nvotes[pred == 0, i] += 1\r\nvotes[pred == 1, j] += 1\r\n\r\nIn SVM case we should substract score from scores[:,i] and add to scores[:,j]"""
3159,33755639,mvdoc,mvdoc,2014-05-18 17:07:22,2014-12-17 21:31:28,2014-12-17 21:28:23,closed,,0.16,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3159,b'ward_tree fails when connectivity matrix is too sparse',"b'Hi,\r\nwhen running some simulations I found out that if the connectivity matrix is too sparse, `ward_tree` fails with an IndexError. It works fine with a different connectivity matrix.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-44-fdfab3cceda7> in <module>()\r\n----> 1 out = ward_tree(X, connectivity)\r\n\r\n/usr/lib/python2.7/dist-packages/sklearn/cluster/hierarchical.pyc in ward_tree(X, connectivity, n_components, copy, n_clusters)\r\n    167         # identify the merge\r\n    168         while True:\r\n--> 169             inert, i, j = heappop(inertia)\r\n    170             if used_node[i] and used_node[j]:\r\n    171                 break\r\n\r\nIndexError: index out of range\r\n```\r\n\r\nSee this notebook for the simulation code: http://nbviewer.ipython.org/github/mvdoc/notebooks/blob/master/ward_tree_failing.ipynb'"
3142,33250285,vm-wylbur,amueller,2014-05-10 22:17:38,2015-02-24 21:39:07,2015-02-24 21:39:07,closed,,0.16,18,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3142,"b""linear_model giving AttributeError: 'numpy.float64' object has no attribute 'exp'""","b'sklearn 0.14.1, and it happens only with a particular dataset I\'m using, thus I\'m not sure how to provide reproducible data, sorry. \r\n\r\nPer the stacktrace, I\'ve tracked the problem to the lines in linear_model/base.py. I think that LinearClassifierMixin.decision_function is returning an array of dtype=object which makes np.exp() fail. None of the values in the array look to my eye like anything other than floats. Casting the array explicitly as a float (as the commented line shows) allows predict_proba to exponentiate. \r\n\r\nThere might be something happening in decision_function such it returns a non-float result, but I can\'t spot it. thanks -- PB. \r\n\r\n```\r\nfrom sklearn.linear_model import LogisticRegression\r\nclf_lr = LogisticRegression(penalty=\'l1\')\r\nfull = clf_lr.fit(X, Y)\r\nprobs = full.predict_proba(X)\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-72-2db71751c630> in <module>()\r\n      2 clf_lr = LogisticRegression(penalty=\'l1\')\r\n      3 full = clf_lr.fit(X, Y)\r\n----> 4 probs = full.predict_proba(X)\r\n\r\n/Users/pball/miniconda3/lib/python3.3/site-packages/sklearn/linear_model/logistic.py in predict_proba(self, X)\r\n    120             where classes are ordered as they are in ``self.classes_``.\r\n    121         """"""\r\n--> 122         return self._predict_proba_lr(X)\r\n    123 \r\n    124     def predict_log_proba(self, X):\r\n\r\n/Users/pball/miniconda3/lib/python3.3/site-packages/sklearn/linear_model/base.py in _predict_proba_lr(self, X)\r\n    237         prob = self.decision_function(X)\r\n    238         # PB hack\r\n--> 239         # prob = prob.astype(float)\r\n    240         prob *= -1\r\n    241         np.exp(prob, prob)\r\n\r\nAttributeError: \'numpy.float64\' object has no attribute \'exp\'\r\n```'"
3128,32748916,nmayorov,ogrisel,2014-05-03 15:30:28,2014-09-30 12:18:50,2014-07-29 09:53:30,closed,,0.15.1,18,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3128,b'[MRG+1] FIX: Handling of multilabel targets in cross_val_score',"b""Currently ``StratifiedKFold`` is used as a split strategy in ``cross_val_score``. But it won't work correctly if ``y`` is multilabel, i.e. a label indicator matrix or a sequence of sequences. \r\n\r\nI added a couple of lines that check the type of ``y`` and if it's multilabel \xa1\xaa simple ``KFold`` strategy is invoked."""
3123,32487636,kmike,ogrisel,2014-04-29 21:37:22,2015-10-07 14:41:34,2015-10-07 14:41:34,closed,,0.17,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3123,b'labels argument of classification_report is not useful when y is a list of strings',"b'In scikit-learn 0.14.1 it was possible to have `y_true` and `y_pred` lists of strings, and pass a list of strings as a `labels` argument to `classification_report`, and it worked as expected: only labels from this list were included to the report. This no longer works in scikit-learn master. \r\n\r\nIt was never documented that it should work: docs say that `labels` is an ""Optional list of label indices to include in the report."" So, according to docs, it was undefined what happens if `y` consists of strings and `labels` argument is passed - caller doesn\'t have correct indices to pass in this case.\r\n\r\nIt seems it is better to either raise an error if `labels` is passed when `y` is not pre-transformed by a LabelEncoder, or to restore and document 0.14.1 behavior. What do you think?'"
3121,32446111,NelleV,amueller,2014-04-29 13:44:43,2014-07-18 15:49:41,2014-07-18 15:49:41,closed,,0.15.1,2,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3121,b'The 0.13 documentation links points toward 0.14 doc',"b'Hi team,\r\nThe 0.13 doc on the website is in fact the 0.14.\r\nhttp://scikit-learn.org/0.13/modules/classes.html (notice the very nice warning on the top left corner indicating this is the documentation for 0.14)'"
3107,32184494,eickenberg,arjoly,2014-04-24 20:50:15,2015-03-15 18:24:16,2014-08-06 12:43:11,closed,,0.16,42,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3107,b'[MRG+1] Feature or Bug? Whitened PCA does not inverse_transform properly and is even document that way',"b'The `PCA.inverse_transform` docstring even explicitly states that the inverse transform after whitening is inexact, because the necessary rescaling to fall back into the right space is not done. Is there a specific reason for this? One would think that the expected behaviour of an `inverse_transform` should be that it maps to the closest possible point in input space given the incurred information loss. Since with (full rank) PCA one can keep all the information, the inverse transform should be the true inverse.\r\n\r\nAny opinions on this?\r\n\r\nMy main concern for changing this is that this behaviour is documented and thus possibly expected by some users.\r\nMaking the `PCA` object do a true inverse is as easy as adding 2 lines to the `inverse_transform` as visible in the diff.'"
3044,30995462,ogrisel,larsmans,2014-04-07 15:40:47,2014-04-11 14:39:10,2014-04-07 21:12:28,closed,,,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3044,b'cross_val_score and GridSearchCV should allow for missing values encoded as NaNs',"b""Assume you want to evaluate or grid search the parameters of the following pipeline\r\n\r\n```python\r\nfrom sklearn.preprocessing import Imputer\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.pipeline import Pipeline\r\n\r\np = Pipeline([\r\n    ('imputer', Imputer(strategy='media', missing_values='NaN')),\r\n    ('classifier', LogisticRegression(C=1)),\r\n])\r\n```\r\n\r\nAt the moment it is not possible as the `cross_val_score` and `GridSearchCV` tools call `check_arrays` internally which raises an exception if there are `NaN` values in the data.\r\n\r\nI think we should add `allow_nans=False` parameter to `check_arrays`, `cross_val_score` and `GridSearchCV` to make it possible to the user to disable the check for nans and hence allow our imputing pipeline to work as expected."""
3039,30816234,HapeMask,larsmans,2014-04-03 21:37:26,2014-05-10 15:44:42,2014-05-10 14:38:10,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3039,b'MiniBatchKMeans fails with certain combinations of n_clusters and feature dimensions in latest master.',"b'Code to reproduce:\r\n```python\r\nimport sys\r\nimport numpy as np\r\nimport scipy as sp\r\nimport sklearn as sk\r\nimport traceback\r\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\r\n\r\nprint(""python:"", sys.version)\r\nprint(""numpy:"", np.__version__)\r\nprint(""scipy:"", sp.__version__)\r\nprint(""sklearn:"", sk.__version__)\r\n\r\n\r\nfor D in [64,128,256]:\r\n    for n_clusters in [100,150,200,250]:\r\n        X = np.random.rand(500, D)\r\n\r\n        print(""KMeans..."")\r\n        KMeans(n_clusters=n_clusters).fit(X)\r\n        try:\r\n            print(""MiniBatchKMeans..."")\r\n            MiniBatchKMeans(n_clusters=n_clusters).fit(X)\r\n        except:\r\n            print(""Failed."")\r\n            print(""%d clusters"" % n_clusters)\r\n            print(""%d dimensions"" % X.shape[1])\r\n            traceback.print_exc()\r\n            continue\r\n```\r\n\r\nWhen X has dimension (500, 200) or (500, 250), or with larger D as well, the batch KMeans fails with an exception like this:\r\n\r\n```Failed.\r\n250 clusters\r\n256 dimensions\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 20, in <module>\r\n    MiniBatchKMeans(n_clusters=n_clusters).fit(X)\r\n  File ""/usr/lib/python3.4/site-packages/sklearn/cluster/k_means_.py"", line 1200, in fit\r\n    verbose=self.verbose)\r\n  File ""/usr/lib/python3.4/site-packages/sklearn/cluster/k_means_.py"", line 862, in _mini_batch_step\r\n    centers[to_reassign] = X[new_centers]\r\nValueError: shape mismatch: value array of shape (100,256) could not be broadcast to indexing result of shape (119,256)\r\n```\r\n\r\nVersion info:\r\npython: 3.4.0 (default, Mar 17 2014, 23:20:09) \r\n[GCC 4.8.2 20140206 (prerelease)]\r\nnumpy: 1.9.0.dev-fd0d7d2\r\nscipy: 0.15.0.dev-2e5b1dd\r\nsklearn: 0.15-git (9c51bc954718146cb1108f1d8c0a7483d7d6da8d)\r\n\r\nThe same issue also appears with the following version combo (stable versions of all but sklearn):\r\n(\'python:\', \'2.7.6 (default, Feb 26 2014, 12:07:17) \\n[GCC 4.8.2 20140206 (prerelease)]\')\r\n(\'numpy:\', \'1.8.0\')\r\n(\'scipy:\', \'0.13.3\')\r\n(\'sklearn:\', \'0.15-git\')'"
3014,30428810,ogrisel,ogrisel,2014-03-28 22:51:28,2014-07-15 16:29:46,2014-03-28 23:45:04,closed,,,0,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3014,b'zlib.error while running test_covtype',"b'The covertype dataset loader test tries to load previously fetched data compressed with joblib / zlib.\r\n\r\nIf this is fetched with a different major version of python (e.g. 2.7 and 3.4), this runs into the following compatibility issue in the zlib compressed data:\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_covtype.test_fetch\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/datasets/covtype.py"", line 81, in fetch_covtype\r\n    X, y\r\nUnboundLocalError: local variable \'X\' referenced before assignment\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/datasets/tests/test_covtype.py"", line 17, in test_fetch\r\n    data1 = fetch(shuffle=True, random_state=42)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/datasets/tests/test_covtype.py"", line 12, in fetch\r\n    return fetch_covtype(*args, download_if_missing=False, **kwargs)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/datasets/covtype.py"", line 83, in fetch_covtype\r\n    X = joblib.load(samples_path)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/externals/joblib/numpy_pickle.py"", line 421, in load\r\n    unpickler = ZipNumpyUnpickler(filename, file_handle=file_handle)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/externals/joblib/numpy_pickle.py"", line 310, in __init__\r\n    mmap_mode=None)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/externals/joblib/numpy_pickle.py"", line 268, in __init__\r\n    self.file_handle = self._open_pickle(file_handle)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/externals/joblib/numpy_pickle.py"", line 313, in _open_pickle\r\n    return BytesIO(read_zfile(file_handle))\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/externals/joblib/numpy_pickle.py"", line 68, in read_zfile\r\n    data = zlib.decompress(file_handle.read(), 15, length)\r\nzlib.error: Error -3 while decompressing data: incorrect header check\r\n```'"
3008,30308081,ogrisel,ogrisel,2014-03-27 14:59:42,2014-07-15 16:29:45,2014-04-03 14:28:17,closed,ogrisel,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3008,b'Failing tests under windows',"b'I am working on fixing the failing tests under windows so as to prepare the 0.15 release (I will configure a jenkins bot as soon as I get them to pass). To track progress here is the log of the remaining failures:\r\n\r\n```\r\n======================================================================\r\nERROR: Check that we obtain the correct number of clusters with\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\cluster\\tests\\test_hierarchical.py"", line 146, in test_agglomerative_clustering\r\n    clustering.fit(X)\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\cluster\\hierarchical.py"", line 656, in fit\r\n    labels = _hierarchical.hc_get_heads(parents, copy=False)\r\n  File ""_hierarchical.pyx"", line 106, in sklearn.cluster._hierarchical.hc_get_heads (sklearn\\cluster\\_hierarchical.cpp:2810)\r\nValueError: Buffer dtype mismatch, expected \'INTP\' but got \'long\'\r\n\r\n======================================================================\r\nERROR: sklearn.utils.tests.test_sparsefuncs.test_densify_rows\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\tests\\test_sparsefuncs.py"", line 43, in test_densify_rows\r\n    assign_rows_csr(X, rows, np.arange(out.shape[0])[::-1], out)\r\n  File ""sparsefuncs_fast.pyx"", line 313, in sklearn.utils.sparsefuncs_fast.assign_rows_csr (sklearn\\utils\\sparsefuncs_fast.c:4235)\r\nValueError: Buffer dtype mismatch, expected \'npy_intp\' but got \'long\'\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.datasets.base.load_boston\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.datasets.base.load_boston\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\base.py"", line 392, in load_boston\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\base.py"", line 413, in sklearn.datasets.base.load_boston\r\nFailed example:\r\n    print(boston.data.shape)\r\nExpected:\r\n    (506, 13)\r\nGot:\r\n    (506L, 13L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x0000000009EE5E08>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.datasets.base.load_digits\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.datasets.base.load_digits\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\base.py"", line 272, in load_digits\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\base.py"", line 306, in sklearn.datasets.base.load_digits\r\nFailed example:\r\n    print(digits.data.shape)\r\nExpected:\r\n    (1797, 64)\r\nGot:\r\n    (1797L, 64L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x0000000009EE5E08>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.datasets.mldata.fetch_mldata\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.datasets.mldata.fetch_mldata\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\mldata.py"", line 38, in fetch_mldata\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\mldata.py"", line 100, in sklearn.datasets.mldata.fetch_mldata\r\nFailed example:\r\n    iris.target.shape\r\nExpected:\r\n    (150,)\r\nGot:\r\n    (150L,)\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\mldata.py"", line 102, in sklearn.datasets.mldata.fetch_mldata\r\nFailed example:\r\n    iris.data.shape\r\nExpected:\r\n    (150, 4)\r\nGot:\r\n    (150L, 4L)\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\mldata.py"", line 110, in sklearn.datasets.mldata.fetch_mldata\r\nFailed example:\r\n    leuk.data.shape\r\nExpected:\r\n    (72, 7129)\r\nGot:\r\n    (72L, 7129L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x0000000009EE5F08>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.datasets.samples_generator.make_blobs\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.datasets.samples_generator.make_blobs\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\samples_generator.py"", line 615, in make_blobs\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\datasets\\samples_generator.py"", line 659, in sklearn.datasets.samples_generator.make_blobs\r\nFailed example:\r\n    print(X.shape)\r\nExpected:\r\n    (10, 2)\r\nGot:\r\n    (10L, 2L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x0000000009EE5E08>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.feature_extraction.image.extract_patches_2d\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.feature_extraction.image.extract_patches_2d\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\feature_extraction\\image.py"", line 274, in extract_patches_2d\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\feature_extraction\\image.py"", line 317, in sklearn.feature_extraction.image.extract_patches_2d\r\nFailed example:\r\n    print(patches.shape)\r\nExpected:\r\n    (9, 2, 2)\r\nGot:\r\n    (9L, 2L, 2L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x0000000009E600C8>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: sklearn.linear_model.tests.test_least_angle.test_lars_drop_for_good\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\linear_model\\tests\\test_least_angle.py"", line 368, in test_lars_drop_for_good\r\n    assert_array_almost_equal(lars_obj / cd_obj, 1.0, decimal=3)\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 811, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 644, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 3 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.6492867988524876)\r\n y: array(1.0)\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 3 decimals\\n\\n(mismatch 100.0%)\\n x: array(0.6492867988524876)\\n y: array(1.0)\')\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.utils.arpack._eigs\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.utils.arpack._eigs\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\arpack.py"", line 1052, in _eigs\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\arpack.py"", line 1168, in sklearn.utils.arpack._eigs\r\nFailed example:\r\n    vecs.shape\r\nExpected:\r\n    (13, 6)\r\nGot:\r\n    (13L, 6L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x000000000B23D4C8>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.utils.arpack._eigsh\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.utils.arpack._eigsh\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\arpack.py"", line 1267, in _eigsh\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\arpack.py"", line 1411, in sklearn.utils.arpack._eigsh\r\nFailed example:\r\n    print(vecs.shape)\r\nExpected:\r\n    (13, 6)\r\nGot:\r\n    (13L, 6L)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x000000000B23D688>.getvalue()))\r\n\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.utils.multiclass.unique_labels\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.utils.multiclass.unique_labels\r\n  File ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\multiclass.py"", line 40, in unique_labels\r\n\r\n----------------------------------------------------------------------\r\nFile ""c:\\Users\\Administrator\\scikit-learn\\sklearn\\utils\\multiclass.py"", line 70, in sklearn.utils.multiclass.unique_labels\r\nFailed example:\r\n    unique_labels(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))\r\nExpected:\r\n    array([0, 1])\r\nGot:\r\n    array([0, 1], dtype=int64)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x000000000B2378C8>.getvalue()))\r\n\r\n\r\n----------------------------------------------------------------------\r\nRan 3961 tests in 337.571s\r\n\r\nFAILED (SKIP=17, errors=2, failures=9)\r\n```\r\n\r\nThose are mostly minor issues I think.'"
3006,30198686,ogrisel,ogrisel,2014-03-26 10:11:56,2014-06-13 12:15:18,2014-03-26 12:39:23,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/3006,b'[MRG] FIX: more robust skip of implicit constructor',b'Parameter introspection requires introspecting the keyword arguments of the\r\nconstructor of Python classes deriving from BaseEstimator.\r\n\r\nThis can be problematic for classes that do not override the default\r\nconstructor. Introspecting the implicit constructor used to raise a TypeError\r\nbut this is no longer the case in Python 3.4+. Hence with use an explicit check\r\nfor the implicit constructor prior to using inspection.'
2986,29891035,ogrisel,ogrisel,2014-03-21 10:36:53,2014-11-15 16:34:07,2014-11-15 16:34:07,closed,,0.16,17,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2986,b'ZeroDivisionError: float division by zero in scipy/sparse/linalg/isolve/lsqr.py',"b'In recent scipy (I think after 0.13.3) built against the reference lapack implementation (not the Atlas variant) the following test fails:\r\n\r\n```\r\n======================================================================\r\nERROR: Test that linear regression also works with sparse data\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/venvs/py27/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/linear_model/tests/test_base.py"", line 77, in test_linear_regression_sparse\r\n    ols.fit(X, y.ravel())\r\n  File ""/Users/ogrisel/code/scikit-learn/sklearn/linear_model/base.py"", line 367, in fit\r\n    out = lsqr(X, y)\r\n  File ""/Users/ogrisel/code/scipy-openblas/scipy/sparse/linalg/isolve/lsqr.py"", line 436, in lsqr\r\n    test3 = 1 / acond\r\nZeroDivisionError: float division by zero\r\n```\r\n\r\nThis is a linear regression (ordinary least squares) on sparse data using the scipy sparse solver. The exact code of the test is:\r\n\r\n```python\r\ndef test_linear_regression_sparse(random_state=0):\r\n    ""Test that linear regression also works with sparse data""\r\n    random_state = check_random_state(random_state)\r\n    n = 100\r\n    X = sparse.eye(n, n)\r\n    beta = random_state.rand(n)\r\n    y = X * beta[:, np.newaxis]\r\n\r\n    ols = LinearRegression()\r\n    ols.fit(X, y.ravel())\r\n    assert_array_almost_equal(beta, ols.coef_ + ols.intercept_)\r\n    assert_array_almost_equal(ols.residues_, 0)\r\n```\r\n\r\nThe tests pass if X is converted to an array and the matrix multiplication replaced by `np.dot`. As the identity matrix is well conditioned and the beta are non-zero:\r\n\r\n```\r\n>>> np.any(np.random.RandomState(0).rand(100) == 0)\r\nFalse\r\n```\r\n\r\nthe OLS solver should be able to recover the exact betas. Before trying to transform it as a scipy-only code snippet I would like to have the confirmation that this is indeed a scipy bug and not a bug in our test.'"
2961,29282803,amueller,larsmans,2014-03-12 17:16:07,2014-05-08 11:42:56,2014-04-04 09:44:57,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2961,b'PIL import in cluster module',"b'I just upgraded sklearn on one of my boxes and current master had a weird issue related to a PIL import (""AccessInit: hash collision: 3 for both 1 and 1""). The error is caused by a mucked up system, but I would argue that we should not try to import PIL when applying a clustering algorithm (I was trying to import ``kmeans``).\r\nI think the PIL import came via matplotlib, but I did not find where that got pulled in.\r\nI could reproduce simply by ``import sklearn.cluster``.'"
2948,28990469,kaushik94,amueller,2014-03-07 19:12:58,2015-01-22 19:01:41,2015-01-22 19:01:41,closed,,0.16,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2948,b'Ward clustering-Too few clusters bug',b'I have raised a ValueError when too few clusters are formed in Ward Clustering.'
2932,28701178,ajtulloch,amueller,2014-03-04 12:24:14,2015-02-24 22:15:51,2015-02-24 22:15:51,closed,,0.16,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2932,b'[BUG] Fix SelectFDR thresholding bug (#2771)',"b'From https://github.com/scikit-learn/scikit-learn/issues/2771, we were\r\nnot correctly scaling the alpha\r\nparameter (http://en.wikipedia.org/wiki/False_discovery_rate#Benjamini.E2.80.93Hochberg_procedure)\r\nwith the number of features (== hypothesis).  Thus, the alpha\r\nparameter was not invariant with respect the number of features.\r\n\r\nThe correction is as suggested in the original issue, and a test has\r\nbeen added that verifies that for various numbers of features, an\r\nappropriate false discovery rate is generated when using the selector.  \r\n\r\nThis test passes with the new FDR logic, and fails with the old FDR logic.'"
2918,28592515,jnothman,jnothman,2014-03-03 00:11:34,2014-04-22 12:04:11,2014-04-22 12:04:11,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2918,b'Imputer test relies on anomalous median behaviour',"b""[test_imputation_mean_median_only_zero](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/preprocessing/tests/test_imputation.py#L97) tests for median value imputation over a column that contains a NaN (not to be imputed). The current system determines a non-NaN result for that median (5). I'm not sure this makes sense, but I don't really know what median means with NaN.\r\n\r\nMoreover, the current implementation is emulating the anomalous output of [`numpy.ma.median`](https://github.com/numpy/numpy/issues/4422) with respect to NaNs, in which results differ from the unmasked equivalent.\r\n\r\nI think we can choose to:\r\n* define median as ignoring all NaNs (a `nanmedian`), but this would be inconsistent with mean;\r\n* return NaN for median including NaNs; or\r\n* leave behaviour undefined for columns containing NaN, and not test it.\r\n\r\n(I have a cleaner and correct implementation of sparse median for `Imputer` waiting to PR, but I wanted to check what the appropriate resolution is.)"""
2903,28349718,ogrisel,ogrisel,2014-02-26 16:58:19,2014-03-03 15:16:54,2014-03-03 15:05:43,closed,,,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2903,b'Memory leak in MinibatchKMeans on sparse data',"b'It seems that we have a memory leak when running `MinibatchKMeans` on sparse data:\r\n\r\n```python\r\nimport os, psutil\r\nfrom sklearn.datasets import fetch_20newsgroups\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.cluster import MiniBatchKMeans\r\n\r\ndef make_text_data(**kwargs):\r\n    twenty = fetch_20newsgroups()\r\n    return TfidfVectorizer(**kwargs).fit_transform(twenty.data)\r\ntext_data = make_text_data(min_df=3, max_df=0.5)\r\n\r\np = psutil.Process(os.getpid())\r\n\r\nfor i in range(5):\r\n    MiniBatchKMeans(n_clusters=100).fit(text_data)\r\n    print(p.get_memory_info().rss / 1e6)\r\n```\r\n\r\nhere is the output:\r\n\r\n```\r\n347.856896\r\n441.024512\r\n534.257664\r\n627.48672\r\n720.715776\r\n```'"
2901,28330674,arjoly,larsmans,2014-02-26 12:50:28,2014-12-01 10:52:08,2014-12-01 10:52:08,closed,,0.16,3,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2901,b'Inconsistencies for the kernel documentation',"b""Here some inconsistencies that I have encountered when I wanted to use kernel:\r\n\r\n* [x] The degree In KernelPCA, svm-kernel and SVC, they don't agree with the presence of the parameter degree for the sigmoid kernel.parameter of [KernelPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA) is said to be used for the rbf kernel. However in the [svm kernel documentation](http://scikit-learn.org/stable/modules/svm.html#svm-kernels), there isn't any mention of the degree parameter.\r\n* [x] In [KernelPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA), [svm-kernel](http://scikit-learn.org/stable/modules/svm.html#svm-kernels) and [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), they don't agree with the presence of the parameter degree for the sigmoid kernel.\r\n* [ ] In the [kernel pca narrative doc](http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca) and in [KernelPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA), there isn't any link to the kernel definition\r\n* [ ] When you browse the narrative doc in [Pairwise metrics, Affinities and Kernels](http://scikit-learn.org/stable/modules/metrics.html), there is no pointer to the kernel approximation, definitions of all kernels and estimators/transformers that can use kernels.\r\n\r\nThe information about kernels is spread in at least four page. It would be nice that [Pairwise metrics, Affinities and Kernels](http://scikit-learn.org/stable/modules/metrics.html) contains most information with links to the relevant pages (and backlink from these page)."""
2899,28321450,arjoly,larsmans,2014-02-26 10:03:32,2014-03-09 22:04:17,2014-03-09 22:04:17,closed,,,3,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2899,b'Example rendering of sklearn.datasets.fetch_mldata',"b""The example of [sklearn.datasets.fetch_mldata](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_mldata.html) with the leukemia dataset doesn't render properly.\r\n"""
2839,27216817,jnothman,larsmans,2014-02-09 05:21:08,2014-02-27 21:11:01,2014-02-27 21:11:01,closed,,,0,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2839,b'OneHotEncoder should raise ValueError when value >= n_values',"b'For example, the following is broken input and output:\r\n```python\r\n>>> OneHotEncoder(n_values=4).fit_transform([[1, 2, 2, 3, 3, 1, 2, 4, 4, 1]]).A.reshape(10, 4)\r\narray([[ 0.,  1.,  0.,  0.],\r\n       [ 0.,  0.,  1.,  0.],\r\n       [ 0.,  0.,  1.,  0.],\r\n       [ 0.,  0.,  0.,  1.],\r\n       [ 0.,  0.,  0.,  1.],\r\n       [ 0.,  1.,  0.,  0.],\r\n       [ 0.,  0.,  1.,  0.],\r\n       [ 0.,  0.,  0.,  0.],\r\n       [ 1.,  0.,  0.,  0.],\r\n       [ 1.,  1.,  0.,  0.]])\r\n```'"
2787,26164565,ogrisel,glouppe,2014-01-23 13:01:15,2014-07-15 16:29:45,2014-01-25 13:15:46,closed,,,18,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2787,b'[Regression] Memory Leak in decision trees',"b'Here is a reproduction script. I used a random tree to make it run much faster but I think this impacts all tree implementations. I used a very large output dimension to make the leak more visible:\r\n\r\n```python\r\nimport gc\r\nimport os\r\nimport psutil\r\nimport numpy as np\r\nfrom sklearn.tree import ExtraTreeRegressor\r\n\r\nX = np.random.normal(size=(100, 50))\r\nY = np.random.normal(size=(100, int(5e4)))\r\n\r\np = psutil.Process(os.getpid())\r\n\r\ndef print_mem():\r\n    print(""{:.0f}MB"".format(p.get_memory_info().rss / 1e6))\r\n\r\nprint_mem()\r\n\r\nfor i in range(3):\r\n    et = ExtraTreeRegressor(max_features=1).fit(X, Y)\r\n    del et\r\n    gc.collect()\r\n    print_mem()\r\n\r\n```\r\n\r\nHere is the output on the current master:\r\n\r\n```\r\n86MB\r\n171MB\r\n251MB\r\n331MB\r\n```\r\n\r\nThe output on sklearn 0.14.1:\r\n\r\n```\r\n74MB\r\n74MB\r\n74MB\r\n74MB\r\n```\r\n'"
2785,26070286,twotimesfiftyfive,larsmans,2014-01-22 06:51:54,2014-07-15 16:29:45,2014-02-18 18:42:55,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2785,b'Numerical Stability Bug in RBM (0.14.1) for Python 2.7.x',"b'Code demonstrating: \r\n\r\n```python\r\nfrom sklearn import preprocessing, cross_validation\r\nfrom scipy.ndimage import convolve\r\nfrom sklearn.neural_network import BernoulliRBM\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn import linear_model, datasets, metrics\r\nimport numpy as np\r\n\r\n# create fake dataset\r\ndata, labels = datasets.make_classification(n_samples=250000)\r\ndata = preprocessing.scale(data)\r\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(data, labels, test_size=0.7, random_state=0)\r\n\r\n# print details\r\nprint X_train.shape, X_test.shape, y_train.shape, y_test.shape\r\nprint np.max(X_train) \r\nprint np.min(X_train)\r\nprint np.mean(X_train, axis=0)\r\nprint np.std(X_train, axis=0)\r\n\r\nif np.sum(np.isnan(X_train)) or np.sum(np.isnan(X_test)):\r\n    print ""NaN found!""\r\n\r\nif np.sum(np.isnan(y_train)) or np.sum(np.isnan(y_test)):\r\n    print ""NaN found!""\r\n\r\nif np.sum(np.isinf(X_train)) or np.sum(np.isinf(X_test)):\r\n    print ""Inf found!""\r\n\r\nif np.sum(np.isinf(y_train)) or np.sum(np.isinf(y_test)):\r\n    print ""Inf found!""  \r\n\r\n# train and test\r\nlogistic = linear_model.LogisticRegression()\r\nrbm = BernoulliRBM(random_state=0, verbose=True)\r\nclassifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)])\r\n\r\n# Training RBM-Logistic Pipeline\r\nclassifier.fit(X_train, y_train)\r\n\r\n# Training Logistic regression\r\nlogistic_classifier = linear_model.LogisticRegression(C=100.0)\r\nlogistic_classifier.fit(X_train, y_train)\r\n\r\nprint(""Logistic regression using RBM features:\\n%s\\n"" % (\r\n    metrics.classification_report(\r\n        y_test,\r\n        classifier.predict(X_test))))\r\n```\r\n\r\nStack trace:\r\n```\r\n(73517, 3) (171540, 3) (73517,) (171540,)\r\n2.0871168057\r\n-2.21062647188\r\n[-0.00237028 -0.00104526  0.00330683]\r\n[ 0.99907225  0.99977328  1.00225843]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/lib/python2.7/dist-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)\r\n    173             else:\r\n    174                 filename = fname\r\n--> 175             __builtin__.execfile(filename, *where)\r\n\r\n/home/test.py in <module>()\r\n     75 \r\n     76 # Training RBM-Logistic Pipeline\r\n\r\n---> 77 classifier.fit(X_train, y_train)\r\n     78 \r\n     79 # Training Logistic regression\r\n\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc in fit(self, X, y, **fit_params)\r\n    128         data, then fit the transformed data using the final estimator.\r\n    129         """"""\r\n--> 130         Xt, fit_params = self._pre_transform(X, y, **fit_params)\r\n    131         self.steps[-1][-1].fit(Xt, y, **fit_params)\r\n    132         return self\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc in _pre_transform(self, X, y, **fit_params)\r\n    118         for name, transform in self.steps[:-1]:\r\n    119             if hasattr(transform, ""fit_transform""):\r\n--> 120                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\r\n    121             else:\r\n    122                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/base.pyc in fit_transform(self, X, y, **fit_params)\r\n    409         else:\r\n    410             # fit method of arity 2 (supervised transformation)\r\n\r\n--> 411             return self.fit(X, y, **fit_params).transform(X)\r\n    412 \r\n    413 \r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/rbm.pyc in fit(self, X, y)\r\n    304 \r\n    305             for batch_slice in batch_slices:\r\n--> 306                 pl_batch = self._fit(X[batch_slice], rng)\r\n    307 \r\n    308                 if verbose:\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/rbm.pyc in _fit(self, v_pos, rng)\r\n    245 \r\n    246         if self.verbose:\r\n--> 247             return self.score_samples(v_pos)\r\n    248 \r\n    249     def score_samples(self, v):\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/rbm.pyc in score_samples(self, v)\r\n    268         fe_ = self._free_energy(v_)\r\n    269 \r\n--> 270         return v.shape[1] * logistic_sigmoid(fe_ - fe, log=True)\r\n    271 \r\n    272     def fit(self, X, y=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.pyc in logistic_sigmoid(X, log, out)\r\n    498     """"""\r\n    499     is_1d = X.ndim == 1\r\n--> 500     X = array2d(X, dtype=np.float)\r\n    501 \r\n    502     n_samples, n_features = X.shape\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in array2d(X, dtype, order, copy, force_all_finite)\r\n     91     X_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\r\n     92     if force_all_finite:\r\n---> 93         _assert_all_finite(X_2d)\r\n     94     if X is X_2d and copy:\r\n     95         X_2d = safe_copy(X_2d)\r\n\r\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in _assert_all_finite(X)\r\n     25     if (X.dtype.char in np.typecodes[\'AllFloat\'] and not np.isfinite(X.sum())\r\n     26             and not np.isfinite(X).all()):\r\n---> 27         raise ValueError(""Array contains NaN or infinity."")\r\n     28 \r\n     29 \r\n\r\nValueError: Array contains NaN or infinity.\r\n```'"
2774,25916187,larsmans,larsmans,2014-01-20 12:35:42,2014-04-13 15:01:04,2014-04-13 15:01:04,closed,,,9,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2774,"b""Imputer doesn't work in grid search""","b'When `GridSearchCV` sees a NaN, it panics. This is annoying when the estimator is a pipeline that starts with an `Imputer`, as now the imputer must be trained outside of the grid search giving potentially skewed results.'"
2771,25885252,depristo,amueller,2014-01-19 18:22:32,2015-02-24 22:15:19,2015-02-24 22:15:19,closed,,0.16,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2771,b'SelectFdr has serious thresholding bug',"b""The current code reads like:\r\n\r\n    def _get_support_mask(self):\r\n        alpha = self.alpha\r\n        sv = np.sort(self.pvalues_)\r\n        threshold = sv[sv < alpha * np.arange(len(self.pvalues_))].max()\r\n        return self.pvalues_ <= threshold\r\n\r\nBut this doesn't actually control FDR at all, the correct implementation should have:\r\n\r\n        bf_alpha = alpha / len(self.pvalues_)\r\n        threshold = sv[sv < bf_alpha * np.arange(len(self.pvalues_))].max()\r\n\r\nNote the k / m term in the equation at:\r\nhttp://en.wikipedia.org/wiki/False_discovery_rate#Benjamini.E2.80.93Hochberg_procedure\r\n\r\n\r\n"""
2758,25718022,shhwang,GaelVaroquaux,2014-01-16 11:14:51,2014-02-08 18:40:42,2014-02-08 18:40:42,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2758,b'Installing problem',"b'Hi, \r\n\r\nI tried to install scikit in my machine, Ubuntu 12.04.4 LTS (GNU/Linux 3.2.0-53-generic x86_64), using the command \r\nsudo apt-get install python-sklearn\r\n\r\nIt seemed to be fine. But when I tried to test it: nosetests sklearn --exe\r\n\r\nI got the following message:\r\n```\r\n/usr/lib/pymodules/python2.7/sklearn/hmm.py:24: UserWarning: sklearn.hmm is orphaned, undocumented and has known numerical stability issues. If nobody volunteers to write documentation and make it more stable, this module will be removed in version 0.11.\r\n  warnings.warn(\'sklearn.hmm is orphaned, undocumented and has known numerical\'\r\n.........../usr/lib/pymodules/python2.7/sklearn/neighbors/base.py:23: UserWarning: kneighbors: neighbor k+1 and neighbor k have the same distance: results will be dependent on data order.\r\n  warnings.warn(msg)\r\n................................/usr/lib/pymodules/python2.7/sklearn/cluster/spectral.py:77: UserWarning: pyamg not available, using scipy.sparse\r\n  warnings.warn(\'pyamg not available, using scipy.sparse\')\r\n.S........SS..EE.E........................../usr/lib/pymodules/python2.7/sklearn/decomposition/dict_learning.py:262: UserWarning: Please note: the interface of sparse_encode has changed: It now follows the dictionary learning API and it also handles parallelization. Please read the docstring for more information.\r\n  warnings.warn(""Please note: the interface of sparse_encode has changed: ""\r\n.........S........................./usr/lib/pymodules/python2.7/sklearn/decomposition/nmf.py:237: UserWarning: Iteration limit reached in nls subproblem.\r\n  warnings.warn(""Iteration limit reached in nls subproblem."")\r\n.................../usr/lib/pymodules/python2.7/sklearn/decomposition/sparse_pca.py:147: RuntimeWarning: invalid value encountered in divide\r\n  U /= np.sqrt((U ** 2).sum(axis=0))\r\n.....S...../usr/lib/pymodules/python2.7/sklearn/ensemble/forest.py:309: RuntimeWarning: divide by zero encountered in log\r\n  return np.log(self.predict_proba(X))\r\n................................................./usr/lib/pymodules/python2.7/sklearn/svm/classes.py:184: FutureWarning: SVM: scale_C will be True by default in scikit-learn 0.11\r\n  cache_size, scale_C)\r\n....................S...../usr/lib/pymodules/python2.7/sklearn/linear_model/coordinate_descent.py:173: UserWarning: Objective did not converge, you might want to increase the number of iterations\r\n  warnings.warn(\'Objective did not converge, you might want\'\r\n.............................../usr/lib/pymodules/python2.7/sklearn/linear_model/omp.py:172: UserWarning:  Orthogonal matching pursuit ended prematurely due to linear\r\ndependence in the dictionary. The requested precision might not have been met.\r\n\r\n  warn(premature)\r\n.../usr/lib/pymodules/python2.7/sklearn/linear_model/omp.py:78: UserWarning:  Orthogonal matching pursuit ended prematurely due to linear\r\ndependence in the dictionary. The requested precision might not have been met.\r\n\r\n  warn(premature)\r\n............................................................................/usr/lib/pymodules/python2.7/sklearn/metrics/cluster/supervised.py:682: RuntimeWarning: underflow encountered in double_scalars\r\n  emi += (term1[nij] * term2 * term3)\r\n.........................................................../usr/lib/pymodules/python2.7/sklearn/mixture/gmm.py:683: RuntimeWarning: underflow encountered in multiply\r\n  avg_cv = np.dot(post * obs.T, obs) / (post.sum() +\r\n................................/usr/lib/pymodules/python2.7/sklearn/svm/sparse/base.py:23: FutureWarning: SVM: scale_C will be True by default in scikit-learn 0.11\r\n  scale_C)\r\n........./usr/lib/pymodules/python2.7/sklearn/svm/classes.py:373: FutureWarning: SVM: scale_C will be True by default in scikit-learn 0.11\r\n  cache_size, scale_C)\r\n/usr/lib/pymodules/python2.7/sklearn/svm/classes.py:497: FutureWarning: SVM: scale_C will be True by default in scikit-learn 0.11\r\n  cache_size, scale_C=scale_C)\r\n....../usr/lib/pymodules/python2.7/sklearn/svm/classes.py:590: FutureWarning: SVM: scale_C will be True by default in scikit-learn 0.11\r\n  False, cache_size, scale_C=None)\r\n../usr/lib/pymodules/python2.7/sklearn/svm/classes.py:275: FutureWarning: SVM: scale_C will be True by default in scikit-learn 0.11\r\n  cache_size, scale_C=None)\r\n.................../usr/lib/pymodules/python2.7/sklearn/tree/tree.py:690: RuntimeWarning: divide by zero encountered in log\r\n  return np.log(self.predict_proba(X))\r\n.....................................................................................................................................................\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/tests/test_lfw.py"", line 120, in test_load_fake_lfw_people\r\n    min_faces_per_person=3)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 337, in load_lfw_people\r\n    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 266, in fetch_lfw_people\r\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\r\nTypeError: __init__() got an unexpected keyword argument \'compress\'\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading LFW people faces from /tmp/scikit_learn_lfw_test_eZK7c4/lfw_home\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people_too_restrictive\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/python2.7/dist-packages/nose/tools.py"", line 80, in newfunc\r\n    func(*arg, **kw)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/tests/test_lfw.py"", line 149, in test_load_fake_lfw_people_too_restrictive\r\n    load_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 337, in load_lfw_people\r\n    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 266, in fetch_lfw_people\r\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\r\nTypeError: __init__() got an unexpected keyword argument \'compress\'\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading LFW people faces from /tmp/scikit_learn_lfw_test_eZK7c4/lfw_home\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_pairs\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/tests/test_lfw.py"", line 158, in test_load_fake_lfw_pairs\r\n    lfw_pairs_train = load_lfw_pairs(data_home=SCIKIT_LEARN_DATA)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 433, in load_lfw_pairs\r\n    return fetch_lfw_pairs(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 404, in fetch_lfw_pairs\r\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\r\nTypeError: __init__() got an unexpected keyword argument \'compress\'\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading train LFW pairs from /tmp/scikit_learn_lfw_test_eZK7c4/lfw_home\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n----------------------------------------------------------------------\r\nRan 613 tests in 36.882s\r\n\r\nFAILED (SKIP=6, errors=3)\r\n```\r\n\r\nCan anyone help me with this? Thanks in advance.\r\n\r\nShing\r\n'"
2751,25556615,mblondel,agramfort,2014-01-14 07:09:02,2015-11-06 14:15:47,2014-02-18 08:12:45,closed,,,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2751,b'sample_weight in Ridge with fit_intercept=True',"b'Currently, there is the following test for `sample_weight` in Ridge:\r\n```python\r\ndef test_ridge_sample_weights():\r\n    rng = np.random.RandomState(0)\r\n\r\n    for solver in (""dense_cholesky"", ):\r\n        for n_samples, n_features in ((6, 5), (5, 10)):\r\n            for alpha in (1.0, 1e-2):\r\n                y = rng.randn(n_samples)\r\n                X = rng.randn(n_samples, n_features)\r\n                sample_weight = 1 + rng.rand(n_samples)\r\n\r\n                coefs = ridge_regression(X, y,\r\n                                         alpha=alpha,\r\n                                         sample_weight=sample_weight,\r\n                                         solver=solver)\r\n                # Sample weight can be implemented via a simple rescaling\r\n                # for the square loss.\r\n                coefs2 = ridge_regression(\r\n                    X * np.sqrt(sample_weight)[:, np.newaxis],\r\n                    y * np.sqrt(sample_weight),\r\n                    alpha=alpha, solver=solver)\r\n                assert_array_almost_equal(coefs, coefs2)\r\n```\r\nThe test calls `ridge_regression` directly so it only tests for the `fit_intercept=False` case. \r\n\r\nIdeally, I would like to add a test for checking the correctness of the sample_weight support in the `fit_intercept=True` case as well. The following test fails but I\'m not sure whether the problem is in the test or in the code.\r\n```python\r\ndef test_ridge_sample_weights_estimator():\r\n    rng = np.random.RandomState(0)\r\n\r\n    for solver in (""dense_cholesky"", ):\r\n        for n_samples, n_features in ((6, 5), (5, 10)):\r\n            for alpha in (1.0, 1e-2):\r\n                y = rng.randn(n_samples)\r\n                X = rng.randn(n_samples, n_features)\r\n                sample_weight = 1 + rng.rand(n_samples)\r\n\r\n                est = Ridge(alpha=alpha, solver=solver, fit_intercept=True)\r\n                est.fit(X, y, sample_weight=sample_weight)\r\n                coefs = est.coef_.copy()\r\n\r\n                # Sample weight can be implemented via a simple rescaling\r\n                # for the square loss.\r\n                est.fit(X * np.sqrt(sample_weight)[:, np.newaxis],\r\n                        y * np.sqrt(sample_weight))\r\n\r\n                coefs2 = est.coef_\r\n\r\n                assert_array_almost_equal(coefs, coefs2)\r\n```\r\nIn any case, I think it is important to check for the correctness of the `fit_intercept=True` case.\r\n\r\nCC @agramfort @GaelVaroquaux '"
2741,25460373,ai8rahim,larsmans,2014-01-12 10:56:15,2014-02-01 14:11:45,2014-02-01 14:11:45,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2741,b'FAILED test after installation (AssertionError)',"b'Hi,\r\n\r\nI have installed scikit-learn 0.14.1 and got the following error after running the test.\r\n\r\nFAILED (SKIP=12, failures=1)\r\n...\r\nFAIL: sklearn.feature_selection.tests.test_feature_select.test_f_oneway_ints\r\n-----\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose-1.3.0-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/ai/.local/lib/python2.7/site-packages/sklearn/feature_selection/tests/test_feature_select.py"", line 45, in test_f_oneway_ints\r\n    assert_array_equal(f, fint)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 707, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 636, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 80.0%)\r\n x: array([ 0.41558442,  0.02061856,  0.12328767,  0.23198317,  0.35865504,\r\n        0.71223022,  0.05188981,  0.44208611,  0.        ,  0.        ])\r\n y: array([ 0.41558203,  0.02061667,  0.12328766,  0.23198143,  0.35865301,\r\n        0.71222866,  0.0518877 ,  0.44208813,  0.        ,  0.        ], dtype=float32)\r\n\r\n\r\n\r\nThe following are the package details:-\r\nLinux platform\r\nPython 2.7.3\r\nnumpy 1.6.1\r\nscipy 0.9.0\r\nscikit 0.14.1\r\nnose 1.3.0\r\n\r\nIs there a fix for this or can it just be ignored?'"
2737,25375617,cli248,cli248,2014-01-10 05:08:11,2014-02-01 21:20:17,2014-02-01 21:20:17,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2737,b'Four testing cases failed',"b'When I was testing using `nose` as follow,\r\n\r\n    nosetests --exe sklearn\r\n\r\nI got `Ran 2966 tests in 122.078s` and `FAILED (SKIP=16, failures=4)`\r\n\r\nThe followings are the four failures. All of them are `AssertionError: First warning for fit is not a` from `assert_warns` function in `sklearn/utils/tesing.py`. I notice comment `# To remove when we support numpy 1.7` before the `assert_warns` function. As current version of numpy is 1.8, I am wondering if `assert_warns` should be removed.\r\n\r\n----------------------------------------------------------------------\r\n    \r\n    FAIL: Check that oob prediction is a good estimation of the generalization\r\n\r\n    Traceback (most recent call last):\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/nose/case.py"", line 198, in runTest\r\n        self.test(*self.arg)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/ensemble/tests/test_bagging.py"", line 205, in test_oob_score_classification\r\n    y_train)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/utils/testing.py"", line 121, in assert_warns\r\n    % (func.__name__, warning_class, w[0]))\r\n    AssertionError: First warning for fit is not a <class \'UserWarning\'>( is {message :  DeprecationWarning(\'using a non-integer number instead of an integer will result in an error in the future\',), category : \'DeprecationWarning\', filename : \'/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/svm/base.py\', lineno : 233, line : None})\r\n\r\n----------------------------------------------------------------------\r\n    \r\n    FAIL: sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_ill_conditioned\r\n\r\n    Traceback (most recent call last):\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/linear_model/tests/test_least_angle.py"", line 326, in test_lasso_lars_vs_lasso_cd_ill_conditioned\r\n    linear_model.lars_path, X, y, method=\'lasso\')\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/utils/testing.py"", line 167, in assert_warns_message\r\n    % (func.__name__, warning_class, w[0]))\r\n    AssertionError: First warning for lars_path is not a <class \'UserWarning\'>( is {message : DeprecationWarning(\'converting an array with ndim > 0 to an index will result in an error in the future\',), category : \'DeprecationWarning\', filename : \'/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/linear_model/least_angle.py\', lineno : 368, line : None})\r\n\r\n----------------------------------------------------------------------\r\n\r\n    FAIL: sklearn.svm.tests.test_sparse.test_timeout\r\n\r\n    Traceback (most recent call last):\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/svm/tests/test_sparse.py"", line 284, in test_timeout\r\n    assert_warns(ConvergenceWarning, sp.fit, X_sp, Y)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/utils/testing.py"", line 121, in assert_warns\r\n    % (func.__name__, warning_class, w[0]))\r\n    AssertionError: First warning for fit is not a <class \'sklearn.utils.ConvergenceWarning\'>( is {message : DeprecationWarning(\'using a non-integer number instead of an integer will result in an error in the future\',), category : \'DeprecationWarning\', filename : \'/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/svm/base.py\', lineno : 233, line : None})\r\n\r\n----------------------------------------------------------------------\r\n\r\n    FAIL: sklearn.svm.tests.test_svm.test_timeout\r\n\r\n    Traceback (most recent call last):\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/svm/tests/test_svm.py"", line 657, in test_timeout\r\n    assert_warns(ConvergenceWarning, a.fit, X, Y)\r\n      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/utils/testing.py"", line 121, in assert_warns\r\n    % (func.__name__, warning_class, w[0]))\r\n    AssertionError: First warning for fit is not a <class \'sklearn.utils.ConvergenceWarning\'>( is {message : DeprecationWarning(\'using a non-integer number instead of an integer will result in an error in the future\',), category : \'DeprecationWarning\', filename : \'/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/sklearn/svm/base.py\', lineno : 233, line : None})\r\n\r\n----------------------------------------------------------------------\r\n'"
2727,25235811,wedgeCountry,agramfort,2014-01-08 12:16:08,2014-03-08 16:50:53,2014-03-08 16:50:53,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2727,b'Ward clustering: Index Error for Few Clusters in Digit Example',"b'Hej together, I just tried to compile the following code and got an Error for Ward when I searched for too few clusters:\r\nFile: ""/usr/local/lib/python2.7/dist-packages/sklearn/cluster/hierarchical.py"", line 169, in ward_tree:     inert, i, j = heappop(inertia)\r\nIndexError: index out of range\r\n\r\nimport sklearn.datasets, sklearn.neighbors, sklearn.cluster\r\nX = sklearn.datasets.load_digits().data\r\nconnectivity = sklearn.neighbors.kneighbors_graph(X, n_neighbors=10)\r\nn_clusters = 6\r\nward = sklearn.cluster.Ward(n_clusters=n_clusters, connectivity=connectivity).fit(X)\r\n\r\nThis problem also arises for other examples when I reduce n_clusters.\r\nCan anybody help me, please?\r\n\r\nBest, wedge'"
2726,25229574,jnothman,glouppe,2014-01-08 10:07:06,2014-01-29 18:31:44,2014-01-29 18:24:16,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2726,b'BUG Tree allows segfault / arbitrary memory access',"b""The properties for accessing attributes on `Tree` return arrays which don't own their data, and which are malloced and freed by the tree. Therefore:\r\n\r\n```python\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nc = DecisionTreeClassifier().fit([[1,2]], [1])\r\ncl = c.tree_.children_left\r\nprint(cl)\r\ndel c\r\nprint(cl)\r\ncl[0] = 1\r\n```\r\n\r\noutputs\r\n```\r\n[-1]\r\n[-849416680]\r\nSegmentation fault (core dumped)\r\n```"""
2721,25105520,ogrisel,ogrisel,2014-01-06 14:50:14,2014-07-15 16:29:45,2014-01-08 15:05:47,closed,ogrisel,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2721,b'Non-deterministic JoblibIndexError in sklearn.ensemble.tests.test_forest.test_parallel_train',b'As seen on https://travis-ci.org/scikit-learn/scikit-learn/builds/16461795\r\n\r\n`BaseEstimator.get_params` handling of the warnings registry is not thread safe and that breaks the new threading-backed parallelization of random forests.'
2716,25074429,amueller,amueller,2014-01-05 17:40:13,2014-07-15 16:29:45,2014-01-07 16:30:45,closed,,,13,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2716,b'CCA test error in master',"b'```\r\n======================================================================\r\nERROR: check_transformer(WardAgglomeration)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/andy/checkout/scikit-learn/sklearn/tests/test_common.py"", line 233, in check_transformer\r\n    transformer.fit(X, y_)\r\n  File ""/home/andy/checkout/scikit-learn/sklearn/cross_decomposition/pls_.py"", line 333, in fit\r\n    linalg.inv(np.dot(self.y_loadings_.T, self.y_weights_)))\r\n  File ""/home/andy/.local/lib/python2.7/site-packages/scipy-0.11.0-py2.7-linux-x86_64.egg/scipy/linalg/basic.py"", line 348, in inv\r\n    raise LinAlgError(""singular matrix"")\r\nLinAlgError: singular matrix\r\n```'"
2713,25035466,venuktan,arjoly,2014-01-03 20:59:14,2014-04-25 20:42:00,2014-04-25 20:42:00,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2713,b'joblib load model from disk error',"b'I am building my model and storing it at a location using \r\n` joblib.dump(model, modelStoreLocation, compress=9) `\r\n\r\nThen in a separate execution I am trying to read the model from the location as show below : \r\n`  #  loading the model stored on disk\r\n    model = joblib.load(modelStoreLocation)`\r\n\r\nI get the following error : \r\n```Traceback (most recent call last):\r\n  File ""/Users/venuktangirala/PycharmProjects/nFlate/compute/prediction/categories/logisticRegression.py"", line 84, in <module>x\r\n    prediction = predictDataFrame(dataFrame.ix[600:623, :-1].values, modelStoreLocation)\r\n  File ""/Users/venuktangirala/PycharmProjects/nFlate/compute/prediction/categories/logisticRegression.py"", line 58, in predictDataFrame\r\n    prediction = model.predict(dataToPredict)\r\n  File ""/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.py"", line 223, in predict\r\n    scores = self.decision_function(X)\r\n  File ""/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.py"", line 207, in decision_function\r\n    dense_output=True) + self.intercept_\r\n  File ""/anaconda/lib/python2.7/site-packages/sklearn/utils/extmath.py"", line 83, in safe_sparse_dot\r\n    return np.dot(a, b)\r\nTypeError: can\'t multiply sequence by non-int of type \'float\'`\r\n```\r\nThe same works with out any problem when its in the same execution i.e. build the model , write to dsik and read from disk in the same execution. \r\n\r\nWhat is the fix for this ? '"
2697,24874349,kespindler,GaelVaroquaux,2013-12-30 05:44:53,2014-01-03 06:10:10,2014-01-03 06:10:10,closed,,,3,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2697,b'copy parameter of Imputer seems to not work',"b""I'm on sklearn 0.14.1, and have the following issue.\r\n\r\nI created a sklearn.preprocessing.Imputer\r\n\r\ni = Imputer(copy=False)\r\na=np.random.random((5,5))\r\na[0,0] = np.nan\r\ni.fit(a)\r\na2 = i.transform(a)\r\n\r\nBecause of `copy = False`, I'd expect\r\n\r\n1. `id(a2) == id(a)`\r\n2. the nan in `a` to be cleared.\r\n\r\nHowever, neither occurs."""
2696,24858939,moravveji,larsmans,2013-12-29 11:48:56,2014-06-03 10:59:01,2014-06-03 10:59:01,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2696,b'Failed test after installation',"b'Hello.\r\nI am trying to install scikit on MacBook Pro Lion 10.7. However, the test fails with this message:\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.cluster.bicluster.tests.test_utils.test_get_submatrix\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Library/Python/2.7/site-packages/sklearn/cluster/bicluster/tests/test_utils.py"", line 43, in test_get_submatrix\r\n    assert_true(np.all(X != -1))\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 422, in assertTrue\r\n    if not expr:\r\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/base.py"", line 183, in __bool__\r\n    raise ValueError(""The truth value of an array with more than one ""\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().\r\n\r\n----------------------------------------------------------------------\r\nRan 1720 tests in 104.398s\r\n\r\nFAILED (SKIP=15, errors=1)\r\n```\r\n\r\nHow can I fix the issue?\r\n\r\nRegards.'"
2686,24746130,amueller,larsmans,2013-12-24 13:07:27,2014-07-15 16:29:45,2013-12-28 00:59:16,closed,,,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2686,b'Minibatch k-means does n_init initialization runs when given explicit cluster centers',"b'Even when ``init`` is an array, the initialization is carried out ``n_init`` times, obviously with identical results. That is pretty weird.'"
2663,24241424,ogrisel,agramfort,2013-12-13 11:37:09,2014-07-15 16:29:45,2013-12-13 20:08:11,closed,,,14,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2663,b'RandomizedPCA.explained_variance_ratio_ is false',"b'As emphasized by [this question on stackoverflow](http://stackoverflow.com/questions/20563239/truncatedsvd-explained-variance/20564849) `RandomizedPCA.explained_variance_ratio_` is false as it cannot cheaply estimate the total variance of the input data.\r\n\r\nI did a [notebook to check that this is actually a bug](http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/Explained%20variances.ipynb).\r\n\r\nI think we should:\r\n\r\n1- plainly remove `explained_variance_ratio_` from the `RandomizedPCA` class (and document the removal in `whats_new.rst`)\r\n2- write an example that demonstrates how to compute the explained variance empirically for `PCA`, `RandomizedPCA`, `TruncatedSVD` with plots similar to my notebook\r\n3- add a new section to the narrative documentation of the `TruncatedSVD` model to explain that and include the plot.\r\n\r\nLet me put that as Easy fix suitable for first time contributor.'"
2652,24014222,nik90,arjoly,2013-12-10 05:15:32,2014-08-11 07:33:23,2014-08-11 07:33:13,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2652,b'twentynewsgroups dataset fetch fail with incomplete download',"b'I am running python 2.7.3, using enthought canopy and am having issues with fetching the twenty news groups dataset. It says empty file when I try using the code provided in the example on the following page:\r\n\r\nhttp://scikit-learn.org/stable/datasets/twenty_newsgroups.html\r\n\r\nThe first two lines of the example don\'t work and it throws the following error:\r\n\r\nBasically it\'s a ReadError because it has empty input from the twentynewsgroups as far as I can tell but I have no idea how to fix it.\r\n\r\n---------------------------------------------------------------------------\r\nReadError                                 Traceback (most recent call last)\r\n<ipython-input-1-f47a4e49b23d> in <module>()\r\n      1 from sklearn.datasets import fetch_20newsgroups\r\n----> 2 newsgroups_train = fetch_20newsgroups(subset=\'train\')\r\n\r\n/Users/Nikhil/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/datasets/twenty_newsgroups.pyc in fetch_20newsgroups(data_home, subset, categories, shuffle, random_state, remove, download_if_missing)\r\n    205         if download_if_missing:\r\n    206             cache = download_20newsgroups(target_dir=twenty_home,\r\n--> 207                                           cache_path=cache_path)\r\n    208         else:\r\n    209             raise IOError(\'20Newsgroups dataset not found\')\r\n\r\n/Users/Nikhil/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/datasets/twenty_newsgroups.pyc in download_20newsgroups(target_dir, cache_path)\r\n     87 \r\n     88     logger.info(""Decompressing %s"", archive_path)\r\n---> 89     tarfile.open(archive_path, ""r:gz"").extractall(path=target_dir)\r\n     90     os.remove(archive_path)\r\n     91 \r\n\r\n/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/tarfile.pyc in open(cls, name, mode, fileobj, bufsize, **kwargs)\r\n   1676             else:\r\n   1677                 raise CompressionError(""unknown compression type %r"" % comptype)\r\n-> 1678             return func(name, filemode, fileobj, **kwargs)\r\n   1679 \r\n   1680         elif ""|"" in mode:\r\n\r\n/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/tarfile.pyc in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\r\n   1725             t = cls.taropen(name, mode,\r\n   1726                 gzip.GzipFile(name, mode, compresslevel, fileobj),\r\n-> 1727                 **kwargs)\r\n   1728         except IOError:\r\n   1729             raise ReadError(""not a gzip file"")\r\n\r\n/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/tarfile.pyc in taropen(cls, name, mode, fileobj, **kwargs)\r\n   1703         if len(mode) > 1 or mode not in ""raw"":\r\n   1704             raise ValueError(""mode must be \'r\', \'a\' or \'w\'"")\r\n-> 1705         return cls(name, mode, fileobj, **kwargs)\r\n   1706 \r\n   1707     @classmethod\r\n\r\n/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/tarfile.pyc in __init__(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel)\r\n   1572             if self.mode == ""r"":\r\n   1573                 self.firstmember = None\r\n-> 1574                 self.firstmember = self.next()\r\n   1575 \r\n   1576             if self.mode == ""a"":\r\n\r\n/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/tarfile.pyc in next(self)\r\n   2332             except EmptyHeaderError:\r\n   2333                 if self.offset == 0:\r\n-> 2334                     raise ReadError(""empty file"")\r\n   2335             except TruncatedHeaderError, e:\r\n   2336                 if self.offset == 0:\r\n\r\nReadError: empty file\r\n\r\n\r\n'"
2645,23928245,zilvinasu,ogrisel,2013-12-08 18:51:22,2014-07-15 16:29:45,2014-01-15 15:36:20,closed,,,6,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2645,"b""Python 3 'bytes' object has no attribute 'encode'""","b'When trying to execute fetch_20newsgroups(subset=\'all\') while being in Python 3.3 environment I get the following error:\r\n```\r\nDownloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\r\nTraceback (most recent call last):\r\n  File ""/Users/hop/VirtualEnvs/data_vis/lib/python3.3/site-packages/IPython/core/interactiveshell.py"", line 2828, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File ""<ipython-input-42-bf8e77d5e59e>"", line 1, in <module>\r\n    news = fetch_20newsgroups(subset=\'all\')\r\n  File ""/Users/hop/VirtualEnvs/data_vis/lib/python3.3/site-packages/sklearn/datasets/twenty_newsgroups.py"", line 207, in fetch_20newsgroups\r\n    cache_path=cache_path)\r\n  File ""/Users/hop/VirtualEnvs/data_vis/lib/python3.3/site-packages/sklearn/datasets/twenty_newsgroups.py"", line 95, in download_20newsgroups\r\n    open(cache_path, \'wb\').write(pickle.dumps(cache).encode(\'zip\'))\r\nAttributeError: \'bytes\' object has no attribute \'encode\'\r\n```\r\n\r\nThe problem is that the value is bytes, and in Python 3 bytes can not be encoded. They can only be decoded.  \r\n\r\n\r\n'"
2640,23842322,arendu,amueller,2013-12-06 07:35:08,2015-02-24 22:41:40,2015-02-24 22:41:40,closed,,0.16,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2640,b'leading minor not positive definite',"b'I seem to get this error at random times while training a GaussianHMM.\r\n\r\nIs this because of underflow?\r\n\r\n```\r\n  File ""train_states.py"", line 156, in <module>\r\n    best_seq = model.decode(t)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.13.1-py2.7-macosx-10.7-intel.egg/sklearn/hmm.py"", line 304, in decode\r\n    logprob, state_sequence = decoder[algorithm](obs)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.13.1-py2.7-macosx-10.7-intel.egg/sklearn/hmm.py"", line 241, in _decode_viterbi\r\n    framelogprob = self._compute_log_likelihood(obs)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.13.1-py2.7-macosx-10.7-intel.egg/sklearn/hmm.py"", line 743, in _compute_log_likelihood\r\n    obs, self._means_, self._covars_, self._covariance_type)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.13.1-py2.7-macosx-10.7-intel.egg/sklearn/mixture/gmm.py"", line 56, in log_multivariate_normal_density\r\n    X, means, covars)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.13.1-py2.7-macosx-10.7-intel.egg/sklearn/mixture/gmm.py"", line 603, in _log_multivariate_normal_density_full\r\n    lower=True)\r\n  File ""/src/scipy/scipy/linalg/decomp_cholesky.py"", line 66, in cholesky\r\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True)\r\n  File ""/src/scipy/scipy/linalg/decomp_cholesky.py"", line 24, in _cholesky\r\n    raise LinAlgError(""%d-th leading minor not positive definite"" % info)\r\nnumpy.linalg.linalg.LinAlgError: 2-th leading minor not positive definite\r\n```'"
2638,23832107,amueller,GaelVaroquaux,2013-12-06 01:35:41,2014-07-15 16:29:45,2014-07-14 18:49:09,closed,,,54,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2638,b'MRG Minibatch reassignment fixes',b'This should address the remaining issues of #2185 including #2611 (which I only saw after writing this).\r\nI still have to merge #2185 for the tests to pass. I thought about using sampling from np.random instead of coding up a new function to do multinomial sampling with replacement (which should exist).'
2625,23517311,tancorko,ogrisel,2013-11-30 08:29:06,2014-07-15 16:29:45,2014-04-02 17:11:27,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2625,b'sklearn.cluster.tests.test_spectral.test_spectral_lobpcg_mode',"b'Ubuntu 13.10\r\n\r\n```\r\nERROR: sklearn.cluster.tests.test_spectral.test_spectral_lobpcg_mode\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/cluster/tests/test_spectral.py"", line 66, in test_spectral_lobpcg_mode\r\n    random_state=0, eigen_solver=""lobpcg"")\r\n  File ""/usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/cluster/spectral.py"", line 268, in spectral_clustering\r\n    eigen_tol=eigen_tol, drop_first=False)\r\n  File ""/usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/manifold/spectral_embedding_.py"", line 303, in spectral_embedding\r\n    largest=False, maxiter=2000)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/sparse/linalg/eigen/lobpcg/lobpcg.py"", line 405, in lobpcg\r\n    activeBlockVectorBP, retInvR = True )\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/sparse/linalg/eigen/lobpcg/lobpcg.py"", line 142, in b_orthonormalize\r\n    gramVBV = sla.cholesky( gramVBV )\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/linalg/decomp_cholesky.py"", line 80, in cholesky\r\n    check_finite=check_finite)\r\n  File ""/usr/lib/python2.7/dist-packages/scipy/linalg/decomp_cholesky.py"", line 30, in _cholesky\r\n    raise LinAlgError(""%d-th leading minor not positive definite"" % info)\r\nLinAlgError: 2-th leading minor not positive definite\r\n\r\n----------------------------------------------------------------------\r\nRan 1720 tests in 106.863s\r\n\r\nFAILED (SKIP=14, errors=1)\r\n```'"
2621,23485998,arjoly,jaquesgrobler,2013-11-29 10:54:05,2013-11-29 16:00:24,2013-11-29 15:54:26,closed,,,8,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2621,b'Dead links in documentation ',"b'From http://scikit-learn.org/stable/documentation.html,\r\nI click on [scikit-learn 0.15 (development)](http://scikit-learn.org/0.15/user_guide.html)\r\nand got\r\n\r\n```\r\nAn error has been encountered in accessing this page.\r\n\r\n1. Server: scikit-learn.org \r\n2. URL path: /0.15/user_guide.html \r\n3. Error notes: NONE \r\n4. Error type: 404 \r\n5. Request method: GET \r\n6. Request query string: NONE \r\n7. Time: 2013-11-29 10:52:13 UTC (1385722333)\r\n```\r\n\r\nFrom http://scikit-learn.org/dev/documentation.html, I click on \r\n[scikit-learn 0.14 (stable)](http://scikit-learn.org/0.14/user_guide.html) and got\r\n\r\n```\r\nAn error has been encountered in accessing this page.\r\n\r\n1. Server: scikit-learn.org \r\n2. URL path: /0.14/user_guide.html \r\n3. Error notes: NONE \r\n4. Error type: 404 \r\n5. Request method: GET \r\n6. Request query string: NONE \r\n7. Time: 2013-11-29 10:53:20 UTC (1385722400)\r\n```'"
2609,23188594,jakevdp,larsmans,2013-11-23 15:26:54,2014-09-02 07:38:49,2014-08-25 15:56:01,closed,,0.16,5,API;Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2609,b'BUG: KNeighborsClassifier & KNeighborsRegressor **kwargs',"b'``KNeighborsClassifier`` and ``KNeighborsRegressor`` (and any others?) handle unspecified ``**kwargs`` in their initialization: this means they will not work correctly with ``clone()`` in, e.g. cross validation.  The API needs to be changed so that kwargs are passed explicitly as a dictionary.\r\n\r\nThe good news is that these keyword arguments are used only for obscure metrics, so there is likely very little code out in the wild that uses the current problematic API.'"
2604,23063072,jaquesgrobler,jnothman,2013-11-21 12:25:45,2014-12-28 13:58:00,2014-12-28 13:57:43,closed,,0.16,8,Bug;Documentation;Enhancement,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2604,b'Fixes and tweaks for scikit-learn example gallery',"b""Recently, the [Nilearn website](http://nilearn.github.io/index.html) received a good update (Congrats again :+1: )\r\nI did a redo of the docstring-popup display for the [examples gallery]( http://nilearn.github.io/auto_examples/index.html) - which uses no javascript, cleaner and more stable than the current example.\r\n\r\nThe current implementation of the popup docstrings on the [scikit-learn examples gallery](http://scikit-learn.org/dev/auto_examples/index.html) looks nice enough but takes a bit of time to initialise due to waiting for other javascripts and can behave buggily if you swoosh the mouse about fast enough. \r\n\r\nSo I propose to change the scikit-learn examples gallery pop-ups to match that of the nilearn site.\r\nOne thing though is that using the exact same implementation as in nilearn means not having the larger thumbnail images appear anymore as @ogrisel mentioned.\r\nElse I could try adding images to the nilearn gallery version.\r\n\r\nLet me know what you think.\r\n\r\n-------------------------------------\r\nOn a side-note, there is some general cleanup of gen_rst that I'll make a PR for anyway. I can either do that separately or together with the above\r\n"""
2588,22642780,mblondel,amueller,2013-11-14 04:50:32,2015-04-02 15:15:56,2015-04-02 15:15:56,closed,,0.16,15,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2588,b'make_scorer needs_threshold makes wrong assumptions',"b""I found two issues with `make_scorer`'s `needs_threshold` option.\r\n\r\nFirst, it doesn't work if the base estimator is a regressor. Using a regressor is a perfectly valid use case, e.g., if you want do use a regressor but still want to optimize your hyper-parameters against AUC (pointwise ranking with two relevance levels). It does work with `Ridge` but this is because `decision_function` and `predict` are aliases of each others. See also the discussion in issue #1404.\r\n\r\nSecond, `_ThresholdScorer` checks that the number of unique values in `y_true` is 2 but this need not be the case. For example, I can use a regressor and optimize my hyper-parameters against NDCG with more than 2 relevance levels."""
2586,22610124,mblondel,ogrisel,2013-11-13 18:10:19,2013-12-05 09:42:16,2013-12-05 09:42:16,closed,,0.15,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2586,b'invalid warning in recall_score',"b'```\r\nIn [1]: from sklearn.metrics import recall_score\r\n\r\nIn [2]: recall_score([0, 1, 0, 1], [0, 0, 0, 0])\r\n/Users/mblondel/Desktop/projects/scikit-learn/sklearn/metrics/metrics.py:1550: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples\r\nOut[2]: 0.0\r\n```\r\nrecall = TP / (TP + FN)\r\nHere TP = 0 and FN = 2 so the result should be 0 without warning.'"
2560,21824226,agramfort,glouppe,2013-10-30 13:24:09,2013-11-06 09:29:15,2013-11-06 09:29:15,closed,,0.15,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2560,b'Imputer bug with median and dense input',"b""this code returns:\r\n\r\n```\r\nimport numpy as np\r\nfrom  scipy import sparse\r\nfrom sklearn.preprocessing import Imputer\r\n\r\nX = np.random.randn(100, 2)\r\nX[::2] = np.nan\r\n\r\nfor strategy in ['mean', 'median', 'most_frequent']:\r\n    imputer = Imputer(strategy=strategy)\r\n    X_ = imputer.fit_transform(X)\r\n    print X.shape, X_.shape\r\n\r\n    X_ = imputer.fit_transform(sparse.csr_matrix(X))\r\n    print X.shape, X_.shape\r\n```\r\n\r\n(100, 2) (100, 2)\r\n(100, 2) (100, 2)\r\n(100, 2) (100, 1)\r\n(100, 2) (100, 2)\r\n(100, 2) (100, 2)\r\n(100, 2) (100, 2)\r\n\r\nso returns (100, 1) with dense + median"""
2555,21672975,sbos,amueller,2013-10-28 06:54:31,2015-03-10 15:18:55,2015-03-10 15:18:55,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2555,b'Variational lower bound for DPGMM is a big _positive_ number',"b""Running this tutorial http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html with verbose=True for DPGMM outputs large positive numbers for variational lower bound:\r\n\r\n```\r\nBound after updating        z: -1934845.442168\r\nBound after updating    gamma: -1934150.091939\r\nBound after updating       mu: -1933891.978692\r\nBound after updating  a and b: 3003.417879\r\nCluster proportions: [ 284.25505063  426.87263604  180.14959307   74.85601527   38.866705  ]\r\ncovariance_type: full\r\nBound after updating        z: 3094.047358\r\nBound after updating    gamma: 3132.540483\r\nBound after updating       mu: 3144.841074\r\nBound after updating  a and b: 3298.298596\r\nCluster proportions: [ 316.48356438  465.75414501  165.93268389   36.67171582   20.1578909 ]\r\ncovariance_type: full\r\nBound after updating        z: 3330.760586\r\nBound after updating    gamma: 3362.603689\r\nBound after updating       mu: 3368.416896\r\nBound after updating  a and b: 3495.014542\r\nCluster proportions: [ 348.75634361  487.06484756  144.80697717   15.30981726    9.0620144 ]\r\ncovariance_type: full\r\nBound after updating        z: 3514.459661\r\nBound after updating    gamma: 3541.721937\r\nBound after updating       mu: 3546.879614\r\nBound after updating  a and b: 3642.306983\r\nCluster proportions: [ 381.67272897  496.40387426  117.34984623    5.90770255    3.66584799]\r\ncovariance_type: full\r\nBound after updating        z: 3656.457600\r\nBound after updating    gamma: 3682.143803\r\nBound after updating       mu: 3689.276215\r\nBound after updating  a and b: 3769.402638\r\nCluster proportions: [ 414.52676328  499.93888129   86.6206661     2.36039212    1.55329721]\r\ncovariance_type: full\r\nBound after updating        z: 3781.287023\r\nBound after updating    gamma: 3804.167230\r\nBound after updating       mu: 3807.623950\r\nBound after updating  a and b: 3878.837565\r\nCluster proportions: [ 443.86058627  501.1003689    57.50749592    1.48547081    1.0460781 ]\r\ncovariance_type: full\r\nBound after updating        z: 3888.054913\r\nBound after updating    gamma: 3906.591947\r\nBound after updating       mu: 3906.929589\r\nBound after updating  a and b: 3972.715677\r\nCluster proportions: [ 467.0884954   501.20938212   34.51950003    1.17563443    1.00698802]\r\ncovariance_type: full\r\nBound after updating        z: 3978.617530\r\nBound after updating    gamma: 3991.408226\r\nBound after updating       mu: 3991.524146\r\nBound after updating  a and b: 4041.321644\r\nCluster proportions: [ 482.65015341  501.19551132   19.12587956    1.02337676    1.00507895]\r\ncovariance_type: full\r\nBound after updating        z: 4045.006598\r\nBound after updating    gamma: 4052.649227\r\nBound after updating       mu: 4052.689962\r\nBound after updating  a and b: 4086.540100\r\nCluster proportions: [ 491.73006778  501.18681869   10.06591119    1.01269412    1.00450823]\r\ncovariance_type: full\r\nBound after updating        z: 4088.833888\r\nBound after updating    gamma: 4093.105416\r\nBound after updating       mu: 4093.119257\r\nBound after updating  a and b: 4114.720741\r\nCluster proportions: [ 496.65513603  501.18152274    5.14740841    1.01165637    1.00427645]\r\ncovariance_type: full\r\n```\r\n\r\nSince lower bound is calculated in log-scale it shouldn't be larger that 0, so it seems like a bug."""
2539,21286281,amueller,amueller,2013-10-20 20:00:02,2013-12-28 06:46:03,2013-12-28 06:45:55,closed,,,17,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2539,b'Test failures on master on my box',"b'```\r\nERROR: Check fast dot blas wrapper function\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/andy/checkout/scikit-learn/sklearn/utils/tests/test_extmath.py"", line 325, in test_fast_dot\r\n    assert_true(isinstance(w.pop(-1).message, NonBLASDotWarning))\r\nIndexError: pop from empty list\r\n\r\n\r\nFAIL: sklearn.tests.test_common.test_transformers\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/andy/checkout/scikit-learn/sklearn/tests/test_common.py"", line 251, in test_transformers\r\n    assert_true(succeeded)\r\nAssertionError: False is not true\r\n    \'False is not true\' = self._formatMessage(\'False is not true\', ""%s is not true"" % safe_repr(False))\r\n>>  raise self.failureException(\'False is not true\')\r\n```'"
2512,20833664,apiszcz,larsmans,2013-10-10 20:35:42,2014-04-06 15:21:32,2014-04-06 15:21:09,closed,,0.15,1,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2512,"b""0.14 documentation build error 'htmlhelp' utf_8.py""","b'```\r\n# Sphinx version: 1.1.3\r\n# Python version: 2.7.4\r\n# Docutils version: 0.10 release\r\n# Jinja2 version: 2.6\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/sphinx/cmdline.py"", line 189, in main\r\n    app.build(force_all, filenames)\r\n  File ""/usr/lib/python2.7/dist-packages/sphinx/application.py"", line 209, in build\r\n    self.emit(\'build-finished\', None)\r\n  File ""/usr/lib/python2.7/dist-packages/sphinx/application.py"", line 314, in emit\r\n    results.append(callback(self, *args))\r\n  File ""/mnt/hgfs/E/p/pythonlib/doc/scikit-learn/doc/sphinxext/gen_rst.py"", line 1035, in embed_code_links\r\n    line = line.decode(\'utf-8\')\r\n  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: \'utf8\' codec can\'t decode byte 0xeb in position 33: invalid continuation byte\r\n```'"
2511,20827665,apiszcz,amueller,2013-10-10 18:57:20,2015-03-18 15:06:15,2015-03-18 14:29:17,closed,,0.16,14,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2511,b'0.14 documentation build error message searchindex.js',"b'```\r\n# Sphinx version: 1.1.3\r\n# Python version: 2.7.4\r\n# Docutils version: 0.10 release\r\n# Jinja2 version: 2.6\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/sphinx/cmdline.py"", line 189, in main\r\n    app.build(force_all, filenames)\r\n  File ""/usr/lib/python2.7/dist-packages/sphinx/application.py"", line 209, in build\r\n    self.emit(\'build-finished\', None)\r\n  File ""/usr/lib/python2.7/dist-packages/sphinx/application.py"", line 314, in emit\r\n    results.append(callback(self, *args))\r\n  File ""/mnt/hgfs/E/p/pythonlib/doc/scikit-learn/doc/sphinxext/gen_rst.py"", line 980, in embed_code_links\r\n    relative=True)\r\n  File ""/mnt/hgfs/E/p/pythonlib/doc/scikit-learn/doc/sphinxext/gen_rst.py"", line 212, in __init__\r\n    sindex = get_data(searchindex_url)\r\n  File ""/mnt/hgfs/E/p/pythonlib/doc/scikit-learn/doc/sphinxext/gen_rst.py"", line 71, in get_data\r\n    with open(url, \'r\') as fid:\r\nIOError: [Errno 2] No such file or directory: \'/mnt/hgfs/E/p/pythonlib/doc/scikit-learn/doc/_build/htmlhelp/searchindex.js\'\r\n```'"
2507,20774554,Felix-neko,amueller,2013-10-09 22:03:04,2015-03-23 15:56:26,2015-03-23 15:56:26,closed,,0.16,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2507,b'IsotonicRegression gives NANs on normal data',"b'I have a problem with IsotonicRegression: it gives some NANs in case of fitting some data with values close to zero (but greater than sys.float_info.min).\r\n\r\nI pickled some breaking data and uploaded them to SendSpace: \r\n\r\nhttp://www.sendspace.com/file/0i18ib\r\n\r\nAnd below is the crashing example.\r\n\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\n\r\nimport pickle\r\n\r\n[xArray, yArray, weightArray, pPredicted] = pickle.load(open(""bugreport.dmp"", \'r\'))\r\n\r\n#xArray and yArray are the raw data I want to fit, weightArray are the sample weights. There are no NANs among them.\r\n\r\nimport sklearn\r\nprint sklearn.__version__ #SKLEARN v. 0.14.1\r\n\r\nimport sklearn.isotonic\r\n\r\nregression = sklearn.isotonic.IsotonicRegression()\r\n\r\nregression.fit(xArray, yArray, sample_weight=weightArray)\r\nprint regression.predict(xArray) # Oh no! It gives some NANs!\r\n\r\n```'"
2503,20736100,jaquesgrobler,ogrisel,2013-10-09 11:21:04,2013-10-21 07:18:34,2013-10-21 07:18:20,closed,,0.15,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2503,b'BUG: coveralls skips `test_common`',b'he `test_common` tests apparently fail to introspect the list of estimators in the package.\r\n\r\nDiscussed in #2495 '
2496,20671496,pprett,larsmans,2013-10-08 10:53:40,2013-10-13 14:01:06,2013-10-13 13:58:19,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2496,b'SGD bug - coef_ layout wrong if partial_fit happens after fit',"b""SGDClassifier.fit calls set_coef which transforms the coef array into fortran layout for faster prediction time. Partial fit does not do this (thus slower at prediction).\r\nPartial fit does not transform from fortran to c in the first place which leads to an exception if you call partial_fit after fit for a multi-class problem.\r\n\r\nFixes: Either transform fortran back to c at the beginning of partial_fit or don't assume c-style arrays in plain_sgd but rather memory views."""
2481,20202133,kmike,ogrisel,2013-09-28 00:17:06,2013-10-16 12:24:23,2013-10-16 12:24:23,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2481,"b""LabelEncoder doesn't work correctly for unicode labels in Python 2.6 + numpy 1.3""",b'LabelEncoder works incorrectly for unicode labels in Python 2.6 + numpy 1.3. This is currently untested; to reproduce replace bytestrings with unicode strings here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/preprocessing/tests/test_label.py#L191\r\n\r\nThis is the cause of Jenkins failure that https://github.com/scikit-learn/scikit-learn/pull/2462 triggered.\r\n'
2472,19872538,chyikwei,ogrisel,2013-09-22 13:07:26,2014-07-29 09:36:16,2014-07-29 09:36:16,closed,,0.15.1,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2472,b'test_spectral_clustering_sparse fail on OS X 10.8.2',"b'I got this error on sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse\r\nAny one knows what might be wrong?\r\n\r\nOS: Max OS X 10.8.2 \r\nnumpy: 1.7.1\r\nscipy: 0.11.0\r\n\r\n<pre><code>\r\n======================================================================\r\nFAIL: sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/chyikwei/getglue-python-webservice/virtual/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/chyikwei/scikit-learn/sklearn/cluster/tests/test_spectral.py"", line 155, in test_spectral_clustering_sparse\r\n    assert_greater(np.mean(labels == [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]), .89)\r\nAssertionError: 0.59999999999999998 not greater than 0.89\r\n    """"""Fail immediately, with the given message.""""""\r\n>>  raise self.failureException(\'0.59999999999999998 not greater than 0.89\')\r\n</code></pre>'"
2467,19854829,rsivapr,larsmans,2013-09-21 10:25:48,2014-02-19 16:24:02,2014-02-19 16:24:02,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2467,b'Multilabel Classification with Feature Selection (RuntimeWarning and MemoryError)',"b'I am using scikit-learn to solving a multi-label classification problem with a large number of labels. I [followed the ideas][1] from @larsmans. It gives me a runtime warning and then eventually a memory error.\r\n\r\n    >>> clf = Pipeline([(\'chi2\', SelectKBest(chi2, k=1000)),(\'svm\', LinearSVC())])\r\n    >>> \r\n    >>> multiclf = OneVsRestClassifier(clf, n_jobs=-1)\r\n    >>> \r\n    >>> multiclf.fit(Xtr, ytr)\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157: \r\n    RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157: \r\n    RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157:             \r\n    RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n\r\nAnd then after multiple such warnings, it fails: The error dump below\r\n\r\n    Process PoolWorker-21:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Exception in thread Thread-1:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/threading.py"", line 808, in __bootstrap_inner\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/threading.py"", line 761, in run\r\n        self.__target(*self.__args, **self.__kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 325, in _handle_workers\r\n        pool._maintain_pool()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 229, in _maintain_pool\r\n        self._repopulate_pool()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 222, in _repopulate_pool\r\n        w.start()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 130, in start\r\n        self._popen = Popen(self)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/forking.py"", line 121, in __init__\r\n        self.pid = os.fork()\r\n    OSError: [Errno 12] Cannot allocate memory\r\n\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157: RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n    Process PoolWorker-22:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-23:\r\n    Process PoolWorker-30:\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157: RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n    Process PoolWorker-31:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-28:\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-32:\r\n    Process PoolWorker-29:\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-27:\r\n    Process PoolWorker-26:\r\n    Traceback (most recent call last):\r\n    Process PoolWorker-25:\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self.run()\r\n        self.run()\r\n        self.run()\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self._target(*self._args, **self._kwargs)\r\n        self._target(*self._args, **self._kwargs)\r\n        self._target(*self._args, **self._kwargs)\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n        return recv()\r\n        return recv()\r\n    MemoryError\r\n    MemoryError\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        return recv()\r\n        return recv()\r\n    MemoryError\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n    Process PoolWorker-24:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: invalid load key, \'k\'.\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-1:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-2:\r\n    Traceback (most recent call last):\r\n    Process PoolWorker-3:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-4:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-6:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n    Process PoolWorker-5:\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-7:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: unpickling stack underflow\r\n    Process PoolWorker-8:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: invalid load key, \'\x01\'.\r\n    Process PoolWorker-10:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: invalid load key, \'\'.\r\n    Process PoolWorker-21:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Exception in thread Thread-1:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/threading.py"", line 808, in __bootstrap_inner\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/threading.py"", line 761, in run\r\n        self.__target(*self.__args, **self.__kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 325, in _handle_workers\r\n        pool._maintain_pool()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 229, in _maintain_pool\r\n        self._repopulate_pool()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 222, in _repopulate_pool\r\n        w.start()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 130, in start\r\n        self._popen = Popen(self)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/forking.py"", line 121, in __init__\r\n        self.pid = os.fork()\r\n    OSError: [Errno 12] Cannot allocate memory\r\n\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157: RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n    Process PoolWorker-22:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-23:\r\n    Process PoolWorker-30:\r\n    /home/rsivapr/scikit-learn/sklearn/feature_selection/univariate_selection.py:157: RuntimeWarning: invalid value encountered in divide\r\n      chisq /= f_exp\r\n    Process PoolWorker-31:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-28:\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-32:\r\n    Process PoolWorker-29:\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-27:\r\n    Process PoolWorker-26:\r\n    Traceback (most recent call last):\r\n    Process PoolWorker-25:\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Traceback (most recent call last):\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self.run()\r\n        self.run()\r\n        self.run()\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self._target(*self._args, **self._kwargs)\r\n        self._target(*self._args, **self._kwargs)\r\n        self._target(*self._args, **self._kwargs)\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n        return recv()\r\n        return recv()\r\n    MemoryError\r\n    MemoryError\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        return recv()\r\n        return recv()\r\n    MemoryError\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n    Process PoolWorker-24:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: invalid load key, \'k\'.\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-1:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-2:\r\n    Traceback (most recent call last):\r\n    Process PoolWorker-3:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-4:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n    Process PoolWorker-6:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n    Process PoolWorker-5:\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n        return recv()\r\n    MemoryError\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    MemoryError\r\n    Process PoolWorker-7:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: unpickling stack underflow\r\n    Process PoolWorker-8:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: invalid load key, \'\x01\'.\r\n    Process PoolWorker-10:\r\n    Traceback (most recent call last):\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap\r\n        self.run()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/process.py"", line 114, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/pool.py"", line 102, in worker\r\n        task = get()\r\n      File ""/home/rsivapr/anaconda/lib/python2.7/multiprocessing/queues.py"", line 376, in get\r\n        return recv()\r\n    UnpicklingError: invalid load key, \'\'.\r\n\r\n\r\n\r\n  [1]: http://stackoverflow.com/questions/16400722/feature-selection-for-multilabel-classification-scikit-learn'"
2445,19549184,untom,amueller,2013-09-16 13:07:58,2015-01-13 23:44:38,2015-01-13 23:44:38,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2445,b'sklearn.metrics.consensus_score potentially gives wrong results',"b'Hi!\r\n\r\n`sklearn.metrics.consensus_score()` gives wrong scores if the two results to be compared contain different numbers of biclusters. This is because the function contains as its final line:\r\n\r\n    return np.trace(matrix[:, indices[:, 1]]) / max(n_a, n_b)\r\n\r\nwhich uses `np.trace` under the assumption that `matrix` (the similarity matrix) is square, and thus contains the most similar items in its diagonal. \r\n\r\nHowever, when `matrix` is non-square (i.e., `n_b != n_a` in the code), this fails. I have an example dataset that shows such a case, deposited under: https://www.dropbox.com/sh/plmsqof84xhtxry/7lIrdvX0mp . Just use:\r\n\r\n\r\n    import sklearn.metrics\r\n    a_rows = np.loadtxt(""/home/tom/a_rows.txt"")\r\n    a_cols = np.loadtxt(""/home/tom/a_cols.txt"")\r\n    b_rows = np.loadtxt(""/home/tom/b_rows.txt"")\r\n    b_cols = np.loadtxt(""/home/tom/b_cols.txt"")\r\n    print sklearn.metrics.consensus_score((a_rows, a_cols), (b_rows, b_cols))\r\n\r\nThis gives a consensus-score of ~0.328, however the real score should be ~0.529\r\n\r\nThe bug can be fixed by exchanging the last line of the function to:\r\n\r\n    return matrix[indices[:, 0], indices[:, 1]].sum() / max(n_a, n_b)\r\n\r\n(I can send a pull request if necessary, however since it\'s just a single-line fix I\'m not sure it\'s worth it)'"
2443,19521016,flukeskywalker,larsmans,2013-09-15 15:08:34,2013-10-02 17:11:11,2013-10-02 17:11:11,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2443,b'Incorrect sorting by feature frequency in feature_extraction.text.CountVectorizer._limit_features?',"b""While generating a word cloud using Andreas Muller's word-cloud script, I noticed that when the parameter max_features for CountVectorizer is not None, the topmost common features are not being returned. This does not seem to be the correct behavior, since if I limit to top N features only, the highest frequency features should remain the same.\r\n\r\nIt appears that the problem is in CountVectorizer._limit_features where document frequencies are being used for selecting the top occurring features. I think feature frequencies should be used instead. I am able to get the desired behavior with this modification.\r\n\r\nHere is the diff:\r\n\r\n```\r\n684a685\r\n>         ffs = np.sum(cscmatrix.toarray(), 0)  # feature frequencies instead of document frequencies are needed for max_features\r\n693c694\r\n<             mask_inds = (-ffs[mask]).argsort()[:limit]\r\n---\r\n>             mask_inds = (-wfs[mask]).argsort()[:limit]\r\n```"""
2431,19208185,joernhees,ogrisel,2013-09-09 16:47:28,2014-05-21 16:04:41,2014-05-13 07:18:00,closed,ogrisel,0.15,5,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2431,b'test never finishes: sklearn.decomposition.tests.test_sparse_pca.test_fit_transform',b'I recently installed scikit-learn with ```pip install scikit-learn``` (version 0.14.1).\r\nTrying to run the tests with ```nosetests --exe sklearn -v``` the above mentioned test never seems to finish. Excluding the test with ```-e test_fit_transform``` all other tests seem to pass.\r\nEnvironment is set-up according to http://joernhees.de/blog/2013/06/08/mac-os-x-10-8-scientific-python-with-homebrew/'
2409,18799279,sagar81,amueller,2013-08-30 17:09:03,2015-01-27 20:28:05,2015-01-27 20:28:05,closed,,0.16,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2409,b'Memory error while predicting with KNeighborsClassifier',"b'Following is the piece of code that I wrote to get feature selection using RFE and estimator LinearSVC and then using the reduced data to fit and predict KNeighborClassifier.\r\n\r\n    clf = LinearSVC(C = 10, class_weight = \'auto\')\r\n    rfe = RFE(estimator = clf, n_features_to_select = 700, step = 42)\r\n    rfe.fit(X, trainLabels)\r\n    reduced_train_data = rfe.transform(X)\r\n    print ""reduced_train_data.shape "", reduced_train_data.shape\r\n    reduced_test_data = rfe.transform(test)\r\n    neigh = KNeighborsClassifier(n_neighbors=5, weights=\'distance\', algorithm = \'ball_tree\')\r\n    print ""knn initiated""\r\n    neigh.fit(reduced_train_data, trainLabels)\r\n    print ""knn fitted""\r\n    test_predict = neigh.predict(reduced_test_data)\r\n    print ""knn predicted""\r\n\r\nFollowing is the output: reduced_train_data.shape (42000, 700) \r\nknn initiated \r\nknn fitted\r\n\r\nAnd then I see the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""E:\\Coursera\\KaggleDataProjects\\DigitRecognition\\main.py"", line 74, in <module>\r\n    test_predict = neigh.predict(reduced_test_data)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\neighbors\\classification.py"", line 146, in predict\r\n    neigh_dist, neigh_ind = self.kneighbors(X)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\neighbors\\base.py"", line 313, in kneighbors\r\n    return_distance=return_distance)\r\n  File ""binary_tree.pxi"", line 1295, in sklearn.neighbors.ball_tree.BinaryTree.query (sklearn\\neighbors\\ball_tree.c:9889)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py"", line 91, in array2d\r\n    X_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\core\\numeric.py"", line 320, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nMemoryError\r\n```\r\n\r\nThis error does not happen everytime I run the code by slightly changing the parameter. Can some one confirm whether this is a bug or something else is going on here..\r\n\r\nInitial dimension of train data (X) = 42000, 784 \r\nInitial dimension of test data (test) = 28000, 784\r\n \r\nI am using version 0.14.1'"
2403,18664248,pprett,amueller,2013-08-28 12:02:48,2015-03-10 21:26:20,2015-03-10 21:26:20,closed,,0.16,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2403,b'RFECV is broken',"b""RFECV has some issues\r\n\r\n1. ``scores.shape[1]`` is chosen as n_features which is the number of evaluations in the worst case (step size 1); if a different step size is chosen not all cells are filled. Cells are initialized with zero; for each evaluation the score is added to the cell - scores can be either all negative or all positive (depending on the scorer); finally the max is chosen: this is a bug in the case of negative scores and step != 1.\r\n\r\n2. best score is the sum of all CV evaluations not the mean - this is confusing because the best score cannot be related to the other CV experiments.\r\n\r\n3. the number of final features selected is ``k`` -- which seems to refer to the RFE.ranking_ of features where ties have the same rank. Imagine a ranking where are two blocks of tied features ``[1,1,1,2,2,2]``. Since ``k`` is at most 1 only two features ``n_features_to_select = k+1`` could be selected at most ... maybe I'm not getting the whole picture but it smells fishy  (ping @amueller @NicolasTr )"""
2402,18653578,pprett,agramfort,2013-08-28 07:48:10,2013-11-19 10:11:21,2013-11-17 19:16:29,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2402,b'ElasticNetCV fails if y.ndim > 1',"b'Error somewhere in extension module cd_fast.so ::\r\n\r\n    X = np.random.rand(100, 10)\r\n    Y = np.random.rand(100, 2)\r\n    from sklearn.linear_model import ElasticNet, ElasticNetCV\r\n    est = ElasticNet().fit(X,Y)\r\n    # fails w/ ValueError: Buffer has wrong number of dimensions (expected 1, got 2)\r\n    est = ElasticNetCV().fit(X,Y)'"
2393,18513643,alanwli,larsmans,2013-08-24 22:54:35,2015-10-14 16:26:30,2013-09-08 19:23:50,closed,,,4,Bug;Large Scale,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2393,b'Segfault with large dataset',"b'A segmentation fault occurs with the following snippet\r\nalong with the core dump output. The scikit version that I\'m on\r\nis scikit_learn-0.14.1.\r\n\r\nimport numpy as np\r\ndata = np.memmap(\'/tmp/features.dat\', dtype=\'float64\', mode=\'r\',\r\nshape=(40000000,100))\r\ntarget = np.memmap(\'/tmp/targets.dat\', dtype=\'uint8\', mode=\'r\',\r\nshape=40000000)\r\nfrom sklearn.linear_model import SGDClassifier\r\nclf = SGDClassifier(loss=""log"", penalty=""l2"")\r\nclf.fit(data, target)\r\n\r\nProgram terminated with signal 11, Segmentation fault.\r\n#0  0x00007fa05a21c8aa in\r\n__pyx_f_7sklearn_5utils_13weight_vector_12WeightVector_dot\r\n(__pyx_v_self=0x329d350, __pyx_v_x_data_ptr=0x7f9c651271a0,\r\n__pyx_v_x_ind_ptr=0x330f400, __pyx_v_xnnz=<value optimized out>)\r\n    at sklearn/utils/weight_vector.c:1456\r\n1456 sklearn/utils/weight_vector.c: No such file or directory.\r\nin sklearn/utils/weight_vector.c\r\n'"
2379,18395075,galvezz,larsmans,2013-08-22 05:07:07,2014-06-16 20:13:37,2014-06-16 20:13:37,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2379,b'Error in nosetests with Numpy 1.7.1. Scipy 0.12.0',"b'Env:\r\nskleanr 0.14.1\r\nNumpy 1.7.1\r\nScipy 0.12.0\r\nPython 2.7.5\r\n\r\nI get this error after trying nosetest, is it critical?\r\n```\r\nMacBook-Pro-de-GALVEZZ:python galvezz$ nosetests sklearn --exe\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/pls.py:7: DeprecationWarning: This module has been moved to cross_decomposition and will be removed in 0.16\r\n  ""removed in 0.16"", DeprecationWarning)\r\n......./Library/Python/2.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py:1664: RuntimeWarning: invalid value encountered in sqrt\r\n  s = np.sqrt(eigvals)\r\n...........................S.........................................../Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/manifold/spectral_embedding_.py:226: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n......./Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/covariance/graph_lasso_.py:193: RuntimeWarning: invalid value encountered in multiply\r\n  * coefs)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/covariance/graph_lasso_.py:195: RuntimeWarning: invalid value encountered in multiply\r\n  * coefs)\r\n....SS...../Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/datasets/tests/test_base.py:118: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n.../Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/datasets/tests/test_base.py:139: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n./Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/datasets/tests/test_base.py:155: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n.....SS....................................................S........./Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/fastica_.py:269: UserWarning: Ignoring n_components with whiten=False.\r\n  warnings.warn(\'Ignoring n_components with whiten=False.\')\r\n.............................................../Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n../Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/decomposition/dict_learning.py:110: DeprecationWarning: coef_init is now ignored and will be removed in 0.15. See enet_path function.\r\n  clf.fit(dictionary.T, X.T, coef_init=init)\r\n...S........................................................SSS................................/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x1080d2520>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................S............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................E...........................................................................................SSS....S....S..................................................................................................................\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_regressors_int\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Library/Python/2.7/site-packages/nose-1.3.0-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/tests/test_common.py"", line 772, in test_regressors_int\r\n    regressor_1.fit(X, y_)\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/linear_model/bayes.py"", line 377, in fit\r\n    X[:, keep_lambda].T))\r\n  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14.1-py2.7-macosx-10.7-intel.egg/sklearn/utils/extmath.py"", line 369, in pinvh\r\n    s, u = linalg.eigh(a, lower=lower)\r\n  File ""/Library/Python/2.7/site-packages/scipy/linalg/decomp.py"", line 383, in eigh\r\n    raise LinAlgError(""unrecoverable internal error."")\r\nLinAlgError: unrecoverable internal error.\r\n\r\n----------------------------------------------------------------------\r\nRan 1715 tests in 125.008s\r\n\r\nFAILED (SKIP=16, errors=1)\r\n```'"
2374,18297863,ogrisel,jnothman,2013-08-20 14:09:49,2014-04-27 19:45:48,2013-12-07 22:36:38,closed,,0.15,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2374,b'Multiclass and multilabel classifiers should accept arrays with string labels with dtype=object',"b""As numpy does not have dtype for variable length strings, is very common to use `dtype=object` for arrays of strings so as to no waste memory: the default fixed width string dtype of numpy allocates zero padded memory otherwise.\r\n\r\nHowever in sklearn 0.14, the `sklearn.multiclass.type_of_target` function explicitly rejects:\r\n\r\n```python\r\nif y.ndim > 2 or y.dtype == object:\r\n        return 'unknown'\r\n```\r\n\r\nIn consequence it's possible to have: `y =  ['cat', 'dog', 'fish']`, but not `y = np.asarray(['cat', 'dog', 'fish', dtype=object])` anymore (it used to work in 0.13).\r\n\r\nNote that `np.array(list_of_string, dtype=object)` is a necessary idiom (instead of just using `list_of_string` directly) to do cross-validation or other fancy indexing operations.\r\n\r\nI think we should accept `y` to have dtype=object if and only if `all(isinstance(y_i, (six.text_type, six.binary_type)) for y_i in y.ravel())`.\r\n\r\nThis regression was found in the sklearn_pandas project: https://github.com/paulgb/sklearn-pandas/issues/2\r\n\r\nWDYT @arjoly ?"""
2372,18288074,ogrisel,ogrisel,2013-08-20 10:25:39,2014-07-17 08:53:58,2013-09-25 13:59:21,closed,,0.15,12,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2372,b'StratifiedKFold should do its best to preserve the dataset dependency structure',b'As highlighted in this [notebook](http://nbviewer.ipython.org/urls/raw.github.com/ogrisel/notebooks/master/Non%2520IID%2520cross-validation.ipynb) the current implementation of `StratifiedKFold` (which is used by default by `cross_val_score` and `GridSearchCV` for classification problems) breaks the dependency structure of the dataset by computing the folds based on the sorted labels.\r\n\r\nInstead one should probably do an implementation that performs individual dependency preserving KFold on for each possible label value and aggregate the folds to get the `StratifiedKFold` final folds.\r\n\r\nThis might incur a refactoring to get rid of the `_BaseKFold` base class. It might also make it easier to implement a `shuffle=True` option for `StratifiedKFold`.'
2362,17997537,ane,amueller,2013-08-13 13:55:17,2014-07-18 15:03:29,2014-07-18 15:03:29,closed,,0.15.1,2,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2362,b'Example file missing or renamed in sklearn.linear_model.lasso_recovery_path documentation',"b'At the bottom of this page http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.lasso_stability_path.html#sklearn.linear_model.lasso_stability_path it states that:\r\n\r\n""See examples/linear_model/plot_sparse_recovery.py for an example.""\r\n\r\nIt looks like this file no longer exists or has been renamed, or combined into another file, with no clear rule as to which file that would be. '"
2360,17993565,karidajiang,amueller,2013-08-13 12:26:41,2014-07-18 15:19:57,2014-07-18 15:18:08,closed,,0.15.1,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2360,"b'OneVsOneClassifier scores calculation, version 0.14'","b""In multiclass.py, function predict_ovo(estimators, classes, X), it has:\r\n\r\n           scores[:, i] += score\r\n           scores[:, j]  -= score\r\n\r\nHowever, I think it should be: \r\n           scores[:, i]   -= score\r\n           scores[:, j]  += score\r\n\r\nI think the idea here should be, when score > 0, the positive class increase its value in 'scores by' 'score', and the negative class decrease its value by 'score'. Because in case of tie, we use 'argmax(scores)' to determine the class, so the 'scores' should represent 'how certain we are about the class'. Therefore in the original code, 'i' should represent the positive class while 'j' represents the negative class. \r\n\r\nSince from 'pred' and 'score' you find out: when score is positive, it has the label of '1' and otherwise '0', it means '1' is the positive class label and '0' is the negative class label.\r\n\r\nHowever, in _fit_ovo_binary(estimator, X, y, i, j), with 'i' and 'j' being the same as used in predict_ovo(estimators, classes, X):\r\n\r\n    y[y == i] = 0\r\n    y[y == j] = 1\r\n\r\nWe know that 'i' < 'j'. Since 1 is the positive class label, then 'j' represent the positive class. i.e. for a classifier trained between 'i' and 'j' with  'i' < 'j', 'j' is the positive class.\r\n\r\nBack to function predict_ovo(estimators, classes, X), we see from line:\r\n    scores[:, j]  -= score\r\n\r\nIf score > 0, we decrease the value corresponding to class 'j', which is incorrect.\r\n\r\nHope it is not too verbose."""
2356,17903225,larsmans,amueller,2013-08-10 19:52:19,2015-02-07 12:26:07,2015-02-07 12:26:07,closed,,0.16,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2356,b'Bug in MeanShift with small number of samples',"b'The code from [this SO question](http://stackoverflow.com/q/18157273/166749), reposted below for reference, sometimes works and sometimes fails. I\'m too tired to check if this is really a bug, but if it isn\'t, the error message could be made friendlier:\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\r\nfrom sklearn.datasets.samples_generator import make_blobs\r\n\r\n# Generate sample data\r\ncenters = [\r\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r\n    [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n]\r\nX, _ = make_blobs(n_samples=100, centers=centers, cluster_std=0.6)\r\n\r\n# Compute clustering with MeanShift\r\n\r\n# The following bandwidth can be automatically detected using\r\nbandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\r\n\r\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\r\nms.fit(X)\r\nlabels = ms.labels_\r\ncluster_centers = ms.cluster_centers_\r\n\r\nlabels_unique = np.unique(labels)\r\nn_clusters_ = len(labels_unique)\r\n\r\nprint(""number of estimated clusters : %d"" % n_clusters_)\r\n```'"
2335,17441161,jaquesgrobler,jaquesgrobler,2013-07-31 09:00:38,2013-08-05 07:39:32,2013-08-05 07:39:32,closed,jaquesgrobler,,2,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2335,b'DOC: user_guide.html has a crazy amount of white-space',"b'Due to the toc-tree collapsing, it leaves the page massive. will load a fix in a few minutes'"
2330,17397337,jaquesgrobler,ogrisel,2013-07-30 14:02:01,2014-04-14 15:40:01,2014-04-14 15:40:01,closed,jaquesgrobler,,1,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2330,b'DOC: Fork me on Github banner blocks `Clear search button`',b'will fix shortly'
2322,17338381,vene,ogrisel,2013-07-29 13:09:21,2014-07-29 19:36:37,2014-07-29 19:36:37,closed,,0.15.1,30,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2322,b'Ward test crashes on Windows with Python 3.3.2 from Anaconda and MSVC 2008 and 2010',"b""The test at fault is Check that we obtain the correct number of clusters with Ward clustering. ...\r\n\r\nI need a break from this and will not pursue at the moment.  I think it works fine with mingw, but binaries can't be released this way.\r\n\r\ncc @ogrisel """
2318,17328658,ogrisel,ogrisel,2013-07-29 08:35:30,2013-09-08 18:36:23,2013-07-29 09:45:19,closed,,0.14-rc,1,Bug;Build / CI,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2318,b'Run and fix the tests on the installed version of scikit-learn 0.14a1 tarball',b'See the test procedure and failures here: https://gist.github.com/ogrisel/6102848'
2312,17317246,amueller,GaelVaroquaux,2013-07-28 21:04:33,2013-07-29 00:00:51,2013-07-29 00:00:51,closed,,0.14-rc,2,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2312,b'Add link to whatsnew on website',b'Currently there is no link to the whatsnew on the frontpage (or anywhere).'
2303,17313915,NelleV,GaelVaroquaux,2013-07-28 16:30:20,2013-07-29 01:11:24,2013-07-29 01:11:24,closed,,0.14-rc,14,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2303,b'Backward compatibility problem with the new scorer API',"b'The following test fails right now, while it worked fine in 0.13::\r\n\r\n    def test_cross_val_score_score_func():\r\n        clf = MockClassifier()\r\n       _score_func1_args = []\r\n       _score_func2_args = []\r\n\r\n       def score_func1(data):\r\n           _score_func1_args.append(data)\r\n           return 1.0\r\n\r\n       def score_func2(y_test, y_predict):\r\n           _score_func2_args.append((y_test, y_predict))\r\n          return 1.0\r\n\r\n       score1 = cval.cross_val_score(clf, X, score_func=score_func1)\r\n       assert_array_equal(score1, [1.0, 1.0, 1.0])\r\n       assert len(_score_func1_args) == 3\r\n\r\n       score2 = cval.cross_val_score(clf, X, y, score_func=score_func2)\r\n       assert_array_equal(score2, [1.0, 1.0, 1.0])\r\n       assert len(_score_func2_args) == 3\r\n'"
2301,17313725,amueller,amueller,2013-07-28 16:11:05,2013-07-28 16:15:05,2013-07-28 16:15:05,closed,,0.14-rc,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2301,b'Failing hashing test on master.',"b'I have no idea what is going on here...\r\nDidn\'t have that before..\r\n\r\n```\r\n======================================================================\r\nFAIL: Check the performance of hashing numpy arrays:\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\nAssertionError: False is not true\r\n    \'False is not true\' = self._formatMessage(\'False is not true\', ""%s is not true"" % safe_repr(False))\r\n>>  raise self.failureException(\'False is not true\')\r\n    \r\n```'"
2296,17312816,amueller,NelleV,2013-07-28 14:53:19,2013-07-30 15:29:13,2013-07-30 15:29:13,closed,,0.14,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2296,b'Run old tests under new version.',b'We NEED to run the tests of 0.13.1 under 0.14-rc to ensure backward compatibility before tagging.\r\nFlagged as bug for the redness of it.\r\nI would consider this blocking.'
2291,17311609,GaelVaroquaux,GaelVaroquaux,2013-07-28 12:51:35,2013-07-29 00:23:04,2013-07-28 16:42:28,closed,,0.14-rc,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2291,b'CCA example looks broken',"b'The multi_label example uses CCA, and looks broken in dev, but good in stable.\r\n\r\nCc @NelleV '"
2289,17311006,GaelVaroquaux,GaelVaroquaux,2013-07-28 11:42:50,2013-07-28 22:17:06,2013-07-28 16:45:52,closed,,0.14-rc,34,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2289,b'Buffer dtype mismatch in Neighbors under Windows',"b'In Windows, 32bit, mingw compiler, the new neighbors module doesn\'t work, and I get the following errors:\r\n\r\n<pre>\r\nFile ""binary_tree.pxi"", line 223, in sklearn.neighbors.kd_tree.get_memview_NodeData_1D (sklearn\\neighbors\\kd_tree.c:3071)\r\n  ValueError: Buffer dtype mismatch; next field is at offset 12 but 16 expected\r\n</pre>\r\n\r\nping @jakevdp , @ogrisel, any clues?'"
2282,17300966,amueller,glouppe,2013-07-27 17:44:18,2013-09-11 12:55:20,2013-09-11 12:55:20,closed,,,14,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2282,b'AdaBoost Classifier takes a mutable default argument',"b""So Adaboost gets as default argument ``base_estimator=DecisionTree()``.\r\n\r\nThis is bad for two reasons:\r\n\r\n* It is ugly. It doesn't cause any bugs because base_estimator is always cloned. But we should really avoid mutable defaults.\r\n\r\n* It breaks sphinx. That means the API doc for AdaBoost is not generated.\r\n\r\nSetting the default argument to ``None`` is not a real option as that doesn't allow grid-searching the parameters of ``base_estimator`` with the default value.\r\nWe could in principle set change ``base_estimator`` in ``__init__`` but that would violate our policies and mean that ``None`` is not a valid parameter setting for ``base_estimator`` in grid searches.\r\n\r\nping @NelleV for the sphinx issue.\r\n"""
2225,17210497,amueller,glouppe,2013-07-25 13:15:51,2014-01-15 06:58:13,2014-01-15 06:58:13,closed,,0.15,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2225,b'gradient boosting oob test error on 32bit',"b'```\r\n======================================================================\r\nFAIL: Check OOB improvement on multi-class dataset.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/local/lamueller/checkout/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py"", line 526, in test_oob_multilcass_iris\r\n    decimal=2)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 800, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 636, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array([ 12.61875849,  10.42618873,   7.97227818,   6.18745436,   4.95108778])\r\n y: array([ 12.68,  10.45,   8.18,   6.43,   5.02])\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 2 decimals\\n\\n(mismatch 100.0%)\\n x: array([ 12.61875849,  10.42618873,   7.97227818,   6.18745436,   4.95108778])\\n y: array([ 12.68,  10.45,   8.18,   6.43,   5.02])\')\r\n```\r\nping @pprett '"
2221,17207986,amueller,amueller,2013-07-25 12:13:56,2013-07-25 14:15:05,2013-07-25 14:15:05,closed,,0.14-rc,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2221,b'cython errors',"b'On current master, I get the following errors when trying\r\n``make cython``\r\nI have\r\n\r\n```\r\n> cython --version\r\nCython version 0.17.2\r\n\r\n```\r\nError messages\r\n```\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    n_node_samples : int*\r\n        n_samples[i] holds the number of training samples reaching node i.\r\n    """"""\r\n    # Wrap for outside world\r\n    property n_classes:\r\n^\r\n------------------------------------------------------------\r\n\r\nsklearn/tree/_tree.pyx:1239:0: Possible inconsistent indentation\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    n_node_samples : int*\r\n        n_samples[i] holds the number of training samples reaching node i.\r\n    """"""\r\n    # Wrap for outside world\r\n    property n_classes:\r\n^\r\n------------------------------------------------------------\r\n\r\nsklearn/tree/_tree.pyx:1239:0: Expected an identifier or literal\r\n\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n    """"""\r\n\r\n    cdef svm_parameter param\r\n    cdef svm_problem problem\r\n    cdef svm_model *model\r\n    cdef const char *error_msg\r\n                   ^\r\n------------------------------------------------------------\r\n\r\nsklearn/svm/libsvm.pyx:139:20: Syntax error in C variable declaration\r\nmake: *** [cython] Error 123\r\n```'"
2190,17074628,erg,amueller,2013-07-22 22:35:33,2013-07-26 08:16:31,2013-07-26 08:16:31,closed,,0.14-rc,17,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2190,b'crash in MeanShift tests after make cython (edited from k_means)',"b'The crash:\r\n\r\n```\r\n[erg@pliny scikit-learn]$ [master*] nosetests -v\r\n/home/erg/python/scikit-learn/sklearn/feature_selection/selector_mixin.py:7: DeprecationWarning: sklearn.feature_selection.selector_mixin.SelectorMixin has been renamed sklearn.feature_selection.from_model._LearntSelectorMixin, and this alias will be removed in version 0.16\r\n  DeprecationWarning)\r\nAffinity Propagation algorithm ... ok\r\nTests the DBSCAN algorithm with a similarity array. ... ok\r\nTests the DBSCAN algorithm with a feature vector array. ... ok\r\nTests the DBSCAN algorithm with a callable metric. ... ok\r\nsklearn.cluster.tests.test_dbscan.test_pickle ... ok\r\nCheck that we obtain the correct solution for structured ward tree. ... ok\r\nCheck that we obtain the correct solution for unstructured ward tree. ... ok\r\nCheck that the height of ward tree is sorted. ... ok\r\nCheck that we obtain the correct number of clusters with Ward clustering. ... ok\r\nCheck that we obtain the correct solution in a simplistic case ... ok\r\nTest scikit ward with full connectivity (i.e. unstructured) vs scipy ... ok\r\nCheck that connectivity in the ward tree is propagated correctly during ... ok\r\nCheck non regression of a bug if a non item assignable connectivity is ... ok\r\nsklearn.cluster.tests.test_k_means.test_square_norms ... ok\r\nsklearn.cluster.tests.test_k_means.test_kmeans_dtype ... ok\r\nsklearn.cluster.tests.test_k_means.test_labels_assignment_and_inertia ... ok\r\nCheck that dense and sparse minibatch update give the same results ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_check_fitted ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_new_centers ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_2_jobs ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_sparse ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_random_init ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_random_init_sparse ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_not_precomputed ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_random_init_not_precomputed ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_perfect_init ... ok\r\nsklearn.cluster.tests.test_k_means.test_mb_k_means_plus_plus_init_dense_array ... ok\r\nsklearn.cluster.tests.test_k_means.test_mb_kmeans_verbose ... ok\r\nsklearn.cluster.tests.test_k_means.test_mb_k_means_plus_plus_init_sparse_matrix ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_init_with_large_k ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_k_means_random_init_dense_array ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_k_means_random_init_sparse_csr ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_k_means_perfect_init_dense_array ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_k_means_perfect_init_sparse_csr ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_reassign ... ok\r\nsklearn.cluster.tests.test_k_means.test_sparse_mb_k_means_callable_init ... ok\r\nsklearn.cluster.tests.test_k_means.test_mini_batch_k_means_random_init_partial_fit ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_default_init_size ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_tol ... ok\r\nsklearn.cluster.tests.test_k_means.test_minibatch_set_init_size ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_invalid_init ... ok\r\nsklearn.cluster.tests.test_k_means.test_mini_match_k_means_invalid_init ... ok\r\nCheck if copy_x=False returns nearly equal X after de-centering. ... ok\r\nCheck k_means with a bad initialization does not yield a singleton ... ok\r\nsklearn.cluster.tests.test_k_means.test_predict ... ok\r\nsklearn.cluster.tests.test_k_means.test_score ... ok\r\nsklearn.cluster.tests.test_k_means.test_predict_minibatch_dense_input ... ok\r\nsklearn.cluster.tests.test_k_means.test_predict_minibatch_kmeanspp_init_sparse_input ... ok\r\nsklearn.cluster.tests.test_k_means.test_predict_minibatch_random_init_sparse_input ... ok\r\nsklearn.cluster.tests.test_k_means.test_input_dtypes ... ok\r\nsklearn.cluster.tests.test_k_means.test_transform ... ok\r\nsklearn.cluster.tests.test_k_means.test_fit_transform ... ok\r\nCheck that increasing the number of init increases the quality ... ok\r\nsklearn.cluster.tests.test_k_means.test_k_means_function ... ok\r\nTest MeanShift algorithm ... Segmentation fault (core dumped)\r\n```\r\n\r\n\r\nSome related warnings?\r\n\r\n```\r\n[erg@pliny ~]$ cython --version\r\nCython version 0.19.1\r\n\r\n[erg@pliny scikit-learn]$ [master*] make cython\r\nfind sklearn -name ""*.pyx"" | xargs cython\r\nwarning: sklearn/neighbors/binary_tree.pxi:1199:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1257:48: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1258:46: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1260:45: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1345:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1355:42: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1357:36: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1398:59: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1400:46: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1401:48: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1403:45: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1491:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1544:64: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1589:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1199:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1257:48: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1258:46: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1260:45: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1345:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1355:42: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1357:36: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1398:59: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1400:46: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1401:48: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1403:45: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1491:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1544:64: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\nwarning: sklearn/neighbors/binary_tree.pxi:1589:20: the result of using negative indices inside of code sections marked as \'wraparound=False\' is undefined\r\n```'"
2185,17052165,ogrisel,GaelVaroquaux,2013-07-22 15:19:46,2014-07-14 19:44:09,2014-07-14 19:44:09,closed,,0.15,17,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2185,b'MinibatchKMeans bad center reallocation causes duplicate centers',"b'For instance have a look at:\r\n\r\n  http://scikit-learn.org/dev/auto_examples/cluster/plot_dict_face_patches.html\r\n\r\nsome of the centroids are duplicated, presumably because of a bug in the bad cluster reallocation heuristic.'"
2174,17020065,ogrisel,GaelVaroquaux,2013-07-21 15:27:38,2013-07-28 23:03:34,2013-07-28 23:03:34,closed,,0.14-rc,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2174,"b""Broken build under windows (_gradient_boosting.o: bad reloc address 0x0 in section `.data')""","b'Apparently a number of windows users could not build the current master under windows. The common error message is the following:\r\n```\r\nc:/mingw/bin/../lib/gcc/mingw32/4.7.2/../../../../mingw32/bin/ld.exe: build\\temp\r\n.win32-2.7\\Release\\sklearn\\ensemble\\_gradient_boosting.o: bad reloc address 0x0 in section `.data\'\r\ncollect2.exe: error: ld returned 1 exit status\r\nerror: Command ""g++ -shared build\\temp.win32-.7\\Release\\sklearn\\ensemble\\_gradient_boosting.o -LC:\\Python27\\libs LC:\\Python27\\PCbuild -Lbuild\\temp.win32-2.7 -lpython27 -lmsvcr90 -o build\\lib.win32-2.7\\sklearn\\ensemble\\_gradient_boosting.pyd"" failed with exit status 1\r\n```\r\nso it might be something in the gradient boosted cython code that is not supported by mingw under windows.\r\n\r\nOn stackoverflow:\r\n- http://stackoverflow.com/questions/17621541/building-issue-of-scikit-learn-python-library-in-windows\r\n- http://stackoverflow.com/questions/17598112/issue-when-building-scikit-learn-in-windows-7\r\n\r\nOn the mailing list:\r\n- http://comments.gmane.org/gmane.comp.python.scikit-learn/7788'"
2173,17006129,jnothman,larsmans,2013-07-20 13:02:38,2013-07-22 08:26:24,2013-07-22 08:26:24,closed,,,0,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2173,b'twenty_newsgroups doc does not match API',b'http://scikit-learn.org/dev/datasets/#the-20-newsgroups-text-dataset includes `from sklearn.feature_extraction.text import Vectorizer` which raises `ImportError: cannot import name Vectorizer`'
2137,16478183,arjoly,arjoly,2013-07-08 15:43:41,2013-07-24 11:47:13,2013-07-24 11:47:13,closed,,,1,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2137,b'Confusion error message with _check_clf_target in the metrics module',"b'````python \r\nimport numpy as np\r\nfrom sklearn.metrics.metrics import _check_clf_targets\r\n\r\ny1 = np.array([[1, 0, 1, 0, 1], \r\n [0, 0, 0, 1, 0],\r\n [0, 1, 0, 0, 1]])\r\ny2 = np.array([[ 0.35,  0.5,   0.5,  0.5,   0.45],\r\n [ 0.2,   0.45,  0.2,   0.65,  0.3 ],\r\n [ 0.35,  0.5,   0.55,  0.3,   0.35]])\r\n\r\n_check_clf_targets(y1, y2)\r\n```\r\n\r\nThis code raise the following error message\r\n\r\n```\r\n----> 1 _check_clf_targets(y1, y2)\r\n\r\n/Users/ajoly/git/scikit-learn/sklearn/metrics/metrics.pyc in _check_clf_targets(y_true, y_pred)\r\n     69     if type_true.startswith(\'multilabel\'):\r\n     70         if not type_pred.startswith(\'multilabel\'):\r\n---> 71             raise ValueError(""Can\'t handle mix of multilabel and multiclass ""\r\n     72                              ""targets"")\r\n     73         if type_true != type_pred:\r\n\r\nValueError: Can\'t handle mix of multilabel and multiclass targets\r\n\r\n```\r\n\r\nInstead of saying that you are try to mix multialbel-indicator and multioutput-continous value. (ping @jnothman)\r\nThis `_check_clf_targets` is used for instance in `accuracy_score`.\r\n'"
2136,16462870,rhoef,jaquesgrobler,2013-07-08 09:45:02,2013-12-10 11:57:36,2013-12-10 11:57:36,closed,,0.15,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2136,b'hmm uses class labels as array index',"b'I try to correct class labels (produced by an svm classifier). Class labels are 1,2,... 8\r\n\r\nThe following exception occurs: \r\n```\r\n File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/hmm.py"", line 322, in predict\r\n    _, state_sequence = self.decode(obs, algorithm)\r\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/hmm.py"", line 305, in decode\r\n    logprob, state_sequence = decoder[algorithm](obs)\r\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/hmm.py"", line 242, in _decode_viterbi\r\n    framelogprob = self._compute_log_likelihood(obs)\r\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/hmm.py"", line 967, in _compute_log_likelihood\r\n    return self._log_emissionprob[:, obs].T\r\nIndexError: index (8) out of range (0<=index<7) in dimension 1\r\n```\r\nIt turns out that class labels are internally used as array indices. That definitely should not be. Labels are completely arbitrary, and have a semantically meaning, while array indices and how indexing works is part of python syntax. This must not be mixed up.\r\n\r\n\r\n\r\nExample from text book.\r\n2 classes (labels -1 and 1) \r\n\r\nThe function \r\n```\r\nself._log_emissionprob[:, obs].T\r\n```\r\n\r\nwill return always the same values since obs is always a list list of that contains -1\'s an 1\'s.\r\n\r\n\r\n'"
2130,16361445,iampat,amueller,2013-07-04 10:35:00,2014-07-18 16:09:10,2014-07-18 16:09:10,closed,pprett,0.15.1,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2130,b'GradientBoostingClassifier with a BaseEstimator',"b'GradientBoostingClassifier does not work correctly, when the BasePredictor (init) is set to something like linear_model.LogisticRegression().\r\n\r\nI thinks the source of the error is in in BaseGradientBoosting.fit() where \r\n        # init predictions\r\n        y_pred = self.init_.predict(X)\r\n\r\ny_pred should be initialized by class probabilities and not with the winner class.\r\n\r\nReplacing y_pred = self.init_.predict(X) with y_pred = self.init_.predict_proba(X) can solve the problem.'"
2126,16255568,seanv507,mblondel,2013-07-02 09:32:48,2013-07-05 14:42:58,2013-07-04 11:32:53,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2126,b'sample_weight default bug in RidgeClassifier : dense_cholesky solver always used',"b""ridge.py ~line 80\r\nbasically RidgeClassifier creates sampleweight VECTOR of 1s by default]\r\nand \r\nhas_sw = isinstance(sample_weight, np.ndarray) or sample_weight != 1.0\r\n\r\n....\r\n    if has_sw:\r\n        solver = 'dense_cholesky'\r\n\r\nwhich meands that the normal cholesky solver is always used (when you don't enter any sample weights)\r\n\r\n"""
2124,16240690,sergeyf,amueller,2013-07-01 23:23:33,2013-07-27 14:13:46,2013-07-27 14:13:46,closed,vene,0.14-rc,16,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2124,"b""decomposition.NMF.transform() isn't working properly""","b'After running the code here without errors on Python 2.7.5 on Windows 7: \r\nhttp://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html#example-applications-topics-extraction-with-nmf-py\r\n\r\nAttempting to transform the input data with the fitted model raises an error:\r\n\r\nnmf.transform(tfidf)\r\n\r\n> Traceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\decomposition\\nmf.py"", line 563, in transform\r\n    W[j, :], _ = nnls(self.components_.T, X[j, :])\r\n  File ""C:\\Python27\\lib\\site-packages\\scipy\\optimize\\nnls.py"", line 48, in nnls\r\n    raise ValueError(""incompatible dimensions"")\r\nValueError: incompatible dimensions\r\n'"
2098,15992324,erg,amueller,2013-06-25 18:03:31,2013-07-27 10:37:50,2013-07-26 12:54:57,closed,,0.14-rc,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2098,b'inconsistency for fitting classifiers with only one class',"b'+1 for letting all classifiers trivially ``fit`` on a dataset with a single class.\r\n\r\n```\r\nIn [25]: from sklearn.tree import DecisionTreeClassifier\r\n\r\nIn [26]: clf = DecisionTreeClassifier()\r\n\r\nIn [27]: clf.fit([[1]], [0])\r\nOut[27]: \r\nDecisionTreeClassifier(compute_importances=False, criterion=\'gini\',\r\n            max_depth=None, max_features=None, min_density=0.1,\r\n            min_samples_leaf=1, min_samples_split=2, random_state=None)\r\n\r\n\r\n\r\nIn [28]: from sklearn.linear_model import SGDClassifier\r\n\r\nIn [29]: clf = SGDClassifier()\r\n\r\nIn [30]: clf.fit([[1]], [0])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-30-d939e271ffd7> in <module>()\r\n----> 1 clf.fit([[1]], [0])\r\n\r\n/usr/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc in fit(self, X, y, coef_init, intercept_init, class_weight, sample_weight)\r\n    522                          coef_init=coef_init, intercept_init=intercept_init,\r\n    523                          class_weight=class_weight,\r\n--> 524                          sample_weight=sample_weight)\r\n    525 \r\n    526 \r\n\r\n/usr/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc in _fit(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, class_weight, sample_weight)\r\n    421 \r\n    422         self._partial_fit(X, y, alpha, C, loss, learning_rate, self.n_iter,\r\n--> 423                           classes, sample_weight, coef_init, intercept_init)\r\n    424 \r\n    425         # fitting is over, we can now transform coef_ to fortran order\r\n\r\n/usr/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc in _partial_fit(self, X, y, alpha, C, loss, learning_rate, n_iter, classes, sample_weight, coef_init, intercept_init)\r\n    380                              sample_weight=sample_weight, n_iter=n_iter)\r\n    381         else:\r\n--> 382             raise ValueError(""The number of class labels must be ""\r\n    383                              ""greater than one."")\r\n    384 \r\n\r\nValueError: The number of class labels must be greater than one.\r\n\r\n\r\n\r\n\r\nIn [31]: from sklearn.svm import SVC\r\n\r\nIn [32]: clf = SVC()\r\n\r\nIn [33]: clf.fit([[1]], [0])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-33-d939e271ffd7> in <module>()\r\n----> 1 clf.fit([[1]], [0])\r\n\r\n/usr/lib/python2.7/site-packages/sklearn/svm/base.pyc in fit(self, X, y, sample_weight)\r\n    132 \r\n    133         X = atleast2d_or_csr(X, dtype=np.float64, order=\'C\')\r\n--> 134         y = self._validate_targets(y)\r\n    135 \r\n    136         sample_weight = np.asarray([]\r\n\r\n/usr/lib/python2.7/site-packages/sklearn/svm/base.pyc in _validate_targets(self, y)\r\n    440             raise ValueError(\r\n    441                 ""The number of classes has to be greater than one; got %d""\r\n--> 442                 % len(cls))\r\n    443 \r\n    444         self.classes_ = cls\r\n\r\nValueError: The number of classes has to be greater than one; got 1\r\n```\r\n'"
2069,15554929,dchatel,amueller,2013-06-14 12:06:34,2015-02-25 19:01:05,2015-02-25 19:01:05,closed,,0.16,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2069,"b""sklearn.manifold.spectral_embedding doesn't return a spectral embedding""","b""from pylab import *\r\nfrom scipy.sparse import linalg as la\r\nfrom sklearn import manifold as m\r\n\r\nw=rand(50,50)\r\nw+=w.T\r\n\r\nk,v=la.eigh(l,3,which='SM')\r\nu=m.spectral_embedding(w,3,drop_first=False)\r\n\r\ndot(v.T,u) # should return Identity.\r\ndot(u.T,u) # should return Identity.\r\n\r\nbut it does not."""
2065,15531612,etseidler,ogrisel,2013-06-13 21:37:05,2014-07-29 19:28:18,2014-07-29 19:28:18,closed,,0.15.1,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2065,b'installing on 64bit windows 7 for Python 2.7 using unofficial installers',"b'So I took your suggestion and installed numpy, scipy, matplotlib and then scikit-learn (in that order) all from the same website that you listed (http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn which was very helpful, btw). Out of curiosity, I ran the nose tests and got the output below. I\'m pretty sure I can still use most of the scikit-learn functionality, but would anyone care to comment on that? Again, my approach was to use only the installers. Am I overlooking anything?\r\n\r\n```\r\nC:\\Python27\\scripts>nosetests.exe sklearn --exe\r\n...............................................................C:\\Python27\\lib\\site-packages\\sklearn\\manifold\\spectral_embedding.py:225: UserWarning: Graph is not fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n.......C:\\Python27\\lib\\site-packages\\sklearn\\covariance\\graph_lasso_.py:45: RuntimeWarning: invalid value encountered in absolute\r\n  gap += alpha * (np.abs(precision_).sum()\r\nC:\\Python27\\lib\\site-packages\\sklearn\\covariance\\graph_lasso_.py:46: RuntimeWarning: invalid value encountered in absolute\r\n  - np.abs(np.diag(precision_)).sum())\r\nC:\\Python27\\lib\\site-packages\\numpy\\linalg\\linalg.py:1664: RuntimeWarning: invalid value encountered in absolute\r\n  absd = absolute(d)\r\nC:\\Python27\\lib\\site-packages\\sklearn\\covariance\\graph_lasso_.py:32: RuntimeWarning: invalid value encountered in absolute\r\n  cost += alpha * (np.abs(precision_).sum()\r\nC:\\Python27\\lib\\site-packages\\sklearn\\covariance\\graph_lasso_.py:33: RuntimeWarning: invalid value encountered in absolute\r\n  - np.abs(np.diag(precision_)).sum())\r\nC:\\Python27\\lib\\site-packages\\sklearn\\covariance\\graph_lasso_.py:195: RuntimeWarning: invalid value encountered in absolute\r\n  if np.abs(d_gap) < tol:\r\n....SS......C:\\Python27\\lib\\site-packages\\sklearn\\datasets\\tests\\test_base.py:124: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n...C:\\Python27\\lib\\site-packages\\sklearn\\datasets\\tests\\test_base.py:145: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n.C:\\Python27\\lib\\site-packages\\sklearn\\datasets\\tests\\test_base.py:161: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n.....SS................................................S.........................................................S.........................................SSS......................C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\test\r\ntest_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x000000000F7D7E08>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n...............................................................................................................................................................................................................................................\r\n...............................................................................................................................................................................................................................................\r\n...............................................An unexpected error occurred while tokenizing input\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: (\'EOF in multi-line statement\', (63, 0))\r\n\r\nAn unexpected error occurred while tokenizing input\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: (\'EOF in multi-line statement\', (63, 0))\r\n\r\n....S........................................................................................................S.F...............................................................................................................................\r\n...............................................................................................................................................................................................................................................\r\n.........................................................................................................................................EE...........................................SSS....S....S............................................\r\n......................................................................................\r\n======================================================================\r\nERROR: sklearn.tests.test_dummy.test_stratified_strategy\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose-1.3.0-py2.7.egg\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\tests\\test_dummy.py"", line 96, in test_stratified_strategy\r\n    y_pred = clf.predict(X)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\dummy.py"", line 133, in predict\r\n    proba = self.predict_proba(X)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\dummy.py"", line 197, in predict_proba\r\n    out = rs.multinomial(1, class_prior_[k], size=n_samples)\r\n  File ""mtrand.pyx"", line 4257, in mtrand.RandomState.multinomial (numpy\\random\\mtrand\\mtrand.c:20210)\r\nTypeError: unsupported operand type(s) for +: \'long\' and \'tuple\'\r\n\r\n======================================================================\r\nERROR: sklearn.tests.test_dummy.test_stratified_strategy_multioutput\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose-1.3.0-py2.7.egg\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\tests\\test_dummy.py"", line 115, in test_stratified_strategy_multioutput\r\n    y_pred = clf.predict(X)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\dummy.py"", line 133, in predict\r\n    proba = self.predict_proba(X)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\dummy.py"", line 197, in predict_proba\r\n    out = rs.multinomial(1, class_prior_[k], size=n_samples)\r\n  File ""mtrand.pyx"", line 4257, in mtrand.RandomState.multinomial (numpy\\random\\mtrand\\mtrand.c:20210)\r\nTypeError: unsupported operand type(s) for +: \'long\' and \'tuple\'\r\n\r\n======================================================================\r\nFAIL: Test BayesianRegression ARD classifier\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose-1.3.0-py2.7.egg\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\tests\\test_bayes.py"", line 58, in test_toy_ard_object\r\n    assert(np.abs(clf.predict(test) - [1, 3, 4]).sum() < 1.e-3)  # identity\r\nAssertionError\r\n\r\n----------------------------------------------------------------------\r\nRan 1598 tests in 96.640s\r\n\r\nFAILED (SKIP=16, errors=2, failures=1)\r\n\r\nC:\\Python27\\scripts>\r\n```'"
2058,15486272,turian,larsmans,2013-06-13 02:56:39,2013-06-22 09:40:30,2013-06-22 09:40:30,closed,,0.14-rc,3,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2058,b'RandomizedPCA n_components is NOT 50 by default',"b'Looking at https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/pca.py\r\n\r\nThe documentation says:\r\nMaximum number of components to keep: default is 50.\r\n\r\nBut, the behavior is:\r\nMaximum number of components to keep: default is X.shape[1], when calling fit().\r\n\r\nHowever, if no value is passed, n_components is None:\r\n\r\n    def __init__(self, n_components=None, copy=True, iterated_power=3,\r\n                 whiten=False, random_state=None):\r\n\r\nAnd, later, the shape of the X matrix is used.\r\nif self.n_components is None:\r\n            n_components = X.shape[1]\r\n\r\n\r\nI have not checked the other PCA methods in this file, just RandomizedPCA.'"
2057,15475140,TimSC,pprett,2013-06-12 21:25:29,2013-07-22 13:11:49,2013-07-22 13:11:49,closed,,0.14-rc,25,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2057,b'_tree.pxd is not copied when sklearn is installed on linux',b'The file _tree.pxd is required for extending the Tree class using 3rd party cython. I am using \r\n```\r\ncimport sklearn.tree._tree\r\n```\r\nbut that fails if this file is unavailable.\r\n\r\nIt should be installed with the other sklearn files. My current work around:\r\n\r\nsudo cp /home/tim/dev/sklearn/sklearn/tree/_tree.pxd /usr/local/lib/python2.7/dist-packages/sklearn/tree\r\n\r\nTim'
2053,15379210,AngeldsWang,amueller,2013-06-11 03:11:37,2015-04-22 06:15:57,2014-07-18 15:23:52,closed,pprett,0.15.1,11,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2053,b'GradientBoostingClassifier train_score bug',"b'When I increased the n_estimators to 1000, the train score, suddenly, jumped to a large number as follows:\r\n...\r\nbuilt tree 723 of 1000, train score = 1.318154e-01\r\nbuilt tree 724 of 1000, train score = 1.317394e-01\r\nbuilt tree 725 of 1000, train score = 1.317101e-01\r\nbuilt tree 726 of 1000, train score = 6.835322e+12\r\nbuilt tree 727 of 1000, train score = 6.835322e+12\r\nbuilt tree 728 of 1000, train score = 6.835322e+12\r\nbuilt tree 729 of 1000, train score = 6.835322e+12\r\nbuilt tree 730 of 1000, train score = 6.835322e+12\r\n...\r\n\r\nand kept 6.835322e+12 with no change.\r\n\r\nThanks'"
2032,15151793,jnothman,ogrisel,2013-06-05 03:01:35,2013-06-05 18:23:17,2013-06-05 18:22:31,closed,,0.14-rc,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2032,b'BUG load_20newsgroups in sklearn.datasets.__all__ but does not exist',"b'Should `load_20newsgroups` exist, or should it be removed from `sklearn.datasets.__all__`?'"
2028,15114597,amueller,amueller,2013-06-04 11:53:46,2013-07-26 09:12:25,2013-07-26 09:12:25,closed,,0.14-rc,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2028,b'Test failure in metrics module on 32bit',"b'```======================================================================\r\nFAIL: Doctest: sklearn.metrics.metrics.precision_recall_fscore_support\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.metrics.metrics.precision_recall_fscore_support\r\n  File ""/home/local/lamueller/checkout/scikit-learn/sklearn/metrics/metrics.py"", line 1468, in precision_recall_fscore_support\r\n\r\n----------------------------------------------------------------------\r\nFile ""/home/local/lamueller/checkout/scikit-learn/sklearn/metrics/metrics.py"", line 1600, in sklearn.metrics.metrics.precision_recall_fscore_support\r\nFailed example:\r\n    precision_recall_fscore_support(y_true, y_pred, average=\'weighted\')\r\n    # doctest: +ELLIPSIS\r\nExpected:\r\n    (0.499..., 1.0, 0.65..., None)\r\nGot:\r\n    (0.5, 1.0, 0.65000000000000002, None)\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0xc5d60cc>.getvalue()))\r\n    \r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.metrics.metrics.precision_score\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.metrics.metrics.precision_score\r\n  File ""/home/local/lamueller/checkout/scikit-learn/sklearn/metrics/metrics.py"", line 1777, in precision_score\r\n\r\n----------------------------------------------------------------------\r\nFile ""/home/local/lamueller/checkout/scikit-learn/sklearn/metrics/metrics.py"", line 1863, in sklearn.metrics.metrics.precision_score\r\nFailed example:\r\n    precision_score(y_true, y_pred, average=\'weighted\')\r\n    # doctest: +ELLIPSIS\r\nExpected:\r\n    0.49...\r\nGot:\r\n    0.5\r\n\r\n>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0xc5d6bcc>.getvalue()))\r\n```\r\n'"
2020,14980359,pprett,pprett,2013-05-31 07:36:52,2013-07-22 13:01:59,2013-07-22 13:01:59,closed,,,4,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/2020,b'UnicodeDecodeError when running `gen_rst`',"b'On my machine, ``gen_rst.py`` throws a UnicodeDecodeError when creating the permalink headers (the paragraph symbol on the right-hand-side of the title).\r\n\r\nhere is a trace::\r\n\r\n    Exception occurred:\r\n      File ""/home/pprett/workspace/scikit-learn/doc/sphinxext/gen_rst.py"", line 882, in embed_code_links\r\n    line = line.replace(name, link)\r\n    UnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xc2 in position 232: ordinal not in range(128)\r\n\r\nMaybe its a particular issue of my machine (haven\'t fiddled with sitecustomized.py though).\r\nWould be great if someone could run ``make clean html`` and check whether it works.\r\n\r\nthanks'"
1995,14673803,arjoly,amueller,2013-05-23 13:26:18,2013-08-01 19:46:25,2013-07-28 08:25:17,closed,,0.14-rc,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1995,b'predicting probabilities with svc is not reproducible',"b'From one run to another sklearn.metrics.tests.test_metrics.test_precision_recall_curve failed. Any ideas @jnothman ?\r\n\r\n````\r\n======================================================================\r\nFAIL: Test Area under Receiver Operating Characteristic (ROC) curve\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ajoly/.virtualenvs/sklearn/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ajoly/git/scikit-learn/sklearn/metrics/tests/test_metrics.py"", line 353, in test_roc_curve\r\n    assert_array_almost_equal(roc_auc, 0.90, decimal=2)\r\n  File ""/Users/ajoly/.virtualenvs/sklearn/lib/python2.7/site-packages/numpy/testing/utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/Users/ajoly/.virtualenvs/sklearn/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.09919999999999998)\r\n y: array(0.9)\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 2 decimals\\n\\n(mismatch 100.0%)\\n x: array(0.09919999999999998)\\n y: array(0.9)\')\r\n\r\n\r\n======================================================================\r\nFAIL: sklearn.metrics.tests.test_metrics.test_precision_recall_curve\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ajoly/.virtualenvs/sklearn/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ajoly/git/scikit-learn/sklearn/metrics/tests/test_metrics.py"", line 866, in test_precision_recall_curve\r\n    _test_precision_recall_curve(y_true, probas_pred)\r\n  File ""/Users/ajoly/git/scikit-learn/sklearn/metrics/tests/test_metrics.py"", line 888, in _test_precision_recall_curve\r\n    assert_array_almost_equal(precision_recall_auc, 0.85, 2)\r\n  File ""/Users/ajoly/.virtualenvs/sklearn/lib/python2.7/site-packages/numpy/testing/utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/Users/ajoly/.virtualenvs/sklearn/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.3218346431373276)\r\n y: array(0.85)\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 2 decimals\\n\\n(mismatch 100.0%)\\n x: array(0.3218346431373276)\\n y: array(0.85)\')\r\n\r\n\r\n```'"
1993,14670294,jnothman,larsmans,2013-05-23 11:52:54,2013-12-13 07:34:04,2013-09-16 11:41:16,closed,,0.14,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1993,b'BUG LabelBinarizer returns label indicator matrices unchanged',"b'`preprocessing.LabelBinarizer`, when passed an indicator matrix, returns it unchanged. It should return it with correct `pos_label` and `neg_label` indicator values.'"
1981,14566021,amueller,amueller,2013-05-21 13:00:13,2013-05-21 13:16:02,2013-05-21 13:16:02,closed,,,1,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1981,b'multi-label support of precision_recall_fscore_support',"b""The narrative docs claim that ``precision_recall_fscore_support`` and ``classification_report`` support multi-label input.\r\nI don't think that is true, trying out a list of labels representation and looking at the code.\r\nAlso, there seem to be no tests for this case.\r\n\r\ncc @arjoly """
1973,14497416,amaatouq,amueller,2013-05-19 15:06:40,2014-07-18 13:11:07,2014-07-18 13:11:07,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1973,b'MinMax Scaler and TypeErrors',"b""I am trying to use the MinMax() from sklearn\r\n\r\nmy code is quite simple\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfollowers = np.array(df['followers_count'].astype('float'))\r\nscaled_followers = scaler.fit(followers)\r\n\r\nI get the following error (I experimented with multiple numpy arrays, same problem)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-22-d82d54e9ed9b> in <module>()\r\n      2 followers = np.array(df['followers_count'].astype('float'))\r\n      3 print followers\r\n----> 4 scaled_followers = scaler.fit(followers)\r\n\r\n/home/amaatouq/anaconda/lib/python2.7/site-packages/sklearn/preprocessing.pyc in fit(self, X, y)\r\n    195         scale_ = np.max(X, axis=0) - min_\r\n    196         # Do not scale constant features\r\n--> 197         scale_[scale_ == 0.0] = 1.0\r\n    198         self.scale_ = (feature_range[1] - feature_range[0]) / scale_\r\n    199         self.min_ = feature_range[0] - min_ / scale_\r\n\r\nTypeError: 'numpy.float64' object does not support item assignment\r\n"""
1967,14361254,twiecki,amueller,2013-05-15 14:31:58,2015-01-09 20:50:53,2015-01-09 20:50:53,closed,,0.16,31,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1967,b'LDA transform gives wrong dimensions',"b'Help doc of `LDA.transform` says:\r\n```\r\nReturns\r\n-------\r\nX_new : array, shape = [n_samples, n_components]\r\n```\r\n\r\nHowever:\r\n```\r\nX = np.array([[-1, -1, -1], [-2, -1, -1], [-3, -2, -2], [1, 1, 3], [2, 1, 4], [3, 2, 1]])\r\ny = np.array([1, 1, 1, 2, 2, 2])\r\nlda = LDA(n_components=3).fit(X, y)\r\nlda.transform(X).shape\r\n```\r\nproduces\r\n`(6, 2)` while I expect it to be `(6, 3)` as `n_components == 3`\r\n\r\nLooking at the code of transform:\r\n`return np.dot(X, self.coef_[:n_comp].T)`\r\n\r\nWhere `self.coef_` is computed as in `.fit()`: \r\n`self.coef_ = np.dot(self.means_ - self.xbar_, self.scalings_)`\r\n\r\nwhich produces a 2x1 matrix (or whatever the number of means is).\r\n\r\nNot sure what the correct thing would be but it seems that `LDA.coef_` should have length `n_compoments`, no?\r\n'"
1965,14347442,pprett,pprett,2013-05-15 08:05:44,2013-06-02 11:15:54,2013-06-02 11:15:36,closed,pprett,0.14-rc,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1965,b'SGD: L2 penalty should be applied before additive update',"b'The issue was caught by @mblondel ::\r\n\r\n    In scikit-learn, we do\r\n    w <- w - eta * g\r\n    then\r\n    w <- w * (1 - eta * alpha)\r\n\r\n    The last step is wrong as this will scale (w - eta * g) instead of w.\r\n    So, if we want to do a proper SGD update, we should first do the regularization step, *then* the additive step.'"
1959,14254568,cheparukhin,amueller,2013-05-13 10:38:36,2014-10-01 23:17:13,2014-07-18 13:09:01,closed,,,14,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1959,b'Error while fitting GridSearchCV',"b'    svm = Pipeline([\r\n        (\'chi2\', SelectKBest(chi2)),\r\n        (\'svm\', LinearSVC(class_weight=\'auto\'))\r\n    ])\r\n\r\n    vectorizer = TfidfVectorizer(input=\'filename\')\r\n\r\n    classifier = Pipeline([\r\n        (\'vect\', vectorizer),\r\n        (\'clf\', OneVsRestClassifier(svm))\r\n    ])\r\n\r\n    parameters = {\r\n        \'vect__min_df\': (1, 2, 5),\r\n        \'vect__max_df\': (0.5, 0.75, 1.0),\r\n        \'vect__ngram_range\': ((1, 1), (1, 2), (1, 3)),\r\n        \'clf__estimator__chi2__k\': (100, 1000, \'all\'),\r\n        \'clf__estimator__svm__C\': (1.0, 3.0, 10.0),\r\n        \'clf__estimator__svm__fit_intercept\': (True, False)\r\n    }\r\n\r\n    data = ...\r\n    target = ...\r\n    hamming_loss = ...\r\n\r\n    grid = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, scoring=Scorer(hamming_loss, greater_is_better=False))\r\n\r\n    grid.fit(data, target)\r\n\r\nTraceback (most recent call last):\r\n  File ""main.py"", line 140, in <module>\r\n    grid.fit(data, target)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py"", line 687, in fit\r\n    return self._fit(X, y, ParameterGrid(self.param_grid), **params)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py"", line 456, in _fit\r\n    parameter_iterator for train, test in cv)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py"", line 516, in __call__\r\n    self.retrieve()\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py"", line 449, in retrieve\r\n    raise exception_type(report)\r\nTypeError: function takes exactly 5 arguments (1 given)'"
1946,14050952,larsmans,amueller,2013-05-07 14:15:27,2014-01-07 16:27:09,2014-01-07 16:27:09,closed,,,21,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1946,"b""BUG OneVsRestClassifier doesn't play nicely with SVC""","b""Reported at [SO](http://stackoverflow.com/q/16402236/166749):\r\n\r\n    classifier = OneVsRestClassifier(SVC(class_weight='auto'))\r\n    classifier.fit(X1, y1)\r\n    y2 = classifier.predict(X2)\r\n\r\nraises an exception because `SVC.predict_proba` doesn't actually work unless `probability=True` is passed.\r\n\r\nSuggested fix: refactor the SVM code to introduce a new class, `PlattSVC`, that functions like `SVC(probability=True)` except that it uses probabilities for `predict` and `decision_function` also. Remove `predict_proba` from `SVC`. That would immediately fix the problem that `SVC.predict_proba` is inconsistent with `decision_function` and `predict`."""
1931,13977455,amueller,amueller,2013-05-05 12:22:11,2014-01-07 16:21:52,2014-01-07 16:21:52,closed,,,8,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1931,b'Test failure in common tests with KernelPCA',"b""As @larsmans observed, KernelPCA doesn't abide by its ``n_components`` parameter.\r\nHe introduced a new parameter ``remove_zero_eig`` in #1758 to control removal of empty components with default=``True``\r\n\r\nMaybe we should aim at changing the default in the future for more expected results?\r\nIn the meantime, I'll push a quick fix for the common tests to set the parameter there."""
1925,13931217,jaquesgrobler,larsmans,2013-05-03 11:48:33,2013-08-26 16:36:18,2013-08-26 16:36:18,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1925,b'Fix setup.py to resolve numpy requirement',"b""As mentioned in [Ark's mailing list report](https://sourceforge.net/mailarchive/message.php?msg_id=30798802).\r\n\r\nFeel free to discuss it further here."""
1921,13927612,ogrisel,larsmans,2013-05-03 09:32:28,2015-04-24 16:13:50,2013-05-03 15:48:55,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1921,b'RidgeCV triggers a call to toarray on sparse matrix input',b'and thus causes a `MemoryError` on high dimensional data.\r\n\r\nDetails here: http://stackoverflow.com/a/16351308/163740'
1905,13724649,sviridenich,arjoly,2013-04-27 18:56:37,2014-01-29 18:25:30,2014-01-29 18:25:30,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1905,b'test_linearsvc_iris fails',"b'I recently installed scikit tool and when testing it got error. Test of the sparse Linear SVC with iris data set failed with the following assertion error: Arrays are not almost equal to 6 decimals < mismatch 0.666666667% >\r\n\r\nSystem Description \r\nscikit version 0.13.1\r\npython 2.7\r\nOS windows 7, 32-bit'"
1903,13693816,tobigue,larsmans,2013-04-26 15:21:35,2013-05-04 15:10:40,2013-05-04 15:10:40,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1903,b'Bug: DictVectorizer throws exception for empty/unknown feature dict',"b'The DictVectorizer throws a ValueError when no features in the dict are known instead of returning an empty vector.\r\n\r\n    >>> from sklearn.feature_extraction import DictVectorizer\r\n    >>> a = DictVectorizer()\r\n    >>> data = [{""j"":1, ""n"":1}]\r\n    >>> a.fit(data)\r\n    DictVectorizer(dtype=<type \'numpy.float64\'>, separator=\'=\', sparse=True)\r\n    >>> a.transform([{""j"":1}])\r\n    <1x2 sparse matrix of type \'<type \'numpy.float64\'>\'\r\n      with 1 stored elements in Compressed Sparse Row format>\r\n\r\n    >>> a.transform([])\r\n    Traceback (most recent call last):\r\n      File ""<console>"", line 1, in <module>\r\n      File ""/home/tobi/projectname/eggs/scikit_learn-0.13.1-py2.7-linux-i686.egg/sklearn/feature_extraction/dict_vectorizer.py"", line 218, in transform\r\n        indices = np.frombuffer(indices, dtype=np.int32)\r\n    ValueError: offset must be non-negative and smaller than buffer lenth (0)\r\n\r\n    >>> a.transform([{}])\r\n    Traceback (most recent call last):\r\n      File ""<console>"", line 1, in <module>\r\n      File ""/home/tobi/projectname/eggs/scikit_learn-0.13.1-py2.7-linux-i686.egg/sklearn/feature_extraction/dict_vectorizer.py"", line 218, in transform\r\n        indices = np.frombuffer(indices, dtype=np.int32)\r\n    ValueError: offset must be non-negative and smaller than buffer lenth (0)\r\n\r\n    >>> a.transform([{""x"":1}])\r\n    Traceback (most recent call last):\r\n      File ""<console>"", line 1, in <module>\r\n      File ""/home/tobi/projectname/eggs/scikit_learn-0.13.1-py2.7-linux-i686.egg/sklearn/feature_extraction/dict_vectorizer.py"", line 218, in transform\r\n        indices = np.frombuffer(indices, dtype=np.int32)\r\n    ValueError: offset must be non-negative and smaller than buffer lenth (0)'"
1901,13685799,jaquesgrobler,larsmans,2013-04-26 12:00:23,2013-04-26 12:15:55,2013-04-26 12:11:50,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1901,b'LibSVM GUI error',"b'as reported by Shishir Pandey [here](https://sourceforge.net/mailarchive/message.php?msg_id=30772772)\r\n\r\n>I get the following error:\r\n\r\n>```\r\nTraceback (most recent call last):\r\n   File ""C:\\Users\\xyz\\ml\\svm_gui.py"", line 30, in <module>\r\n     from sklearn.externals.six.moves import xrange\r\nImportError: No module named six.moves\r\n```\r\nThis error is for sklearn version 0.13.1 I am assuming six.move might be \r\nadded to 0.14 version. Because the link I used was for the dev version \r\nof sklearn - \r\nhttp://scikit-learn.org/dev/auto_examples/applications/svm_gui.html#example-applications-svm-gui-py\r\n\r\n>Whats new about xrange in six.moves?'"
1900,13682725,jaquesgrobler,amueller,2013-04-26 10:45:30,2013-05-04 11:36:18,2013-05-04 11:36:18,closed,,,0,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1900,"b""DOC: MiniBatchKMeans doesn't re-run its algorithm 'n_init' needs to be documented""","b""As Stefano Lattarini mentioned [here](http://permalink.gmane.org/gmane.comp.python.scikit-learn/7090)\r\n\r\n>When the 'n_init' argument is given, I'd expect both of these classes\r\nto run the corresponding algorithm (Lloyd and mini-batch k-means,\r\nrespectively) 'n_init' times on the data to be fitted, each time with\r\na different initialization, and then select the result which gives the\r\nsmallest inertia.\r\n\r\n>However, while this expectation is met by the KMeans class, it's not\r\nreally met the by the MiniBatchKMeans class: the latter only executes\r\nthe *initialization* of centroids 'n_init' times, then selecting the\r\ninitialization that gives the smallest inertia, and running the mini-batch\r\nk-means algorithm only once, with that initialization.\r\n\r\nThe above is intended behavior and needs to be mentioned in the docs, as the MiniBatch is meant to do only a few passes on the data, for efficiency reasons"""
1897,13681178,larsmans,larsmans,2013-04-26 09:57:40,2013-05-20 20:59:23,2013-05-20 20:59:23,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1897,"b""AdaBoost predict_proba doesn't do any weighting""","b'`AdaBoostClassifier`\'s current implementation of `predict_proba` promises to return ""the ""weighted mean predicted class probabilities of the classifiers in the ensemble"", but it doesn\'t touch its `estimator_weights_` (or rather, it loops over that attribute\'s values but ignores them).\r\n\r\nPing @glouppe.'"
1896,13679628,larsmans,larsmans,2013-04-26 09:12:35,2013-04-26 10:30:51,2013-04-26 10:30:08,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1896,b'SVR complains about single class in AdaBoost tests',"b'This one\'s escaping me:\r\n\r\n    $ nosetests sklearn/ensemble/tests/test_weight_boosting.py\r\n    .........E\r\n    ======================================================================\r\n    ERROR: Test different base estimators.\r\n    ----------------------------------------------------------------------\r\n    Traceback (most recent call last):\r\n      File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n        self.test(*self.arg)\r\n      File ""/scratch/apps/src/scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py"", line 213, in test_base_estimator\r\n        clf.fit(X, y)\r\n      File ""/scratch/apps/src/scikit-learn/sklearn/ensemble/weight_boosting.py"", line 911, in fit\r\n        return super(AdaBoostRegressor, self).fit(X, y, sample_weight)\r\n      File ""/scratch/apps/src/scikit-learn/sklearn/ensemble/weight_boosting.py"", line 126, in fit\r\n        X_argsorted=X_argsorted)\r\n      File ""/scratch/apps/src/scikit-learn/sklearn/ensemble/weight_boosting.py"", line 972, in _boost\r\n        estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\r\n      File ""/scratch/apps/src/scikit-learn/sklearn/svm/base.py"", line 143, in fit\r\n        raise ValueError(""The number of classes has to be greater than""\r\n    ValueError: The number of classes has to be greater than one.\r\n    \r\n    ----------------------------------------------------------------------\r\n    Ran 10 tests in 0.893s\r\n    \r\n    FAILED (errors=1)\r\n\r\nThis doesn\'t happen every time, though:\r\n\r\n    $ nosetests sklearn/ensemble/tests/test_weight_boosting.py\r\n    ..........\r\n    ----------------------------------------------------------------------\r\n    Ran 10 tests in 0.899s\r\n\r\n    OK\r\n\r\n... since the AdaBoost tests don\'t set `random_state`.'"
1894,13662924,bwhite,amueller,2013-04-25 21:28:33,2015-02-13 22:09:19,2013-05-05 09:34:12,closed,,,15,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1894,b'Decision function is broken in SVC',"b""Here is a gist illustrating the problem\r\nhttps://gist.github.com/bwhite/5463217\r\n\r\nHere are two lines of the output (run the script to make 100 of these)\r\n\r\nExample 1\r\n\r\n    true_label[0] predictions: svm.predict[1] svm2.predict[1] svm2.dec >= intercept[1] manual[1] decision_functions: svm.decision_function[nan] svm2.decision_function[0.305067] manual[-0.472692]\r\n\r\n\r\nExample 2\r\n\r\n    true_label[0] predictions: svm.predict[0] svm2.predict[0] svm2.dec >= intercept[1] manual[0] decision_functions: svm.decision_function[nan] svm2.decision_function[-0.112419] manual[-0.0552063]\r\n\r\nWe'd expect all prediction and decision function methods to agree, but the predictions are all the same (expect for the one that I derive from svm2's decision_function) but all decision_functions disagree.  The reason I show the prediction derived from svm2's decision_function is because it shows that it is almost certainly wrong because it doesn't agree with it's own prediction function.  However, my own custom predictor and decision_function (derived from the raw model parameters) are all consistent with the true predictions.\r\n\r\nIn summary, I think the following are problematic\r\n* svm and svm2 disagree at all, since there shouldn't be a difference between computing the gram matrix manually vs letting sklearn do it\r\n* svm's decision function (the one where we let sklearn compute the gram matrix) is basically infinite, so that is really wrong.\r\n* svm2's seems more reasonable but still wrong since using it's intercept as a threshold disagrees with it's own prediction\r\n- my handmade predictor/decision_function agree in prediction with svm/svm2 but the decision_function is way off\r\n"""
1877,13442331,atulskulkarni,GaelVaroquaux,2013-04-20 23:09:41,2014-04-15 14:41:10,2013-04-24 15:13:27,closed,,,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1877,b'pip install fails on Ubuntu 12.04 ',"b'I a trying to install scikits learn on Ubuntu 12.04 box the commands from ""http://scikit-learn.org/stable/install.html#"" but for some reason after following ""1.1.1.1. Easy install"" -> sudo pip install -U scikit-learn I get the following error. \r\n\r\nerror: Command ""/usr/bin/g77 -g -Wall -g -Wall -shared build/temp.linux-x86_64-2.7/sklearn/cluster/_k_means.o -L/usr/lib -Lbuild/temp.linux-x86_64-2.7 -lcblas -lm -lg2c -o build/lib.linux-x86_64-2.7/sklearn/cluster/_k_means.so"" failed with exit status 1\r\n\r\nThe error states the following...\r\n\r\ncustomize GnuFCompiler\r\n\r\nFound executable /usr/bin/g77\r\n\r\ngnu: no Fortran 90 compiler found\r\n\r\ngnu: no Fortran 90 compiler found\r\n\r\ncustomize GnuFCompiler\r\n\r\ngnu: no Fortran 90 compiler found\r\n\r\ngnu: no Fortran 90 compiler found\r\n\r\ncustomize GnuFCompiler using build_ext\r\n\r\nbuilding \'sklearn.cluster._k_means\' extension\r\n\r\ncompiling C sources\r\n\r\nC compiler: gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC\r\n\r\n\r\n\r\ncompile options: \'-DNO_ATLAS_INFO=1 -Isklearn/src/cblas -I/usr/lib/python2.7/dist-packages/numpy/core/include -I/usr/lib/python2.7/dist-packages/numpy/core/include -I/usr/include/python2.7 -c\'\r\n\r\ngcc: sklearn/cluster/_k_means.c\r\n\r\n/usr/lib/python2.7/dist-packages/numpy/core/include/numpy/__ufunc_api.h:226:1: warning: _import_umath defined but not used [-Wunused-function]\r\n\r\n/usr/bin/g77 -g -Wall -g -Wall -shared build/temp.linux-x86_64-2.7/sklearn/cluster/_k_means.o -L/usr/lib -Lbuild/temp.linux-x86_64-2.7 -lcblas -lm -lg2c -o build/lib.linux-x86_64-2.7/sklearn/cluster/_k_means.so\r\n\r\n/usr/bin/ld: cannot find crti.o: No such file or directory\r\n\r\n/usr/bin/ld: cannot find -lgcc_s\r\n\r\ncollect2: ld returned 1 exit status\r\n\r\n/usr/bin/ld: cannot find crti.o: No such file or directory\r\n\r\n/usr/bin/ld: cannot find -lgcc_s\r\n\r\nis it due to the fact that Ubuntu 12.04 does not have any fortran compiler and I installed g77 and setup expects Fortran 90? - just guessing... \r\n\r\nAppreciate any help on this.'"
1876,13415804,jaquesgrobler,jaquesgrobler,2013-04-19 19:06:49,2013-04-20 15:11:11,2013-04-20 15:11:11,closed,,,5,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1876,b'DOC: Numbering on Tutorials page is buggy',b'The numbering on the tutorials menu of the docs are kind of screwy as shown below\r\n\r\n![Screenshot at 2013-04-19 21:02:33](https://f.cloud.github.com/assets/1378870/403585/13e57102-a924-11e2-93de-f3cee7307989.png)\r\n'
1872,13365191,jseabold,larsmans,2013-04-18 18:45:18,2013-04-22 12:41:07,2013-04-22 12:41:07,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1872,b'Problem downloading faces dataset',"b'On dbd3109. Python version 2.7.\r\n\r\n```\r\n>>> from sklearn.datasets import fetch_olivetti_faces\r\n>>> dataset = fetch_olivetti_faces(shuffle=True)\r\ndownloading Olivetti faces from http://cs.nyu.edu/~roweis/data/olivettifaces.mat to /home/skipper/scikit_learn_data\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/datasets/olivetti_faces.py"", line 96, in fetch_olivetti_faces\r\n    buf = StringIO(fhandle.read())\r\nTypeError: initial_value must be unicode or None, not str\r\n```'"
1869,13338569,jaquesgrobler,jaquesgrobler,2013-04-18 08:19:38,2013-05-05 21:33:25,2013-05-05 21:33:25,closed,,,11,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1869,b'Sidebar bug when returning to docs homepage',"b'Due to the sidebar button being hidden on the home page, the following bug arises: If the sidebar is collapsed elsewhere in the documentation, and the user returns to the home-page, the sidebar stays collapsed and cannot be re-expanded as the button is hidden on this page.'"
1860,13202693,breuderink,larsmans,2013-04-15 14:38:43,2015-02-06 20:47:17,2015-02-06 20:47:17,closed,,0.16,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1860,b'Divide by zero in Nystrm approximation.',"b'Sometimes I get what seems to be a divide-by-zero in the Nystrm kernel approximation:\r\n\r\n```\r\n/usr/local/lib/python2.7/site-packages/sklearn/kernel_approximation.py:445: RuntimeWarning: divide by zero encountered in divide\r\n  self.normalization_ = np.dot(U * 1. / np.sqrt(S), V)\r\n/usr/local/lib/python2.7/site-packages/sklearn/kernel_approximation.py:445: RuntimeWarning: invalid value encountered in divide\r\n  self.normalization_ = np.dot(U * 1. / np.sqrt(S), V)\r\n```\r\n\r\nI am trying to minimal example to reproduce the problem, but so far I have not been able to do so. I expect that some eigenvalues of the kernel matrix are rounded to zero, which could be caused by a rank-deficient kernel matrix. This is with sklearn version 0.13.1.\r\n\r\nIf that is the case, this problem can be fixed by adding a tiny ridge to the kernel matrix, or by adding a tiny value to the eigenvalues ``S``. Alternatively, one could use only positive eigenvalues in the computation.'"
1838,12800084,manaswis,amueller,2013-04-04 13:34:46,2014-09-23 17:25:53,2013-04-04 19:53:03,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1838,b'nose test fails - tried with both methods',"b'I have tried both the methods.\r\nI am using Ubuntu 12.04 (32bit) with python 2.7.3, numpy 1.6.1, scipy 0.9.0\r\n\r\nI first installed using apt-get install python-sklearn. But there were many failures from both the methods. Then I remove python-sklearn with apt-get remove python-sklearn. Then I tried installing it from the source package using \r\n\r\n`python setup.py build`   \r\n`sudo python setup.py install`\r\n\r\nWhen I tried running the nose test, it failed (the error is shown below).\r\n\r\n-------------------------------------------------------------------------ERROR trace1---------------------------------------------------------------------------\r\n\r\n~/OpenSource$ nosetests sklearn --exe\r\n\r\nE\r\n======================================================================\r\nERROR: Failure: ImportError (No module named _check_build\r\n___________________________________________________________________________\r\nContents of /home/manaswi/OpenSource/scikit-learn-0.13.1/sklearn/__check_build:\r\n_check_build.pyx          __init__.pyc              _check_build.c\r\n__init__.py               setup.pyc                 setup.py\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget to build the package before using it: run `python setup.py install` or `make` in the source directory. If you have used an installer, please check that it is suited for your Python version, your operating system and your platform.\r\n\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/loader.py"", line 390, in loadTestsFromName\r\n    addr.filename, addr.module)\r\n  File ""/usr/lib/python2.7/dist-packages/nose/importer.py"", line 39, in importFromPath\r\n    return self.importFromDir(dir_path, fqname)\r\n  File ""/usr/lib/python2.7/dist-packages/nose/importer.py"", line 86, in importFromDir\r\n    mod = load_module(part_fqname, fh, filename, desc)\r\n  File ""/home/manaswi/OpenSource/scikit-learn-0.13.1/sklearn/__init__.py"", line 31, in <module>\r\n    from . import __check_build\r\n  File ""/home/manaswi/OpenSource/scikit-learn-0.13.1/sklearn/__check_build/__init__.py"", line 47, in <module>\r\n    raise_build_error(e)\r\n  File ""/home/manaswi/OpenSource/scikit-learn-0.13.1/sklearn/__check_build/__init__.py"", line 42, in raise_build_error\r\n    %s"""""" % (e, local_dir, \'\'.join(dir_content).strip(), msg))\r\nImportError: No module named _check_build\r\n___________________________________________________________________________\r\nContents of /home/manaswi/OpenSource/scikit-learn-0.13.1/sklearn/__check_build:\r\n_check_build.pyx          __init__.pyc              _check_build.c\r\n__init__.py               setup.pyc                 setup.py\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget to build the package before using it: run `python setup.py install` or `make` in the source directory. If you have used an installer, please check that it is suited for your\r\nPython version, your operating system and your platform.\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.001s\r\n\r\nFAILED (errors=1)\r\n\r\n***\r\n\r\n**Then I installed sklearn package again. I tried the nosetests as well with python -c ""import sklearn; sklearn.test()"".\r\nThe errors are shown below:**\r\n\r\n-------------------------------------------------------------------------ERROR trace2---------------------------------------------------------------------------\r\n~/OpenSource$ nosetests sklearn --exe\r\n.............................................................../usr/local/lib/python2.7/dist-packages/sklearn/manifold/spectral_embedding.py:225: UserWarning: Graph is not fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n...........SS...............S.....................................................S.........................................................S................................................................../usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0xcee2fa4>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................S...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................F....................................................................................SSS....S....S...................................................................................................................................\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_transformers\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/tests/test_common.py"", line 230, in test_transformers\r\n    ""fit_transform not correct in %s"" % Trans)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 800, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 636, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\nfit_transform not correct in <class \'sklearn.pls.CCA\'>\r\n(mismatch 50.0%)\r\n x: array([[ 0.97459066, -0.37275475],\r\n       [-0.97199043,  0.22191067],\r\n       [-0.97199043,  0.22191067],...\r\n y: array([[  9.74590662e-01,  -8.38417465e-16],\r\n       [ -9.71990427e-01,   4.91932783e-16],\r\n       [ -9.71990427e-01,   4.91932783e-16],...\r\n\r\n----------------------------------------------------------------------\r\nRan 1603 tests in 90.243s\r\n\r\nFAILED (SKIP=11, failures=1)\r\n\r\n***\r\n\r\n**Trying with python -c ""import sklearn; sklearn.test()"" gave me the following: **\r\n\r\n***\r\n\r\n-------------------------------------------------------------------------ERROR trace3---------------------------------------------------------------------------\r\n~/OpenSource$ python -c ""import sklearn; sklearn.test()""\r\n\r\nOther than the Doctest failures, I get the following error:\r\n\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_transformers\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/tests/test_common.py"", line 237, in test_transformers\r\n    ""fit_transform not correct in %s"" % Trans)\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 800, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 600, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\nfit_transform not correct in <class \'sklearn.decomposition.kernel_pca.KernelPCA\'>\r\n(shapes (30, 20), (30, 18) mismatch)\r\n x: array([[  1.87664949e+00,   8.57398986e-02,   4.20312700e-02,\r\n          3.63938109e-08,   0.00000000e+00,   0.00000000e+00,\r\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,...\r\n y: array([[  1.87664949e+00,   8.57398986e-02,   4.20312700e-02,\r\n         -3.35096502e-08,  -5.65023583e-09,   6.94581104e-09,\r\n          1.93372216e-08,   7.76708475e-09,   4.25320650e-08,...\r\n\r\n----------------------------------------------------------------------\r\nRan 1737 tests in 270.588s\r\n\r\nFAILED (SKIP=11, errors=21, failures=15)\r\n\r\n***\r\nHow should I resolve this?'"
1835,12764122,fraka6,amueller,2013-04-03 17:53:43,2013-07-26 17:42:07,2013-07-26 17:42:07,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1835,b'metrics.confusion_matrix(...) bug ',"b""if you specify the optional labels has an input, it has to be int (labels = np.asarray(labels), dtype=np.int)) which doesn't make sense. \r\n\r\nThe proposed fix it simple, we should remove the dtype constraint. """
1832,12754405,amueller,amueller,2013-04-03 14:18:15,2013-04-03 16:37:25,2013-04-03 16:37:25,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1832,"b""MinMaxScaler doesn't convert input to float""",b'So it converts mnist to all zeros basically :-/'
1831,12749422,amueller,amueller,2013-04-03 12:09:04,2015-05-04 20:03:18,2015-05-04 20:01:30,closed,,0.16,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1831,b'Bug in score method in GridSearchCV',"b'~~The call to ``scoring`` is broken.~~\r\nAlso, I think it is weird that the base estimator ``score`` method is preferred over the ``scorer``. This means that ``best_score_`` is not something you can get by calling ``score``.'"
1826,12698759,amueller,amueller,2013-04-02 10:17:01,2014-09-23 17:25:53,2014-01-07 16:29:17,closed,,0.15,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1826,b'Test failure in common tests / CCA',"b""In ``test_transformers`` CCA's ``fit_transform`` fails for me on 32bit ubuntu. Anyone else?"""
1815,12452065,isofer,amueller,2013-03-26 14:53:03,2013-05-04 12:52:37,2013-05-04 12:52:37,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1815,"b'GridSearchCV().fit(x, y, **params) does not use the params argument'","b'Hi, \r\nI believe this is a small and easy bug to fix.\r\nGridSearchCV().fit(x, y, **params) does not use the params argument, so it can be removed from the function.\r\n\r\nAlso, in the example: http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py\r\nthe following lines should be updated:\r\nfrom:\r\n```\r\n    clf = GridSearchCV(SVC(C=1), tuned_parameters, score_func=score_func)\r\n    clf.fit(X_train, y_train, cv=5)\r\n```\r\n\r\nto:\r\n```\r\n    clf = GridSearchCV(SVC(C=1), tuned_parameters, score_func=score_func, cv=5)\r\n    clf.fit(X_train, y_train)\r\n```'"
1808,12389131,xianping,larsmans,2013-03-25 07:00:49,2014-08-23 03:59:55,2013-12-05 13:17:54,closed,,,19,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1808,b'test failed after installation in ubuntu12',"b'I installed the package using \'apt-get install python-sklearn\', and then tested it by using \'nosetests sklearn --exe\', but failed. Error messages below:\r\n\r\n\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/tests/test_lfw.py"", line 120, in test_load_fake_lfw_people\r\n    min_faces_per_person=3)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 337, in load_lfw_people\r\n    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 266, in fetch_lfw_people\r\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\r\nTypeError: __init__() got an unexpected keyword argument \'compress\'\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading LFW people faces from /tmp/scikit_learn_lfw_test_yijxlU/lfw_home\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people_too_restrictive\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/python2.7/dist-packages/nose/tools.py"", line 80, in newfunc\r\n    func(*arg, **kw)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/tests/test_lfw.py"", line 149, in test_load_fake_lfw_people_too_restrictive\r\n    load_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 337, in load_lfw_people\r\n    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 266, in fetch_lfw_people\r\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\r\nTypeError: __init__() got an unexpected keyword argument \'compress\'\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading LFW people faces from /tmp/scikit_learn_lfw_test_yijxlU/lfw_home\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n======================================================================\r\nERROR: sklearn.datasets.tests.test_lfw.test_load_fake_lfw_pairs\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/tests/test_lfw.py"", line 158, in test_load_fake_lfw_pairs\r\n    lfw_pairs_train = load_lfw_pairs(data_home=SCIKIT_LEARN_DATA)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 433, in load_lfw_pairs\r\n    return fetch_lfw_pairs(download_if_missing=download_if_missing, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/datasets/lfw.py"", line 404, in fetch_lfw_pairs\r\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\r\nTypeError: __init__() got an unexpected keyword argument \'compress\'\r\n-------------------- >> begin captured logging << --------------------\r\nsklearn.datasets.lfw: INFO: Loading train LFW pairs from /tmp/scikit_learn_lfw_test_yijxlU/lfw_home\r\n--------------------- >> end captured logging << ---------------------\r\n\r\n----------------------------------------------------------------------\r\nRan 613 tests in 64.717s\r\n\r\nFAILED (SKIP=6, errors=3)\r\n\r\nps: my environment :(lsb_release -a)\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 12.04.1 LTS\r\nRelease:        12.04\r\nCodename:       precise'"
1793,12208639,erg,amueller,2013-03-20 00:14:49,2015-03-06 15:38:49,2013-03-31 16:05:53,closed,,,11,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1793,"b'input validation with shape (0,N>0) for RandomForestClassifier, DecisionTreeClassifier, others?'","b'```\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\nX = np.ones(shape=(0,1))\r\ny = np.ones(shape=(0,1))\r\n\r\nrfc = RandomForestClassifier()\r\n\r\nIn [36]: rfc.fit(X,y)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-36-eec77b0a9246> in <module>()\r\n----> 1 rfc.fit(XX,yy)\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc in fit(self, X, y, sample_weight)\r\n    363                 random_state.randint(MAX_INT),\r\n    364                 verbose=self.verbose)\r\n--> 365             for i in xrange(n_jobs))\r\n    366 \r\n    367         # Reduce\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\r\n    512         try:\r\n    513             for function, args, kwargs in iterable:\r\n--> 514                 self.dispatch(function, args, kwargs)\r\n    515 \r\n    516             self.retrieve()\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in dispatch(self, func, args, kwargs)\r\n    309         """"""\r\n    310         if self._pool is None:\r\n--> 311             job = ImmediateApply(func, args, kwargs)\r\n    312             index = len(self._jobs)\r\n    313             if not _verbosity_filter(index, self.verbose):\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __init__(self, func, args, kwargs)\r\n    133         # Don\'t delay the application, to avoid keeping the input\r\n    134         # arguments in memory\r\n--> 135         self.results = func(*args, **kwargs)\r\n    136 \r\n    137     def get(self):\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc in _parallel_build_trees(n_trees, forest, X, y, sample_weight, sample_mask, X_argsorted, seed, verbose)\r\n     86                 curr_sample_weight = sample_weight.copy()\r\n     87 \r\n---> 88             indices = random_state.randint(0, n_samples, n_samples)\r\n     89             sample_counts = bincount(indices, minlength=n_samples)\r\n     90 \r\n\r\n/usr/local/lib/python2.7/site-packages/numpy/random/mtrand.so in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:6443)()\r\n\r\nValueError: low >= high\r\n```\r\n\r\nAlso for decision trees:\r\n```\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\nX = np.ones(shape=(0,1))\r\ny = np.ones(shape=(0,1))\r\n\r\ndtc = DecisionTreeClassifier()\r\ndtc.fit(X,y)\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-41-bd6795874945> in <module>()\r\n----> 1 dtc.fit(X,y)\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/tree/tree.pyc in fit(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\r\n    358                          sample_weight=sample_weight,\r\n    359                          sample_mask=sample_mask,\r\n--> 360                          X_argsorted=X_argsorted)\r\n    361 \r\n    362         if self.n_outputs_ == 1:\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/tree/_tree.so in sklearn.tree._tree.Tree.build (sklearn/tree/_tree.c:4823)()\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/tree/_tree.so in sklearn.tree._tree.Tree.build (sklearn/tree/_tree.c:4636)()\r\n\r\n/usr/local/lib/python2.7/site-packages/sklearn/tree/_tree.so in sklearn.tree._tree.Tree.recursive_partition (sklearn/tree/_tree.c:5156)()\r\n\r\nValueError: Attempting to find a split with an empty sample_mask.\r\n```'"
1789,12141980,amueller,ogrisel,2013-03-18 16:38:37,2013-04-10 13:53:17,2013-04-10 13:53:17,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1789,"b""GridSearchCV can't be pickled""",b'which is really unfortunate :-/'
1782,12096836,larsmans,larsmans,2013-03-16 16:12:00,2013-03-16 20:18:21,2013-03-16 20:18:21,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1782,"b""GridSearchCV doesn't set cv_scores_ when the grid contains only one point""",b'This corner case is handled specially. The easiest fix would probably to handle it in the loop.'
1771,11971933,duschendestroyer,agramfort,2013-03-13 12:58:24,2014-12-15 09:31:26,2014-12-15 09:31:26,closed,,0.16,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1771,b'RFECV step option broken',"b'the scores array is always initialized to np.zeros(X.shape[1]), but with a step != 1 less scores are collected. The trailing zeros lead to the false number of features being selected.'"
1762,11899407,amueller,glouppe,2013-03-11 21:41:44,2013-07-28 07:47:25,2013-07-28 07:47:00,closed,,,12,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1762,b'AdaBoost docs broken',"b'The reference documentation for the AdaBoost estimators is not build.\r\nSphinx throws a lot of errors, but I have no idea what is appening :-/'"
1759,11855052,amueller,NelleV,2013-03-10 17:36:01,2013-07-26 16:22:24,2013-07-26 16:22:24,closed,,0.14-rc,17,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1759,b'Error building docs',"b'When trying to build the docs (``make html``), I get the following error on each example.\r\nIt still runs through, though. Not sure what is happening here.\r\n```\r\ninvalid syntax (<string>, line 1)\r\nextracting function failed\r\n```\r\n'"
1757,11840309,xiyancn,amueller,2013-03-09 16:15:08,2014-07-18 20:39:49,2014-07-18 16:11:30,closed,,0.15.1,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1757,b'nosetests fail: singular matrix in CCA',"b'```\r\nnosetests sklearn --exe\r\n.............................................................../usr/local/lib/python2.7/site-packages/scikit_learn-0.13.1-py2.7-linux-x86_64.egg/sklearn/manifold/spectral_embedding.py:225: UserWarning: Graph is not fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n...........SS....../usr/local/lib/python2.7/site-packages/scikit_learn-0.13.1-py2.7-linux-x86_64.egg/sklearn/datasets/tests/test_base.py:124: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n.../usr/local/lib/python2.7/site-packages/scikit_learn-0.13.1-py2.7-linux-x86_64.egg/sklearn/datasets/tests/test_base.py:145: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n./usr/local/lib/python2.7/site-packages/scikit_learn-0.13.1-py2.7-linux-x86_64.egg/sklearn/datasets/tests/test_base.py:161: UserWarning: Could not load sample images, PIL is not available.\r\n  warnings.warn(""Could not load sample images, PIL is not available."")\r\n.....SS................................................S.........................................................S.........................................SSS....................../usr/local/lib/python2.7/site-packages/scikit_learn-0.13.1-py2.7-linux-x86_64.egg/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x6e36f18>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................S...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................F............................................................................SSS....S....S...................................................................................................................................\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_regressors_train\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/local/lib/python2.7/site-packages/scikit_learn-0.13.1-py2.7-linux-x86_64.egg/sklearn/tests/test_common.py"", line 655, in test_regressors_train\r\n    assert_true(succeeded)\r\nAssertionError: False is not true\r\n-------------------- >> begin captured stdout << ---------------------\r\nCCA(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06)\r\nsingular matrix\r\n\r\n\r\n--------------------- >> end captured stdout << ----------------------\r\n\r\n----------------------------------------------------------------------\r\nRan 1598 tests in 81.829s\r\n\r\nFAILED (SKIP=15, failures=1)\r\n```\r\n\r\nIs the failures a big deal ? Can I use it anyway ? I want to use sklearn to do logistic regression . Thanks. '"
1749,11736963,amueller,larsmans,2013-03-06 22:47:40,2013-03-08 16:45:10,2013-03-08 16:45:10,closed,,,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1749,"b""Fix class_weight='auto' in SGDClassifier""","b""afaik SGDClassifier calls ``compute_class_weight`` with the ``y`` it got as input. Instead, it should use the class-indices corresponding to ``y``.\r\nNot sure if @larsmans is on it.\r\nI think ``y`` should just be replaced by ``np.searchsorted(classes, y)`` but I'm to tired to do it right now."""
1744,11703172,amueller,mblondel,2013-03-06 08:04:07,2013-03-06 11:49:55,2013-03-06 11:31:46,closed,,,3,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1744,b'svm_light loader docstring out of data?',"b""The [svm light loader docstring](http://scikit-learn.sourceforge.net/dev/modules/generated/sklearn.datasets.load_svmlight_file.html#sklearn.datasets.load_svmlight_file) says that the function is written in python and doesn't scale well. It does call a cython implmentation, though, as far as I can see.\r\nIs the docstring still appropriate?\r\nping @larsmans @mblondel """
1734,11644501,jamestwebber,amueller,2013-03-04 23:20:12,2013-05-08 00:08:04,2013-03-10 17:42:34,closed,,,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1734,b'Possible memory leak? Lasso/LassoCV in 0.13.1 uses far more memory than in 0.12.1.',"b""I'm not sure what's causing this, but after upgrading I noticed exploding memory usage (several GB in use) when I tried to run Lasso or LassoCV on my dataset.\r\n\r\nLasso in particular was a big difference--the version in 0.12.1 would use on the order of 100 MB of memory and finish quickly, while doing the same thing in 0.13.1 hit 5+ GB and I had to kill it. The story for LassoCV was similar but it uses more memory overall (<1 GB for 0.12.1, maybe GB for 0.13.1).\r\n\r\nPresumably the leak is in Lasso, although I'm not sure why LassoCV is using so much more (shouldn't it use the same amount?). Maybe there is a separate leak in LassoCV in 0.12.1, and in 0.13.1 this explodes due to the new Lasso leak.\r\n\r\nOn my laptop this renders these methods unusable--it starts to swap and it's way too slow.\r\n\r\nedit: I am using numpy 1.6.2, based on other tickets I will upgrade and see if this fixes the problem."""
1729,11590831,meimeifish,amueller,2013-03-03 12:53:22,2013-03-05 08:20:02,2013-03-05 08:20:02,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1729,b'[Install problem] nosetest failed. ',"b'Please help me with my installation.\r\nI followed the guideline to install sklearn and numpy, but when I ran the nosetest, it kept showing the following testing message, and finally a failed message also cited below:\r\n\r\n         "" nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),""\r\n         .......................................................................\r\n         ""Ran 1603 tests in 92.776s\r\n          FAILED (SKIP=14, failures=1) ""\r\n\r\nBecause the nosetest log is messy, I tried to excerpt non-repetitive paragraph for your diagnosis. \r\n\r\nI very appreciate your kind help.\r\n\r\n=============================================================\r\n\r\nMicrosoft Windows [\xb0\xe6\xb1\xbe 6.1.7601]\r\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n\r\nC:\\Windows\\system32>python -c ""import sklearn; sklearn.test()""\r\nRunning unit tests and doctests for sklearn\r\nNumPy version 1.7.0\r\nNumPy is installed in C:\\Python27\\lib\\site-packages\\numpy\r\nPython version 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)\r\n]\r\nnose version 1.2.1\r\nI: Seeding RNGs with 369887653\r\nConverged after 58 iterations.\r\n...............................................................C:\\Python27\\lib\\s\r\nite-packages\\sklearn\\manifold\\spectral_embedding.py:225: UserWarning: Graph is n\r\not fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n..2.17927109077 55.4082834902\r\n........None\r\n.FFFFFFFFF.........S...............S............................................\r\n..............S.........................................................S..F....\r\n......................................SSS.......................C:\\Python27\\lib\\\r\nsite-packages\\sklearn\\externals\\joblib\\test\\test_func_inspect.py:122: UserWarnin\r\ng: Cannot inspect object <functools.partial object at 0x07500900>, ignore list w\r\nill not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n.............................___________________________________________________\r\n_____________________________\r\ntest_memory setup\r\n________________________________________________________________________________\r\n\r\n________________________________________________________________________________\r\n\r\ntest_memory teardown\r\n________________________________________________________________________________\r\n\r\n..._____________________________________________________________________________\r\n___\r\n[Memory] Calling sklearn.externals.joblib.test.test_hashing.KlassWithCachedMetho\r\nd.f...\r\nf({ \'#s12069__c_maps.nii.gz\': [33],\r\n  \'#s12158__c_maps.nii.gz\': [33],\r\n  \'#s12258__c_maps.nii.gz\': [33],\r\n  \'#s12277__c_maps.nii.gz\': [33],\r\n  \'#s12300__c_maps.nii.gz\': [33],\r\n  \'#s12401__c_maps.nii.gz\': [33],\r\n  \'#s12430__c_maps.nii.gz\': [33],\r\n  \'#s13817__c_maps.nii.gz\': [33],\r\n  \'#s13903__c_maps.nii.gz\': [33],\r\n  \'#s13916__c_maps.nii.gz\': [33],\r\n  \'#s13981__c_maps.nii.gz\': [33],\r\n  \'#s13982__c_maps.nii.gz\': [33],\r\n  \'#s13983__c_maps.nii.gz\': [33]})\r\n________________________________________________________________f - 0.0s, 0.0min\r\n\r\n._______________________________________________________________________________\r\n_\r\n[Memory] Calling sklearn.externals.joblib.test.test_hashing.KlassWithCachedMetho\r\nd.f...\r\nf(set([ \'#s12069__c_maps.nii.gz\',\r\n      \'#s12158__c_maps.nii.gz\',\r\n      \'#s12258__c_maps.nii.gz\',\r\n      \'#s12277__c_maps.nii.gz\',\r\n      \'#s12300__c_maps.nii.gz\',\r\n      \'#s12401__c_maps.nii.gz\',\r\n      \'#s12430__c_maps.nii.gz\',\r\n      \'#s13817__c_maps.nii.gz\',\r\n      \'#s13903__c_maps.nii.gz\',\r\n      \'#s13916__c_maps.nii.gz\',\r\n      \'#s13981__c_maps.nii.gz\',\r\n      \'#s13982__c_maps.nii.gz\',\r\n      \'#s13983__c_maps.nii.gz\']))\r\n________________________________________________________________f - 0.0s, 0.0min\r\n\r\n..______________________________________________________________________________\r\n__\r\ntest_memory setup\r\n________________________________________________________________________________\r\n\r\n.............[Memory]    0.0s, 0.0min: Loading sklearn.externals.joblib.test.tes\r\nt_memory.f-alias from c:\\users\\navicat\\appdata\\local\\temp\\tmpydjb57\\joblib\\sklea\r\nrn\\externals\\joblib\\test\\test_memory\\f-alias\\48c4a83e3726da917be80d0e1edc1be1\r\n_____________________________________________f-alias cache loaded - 0.0s, 0.0min\r\n\r\n...[Memory]    0.0s, 0.0min: Loading sklearn.externals.joblib.test.test_memory.f\r\n-alias from c:\\users\\navicat\\appdata\\local\\temp\\tmpydjb57\\joblib\\sklearn\\externa\r\nls\\joblib\\test\\test_memory\\f-alias\\48c4a83e3726da917be80d0e1edc1be1\r\n_____________________________________________f-alias cache loaded - 0.0s, 0.0min\r\n\r\n................................................................................\r\n........................________________________________________________________\r\n________________________\r\ntest_memory teardown\r\n________________________________________________________________________________\r\n\r\n._______________________________________________________________________________\r\n_\r\nsetup numpy_pickle\r\n________________________________________________________________________________\r\n\r\n.....................................C:\\Python27\\lib\\site-packages\\sklearn\\exter\r\nnals\\joblib\\test\\test_numpy_pickle.py:197: Warning: file ""c:\\users\\navicat\\appda\r\nta\\local\\temp\\tmpfsyive\\test.pkl96"" appears to be a zip, ignoring mmap_mode ""r""\r\nflag passed\r\n  numpy_pickle.load(this_filename, mmap_mode=\'r\')\r\n.....___________________________________________________________________________\r\n_____\r\nteardown numpy_pickle\r\n________________________________________________________________________________\r\n\r\n.............S......C:\\Python27\\lib\\site-packages\\sklearn\\feature_selection\\univ\r\nariate_selection.py:327: UserWarning: Duplicate scores. Result may depend on fea\r\nture ordering.There are probably duplicate features, or you used a classificatio\r\nn score for a regression task.\r\n  warn(""Duplicate scores. Result may depend on feature ordering.""\r\n...........C:\\Python27\\lib\\site-packages\\numpy\\lib\\utils.py:139: DeprecationWarn\r\ning: `cs_graph_components` is deprecated!\r\nIn the future, use csgraph.connected_components. Note that this new function has\r\n a slightly different interface: see the docstring for more information.\r\n  warnings.warn(depdoc, DeprecationWarning)\r\nC:\\Python27\\lib\\site-packages\\numpy\\lib\\utils.py:139: DeprecationWarning: `cs_gr\r\naph_components` is deprecated!\r\nIn the future, use csgraph.connected_components. Note that this new function has\r\n a slightly different interface: see the docstring for more information.\r\n  warnings.warn(depdoc, DeprecationWarning)\r\n.C:\\Python27\\lib\\site-packages\\numpy\\lib\\utils.py:139: DeprecationWarning: `cs_g\r\nraph_components` is deprecated!\r\nIn the future, use csgraph.connected_components. Note that this new function has\r\n a slightly different interface: see the docstring for more information.\r\n  warnings.warn(depdoc, DeprecationWarning)\r\nC:\\Python27\\lib\\site-packages\\numpy\\lib\\utils.py:139: DeprecationWarning: `cs_gr\r\naph_components` is deprecated!\r\nIn the future, use csgraph.connected_components. Note that this new function has\r\n a slightly different interface: see the docstring for more information.\r\n  warnings.warn(depdoc, DeprecationWarning)\r\n.C:\\Python27\\lib\\site-packages\\numpy\\lib\\utils.py:139: DeprecationWarning: `cs_g\r\nraph_components` is deprecated!\r\nIn the future, use csgraph.connected_components. Note that this new function has\r\n a slightly different interface: see the docstring for more information.\r\n  warnings.warn(depdoc, DeprecationWarning)\r\nC:\\Python27\\lib\\site-packages\\numpy\\lib\\utils.py:139: DeprecationWarning: `cs_gr\r\naph_components` is deprecated!\r\nIn the future, use csgraph.connected_components. Note that this new function has\r\n a slightly different interface: see the docstring for more information.\r\n  warnings.warn(depdoc, DeprecationWarning)\r\n........................................................................C:\\Pytho\r\nn27\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:109: Run\r\ntimeWarning: invalid value encountered in divide\r\n  f = msb / msw\r\n.......................F...............S...................C:\\Python27\\lib\\site-\r\npackages\\sklearn\\linear_model\\least_angle.py:233: UserWarning: Regressors in act\r\nive set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.870e-\r\n02, with an active set of 2 regressors, and the smallest cholesky pivot element\r\nbeing 2.220e-16\r\n  % (n_iter, alpha, n_active, diag))\r\n...---\r\n................................................................................\r\n...............................................................C:\\Python27\\lib\\s\r\nite-packages\\nose\\util.py:14: DeprecationWarning: The compiler package is deprec\r\nated and removed in Python 3.x.\r\n  from compiler.consts import CO_GENERATOR\r\nC:\\Python27\\lib\\site-packages\\nose\\util.py:14: DeprecationWarning: The compiler\r\npackage is deprecated and removed in Python 3.x.\r\n  from compiler.consts import CO_GENERATOR\r\n...........................C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\te\r\nsts\\test_sparse_coordinate_descent.py:104: UserWarning: With alpha=0, this algor\r\nithm does not converge well. You are advised to use the LinearRegression estimat\r\nor\r\n  clf.fit(X, Y)\r\n....C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:666\r\n: UserWarning: precompute is ignored for sparse data\r\n  warnings.warn(""precompute is ignored for sparse data"")\r\nC:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:288: Us\r\nerWarning: Objective did not converge for target 0, you might want to increase t\r\nhe number of iterations\r\n  \' to increase the number of iterations\')\r\n....................C:\\Python27\\lib\\site-packages\\numpy\\core\\_methods.py:57: Run\r\ntimeWarning: invalid value encountered in double_scalars\r\n  ret = ret / float(rcount)\r\n............................................................................C:\\P\r\nython27\\lib\\site-packages\\sklearn\\utils\\extmath.py:244: RuntimeWarning: underflo\r\nw encountered in exp\r\n  out = np.log(np.sum(np.exp(arr - vmax), axis=0))\r\nC:\\Python27\\lib\\site-packages\\sklearn\\mixture\\dpgmm.py:42: RuntimeWarning: under\r\nflow encountered in exp\r\n  v = np.exp(v - out)\r\n..........................................C:\\Python27\\lib\\site-packages\\sklearn\\\r\nmixture\\gmm.py:307: RuntimeWarning: underflow encountered in exp\r\n  responsibilities = np.exp(lpr - logprob[:, np.newaxis])\r\n..................................................F.......C:\\Python27\\lib\\site-p\r\nackages\\sklearn\\utils\\__init__.py:71: DeprecationWarning: Class NuSVC is depreca\r\nted; to be removed in v0.14;\r\nuse sklearn.svm.NuSVC instead\r\n  warnings.warn(msg, category=DeprecationWarning)\r\n.C:\\Python27\\lib\\site-packages\\sklearn\\utils\\__init__.py:71: DeprecationWarning:\r\n Class NuSVR is deprecated; to be removed in v0.14;\r\nuse sklearn.svm.NuSVR instead\r\n  warnings.warn(msg, category=DeprecationWarning)\r\n.C:\\Python27\\lib\\site-packages\\sklearn\\utils\\__init__.py:71: DeprecationWarning:\r\n Class SVC is deprecated; to be removed in v0.14;\r\nuse sklearn.svm.SVC instead\r\n  warnings.warn(msg, category=DeprecationWarning)\r\n.C:\\Python27\\lib\\site-packages\\sklearn\\utils\\__init__.py:71: DeprecationWarning:\r\n Class SVR is deprecated; to be removed in v0.14;\r\nuse sklearn.svm.SVR instead\r\n  warnings.warn(msg, category=DeprecationWarning)\r\n.......................................\r\nC:\\Windows\\system32>nosetests sklearn --exe\r\n................................................................................\r\n.............................................................C:\\Python27\\lib\\sit\r\ne-packages\\sklearn\\manifold\\spectral_embedding.py:225: UserWarning: Graph is not\r\n fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n......C:\\Python27\\lib\\site-packages\\sklearn\\manifold\\spectral_embedding.py:225:\r\nUserWarning: Graph is not fully connected, spectral embedding may not works as e\r\nxpected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n......................S........S.................S......S.......................\r\n................................................................................\r\nS.........S.....................................................................\r\n......................................S....S....................................\r\n.........................................SSS........................C:\\Python27\\\r\nlib\\site-packages\\sklearn\\externals\\joblib\\test\\test_func_inspect.py:122: UserWa\r\nrning: Cannot inspect object <functools.partial object at 0x04A099C0>, ignore li\r\nst will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n....................................S.SS........................................\r\n................................................................................\r\n.......................C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\te\r\nst\\test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.parti\r\nal object at 0x04A099C0>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n...........S..............S.....................................................\r\n................................................................................\r\n.........................................................................S...S..\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n..................................................................FF............\r\n................................................................................\r\n....................................................C:\\Python27\\lib\\site-package\r\ns\\sklearn\\manifold\\spectral_embedding.py:225: UserWarning: Graph is not fully co\r\nnnected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n................C:\\Python27\\lib\\site-packages\\sklearn\\manifold\\spectral_embeddin\r\ng.py:225: UserWarning: Graph is not fully connected, spectral embedding may not\r\nworks as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n...............S...............S.S................S.............................\r\n.......................................................................S........\r\n.....................S..........................................................\r\n............................S...........S.......................................\r\n..........................SSS......................C:\\Python27\\lib\\site-packages\r\n\\sklearn\\externals\\joblib\\test\\test_func_inspect.py:122: UserWarning: Cannot ins\r\npect object <functools.partial object at 0x04A099C0>, ignore list will not work.\r\n\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n................................................................................\r\n.........................................................SSS....................\r\n.....C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\test\\test_func_inspe\r\nct.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x04A\r\n099C0>, ignore list will not work.\r\n\r\n\r\n\r\n\r\n\r\n======================================================================\r\nFAIL: Test the sparse LinearSVC with the iris dataset\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Python27\\lib\\site-packages\\nose\\case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\svm\\tests\\test_sparse.py"", line 17\r\n3, in test_linearsvc_iris\r\n    clf.predict(iris.data.todense()), sp_clf.predict(iris.data))\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in asse\r\nrt_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in asse\r\nrt_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 0.666666666667%)\r\n x: array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,\r\n       0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 0, 0, 1,\r\n       2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0,...\r\n y: array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,\r\n       0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 0, 0, 1,\r\n       2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0,...\r\n\r\n----------------------------------------------------------------------\r\nRan 1603 tests in 92.776s\r\n\r\nFAILED (SKIP=14, failures=1)\r\n.........................S.................S....................................\r\n.....C:\\Python27\\lib\\site-packages\\sklearn\\manifold\\spectral_embedding.py:225: U\r\nserWarning: Graph is not fully connected, spectral embedding may not works as ex\r\npected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n............................................................S...........S.......\r\n.................S.............................................S................\r\n....................................................S...........................\r\n...........................SSS......................C:\\Python27\\lib\\site-package\r\ns\\sklearn\\externals\\joblib\\test\\test_func_inspect.py:122: UserWarning: Cannot in\r\nspect object <functools.partial object at 0x049F99C0>, ignore list will not work\r\n.\r\n\r\n\r\n'"
1724,11557310,desilinguist,larsmans,2013-03-01 19:05:04,2013-03-14 14:11:03,2013-03-14 13:11:35,closed,,,11,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1724,b'StandardScaler ignores with_std for sparse matrices',"b""Hi,\r\n\r\nLooking at the code for StandardScaler's fit() method in preprocessing.py, I noticed that with_std is ignored when sp.issparse(X) is true. I noticed this when trying to make a dummy transformation in a pipeline where I was trying to decide whether to scale feature values or not. """
1723,11553627,httassadar,larsmans,2013-03-01 17:23:25,2013-03-02 13:35:58,2013-03-02 13:19:28,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1723,"b'MinMaxScaler bug when target is not [0,1], version 0.13'","b'First of all, the formula in the document is wrong. The division in the second line should be multiplication. \r\n\r\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\r\n    X_scaled = X_std / (max - min) + min\r\n\r\nMore interestingly, the script neither follows this formula nor gives the correct result,\r\n\r\n    X_train = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])\r\n    scaler = MinMaxScaler(feature_range = (0,0.5))\r\n    scaler.fit_transform(X_train)\r\n\r\nThis gives \r\n\r\n     0.25        0.25        0.66666667\r\n     0.5         0.5         0.33333333\r\n     0.          0.75        0.16666667\r\n\r\nNote the second column has values > 0.5'"
1709,11332802,pemistahl,amueller,2013-02-24 11:08:13,2014-08-20 02:17:53,2013-07-27 13:20:18,closed,,0.14-rc,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1709,"b""ValueError: Buffer dtype mismatch, expected 'DOUBLE' but got 'long long'""","b'In scikit-learn 0.13.0, I\'m trying to use the class `sklearn.preprocessing.StandardScaler` to scale my data for being used in an SVM classifier of class `sklearn.svm.LinearSVC`. The essential parts of my code are the following:\r\n\r\n```python\r\n\r\nvectorizer = CountVectorizer(...)\r\n\r\nX_train = vectorizer.fit_transform(my_training_data_here)\r\ny_train = np.array(my_labels_here)\r\nX_test = vectorizer.transform(my_test_data_here)\r\n\r\nscaler = StandardScaler(with_mean=False)\r\nX_train_scaled = scaler.fit_transform(X=X_train)\r\nX_test_scaled = scaler.transform(X=X_test)\r\n\r\nlinear_svm_classifier = LinearSVC()\r\nlinear_svm_classifier.fit(X=X_train_scaled, y=y_train)\r\npredictions = linear_svm_classifier.predict(X=X_test_scaled)\r\n```\r\n\r\nUnfortunately, an exception is raised by the line `X_train_scaled = scaler.fit_transform(X=X_train)`. This is the relevant part of the stacktrace:\r\n\r\n```python\r\n\r\n/[...]/sklearn/utils/validation.py:230: UserWarning: StandardScaler assumes floating point values as input, got int64\r\n""got %s"" % (estimator, X.dtype))\r\nTraceback (most recent call last):\r\n[...]\r\nX_train_scaled = scaler.fit_transform(X=X_train)\r\n  File ""/[...]/sklearn/base.py"", line 361, in fit_transform\r\n    return self.fit(X, **fit_params).transform(X)\r\n  File ""/[...]/sklearn/preprocessing.py"", line 302, in fit\r\n    var = mean_variance_axis0(X)[1]\r\n  File ""sparsefuncs.pyx"", line 272, in sklearn.utils.sparsefuncs.mean_variance_axis0 (sklearn/utils/sparsefuncs.c:3551)\r\n  File ""sparsefuncs.pyx"", line 41, in sklearn.utils.sparsefuncs.csr_mean_variance_axis0 (sklearn/utils/sparsefuncs.c:1416)\r\nValueError: Buffer dtype mismatch, expected \'DOUBLE\' but got \'long long\'\r\n```\r\n\r\nDo I have to change the `dtype` myself? If so, how do I do that? Thank youl'"
1708,11330172,chie4live,amueller,2013-02-24 04:55:46,2013-07-25 08:30:37,2013-07-25 08:30:37,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1708,b'Nosetests fails repeatedly in python 3',"b'I get this eror each time i run Nosetests using the command "">>nosetests sklearn --exe"".\r\nI guess the nosetests has to pass before i continue using scikit-learn?    \r\n\r\nclf.fit(X, y)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in fit\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 513, in __call__\r\n    for function, args, kwargs in iterable:\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in <genexpr>\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\cross_validation.py"", line 284, in __iter__\r\n    for i in xrange(n_folds):\r\nNameError: global name \'xrange\' is not defined\r\n\r\n======================================================================\r\nERROR: Pass X as list in GridSearchCV\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\tests\\test_grid_search.py"", line 277, in test_X_as_list\r\n    grid_search.fit(X.tolist(), y).score(X, y)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in fit\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 513, in __call__\r\n    for function, args, kwargs in iterable:\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in <genexpr>\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\cross_validation.py"", line 284, in __iter__\r\n    for i in xrange(n_folds):\r\nNameError: global name \'xrange\' is not defined\r\n\r\n======================================================================\r\nERROR: sklearn.tests.test_grid_search.test_unsupervised_grid_search\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\tests\\test_grid_search.py"", line 286, in test_unsupervised_grid_search\r\n    grid_search.fit(X)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in fit\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 513, in __call__\r\n    for function, args, kwargs in iterable:\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in <genexpr>\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\cross_validation.py"", line 284, in __iter__\r\n    for i in xrange(n_folds):\r\nNameError: global name \'xrange\' is not defined\r\n\r\n======================================================================\r\nERROR: sklearn.tests.test_multiclass.test_ovr_gridsearch\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\tests\\test_multiclass.py"", line 200, in test_ovr_gridsearch\r\n    cv.fit(iris.data, iris.target)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in fit\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 513, in __call__\r\n    for function, args, kwargs in iterable:\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in <genexpr>\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\cross_validation.py"", line 379, in __iter__\r\n    for i in xrange(n_folds):\r\nNameError: global name \'xrange\' is not defined\r\n\r\n======================================================================\r\nERROR: sklearn.tests.test_multiclass.test_ovo_gridsearch\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\tests\\test_multiclass.py"", line 258, in test_ovo_gridsearch\r\n    cv.fit(iris.data, iris.target)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in fit\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 513, in __call__\r\n    for function, args, kwargs in iterable:\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in <genexpr>\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\cross_validation.py"", line 379, in __iter__\r\n    for i in xrange(n_folds):\r\nNameError: global name \'xrange\' is not defined\r\n\r\n======================================================================\r\nERROR: sklearn.tests.test_multiclass.test_ecoc_gridsearch\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\tests\\test_multiclass.py"", line 286, in test_ecoc_gridsearch\r\n    cv.fit(iris.data, iris.target)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in fit\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py"", line 513, in __call__\r\n    for function, args, kwargs in iterable:\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\grid_search.py"", line 372, in <genexpr>\r\n    for clf_params in grid for train, test in cv)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\cross_validation.py"", line 379, in __iter__\r\n    for i in xrange(n_folds):\r\nNameError: global name \'xrange\' is not defined\r\n\r\n======================================================================\r\nFAIL: Test Area under Receiver Operating Characteristic (ROC) curve\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 92, in test_roc_curve\r\n    assert_array_almost_equal(roc_auc, 0.80, decimal=2)\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.8932676518883417)\r\n y: array(0.8)\r\n\r\n======================================================================\r\nFAIL: roc_curve for confidence scores\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 126, in test_roc_curve_confidence\r\n    assert_array_almost_equal(roc_auc, 0.80, decimal=2)\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.8932676518883417)\r\n y: array(0.8)\r\n\r\n======================================================================\r\nFAIL: roc_curve for hard decisions\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 148, in test_roc_curve_hard\r\n    assert_array_almost_equal(roc_auc, 0.74, decimal=2)\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.7627257799671592)\r\n y: array(0.74)\r\n\r\n======================================================================\r\nFAIL: Test Precision Recall and F1 Score for binary classification task\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 194, in test_precision_recall_f1_score_binary\r\n    assert_array_almost_equal(p, [0.73, 0.75], 2)\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.63333333,  0.9       ])\r\n y: array([ 0.73,  0.75])\r\n\r\n======================================================================\r\nFAIL: Test confusion matrix - binary classification case\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 257, in test_confusion_matrix_binary\r\n    assert_array_equal(cm, [[19, 6], [7, 18]])\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 719, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 50.0%)\r\n x: array([[19,  2],\r\n       [11, 18]])\r\n y: array([[19,  6],\r\n       [ 7, 18]])\r\n\r\n======================================================================\r\nFAIL: Test Precision Recall and F1 Score for multiclass classification task\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 286, in test_precision_recall_f1_score_multiclass\r\n    assert_array_almost_equal(p, [0.82, 0.55, 0.47], 2)\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.79166667,  0.48148148,  0.625     ])\r\n y: array([ 0.82,  0.55,  0.47])\r\n\r\n======================================================================\r\nFAIL: Test confusion matrix - multi-class case\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 371, in test_confusion_matrix_multiclass\r\n    [0, 2, 18]])\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 719, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 77.77777777777777%)\r\n x: array([[19,  4,  1],\r\n       [ 5, 13,  8],\r\n       [ 0, 10, 15]])\r\n y: array([[23,  2,  0],\r\n       [ 5,  5, 20],\r\n       [ 0,  2, 18]])\r\n\r\n======================================================================\r\nFAIL: Test confusion matrix - multi-class case with subset of labels\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 387, in test_confusion_matrix_multiclass_subset_labels\r\n    [5, 5]])\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 719, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 75.0%)\r\n x: array([[19,  4],\r\n       [ 5, 13]])\r\n y: array([[23,  2],\r\n       [ 5,  5]])\r\n\r\n======================================================================\r\nFAIL: Test performance report\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 414, in test_classification_report\r\n    assert_equal(report, expected_report)\r\nAssertionError: \'             precision    recall  f1-score   support\\n\\n     setosa       0.79  [truncated]... != \'             precision    recall\r\nf1-score   support\\n\\n     setosa       0.82  [truncated]...\r\nDiff is 701 characters long. Set self.maxDiff to None to see it.\r\n\r\n======================================================================\r\nFAIL: sklearn.metrics.tests.test_metrics.test_precision_recall_curve\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""c:\\python33\\lib\\site-packages\\nose\\case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 432, in test_precision_recall_curve\r\n    _test_precision_recall_curve(y_true, probas_pred)\r\n  File ""c:\\python33\\lib\\site-packages\\sklearn\\metrics\\tests\\test_metrics.py"", line 452, in _test_precision_recall_curve\r\n    assert_array_almost_equal(precision_recall_auc, 0.82, 2)\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""c:\\python33\\lib\\site-packages\\numpy\\testing\\utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not almost equal to 2 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array(0.9315492337212434)\r\n y: array(0.82)\r\n\r\n----------------------------------------------------------------------\r\nRan 1328 tests in 258.019s\r\n\r\nFAILED (SKIP=15, errors=77, failures=10)\r\n'"
1703,11281424,amueller,larsmans,2013-02-22 10:55:27,2013-03-15 12:21:59,2013-03-15 11:37:41,closed,pprett,,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1703,"b""SGDClassifier doesn't forget previous fit""","b""SGDClassifier doesn't forget the previous fit. The ``classes_`` parameter is stored, leading to an error if fit repeatedly with different label sets.\r\nCould be as easy as removing ``classes_`` in ``fit``.\r\nShows the need for a test in ``test_common``."""
1690,11114810,larsmans,larsmans,2013-02-18 13:31:58,2013-03-17 10:17:41,2013-03-17 10:17:41,closed,,,8,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1690,"b""KernelPCA doesn't abide by its n_components parameter""","b'`KernelPCA` may return fewer components than requested, because it filters out zero eigenvalues. This should be documented, and maybe it should be an optional feature.'"
1671,10825394,burakbayramli,burakbayramli,2013-02-10 16:56:40,2013-02-10 17:32:32,2013-02-10 17:32:32,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1671,b'LassoCV Does Not Find Best Lambda',"b'I am on scikit-learn version 0.13-git. Here is the problem: The lambda value entered by hand 0.3 for Lasso performs much better than 0.013 found by LassoCV which utilizes crossvalidation. I used the standard diabetes data. \r\n\r\nhttps://gist.github.com/burakbayramli/4750196\r\n\r\nI based the code on this page\r\n\r\nhttp://scipy-lectures.github.com/advanced/scikit-learn/index.html#sparse-models\r\n\r\nThanks,'"
1659,10698282,bobhancock,larsmans,2013-02-06 16:22:10,2013-06-24 12:20:24,2013-03-08 17:54:59,closed,,,27,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1659,b'Test hangs on Ubuntu 12.04 LTS',b'nosetests sklearn --exe  \r\n...\r\nsklearn.decomposition.tests.test_pca.test_infer_dim_3 ... ok\r\nsklearn.decomposition.tests.test_pca.test_infer_dim_by_explained_variance ... ok\r\nTest that probabilistic PCA yields a reasonable score ... ok\r\nTest that probabilistic PCA correctly separated different datasets ... ok\r\nThe homoscedastic model should work slightly worth ... ok\r\nCheck that ppca select the right model ... ok\r\nsklearn.decomposition.tests.test_sparse_pca.test_correct_shapes ... ok\r\nsklearn.decomposition.tests.test_sparse_pca.test_fit_transform ... \r\n\r\nHangs here.\r\n'
1658,10678522,yarikoptic,GaelVaroquaux,2013-02-06 03:12:36,2013-02-17 10:20:01,2013-02-17 10:20:01,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1658,"b'incorrect auc() value if [0,0] is not included in x,y inputs'","b""```\r\nIn [114]: auc(fpr, tpr)\r\nOut[114]: 0.79316049999999971\r\n\r\nIn [115]: auc([0] + fpr, [0] + tpr)\r\nOut[115]: 0.8437834999999998\r\n```\r\nAFAIK (0,0) point must be included in AUC computation.  ATM roc_curve doesn't guarantee producing 0,0 results, thus leading to incorrect AUC estimation.  I was not sure if you want to fix it in roc_curve (probably not) or just in auc, so just filing a report."""
1649,10564372,ksemb,agramfort,2013-02-02 00:03:08,2015-01-28 08:15:10,2015-01-28 08:15:10,closed,,0.16,27,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1649,b'Univariate variate scaling in LDA',"b'This bug report concerns the univariate normalization that is done in the scikit-learn LDA implementation. The following shows the source code lines from file /usr/lib/pymodules/python2.7/sklearn/lda.py of scikit-learn version 0.13-1 which are relevant for the bug report.\r\n\r\n```matlab\r\n148: std = Xc.std(axis=0)\r\n150: std[std == 0] = 1.\r\n154: X = np.sqrt(fac) * (Xc / std)\r\n156: U, S, V = linalg.svd(X, full_matrices=0)\r\n162: scalings = (V[:rank] / std).T / S[:rank]\r\n```\r\n\r\nIf I understand the code correctly it does the following: Normalize X before SVD (line 154), perform SVD (line 156), correct SVD results for the previous normalization (line 162).\r\n\r\nThe code seems to work when more training samples than features are present, i.e. when the linear system is overdetermined. When the linear system is underdetermined, i.e. when there are less samples than features, the code fails. I have, however, not yet been able to find a mathematical formulation of the algorithm that shows what is going on in the code and why it is working at all.\r\n\r\nI have not yet seen this normalization step in any other LDA code (e.g. Matlab\'s ""classify"" method does not do this normalization), nor have I ever heard of this normalization before.\r\n\r\nBelow is the test case that I ran with normalization enabled (the original code) and with normalization disabled (I replaced line 150 by ""std[:] = 1.""). If normalization is disabled, everything works fine. If normalization is enabled, the code works in the overdetermined case (i.e. more training samples than feature dimensions) and it fails in the underdetermined case (less training samples than data dimensions).\r\n\r\n```python\r\n############# test case ############\r\nimport sklearn.lda\r\nimport numpy.random\r\n\r\nclassif = sklearn.lda.LDA()\r\n\r\ndef test(N):\r\n    numpy.random.seed(0)\r\n\r\n    print \'N=%d\'%N\r\n    p=80\r\n\r\n    Xf=numpy.random.rand(N,p)\r\n    y=numpy.random.random_integers(0,1,(N,1))\r\n    Xp=numpy.random.rand(11,p)\r\n\r\n    # Generate a random unitary matrix Q\r\n    [Q,R] = numpy.linalg.qr(numpy.random.rand(p,p))\r\n\r\n    # Apply different scalings to the features.\r\n    # The classification results should NOT change if the LDA implementation is correct.\r\n    for scale in numpy.logspace(1,10,5):\r\n        D = numpy.eye(p)\r\n        D[0,0] = scale\r\n        # generate a scaling matrix\r\n        S = Q.dot(D).dot(Q.T)\r\n        classif.fit(Xf.dot(S),y,tol=1e-11)\r\n        print classif.predict(Xp.dot(S))\r\n\r\nprint(\'overdetermined case (many more samples than dimensions)\')\r\ntest(100)\r\nprint(\'underdetermined case (less samples than dimensions)\')\r\ntest(10)\r\n\r\n############# results with normalization enabled ##############\r\n./bugreport.py \r\noverdetermined case (many more samples than dimensions)\r\nN=100\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\nunderdetermined case (less samples than dimensions)\r\nN=10\r\n[1 0 0 0 1 1 1 0 1 1 1]\r\n[1 0 0 1 0 1 0 0 1 1 0]\r\n[1 0 0 1 0 1 0 0 1 1 0]\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n\r\n############# results with normalization disabled ##############\r\n./bugreport.py \r\noverdetermined case (many more samples than dimensions)\r\nN=100\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\n[0 0 0 0 0 1 0 0 1 1 1]\r\nunderdetermined case (less samples than dimensions)\r\nN=10\r\n/usr/lib/pymodules/python2.7/sklearn/lda.py:161: UserWarning: Variables are collinear\r\n  warnings.warn(""Variables are collinear"")\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n[1 0 0 1 1 1 0 0 1 1 1]\r\n```\r\n'"
1641,10459314,skorokithakis,larsmans,2013-01-30 13:59:25,2013-07-26 08:26:25,2013-07-26 08:26:25,closed,ogrisel,0.14-rc,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1641,b'GridSearchCV documentation sample not working.',"b'Hello,\r\nI am trying to run the ""working with text"" example on the tutorial. However, I get an error:\r\n\r\n```python\r\nclassifier = Pipeline([(\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'clf\', LinearSVC())])\r\nparameters = {\'vect__analyzer__max_n\': (1, 2), \'tfidf__use_idf\': (True, False), \'clf__C\': (100, 1000)}\r\ngs_clf = GridSearchCV(classifier, parameters, n_jobs=1)\r\ngs_clf.fit([""foo"", ""bar"", ""baz""], [0, 1, 0])\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/proj/env/local/lib/python2.7/site-packages/django/core/management/commands/shell.pyc in <module>()\r\n----> 1 gs_clf.fit([""foo"", ""bar"", ""baz""], [0, 1, 0])\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit(self, X, y, **params)\r\n    370                                self.loss_func, self.score_func, self.verbose,\r\n    371                                **self.fit_params)\r\n--> 372                            for clf_params in grid for train, test in cv)\r\n    373 \r\n    374         # Out is a list of triplet: score, estimator, n_test_samples\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\r\n    512         try:\r\n    513             for function, args, kwargs in iterable:\r\n--> 514                 self.dispatch(function, args, kwargs)\r\n    515 \r\n    516             self.retrieve()\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in dispatch(self, func, args, kwargs)\r\n    309         """"""\r\n    310         if self._pool is None:\r\n--> 311             job = ImmediateApply(func, args, kwargs)\r\n    312             index = len(self._jobs)\r\n    313             if not _verbosity_filter(index, self.verbose):\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __init__(self, func, args, kwargs)\r\n    133         # Don\'t delay the application, to avoid keeping the input\r\n    134         # arguments in memory\r\n--> 135         self.results = func(*args, **kwargs)\r\n    136 \r\n    137     def get(self):\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit_grid_point(X, y, base_clf, clf_params, train, test, loss_func, score_func, verbose, **fit_params)\r\n     82     # update parameters of the classifier after a copy of its base structure\r\n     83     clf = clone(base_clf)\r\n---> 84     clf.set_params(**clf_params)\r\n     85 \r\n     86     if hasattr(base_clf, \'kernel\') and hasattr(base_clf.kernel, \'__call__\'):\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/base.pyc in set_params(self, **params)\r\n    235                                      (name, self))\r\n    236                 sub_object = valid_params[name]\r\n--> 237                 sub_object.set_params(**{sub_name: value})\r\n    238             else:\r\n    239                 # simple objects case\r\n\r\n/proj/env/local/lib/python2.7/site-packages/sklearn/base.pyc in set_params(self, **params)\r\n    235                                      (name, self))\r\n    236                 sub_object = valid_params[name]\r\n--> 237                 sub_object.set_params(**{sub_name: value})\r\n    238             else:\r\n    239                 # simple objects case\r\n\r\nAttributeError: \'str\' object has no attribute \'set_params\'\r\n```'"
1634,10417221,superphil0,amueller,2013-01-29 14:07:55,2013-01-30 12:23:15,2013-01-30 12:23:15,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1634,"b'Failure in nostests sklearn in python2.7/dist-packages/sklearn/cross_validation.py line 1349, in train_test_split'","b'When trying to run the nosetests I get the following error:\r\n\r\nnosetests sklearn --exe\r\n.............................................................../usr/local/lib/python2.7/dist-packages/sklearn/manifold/spectral_embedding.py:225: UserWarning: Graph is not fully connected, spectral embedding may not works as expected.\r\n  warnings.warn(""Graph is not fully connected, spectral embedding""\r\n...........SS....................................................................S.........................................................S................................................................../usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x3956100>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n.....................................................................................................................................................................................................................................................................................................................................................................................S................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................E.......................................................................SSS....S....S..................................................................................................................................\r\n======================================================================\r\nERROR: Split arrays or matrices into random train and test subsets\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/lib/python2.7/dist-packages/nose/util.py"", line 622, in newfunc\r\n    return func(*arg, **kw)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py"", line 1349, in train_test_split\r\n    raise ValueError(""At least one array required as input"")\r\nValueError: At least one array required as input\r\n\r\n----------------------------------------------------------------------\r\nRan 1338 tests in 240.836s\r\n\r\nFAILED (SKIP=10, errors=1)\r\n'"
1632,10411082,amueller,kastnerkyle,2013-01-29 10:19:35,2014-07-21 14:29:05,2014-07-21 14:28:26,closed,,0.15.1,13,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1632,b'Test Failure in LLE  on ubuntu 12.10',"b'So my box in uni got upgraded and now I have this test failure:\r\n```\r\n======================================================================\r\nFAIL: sklearn.manifold.tests.test_locally_linear.test_lle_simple_grid\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/local/lamueller/checkout/scikit-learn/sklearn/manifold/tests/test_locally_linear.py"", line 68, in test_lle_simple_grid\r\n    assert_less(np.linalg.norm(X_reembedded - clf.embedding_), tol)\r\nAssertionError: 0.12820616722921346 not less than 0.1\r\n    """"""Fail immediately, with the given message.""""""\r\n>>  raise self.failureException(\'0.12820616722921346 not less than 0.1\')\r\n```\r\nscipy 0.10.1, 32bit.'"
1630,10379668,larsmans,larsmans,2013-01-28 15:46:26,2013-11-25 18:11:10,2013-11-25 18:11:10,closed,,,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1630,b'SGDClassifier dumps core when given CSR matrices with lots of features',"b'To reproduce, get [`url_classify.py`](https://gist.github.com/4656581), get [`training data`](http://www.sysnet.ucsd.edu/projects/url/url_svmlight.tar.gz), then run the script directly on the tarball. The SVMlight loader seems to work fine, but the `SGDClassifier` dumps core, usually right after printing `-- Epoch 1`.'"
1625,10318070,ChrisBeaumont,pprett,2013-01-25 16:57:11,2016-03-18 14:41:46,2013-07-25 13:55:17,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1625,b'Possible math error in Binomial deviance calculations',"b'The calculations of the binomial deviance and its gradient in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py are bothering me. I could be missing something, but:\r\n\r\nThe deviance is calculated as `log(1 + exp(-2 * y * pred))` [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py#L339). This matches equation 10.18 on p 346 of [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/). However, that derivation assumes that y is {-1, 1} valued, whereas in sklearn y is {0, 1}. Effectively, the calculation is insensitive to `pred` whenever `y=0`. The fix is to change the return line to `np.sum(np.logaddexp(0.0, -2 * (2 * y - 1) * pred)) / y.shape[0]`.\r\n\r\nThe calculation of the gradient makes sense to me if the `pred` values map to class probabilities via `P(y=1) = 1 / (1 + exp(-pred))`. However, the loss function calculation above seems to follow the convention that `P(y=1) = 1 / (1 + exp(-2 * pred))` (again, see the link above). One way to make the two equations consistent with each other is to remove the first 2 in the above equation:\r\n\r\n `np.sum(np.logaddexp(0.0, -(2 * y - 1) * pred)) / y.shape[0]`\r\n\r\n'"
1622,10287824,AWinterman,ogrisel,2013-01-24 20:30:03,2013-10-29 10:32:58,2013-10-29 10:31:33,closed,,0.15,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1622,b'Slow Unpickling of OneVsRest Random Forests:',"b'I\'m seeing very slow unpickling times for multilabel random forests. I\'m opening this as an issue re: my [discussion](http://stackoverflow.com/questions/14472574/addressing-slow-unpickling-of-scikit-learn-onevsrest-randomforests) with @ogrisel on Stack Overflow.\r\n\r\ntrainX is my training feature matrix. It is a sparse matrix with shape (926, 1236). validX is the feature matrix I want to categorize. By coincidence, it has the same size as the training matrix. Y is a numpy array of python lists (since this is a multilabel problem.). Each sample has at most 4 labels, with most having only one. There are 52 unique labels.\r\n\r\n```python\r\nimport numpy as np\r\nimport cPickle as pickle\r\nimport cProfile\r\nimport os\r\nfrom collections import defaultdict\r\n\r\nfrom sklearn.multiclass import OneVsRestClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier as classifier\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.decomposition import RandomizedPCA\r\n\r\nclf_params = {\'n_estimators\':15, \'n_jobs\': -1, \'min_density\': 0,\r\n                       \'max_depth\': np.log2((trainX.shape[0]))-1}\r\n#Classifier\r\nclf = Pipeline([(\'reduce_dim\',\r\n                RandomizedPCA(n_components=100,\r\n                                                    whiten=False),),\r\n               (\'clf\',  OneVsRestClassifier(\r\n                                classifier(**clf_params))\r\n               )\r\n ])\r\n\r\n#Training\r\nclf.fit(trainX, Y)\r\nscores = clf.predict_proba(validX)\r\n\r\n#serializing\r\nserialized = pickle.dumps(clf, protocol=-1)\r\n\r\ncProfile.run(""pickle.loads(serialized)"")\r\n\r\n```\r\n\r\n\r\nOutput:\r\n\r\n```\r\n\r\n>>> cProfile.run(""pickle.loads(serialized)"")\r\n\r\n         1465558 function calls in 5.188 seconds\r\n\r\n   Ordered by: standard name\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.006    0.006    5.188    5.188 <string>:1(<module>)\r\n      781    4.245    0.005    4.245    0.005 __init__.py:93(__RandomState_ctor)\r\n      780    0.001    0.000    0.007    0.000 fromnumeric.py:1774(amax)\r\n    20393    0.009    0.000    0.009    0.000 pickle.py:1002(load_tuple1)\r\n     1044    0.001    0.000    0.001    0.000 pickle.py:1006(load_tuple2)\r\n    10618    0.012    0.000    0.012    0.000 pickle.py:1010(load_tuple3)\r\n       55    0.000    0.000    0.000    0.000 pickle.py:1014(load_empty_list)\r\n     2451    0.003    0.000    0.003    0.000 pickle.py:1018(load_empty_dictionary)\r\n      890    0.001    0.000    0.002    0.000 pickle.py:1080(load_newobj)\r\n       13    0.000    0.000    0.000    0.000 pickle.py:1087(load_global)\r\n       13    0.000    0.000    0.000    0.000 pickle.py:1122(find_class)\r\n    13220    0.040    0.000    4.310    0.000 pickle.py:1129(load_reduce)\r\n    66498    0.083    0.000    0.121    0.000 pickle.py:1154(load_binget)\r\n      256    0.000    0.000    0.001    0.000 pickle.py:1168(load_binput)\r\n    72387    0.105    0.000    0.162    0.000 pickle.py:1173(load_long_binput)\r\n       54    0.000    0.000    0.000    0.000 pickle.py:1185(load_appends)\r\n     1671    0.011    0.000    0.022    0.000 pickle.py:1201(load_setitems)\r\n    13849    0.031    0.000    0.071    0.000 pickle.py:1211(load_build)\r\n    13905    0.007    0.000    0.008    0.000 pickle.py:1250(load_mark)\r\n        1    0.000    0.000    0.000    0.000 pickle.py:1254(load_stop)\r\n        1    0.017    0.017    5.355    5.355 pickle.py:1380(loads)\r\n        1    0.000    0.000    0.000    0.000 pickle.py:829(__init__)\r\n        1    0.000    0.000    0.000    0.000 pickle.py:83(__init__)\r\n        1    0.238    0.238    5.338    5.338 pickle.py:845(load)\r\n    13905    0.029    0.000    0.031    0.000 pickle.py:870(marker)\r\n        1    0.000    0.000    0.000    0.000 pickle.py:883(load_proto)\r\n     2580    0.001    0.000    0.001    0.000 pickle.py:899(load_none)\r\n    11138    0.005    0.000    0.006    0.000 pickle.py:903(load_false)\r\n       55    0.000    0.000    0.000    0.000 pickle.py:907(load_true)\r\n      893    0.001    0.000    0.002    0.000 pickle.py:925(load_binint)\r\n    44845    0.038    0.000    0.056    0.000 pickle.py:929(load_binint1)\r\n     2059    0.003    0.000    0.005    0.000 pickle.py:933(load_binint2)\r\n     2394    0.003    0.000    0.006    0.000 pickle.py:957(load_binfloat)\r\n     3125    0.005    0.000    0.010    0.000 pickle.py:974(load_binstring)\r\n     8654    0.010    0.000    0.017    0.000 pickle.py:988(load_short_binstring)\r\n    12180    0.034    0.000    0.056    0.000 pickle.py:993(load_tuple)\r\n     1671    0.001    0.000    0.001    0.000 pickle.py:998(load_empty_tuple)\r\n       13    0.000    0.000    0.000    0.000 {__import__}\r\n     2394    0.002    0.000    0.002    0.000 {_struct.unpack}\r\n      890    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x101d00178}\r\n        1    0.000    0.000    0.000    0.000 {cStringIO.StringIO}\r\n    13862    0.006    0.000    0.006    0.000 {getattr}\r\n    14407    0.005    0.000    0.005    0.000 {intern}\r\n      890    0.001    0.000    0.001    0.000 {isinstance}\r\n    15576    0.003    0.000    0.003    0.000 {len}\r\n    82190    0.030    0.000    0.030    0.000 {marshal.loads}\r\n      781    0.005    0.000    0.005    0.000 {method \'__setstate__\' of \'mtrand.RandomState\' objects}\r\n      421    0.001    0.000    0.001    0.000 {method \'__setstate__\' of \'numpy.dtype\' objects}\r\n    10197    0.017    0.000    0.017    0.000 {method \'__setstate__\' of \'numpy.ndarray\' objects}\r\n      780    0.000    0.000    0.000    0.000 {method \'__setstate__\' of \'sklearn.tree._tree.ClassificationCriterion\' objects}\r\n      780    0.002    0.000    0.002    0.000 {method \'__setstate__\' of \'sklearn.tree._tree.Tree\' objects}\r\n   164062    0.016    0.000    0.016    0.000 {method \'append\' of \'list\' objects}\r\n        1    0.000    0.000    0.000    0.000 {method \'disable\' of \'_lsprof.Profiler\' objects}\r\n       54    0.000    0.000    0.000    0.000 {method \'extend\' of \'list\' objects}\r\n      890    0.001    0.000    0.001    0.000 {method \'iteritems\' of \'dict\' objects}\r\n      780    0.008    0.000    0.008    0.000 {method \'max\' of \'numpy.ndarray\' objects}\r\n    27960    0.007    0.000    0.007    0.000 {method \'pop\' of \'list\' objects}\r\n   527243    0.129    0.000    0.129    0.000 {method \'read\' of \'cStringIO.StringI\' objects}\r\n       26    0.000    0.000    0.000    0.000 {method \'readline\' of \'cStringIO.StringI\' objects}\r\n    10197    0.015    0.000    0.015    0.000 {numpy.core.multiarray._reconstruct}\r\n      261    0.000    0.000    0.000    0.000 {numpy.core.multiarray.scalar}\r\n   120254    0.013    0.000    0.013    0.000 {ord}\r\n     1671    0.002    0.000    0.002    0.000 {range}\r\n   142867    0.024    0.000    0.024    0.000 {repr}\r\n\r\n```\r\n\r\nI can include the rest, but the bulk of the time is spend on `__init__.py:93(__RandomStat_ctor)`.\r\n\r\nNote that this is actually a slightly smaller example than the one I was talking about on stack overflow (for the sake of ease). It still illustrates the problem.'"
1621,10275939,erg,amueller,2013-01-24 15:16:44,2013-02-01 17:56:33,2013-02-01 17:56:33,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1621,b'unbound variable alphas in lars_path()',"b'Found this one doing ``nosetests`` in a loop:\r\n\r\n```\r\nERROR: sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_overcomplete\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/erg/python/scikit-learn/sklearn/decomposition/tests/test_dict_learning.py"", line 117, in test_dict_learning_online_overcomplete\r\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=20).fit(X)\r\n  File ""/home/erg/python/scikit-learn/sklearn/decomposition/dict_learning.py"", line 1099, in fit\r\n    random_state=self.random_state)\r\n  File ""/home/erg/python/scikit-learn/sklearn/decomposition/dict_learning.py"", line 625, in dict_learning_online\r\n    alpha=alpha).T\r\n  File ""/home/erg/python/scikit-learn/sklearn/decomposition/dict_learning.py"", line 239, in sparse_encode\r\n    init=init, max_iter=max_iter)\r\n  File ""/home/erg/python/scikit-learn/sklearn/decomposition/dict_learning.py"", line 100, in _sparse_encode\r\n    lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n  File ""/home/erg/python/scikit-learn/sklearn/linear_model/least_angle.py"", line 576, in fit\r\n    eps=self.eps, return_path=False)\r\n  File ""/home/erg/python/scikit-learn/sklearn/linear_model/least_angle.py"", line 233, in lars_path\r\n    % (n_iter, alphas[n_iter], n_active, diag))\r\nUnboundLocalError: local variable \'alphas\' referenced before assignment\r\n```\r\n\r\nThe error happens in ``sklearn/linear_model/least_angle.py``.\r\n\r\nIn ``lars_path``, ``alphas`` does not get set because ``return_path`` is ``False``.\r\n```\r\n    if return_path:\r\n        coefs = np.zeros((max_features + 1, n_features))\r\n        alphas = np.zeros(max_features + 1)\r\n    else:\r\n        coef, prev_coef = np.zeros(n_features), np.zeros(n_features)\r\n        alpha, prev_alpha = np.array([0.]), np.array([0.])  # better ideas?\r\n```\r\n\r\nSo when the warning is triggered, it throws an exception:\r\n```\r\n            if diag < 1e-7:\r\n                # The system is becoming too ill-conditioned.\r\n                # We have degenerate vectors in our active set.\r\n                # We\'ll \'drop for good\' the last regressor added\r\n                warnings.warn(\'Regressors in active set degenerate. \'\r\n                              \'Dropping a regressor, after %i iterations, \'\r\n                              \'i.e. alpha=%.3e, \'\r\n                              \'with an active set of %i regressors, and \'\r\n                              \'the smallest cholesky pivot element being %.3e\'\r\n                              % (n_iter, alphas[n_iter], n_active, diag))\r\n                # XXX: need to figure a \'drop for good\' way\r\n                Cov = Cov_not_shortened\r\n                Cov[0] = 0\r\n                Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\r\n                continue\r\n```\r\n\r\n\r\nTo trigger the bug, in ``sklearn/decomposition/tests/test_dict_learning.py``:\r\n```\r\ndef test_dict_learning_online_overcomplete():\r\n    np.random.seed(7059323924)\r\n    n_components = 12\r\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=20).fit(X)\r\n    assert_true(dico.components_.shape == (n_components, n_features))\r\n```'"
1617,10257163,erg,GaelVaroquaux,2013-01-24 00:32:26,2013-02-16 19:39:52,2013-02-16 19:39:52,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1617,b'random error in sklearn.cluster.tests.test_k_means.test_input_dtypes test',"b'I found this error when running tests in a loop.\r\n\r\n```\r\nERROR: sklearn.cluster.tests.test_k_means.test_input_dtypes\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/erg/python/scikit-learn/sklearn/cluster/tests/test_k_means.py"", line 523, in test_input_dtypes\r\n    MiniBatchKMeans(n_clusters=2, n_init=10, batch_size=2).fit(X_int_csr),\r\n  File ""/home/erg/python/scikit-learn/sklearn/cluster/k_means_.py"", line 1200, in fit\r\n    verbose=self.verbose)\r\n  File ""/home/erg/python/scikit-learn/sklearn/cluster/k_means_.py"", line 866, in _mini_batch_step\r\n    new_centers = X[new_centers]\r\n  File ""/usr/lib/python2.7/site-packages/scipy/sparse/csr.py"", line 286, in __getitem__\r\n    return self[asindices(key),:]                     #[[1,2]]\r\n  File ""/usr/lib/python2.7/site-packages/scipy/sparse/csr.py"", line 252, in __getitem__\r\n    P = extractor(row, self.shape[0])        #[[1,2],j] or [[1,2],1:2]\r\n  File ""/usr/lib/python2.7/site-packages/scipy/sparse/csr.py"", line 214, in extractor\r\n    (min_indx,max_indx) = check_bounds(indices,N)\r\n  File ""/usr/lib/python2.7/site-packages/scipy/sparse/csr.py"", line 198, in check_bounds\r\n    max_indx = indices.max()\r\n  File ""/usr/lib/python2.7/site-packages/numpy/core/_methods.py"", line 10, in _amax\r\n    out=out, keepdims=keepdims)\r\nValueError: zero-size array to reduction operation maximum which has no identity\r\n```\r\n'"
1615,10248304,shoyer,shoyer,2013-01-23 20:12:33,2013-02-14 08:26:14,2013-02-14 03:17:09,closed,,,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1615,b'Lars.coef_ broken when fit_path=True',"b'I am using LassoLars to solve a multi-dimensional sparse coding problem. After upgrading from 0.12.1 to 0.13, I have encountered a bug where the coef_ attribute of LassoLars is a list, not a numpy array. This breaks the decision_function method, since it takes the transpose of coef_:\r\n\r\n/export/disk0/wb/python2.6/lib/python2.6/site-packages/sklearn/linear_model/base.pyc in decision_function(self, X)\r\n    138         """"""\r\n    139         X = safe_asarray(X)\r\n--> 140         return safe_sparse_dot(X, self.coef_.T) + self.intercept_\r\n    141 \r\n    142     def predict(self, X):\r\n\r\nAttributeError: \'list\' object has no attribute \'T\'\r\n\r\nThis seems to be due to this change that made it into the 0.13 release by @GaelVaroquaux:\r\nhttps://github.com/scikit-learn/scikit-learn/commit/e18465da6a4c11f2364b2dcdbd1433a094093dab'"
1602,10156618,basvandenberg,larsmans,2013-01-21 13:27:20,2013-01-26 19:22:35,2013-01-26 19:22:35,closed,,0.14-rc,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1602,b'Label 0 cannot be used as outlier_label for RadiusNeighborClassifier',"b'The RadiusNeighborClassifier outlier_label parameter is set to None by default. The predict method checks if an outlier_label is provided using:\r\n\r\nif self.outlier_label:\r\n\r\nThis evaluates to False if the outlier_label is set to 0. As far as I know 0 is generally used as label, so the test should probably check if self.outlier is not None.\r\n\r\nCheers,\r\nBastiaan'"
1593,10096830,jaganadhg,amueller,2013-01-18 12:30:51,2014-07-18 15:38:53,2014-07-18 15:38:53,closed,,0.15.1,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1593,b'Joblib saved classifier slow prediction',b'As per the discussion in this thread (http://comments.gmane.org/gmane.comp.python.scikit-learn/5716) I am filing the issue.\r\nCode to reproduce the same https://gist.github.com/4564287\r\n\r\nBest regards\r\n\r\nJaggu'
1565,9912372,amueller,ogrisel,2013-01-12 17:00:43,2014-01-17 09:14:07,2014-01-15 15:30:49,closed,ogrisel,0.15,20,Bug;Large Scale,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1565,b'Trouble with parallel GridSearchCV [update]',"b'I am not sure what the root is but the following was reported on IRC and I could reproduce:\r\n~~when doing a nested cross-validation with ``GridSearchCV`` and ``cross_val_score``, there seem to be some parallelization issues.~~\r\nThere is a random error in ``GridSearchCV``\r\nMaybe this is joblib related?\r\n\r\nReproduce using\r\n```\r\n for x in xrange(1000): GridSearchCV(LinearSVC(), param_grid=dict(C=[1, 10]), n_jobs=10,cv=10))\r\n```\r\n(X, y was iris for me)\r\n\r\nI get ``[Parallel] Pool seems closed`` and \r\n``ValueError: generator already executing``\r\n\r\nThe n_jobs parameter is higher than the number of cores. Should that in general be forbidden?\r\n\r\nping @GaelVaroquaux.'"
1555,9836657,ogrisel,larsmans,2013-01-10 10:10:07,2013-01-20 15:51:34,2013-01-20 15:51:07,closed,,0.14-rc,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1555,b'Make the random forests predict method check the shape of the data and raise ValueError with informative error message',"b'As reported on this [SO question](http://stackoverflow.com/questions/14207410/trouble-understanding-output-from-scikit-random-forest/14212533).\r\n\r\nIt would be interesting to check whether such improvement / fix can be made more general in the scikit code base, maybe as a new private method in the classifier mixin or as a function in the `sklearn.utils.validation` module.'"
1547,9790609,kyleabeauchamp,kyleabeauchamp,2013-01-09 01:39:33,2013-01-14 01:33:48,2013-01-13 20:12:53,closed,,0.13,23,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1547,b'Stochastic failures of lobpcg test',"b'So when diagnosing some other issues, I\'ve found that running ""make"" leads to test failures approximately 30% of the time.  The error is reported below.  I suspect this is due to the default tolerance of np.allclose being 1E-8.  I\'m not sure why the test fail is random, because the random state should be fixed.\r\n\r\nMy system is Intel I7 3770, Ubuntu64 12.04, Enthought Python 7.3.\r\n\r\n\r\n======================================================================\r\nFAIL: sklearn.cluster.tests.test_spectral.test_spectral_lobpcg_mode\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/home/kyleb/opt/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/kyleb/src/scikit-learn/sklearn/cluster/tests/test_spectral.py"", line 64, in test_spectral_lobpcg_mode\r\n    random_state=0, eigen_solver=""lobpcg"")\r\n  File ""/home/kyleb/src/scikit-learn/sklearn/cluster/spectral.py"", line 265, in spectral_clustering\r\n    eigen_tol=eigen_tol, drop_first=False)\r\n  File ""/home/kyleb/src/scikit-learn/sklearn/manifold/spectral_embedding.py"", line 300, in spectral_embedding\r\n    largest=False, maxiter=2000)\r\n  File ""/home/kyleb/opt/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/lobpcg/lobpcg.py"", line 434, in lobpcg\r\n    assert np.allclose( gramA.T, gramA )\r\nAssertionError: \r\n  assert <module \'numpy\' from \'/home/kyleb/opt/lib/python2.7/site-packages/numpy/__init__.pyc\'>.allclose( matri'"
1533,9751494,RONNCC,amueller,2013-01-07 22:53:20,2013-01-08 12:21:40,2013-01-08 09:09:06,closed,,,21,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1533,b'GridsearchCV does not work with mix of floats + None for LogisticRegression',"b""When i set C = \r\n  'C':[.01,.1,10,20,30,40,50,100,None], or   'C':[None,.01,.1,10,20,30,40,50,100], it complains C must be a float. Yet   'C':[.01,.1,10,20,30,40,50,100] works.\r\n\r\n It does not seem to parse out the None correctly?"""
1510,9635271,bhy,larsmans,2013-01-03 03:53:49,2013-01-03 14:03:56,2013-01-03 14:03:56,closed,,,6,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1510,b'Build failure: error: expected identifier or \xa1\xae(\xa1\xaf before numeric constant ',"b""Building failed on a Solaris system with the following error:\r\n\r\n    sklearn/utils/src/cholesky_delete.c:55:13: error: expected identifier or \xa1\xae(\xa1\xaf before numeric constant\r\n\r\nThere's also same error reported for Cygwin:\r\n\r\nhttp://permalink.gmane.org/gmane.comp.python.scikit-learn/3353\r\n\r\n\r\nIt turns out in `cholesky_delete.c` it has a variable named `_L`, but the `_L` is defined somewhere as a macro, put a `#undef _L` fixes the build error. But a proper fix should be rename the `_L` to something else.."""
1509,9628978,ogrisel,mblondel,2013-01-02 21:59:49,2013-01-02 23:54:10,2013-01-02 22:59:47,closed,,,4,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1509,b'The threshold parameter of the SquaredHinge loss is ignored',b'The cython implementation is hardcoded with 1.0 although `threshold` is a (badly) documented constructor parameter.'
1493,9522444,zenpoy,vene,2012-12-26 15:36:25,2013-07-29 00:17:17,2013-07-29 00:17:17,closed,,0.14-rc,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1493,b'undefined reference to sync_fetch_and_add_4 when building with mingw on 64bit pc',"b'When building `metrics.pairwise_fast` and `ensamble._gradient_boosting` I got `undefined reference to sync_fetch_and_add_4`. I used mingw compiler on 64bit pc (windows). To resolve this I added `extra_compile_args=[""-march=i486""]` to `config.add_extension` of `ensamble` and `metrics`\'s `setup.py` files. I\'m not sure what are the implications on other machines. This  [stackoverflow answer](http://stackoverflow.com/questions/7994614/undefined-reference-to-sync-fetch-and-add-4) helped me.'"
1492,9521688,ishalyminov,ishalyminov,2012-12-26 14:35:35,2012-12-26 20:02:03,2012-12-26 20:02:03,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1492,b'dict_vectorizer: variable referenced before assignment',"b'Hello!\r\n\r\nHere\'s an error I\'ve caught:\r\n\r\nFile ""/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/dict_vectorizer.py"", line 123, in fit_transform\r\n    return self.transform(X)\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/dict_vectorizer.py"", line 206, in transform\r\n    shape = (i + 1, len(vocab))\r\nUnboundLocalError: local variable \'i\' referenced before assignment\r\n\r\nI\'ve tried the latest scikit-learn versions from PIP and Ubuntu repo, and both have this problem.'"
1489,9502849,amueller,GaelVaroquaux,2012-12-24 16:00:45,2014-01-14 08:47:32,2013-01-20 18:57:21,closed,,0.13,28,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1489,b'Unexpected class_weight behavior in RidgeClassifier',"b""While testing the class_weight parameter in several estimators to investigate #1411, I noticed that the parameter doesn't work as I would expect in RidgeClassifier either.\r\nMaybe this is some misunderstanding on my part or some regularization issue, but if I have noisy labels, I would have expected to be be able to move the decision boundary.\r\n\r\nThis is not the case as illustrated in [this notebook](http://nbviewer.ipython.org/4369742/)\r\n\r\nAny input would be appreciated."""
1481,9469825,mblondel,ogrisel,2012-12-21 16:48:16,2012-12-21 17:15:04,2012-12-21 17:15:04,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1481,b'bug in random projection example',"b'```\r\n$ python examples/plot_johnson_lindenstrauss_bound.py\r\nTraceback (most recent call last):\r\n  File ""examples/plot_johnson_lindenstrauss_bound.py"", line 112, in <module>\r\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\r\n  File ""/Users/mathieublondel/Desktop/projects/scikit-learn/sklearn/random_projection.py"", line 118, in johnson_lindenstrauss_min_dim\r\n    ""The JL bound is defined for eps in ]0, 1[, got %r"" % eps)\r\nValueError: The JL bound is defined for eps in ]0, 1[, got array(1.0)\r\n```'"
1476,9306229,fannix,larsmans,2012-12-15 12:34:23,2013-01-17 15:57:54,2013-01-17 15:57:54,closed,,0.13,29,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1476,"b'SVC(kernel=""linear"") for dense and sparse matrices differ significantly [renamed by amueller]'","b'I compare the performance between SVC(kernel=""linear"") and LinearSVC. The difference is very large. LinearSVC is far better than SVC(kernel=""linear""). The code is in https://gist.github.com/4294378. I also test with the original LibSVM toolkit; its performance is close to LinearSVC under 5-fold cross validation.\r\n\r\nI don\'t think it is expected from a user\'s perspective.'"
1472,9277141,salideng,amueller,2012-12-14 06:22:27,2013-01-03 13:35:09,2013-01-03 13:35:09,closed,,,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1472,b'About plot_classifier_comparison.py ',"b'when i run python plot_classifier_comparison.py  ,I get error ,\r\n\r\n probabilities[all_rows, idx] += weights[:, i]\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n\r\nmy python is 2.7\r\n\r\n\r\nthanks'"
1469,9230420,arnevd,larsmans,2012-12-12 20:13:51,2014-09-03 10:50:25,2013-09-16 10:08:21,closed,,0.15,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1469,b'univariate_selection.py:92: RuntimeWarning: invalid value encountered in divide f = msb / msw',"b""I always seem to get the same error when wanting to use univariate_selection.\r\n\r\n> univariate_selection.py:92: RuntimeWarning: invalid value encountered in divide f = msb / msw\r\n\r\nI am using python 2.7.3 64 bit version and numpy 1.6.2.\r\n\r\nThe train array I use has the shape (957L, 3317L). Which means there are currently 3317 features and I would like to select the 10% most useful ones using 'SelectPercentile'.\r\n\r\nThe arrays msb and msw are looking as follows:\r\n\r\n> msb : ndarray: [-0.01144492 -0.00071531 -0.00643777 ..., -0.00643777 -0.00643777\r\n -0.00286123] with dtype: float64\r\n> msw: ndarray: [ 0.00574713  0.00143678  0.00431034 ...,  0.00431034  0.00431034\\n  0.00287356] with dtype: float64\r\n\r\nI was first running python in 32 bit-modus so I figured that would've been the problem, but after removing it and installing the 64 bit version and all 64 bit extention packages I am still experiencing the same problem.\r\n\r\n"""
1466,9208629,glouppe,glouppe,2012-12-12 08:01:53,2013-04-11 09:06:15,2013-04-11 09:06:15,closed,,0.14-rc,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1466,b'_tree.pyx: Use np.intp_t instead of int',"b'As pointed out in #1458, the use of `int` for everything related to indexing numpy arrays might not be safe. @seberg recommends using `np.intp_t` instead. '"
1462,9188929,glouppe,ogrisel,2012-12-11 18:21:09,2012-12-16 16:44:39,2012-12-16 16:44:39,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1462,b'Random failure of test_classifiers_classes',"b'For an unknown reason `test_classifiers_classes` just failed on my machine. I cannot reproduce the error :/\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_classifiers_classes\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose-1.0.0-py2.7.egg/nose/case.py"", line 187, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/gilles/Sources/scikit-learn/sklearn/tests/test_common.py"", line 527, in test_classifiers_classes\r\n    assert_array_equal(np.unique(y), np.unique(y_pred))\r\n  File ""/usr/lib/pymodules/python2.7/numpy/testing/utils.py"", line 686, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""/usr/lib/pymodules/python2.7/numpy/testing/utils.py"", line 579, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(shapes (3,), (2,) mismatch)\r\n x: array([1, 3, 5])\r\n y: array([1, 5])\r\n```'"
1461,9182355,ogrisel,ogrisel,2012-12-11 15:09:37,2012-12-29 16:28:23,2012-12-29 16:28:23,closed,,0.13,11,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1461,b'Failing spectral clustering test on OSX 10.8 with numpy 1.6.2 built against system BLAS',"b'```\r\n======================================================================\r\nFAIL: sklearn.cluster.tests.test_spectral.test_spectral_clustering\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/coding/scikit-learn/sklearn/cluster/tests/test_spectral.py"", line 44, in test_spectral_clustering\r\n    assert_array_equal(labels, [1, 1, 1, 0, 0, 0, 0])\r\n  File ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 707, in assert_array_equal\r\n    verbose=verbose, header=\'Arrays are not equal\')\r\n  File ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 636, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 57.1428571429%)\r\n x: array([1, 1, 0, 1, 0, 1, 1], dtype=int32)\r\n y: array([1, 1, 1, 0, 0, 0, 0])\r\n>>  raise AssertionError(\'\\nArrays are not equal\\n\\n(mismatch 57.1428571429%)\\n x: array([1, 1, 0, 1, 0, 1, 1], dtype=int32)\\n y: array([1, 1, 1, 0, 0, 0, 0])\')\r\n```'"
1455,9120944,cdeil,cdeil,2012-12-09 15:14:57,2012-12-09 15:27:33,2012-12-09 15:26:21,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1455,b'sklearn.tree unit test crash on Linux with numpy master',"b""I just installed `sklearn` 61e2a71400541c8440f65629231cd127b3e3c4e8 with `numpy 1.8.0.dev-b7b54cd` on 64-bit Linux and get this crash trying to run the unit tests:\n```\nProgram received signal SIGFPE, Arithmetic exception.\n0x00007fffde4631ef in __pyx_f_7sklearn_4tree_5_tree_4Tree_recursive_partition (\n    __pyx_v_self=0x3787050, __pyx_v_X=0x34001a0, __pyx_v_X_argsorted=0x32fe3b0, \n    __pyx_v_y=0x3204460, __pyx_v_sample_mask=0x33c2f70, __pyx_v_n_node_samples=2, \n    __pyx_v_depth=0, __pyx_v_parent=-1, __pyx_v_is_left_child=0, __pyx_v_buffer_value=0x35248e0)\n    at sklearn/tree/_tree.c:4698\n```\n\nAt least I think the problem is in `sklearn.tree`, I didn't look closely. The full log is here: https://gist.github.com/4245504\n"""
1439,8944442,jaquesgrobler,pprett,2012-12-03 10:27:18,2012-12-04 17:25:51,2012-12-04 17:25:51,closed,,,20,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1439,b'Test failing on partial_dependence',"b'I\'ve been busy on another branch, so I\'m not sure when this popped up or if it\'s been mentioned, so close this if it has, but on my latest build I get the following Errors:\n\n```\n\n...\n======================================================================\nERROR: Test partial dependence plot function.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\n    self.test(*self.arg)\n  File ""/home/jaques/scikit-learn/sklearn/utils/testing.py"", line 193, in run_test\n    return func(*args, **kwargs)\n  File ""/home/jaques/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py"", line 116, in test_plot_partial_dependence\n    feature_names=boston.feature_names)\n  File ""/home/jaques/scikit-learn/sklearn/ensemble/partial_dependence.py"", line 382, in plot_partial_dependence\n    fig.tight_layout()\nAttributeError: \'Figure\' object has no attribute \'tight_layout\'\n\n======================================================================\nERROR: Test partial dependence plot function on multi-class input.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\n    self.test(*self.arg)\n  File ""/home/jaques/scikit-learn/sklearn/utils/testing.py"", line 193, in run_test\n    return func(*args, **kwargs)\n  File ""/home/jaques/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py"", line 183, in test_plot_partial_dependence_multiclass\n    grid_resolution=grid_resolution)\n  File ""/home/jaques/scikit-learn/sklearn/ensemble/partial_dependence.py"", line 382, in plot_partial_dependence\n    fig.tight_layout()\nAttributeError: \'Figure\' object has no attribute \'tight_layout\'\n\n----------------------------------------------------------------------\nRan 1430 tests in 77.272s\n\nFAILED (SKIP=10, errors=2)\nmake: *** [test-code] Error 1\n```\n\nThanks'"
1431,8905912,erg,amueller,2012-12-01 23:01:14,2012-12-04 21:16:06,2012-12-04 21:16:06,closed,,,14,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1431,b'sklearn.cluster.tests.test_spectral.test_discretize in fails testing adjusted_rand_score() (at random?)',"b'Travis-CI found this unrelated issue when testing my pull request.\n\nhttps://travis-ci.org/scikit-learn/scikit-learn/builds/3454819\n\n```\n936FAIL: sklearn.cluster.tests.test_spectral.test_discretize\n937----------------------------------------------------------------------\n938Traceback (most recent call last):\n939  File ""/home/travis/virtualenv/python2.7_with_system_site_packages/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\n940    self.test(*self.arg)\n941  File ""/home/travis/builds/scikit-learn/scikit-learn/sklearn/cluster/tests/test_spectral.py"", line 170, in test_discretize\n942    assert_greater(adjusted_rand_score(y_true, y_pred), 0.9)\n943AssertionError: 0.89082564037817402 not greater than 0.9\n944    """"""Fail immediately, with the given message.""""""\n945>>  raise self.failureException(\'0.89082564037817402 not greater than 0.9\')\n```'"
1423,8760270,Fkawala,arjoly,2012-11-28 10:39:10,2014-05-12 12:42:58,2013-07-26 10:11:05,closed,,0.14-rc,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1423,b'sklearn.metrics.precision_recall_curve occasionally fails ',"b'The _precision_recall_curve metric_\'s output is occasionally meaningless, as in the following example, ![output caption](http://s10.postimage.org/d8uazvjt5/pr_curve_1353936568.png)\n\n\nThree remarks : \n\n* Meaningless outputs occurs as well using 0.13 or 0.12\n\n* if I use the lastest git version the _auc_ function raises an exception, while it doesn\'t when using 0.12 version.\n\n        AssertionError: Reordering is not turned on, and The x array is not increasing:  [ 0.24195  0.24145  0.24161 ...,  1. 1.       1.     ]\n\n* The _y_true_ and _probas__ dumps are available here [y_true](http://pastebin.com/LmFTFdD4) and there [probas_](http://pastebin.com/Qnd9Bkj9)\n\n\nStarting from the available example covering _sklearn.metrics.precision_recall_curve_ I\'ve wrote this piece of code which is responsible for the aforementioned picture\n\n    def plot_pr(model, X_test, y_test):\n\n      probas_ = model.predict_proba(X_test)\n      precision, recall, _thresholds = precision_recall_curve(y_test, probas_[:, 1])\n      pr_auc = auc(precision, recall)\n\n      pl.clf()\n      pl.plot(precision, recall, label=\'P/R curve\')\n      pl.grid()\n      pl.xlabel(\'Recall\')\n      pl.ylabel(\'Precision\')\n      pl.ylim([0.0, 1.05])\n      pl.xlim([0.0, 1.0])\n      pl.title(\'Precision-Recall: AUC=%0.2f\' % pr_auc)\n      pl.legend(loc=""best"")\n      pl.savefig(open(\'./pr_curve_%d.png\' % int(time.time()), \'a\'), format=""png"")\n\n      np.savetxt(\'precision.log\', precision)\n      np.savetxt(\'recall.log\', recall)\n      np.savetxt(\'proba.log\', probas_)\n\nThank you for your help.\n\nFranois Kawala.'"
1422,8760121,ogrisel,ogrisel,2012-11-28 10:34:06,2012-11-29 13:48:12,2012-11-29 13:48:12,closed,,,5,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1422,b'Broken plots in OMP example',b'See:  http://scikit-learn.org/dev/auto_examples/linear_model/plot_omp.html'
1420,8736722,amueller,amueller,2012-11-27 20:47:16,2012-12-05 22:41:41,2012-12-05 22:41:41,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1420,b'Test errors in Master in SpectralEmbedding',"b'Since the merge I have three test errors in master.\nThey don\'t look 32bit related to me, so I\'m kind of surprised that neither Travis nor Jenkins picked them up....\n```\n======================================================================\nERROR: Test spectral embedding with amg solver\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\n    self.test(*self.arg)\n  File ""/home/andy/checkout/scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py"", line 126, in test_spectral_embedding_amg_solver\n    embed_amg = se_amg.fit_transform(S)\n  File ""/home/andy/checkout/scikit-learn/sklearn/manifold/spectral_embedding.py"", line 481, in fit_transform\n    self.fit(X)\n  File ""/home/andy/checkout/scikit-learn/sklearn/manifold/spectral_embedding.py"", line 460, in fit\n    random_state=self.random_state)\n  File ""/home/andy/checkout/scikit-learn/sklearn/manifold/spectral_embedding.py"", line 267, in spectral_embedding\n    ml = smoothed_aggregation_solver(laplacian.tocsr())\nAttributeError: \'numpy.ndarray\' object has no attribute \'tocsr\'\n\n======================================================================\nFAIL: Test spectral embedding with precomputed kernel\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\n    self.test(*self.arg)\n  File ""/home/andy/checkout/scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py"", line 87, in test_spectral_embedding_precomputed_affinity\n    assert_true(_check_with_col_sign_flipping(embed_precomp, embed_rbf, 0.01))\nAssertionError: False is not true\n    \'False is not true\' = self._formatMessage(\'False is not true\', ""%s is not true"" % safe_repr(False))\n>>  raise self.failureException(\'False is not true\')\n    \n\n======================================================================\nFAIL: Test spectral embedding with callable affinity\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\n    self.test(*self.arg)\n  File ""/home/andy/checkout/scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py"", line 109, in test_spectral_embedding_callable_affinity\n    _check_with_col_sign_flipping(embed_rbf, embed_callable, 0.01))\nAssertionError: False is not true\n    \'False is not true\' = self._formatMessage(\'False is not true\', ""%s is not true"" % safe_repr(False))\n>>  raise self.failureException(\'False is not true\')\n    \n```\n\nI\'ll investiage now.'"
1417,8698215,amueller,ogrisel,2012-11-26 22:58:40,2013-05-06 18:35:31,2013-05-01 11:51:07,closed,,0.14-rc,35,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1417,"b""mldata.org doesn't like Travis""",b'My travis builds (and also one in #1416 by @awinterman) often fail because mldata.org throws a 500 - Internal server error.\nThis is somewhat annoying :-/'
1414,8691318,abhirk,ogrisel,2012-11-26 19:55:40,2014-01-28 13:55:15,2014-01-28 13:55:15,closed,,,8,Bug;Large Scale,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1414,b'Joblib dump memory error',"b'Memory usage by joblib.dump jumps to more than 3 times the size of the object.\nWhile  training the classifier takes about 11g of memory, but when dumping the classifier\nobject using joblib.dump(compress=9), the usage jumps up to 38.4g\n[Causing issues with limited RAM]. [I tried values compress=3, 5, 7, 9, always get memory\n error]. If ""compress"" is not used for joblib.dump, the classifier object  is about 11g.\nFollowing is the minimalistic script that demonstrates the steps in my classifier script.\n\n\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.externals import joblib\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.linear_model import SGDClassifier\n    import time\n\n    # Here ""data"" is the list of plaintext paragraphs and target array has the \n    # category . Each paragraph belongs to one of the n categories. \n\n    def train(data, target):\n        vectorizer = TfidfVectorizer(stop_words=\'english\', ngram_range=(1,2),\n                        smooth_idf=True, sublinear_tf=True, max_df=0.5,\n                        token_pattern=ur\'\\b(?!\\d)\\w\\w+\\b\', use_idf=False)\n\n        # Vectorize the input data\n        print ""Extracting features from dataset using %s."" % (self.vectorizer)\n        start_time=time()\n        data_vectors = vectorizer.fit_transform(data)\n        extract_features_time = time() - start_time\n        print ""Feature extraction of training data done in %s seconds"" % extract_features_time\n        print ""Number of samples in training data: %d\\n Number of features: %d"" % data_vectors.shape\n        print """"\n        \n        # Dump the vectorizer, dataset and target array objects for later use. This seems to work correctly\n        # with any value of compress.\n        print ""Dumping vectorizer..."",\n        joblib.dump(self.vectorizer, ""vectorizer.joblib"", compress=9)\n        print  ""done""\n\n        print ""Dumping data vectors..."",\n        joblib.dump(data_vectors,  ""datavectors.joblib"",compress=9)\n        print ""done."" \n\n        print ""Dumping target array..."",\n         joblib.dump(target, ""targetarray.joblib"", compress=9)\n         print ""done."" \n        \n\n        # Train the classifer with OvR. The maximum memory used during training is\n        # 11g for a dataset of size 655M.\n        clf = OneVsRestClassifier(SGDClassifier(loss=\'log\', n_iter=35,\n                                            alpha=0.00001, n_jobs=-1))\n        start_time=time()\n        print ""Training %s"" % clf\n        clf.fit(data_vectors, target)\n        print ""done [%.3fs]"" % (start_time-time())\n     \n        # Dump the classifier for later use. Joblib dumps the classifier correctly\n        # without any compression. However the size of the vector dumped is about 10-11g.\n        # This seems to be too large for our purpose, and hence trying to compress\n        # the dumped object.  For compress=3,5,7,9 the  memory usage jumps to 38.4g\n        # Since the available memory is only 32g, the process ends up using swap space\n        # where the process is stalled for a long time and eventually killed.\n        print ""Dumping classifier....."",\n        joblib.dump(clf, ""classifier.joblib"", compress=9)\n        print ""done""'"
1411,8682377,ShusenLiu,amueller,2012-11-26 16:18:28,2013-01-03 12:58:04,2013-01-03 12:58:04,closed,,0.13,31,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1411,b'Weird behavior in LogisticRegression on parameter class_weight',"b'The class_weight parameter set different weights for misclassify that class. For example, in a 0/1 classification problem, if we set class_weight={0:0.95, 1:0.05}, then we can expect the classifier to be more careful when it try to classify a data sample to be 0, since misclassify a 0 to 1 is heavily penalized.\n\nBut the LogisticRegression class seems go wrong:\n\n<pre>\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn import linear_model\n\n#100 sample, half of its label is 0, others are 1\nX, Y = datasets.make_classification()\nY.sum()\n>>> 50 \n\n#balance LR classifier\nclr0 = linear_model.LogisticRegression()\nclr0.fit(X, Y)\nclr0.score(X,Y)\n>>> 0.84999999999999998\nclr0.predict(X).sum()\n>>> 49\n\n#imbalance LR classifier\nclr1 = linear_model.LogisticRegression(class_weight={0:0.9, 1:0.1})\nclr1.fit(X, Y)\nclr1.score(X,Y)\n>>> 0.63\nclr1.predict(X).sum()\n>>> 85\n</pre>\n\nThe imbalance classifier clr1 is supposed to classifier more data to be label 0, but it actually predict far more data to be 1. But when we choose another classfier, say SVM, the behavior seems resonable:\n\n<pre> \n#balance SVM classifier\nclr2 = svm.SVC()\nclr2.fit(X,Y)\nclr2.score(X,Y)\n>>> 0.95999999999999996\nclr2.predict(X).sum()\n>>> 46\n\n#imbalance SVM classifier\nclr3 = svm.SVC(class_weight={0:0.6, 1:0.4})\nclr3.fit(X,Y)\nclr3.score(X,Y)\n>>> 0.84999999999999998\nclr1.predict(X).sum()\n>>> 35.0\n\n#another imbalance SVM classifier\nclr4 = svm.SVC(class_weight={0:0.9, 1:0.1})\nclr4.fit(X,Y)\nclr4.score(X,Y)\n>>> 0.5\nclr4.predict(X).sum()\n>>> 0.0\n</pre>\n\nWhen the class_weight[0] vs class_weight[0] is 5:5, SVC predict roughly half of data to be 0.\nWhen the class_weight[0] vs class_weight[0] is 6:4, SVC predict more data to be 0.\nWhen the class_weight[0] vs class_weight[0] is 9:1, SVC predict all the data to be 0.\n\nIs this a bug?'"
1407,8636159,erg,ogrisel,2012-11-24 20:58:48,2012-12-02 01:10:20,2012-12-02 01:10:20,closed,,0.13,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1407,b'random malloc error on test_k_means_plus_plus_init_2_jobs test',b'Happens on current master 3c46ebaa6e5f7bd504158da4aeb8dddd0b8db705 on MacOSX 10.8.2 64bit. Happens like 1/10th of the time?\n\n```\nmodern:scikit-learn erg$ [master*] nosetests -v\nAffinity Propagation algorithm ... ok\nTests the DBSCAN algorithm with a similarity array. ... ok\nTests the DBSCAN algorithm with a feature vector array. ... ok\nTests the DBSCAN algorithm with a callable metric. ... ok\nsklearn.cluster.tests.test_dbscan.test_pickle ... ok\nCheck that we obtain the correct solution for structured ward tree. ... ok\nCheck that we obtain the correct solution for unstructured ward tree. ... ok\nCheck that the height of ward tree is sorted. ... ok\nCheck that we obtain the correct number of clusters with Ward clustering. ... ok\nCheck that we obtain the correct solution in a simplistic case ... ok\nTest scikit ward with full connectivity (i.e. unstructured) vs scipy ... ok\nCheck that connectivity in the ward tree is propagated correctly during ... ok\nCheck non regression of a bug if a non item assignable connectivity is ... ok\nsklearn.cluster.tests.test_k_means.test_square_norms ... ok\nsklearn.cluster.tests.test_k_means.test_kmeans_dtype ... ok\nsklearn.cluster.tests.test_k_means.test_labels_assignement_and_inertia ... ok\nCheck that dense and sparse minibatch update give the same results ... ok\nsklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init ... ok\nsklearn.cluster.tests.test_k_means.test_k_means_new_centers ... ok\nsklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_2_jobs ... Python(33971) malloc: *** error for object 0x10dae4000: pointer being freed already on death-row\n*** set a breakpoint in malloc_error_break to debug\nPython(33972) malloc: *** error for object 0x10daa3000: pointer being freed already on death-row\n```'
1406,8636106,erg,glouppe,2012-11-24 20:55:52,2012-12-12 07:54:21,2012-12-12 07:54:21,closed,,0.13,19,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1406,b'Floating point exception in GradientBoostingClassifier during nosetests on macosx 10.8.2 64bit (and others?)',b'Fails every time. Using current master and either ``nosetests`` or ``make``.  Git id 3c46ebaa6e5f7bd504158da4aeb8dddd0b8db705.\n\n```\nsklearn.decomposition.tests.test_sparse_pca.test_initialization ... ok\nsklearn.decomposition.tests.test_sparse_pca.test_mini_batch_correct_shapes ... ok\nsklearn.decomposition.tests.test_sparse_pca.test_mini_batch_fit_transform ... SKIP\nDoctest: sklearn.ensemble.gradient_boosting.GradientBoostingClassifier ... Floating point exception: 8\nmodern:scikit-learn erg$ [master*] \n```'
1398,8605095,ruizm,larsmans,2012-11-23 13:38:52,2013-02-14 14:17:47,2013-02-14 14:17:47,closed,,0.14-rc,10,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1398,b'[ENH] One vs one heuristic voting strategy biased',"b'Hello, \n\nIn the documentation for the one versus one heuristic on multiclass classification, it is written ""At prediction time, the class which received the most votes is selected."" The predict_ovo def returns the argmax of the votes matrix. My intuition is that in cases of ties, the vote will be biased towards the classes from the first binary classifiers because argmax selects automatically the first occurrence in cases of ties. \n\nI wondered if it would be possible to implement a classification strategy less biased, such as selecting the classifier which has the highest output function as is done by the one vs. the rest heuristic if I am not mistaken. \n\nThank you for your help. \n\nMathieu Ruiz'"
1368,8381192,amueller,ogrisel,2012-11-15 08:59:50,2012-12-16 16:44:39,2012-12-16 16:44:39,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1368,b'Random Test failure in test transformers',b'There is a random failure in the common tests. Will try to have a look later.'
1349,8247261,erg,amueller,2012-11-09 18:05:28,2013-01-06 20:57:48,2013-01-06 20:57:48,closed,,0.13,20,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1349,b'FastICA unit test fails at random',"b'```\r\nFAIL: Test the FastICA algorithm on very simple data.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/erg/python/scikit-learn/sklearn/decomposition/tests/test_fastica.py"", line 100, in test_fastica\r\n    assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=2)\r\n  File ""/usr/lib/python2.7/site-packages/numpy/testing/utils.py"", line 468, in assert_almost_equal\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\n ACTUAL: 0.99476064813705389\r\n DESIRED: 1\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 2 decimals\\n ACTUAL: 0.99476064813705389\\n DESIRED: 1\')\r\n```'"
1342,8211802,conradlee,amueller,2012-11-08 17:21:23,2012-12-24 22:28:35,2012-12-24 22:28:35,closed,,,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1342,b'Bug: Precision recall curve',"b""I recently updated the implementation of `precision_recall_curve` in metrics.py to make it more efficient.  I now see that the output has changed in some situations---for example, consider the following code:\r\n\r\n    from sklearn.metrics import precision_recall_curve\r\n    labels = [1,0,0,1]\r\n    predict_probas = [1,2,3,4]\r\n    precision_recall_curve(labels, predict_probas)\r\n\r\nThe output here will differ depending on whether you use 0.12 (stable) or the latest version of master.\r\nIn the stable branch (using the old implementation), the output is:\r\n\r\n    >>> (array([ 0.5       ,  0.33333333,  0.5       ,  1.        ,  1.        ]),\r\n       array([ 1. ,  0.5,  0.5,  0.5,  0. ]),\r\n       array([1, 2, 3 ,4]))\r\n\r\nIn the dev branch (using the new implementation), the output is:\r\n\r\n    >>>(array([ 0.5       ,  0.33333333,  0.5       ,  1.        ]),\r\n      array([ 1. ,  0.5,  0.5,  0. ]),\r\n      array([2, 3]))\r\n\r\nThe output of the new code is wrong.  I've already created a fix for this--I'll post the pull request here in a minute."""
1341,8205187,beomjoonkim,beomjoonkim,2012-11-08 14:11:15,2012-11-08 21:43:51,2012-11-08 21:43:51,closed,pprett,,8,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1341,b'Documentation on optional parameters of decision tree',"b'I am looking at the code for decision tree, and the document[1] does not describe what the arguments for fit function, sample_mask and X_argsorted, are. I figured out by reading code what X_argsorted is, but still have hard time understanding sample_mask. Perhaps explanation on these arguments should be included in [1].\r\n\r\n[1] http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\r\n'"
1336,8129704,mblondel,mblondel,2012-11-06 06:54:43,2012-11-07 05:19:38,2012-11-07 05:19:38,closed,,,0,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1336,b'Perceptron and PassiveAggressiveClassifier should not inherit from predict_proba',b'Perceptron and PassiveAggressiveClassifier should not inherit from predict_proba and predict_log_proba.\r\n\r\nSolution: create a new class BaseSGDClassifier that does not define predict_proba and predict_log_proba.'
1312,8052839,aeweiwi,NelleV,2012-11-02 08:14:46,2013-07-26 13:50:27,2013-07-26 13:50:27,closed,,0.14-rc,2,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1312,b'PLSSVD module does not extract the required n_component specified by user',"b'The PLSSVD specify a number of components to be extracted (n_components) using SVD, which is actually not used in the code at all. It simply returns all possible components!! Is this true ?\r\n\r\n'"
1307,8038137,mbatchkarov,amueller,2012-11-01 18:06:12,2012-11-14 22:36:51,2012-11-14 22:36:51,closed,,0.13,8,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1307,b'sklearn.metrics.confusion_matrix() fails on string labels',"b'The confusion matrix method outputs zero matrix when the labels are string. To replicate, run\r\n    print confusion_matrix([\'1\', \'0\', \'0\', \'1\'], [\'1\', \'1\', \'0\', \'0\'])\r\n\r\nAnd the output is:\r\n\r\n    [[0 0]\r\n     [0 0]]\r\n\r\nRunning the same command with integer labels produces the expected output:\r\n\r\n    print confusion_matrix([1, 0, 0, 1], [1, 1, 0, 0])\r\n    [[1 1]\r\n     [1 1]]\r\n------------------------- UPDATE -------------------------\r\n\r\nIt appears I have two version of confusion_matrix--- one from sklearn and one from scikits.learn. I am not sure how the sklearn package appeared on my machine, I suspect it shipped with my python distribution (EPD). The bug reported above only occurs with the sklearn version. If I install scikit.learn 0.8.1 through pip and run \r\n    \r\n    from scikits.learn.metrics import confusion_matrix\r\n    import numpy as np\r\n    print confusion_matrix(np.array([1, 0, 0, 1]), np.array([1, 1, 0, 0]))\r\nI get the correct result\r\n    \r\n    [[1 1]\r\n     [1 1]]\r\n\r\nThe call requires me to convert the list explicitly to a numpy array, otherwise I get \r\n\r\n    print confusion_matrix([1, 0, 0, 1], [1, 1, 0, 0])\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    /<ipython-input-3-8a31087aae8f> in <module>()\r\n    ----> 1 print confusion_matrix([1, 0, 0, 1], [1, 1, 0, 0])\r\n    /Library/Frameworks/EPD64.framework/Versions/7.2/lib/python2.7/site-packages/scikits/learn/metrics/metrics.pyc in     confusion_matrix(y_true, y_pred, labels)\r\n     52     """"""\r\n     53     if labels is None:\r\n    ---> 54         labels = unique_labels(y_true, y_pred)\r\n     55     else:\r\n     56         labels = np.asarray(labels, dtype=np.int)\r\n\r\n    /Library/Frameworks/EPD64.framework/Versions/7.2/lib/python2.7/site-packages/scikits/learn/metrics/metrics.pyc in     unique_labels(*list_of_labels)\r\n     22     """"""\r\n     23     list_of_labels = [np.unique(labels[np.isfinite(labels)].ravel())\r\n    ---> 24                       for labels in list_of_labels]\r\n     25     list_of_labels = np.concatenate(list_of_labels)\r\n     26     return np.unique(list_of_labels)\r\n\r\n    TypeError: only integer arrays with one element can be converted to an index\r\n\r\nWith string lists, I get \r\n\r\n    print confusion_matrix([\'1\', \'0\', \'0\', \'1\'], [\'1\', \'1\', \'0\', \'0\'])\r\n    ---------------------------------------------------------------------------\r\n    NotImplementedError                       Traceback (most recent call last)\r\n    /<ipython-input-2-b1aa3286a9d3> in <module>()\r\n    ----> 1 print confusion_matrix([\'1\', \'0\', \'0\', \'1\'], [\'1\', \'1\', \'0\', \'0\'])\r\n    /Library/Frameworks/EPD64.framework/Versions/7.2/lib/python2.7/site-packages/scikits/learn/metrics/metrics.pyc in         confusion_matrix(y_true, y_pred, labels)\r\n     52     """"""\r\n     53     if labels is None:\r\n    ---> 54         labels = unique_labels(y_true, y_pred)\r\n     55     else:\r\n     56         labels = np.asarray(labels, dtype=np.int)\r\n    /Library/Frameworks/EPD64.framework/Versions/7.2/lib/python2.7/site-packages/scikits/learn/metrics/metrics.pyc in     unique_labels(*list_of_labels)\r\n     22     """"""\r\n     23     list_of_labels = [np.unique(labels[np.isfinite(labels)].ravel())\r\n    ---> 24                       for labels in list_of_labels]\r\n     25     list_of_labels = np.concatenate(list_of_labels)\r\n     26     return np.unique(list_of_labels)\r\n\r\n    NotImplementedError: Not implemented for this type\r\n'"
1303,8016895,erg,erg,2012-10-31 21:50:48,2012-11-07 15:59:15,2012-11-07 15:59:15,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1303,b'sklearn.tests.test_multiclass.test_ovr_fit_predict unit tests fails at random',"b'```\r\n======================================================================\r\nFAIL: sklearn.tests.test_multiclass.test_ovr_fit_predict\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/erg/python/scikit-learn/sklearn/tests/test_multiclass.py"", line 67, in test_ovr_fit_predict\r\n    assert_equal(np.mean(iris.target == pred), np.mean(iris.target == pred2))\r\nAssertionError: 0.96666666666666667 != 0.97333333333333338\r\n```\r\n'"
1302,8016297,erg,erg,2012-10-31 21:27:49,2012-11-07 16:01:15,2012-11-07 16:01:15,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1302,b'sklearn.decomposition.nmf.ProjectedGradientNMF unit test fails at random',"b'```\r\nerg@ommegang ~/python/scikit-learn/sklearn $ [master*] nosetests\r\n..................................................................SS.S..............................................S........................................................S...................................................../home/erg/python/scikit-learn/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x2773890>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n.........................................................................................................................................................................................................................................................../home/erg/python/scikit-learn/sklearn/externals/joblib/test/test_numpy_pickle.py:182: Warning: file ""/tmp/tmpWZukqv/test.pkl297"" appears to be a zip, ignoring mmap_mode ""r"" flag passed\r\n  numpy_pickle.load(this_filename, mmap_mode=\'r\')\r\n...........................................................................................................S..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................F.........................................................SSS....S....S.........................................................................................................\r\n======================================================================\r\nFAIL: sklearn.tests.test_common.test_transformers\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/erg/python/scikit-learn/sklearn/tests/test_common.py"", line 179, in test_transformers\r\n    ""fit_transform not correct in %s"" % Trans)\r\n  File ""/usr/lib/python2.7/site-packages/numpy/testing/utils.py"", line 812, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/usr/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 2 decimals\r\nfit_transform not correct in <class \'sklearn.decomposition.nmf.ProjectedGradientNMF\'>\r\n(mismatch 1.11111111111%)\r\n x: array([[  2.85073380e-01,   2.26715545e-01,   3.43223579e+00],\r\n       [  1.13547241e-01,   2.40893053e-02,   7.24521866e-01],\r\n       [ -0.00000000e+00,   1.06990298e-01,   8.36199787e-01],...\r\n y: array([[  2.84490731e-01,   2.27413683e-01,   3.43280990e+00],\r\n       [  1.12595386e-01,   2.49583906e-02,   7.24250396e-01],\r\n       [  0.00000000e+00,   1.06929012e-01,   8.36705190e-01],...\r\n\r\n----------------------------------------------------------------------\r\nRan 1223 tests in 130.546s\r\n\r\nFAILED (SKIP=11, failures=1)\r\n\r\n\r\n\r\n\r\nerg@ommegang ~/python/scikit-learn/sklearn $ [master*] nosetests\r\n..................................................................SS.S..............................................S........................................................S...................................................../home/erg/python/scikit-learn/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0x2932890>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n.........................................................................................................................................................................................................................................................../home/erg/python/scikit-learn/sklearn/externals/joblib/test/test_numpy_pickle.py:182: Warning: file ""/tmp/tmpoCsn5u/test.pkl597"" appears to be a zip, ignoring mmap_mode ""r"" flag passed\r\n  numpy_pickle.load(this_filename, mmap_mode=\'r\')\r\n...........................................................................................................S........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................SSS....S....S.........................................................................................................\r\n----------------------------------------------------------------------\r\nRan 1223 tests in 88.815s\r\n\r\nOK (SKIP=11)\r\nerg@ommegang ~/python/scikit-learn/sklearn $ [master*] \r\n```'"
1301,8004198,reesdooley,amueller,2012-10-31 15:46:36,2013-07-26 07:29:33,2013-07-26 07:29:33,closed,,0.14-rc,14,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1301,b'Segmentation Fault in Kmeans clustering',"b'Using the kmeans clustering to cluster a csr matrix of 10,000x8,000 or greater dimension has produced a segmentation fault in a title clustering application I am working on.\r\nWe have started using a recursive hierarchical clustering to avoid the issue, but as this has produced less optimal results it is in our interest to solve the segmentation fault.'"
1295,7972524,ogrisel,GaelVaroquaux,2012-10-30 16:10:09,2012-11-10 17:34:21,2012-11-10 17:34:21,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1295,b'Random test failure in PLS',"b'I got this once on the current master:\r\n\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_transformers\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/coding/scikit-learn/sklearn/tests/test_common.py"", line 170, in test_transformers\r\n    X_pred2 = trans.transform(X, y_)\r\n  File ""/Users/ogrisel/coding/scikit-learn/sklearn/pls.py"", line 372, in transform\r\n    x_scores = np.dot(Xc, self.x_rotations_)\r\nAttributeError: \'CCA\' object has no attribute \'x_rotations_\'\r\n```\r\n\r\nI cannot reproduce it even by fixing the `SKLEARN_SEED` to the one of the failure `1472386125` but the attribute error should be fixable by manual code inspection.'"
1278,7864207,tjanez,GaelVaroquaux,2012-10-25 15:08:28,2012-10-26 10:26:01,2012-10-25 16:12:22,closed,,,4,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1278,b'Misleading exercise in the Cross-validation estimators section of the tutorial',"b'In the exercise in [section on Cross-validation estimators](http://scikit-learn.org/dev/tutorial/statistical_inference/model_selection.html#cross-validated-estimators) one has to find an optimal regularization parameter alpha for Lasso regression.\r\n\r\nI think the exercise is misleading, to say the least. The main issue is that the scores for different `alphas` differ insignificantly, yet the plot of the [solution](http://scikit-learn.org/dev/_downloads/plot_cv_diabetes1.py) portrays them as something significant.\r\n\r\nTo be more concrete, the `scores` are:\r\n`[0.48803450860197256, 0.48803099969675356, 0.48802573754364903, 0.48801773427191092, 0.48800529096977491, 0.48798554724918719, 0.48795319169977985, 0.48789864301061825, 0.48790979589257422, 0.48810545077662604, 0.48833952711517042, 0.4885892181426163, 0.48879397803865515, 0.48900347108427633, 0.48895024452530594, 0.48886339735804762, 0.48836850577232954, 0.48762167781541343, 0.48712963815557303, 0.48692470890140377]`\r\n\r\nYet the plot is something like ![this](https://content.wuala.com/contents/tadej.janez/github_attachments/plot-lasso-alphas.png?dl=1).\r\nIt hides the y scale so that the observer can\'t see just how similar the scores are.\r\n\r\nHow I encountered the bug was that I programmed my own [solution to the exercise](https://content.wuala.com/contents/tadej.janez/github_attachments/sklearn_diabetes.py?dl=1). The plot this solution generates is this one: ![](https://content.wuala.com/contents/tadej.janez/github_attachments/plot-lasso-alphas2.png?dl=1)\r\n\r\nThe other problem with the exercise is the Bonus question: ""How much can you trust the selection of alpha?"".\r\nThe answer given in the solution is:\r\n```python\r\nk_fold = cross_validation.KFold(len(X), 3)\r\nprint [lasso.fit(X[train], y[train]).alpha for train, _ in k_fold]\r\n```\r\nwhich produces:\r\n`[0.10000000000000001, 0.10000000000000001, 0.10000000000000001]`.\r\n\r\nIn essence, the code outputs the last used `alpha` from the `alphas` list. This is obviously not the answer to the question.\r\n\r\nThe correct solution would preferably say something like:\r\n""The process used to find the optimal value of parameter `alpha` is prone to over-fitting.\r\nA possible way to avoid this problem is to perform nested cross-validation: select the value of `alpha` using *internal* cross-validation on the current training data and compute the estimator\'s score on separate test data obtained via *external* cross-validation.""\r\n\r\nThe code to perform this would be:\r\n```python\r\nlasso_cv = linear_model.LassoCV()\r\nk_fold = cross_validation.KFold(len(X), 3)\r\nfor k, (train, test) in enumerate(k_fold):\r\n    lasso_cv.fit(X[train], y[train])\r\n    print ""Fold {}: best alpha obtained with internal CV: {}, score: {}"".\\\r\n        format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test]))\r\n```\r\n\r\nThis was my first issue with scikit-learn since I started using it a week ago. \r\nI\'m trying to be a helpful user and contribute something back, so please read my comments in a constructive light.\r\n'"
1277,7863517,pmeier82,amueller,2012-10-25 14:47:14,2013-01-06 21:48:46,2013-01-06 21:48:46,closed,,0.13,11,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1277,"b""GMM.covars_ structure changed for 'spherical' cvtype""","b""It seems the 'spherical' cvtype is using a different representation of the covariances. Docs still state it to be: \r\n\r\n(n_components,)                        if 'spherical',\r\n(n_features, n_features)               if 'tied',\r\n(n_components, n_features)             if 'diag',\r\n(n_components, n_features, n_features) if 'full'\r\n\r\nindeed not it seems to be:\r\n\r\n(n_components, n_features) if 'spherical', so that 'spherical' is the same as 'diag' with a constant diagonal\r\n\r\nIt is a pita to change my code with every update of sklearn tbh.. could you perhaps settle for a naming and representation schema?"""
1268,7800445,GaelVaroquaux,larsmans,2012-10-23 15:18:35,2014-07-06 14:32:42,2014-07-06 14:32:42,closed,,,19,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1268,b'BUG: fix segfault at import of the scikit',"b""Hot fix to solve the segfault on the buildbot.\r\n\r\nIn general to use the C API of numpy (cimport numpy in Cython), you\r\nshould always insert 'np.import_array()' after the import. If not, you\r\ncan have race conditions during the imports."""
1260,7750597,ogrisel,ogrisel,2012-10-21 17:40:06,2013-01-07 23:12:41,2013-01-07 23:12:41,closed,,0.13,14,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1260,b'Test failure on feature stacker pipeline',"b'```\r\n======================================================================\r\nFAIL: sklearn.tests.test_pipeline.test_feature_stacker_weights\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/coding/scikit-learn/sklearn/tests/test_pipeline.py"", line 243, in test_feature_stacker_weights\r\n    assert_array_almost_equal(X_transformed[:, :-1], 10 * pca.fit_transform(X))\r\n  File ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 800, in assert_array_almost_equal\r\n    header=(\'Arrays are not almost equal to %d decimals\' % decimal))\r\n  File ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 636, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not almost equal to 6 decimals\r\n\r\n(mismatch 100.0%)\r\n x: array([[-26.84207125,   3.26607315],\r\n       [-27.15390616,  -1.69556848],\r\n       [-28.8981954 ,  -1.3734561 ],...\r\n y: array([[ 26.84207125,  -3.26607315],\r\n       [ 27.15390616,   1.69556848],\r\n       [ 28.8981954 ,   1.3734561 ],...\r\n>>  raise AssertionError(\'\\nArrays are not almost equal to 6 decimals\\n\\n(mismatch 100.0%)\\n x: array([[-26.84207125,   3.26607315],\\n       [-27.15390616,  -1.69556848],\\n       [-28.8981954 ,  -1.3734561 ],...\\n y: array([[ 26.84207125,  -3.26607315],\\n       [ 27.15390616,   1.69556848],\\n       [ 28.8981954 ,   1.3734561 ],...\')\r\n```\r\n\r\nUnder OX 10.8 with numpy 1.6.1.\r\n\r\nWe could try to make PCA deterministic by multiplying the components matrix by the sign of the coefficient with the largest absolute value in the largest component for instance.'"
1257,7736359,drbunsen,amueller,2012-10-20 10:08:26,2015-08-24 18:35:06,2012-12-15 10:55:15,closed,,0.16,24,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1257,b'Bug in AUC metric when TP = 100%?',"b'As an example, this works correctly:\r\n```\r\nIn [13]: import numpy as np                                                                                                                         \r\n\r\nIn [14]: from sklearn import metrics                                                                                                                \r\n\r\nIn [15]: true = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0.99]                                                                                                   \r\n\r\nIn [16]: pred = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]                                                                                                      \r\n\r\nIn [17]: fpr, tpr, thresholds = metrics.roc_curve(true, pred)                                                                                       \r\n\r\nIn [18]: metrics.auc(fpr, tpr)                                                                                                                      \r\nOut[18]: 0.22222222222222221\r\n```\r\n\r\n__However, if there are no true negatives (e.g. there is only one class), an error is thrown:__\r\n\r\n```\r\nIn [19]: true = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n\r\nIn [20]: pred = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]                                                                                                      \r\n\r\nIn [21]: fpr, tpr, thresholds = metrics.roc_curve(true, pred)                                                                                       \r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-21-35631f51a7c5> in <module>()\r\n----> 1 fpr, tpr, thresholds = metrics.roc_curve(true, pred)\r\n\r\n    132     # ROC only for binary classification\r\n    133     if classes.shape[0] != 2:\r\n--> 134         raise ValueError(""ROC is defined for binary classification only"")\r\n    135 \r\n    136     y_score = np.ravel(y_score)\r\n\r\nValueError: ROC is defined for binary classification only\r\n```\r\nIs this the correct behavior?'"
1255,7722219,erg,amueller,2012-10-19 17:15:02,2012-10-24 23:05:03,2012-10-24 23:05:03,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1255,b'custom CrossValidation classes look like a number type sometimes',"b'Someone on IRC was trying to write their own cross validation class, but when calling ``check_cv`` it got replaced with a ``StratifiedKFolds`` object.\r\n\r\nThe validator in question: https://gist.github.com/3919224\r\n\r\nThe reason for this is that\r\n```python\r\noperator.isNumberType(CohortCrossValidator(np.array([True, False, True, False]), 2))\r\n```\r\nreturns ``True`` because ``operator`` is written with the C API.\r\n\r\n\r\n```\r\n12:59 < Yhg1s> erg: classic class instances are a separate type that implement all the special \r\n               methods (and when called, call the methods on the instance object.)\r\n12:59 < Yhg1s> erg: so to the C API (which the operator module uses), they look like they \r\n               implement everything.\r\n```\r\n\r\nTo make the ``CohortCrossValidator`` work, just extend the ``object`` class.\r\n```python\r\nclass CohortCrossValidator(object):\r\n```\r\nHowever, this is apparently something that should have gone away in python 2.5 and is no longer a good idiom.\r\n\r\nNot sure what to do here, but it\'s confusing and a bug IMHO.\r\n\r\n\r\nFrom ``sklearn/cross_validation.py``:\r\n\r\n```python\r\ndef check_cv(cv, X=None, y=None, classifier=False):\r\n    is_sparse = sp.issparse(X)\r\n    if cv is None:\r\n        cv = 3\r\n    if operator.isNumberType(cv):  # <----- problem line\r\n        if classifier:\r\n            cv = StratifiedKFold(y, cv, indices=is_sparse)\r\n        else:\r\n            if not is_sparse:\r\n                n_samples = len(X)\r\n            else:\r\n                n_samples = X.shape[0]\r\n            cv = KFold(n_samples, cv, indices=is_sparse)\r\n    if is_sparse and not getattr(cv, ""indices"", True):\r\n        raise ValueError(""Sparse data require indices-based cross validation""\r\n                         "" generator, got: %r"", cv)\r\n    return cv\r\n```'"
1254,7719473,amueller,glouppe,2012-10-19 15:35:18,2012-10-23 11:43:19,2012-10-23 11:43:19,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1254,b'Unexpected ExtraTrees behavior',"b""In certain situations, ExtraTrees behave differently from RandomForests.\r\nWhen no progress can be made on a dataset using a single split, ExtraTrees don't grow at all (at least this is my diagnosis at the moment).\r\nThis [gist](https://gist.github.com/7bb07fe022b61b2cd4b4) illustrates the problem.\r\nI'd really prefer the ExtraTrees to just pick a random split (which I guess is what the random forests do).\r\n\r\nI am not terribly familiar with the tree building code.\r\nCould someone else maybe have a look?"""
1251,7701258,erg,GaelVaroquaux,2012-10-18 22:27:00,2013-07-25 15:47:37,2013-07-25 15:37:24,closed,,0.14-rc,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1251,"b""building with python3 doesn't make a _check_build.so and nosetests fails""","b'When compiling with python2 with the line ``python2 setup.py build_ext -i``, we get the file ``sklearn/__check_build/_check_build.so``. However, ``python3`` doesn\'t make this file.\r\n\r\nWhen attempting to run nosetests, the output is something like:\r\n\r\n\r\n```\r\nerg@ommegang ~/python/scikit-learn $ [master*] nosetests\r\nE\r\n======================================================================\r\nERROR: Failure: ImportError (No module named _check_build\r\n___________________________________________________________________________\r\nContents of /home/erg/python/scikit-learn/sklearn/__check_build:\r\n__init__.py               _check_build.pyx          setup.py\r\nsetup.pyc                 _check_build.c            __init__.pyc\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget\r\nto build the package before using it: run `python setup.py install` or\r\n`make` in the source directory.\r\n\r\nIf you have used an installer, please check that it is suited for your\r\nPython version, your operating system and your platform.)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/loader.py"",\r\nline 390, in loadTestsFromName\r\n    addr.filename, addr.module)\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/importer.py"",\r\nline 39, in importFromPath\r\n    return self.importFromDir(dir_path, fqname)\r\n  File ""/usr/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/importer.py"",\r\nline 86, in importFromDir\r\n    mod = load_module(part_fqname, fh, filename, desc)\r\n  File ""/home/erg/python/scikit-learn/sklearn/__init__.py"", line 31, in <module>\r\n    from . import __check_build\r\n  File ""/home/erg/python/scikit-learn/sklearn/__check_build/__init__.py"",\r\nline 47, in <module>\r\n    raise_build_error(e)\r\n  File ""/home/erg/python/scikit-learn/sklearn/__check_build/__init__.py"",\r\nline 42, in raise_build_error\r\n    %s"""""" % (e, local_dir, \'\'.join(dir_content).strip(), msg))\r\nImportError: No module named _check_build\r\n___________________________________________________________________________\r\nContents of /home/erg/python/scikit-learn/sklearn/__check_build:\r\n__init__.py               _check_build.pyx          setup.py\r\nsetup.pyc                 _check_build.c            __init__.pyc\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget\r\nto build the package before using it: run `python setup.py install` or\r\n`make` in the source directory.\r\n\r\nIf you have used an installer, please check that it is suited for your\r\nPython version, your operating system and your platform.\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.002s\r\n\r\nFAILED (errors=1)\r\n```'"
1240,7572929,ogrisel,ogrisel,2012-10-14 12:09:13,2012-10-16 15:41:25,2012-10-16 15:41:14,closed,,0.13,15,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1240,b'Broken SVM anova example',"b'When running `plot_svm_anova.py`:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/doc/sphinxext/gen_rst.py"", line 306, in generate_file_rst\r\n    execfile(os.path.basename(src_file), my_globals)\r\n  File ""plot_svm_anova.py"", line 45, in <module>\r\n    this_scores = cross_validation.cross_val_score(clf, X, y, n_jobs=1)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/cross_validation.py"", line 1101, in cross_val_score\r\n    for train, test in cv)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/externals/joblib/parallel.py"", line 473, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/externals/joblib/parallel.py"", line 296, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/externals/joblib/parallel.py"", line 124, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/cross_validation.py"", line 1039, in _cross_val_score\r\n    estimator.fit(X[train], y[train], **fit_params)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/pipeline.py"", line 132, in fit\r\n    self.steps[-1][-1].fit(Xt, y, **fit_params)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/svm/base.py"", line 187, in fit\r\n    self._gamma = 1.0 / X.shape[1]\r\nZeroDivisionError: float division\r\n```'"
1239,7572909,ogrisel,amueller,2012-10-14 12:06:01,2012-10-18 15:24:12,2012-10-18 15:24:12,closed,,0.13,9,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1239,b'Label propagation example is broken',"b'```\r\n$ python examples/semi_supervised/plot_label_propagation_digits_active_learning.py\r\n\r\n========================================\r\nLabel Propagation digits active learning\r\n========================================\r\n\r\nDemonstrates an active learning technique to learn handwritten digits\r\nusing label propagation.\r\n\r\nWe start by training a label propagation model with only 10 labeled points,\r\nthen we select the top five most uncertain points to label. Next, we train\r\nwith 15 labeled points (original 10 + 5 new ones). We repeat this process\r\nfour times to have a model trained with 30 labeled examples.\r\n\r\nA plot will appear showing the top 5 most uncertain digits for each iteration\r\nof training. These may or may not contain mistakes, but we will train the next\r\nmodel with their true labels.\r\n\r\nTraceback (most recent call last):\r\n  File ""examples/semi_supervised/plot_label_propagation_digits_active_learning.py"", line 57, in <module>\r\n    labels=lp_model.classes_)\r\n  File ""/Users/oliviergrisel/coding/scikit-learn/sklearn/metrics/metrics.py"", line 69, in confusion_matrix\r\n    y_true = np.array([label_to_ind[x] for x in y_true])\r\nKeyError: 0\r\n```'"
1233,7495813,mheilman,amueller,2012-10-10 21:02:18,2015-02-09 09:33:14,2012-10-13 14:31:47,closed,,0.13,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1233,b'GridSearchCV fails on some sparse matrices',"b'The GridSearchCV class appears to run into trouble with some sparse matrix formats, in particular, the COO format that comes out of the CountVectorizer class.  It looks like it tries to use integer indexing, and that isn\'t supported for all sparse matrix types (maybe just CSR?).\r\n\r\nI\'m running into this with sklearn version 0.12 and scipy version 0.11.0.\r\n\r\n\r\nHere\'s some code that demonstrates it, modified from http://scikit-learn.org/0.11/_downloads/document_classification_20newsgroups2.py.\r\n\r\n\r\nimport os\r\nimport sys\r\nfrom sklearn.datasets import fetch_20newsgroups\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.svm import LinearSVC\r\nfrom sklearn.grid_search import GridSearchCV\r\n\r\ncategories = None\r\n\r\ndata_train_all = fetch_20newsgroups(subset=\'train\', categories=categories,\r\n                               shuffle=True, random_state=42)\r\n\r\nn_all = len(data_train_all.data)\r\ncategories = data_train_all.target_names\r\ny_train = data_train_all.target\r\n\r\nprint ""Extracting features from the training dataset using a sparse vectorizer""\r\n\r\nvectorizer = CountVectorizer(min_df=500, max_df=0.5, stop_words=\'english\', dtype=float)\r\nX_train = vectorizer.fit_transform(data_train_all.data)\r\n\r\nsvc = LinearSVC(loss=\'l2\', penalty=\'l2\', dual=False, tol=1e-3)\r\ngridsearch = GridSearchCV(estimator=svc, param_grid={\'C\': [0.1, 1.0]})\r\n\r\nprint >> sys.stderr, ""trying with csr...""\r\ngridsearch.fit(X_train.tocsr(), y_train)\r\n\r\nprint >> sys.stderr, ""trying with default coo... FAILS HERE""\r\ngridsearch.fit(X_train, y_train)'"
1229,7446188,yarikoptic,amueller,2012-10-09 12:46:21,2013-07-27 09:44:33,2013-07-27 09:44:33,closed,,0.14-rc,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1229,b'failed test_multiclass.test_ovr_fit_predict -- random failure or precision issue?',"b'failed while building for ubuntu 11.04 32bit:\r\n\r\n```\r\n======================================================================\r\nFAIL: sklearn.tests.test_multiclass.test_ovr_fit_predict\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 186, in runTest\r\n    self.test(*self.arg)\r\n  File ""/tmp/buildd/scikit-learn-0.12.1/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/tests/test_multiclass.py"", line 66, in test_ovr_fit_predict\r\n    assert_equal(np.mean(iris.target == pred), np.mean(iris.target == pred2))\r\nAssertionError: 0.96666666666666667 != 0.97333333333333338\r\n```'"
1228,7446123,yarikoptic,GaelVaroquaux,2012-10-09 12:42:57,2012-10-12 09:27:50,2012-10-12 09:27:50,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1228,b'0.12.1 - test_spectral_clustering_sparse fails on 64bit',"b'it is a fun one:\r\n```\r\n======================================================================\r\nFAIL: sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/tmp/buildd/scikit-learn-0.12.1/debian/python-sklearn/usr/lib/python2.7/dist-packages/sklearn/cluster/tests/test_spectral.py"", line 112, in test_spectral_clustering_sparse\r\n    assert_greater(np.mean(labels == [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]), .9)\r\nAssertionError: 0.90000000000000002 not greater than 0.9\r\n```\r\n\r\nnot sure how numpy maintains original decimal representation so it gets this, since all hex views are the same:\r\n\r\n```\r\n*In [23]: (0.9).hex()\r\nOut[23]: \'0x1.ccccccccccccdp-1\'\r\n\r\nIn [24]: np.float64(0.9).hex()\r\nOut[24]: \'0x1.ccccccccccccdp-1\'\r\n\r\nIn [25]: np.float64(0.90000000000000002).hex()\r\nOut[25]: \'0x1.ccccccccccccdp-1\'\r\n```\r\n\r\nbut in any case -- it seems just to be a failing test anyways if they should not be equal\r\n'"
1225,7437931,datakungfu,larsmans,2012-10-09 04:11:44,2013-01-18 16:55:28,2013-01-18 16:55:23,closed,,0.14-rc,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1225,b'installation failed in Solaris x86 64bit',"b'Hi guys,\r\nI tried everything but cannot find any solution. Please help. thanks a lot.\r\n\r\nThe following is from the compilation:\r\n\r\ncompiling C sources\r\n\r\nC compiler: gcc -m64 -fno-strict-aliasing -g -O2 -DNDEBUG -O2 -DHAVE_NCURSES_H -fPIC\r\n\r\n\r\n\r\ncompile options: \'-DNO_ATLAS_INFO=1 -Isklearn/src/cblas -I/users/home/aagns9c3/django_projects/epd/lib/python2.7/site-packages/numpy/core/include -I/users/home/aagns9c3/django_projects/epd/lib/python2.7/site-packages/numpy/core/include -I/users/home/aagns9c3/django_projects/epd/include/python2.7 -c\'\r\n\r\ngcc: sklearn/utils/arrayfuncs.c\r\n\r\nIn file included from sklearn/utils/arrayfuncs.c:241:\r\n\r\nsklearn/utils/src/cholesky_delete.c: In function `float_cholesky_delete\':\r\n\r\nsklearn/utils/src/cholesky_delete.c:55: error: syntax error before numeric constant\r\n\r\nsklearn/utils/src/cholesky_delete.c:58: warning: passing arg 2 of `cblas_scopy\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:58: warning: passing arg 4 of `cblas_scopy\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:59: error: invalid lvalue in assignment\r\n\r\nsklearn/utils/src/cholesky_delete.c:62: error: invalid lvalue in assignment\r\n\r\nsklearn/utils/src/cholesky_delete.c:65: warning: passing arg 1 of `cblas_srotg\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:65: warning: passing arg 2 of `cblas_srotg\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:66: error: subscripted value is neither array nor pointer\r\n\r\nsklearn/utils/src/cholesky_delete.c:72: error: subscripted value is neither array nor pointer\r\n\r\nsklearn/utils/src/cholesky_delete.c:73: error: invalid lvalue in assignment\r\n\r\nsklearn/utils/src/cholesky_delete.c:76: warning: passing arg 2 of `cblas_srot\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:76: warning: passing arg 4 of `cblas_srot\' makes pointer from integer without a cast\r\n\r\nIn file included from sklearn/utils/arrayfuncs.c:241:\r\n\r\nsklearn/utils/src/cholesky_delete.c: In function `float_cholesky_delete\':\r\n\r\nsklearn/utils/src/cholesky_delete.c:55: error: syntax error before numeric constant\r\n\r\nsklearn/utils/src/cholesky_delete.c:58: warning: passing arg 2 of `cblas_scopy\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:58: warning: passing arg 4 of `cblas_scopy\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:59: error: invalid lvalue in assignment\r\n\r\nsklearn/utils/src/cholesky_delete.c:62: error: invalid lvalue in assignment\r\n\r\nsklearn/utils/src/cholesky_delete.c:65: warning: passing arg 1 of `cblas_srotg\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:65: warning: passing arg 2 of `cblas_srotg\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:66: error: subscripted value is neither array nor pointer\r\n\r\nsklearn/utils/src/cholesky_delete.c:72: error: subscripted value is neither array nor pointer\r\n\r\nsklearn/utils/src/cholesky_delete.c:73: error: invalid lvalue in assignment\r\n\r\nsklearn/utils/src/cholesky_delete.c:76: warning: passing arg 2 of `cblas_srot\' makes pointer from integer without a cast\r\n\r\nsklearn/utils/src/cholesky_delete.c:76: warning: passing arg 4 of `cblas_srot\' makes pointer from integer without a cast\r\n\r\nerror: Command ""gcc -fno-strict-aliasing -g -O2 -DNDEBUG -O2 -DHAVE_NCURSES_H -fPIC -DNO_ATLAS_INFO=1 -Isklearn/src/cblas -I/users/home/aagns9c3/django_projects/epd/lib/python2.7/site-packages/numpy/core/include -I/users/home/aagns9c3/django_projects/epd/lib/python2.7/site-packages/numpy/core/include -I/users/home/aagns9c3/django_projects/epd/include/python2.7 -c sklearn/utils/arrayfuncs.c -o build/temp.solaris-2.11-i86pc-2.7/sklearn/utils/arrayfuncs.o"" failed with exit status 1\r\n\r\n----------------------------------------\r\nCommand /users/home/aagns9c3/django_projects/recco_site/bin/python -c ""import setuptools;__file__=\'/users/home/aagns9c3/django_projects/recco_site/build/scikit-learn/setup.py\';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-yWjH4h-record/install-record.txt --install-headers /users/home/aagns9c3/django_projects/recco_site/include/site/python2.7 failed with error code 1\r\nException information:\r\nTraceback (most recent call last):\r\n  File ""/users/home/aagns9c3/django_projects/recco_site/lib/python2.7/site-packages/pip-0.8.1-py2.7.egg/pip/basecommand.py"", line 130, in main\r\n    self.run(options, args)\r\n  File ""/users/home/aagns9c3/django_projects/recco_site/lib/python2.7/site-packages/pip-0.8.1-py2.7.egg/pip/commands/install.py"", line 228, in run\r\n    requirement_set.install(install_options, global_options)\r\n  File ""/users/home/aagns9c3/django_projects/recco_site/lib/python2.7/site-packages/pip-0.8.1-py2.7.egg/pip/req.py"", line 1043, in install\r\n    requirement.install(install_options, global_options)\r\n  File ""/users/home/aagns9c3/django_projects/recco_site/lib/python2.7/site-packages/pip-0.8.1-py2.7.egg/pip/req.py"", line 559, in install\r\n    cwd=self.source_dir, filter_stdout=self._filter_install, show_stdout=False)\r\n  File ""/users/home/aagns9c3/django_projects/recco_site/lib/python2.7/site-packages/pip-0.8.1-py2.7.egg/pip/__init__.py"", line 249, in call_subprocess\r\n    % (command_desc, proc.returncode))\r\nInstallationError: Command /users/home/aagns9c3/django_projects/recco_site/bin/python -c ""import setuptools;__file__=\'/users/home/aagns9c3/django_projects/recco_site/build/scikit-learn/setup.py\';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-yWjH4h-record/install-record.txt --install-headers /users/home/aagns9c3/django_projects/recco_site/include/site/python2.7 failed with error code 1\r\n:\r\n\r\n'"
1222,7416117,mattthieu,amueller,2012-10-08 10:07:39,2012-11-02 16:17:07,2012-11-02 16:17:07,closed,,0.13,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1222,b'cross_validation does not work with precomputed kernel in SVC',"b'hello,\r\n\r\nI believe cross_validation.cross_val_score does not work with\r\nclf = svm.SVC(kernel=\'precomputed\')\r\nWhen I use them I get the error\r\n\r\nscores = cross_validation.cross_val_score(clf, dists, lbls, cv=5)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/cross_validation.py"", line 838, in cross_val_score\r\n    for train, test in cv)\r\n  File ""/usr/lib/pymodules/python2.7/joblib/parallel.py"", line 409, in __call__\r\n    self.dispatch(function, args, kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/joblib/parallel.py"", line 295, in dispatch\r\n    job = ImmediateApply(func, args, kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/joblib/parallel.py"", line 101, in __init__\r\n    self.results = func(*args, **kwargs)\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/cross_validation.py"", line 785, in _cross_val_score\r\n    estimator.fit(X[train], y[train])\r\n  File ""/usr/lib/pymodules/python2.7/sklearn/svm/base.py"", line 197, in fit\r\n    raise ValueError(""X.shape[0] should be equal to X.shape[1]"")\r\nValueError: X.shape[0] should be equal to X.shape[1]\r\n\r\nI suspect it is due to the argument of SVC which is a square similarity matrix for kernel = \'precomputed\' and a rectangular matrix of points coordinates for other kernels.\r\n\r\nSorry I don\'t have time to go much deaper. Hope this helps.\r\nCheers'"
1216,7407317,amueller,amueller,2012-10-07 18:57:56,2012-10-18 11:54:34,2012-10-18 11:54:34,closed,,,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1216,b'Non-deterministic test-error in CCA',"b'My favorite kind\r\n```\r\n======================================================================\r\nERROR: sklearn.tests.test_common.test_transformers\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 186, in runTest\r\n    self.test(*self.arg)\r\n  File ""/home/local/lamueller/checkout/scikit-learn/sklearn/tests/test_common.py"", line 167, in test_transformers\r\n    X_pred2 = trans.transform(X, y_)\r\n  File ""/home/local/lamueller/checkout/scikit-learn/sklearn/pls.py"", line 372, in transform\r\n    x_scores = np.dot(Xc, self.x_rotations_)\r\nAttributeError: \'CCA\' object has no attribute \'x_rotations_\'\r\n```'"
1202,7338436,udibr,ogrisel,2012-10-03 23:58:38,2014-07-29 19:25:20,2014-07-29 19:25:20,closed,,0.15.1,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1202,b'Exception when doing - from sklearn import linear_model - in scikit-learn 0.12 on OS X 10.8.2',"b'This problem does not happens with scikit-learn 0.11\r\nThe problem happens  when calling\r\n>>> from cd_fast import sparse_std\r\nin base.py\r\nwhich is called from\r\n>>> from .base import LinearRegression\r\nin linear_model/__init__.py\r\n\r\nImportError: dlopen(/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/cd_fast.so, 2): Symbol not found: _ATL_daxpy\r\nReferenced from: /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/cd_fast.so\r\nExpected in: flat namespace\r\nin /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/cd_fast.so\r\nFile ""/Users/udi/Documents/MyProjects/Edgar/xbrl2scikit.py"", line 33, in <module>\r\n  from sklearn import linear_model\r\nFile ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/__init__.py"", line 12, in <module>\r\n  from .base import LinearRegression\r\nFile ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/base.py"", line 28, in <module>\r\n  from cd_fast import sparse_std'"
1192,7239407,eliawry,amueller,2012-09-29 18:14:38,2013-01-05 14:19:05,2013-01-05 14:19:05,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1192,b'Nose tests fail',"b'On Ubuntu 12.04\r\n\r\n<pre>\r\n python -c ""import sklearn; sklearn.test()""\r\nRunning unit tests and doctests for sklearn\r\n/usr/local/lib/python2.7/dist-packages/nose/util.py:14: DeprecationWarning: The compiler package is deprecated and removed in Python 3.x.\r\n  from compiler.consts import CO_GENERATOR\r\nNumPy version 1.6.1\r\nNumPy is installed in /usr/lib/python2.7/dist-packages/numpy\r\nPython version 2.7.3 (default, Aug  1 2012, 05:16:07) [GCC 4.6.3]\r\nnose version 1.2.1\r\nI: Seeding RNGs with 756615311\r\n......................................................S.2.17927109077 55.4082834902\r\n........None\r\n..............EE..SS........................................................S.......................................................SEE....................................................../usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/test/test_func_inspect.py:122: UserWarning: Cannot inspect object <functools.partial object at 0xaf2cfcc>, ignore list will not work.\r\n  nose.tools.assert_equal(filter_args(ff, [\'y\'], (1, )),\r\n.........................................................................................________________________________________________________________________________\r\ntest_memory setup\r\n________________________________________________________________________________\r\n________________________________________________________________________________\r\ntest_memory teardown\r\n________________________________________________________________________________\r\n..________________________________________________________________________________\r\ntest_memory setup\r\n________________________________________________________________________________\r\n..........................................................................................................................________________________________________________________________________________\r\ntest_memory teardown\r\n________________________________________________________________________________\r\n.________________________________________________________________________________\r\nsetup numpy_pickle\r\n________________________________________________________________________________\r\n.....................................Exception AttributeError: AttributeError(""\'NoneType\' object has no attribute \'tell\'"",) in <bound method memmap.__del__ of memmap(1.7066903169e-313)> ignored\r\nException AttributeError: AttributeError(""\'NoneType\' object has no attribute \'tell\'"",) in <bound method memmap.__del__ of memmap(1.7065495094e-313)> ignored\r\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/test/test_numpy_pickle.py:182: Warning: file ""/tmp/tmpevHBa9/test.pkl30"" appears to be a zip, ignoring mmap_mode ""r"" flag passed\r\n  numpy_pickle.load(this_filename, mmap_mode=\'r\')\r\nException AttributeError: AttributeError(""\'NoneType\' object has no attribute \'tell\'"",) in <bound method memmap.__del__ of memmap(1.7065550512e-313)> ignored\r\n.....________________________________________________________________________________\r\nteardown numpy_pickle\r\n________________________________________________________________________________\r\n............An unexpected error occurred while tokenizing input\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: (\'EOF in multi-line statement\', (39, 0))\r\n\r\n......................................................................................................EE........S........................---\r\n.........................................................................................................................................................................................................................................................................................................................................................................EEE.................FE............................................................................................................SSS....S....S............................................................................................EE\r\n======================================================================\r\nERROR: Doctest: sklearn.datasets.base.load_sample_image\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.datasets.base.load_sample_images\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.ensemble.gradient_boosting.GradientBoostingClassifier\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.ensemble.gradient_boosting.GradientBoostingRegressor\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.linear_model.randomized_l1.RandomizedLasso\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.linear_model.randomized_l1.RandomizedLogisticRegression\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.tree.tree.DecisionTreeClassifier\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.tree.tree.DecisionTreeRegressor\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.tree.tree.export_graphviz\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.utils.extmath.pinvh\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn._NoseTester.test\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nERROR: Doctest: sklearn.test\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/nose/plugins/doctests.py"", line 419, in tearDown\r\n    delattr(builtin_mod, self._result_var)\r\nAttributeError: _\r\n\r\n======================================================================\r\nFAIL: Doctest: sklearn.utils.extmath.pinvh\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/doctest.py"", line 2201, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for sklearn.utils.extmath.pinvh\r\n  File ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py"", line 302, in pinvh\r\n\r\n----------------------------------------------------------------------\r\nFile ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py"", line 336, in sklearn.utils.extmath.pinvh\r\nFailed example:\r\n    B = pinvh(a)\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File ""/usr/lib/python2.7/doctest.py"", line 1289, in __run\r\n        compileflags, 1) in test.globs\r\n      File ""<doctest sklearn.utils.extmath.pinvh[3]>"", line 1, in <module>\r\n        B = pinvh(a)\r\n    NameError: name \'pinvh\' is not defined\r\n----------------------------------------------------------------------\r\nFile ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py"", line 337, in sklearn.utils.extmath.pinvh\r\nFailed example:\r\n    allclose(a, dot(a, dot(B, a)))\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File ""/usr/lib/python2.7/doctest.py"", line 1289, in __run\r\n        compileflags, 1) in test.globs\r\n      File ""<doctest sklearn.utils.extmath.pinvh[4]>"", line 1, in <module>\r\n        allclose(a, dot(a, dot(B, a)))\r\n    NameError: name \'B\' is not defined\r\n----------------------------------------------------------------------\r\nFile ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py"", line 339, in sklearn.utils.extmath.pinvh\r\nFailed example:\r\n    allclose(B, dot(B, dot(a, B)))\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File ""/usr/lib/python2.7/doctest.py"", line 1289, in __run\r\n        compileflags, 1) in test.globs\r\n      File ""<doctest sklearn.utils.extmath.pinvh[5]>"", line 1, in <module>\r\n        allclose(B, dot(B, dot(a, B)))\r\n    NameError: name \'B\' is not defined\r\n\r\n\r\n----------------------------------------------------------------------\r\nRan 1255 tests in 85.175s\r\n\r\nFAILED (SKIP=11, errors=12, failures=1)\r\n\r\n</pre>'"
1177,7099552,bobpoekert,amueller,2012-09-24 17:43:07,2012-09-24 20:11:55,2012-09-24 20:05:31,closed,,,4,Bug;Documentation,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1177,"b""The manifold.Isomap docs say that it supports sparse matrices, which it doesn't""","b""The docs for [sklearn.manifold.Isomap](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap.fit) say that the fit and fit_transform methods can take sparse matrices. But if you pass a sparse matrix (I've tried scipy.sparse.dok_matrix and scipy.sparse.csc_matrix) it throws a TypeError:\n\n> TypeError: A sparse matrix was passed, but dense data is required. Use X.todense() to convert to dense."""
1163,6924553,tdhopper,tdhopper,2012-09-17 16:18:37,2012-09-17 16:28:33,2012-09-17 16:28:33,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1163,b'Possible bug in linear_model.LinearRegression()',"b""For certain matrices, I seem to be getting erroneous results in trying to do least-squares. The answers, at least, don't match what I'm getting from the pseudo-inverse and from numpy.lstsq. Here is an example:\nhttp://nbviewer.ipython.org/3738197/"""
1160,6911143,mhlr,GaelVaroquaux,2012-09-17 06:22:59,2012-11-07 15:25:37,2012-11-07 15:25:37,closed,,0.13,9,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1160,b'NMF throws ValueError when sparseness parameter is passed in',"b'I am running scikit from trunk, and I am synced.\nThe following code crasshes for me on python 2.7.2 under Ubuntu:\n\n    from sklearn import datasets, decomposition\n    from sklearn.feature_extraction import text\n    from sklearn.pipeline import Pipeline\n    pipe = Pipeline([\n            (\'vect\',\n             text.TfidfVectorizer(analyzer=\'word\', ngram_range=(1,1), max_features=500, sublinear_tf=True, norm=None)\n             ),\n            (\'extract\',\n             decomposition.ProjectedGradientNMF(10, init=\'nndsvd\', sparseness=\'data\')\n             )\n            ])\n    train_corpus = datasets.fetch_20newsgroups(subset=\'train\')\n    train_data=pipe.fit_transform(train_corpus.data, train_corpus.target)\n\nthe error is:\n\n    >>> train_data=pipe.fit_transform(train_corpus.data, train_corpus.target)\n    Traceback (most recent call last):\n      File ""<stdin>"", line 1, in <module>\n      File ""sklearn/pipeline.py"", line 137, in fit_transform\n        return self.steps[-1][-1].fit_transform(Xt, y, **fit_params)\n      File ""sklearn/decomposition/nmf.py"", line 475, in fit_transform\n        W, gradW, iterW = self._update_W(X, H, W, tolW)\n      File ""sklearn/decomposition/nmf.py"", line 398, in _update_W\n        np.r_[X.T, np.zeros((1, n_samples))],\n      File ""/usr/lib/pymodules/python2.7/numpy/lib/index_tricks.py"", line 383, in __getitem__\n        res = _nx.concatenate(tuple(objs),axis=self.axis)\n    ValueError: 0-d arrays can\'t be concatenated\n\nThere is no error if the sparseness parameter to NMF is omitted'"
1158,6905548,vgoklani,GaelVaroquaux,2012-09-16 19:42:03,2013-03-05 23:25:03,2012-10-27 21:41:20,closed,,0.13,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1158,"b""AttributeError: 'MultinomialHMM' object has no attribute 'n_symbols'""","b'I\'ve defined 3 different HMMs:\n\n\'GMM-HMM\': hmm.GMMHMM(n_components=4, covariance_type=\'diag\', n_iter=20000),\n\'Gaussian-HMM\': hmm.GaussianHMM(n_components=4,covariance_type=\'diag\', n_iter=20000),\n\'Gaussian-Multinomial\': hmm.MultinomialHMM(n_components=4),\n\nbut the latter does not have the ""n_symbols"" attribute. The exact error is:\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-31-e2628b177672> in <module>()\n      9 \n     10 for model_name, model in models.items():\n---> 11     model.fit([X_train])\n     12     model_scores = np.array( [model.score([X_train.xs(i)]) for i in range(X_train.shape[0])] )\n     13 \n\n/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/scikit_learn-0.12-py2.7-macosx-10.5-x86_64.egg/sklearn/hmm.pyc in fit(self, obs, **kwargs)\n    434             self._algorithm = ""viterbi""\n    435 \n--> 436         self._init(obs, self.init_params)\n    437 \n    438         logprob = []\n\n/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/scikit_learn-0.12-py2.7-macosx-10.5-x86_64.egg/sklearn/hmm.pyc in _init(self, obs, params)\n    980         if \'e\' in params:\n    981             emissionprob = normalize(self.random_state.rand(self.n_components,\n--> 982                 self.n_symbols), 1)\n    983             self.emissionprob_ = emissionprob\n    984 \n\nAttributeError: \'MultinomialHMM\' object has no attribute \'n_symbols\'\n\nI am running the latest version of sklearn (checked out from github): 0.12\n\nThanks!\n\nVishal'"
1155,6903443,pemistahl,amueller,2012-09-16 14:48:03,2012-09-17 20:25:57,2012-09-17 20:25:49,closed,,,2,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1155,b'AttributeError in CountVectorizer.build_analyzer()',"b'In [line 381](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L381) in `CountVectorizer.build_analyzer()` you refer to an attribute named `self.tokenize` which does not exist. According to the if-else structure, I assume the correct attribute here is `self.analyzer`, right? '"
1153,6899925,amueller,VirgileFritsch,2012-09-16 00:43:36,2012-10-01 14:29:39,2012-10-01 14:29:39,closed,,,25,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1153,b'Problem with EllipticEnvelope',"b""It tells me\n``*** ValueError: Singular covariance matrix. Please check that the covariance matrix corresponding to the dataset is full rank.``\nEven though the smallest eigenvalue of the covariance matrix is 5.  (the largest is 500).\nI haven't had time to really investigate, Any ideas?\n"""
1142,6825167,ogrisel,ogrisel,2012-09-12 17:02:58,2013-09-08 18:41:26,2013-09-08 18:41:16,closed,ogrisel,1.0,11,Bug;Large Scale,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1142,b'Make sklearn.utils.validation tools preserve memmap when possible',"b'<del>Most validation utilities in `sklearn.utils.validation` will trigger expensive memory copy of data inputs that are `np.memmap` because those utilities mostly use `np.asarray` instead of `np.asanyarray` (that preserve subclasses of `np.ndarray` as is the case for `np.memmap`).</del>\n\n<del>It seems to be that we used to use `np.asanyarray` in the past. Anybody know why those functions use `asarray` instead?</del>\n\n<del>Also it would be better to have and explicit parameter `order` with values in `[""C"", ""F"", None]` to respectively force C contiguity, Fortran contiguity or accept any of those.</del>\n\n**Edit**: the above description is incorrect and the issue should be solved in the joblib pickler instead as `np.asarray` does not copy the memory buffer hence is perfectly legit in the validation functions.'"
1140,6816944,pemistahl,GaelVaroquaux,2012-09-12 12:33:11,2014-05-20 18:47:20,2013-07-25 16:58:19,closed,,0.14-rc,20,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1140,b'Cannot build docs using docutils 0.9.1',"b'I tried to build the scikit-learn docs for version 0.12 using Python 2.7.3 and Sphinx 1.1.3 on OSX 10.8 with the command `make html-noplot` but Sphinx fails with the following stacktrace:\n\n```python\n# Sphinx version: 1.1.3\n# Python version: 2.7.3\n# Docutils version: 0.9.1 release\n# Jinja2 version: 2.6\nTraceback (most recent call last):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/cmdline.py"", line 189, in main\n    app.build(force_all, filenames)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/application.py"", line 204, in build\n    self.builder.build_update()\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/builders/__init__.py"", line 196, in build_update\n    \'out of date\' % len(to_build))\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/builders/__init__.py"", line 252, in build\n    self.write(docnames, list(updated_docnames), method)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/builders/__init__.py"", line 292, in write\n    self.write_doc(docname, doctree)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/builders/html.py"", line 419, in write_doc\n    self.docwriter.write(doctree, destination)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/writers/__init__.py"", line 77, in write\n    self.translate()\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/writers/html.py"", line 38, in translate\n    self.document.walkabout(visitor)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 173, in walkabout\n    if child.walkabout(visitor):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 173, in walkabout\n    if child.walkabout(visitor):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 173, in walkabout\n    if child.walkabout(visitor):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 173, in walkabout\n    if child.walkabout(visitor):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 173, in walkabout\n    if child.walkabout(visitor):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 173, in walkabout\n    if child.walkabout(visitor):\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 165, in walkabout\n    visitor.dispatch_visit(self)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/nodes.py"", line 1611, in dispatch_visit\n    return method(node)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sphinx/writers/html.py"", line 377, in visit_image\n    BaseTranslator.visit_image(self, node)\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/writers/html4css1/__init__.py"", line 1026, in visit_image\n    and self.settings.file_insertion_enabled):\nAttributeError: Values instance has no attribute \'file_insertion_enabled\'\n```\nI\'ve already posted this to the [Sphinx issue tracker](https://bitbucket.org/birkenfeld/sphinx/issue/1004/attributeerror-values-instance-has-no) and also tried a possible fix stated in [this issue](https://bitbucket.org/birkenfeld/sphinx/issue/883/img-scale-option-is-broken-for-html-output) but to no avail.\n\nThe following is the whole console output that Sphinx gives me. A lot of image files are not readable. Why is this the case?\n\n```sh\nPeter$ make html-noplot\nsphinx-build -D plot_gallery=False -b html -d _build/doctrees   . _build/html\nMaking output directory...\nRunning Sphinx v1.1.3\nloading pickled environment... not yet created\n[autosummary] generating autosummary for: about.rst, data_transforms.rst, datasets/index.rst, developers/debugging.rst, developers/index.rst, developers/performance.rst, developers/utilities.rst, index.rst, install.rst, model_selection.rst, ..., tutorial/statistical_inference/finding_help.rst, tutorial/statistical_inference/index.rst, tutorial/statistical_inference/model_selection.rst, tutorial/statistical_inference/putting_together.rst, tutorial/statistical_inference/settings.rst, tutorial/statistical_inference/supervised_learning.rst, tutorial/statistical_inference/unsupervised_learning.rst, unsupervised_learning.rst, user_guide.rst, whats_new.rst\n[autosummary] generating autosummary for: /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.AffinityPropagation.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.DBSCAN.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.KMeans.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.MeanShift.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.MiniBatchKMeans.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.SpectralClustering.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.Ward.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.affinity_propagation.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.estimate_bandwidth.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.cluster.k_means.rst, ..., /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.svm.libsvm.predict.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.svm.libsvm.predict_proba.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.tree.DecisionTreeClassifier.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.tree.DecisionTreeRegressor.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.tree.ExtraTreeClassifier.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.tree.ExtraTreeRegressor.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.tree.export_graphviz.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.utils.check_random_state.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.utils.resample.rst, /Users/Peter/Downloads/scikit-learn-0.12/doc/modules/generated/sklearn.utils.shuffle.rst\nbuilding [html]: targets for 53 source files that are out of date\nupdating environment: 458 added, 0 changed, 0 removed\nreading sources... [100%] whats_new                                                                                                                         \n/Users/Peter/Downloads/scikit-learn-0.12/doc/datasets/index.rst:None: WARNING: image file not readable: datasets/../auto_examples/cluster/images/plot_color_quantization_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/datasets/index.rst:None: WARNING: image file not readable: datasets/../auto_examples/datasets/images/plot_random_dataset_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:24: WARNING: image file not readable: auto_examples/svm/images/plot_oneclass_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:28: WARNING: image file not readable: auto_examples/cluster/images/plot_ward_structured_vs_unstructured_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:32: WARNING: image file not readable: auto_examples/gaussian_process/images/plot_gp_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:36: WARNING: image file not readable: auto_examples/cluster/images/plot_lena_ward_segmentation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:40: WARNING: image file not readable: auto_examples/svm/images/plot_svm_nonlinear_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:44: WARNING: image file not readable: auto_examples/applications/images/plot_species_distribution_modeling_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:48: WARNING: image file not readable: auto_examples/gaussian_process/images/plot_gp_probabilistic_classification_after_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:52: WARNING: image file not readable: auto_examples/ensemble/images/plot_forest_importances_faces_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:56: WARNING: image file not readable: auto_examples/svm/images/plot_weighted_samples_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:60: WARNING: image file not readable: auto_examples/linear_model/images/plot_sgd_weighted_samples_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:64: WARNING: image file not readable: auto_examples/cluster/images/plot_kmeans_digits_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:68: WARNING: image file not readable: auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:72: WARNING: image file not readable: auto_examples/decomposition/images/plot_faces_decomposition_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:76: WARNING: image file not readable: auto_examples/images/plot_lda_qda_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:80: WARNING: image file not readable: auto_examples/cluster/images/plot_cluster_comparison_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/svm/images/plot_oneclass_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/cluster/images/plot_ward_structured_vs_unstructured_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/gaussian_process/images/plot_gp_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/cluster/images/plot_lena_ward_segmentation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/svm/images/plot_svm_nonlinear_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/applications/images/plot_species_distribution_modeling_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/gaussian_process/images/plot_gp_probabilistic_classification_after_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/ensemble/images/plot_forest_importances_faces_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/svm/images/plot_weighted_samples_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/linear_model/images/plot_sgd_weighted_samples_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/cluster/images/plot_kmeans_digits_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/decomposition/images/plot_faces_decomposition_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/index.rst:113: WARNING: image file not readable: auto_examples/images/plot_lda_qda_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:43: WARNING: toctree references unknown document u\'modules/generated/sklearn.cluster.dbscan\'\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:81: WARNING: toctree references unknown document u\'modules/generated/sklearn.covariance.oas\'\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:224: WARNING: toctree references unknown document u\'modules/generated/sklearn.decomposition.fastica\'\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:2: WARNING: Literal block expected; none found.\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:2: WARNING: Literal block expected; none found.\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:2: WARNING: Literal block expected; none found.\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/classes.rst:2: WARNING: Literal block expected; none found.\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:None: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_cluster_comparison_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:None: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_mini_batch_kmeans_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:None: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_affinity_propagation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:None: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_mean_shift_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:244: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_segmentation_toy_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:248: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_segmentation_toy_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:253: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_segmentation_toy_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:253: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_segmentation_toy_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:330: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_ward_structured_vs_unstructured_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:334: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_ward_structured_vs_unstructured_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:340: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_ward_structured_vs_unstructured_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:340: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_ward_structured_vs_unstructured_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:384: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_dbscan_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:389: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_dbscan_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/clustering.rst:None: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_adjusted_for_chance_measures_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:None: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_covariance_estimation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:None: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_lw_vs_oas_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:None: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_sparse_cov_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:302: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_robust_vs_empirical_covariance_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:306: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_mahalanobis_distances_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:320: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_robust_vs_empirical_covariance_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/covariance.rst:321: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_mahalanobis_distances_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:None: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_pca_vs_lda_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:81: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:85: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:90: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:90: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:None: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_kernel_pca_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:189: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_5.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:194: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:194: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_5.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:312: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:316: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_6.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:322: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:322: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_6.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:None: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_image_denoising_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:None: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_ica_blind_source_separation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:390: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:394: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:399: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:399: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:427: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:431: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:437: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/decomposition.rst:437: WARNING: image file not readable: modules/../auto_examples/decomposition/images/plot_faces_decomposition_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/ensemble.rst:None: WARNING: image file not readable: modules/../auto_examples/ensemble/images/plot_forest_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/ensemble.rst:None: WARNING: image file not readable: modules/../auto_examples/ensemble/images/plot_forest_importances_faces_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/ensemble.rst:None: WARNING: image file not readable: modules/../auto_examples/ensemble/images/plot_gradient_boosting_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/ensemble.rst:None: WARNING: image file not readable: modules/../auto_examples/ensemble/images/plot_gradient_boosting_regularization_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/feature_extraction.rst:None: WARNING: image file not readable: modules/../auto_examples/cluster/images/plot_lena_ward_segmentation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/feature_selection.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_sparse_recovery_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/gaussian_process.rst:None: WARNING: image file not readable: modules/../auto_examples/gaussian_process/images/plot_gp_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/gaussian_process.rst:None: WARNING: image file not readable: modules/../auto_examples/gaussian_process/images/plot_gp_regression_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/hmm.rst:None: WARNING: image file not readable: modules/../auto_examples/images/plot_hmm_sampling_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/kernel_approximation.rst:None: WARNING: image file not readable: modules/../auto_examples/images/plot_kernel_approximation_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/label_propagation.rst:None: WARNING: image file not readable: modules/../auto_examples/semi_supervised/images/plot_label_propagation_structure_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/lda_qda.rst:17: WARNING: image file not readable: modules/../auto_examples/images/plot_lda_qda_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/lda_qda.rst:22: WARNING: image file not readable: modules/../auto_examples/images/plot_lda_qda_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_ols_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_ridge_path_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:231: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_model_selection_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:235: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_model_selection_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:241: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_model_selection_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:241: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_model_selection_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_model_selection_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_coordinate_descent_path_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:309: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_multi_task_lasso_support_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:313: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_multi_task_lasso_support_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:318: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_multi_task_lasso_support_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:318: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_multi_task_lasso_support_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_lasso_lars_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_bayesian_ridge_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/linear_model.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_ard_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_compare_methods_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:49: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:53: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:59: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:59: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:69: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:73: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:78: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:78: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_5.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_6.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_7.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_8.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_9.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_lle_digits_10.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/manifold.rst:None: WARNING: image file not readable: modules/../auto_examples/manifold/images/plot_mds_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:7: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_pdf_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:None: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_classifier_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:None: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_selection_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:213: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:217: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_sin_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:223: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/mixture.rst:223: WARNING: image file not readable: modules/../auto_examples/mixture/images/plot_gmm_sin_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/multiclass.rst:None: WARNING: image file not readable: modules/../auto_examples/images/plot_multilabel_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:99: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_classification_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:103: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_classification_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:108: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_classification_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:108: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_classification_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:None: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:394: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_nearest_centroid_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:398: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_nearest_centroid_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:403: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_nearest_centroid_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/neighbors.rst:403: WARNING: image file not readable: modules/../auto_examples/neighbors/images/plot_nearest_centroid_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:None: WARNING: image file not readable: modules/../auto_examples/svm/images/plot_oneclass_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:None: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_mahalanobis_distances_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:141: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_outlier_detection_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:145: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_outlier_detection_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:149: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_outlier_detection_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:164: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_outlier_detection_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:173: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_outlier_detection_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/outlier_detection.rst:180: WARNING: image file not readable: modules/../auto_examples/covariance/images/plot_outlier_detection_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/pls.rst:None: WARNING: image file not readable: modules/../auto_examples/images/plot_pls_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/sgd.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_sgd_separating_hyperplane_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/sgd.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_sgd_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/sgd.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_sgd_ols_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/sgd.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_sgd_loss_functions_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/sgd.rst:None: WARNING: image file not readable: modules/../auto_examples/linear_model/images/plot_sgd_penalties_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/svm.rst:None: WARNING: image file not readable: modules/../auto_examples/svm/images/plot_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/svm.rst:None: WARNING: image file not readable: modules/../auto_examples/svm/images/plot_separating_hyperplane_unbalanced_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/svm.rst:None: WARNING: image file not readable: modules/../auto_examples/svm/images/plot_weighted_samples_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/svm.rst:None: WARNING: image file not readable: modules/../auto_examples/svm/images/plot_oneclass_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/svm.rst:None: WARNING: image file not readable: modules/../auto_examples/svm/images/plot_separating_hyperplane_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/tree.rst:None: WARNING: image file not readable: modules/../auto_examples/tree/images/plot_tree_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/tree.rst:None: WARNING: image file not readable: modules/../auto_examples/tree/images/plot_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/tree.rst:None: WARNING: image file not readable: modules/../auto_examples/tree/images/plot_tree_regression_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/tree.rst:None: WARNING: image file not readable: modules/../auto_examples/tree/images/plot_tree_regression_multioutput_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/modules/tree.rst:None: WARNING: image file not readable: modules/../auto_examples/ensemble/images/plot_forest_multioutput_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/basic/tutorial.rst:None: WARNING: image file not readable: tutorial/basic/../../auto_examples/datasets/images/plot_digits_last_image_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/model_selection.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/exercises/images/plot_cv_digits_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/putting_together.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/images/plot_digits_pipe_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/settings.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/datasets/images/plot_digits_last_image_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/datasets/images/plot_iris_dataset_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/neighbors/images/plot_classification_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_ridge_variance_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_ridge_variance_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:276: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_3d_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:280: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_3d_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:284: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_3d_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:296: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_3d_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:296: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_3d_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:296: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_ols_3d_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_logistic_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/linear_model/images/plot_iris_logistic_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:426: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_margin_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:430: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_margin_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:439: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_margin_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:439: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_margin_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:475: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_kernels_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:479: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_kernels_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:497: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_kernels_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:499: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_kernels_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:517: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_kernels_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:532: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/svm/images/plot_svm_kernels_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/supervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/datasets/images/plot_iris_dataset_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:48: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:52: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:56: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:73: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:75: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:77: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_cluster_iris_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:89: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:93: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:97: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:101: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:132: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:134: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_3.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:136: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:138: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_compress_4.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_lena_ward_segmentation_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/cluster/images/plot_digits_agglomeration_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:244: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/decomposition/images/plot_pca_3d_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:248: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/decomposition/images/plot_pca_3d_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:254: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/decomposition/images/plot_pca_3d_1.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:254: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/decomposition/images/plot_pca_3d_2.png\n/Users/Peter/Downloads/scikit-learn-0.12/doc/tutorial/statistical_inference/unsupervised_learning.rst:None: WARNING: image file not readable: tutorial/statistical_inference/../../auto_examples/decomposition/images/plot_ica_blind_source_separation_1.png\nlooking for now-outdated files... none found\npickling environment... done\nchecking consistency... done\npreparing documents... done\nwriting output... [ 31%] datasets/index                                                                                                                     \nException occurred:\n  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docutils/writers/html4css1/__init__.py"", line 1026, in visit_image\n    and self.settings.file_insertion_enabled):\nAttributeError: Values instance has no attribute \'file_insertion_enabled\'\nThe full traceback has been saved in /var/folders/60/j3g5rh0d0371gf2pvpz9kdvh0000gn/T/sphinx-err-nK9VxR.log, if you want to report the issue to the developers.\nPlease also report this if it was a user error, so that a better error message can be provided next time.\nEither send bugs to the mailing list at <http://groups.google.com/group/sphinx-dev/>,\nor report them in the tracker at <http://bitbucket.org/birkenfeld/sphinx/issues/>. Thanks!\nmake: *** [html-noplot] Error 1\n``` \n\nThanks a lot!'"
1139,6801302,amueller,amueller,2012-09-11 20:42:39,2012-10-17 08:00:33,2012-10-17 08:00:33,closed,,0.13,28,Bug;Documentation;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1139,b'ElasticNet rho',"b'In the docstring of ``ElasticNet`` it seems to me the role of ``rho`` in the formular and text are inconsistent. In the formular, ``rho`` and ``1 - rho`` should  be exchanged.\nAlthough it is late, so better double check ;)'"
1137,6785795,ogrisel,amueller,2012-09-11 09:15:36,2012-12-21 09:56:46,2012-12-21 09:56:46,closed,,0.13,31,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1137,b'Unboxing of lists of strings to numpy arrays wastes memory when doing grid search on text inputs',"b""Some pipelinable scikit-learn estimators such as the vectorizers accept lists of string as input. When passing this kind input datastructure to the `GridSearchCV` object, the strings are actually unboxed which can be very wasteful if the longest string of the collection is much larger than the median (which is quite common in practice).\n\nThe reason is that check_arrays (used in GridSearchCV) is using `np.array(list_of_strings)` without giving a dtype which causes the unboxing of the strings to the maximum size of the collection:\n\n```\n>>> from sklearn.utils import check_arrays\n>>> from pprint import pprint\n>>> l = [('%02d' % i) * i for i in range(10)]\n>>> pprint(l)\n['',\n '01',\n '0202',\n '030303',\n '04040404',\n '0505050505',\n '060606060606',\n '07070707070707',\n '0808080808080808',\n '090909090909090909']\n>>> sum(len(s) for s in l)\n90\n>>> a = check_arrays(l)[0]\n>>> a\narray(['', '01', '0202', '030303', '04040404', '0505050505',\n       '060606060606', '07070707070707', '0808080808080808',\n       '090909090909090909'], \n      dtype='|S18')\n>>> a.nbytes\n180\n```\n\nTo avoid this, we should probably build arrays for text document of variable length with `np.array(list_of_strings, dtype=np.object_)` to avoid the memory wasteful string unboxing instead.\n\nThe `np.array(list_of_strings)` pattern also occurs inside the `CountVectorizer` code itself (for the list of terms and building the vocabulary). Here again passing `np.array(list_of_strings, dtype=np.object_)` might save memory."""
1129,6720837,JBionics,amueller,2012-09-07 18:22:39,2012-09-17 20:26:56,2012-09-17 20:26:56,closed,,,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1129,"b""Can't install on Mac""","b""MacPorts is version 2.1.2 (up to date).\n\nsudo port install py26-scikits-learn\nError: Port py26-scikits-learn not found\n\nsudo port install py-scikits-learn\n--->  Computing dependencies for py-scikits-learn\nError: Dependency 'py27-scikits-learn' not found.\nTo report a bug, follow the instructions in the guide:\n    http://guide.macports.org/#project.tickets\nError: Processing of port py-scikits-learn failed\n\nsudo port install py27-scikits-learn\nError: Port py27-scikits-learn not found\n\nFrustrating."""
1127,6702237,kyleabeauchamp,VirgileFritsch,2012-09-06 23:01:12,2012-09-18 08:51:45,2012-09-18 08:44:33,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1127,b'Failure in Low Dimensional case for MinCovDet()',"b'What are the minimum number of samples (n0) and features (n1) required to use MinCovDet?  To me, this method should be feasible for n0 >= 3.  However, I get an exception for (n0,n1) = (3,1).  The error does not occur for (3,2) or for (4,1).  I suspect this issue has to do with an array being autocast to a lower-rank object.\n\n```\nimport numpy as np\nimport sklearn.covariance\n\nn0,n1 = (3,1)\nx = np.random.normal(size=(n0,n1))\n\nmodel = sklearn.covariance.outlier_detection.MinCovDet()\nmodel.fit(x)\n```\n\nYields the following error: \nrobust_covariance.pyc in fast_mcd(X, support_fraction, cov_computation_method, random_state)\n--> 337         halves_start = np.where(diff == np.min(diff))[0]\nValueError: zero-size array to minimum.reduce without identity\n'"
1121,6694986,yarikoptic,GaelVaroquaux,2012-09-06 17:57:24,2013-07-26 06:52:55,2013-07-26 06:52:55,closed,,0.14-rc,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1121,b'test_affinities: RuntimeError: Factor is exactly singular   on older scipy 64bit',"b'as reported on the list\n\n```\n======================================================================\nERROR: sklearn.cluster.tests.test_spectral.test_affinities\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File ""/usr/lib/pymodules/python2.6/nose/case.py"", line 183, in runTest\n    self.test(*self.arg)\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/cluster/tests/test_spectral.py"", line 121, in test_affinities\n    labels = sp.fit(X).labels_\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/cluster/spectral.py"", line 360, in fit\n    random_state=self.random_state, n_init=self.n_init)\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/cluster/spectral.py"", line 218, in spectral_clustering\n    mode=mode, random_state=random_state)\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/cluster/spectral.py"", line 122, in spectral_embedding\n    sigma=1.0, which=\'LM\')\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/utils/arpack.py"", line 1493, in _eigsh\n    symmetric=True, tol=tol)\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/utils/arpack.py"", line 1031, in get_OPinv_matvec\n    return SpLuInv(A.tocsc()).matvec\n  File ""/tmp/buildd/scikit-learn-0.12.0/debian/python-sklearn/usr/lib/python2.6/dist-packages/sklearn/utils/arpack.py"", line 899, in __init__\n    self.M_lu = splu(M)\n  File ""/usr/lib/python2.6/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py"", line 129, in splu\n    diag_pivot_thresh, drop_tol, relax, panel_size)\nRuntimeError: Factor is exactly singular\n```\n\non amd64 (ok on 32bit) Debian squeeze, Ubuntu 10.04 and 10.10 where\nscipy\'s are:\n\n```\nscikit-learn_0.12.0-1~nd10.04+1_amd64.build:Unpacking python-scipy (from .../python-scipy_0.7.0-2ubuntu0.1_amd64.deb) ...\nscikit-learn_0.12.0-1~nd10.10+1_amd64.build:Unpacking python-scipy (from .../python-scipy_0.7.2-2ubuntu1_amd64.deb) ...\nscikit-learn_0.12.0-1~nd60+1_amd64.build:Unpacking python-scipy (from .../python-scipy_0.7.2+dfsg1-1+squeeze1~nd60+1_amd64.deb) ...\n```\n\n\n'"
1119,6682784,pprett,pprett,2012-09-06 09:00:58,2014-06-13 09:38:07,2012-09-10 06:40:54,closed,,,18,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1119,b'MRG Sgd clone fix',"b'The SGD* object initialization does not work properly with ``set_params`` and thus ``clone``. \n\nThis PR moves input argument parsing and creation of helper classes (e.g. loss_function) to the ``fit`` (actually, ``partial_fit``) methods. \n\nThis PR addresses #1114.\n\nThe following issues need to be resolved:\n\n - Factory for loss function objects (epsilon issue - see failing test)\n - Should we do input argument checks in ``__init__`` or exclusively in ``fit``?\n - The sgd holds some opportunities for refactoring (code duplication, etc.)'"
1114,6657372,tobigue,tobigue,2012-09-05 10:21:20,2012-09-08 12:12:06,2012-09-08 12:12:06,closed,,,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1114,b'GridSearchCV and cross_validation not consistent',"b""When performing a cross_validation on a classifier trained with the best parameters found by a GridSearchCV the results differ, despite using the same random seed, folds and evaluation metric. So maybe one of them is not working correctly?!\n\nCode to reproduce this issue can be found here: https://gist.github.com/3188762\n\n\n    GRID SEARCH:\n    Best f1_score: 0.556\n    Best parameters set:\n        alpha: 0.0001\n        loss: 'log'\n        penalty: 'l1'\n        seed: 0\n\n    CROSS VALIDATION:\n    Best f1_score: 0.521 (+/- 0.05)\n"""
1113,6650325,johnb30,amueller,2012-09-05 01:00:48,2012-09-13 13:36:32,2012-09-13 13:36:32,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1113,b'Inability to upgrade on OS X 10.8.1',"b'When I try to update scikit-learn on OS X 10.8.1 I receive the following error:\n\nerror: Setup script exited with error: Command ""g++ -fno-strict-aliasing -fno-common -dynamic -arch i386 -isysroot /Developer/SDKs/MacOSX10.5.sdk -DNDEBUG -g -O3 -arch i386 -isysroot /Developer/SDKs/MacOSX10.5.sdk -D__STDC_FORMAT_MACROS=1 -I/Library/Frameworks/Python.framework/Versions/7.3/lib/python2.7/site-packages/numpy/core/include -I/Library/Frameworks/Python.framework/Versions/7.3/include/python2.7 -c /private/var/folders/g9/1dsf781n34sdxx74jffctxkc0000gn/T/easy_install-UzF1b0/scikit-learn-0.12/sklearn/utils/sparsetools/csgraph_wrap.cxx -o build/temp.macosx-10.5-i386-2.7/private/var/folders/g9/1dsf781n34sdxx74jffctxkc0000gn/T/easy_install-UzF1b0/scikit-learn-0.12/sklearn/utils/sparsetools/csgraph_wrap.o"" failed with exit status 1\n/Library/Frameworks/Python.framework/Versions/7.3/lib/python2.7/site-packages/numpy/distutils/misc_util.py:252: RuntimeWarning: Parent module \'numpy.distutils\' not found while handling absolute import\n  from numpy.distutils import log\n\nI\'m trying to upgrade using:\n\neasy_install -U scikit-learn\n\nI have the Enthought Python distribution installed and scikit-learn .11 works fine. '"
1112,6644024,amueller,amueller,2012-09-04 19:33:51,2012-10-12 12:25:02,2012-10-12 12:25:02,closed,,0.13,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1112,b'Random states not robust',"b'If I run a test with\n``\nshuffle(np.arange(10), random_state=0)\n``\n~~I will get a different result when another test was run before.~~\nit is more complicated...'"
1104,6603517,amueller,amueller,2012-09-02 15:55:27,2014-06-13 09:37:24,2012-09-02 18:50:20,closed,,,3,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1104,b'MRG raise ValueError in r2_score when given only a single sample.',b'Closes #1054.'
1099,6602082,amueller,amueller,2012-09-02 12:00:12,2012-10-24 16:47:21,2012-10-24 16:47:21,closed,,0.13,1,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1099,b'Random joblib failure',"b""In master in this test:\n\nFAIL: Check that using pre_dispatch Parallel does indeed dispatch items\nError:\n\n```\n- ['Produced 0', 'Produced 1', 'Consumed 0', 'Produced 2']\n?                                          --------------\n\n+ ['Produced 0', 'Produced 1', 'Produced 2', 'Consumed 0']\n?                             ++++++++++++++\n```"""
1098,6598409,artyomboyko,amueller,2012-09-01 21:24:17,2012-09-20 14:51:53,2012-09-20 14:36:58,closed,,0.13,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1098,"b""CountVectorizer can't find russian n-grams""","b'Here is the code:\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nhtml = urllib2.urlopen(""http://habrahabr.ru/"").read()\ndata = nltk.clean_html(html)\ndata = re.sub(\'&([^;]+);\', \'\', data).lower()\n\nv = CountVectorizer(min_n=2, max_n=4); \nX = v.fit_transform([data]);\nkeys = [x for x in zip(v.inverse_transform(X)[0], X.A[0])]\n\nprint keys\n\nIn result I get few english n-grams. Changing encoding didn\'t help.\n\nUPDATE: another url you can try is http://vesna.yandex.ru/'"
1085,6553285,larsmans,pprett,2012-08-30 13:31:59,2013-07-28 17:21:25,2012-09-06 06:31:01,closed,,0.13,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1085,"b""GradientBoostingClassifier doesn't work with least squares loss""","b'Triggered by [this SO question](http://stackoverflow.com/questions/12197841/why-scikit-gradientboostingclassifier-wont-let-me-use-least-squares-regression): `GradientBoostingClassifier`\'s docstring states that `loss` may be `""ls""`, in which case least squares regression will be performed, but when you only try to do that, a `ValueError` is raised. I\'m not sure if the code or the docs should be changed.\n\n(I also noticed that Huber and quantile loss are not advertised in the regressor\'s docstring.)'"
1076,6472150,amueller,amueller,2012-08-27 12:15:20,2014-06-13 09:34:52,2012-09-03 09:05:32,closed,,0.12,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1076,b'MRG support custom kernels on sparse matrices',"b""I think I mentioned this on the ML. We don't actually need sparse kernels, we just need kernels on sparse data.\nThis fixes code that previously did random memory access.... maybe should put a guard up somewhere.\n\nCloses #918."""
1059,6428107,VolkerH,VirgileFritsch,2012-08-24 08:13:55,2015-02-09 05:57:15,2012-09-13 13:10:51,closed,,0.13,12,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1059,"b'Bug in gmm.py, referencing best_params before assignment'","b""Dear all,\n\nThere appears to be a bug in gmm.py where under certain circumstances (specifically  log_likelihood[-1] > max_log_prob never true during the EM iterations) best_params doesn't get assigned but is referenced later on. \n\nI am using version scikit_learn-0.11-py2.7-win32.egg.\n\nHere is an Ipython session that shows what's going on:\n\n```\nIn [9]: im\nOut[9]:\narray([[ 5,  5,  5, ..., 16, 13, 11],\n       [ 5,  5,  6, ..., 10, 12, 10],\n       [ 5,  5,  6, ..., 10,  7,  6],\n       ...,\n       [ 6,  7,  6, ...,  5,  6,  7],\n       [ 5,  6,  6, ...,  6,  6,  6],\n       [ 6,  7,  9, ...,  6,  6,  5]], dtype=uint8)\n\nIn [10]: from sklearn.mixture import GMM\n\nIn [11]: classif = GMM(n_components=2)\n\nIn [12]: classif.fit(im.reshape((im.size, 1)))\n---------------------------------------------------------------------------\nUnboundLocalError                         Traceback (most recent call last)\n\nC:\\Documents and Settings\\Administrator\\My Documents\\Dropbox\\readstacks_python.p\ny in <module>()\n----> 1\n      2\n      3\n      4\n      5\n\nC:\\Python27\\lib\\site-packages\\scikit_learn-0.11-py2.7-win32.egg\\sklearn\\mixture\\\ngmm.pyc in fit(self, X, **kwargs)\n    519                                    'covars': self.covars_}\n    520         if self.n_iter:\n--> 521             self.covars_ = best_params['covars']\n    522             self.means_ = best_params['means']\n    523             self.weights_ = best_params['weights']\n\nUnboundLocalError: local variable 'best_params' referenced before assignment\n\nIn [13]:\n\nIn [14]: %debug\n> c:\\python27\\lib\\site-packages\\scikit_learn-0.11-py2.7-win32.egg\\sklearn\\mixtur\ne\\gmm.py(521)fit()\n    520         if self.n_iter:\n--> 521             self.covars_ = best_params['covars']\n    522             self.means_ = best_params['means']\n\nipdb> best_params\n*** NameError: name 'best_params' is not defined\nipdb> self.n_iter\n100\nipdb> log_likelihood\n[-20135.341019560929, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan\n, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan\n, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan\n, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan\n, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan\n, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan\n, nan, nan, nan, nan, nan, nan, nan]\nipdb>\n```"""
1054,6417888,vgoklani,amueller,2012-08-23 19:56:02,2012-09-02 18:53:49,2012-09-02 18:53:49,closed,,0.12,7,Bug;Easy,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1054,b'problems doing cross-validation with SVR() and gridsearchCV()',"b""I am using sklearn 0.11, and trying to do a gridsearch using SVR:\n\nC_range = 2. ** np.array([-5,-3,-1, 1, 3, 5, 7, 9, 11, 13, 15 ,17])\ngamma_range = 2. ** np.array([-15 ,-13, -11, -9, -7, -5, -3, -1, 1, 3, 5])\n\nparam_grid = {'gamma':gamma_range, 'C':C_range}\n\nclf = GridSearchCV(SVR(), param_grid=param_grid, n_jobs=-1, cv=10)\nclf.fit(X_train,Y_train)\n\nthe code runs in less than a second, and outputs:\n\nSVR(C=0.03125, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0625,\n  kernel=rbf, probability=False, shrinking=True, tol=0.001, verbose=False)\n\nbut those numbers are garbage, and it's clear that there is no cross-validation being done. For the sake of comparison, I switched to SVC() (and also defined a target variable), and the code ran properly (and took a fair amount of time).  Am I doing something wrong, or is there a bug?\n\nThanks!\n\nVishal"""
1027,6248021,erg,amueller,2012-08-15 19:00:23,2012-11-10 16:03:49,2012-11-10 16:03:49,closed,,0.13,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1027,b'calling fit() on a decision tree can cause infinite loops',"b""```\nclf = DecisionTreeClassifier(min_samples_split=20,\n                                 min_samples_leaf=5,\n                                 max_features=None)\n\n\n\nshape X (3220, 3375)\nshape y (3220,)\ncalling clf.fit(X,y)\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\nException ValueError: ValueError('Attempting to find a split with an empty sample_mask',) in 'sklearn.tree._tree.Tree.recursive_partition' ignored\n```\n\n~~I tested this with the current git version. Using cython 0.16, the problem goes away.~~\n\n~~Broken:~~\n```\nsudo make clean && sudo python2 setup.py install\n```\n\n~~Fixed:~~\n```\nsudo make clean && make cython && sudo python2 setup.py install\n```"""
1011,6165773,briancheung,amueller,2012-08-10 23:02:00,2012-09-03 09:45:54,2012-09-03 09:45:54,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/1011,b'Spectral Clustering consistently creates one sample cluster',"b""Code to reproduce the issue:\n\n```python\nimport numpy as np\ndef generate_blobs_3d():\n    np.random.seed(27)\n    x1 = np.random.randn(200,3) + np.array([1.4, 1.8, 22.2])\n    x2 = np.random.randn(100,3) + np.array([4.7, 4.0, 9.6])\n    x3 = np.random.randn(400,3) + np.array([100.7, 100.0, 100.8])\n    blobs = np.vstack((x1,x2,x3))\n    return blobs\nblobs = generate_blobs_3d()\nS = np.corrcoef(blobs)\nS[S < 0] = 0\nfrom sklearn import cluster\nalgorithm = cluster.SpectralClustering(k=3, mode='arpack')\nalgorithm.fit(S)\ny_pred = algorithm.labels_.astype(np.int)\n```\n\nThe one sample cluster consistently occurs for the first sample."""
998,6048199,buma,larsmans,2012-08-06 11:50:42,2014-05-10 14:50:24,2014-05-10 14:50:24,closed,,1.0,8,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/998,b'Program segfaults with MultinomialNB and Cross validation',"b""I am trying to create a Naive bayes classifier but If I cross validate with 5 folds it segfaults.\n\nI have finally succeeded in creating example that also segfaults.\n\nThe problem is that program segfaults if I change cv to 5 in here:\nscores = cross_val_score(classifier, X_train, y_train, cv=3, n_jobs=1)\n\n\n[primer.py](http://dl.dropbox.com/u/25828692/example1/primer.py)\n[X_all.pickle](http://dl.dropbox.com/u/25828692/example1/X_all.pickle)\n[y.pickle](http://dl.dropbox.com/u/25828692/example1/y.pickle)\n\nValgrind dump:\n==32594== Warning: set address range perms: large range [0x18851018, 0x28851038) (noaccess)\n==32594== Warning: set address range perms: large range [0x8c4b028, 0x1c3252a0) (undefined)\n==32594== Warning: set address range perms: large range [0x1c326028, 0x2fa002b1) (undefined)\n==32594== Warning: set address range perms: large range [0x1c326018, 0x2fa002c1) (noaccess)\n==32594== Warning: set address range perms: large range [0x8c4b018, 0x1c3252b0) (noaccess)\n==32594== Warning: set address range perms: large range [0x8c4b028, 0x2726235c) (undefined)\n==32594== Warning: set address range perms: large range [0x76b9f028, 0x951b636d) (undefined)\n==32594== Warning: set address range perms: large range [0x76b9f018, 0x951b637d) (noaccess)\n==32594== Warning: set address range perms: large range [0x8c4b018, 0x2726236c) (noaccess)\n==32594== Warning: set address range perms: large range [0x38d41018, 0x58d41038) (noaccess)\nLoaded X\nLoaded y\nSPlit\n==32594== Warning: set address range perms: large range [0x7389b028, 0x8eed5318) (undefined)\n==32594== Invalid read of size 4\n==32594==    at 0x4F3960B: trivial_three_operand_loop (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==32594==    by 0x4F4EEF7: PyUFunc_GenericFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==32594==    by 0x4F4F1BA: ufunc_generic_call (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==32594==    by 0x409025F: PyObject_Call (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4090347: call_function_tail (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x409045F: _PyObject_CallFunction_SizeT (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4E67B80: PyArray_GenericBinaryFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==32594==    by 0x408BE25: binary_op1 (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x408DC35: PyNumber_Add (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4125781: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==32594==  Address 0x1c is not stack'd, malloc'd or (recently) free'd\n==32594==\n==32594==\n==32594== Process terminating with default action of signal 11 (SIGSEGV)\n==32594==  Access not within mapped region at address 0x1C\n==32594==    at 0x4F3960B: trivial_three_operand_loop (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==32594==    by 0x4F4EEF7: PyUFunc_GenericFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==32594==    by 0x4F4F1BA: ufunc_generic_call (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==32594==    by 0x409025F: PyObject_Call (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4090347: call_function_tail (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x409045F: _PyObject_CallFunction_SizeT (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4E67B80: PyArray_GenericBinaryFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==32594==    by 0x408BE25: binary_op1 (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x408DC35: PyNumber_Add (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4125781: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==32594==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n\nThis is on yesterday's master scikit, scipy, numpy.\n"""
994,5999400,erg,amueller,2012-08-02 18:53:29,2012-09-21 08:56:00,2012-09-21 08:56:00,closed,,0.13,45,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/994,"b'SelectKBest, SelectPercentile feature selection is broken for certain inputs'","b'Test case. Fails for ``score_func=f_classif`` as well.\n\n```python\n# Fixed already, I had an old .11-git release\nimport numpy as np\nfrom sklearn.feature_selection import *\nX = np.array([[-0.35407973, -0.73858161, -1.21871862,  0.01362241, -0.3565459 ],\n       [-2.92151461, -0.64259241, -0.43849487,  1.01860028, -0.84427892],\n       [-0.83171833, -0.0852727 , -1.89007728, -1.31564981,  0.1420426 ],\n       [-0.62100712,  0.86350539, -0.49176233, -0.51831504,  1.98832398],\n       [ 1.2643767 ,  0.33973403, -0.82126126, -0.08305002, -0.4985046 ]])\n\ny = np.array([-0.17966557, -0.19387233,  1.77140599,  0.78315866, -1.40862817])\n\ntransformer = SelectKBest(score_func=chi2, k=3)\ntransformer.fit(X, y)\ntransformer.transform(X).shape[1]\nassert transformer.transform(X).shape[1] == 3\n```\n\n\n```python\nimport numpy as np\nfrom sklearn.feature_selection import *\nX = np.array([[-0.35407973, -0.73858161, -1.21871862,  0.01362241, -0.3565459 ],\n       [-2.92151461, -0.64259241, -0.43849487,  1.01860028, -0.84427892],\n       [-0.83171833, -0.0852727 , -1.89007728, -1.31564981,  0.1420426 ],\n       [-0.62100712,  0.86350539, -0.49176233, -0.51831504,  1.98832398],\n       [ 1.2643767 ,  0.33973403, -0.82126126, -0.08305002, -0.4985046 ]])\n\ny = np.array([-0.17966557, -0.19387233,  1.77140599,  0.78315866, -1.40862817])\n\ntransformer = SelectPercentile(score_func=chi2, percentile=60)\ntransformer.fit(X, y)\ntransformer.transform(X).shape[1]\nassert transformer.transform(X).shape[1] == 3\n```'"
965,5685296,pprett,glouppe,2012-07-18 10:18:04,2013-07-27 14:10:18,2013-07-27 14:10:18,closed,pprett,0.14-rc,7,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/965,b'GBRT 32bit issue',b'the ``gradient_boosting`` module has stability issues on 32bit arch; the source of the instability seems to lie in the fitting procedure - it may even lie in ``_tree.pyx``. '
960,5669570,aflag,amueller,2012-07-17 17:27:29,2012-09-05 20:10:07,2012-09-05 20:10:07,closed,,,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/960,b'silhouette_score returning NaN',"b""Hello, the following code makes silhoutte_score return NaN. That looks like a bug to me. If it's impossible to generate a score, then I expected silhouette_score to raise a meaningful exception instead of returning NaN. The following code prints -nan.\n\n```python\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\n\ndata = np.array([\n    [ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., 1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1., 0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,],\n    [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0., 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0., 2.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  2.,  0.,  0.,  0.,],\n    [ 0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1., 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0., 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,],\n    [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  2.,  0.,  0.,  1.,],\n    [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,]\n], dtype=np.float)\n\nkmeans = KMeans(init='k-means++', k=2)\nkmeans.fit(data)\n\nprint '%f' % metrics.silhouette_score(data, kmeans.labels_, metric='euclidean')\n```\n\nThe first matrix I've tried came from a bigger sample of 14k items (and many more features for each sample). I reduced the number of samples to 5 and k to 2 so that it's easier to test. In the original sample silhoutte score would fail if k was greater than 80. On that test it would print nan (instead of -nan),."""
919,5306261,amueller,amueller,2012-06-27 20:31:26,2012-10-16 20:41:21,2012-10-16 20:41:21,closed,,0.13,5,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/919,b'Unexpected nosetest results / random things',"b""When I run\n``nosetests sklearn``\neverything looks fine, If I do\n``nosetests sklearn/svm/tests/``\nI get an error in ``test_sparse.py``, but when I do\n``nosetests sklearn/svm/tests/test_sparse.py``\nI get no error.\n\nI guess this has something to do with the shuffling of the dataset or other random things.\nI thought the way it is written the shuffling is done once and for all for all the tests so I don't really understand how this can happen. Any ideas?"""
918,5304615,larsmans,amueller,2012-06-27 19:17:18,2012-09-03 09:06:02,2012-09-03 09:06:02,closed,,0.12,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/918,"b""Sparse SVM won't handle callable kernel""","b""When the feature matrix `X` passed to an `SVC` is sparse and the kernel is a callable, it never gets called. Instead, the SVM seems to just go ahead and make abysmally bad predictions, without an error or warning.\n\nI'm not exactly sure how to fix this right now (tired and getting a bit lost in the code). What I would like is for `SVC` to store my sparse input matrix and call my kernel function with two sparse matrices."""
884,4878173,amueller,amueller,2012-06-03 17:31:35,2012-10-11 20:04:41,2012-10-11 20:04:41,closed,,0.13,1,Bug;Moderate,https://api.github.com/repos/scikit-learn/scikit-learn/issues/884,b'Numerical issues and possible bug in mutual information / v-score',"b'There are numerical issues in computing the mutual information.\nAlso, the v-score should agree with a certain form of normalized mutual information, but it does not.\nSee details at #776.'"
871,4801043,fmder,GaelVaroquaux,2012-05-29 18:23:31,2012-10-27 21:41:48,2012-10-27 21:41:48,closed,,0.13,47,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/871,b'Multinomial HMM fit Errors',"b'When fitting data with the multinomial HMM,\n\n- First, the number of symbols is undefined\n- Second, it crashes when unpacking a shape on the first line of the forward pass.\n\nHere is how to reproduce the bug\n\n    import numpy\n    from sklearn import hmm\n    \n    n_components = 2\n    n_symbols = 2\n    N = 5000\n    \n    # Generator Multinomial Hidden Markov Model\n    transmat = numpy.array([[0.8, 0.2], [0.25, 0.75]])\n    emissionprob_ = numpy.array([[0.85, 0.15], [0.4, 0.6]])\n    startprob = numpy.array([0.8, 0.2])\n    \n    emitter = hmm.MultinomialHMM(n_components, startprob, transmat)\n    emitter.emissionprob_ = emissionprob_\n    \n    observations, true_states = emitter.sample(N)\n    \n    learner = hmm.MultinomialHMM(n_components)\n    # learner.n_symbols = n_symbols\n    learner.fit(observations)\n\nThe second bug is enlighten by uncommenting the second last line.\n\nThanks for this great library.\nFranois-Michel De Rainville'"
870,4795640,chrivo,amueller,2012-05-29 14:01:23,2012-11-03 14:10:17,2012-11-03 14:10:17,closed,,0.13,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/870,b'Forward pass of HMM creates NAN when transition matrix contains zeros',"b""I tried to train a HMM with the codebelow. The call to fit threw an exception and complained that the transition probabilities probabilities didn't summed to one. After some debuging I saw that the call to _hmmc._forward resulted in NANs in the forward-lattice, which where (i think) caused by the zeros in the initial transmat. I temporarily solved the problem by setting the zeros to 1e-6 and renormalized transmat before calling fit. I guess a check for NANs would help here.\n\nimport numpy as np\nfrom sklearn.hmm import GaussianHMM\n\nstartprob = np.array([1,0,0,0,0])\ntransmat = np.array([[0.9,0.1,0,0,0],\n                                 [0,0.9,0.1,0,0],\n                                 [0,0,0.9,0.1,0],\n                                 [0,0,0,0.9,0.1],\n                                 [0,0,0,0,1.0]])\n            \nhmm = GaussianHMM(n_components=5, \n                              covariance_type='full', \n                              startprob=startprob, \n                              transmat=transmat)\n            \nhmm.means_ = np.zeros((5,10))\nhmm.covars_ = np.tile(np.identity(10), (5, 1, 1))\n\nobs = [hmm.sample(10)[0] for _ in range(10)]\n            \nhmm.fit(obs=obs, n_iter=100, init_params='st')"""
860,4596187,ndwns,amueller,2012-05-16 00:40:59,2012-09-18 12:08:44,2012-09-18 12:08:44,closed,,0.13,23,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/860,b'missing cblas dependency for utils/arrayfuncs.so on FreeBSD',"b'`pip install` installs ok but there\'s a missing runtime dependency on cblas\n\n```\nPython 2.7.3 (default, May  3 2012, 05:58:58) \n[GCC 4.2.1 20070831 patched [FreeBSD]] on freebsd9\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> from sklearn.decomposition import NMF\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/decomposition/__init__.py"", line 10, in <module>\n    from .sparse_pca import SparsePCA, MiniBatchSparsePCA\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/decomposition/sparse_pca.py"", line 8, in <module>\n    from ..linear_model import ridge_regression\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/linear_model/__init__.py"", line 15, in <module>\n    from .least_angle import Lars, LassoLars, lars_path, LARS, LassoLARS, \\\n  File ""/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py"", line 20, in <module>\n    from ..utils import array2d, arrayfuncs, deprecated\nImportError: /usr/local/lib/python2.7/site-packages/sklearn/utils/arrayfuncs.so: Undefined symbol ""cblas_srotg""\n```\n\n`cblas_srotg` is in:\n```\n[root@ /]# nm /usr/local/lib/libcblas.so | grep cblas_srotg\n0000000000005480 T cblas_srotg\n```\n\nbut `arrayfuncs.so` doesn\'t have libcblas as a dependency:\n```\n[root@ /]# ldd /usr/local/lib/python2.7/site-packages/sklearn/utils/arrayfuncs.so \n/usr/local/lib/python2.7/site-packages/sklearn/utils/arrayfuncs.so:\n        liblapack.so.4 => /usr/local/lib/liblapack.so.4 (0x801208000)\n        libblas.so.2 => /usr/local/lib/libblas.so.2 (0x801a0d000)\n        libm.so.5 => /lib/libm.so.5 (0x801c60000)\n        libthr.so.3 => /lib/libthr.so.3 (0x801e81000)\n        libc.so.7 => /lib/libc.so.7 (0x80084a000)\n        libgfortran.so.3 => /usr/local/lib/gcc46/libgfortran.so.3 (0x8020a4000)\n        libgcc_s.so.1 => /usr/local/lib/gcc46/libgcc_s.so.1 (0x8023b8000)\n        libquadmath.so.0 => /usr/local/lib/gcc46/libquadmath.so.0 (0x8025cd000)\n```\n\n`FreeBSD  9.0-RELEASE FreeBSD 9.0-RELEASE #0: Tue Jan  3 07:46:30 UTC 2012     root@farrell.cse.buffalo.edu:/usr/obj/usr/src/sys/GENERIC  amd64`\n`\n'"
799,4300780,amueller,larsmans,2012-04-26 12:34:58,2013-11-21 19:59:12,2013-11-21 19:59:12,closed,,0.15,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/799,b'Segfault in MiniBatchKMeans with large dataset',"b""I have a problem with a segfault in MiniBatchKMeans :-(\r\nI have ``k=5000, X.shape=[1000000,  128]``. If I make any of the two much smaller, the problem disappears.\r\n\r\nInterestingly the segfault is after convergence, in the final call to ``_labels_inertia``, inside the distance computation.\r\nIf ``tracer`` doesn't lie to me, it's inside ``np.dot``.\r\nThis seems a bit weird.\r\n\r\nI guess it has something to do with the dataset being that big? Before, the function was always just called for a batch. I can avoid the problem by using ``compute_labels=False``.\r\nStill, somewhat unsatisfactory :-/"""
792,4271019,erg,larsmans,2012-04-24 22:39:59,2013-03-28 00:31:09,2013-03-28 00:31:09,closed,ogrisel,0.14-rc,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/792,b'fetch_data.py fails',"b'```\r\nerg@ommegang ~/scikit-learn-tutorial/data/languages $ [master] python2 fetch_data.py \r\nTraceback (most recent call last):\r\n  File ""fetch_data.py"", line 59, in <module>\r\n    html_content = open(html_filename).read().decode(\'utf-8\')\r\n  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: \'utf8\' codec can\'t decode byte 0x8b in position 1: invalid start byte\r\nerg@ommegang ~/scikit-learn-tutorial/data/languages $ [master] \r\n```\r\n\r\nMy system is running Arch Linux 64-bit.\r\n```\r\nuname -a\r\nLinux ommegang 3.3.2-1-ARCH #1 SMP PREEMPT Sat Apr 14 09:48:37 CEST 2012 x86_64 Intel(R) Core(TM) i7-3960X CPU @ 3.30GHz GenuineIntel GNU/Linux\r\n\r\npython2 --version\r\nPython 2.7.3\r\n```\r\n\r\nThanks.'"
784,4191425,conradlee,amueller,2012-04-19 13:24:36,2012-10-18 11:36:04,2012-10-18 11:36:04,closed,,0.13,4,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/784,b'Randomized Lasso causes error with run on data with very few examples',"b""In a test run of a classification system I've set up I run the whole thing through on a test dataset that has only 20 examples.\r\n\r\nWhen I do feature selection using the randomized lasso, I get an error -- the traceback is [here](http://pastebin.com/hyGBYSDX).  Any ideas what might be causing this problem?  It doesn't seem to occur if I run the randomized lasso with more data."""
713,3713354,XiaoLiuAI,amueller,2012-03-19 16:42:48,2012-09-05 20:08:55,2012-09-05 20:08:55,closed,,,13,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/713,b'irrational huge pickle file with sparse linearSVC',"b'I found that the pickle file of sparse linearSVC became irrational larger after shifting the index of features.\r\nI had trained a series of models using window of tokens in natural language processing. The pickle files are less than 10M with 20,000~50,000 dense features. In order to create a flexible approach, I shifted the index of features to left by 9 bits, and all the new pickle files have the same size 582M. It seems that all the void feature weights were saved. Can we just save the useful data?'"
636,3248768,vene,ogrisel,2012-02-16 10:26:14,2015-08-26 17:27:30,2014-06-20 13:20:28,closed,,1.0,59,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/636,b'Parallel K-Means hangs on Mac OS X Lion',"b""I first noticed this when running 'make test' hanged. I tried with stable and bleeding edge scipy (I initially thought it was something arpack related).\r\n\r\nThe test `sklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_2_jobs` hangs the process.\r\n\r\nRunning in IPython something like `KMeans(init='k-means++', n_jobs=2).fit(np.random.randn(100, 100))` hangs as well.\r\n\r\nI thought maybe there was something wrong with my setup, but `cross_val_score` works OK with `n_jobs=2`."""
604,3054549,vicpara,amueller,2012-02-01 16:37:56,2015-10-22 12:27:06,2013-07-26 15:17:45,closed,,0.14-rc,22,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/604,"b'linear_model.SGDRegressor : Buffer has wrong number of dimensions (expected 1, got 2)'","b'Hello,\r\nI\'m using SGDRegressor to fit a data set of 74K entries and 11 features.When I call the fit method I get the error from below. I\'ve tried to understand the problem but I cannot say what shall I do differently. The entries from y are equal with the entries in X.\r\n\r\nThe error:\r\nTraceback (most recent call last):\r\n  File ""D:\\Work\\Python\\HeritagePrize\\HeritagePrize\\Program.py"", line 45, in <module>\r\n    error = sgd.Train()\r\n  File ""D:\\Work\\Python\\HeritagePrize\\HeritagePrize\\MyPredictors.py"", line 103, in Train\r\n    self.predictor.fit(data.trainData, data.trainTarget)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\base.py"", line 506, in fit\r\n    self._fit_regressor(X, y)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py"", line 330, in _fit_regressor\r\n    self.eta0, self.power_t)\r\n  File ""sgd_fast.pyx"", line 214, in sklearn.linear_model.sgd_fast.plain_sgd (sklearn\\linear_model\\sgd_fast.c:4954)\r\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)'"
598,3023992,vicpara,amueller,2012-01-30 17:51:13,2012-09-05 20:12:24,2012-09-05 20:12:24,closed,,,6,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/598,b'ARDRegression -  array is too big error',"b'Hello,\r\nWhen I run the ARDRegression algorithm I get the ""array is too big"" error. The data is indeed large about 113K records or 11 integer columns. \r\n1. Is there a work around for this issue ?\r\n2. Are there methods for the learner to load the data progressively ?\r\n3. If I divide the data into chunks and train the algorithm sequentially what is the effect ? I know this sounds really stupid but how can I divide the work in smaller pieces.\r\n\r\nTraceback (most recent call last):\r\n  File ""\\Program.py"", line 58, in <module>\r\n    clf.fit(input, target)\r\n  File ""C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\bayes.py"", line 401, in fit\r\n    sigma_ = linalg.pinv(np.eye(n_samples) / alpha_ +\r\n  File ""C:\\Python27\\lib\\site-packages\\numpy\\lib\\twodim_base.py"", line 210, in eye\r\n    m = zeros((N, M), dtype=dtype)\r\nValueError: array is too big.'"
584,2947922,amueller,amueller,2012-01-24 09:59:55,2012-09-18 11:51:22,2012-09-18 11:51:22,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/584,b'NMF inititialization does not work for n_samples < n_components',"b'It raises an index error. NMF works with random init.\r\nIf there is an easy fix for still using SVD, that would be best. If not, we could check if n_samples < n_components and then use random init.'"
393,1888929,mfergie,GaelVaroquaux,2011-10-12 16:07:59,2013-01-17 16:43:13,2013-01-17 16:43:13,closed,alextp,0.13,17,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/393,b'DPGMM and VBGMM clustering weights not updating',"b""When trying to cluster the old faithful data set from [1] and the data set from [2] both the DPGMM and VBGMM are unable to find a suitable fit. It looks as if the component weights are not updating properly as they remain at 1/K after fitting the model. I've attached an example script which contains 2 example data sets where this problem is displayed.\r\n\r\nTar file including script, data and an example figure:\r\nhttp://www.sendspace.com/file/vyecbg\r\n\r\n[1] Old faithful dataset: http://research.microsoft.com/en-us/um/people/cmbishop/prml/webdatasets/datasets.htm\r\n[2] Figueiredo and Jain, Unsupervised Learning of Finite Mixture Models, PAMI 2002\r\n\r\nThanks,\r\nMartin"""
291,1333451,goretkin,amueller,2011-08-02 21:43:09,2015-02-25 19:02:30,2015-02-25 19:02:28,closed,,,10,Bug,https://api.github.com/repos/scikit-learn/scikit-learn/issues/291,b'fmin_cobyla used in gaussian_process.py does not like np.inf',"b'The reduced_likelihood_function() in gaussian_process/gaussian_process.py returns np.inf when the Cholesky decomposition of R raises an np.linalg.LinAlg exception. When the objective function of scipy.optimize.fmin_cobyla returns np.inf, the optimizer passes np.nan to the objective function (I raised this issue on scipy-user). This np.nan causes an exception when trying to do the Cholesky decomposition again, because this time R is full of np.nan.\r\n\r\nMaybe there are three routes to fixing this issue\r\n\r\n1. Do something other than return np.inf  when the Cholesky decomposition raises a LinAlg exception\r\n2. Have reduced_likelihood_function() treat a nan argument better than raising an exception, in such a way that fmin_cobyla can proceed.\r\n3. Use an optimizer different from fmin_cobyla -- one that handles values of np.inf better.\r\n'"
