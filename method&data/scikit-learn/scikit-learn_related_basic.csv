issue,id,reporter,closed_by,created_at,updated_at,closed_at,state,assignee,milestone,comments,label_name,url,title,body
numpy/numpy#6271,104090544,charris,charris,2015-08-31 16:56:24,2016-02-10 18:25:05,2015-09-10 18:02:38,closed,,,7,10 - Maintenance;component: numpy.core,https://api.github.com/repos/numpy/numpy/issues/6271,"b'DEP,MAINT: Change deprecated indexing to errors.'","b'The deprecated operations changed to errors are\r\n\r\n* Conversion of floats and booleans to npy_intp indexes.\r\n* Conversion of single element integer arrays of ndim > 1 to indexes.\r\n* Multiple ellipsis in index.\r\n\r\nThe affected deprecation tests have been been changed to check for\r\nraised errors instead, but remain in test_deprecations.py for the\r\nmoment. They will be moved when the conversion of the remaining indexing\r\ndeprecations is complete.'"
joblib/joblib#263,114068961,fjanoos,ogrisel,2015-10-29 15:02:08,2015-10-30 18:16:39,2015-10-30 18:16:39,closed,,,13,,https://api.github.com/repos/joblib/joblib/issues/263,b'joblib.Parallel crashes on interactively defined functions',"b'Hello,\r\n\r\nI\'m trying to parallelize an operation on the columns of pandas dataframe using joblib, using this logic:\r\n\r\n`parallel =  Parallel(n_jobs=-1, backend=\'multiprocessing\', verbose=12);`\r\n`joblist  =  [delayed(myfunc)( df[col] )  for col in columns ];`\r\n`retlist  =  parallel( joblist );`\r\n\r\nwhere myfunc takes a column and returns a column. This code used to work fine until a few ago, before I did a conda update of pandas and joblib (and some other libraries). After the update, however, the following error gets thrown:\r\n\r\n`File ""<string>"", line unknown`\r\n`SyntaxError: invalid or missing encoding declaration for \'/home/firdaus/local/software/anaconda/lib/python3.4/site-packages/pandas/lib.cpython-34m.so\'`\r\n\r\nAny suggestions would be appreciated.\r\n\r\nthanks.'"
joblib/joblib#258,112790030,mfrodl,mfrodl,2015-10-22 12:12:03,2015-11-15 11:27:10,2015-11-15 11:27:10,closed,,,2,,https://api.github.com/repos/joblib/joblib/issues/258,b'format_records attempts to tokenize .pyc file',"b""This issue was originally reported as https://github.com/scikit-learn/scikit-learn/issues/5412, but since the root cause appears to be in Joblib, let us discuss it here instead.\r\n\r\nSteps to reproduce:\r\n```python\r\n# imported.py\r\ndef raise_exc(x):\r\n    raise Exception\r\n```\r\n\r\n```python\r\n# test.py\r\nfrom joblib import delayed, Parallel\r\nfrom imported import raise_exc\r\n\r\nprint Parallel(n_jobs=2)(delayed(raise_exc)(i) for i in range(2))\r\n\r\n```\r\nNow try to run the test.py script:\r\n```\r\n$ python test.py\r\nAn unexpected error occurred while tokenizing input file /home/mfrodl/sandbox/imported.pyc\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: ('EOF in multi-line statement', (4, 0))\r\n\r\nAn unexpected error occurred while tokenizing input file /home/mfrodl/sandbox/imported.pyc\r\nThe following traceback may be corrupted or invalid\r\nThe error message is: ('EOF in multi-line statement', (4, 0))\r\n\r\n...\r\n```\r\nProposed fix follows, see the original issue for a more detailed justification:\r\n```diff\r\ndiff --git a/joblib/format_stack.py b/joblib/format_stack.py\r\nindex bef5c3b..1abd496 100644\r\n--- a/joblib/format_stack.py\r\n+++ b/joblib/format_stack.py\r\n@@ -195,6 +195,10 @@ def format_records(records):   # , print_globals=False):\r\n             # the abspath call will throw an OSError.  Just ignore it and \r\n             # keep the original file string.\r\n             pass\r\n+\r\n+            if file.endswith('.pyc'):\r\n+                file = file[:-4] + '.py'\r\n+\r\n         link = file\r\n         try:\r\n             args, varargs, varkw, locals = inspect.getargvalues(frame)\r\n```"""
PyAbel/PyAbel#51,121122797,DanHickstein,DanHickstein,2015-12-08 23:15:36,2015-12-14 18:38:27,2015-12-14 18:38:27,closed,,Version 0.7,3,bug,https://api.github.com/repos/PyAbel/PyAbel/issues/51,b'Compatibility with numpy 1.10.1',"b'Numpy 1.10 apparently has stricter rules for casting floats as integers:\r\nhttps://github.com/scikit-learn/scikit-learn/issues/5397\r\n\r\nThis causes an error with PyAbel as well, specifically for the ``example_hansenlaw.py``.\r\n\r\nI think that we just need to specifically cast the numbers to the correct types, but I think I recall there is something weird going on with a half-integer origin in the case of the hansen-law transform? @stggh \r\n\r\nHere is the output from trying to run the example:\r\n\r\n```\r\nDaniels-MacBook-Pro-3:examples danhickstein$ python example_hansenlaw.py\r\nLoading data/O2-ANU1024.txt.bz2\r\nimage size 1024x1024\r\nPerforming Hansen and Law inverse Abel transform:\r\nHL: Calculating inverse Abel transform:  image size 1024x1024\r\nHL: Individual quadrants\r\nHL: Calculating inverse Abel transform ...\r\n0.29 seconds\r\nGenerating speed distribution ...\r\nTraceback (most recent call last):\r\n  File ""example_hansenlaw.py"", line 52, in <module>\r\n    recon, speeds = iabel_hansenlaw (im,quad=(False,False,False,False),verbose=True)\r\n  File ""/Users/danhickstein/Documents/PyAbel-master/abel/hansenlaw.py"", line 210, in iabel_hansenlaw\r\n    speeds = calculate_speeds(recon,origin=image_centre)\r\n  File ""/Users/danhickstein/Documents/PyAbel-master/abel/tools.py"", line 24, in calculate_speeds\r\n    polarIM, ri, thetai = reproject_image_into_polar(IM,origin)\r\n  File ""/Users/danhickstein/Documents/PyAbel-master/abel/tools.py"", line 145, in reproject_image_into_polar\r\n    x, y = index_coords(data, origin=origin)\r\n  File ""/Users/danhickstein/Documents/PyAbel-master/abel/tools.py"", line 179, in index_coords\r\n    x -= origin_x\r\nTypeError: Cannot cast ufunc subtract output from dtype(\'float64\') to dtype(\'int64\') with casting rule \'same_kind\'\r\n```\r\n\r\nThanks to @ellisje624 for pointing this out.'"
nilearn/nilearn#896,121109606,rschmaelzle,GaelVaroquaux,2015-12-08 22:03:25,2015-12-20 09:51:14,2015-12-20 09:51:14,closed,,0.2.2,9,,https://api.github.com/repos/nilearn/nilearn/issues/896,b'cast/type issue in high_variance confounds',"b'Hi All,\r\nI am hitting a cast error with this line\r\n\r\n```python\r\nhv_confounds = nilearn.image.high_variance_confounds(func_filename)\r\n```\r\nThe error goes like this\r\n```\r\n.....\r\n --> 144     signals -= np.mean(signals, axis=0)\r\n    145     if type == ""linear"":\r\n    146         # Keeping ""signals"" dtype avoids some type conversion further down,\r\n\r\nTypeError: Cannot cast ufunc subtract output from dtype(\'float64\') to dtype(\'<i2\') with casting rule \'same_kind\'\r\n....\r\n```\r\n\r\nthe versions we have installed are: nilearn 0.1.4.post1  and numpy 1.10.1\r\nAny help is much appreciated! Love nilearn!\r\nBest, Ralf.\r\n\r\nPS see this seems to relate to https://github.com/scikit-learn/scikit-learn/issues/5397\r\nPPS tried downgrading to previous numpy ""conda install numpy=1.9.1"", but then there are dependencies to scipy... '"
NixOS/nixpkgs#9146,99520362,FRidh,FRidh,2015-08-06 20:40:16,2015-10-22 14:04:35,2015-10-22 14:04:32,closed,,,8,old-label: update-package,https://api.github.com/repos/NixOS/nixpkgs/issues/9146,b'python-packages updates scipy and scikitlearn',b'Built and installed locally. Tested with Python 3.4.'
,,,,,,,,,,,,,,
nilearn/nilearn#712,95997813,lesteve,lesteve,2015-07-20 06:44:15,2015-10-27 10:29:27,2015-10-27 10:29:27,closed,,0.2,3,Documentation,https://api.github.com/repos/nilearn/nilearn/issues/712,b'Sphinx 1.3.1 auto documentation issues',b'It looks like it is missing some info for automatically generated classes/functions:\r\nhttp://nilearn.github.io/modules/reference.html#module-nilearn.decoding\r\n\r\nFor completeness the same thing happens on circleci:\r\nhttps://circle-artifacts.com/gh/nilearn/nilearn/150/artifacts/0/home/ubuntu/nilearn/doc/_build/html/modules/reference.html#module-nilearn.datasets'
sphinx-doc/sphinx#1892,78173295,Eric89GXL,shimizukawa,2015-05-19 17:10:37,2015-10-15 16:01:02,2015-07-05 03:11:36,closed,,,13,,https://api.github.com/repos/sphinx-doc/sphinx/issues/1892,b'FIX: Only check if members is True',"b'Closes #1891.\r\nCloses #1822.\r\n\r\n`check_module` should only be called if `members` is currently activated, otherwise formely working use cases like those in #1891 are broken. This is also consistent with the docs:\r\n\r\n> In an automodule directive with the members option set, only module members whose __module__ attribute is equal to the module name as given to automodule will be documented. This is to prevent documentation of imported classes or functions. Set the imported-members option if you want to prevent this behavior and document all available members. Note that attributes from imported modules will not be documented, because attribute documentation is discovered by parsing the source file of the current module.\r\n\r\nReady for review/merge from my end.'"
numpy/numpy#5533,56392018,dschallis,,2015-02-03 15:28:00,2015-06-23 07:33:45,None,open,,,29,,https://api.github.com/repos/numpy/numpy/issues/5533,b'numpy.dot cannot handle arrays with >2**31 elements without support in the underlying BLAS',"b""[The bug here is that some BLAS libraries will crash if you pass in > 16 GiB arrays, because they use 32-bit indices internally. The most obvious solution is to break the dot call into multiple calls to dgemm, though there are also [other possibilities](https://github.com/numpy/numpy/issues/5533#issuecomment-114309520). Original report follows:]\r\n\r\n---\r\n\r\nOn python 2.7.8, numpy 1.9.1, on Mac OS X:\r\n\r\n    import numpy\r\n    numpy.random.seed(1)\r\n    X = numpy.random.random((50000,100))\r\n    numpy.dot(X, X.T)\r\n\r\nResults in:\r\n\r\n    Segmentation fault: 11\r\n\r\nSegfault doesn't occur on smaller arrays (e.g. 30000x100 is fine).  In case it's useful, some linkage info:\r\n\r\n    >>> numpy.__config__.show()\r\n    atlas_threads_info:\r\n      NOT AVAILABLE\r\n    blas_opt_info:\r\n        extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\r\n        extra_compile_args = ['-msse3', '-DAPPLE_ACCELERATE_SGEMV_PATCH', '-I/System/Library/Frameworks/vecLib.framework/Headers']\r\n        define_macros = [('NO_ATLAS_INFO', 3)]\r\n    atlas_blas_threads_info:\r\n      NOT AVAILABLE\r\n    openblas_info:\r\n      NOT AVAILABLE\r\n    lapack_opt_info:\r\n        extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\r\n        extra_compile_args = ['-msse3', '-DAPPLE_ACCELERATE_SGEMV_PATCH']\r\n        define_macros = [('NO_ATLAS_INFO', 3)]\r\n    openblas_lapack_info:\r\n      NOT AVAILABLE\r\n    atlas_info:\r\n      NOT AVAILABLE\r\n    lapack_mkl_info:\r\n      NOT AVAILABLE\r\n    blas_mkl_info:\r\n      NOT AVAILABLE\r\n    atlas_blas_info:\r\n      NOT AVAILABLE\r\n    mkl_info:\r\n      NOT AVAILABLE"""
joblib/joblib#150,36825823,ogrisel,ogrisel,2014-06-30 18:46:22,2014-06-30 19:33:28,2014-06-30 19:32:41,closed,,,4,,https://api.github.com/repos/joblib/joblib/issues/150,"b""Use mmap_mode='r' by default in MemmapingPool""","b""`mmap_mode='c'` triggers unexpected memory paging under Windows. See: scikit-learn/scikit-learn#3313."""
joblib/joblib#214,73560817,ogrisel,lesteve,2015-05-06 08:47:20,2015-07-13 12:08:17,2015-05-06 09:30:33,closed,,0.9.0,3,Bug,https://api.github.com/repos/joblib/joblib/issues/214,b'[MRG] mmap_mode for mixed dtype objects',"b'Here is the kind of error message that users get when trying to load an object that has both memmap-able arrays and object-dtyped arrays in ther internal data-structures:\r\n\r\n```\r\n======================================================================\r\nERROR: joblib.test.test_numpy_pickle.test_memmap_persistence_mixed_dtypes\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/nose/case.py"", line 198, in runTest\r\n    self.test(*self.arg)\r\n  File ""/Users/ogrisel/code/joblib/joblib/test/test_numpy_pickle.py"", line 225, in test_memmap_persistence_mixed_dtypes\r\n    a_clone, b_clone = numpy_pickle.load(filename, mmap_mode=\'r\')\r\n  File ""/Users/ogrisel/code/joblib/joblib/numpy_pickle.py"", line 520, in load\r\n    obj = unpickler.load()\r\n  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/pickle.py"", line 1038, in load\r\n    dispatch[key[0]](self)\r\n  File ""/Users/ogrisel/code/joblib/joblib/numpy_pickle.py"", line 386, in load_build\r\n    array = nd_array_wrapper.read(self)\r\n  File ""/Users/ogrisel/code/joblib/joblib/numpy_pickle.py"", line 136, in read\r\n    mmap_mode=unpickler.mmap_mode)\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/numpy/lib/npyio.py"", line 391, in load\r\n    return format.open_memmap(file, mode=mmap_mode)\r\n  File ""/Users/ogrisel/venvs/py34/lib/python3.4/site-packages/numpy/lib/format.py"", line 725, in open_memmap\r\n    raise ValueError(msg)\r\nValueError: Array can\'t be memory-mapped: Python objects in dtype.\r\n```\r\n\r\nThis fix in `NDArrayWrapper` makes `joblib.load` avoid tying memmory-mapping inner arrays with `dtype=object`.'"
pydata/pandas#10043,72474981,amueller,jreback,2015-05-01 17:05:44,2015-05-08 05:00:36,2015-05-08 00:06:40,closed,,0.16.1,28,Difficulty Intermediate;Effort Low;Indexing,https://api.github.com/repos/pydata/pandas/issues/10043,b'iloc breaks on read-only dataframe',"b""This is picking up #9928 again. I don't know if the behavior is expected, but it is a bit odd to me. Maybe I'm doing something wrong, I'm not that familiar with the pandas internals.\r\n\r\nWe call ``df.iloc[indices]`` and that breaks with a read-only dataframe. I feel that it shouldn't though, as it is not writing.\r\n\r\nMinimal reproducing example:\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\narray = np.eye(10)\r\narray.setflags(write=False)\r\n\r\nX = pd.DataFrame(array)\r\nX.iloc[[1, 2, 3]]\r\n```\r\n> ValueError buffer source array is read-only\r\n\r\nIs there a way to slice the rows of the dataframe in another way that doesn't need a writeable array?"""
pydata/pandas#9928,69262465,ek-ok,ek-ok,2015-04-18 03:43:50,2015-04-28 12:11:32,2015-04-18 05:59:42,closed,,,4,,https://api.github.com/repos/pydata/pandas/issues/9928,b'Pandas gives 600 line long ValueError while runing sklearn GridSearchCV',"b'I am using sklearn GridSearchCV to classify some data. When I run this code, pandas gives 600 line long error. If I set n_jobs=1, it runs fine without any error. Please kindly take a look at it. Thanks\r\n\r\n    rfc = RandomForestClassifier(random_state=1)\r\n    tuned_parameters = [{\'max_features\': [\'sqrt\', \'log2\'], \r\n                     \'n_estimators\': [20, 100, 200, 500, 1000]}]\r\n\r\n    clf = GridSearchCV(rfc, tuned_parameters, scoring=\'roc_auc\', cv=3, n_jobs=-1, verbose=2)\r\n    clf.fit(X, y)\r\n\r\nError message\r\n\r\n    Fitting 3 folds for each of 10 candidates, totalling 30 fits\r\n    [CV] max_features=sqrt, n_estimators=20 ..............................\r\n    [CV] max_features=sqrt, n_estimators=20 ..............................\r\n    [CV] max_features=sqrt, n_estimators=20 ..............................\r\n    [CV] max_features=sqrt, n_estimators=100 .............................\r\n    An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: (\'EOF in multi-line statement\', (101150, 0))\r\n    An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: (\'EOF in multi-line statement\', (101150, 0))\r\n    An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: (\'EOF in multi-line statement\', (101150, 0))\r\n    An unexpected error occurred while tokenizing input\r\n    The following traceback may be corrupted or invalid\r\n    The error message is: (\'EOF in multi-line statement\', (101150, 0))\r\n    \r\n    \r\n    \r\n    \r\n    [CV] max_features=sqrt, n_estimators=200 .............................[CV] max_features=sqrt, n_estimators=100 .............................[CV] max_features=sqrt, n_estimators=100 .............................[CV] max_features=sqrt, n_estimators=200 .............................\r\n    \r\n    \r\n    \r\n    ---------------------------------------------------------------------------\r\n    JoblibValueError                          Traceback (most recent call last)\r\n    <ipython-input-25-99a52d0f19db> in <module>()\r\n    4\r\n          5 clf = GridSearchCV(rfc, tuned_parameters, scoring=\'roc_auc\', cv=3, n_jobs=-1, verbose=2)\r\n    ----> 6 clf.fit(X, y)\r\n    \r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit(self, X, y)\r\n    730\r\n        731         """"""\r\n    --> 732         return self._fit(X, y, ParameterGrid(self.param_grid))\r\n    733\r\n    734\r\n    \r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc in _fit(self, X, y, parameter_iterable)\r\n        503                                     self.fit_params, return_parameters=True,\r\n        504                                     error_score=self.error_score)\r\n    --> 505                 for parameters in parameter_iterable\r\n        506                 for train, test in cv)\r\n    507\r\n    \r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\r\n        664                 # consumption.\r\n        665                 self._iterating = False\r\n    --> 666             self.retrieve()\r\n        667             # Make sure that we get a last message telling us we are done\r\n        668             elapsed_time = time.time() - self._start_time\r\n    \r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in retrieve(self)\r\n        547                         # Convert this to a JoblibException\r\n        548                         exception_type = _mk_exception(exception.etype)[0]\r\n    --> 549                         raise exception_type(report)\r\n        550                     raise exception\r\n        551                 finally:\r\n    \r\n    JoblibValueError: JoblibValueError\r\n    ___________________________________________________________________________\r\n    Multiprocessing exception:\r\n        ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/runpy.py in _run_module_as_main(mod_name=\'IPython.kernel.__main__\', alter_argv=1)\r\n        157     pkg_name = mod_name.rpartition(\'.\')[0]\r\n        158     main_globals = sys.modules[""__main__""].__dict__\r\n        159     if alter_argv:\r\n        160         sys.argv[0] = fname\r\n        161     return _run_code(code, main_globals, None,\r\n    --> 162                      ""__main__"", fname, loader, pkg_name)\r\n            fname = \'/home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py\'\r\n            loader = <pkgutil.ImpLoader instance>\r\n            pkg_name = \'IPython.kernel\'\r\n    163\r\n        164 def run_module(mod_name, init_globals=None,\r\n        165                run_name=None, alter_sys=False):\r\n        166     """"""Execute a module\'s code without importing it\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f8c57a1c630, file ""/...ite-packages/IPython/kernel/__main__.py"", line 1>, run_globals={\'__builtins__\': <module \'__builtin__\' (built-in)>, \'__doc__\': None, \'__file__\': \'/home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py\', \'__loader__\': <pkgutil.ImpLoader instance>, \'__name__\': \'__main__\', \'__package__\': \'IPython.kernel\', \'app\': <module \'IPython.kernel.zmq.kernelapp\' from \'/ho.../site-packages/IPython/kernel/zmq/kernelapp.pyc\'>}, init_globals=None, mod_name=\'__main__\', mod_fname=\'/home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py\', mod_loader=<pkgutil.ImpLoader instance>, pkg_name=\'IPython.kernel\')\r\n         67         run_globals.update(init_globals)\r\n         68     run_globals.update(__name__ = mod_name,\r\n         69                        __file__ = mod_fname,\r\n         70                        __loader__ = mod_loader,\r\n         71                        __package__ = pkg_name)\r\n    ---> 72     exec code in run_globals\r\n            code = <code object <module> at 0x7f8c57a1c630, file ""/...ite-packages/IPython/kernel/__main__.py"", line 1>\r\n            run_globals = {\'__builtins__\': <module \'__builtin__\' (built-in)>, \'__doc__\': None, \'__file__\': \'/home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py\', \'__loader__\': <pkgutil.ImpLoader instance>, \'__name__\': \'__main__\', \'__package__\': \'IPython.kernel\', \'app\': <module \'IPython.kernel.zmq.kernelapp\' from \'/ho.../site-packages/IPython/kernel/zmq/kernelapp.pyc\'>}\r\n         73     return run_globals\r\n    74\r\n         75 def _run_module_code(code, init_globals=None,\r\n         76                     mod_name=None, mod_fname=None,\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py in <module>()\r\n    1\r\n    2\r\n    ----> 3 \r\n          4 if __name__ == \'__main__\':\r\n          5     from IPython.kernel.zmq import kernelapp as app\r\n          6     app.launch_new_instance()\r\n    7\r\n    8\r\n    9\r\n    10\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/config/application.py in launch_instance(cls=<class \'IPython.kernel.zmq.kernelapp.IPKernelApp\'>, argv=None, **kwargs={})\r\n    569\r\n        570         If a global instance already exists, this reinitializes and starts it\r\n        571         """"""\r\n        572         app = cls.instance(**kwargs)\r\n        573         app.initialize(argv)\r\n    --> 574         app.start()\r\n            app.start = <bound method IPKernelApp.start of <IPython.kernel.zmq.kernelapp.IPKernelApp object>>\r\n    575\r\n        576 #-----------------------------------------------------------------------------\r\n        577 # utility functions, for convenience\r\n        578 #-----------------------------------------------------------------------------\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py in start(self=<IPython.kernel.zmq.kernelapp.IPKernelApp object>)\r\n        368     def start(self):\r\n        369         if self.poller is not None:\r\n        370             self.poller.start()\r\n        371         self.kernel.start()\r\n        372         try:\r\n    --> 373             ioloop.IOLoop.instance().start()\r\n        374         except KeyboardInterrupt:\r\n        375             pass\r\n    376\r\n        377 launch_new_instance = IPKernelApp.launch_instance\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\r\n        146             PollIOLoop.configure(ZMQIOLoop)\r\n        147         return PollIOLoop.instance()\r\n    148\r\n        149     def start(self):\r\n        150         try:\r\n    --> 151             super(ZMQIOLoop, self).start()\r\n            self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\r\n        152         except ZMQError as e:\r\n        153             if e.errno == ETERM:\r\n        154                 # quietly return on ETERM\r\n        155                 pass\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\r\n        835                 self._events.update(event_pairs)\r\n        836                 while self._events:\r\n        837                     fd, events = self._events.popitem()\r\n        838                     try:\r\n        839                         fd_obj, handler_func = self._handlers[fd]\r\n    --> 840                         handler_func(fd_obj, events)\r\n            handler_func = <function null_wrapper>\r\n            fd_obj = <zmq.sugar.socket.Socket object>\r\n            events = 5\r\n        841                     except (OSError, IOError) as e:\r\n        842                         if errno_from_exception(e) == errno.EPIPE:\r\n        843                             # Happens when the client closes the connection\r\n        844                             pass\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\r\n        270         # Fast path when there are no active contexts.\r\n        271         def null_wrapper(*args, **kwargs):\r\n        272             try:\r\n        273                 current_state = _state.contexts\r\n        274                 _state.contexts = cap_contexts[0]\r\n    --> 275                 return fn(*args, **kwargs)\r\n            args = (<zmq.sugar.socket.Socket object>, 5)\r\n            kwargs = {}\r\n        276             finally:\r\n        277                 _state.contexts = current_state\r\n        278         null_wrapper._wrapped = True\r\n        279         return null_wrapper\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\r\n        428             # dispatch events:\r\n        429             if events & IOLoop.ERROR:\r\n        430                 gen_log.error(""got POLLERR event on ZMQStream, which doesn\'t make sense"")\r\n        431                 return\r\n        432             if events & IOLoop.READ:\r\n    --> 433                 self._handle_recv()\r\n            self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\r\n        434                 if not self.socket:\r\n        435                     return\r\n        436             if events & IOLoop.WRITE:\r\n        437                 self._handle_send()\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\r\n        460                 gen_log.error(""RECV Error: %s""%zmq.strerror(e.errno))\r\n        461         else:\r\n        462             if self._recv_callback:\r\n        463                 callback = self._recv_callback\r\n        464                 # self._recv_callback = None\r\n    --> 465                 self._run_callback(callback, msg)\r\n            self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\r\n            callback = <function null_wrapper>\r\n            msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\r\n    466\r\n        467         # self.update_state()\r\n    468\r\n    469\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\r\n        402         close our socket.""""""\r\n        403         try:\r\n        404             # Use a NullContext to ensure that all StackContexts are run\r\n        405             # inside our blanket exception handler rather than outside.\r\n        406             with stack_context.NullContext():\r\n    --> 407                 callback(*args, **kwargs)\r\n            callback = <function null_wrapper>\r\n            args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\r\n            kwargs = {}\r\n        408         except:\r\n        409             gen_log.error(""Uncaught exception, closing connection."",\r\n        410                           exc_info=True)\r\n        411             # Close the socket on an uncaught exception from a user callback\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\r\n        270         # Fast path when there are no active contexts.\r\n        271         def null_wrapper(*args, **kwargs):\r\n        272             try:\r\n        273                 current_state = _state.contexts\r\n        274                 _state.contexts = cap_contexts[0]\r\n    --> 275                 return fn(*args, **kwargs)\r\n            args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\r\n            kwargs = {}\r\n        276             finally:\r\n        277                 _state.contexts = current_state\r\n        278         null_wrapper._wrapped = True\r\n        279         return null_wrapper\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\r\n        247         if self.control_stream:\r\n        248             self.control_stream.on_recv(self.dispatch_control, copy=False)\r\n    249\r\n        250         def make_dispatcher(stream):\r\n        251             def dispatcher(msg):\r\n    --> 252                 return self.dispatch_shell(stream, msg)\r\n            msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\r\n        253             return dispatcher\r\n    254\r\n        255         for s in self.shell_streams:\r\n        256             s.on_recv(make_dispatcher(s), copy=False)\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py in dispatch_shell(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={\'buffers\': [], \'content\': {u\'allow_stdin\': True, u\'code\': u""rfc = RandomForestClassifier(random_state=1)\\n...auc\', cv=3, n_jobs=-1, verbose=2)\\nclf.fit(X, y)"", u\'silent\': False, u\'stop_on_error\': True, u\'store_history\': True, u\'user_expressions\': {}}, \'header\': {u\'msg_id\': u\'122E08A036D8493A821D16CB2F4EE32A\', u\'msg_type\': u\'execute_request\', u\'session\': u\'D2F6FC266E4F453AB36721F7AF6CAD31\', u\'username\': u\'username\', u\'version\': u\'5.0\'}, \'metadata\': {}, \'msg_id\': u\'122E08A036D8493A821D16CB2F4EE32A\', \'msg_type\': u\'execute_request\', \'parent_header\': {}})\r\n        208         else:\r\n        209             # ensure default_int_handler during handler call\r\n        210             sig = signal(SIGINT, default_int_handler)\r\n        211             self.log.debug(""%s: %s"", msg_type, msg)\r\n        212             try:\r\n    --> 213                 handler(stream, idents, msg)\r\n            handler = <bound method IPythonKernel.execute_request of <IPython.kernel.zmq.ipkernel.IPythonKernel object>>\r\n            stream = <zmq.eventloop.zmqstream.ZMQStream object>\r\n            idents = [\'D2F6FC266E4F453AB36721F7AF6CAD31\']\r\n            msg = {\'buffers\': [], \'content\': {u\'allow_stdin\': True, u\'code\': u""rfc = RandomForestClassifier(random_state=1)\\n...auc\', cv=3, n_jobs=-1, verbose=2)\\nclf.fit(X, y)"", u\'silent\': False, u\'stop_on_error\': True, u\'store_history\': True, u\'user_expressions\': {}}, \'header\': {u\'msg_id\': u\'122E08A036D8493A821D16CB2F4EE32A\', u\'msg_type\': u\'execute_request\', u\'session\': u\'D2F6FC266E4F453AB36721F7AF6CAD31\', u\'username\': u\'username\', u\'version\': u\'5.0\'}, \'metadata\': {}, \'msg_id\': u\'122E08A036D8493A821D16CB2F4EE32A\', \'msg_type\': u\'execute_request\', \'parent_header\': {}}\r\n        214             except Exception:\r\n        215                 self.log.error(""Exception in message handler:"", exc_info=True)\r\n        216             finally:\r\n        217                 signal(SIGINT, sig)\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py in execute_request(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[\'D2F6FC266E4F453AB36721F7AF6CAD31\'], parent={\'buffers\': [], \'content\': {u\'allow_stdin\': True, u\'code\': u""rfc = RandomForestClassifier(random_state=1)\\n...auc\', cv=3, n_jobs=-1, verbose=2)\\nclf.fit(X, y)"", u\'silent\': False, u\'stop_on_error\': True, u\'store_history\': True, u\'user_expressions\': {}}, \'header\': {u\'msg_id\': u\'122E08A036D8493A821D16CB2F4EE32A\', u\'msg_type\': u\'execute_request\', u\'session\': u\'D2F6FC266E4F453AB36721F7AF6CAD31\', u\'username\': u\'username\', u\'version\': u\'5.0\'}, \'metadata\': {}, \'msg_id\': u\'122E08A036D8493A821D16CB2F4EE32A\', \'msg_type\': u\'execute_request\', \'parent_header\': {}})\r\n        357         if not silent:\r\n        358             self.execution_count += 1\r\n        359             self._publish_execute_input(code, parent, self.execution_count)\r\n    360\r\n        361         reply_content = self.do_execute(code, silent, store_history,\r\n    --> 362                                         user_expressions, allow_stdin)\r\n            user_expressions = {}\r\n            allow_stdin = True\r\n    363\r\n        364         # Flush output before sending the reply.\r\n        365         sys.stdout.flush()\r\n        366         sys.stderr.flush()\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py in do_execute(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, code=u""rfc = RandomForestClassifier(random_state=1)\\n...auc\', cv=3, n_jobs=-1, verbose=2)\\nclf.fit(X, y)"", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\r\n    176\r\n        177         reply_content = {}\r\n        178         # FIXME: the shell calls the exception handler itself.\r\n        179         shell._reply_content = None\r\n        180         try:\r\n    --> 181             shell.run_cell(code, store_history=store_history, silent=silent)\r\n            shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\r\n            code = u""rfc = RandomForestClassifier(random_state=1)\\n...auc\', cv=3, n_jobs=-1, verbose=2)\\nclf.fit(X, y)""\r\n            store_history = True\r\n            silent = False\r\n        182         except:\r\n        183             status = u\'error\'\r\n        184             # FIXME: this code right now isn\'t being used yet by default,\r\n        185             # because the run_cell() call above directly fires off exception\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, raw_cell=u""rfc = RandomForestClassifier(random_state=1)\\n...auc\', cv=3, n_jobs=-1, verbose=2)\\nclf.fit(X, y)"", store_history=True, silent=False, shell_futures=True)\r\n       2866                 self.displayhook.exec_result = result\r\n    2867\r\n       2868                 # Execute the user code\r\n       2869                 interactivity = ""none"" if silent else self.ast_node_interactivity\r\n       2870                 self.run_ast_nodes(code_ast.body, cell_name,\r\n    -> 2871                    interactivity=interactivity, compiler=compiler, result=result)\r\n            interactivity = \'last_expr\'\r\n            compiler = <IPython.core.compilerop.CachingCompiler instance>\r\n    2872\r\n       2873                 # Reset this so later displayed values do not modify the\r\n       2874                 # ExecutionResult\r\n       2875                 self.displayhook.exec_result = None\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name=\'<ipython-input-25-99a52d0f19db>\', interactivity=\'last\', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\r\n       2976                     return True\r\n    2977\r\n       2978             for i, node in enumerate(to_run_interactive):\r\n       2979                 mod = ast.Interactive([node])\r\n       2980                 code = compiler(mod, cell_name, ""single"")\r\n    -> 2981                 if self.run_code(code, result):\r\n            self.run_code = <bound method ZMQInteractiveShell.run_code of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\r\n            code = <code object <module> at 0x7f8c215a43b0, file ""<ipython-input-25-99a52d0f19db>"", line 6>\r\n            result = <IPython.core.interactiveshell.ExecutionResult object>\r\n       2982                     return True\r\n    2983\r\n       2984             # Flush softspace\r\n       2985             if softspace(sys.stdout, 0):\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f8c215a43b0, file ""<ipython-input-25-99a52d0f19db>"", line 6>, result=<IPython.core.interactiveshell.ExecutionResult object>)\r\n       3030         outflag = 1  # happens in more places, so it\'s easier as default\r\n       3031         try:\r\n       3032             try:\r\n       3033                 self.hooks.pre_run_code_hook()\r\n       3034                 #rprint(\'Running code\', repr(code_obj)) # dbg\r\n    -> 3035                 exec(code_obj, self.user_global_ns, self.user_ns)\r\n            code_obj = <code object <module> at 0x7f8c215a43b0, file ""<ipython-input-25-99a52d0f19db>"", line 6>\r\n            self.user_global_ns = {\'ALLOW_THREADS\': 1, \'Annotation\': <class \'matplotlib.text.Annotation\'>, \'Arrow\': <class \'matplotlib.patches.Arrow\'>, \'Artist\': <class \'matplotlib.artist.Artist\'>, \'AutoLocator\': <class \'matplotlib.ticker.AutoLocator\'>, \'Axes\': <class \'matplotlib.axes._axes.Axes\'>, \'BUFSIZE\': 8192, \'Button\': <class \'matplotlib.widgets.Button\'>, \'CLIP\': 0, \'Circle\': <class \'matplotlib.patches.Circle\'>, ...}\r\n            self.user_ns = {\'ALLOW_THREADS\': 1, \'Annotation\': <class \'matplotlib.text.Annotation\'>, \'Arrow\': <class \'matplotlib.patches.Arrow\'>, \'Artist\': <class \'matplotlib.artist.Artist\'>, \'AutoLocator\': <class \'matplotlib.ticker.AutoLocator\'>, \'Axes\': <class \'matplotlib.axes._axes.Axes\'>, \'BUFSIZE\': 8192, \'Button\': <class \'matplotlib.widgets.Button\'>, \'CLIP\': 0, \'Circle\': <class \'matplotlib.patches.Circle\'>, ...}\r\n       3036             finally:\r\n       3037                 # Reset our crash handler in place\r\n       3038                 sys.excepthook = old_excepthook\r\n       3039         except SystemExit as e:\r\n    \r\n    ...........................................................................\r\n    /home/user/data-analysis/GA/project/final/<ipython-input-25-99a52d0f19db> in <module>()\r\n          1 rfc = RandomForestClassifier(random_state=1)\r\n          2 tuned_parameters = [{\'max_features\': [\'sqrt\', \'log2\'], \r\n          3                      \'n_estimators\': [20, 100, 200, 500, 1000]}]\r\n    4\r\n          5 clf = GridSearchCV(rfc, tuned_parameters, scoring=\'roc_auc\', cv=3, n_jobs=-1, verbose=2)\r\n    ----> 6 clf.fit(X, y)\r\n    7\r\n    8\r\n    9\r\n    10\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=3, error_score=\'raise\',\r\n           e...e_func=None,\r\n           scoring=\'roc_auc\', verbose=2), X=      1516 Quinoa Weisse  18th Street / Against ...0  0.000000      0  \r\n    \r\n    [20000 rows x 5814 columns], y=0       False\r\n    0        True\r\n    1       False\r\n    1     ...lse\r\n    9999     True\r\n    Name: is_five_star, dtype: bool)\r\n        727         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\r\n        728             Target relative to X for classification or regression;\r\n        729             None for unsupervised learning.\r\n    730\r\n        731         """"""\r\n    --> 732         return self._fit(X, y, ParameterGrid(self.param_grid))\r\n            self._fit = <bound method GridSearchCV._fit of GridSearchCV(..._func=None,\r\n           scoring=\'roc_auc\', verbose=2)>\r\n            X =       1516 Quinoa Weisse  18th Street / Against ...0  0.000000      0  \r\n    \r\n    [20000 rows x 5814 columns]\r\n            y = 0       False\r\n    0        True\r\n    1       False\r\n    1     ...lse\r\n    9999     True\r\n    Name: is_five_star, dtype: bool\r\n            self.param_grid = [{\'max_features\': [\'sqrt\', \'log2\'], \'n_estimators\': [20, 100, 200, 500, 1000]}]\r\n    733\r\n    734\r\n        735 class RandomizedSearchCV(BaseSearchCV):\r\n        736     """"""Randomized search on hyper parameters.\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=3, error_score=\'raise\',\r\n           e...e_func=None,\r\n           scoring=\'roc_auc\', verbose=2), X=      1516 Quinoa Weisse  18th Street / Against ...0  0.000000      0  \r\n    \r\n    [20000 rows x 5814 columns], y=0       False\r\n    0        True\r\n    1       False\r\n    1     ...lse\r\n    9999     True\r\n    Name: is_five_star, dtype: bool, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\r\n        500         )(\r\n        501             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\r\n        502                                     train, test, self.verbose, parameters,\r\n        503                                     self.fit_params, return_parameters=True,\r\n        504                                     error_score=self.error_score)\r\n    --> 505                 for parameters in parameter_iterable\r\n            parameters = undefined\r\n            parameter_iterable = <sklearn.grid_search.ParameterGrid object>\r\n        506                 for train, test in cv)\r\n    507\r\n        508         # Out is a list of triplet: score, estimator, n_test_samples\r\n        509         n_fits = len(out)\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<itertools.islice object>)\r\n        661             if pre_dispatch == ""all"" or n_jobs == 1:\r\n        662                 # The iterable was consumed all at once by the above for loop.\r\n        663                 # No need to wait for async callbacks to trigger to\r\n        664                 # consumption.\r\n        665                 self._iterating = False\r\n    --> 666             self.retrieve()\r\n            self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\r\n        667             # Make sure that we get a last message telling us we are done\r\n        668             elapsed_time = time.time() - self._start_time\r\n        669             self._print(\'Done %3i out of %3i | elapsed: %s finished\',\r\n        670                         (len(self._output),\r\n    \r\n        ---------------------------------------------------------------------------\r\n        Sub-process traceback:\r\n        ---------------------------------------------------------------------------\r\n        ValueError                                         Fri Apr 17 23:31:01 2015\r\n    PID: 25128                    Python 2.7.9: /home/user/anaconda/bin/python\r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...lse, random_state=1, verbose=0, warm_start=False), X=<class \'pandas.core.frame.DataFrame\'> instance, y=0       False\r\n    0        True\r\n    1       False\r\n    1     ...lse\r\n    9999     True\r\n    Name: is_five_star, dtype: bool, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]), test=array([   0,    1,    2, ..., 6723, 6725, 6729]), verbose=2, parameters={\'max_features\': \'sqrt\', \'n_estimators\': 20}, fit_params={}, return_train_score=False, return_parameters=True, error_score=\'raise\')\r\n       1447     if parameters is not None:\r\n       1448         estimator.set_params(**parameters)\r\n    1449\r\n       1450     start_time = time.time()\r\n    1451\r\n    -> 1452     X_train, y_train = _safe_split(estimator, X, y, train)\r\n       1453     X_test, y_test = _safe_split(estimator, X, y, test, train)\r\n    1454\r\n       1455     try:\r\n       1456         if y_train is None:\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _safe_split(estimator=RandomForestClassifier(bootstrap=True, class_wei...lse, random_state=1, verbose=0, warm_start=False), X=<class \'pandas.core.frame.DataFrame\'> instance, y=0       False\r\n    0        True\r\n    1       False\r\n    1     ...lse\r\n    9999     True\r\n    Name: is_five_star, dtype: bool, indices=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]), train_indices=None)\r\n       1514             if train_indices is None:\r\n       1515                 X_subset = X[np.ix_(indices, indices)]\r\n       1516             else:\r\n       1517                 X_subset = X[np.ix_(indices, train_indices)]\r\n       1518         else:\r\n    -> 1519             X_subset = safe_indexing(X, indices)\r\n    1520\r\n       1521     if y is not None:\r\n       1522         y_subset = safe_indexing(y, indices)\r\n       1523     else:\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/sklearn/utils/__init__.pyc in safe_indexing(X=<class \'pandas.core.frame.DataFrame\'> instance, indices=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]))\r\n        147     indices : array-like, list\r\n        148         Indices according to which X will be subsampled.\r\n        149     """"""\r\n        150     if hasattr(X, ""iloc""):\r\n        151         # Pandas Dataframes and Series\r\n    --> 152         return X.iloc[indices]\r\n        153     elif hasattr(X, ""shape""):\r\n        154         if hasattr(X, \'take\') and (hasattr(indices, \'dtype\') and\r\n        155                                    indices.dtype.kind == \'i\'):\r\n        156             # This is often substantially faster than X[indices]\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc in __getitem__(self=<pandas.core.indexing._iLocIndexer object>, key=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]))\r\n    1212\r\n       1213     def __getitem__(self, key):\r\n       1214         if type(key) is tuple:\r\n       1215             return self._getitem_tuple(key)\r\n       1216         else:\r\n    -> 1217             return self._getitem_axis(key, axis=0)\r\n    1218\r\n       1219     def _getitem_axis(self, key, axis=0):\r\n       1220         raise NotImplementedError()\r\n    1221\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc in _getitem_axis(self=<pandas.core.indexing._iLocIndexer object>, key=[6634, 6636, 6638, 6639, 6640, 6642, 6644, 6646, 6648, 6650, 6652, 6653, 6654, 6655, 6656, 6658, 6659, 6660, 6662, 6663, ...], axis=0)\r\n       1503                                     ""non-integer key"")\r\n    1504\r\n       1505                 # validate the location\r\n       1506                 self._is_valid_integer(key, axis)\r\n    1507\r\n    -> 1508             return self._get_loc(key, axis=axis)\r\n    1509\r\n       1510     def _convert_to_indexer(self, obj, axis=0, is_setter=False):\r\n       1511         """""" much simpler as we only have to deal with our valid types """"""\r\n    1512\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc in _get_loc(self=<pandas.core.indexing._iLocIndexer object>, key=[6634, 6636, 6638, 6639, 6640, 6642, 6644, 6646, 6648, 6650, 6652, 6653, 6654, 6655, 6656, 6658, 6659, 6660, 6662, 6663, ...], axis=0)\r\n         87             raise IndexingError(\'no slices here, handle elsewhere\')\r\n    88\r\n         89         return self.obj._xs(label, axis=axis)\r\n    90\r\n         91     def _get_loc(self, key, axis=0):\r\n    ---> 92         return self.obj._ixs(key, axis=axis)\r\n    93\r\n         94     def _slice(self, obj, axis=0, kind=None):\r\n         95         return self.obj._slice(obj, axis=axis, kind=kind)\r\n    96\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc in _ixs(self=<class \'pandas.core.frame.DataFrame\'> instance, i=[6634, 6636, 6638, 6639, 6640, 6642, 6644, 6646, 6648, 6650, 6652, 6653, 6654, 6655, 6656, 6658, 6659, 6660, 6662, 6663, ...], axis=0)\r\n       1709                 return self[i]\r\n       1710             else:\r\n       1711                 label = self.index[i]\r\n       1712                 if isinstance(label, Index):\r\n       1713                     # a location index by definition\r\n    -> 1714                     result = self.take(i, axis=axis)\r\n            i = [6634, 6636, 6638, 6639, 6640, 6642, 6644, 6646, 6648, 6650, 6652, 6653, 6654, 6655, 6656, 6658, 6659, 6660, 6662, 6663, ...]\r\n            values = undefined\r\n            values.DataFrame.stack = undefined\r\n            axis = 0\r\n            axis.Return = undefined\r\n            result = undefined\r\n       1715                     copy=True\r\n       1716                 else:\r\n       1717                     new_values = self._data.fast_xs(i)\r\n    1718\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc in take(self=<class \'pandas.core.frame.DataFrame\'> instance, indices=[6634, 6636, 6638, 6639, 6640, 6642, 6644, 6646, 6648, 6650, 6652, 6653, 6654, 6655, 6656, 6658, 6659, 6660, 6662, 6663, ...], axis=0, convert=True, is_copy=True)\r\n       1346         taken : type of caller\r\n       1347         """"""\r\n    1348\r\n       1349         new_data = self._data.take(indices,\r\n       1350                                    axis=self._get_block_manager_axis(axis),\r\n    -> 1351                                    convert=True, verify=True)\r\n       1352         result = self._constructor(new_data).__finalize__(self)\r\n    1353\r\n       1354         # maybe set copy if we didn\'t actually change the index\r\n       1355         if is_copy:\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc in take(self=BlockManager\r\n    Items: Index([u\'1516 Quinoa Weisse\'...ock: slice(5239, 5240, 1), 1 x 20000, dtype: bool, indexer=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]), axis=1, verify=True, convert=True)\r\n       3264                 raise Exception(\'Indices must be nonzero and less than \'\r\n       3265                                 \'the axis length\')\r\n    3266\r\n       3267         new_labels = self.axes[axis].take(indexer)\r\n       3268         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\r\n    -> 3269                                     axis=axis, allow_dups=True)\r\n    3270\r\n       3271     def merge(self, other, lsuffix=\'\', rsuffix=\'\'):\r\n       3272         if not self._is_indexed_like(other):\r\n       3273             raise AssertionError(\'Must have same axes to merge managers\')\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc in reindex_indexer(self=BlockManager\r\n    Items: Index([u\'1516 Quinoa Weisse\'...ock: slice(5239, 5240, 1), 1 x 20000, dtype: bool, new_axis=Int64Index([3317, 3318, 3319, 3319, 3320, 3321, ...381, 3382, 3382, 3383, 3383, ...], dtype=\'int64\'), indexer=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]), axis=1, fill_value=None, allow_dups=True, copy=True)\r\n       3151                 indexer, fill_tuple=(fill_value,))\r\n       3152         else:\r\n       3153             new_blocks = [blk.take_nd(indexer, axis=axis,\r\n       3154                                       fill_tuple=(fill_value if fill_value is not None else\r\n       3155                                                   blk.fill_value,))\r\n    -> 3156                           for blk in self.blocks]\r\n    3157\r\n       3158         new_axes = list(self.axes)\r\n       3159         new_axes[axis] = new_axis\r\n       3160         return self.__class__(new_blocks, new_axes)\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc in take_nd(self=FloatBlock: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1...6, 97, 98, 99, ...], 5813 x 20000, dtype: float64, indexer=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]), axis=1, new_mgr_locs=None, fill_tuple=(nan,))\r\n        846             new_values = com.take_nd(self.get_values(), indexer, axis=axis,\r\n        847                                      allow_fill=False)\r\n        848         else:\r\n        849             fill_value = fill_tuple[0]\r\n        850             new_values = com.take_nd(self.get_values(), indexer, axis=axis,\r\n    --> 851                                      allow_fill=True, fill_value=fill_value)\r\n    852\r\n        853         if new_mgr_locs is None:\r\n        854             if axis == 0:\r\n        855                 slc = lib.indexer_as_slice(indexer)\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/core/common.pyc in take_nd(arr=memmap([[ 0.        ,  0.        ,  0.        , ...  0.        ,\r\n             0.        ,  0.        ]]), indexer=array([ 6634,  6636,  6638, ..., 19997, 19998, 19999]), axis=1, out=array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n        ...0.],\r\n           [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), fill_value=nan, mask_info=None, allow_fill=True)\r\n    818\r\n        819     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype,\r\n        820                                  axis=axis, mask_info=mask_info)\r\n    821\r\n        822     indexer = _ensure_int64(indexer)\r\n    --> 823     func(arr, indexer, out, fill_value)\r\n    824\r\n        825     if flip_order:\r\n        826         out = out.T\r\n        827     return out\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/algos.so in pandas.algos.take_2d_axis1_float64_float64 (pandas/algos.c:95466)()\r\n    4000\r\n    4001\r\n    4002\r\n    4003\r\n    4004\r\n    -> 4005 \r\n    4006\r\n    4007\r\n    4008\r\n    4009\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/algos.so in View.MemoryView.memoryview_cwrapper (pandas/algos.c:175798)()\r\n    609\r\n    610\r\n    611\r\n    612\r\n    613\r\n    --> 614 \r\n    615\r\n    616\r\n    617\r\n    618\r\n    \r\n    ...........................................................................\r\n    /home/user/anaconda/lib/python2.7/site-packages/pandas/algos.so in View.MemoryView.memoryview.__cinit__ (pandas/algos.c:172387)()\r\n    316\r\n    317\r\n    318\r\n    319\r\n    320\r\n    --> 321 \r\n    322\r\n    323\r\n    324\r\n    325\r\n    \r\n    ValueError: buffer source array is read-only\r\n    ___________________________________________________________________________\r\n\r\npd.show_versions()\r\n\r\n    INSTALLED VERSIONS\r\n    ------------------\r\n    commit: None\r\n    python: 2.7.9.final.0\r\n    python-bits: 64\r\n    OS: Linux\r\n    OS-release: 3.16.0-34-generic\r\n    machine: x86_64\r\n    processor: x86_64\r\n    byteorder: little\r\n    LC_ALL: None\r\n    LANG: en_US.UTF-8\r\n    \r\n    pandas: 0.16.0\r\n    nose: 1.3.6\r\n    Cython: 0.22\r\n    numpy: 1.9.2\r\n    scipy: 0.15.1\r\n    statsmodels: 0.6.1\r\n    IPython: 3.1.0\r\n    sphinx: 1.2.3\r\n    patsy: 0.3.0\r\n    dateutil: 2.4.2\r\n    pytz: 2015.2\r\n    bottleneck: None\r\n    tables: 3.1.1\r\n    numexpr: 2.3.1\r\n    matplotlib: 1.4.3\r\n    openpyxl: 2.0.2\r\n    xlrd: 0.9.3\r\n    xlwt: 0.7.5\r\n    xlsxwriter: 0.7.2\r\n    lxml: 3.4.2\r\n    bs4: 4.3.2\r\n    html5lib: None\r\n    httplib2: None\r\n    apiclient: None\r\n    sqlalchemy: 0.9.9\r\n    pymysql: None\r\n    psycopg2: 2.6 (dt dec pq3 ext)'"
joblib/joblib#168,43033387,ogrisel,,2014-09-17 16:27:16,2014-09-17 17:04:39,None,open,,,1,Bug,https://api.github.com/repos/joblib/joblib/issues/168,b'Detect when /dev/shm is size limited',"b""`/dev/shm` is limited to a small size (64MB) by default in docker: https://github.com/docker/docker/pull/4981\r\n\r\nThis is problematic as joblib is using `/dev/shm` when available for temporary memory mapped data in `joblib.Parallel`.\r\n\r\nPossible solutions:\r\n\r\n- stop using `/dev/shm` by default (use the HDD backed '/tmp' folder instead): but first we would we need to check that it's not too size limited either in docker with the default settings\r\n- find a way to detect the size of `/dev/shm` and only use it when large enough\r\n- find a way to catch exceptions caused by small `/dev/shm` and raise an exception with a more informative error message instead (suggesting to set the `JOBLIB_TEMP_FOLDER` to another folder)."""
joblib/joblib#198,63308015,dave1g,,2015-03-20 21:08:21,2015-03-26 09:49:36,None,open,,,2,Bug,https://api.github.com/repos/joblib/joblib/issues/198,b'joblib files not cleaned up from /dev/shm',"b'I incorrectly reported this to scikit https://github.com/scikit-learn/scikit-learn/issues/4430 first as that was the code I was most directly making use of.\r\n\r\nBasically I kicked of a multiprocesses script that ended up copying lots of data around, that data eventually filled up /dev/shm and no further copying was possible, the python processes locked up. I killed them, attempted to restart and /dev/shm was left in a bad state with all these large files sitting there (Is it possible to make sure they don\'t live longer than the process that created them?)\r\n\r\n```\r\n$ ls -lrt /dev/shm/joblib\r\n/dev/shm/joblib_memmaping_pool_64060_189405392:\r\ntotal 13528188\r\n-rw------- 1 general 13852862624 Mar 20 09:09 64060-190150864-190138048-0.pkl_01.npy\r\n-rw------- 1 general 152 Mar 20 09:09 64060-190150864-190138048-0.pkl\r\n```\r\n\r\n4 or 5 similar files/directories were present\r\n\r\n\r\nIf nothing can be done about this situation. It would at least be nice for the exception that gets thrown to be more informative.\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile "".../python/linux/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 240, in save\r\nobj, filename = self._write_array(obj, filename)\r\nFile "".../python/linux/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 203, in _write_array\r\nself.np.save(filename, array)\r\nFile "".../python/linux/lib/python2.6/site-packages/numpy/lib/npyio.py"", line 453, in save\r\nformat.write_array(fid, arr)\r\nFile "".../python2.6/site-packages/numpy/lib/format.py"", line 521, in write_array\r\narray.tofile(fp)\r\nIOError: 1731607818 requested and 502 written\r\n```\r\n\r\n'"
numpy/numpy#5696,63010876,amueller,charris,2015-03-19 15:06:41,2015-03-21 02:04:23,2015-03-21 02:04:23,closed,,,14,,https://api.github.com/repos/numpy/numpy/issues/5696,b'Mean of dtype object array works sometimes',"b""Hi.\r\nWe ran into this at scikit-learn and it is more of an integration/testing issue than anything else.\r\nIt boils down to this expression:\r\n```python\r\nnp.arange(10).astype(object).mean(axis=0)\r\n```\r\nwhich either produces ``4.5`` or raises an error. I'd like to know what causes the error, in which version it is, and how to robustly test for it.\r\nMaybe I'm overlooking something, but testing it on several platforms, what I get is this:\r\n\r\n```\r\nUbuntu 12.04 + Py 2.7.3 + np 1.6.1 == Pass\r\nUbuntu 12.04 + Py 2.6.9 + np 1.6.2 == Pass\r\nUbuntu 12.04 + Py 3.4.3 + np 1.9.2 == Pass\r\n\r\nUbuntu 14.04 + Py 2.7.9 + np 1.8.0 == Fail\r\nUbuntu 14.04 + Py 2.7.9 + np 1.9.2 == Pass\r\n\r\nOSX Yosemite + Py 2.7.6 + np 1.9.0 == Fail\r\nOSX Yosemite + Py 2.7.6 + np 1.9.1 == Fail\r\nOSX Yosemite + Py 2.7.6 + np 1.9.2 == Fail\r\nOSX Yosemite + Py 2.7.9 + np 1.9.2 == Pass\r\n\r\nUbuntu 14.04 + Py 2.7.3 + np 1.6.1 == Pass\r\nUbuntu 14.04 + Py 2.7.6 + np 1.8.0 == Fail\r\nUbuntu 14.04 + Py 2.7.6 + np 1.9.0 == Pass\r\n```\r\nSo even for the same Python and numpy version, the behavior is different between Ubuntu and OS X.\r\n\r\nAny help would be much appreciated.\r\ncc @trevorstephens"""
,,,,,,,,,,,,,,
joblib/joblib#57,10451843,lsartran,,2013-01-30 09:30:22,2015-01-20 18:01:46,None,open,,,1,,https://api.github.com/repos/joblib/joblib/issues/57,b'Filesystem compression breaks joblib.test.test_disk.test_disk_used',"b'Hello, I am getting the following error when running nosetests on the latest version (latest commit dated Jan 15th.)\r\n\r\n`````\r\n======================================================================\r\nFAIL: joblib.test.test_disk.test_disk_used\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/site-packages/nose-1.2.1-py2.7.egg/nose/case.py"", line 197, in runTest\r\n    self.test(*self.arg)\r\n  File ""/usr/home/ls/software/joblib/joblib/test/test_disk.py"", line 40, in test_disk_used\r\n    nose.tools.assert_true(disk_used(cachedir) >= target_size)\r\nAssertionError: False is not true\r\n    \'False is not true\' = self._formatMessage(\'False is not true\', ""%s is not true"" % safe_repr(False))\r\n>>  raise self.failureException(\'False is not true\')\r\n    \r\n\r\n----------------------------------------------------------------------\r\nRan 299 tests in 2.890s\r\n\r\nFAILED (failures=1)\r\n````\r\n/tmp is in a ZFS volume on FreeBSD with on-the-fly transparent compression enabled, so disk_used can be smaller than target_size. It indeed is:\r\n\r\n```\r\n[10:14:30] [ls@coyote /tmp]$ dd if=/dev/zero of=test.1M bs=1M count=1\r\n1+0 records in\r\n1+0 records out\r\n1048576 bytes transferred in 0.000609 secs (1721348928 bytes/sec)\r\n[10:14:36] [ls@coyote /tmp]$ ls -al test.1M\r\n-rw-r--r--  1 ls  wheel  1048576 Jan 30 10:14 test.1M\r\n[10:14:39] [ls@coyote /tmp]$ du -k test.1M\r\n1\ttest.1M\r\n[10:14:50] [ls@coyote /tmp]$ du -Ak test.1M\r\n1024\ttest.1M\r\n```\r\n\r\nI\'m unsure about what to suggest, as I don\'t know what the authors meant to check here:\r\n* to get the apparent size of a file, using stat.st_size is the way to go\r\n* however, to get its actual disk usage (to impose a limit upon how much space the cache can use, for instance) stat.st_blocks is fine, but the assumption test_disk_used makes doesn\'t hold.'"
scipy/scipy#3025,21692957,bretter,pv,2013-10-28 14:36:41,2013-11-16 18:28:34,2013-11-09 14:10:43,closed,,0.13.1,7,defect;scipy.ndimage,https://api.github.com/repos/scipy/scipy/issues/3025,b'scipy.ndimage.label() broken in scipy 0.13',"b'I am using Python 3.3.2 on 64-bit Arch Linux.\r\n\r\nI have a script that skeletonizes an image and then i run:\r\nndimage.label(skeletonized_image, np.ones((3, 3)))\r\n\r\nWhen using scipy 0.12, label counts about 30 seperate objects which are clearly disconnected.\r\n\r\nWhen using scipy 0.13, I label generates about twice the amount of objects many of which are clearly connected.\r\n\r\ni.e.\r\n[0, 0, 0, 0;\r\n 1, 1, 2, 2;\r\n 0, 0, 0, 0]\r\n\r\nFor now downgrading to scipy 0.12 has solved the problem.'"
numpy/numpy#5456,54538888,MechCoder,MechCoder,2015-01-16 04:31:41,2015-01-16 18:13:19,2015-01-16 18:13:19,closed,,,4,,https://api.github.com/repos/numpy/numpy/issues/5456,b'BUG? : Inconsistency in slicing of object arrays',"b'I am not sure again if this is the expected behavior or not.\r\n\r\n    In [16]: x1 = [[1], [2, 3]]\r\n    In [17]: b = np.array(x1, dtype=object)\r\n    In [18]: x = np.random.rand(5)\r\n    In [19]: x[b[1]]\r\n    Out[19]: array([ 0.65138485,  0.52464837])\r\n    In [20]: x2 = [[1], [2]]\r\n    In [21]: b = np.array(x2, dtype=object)\r\n    In [22]: x[b[0]]\r\n    ---------------------------------------------------------------------------\r\n    IndexError                                Traceback (most recent call last)\r\n    <ipython-input-22-89c92d26bcc0> in <module>()\r\n    ----> 1 x[b[0]]\r\n\r\n    IndexError: arrays used as indices must be of integer (or boolean) type\r\n    # Should this be a list?\r\n    In [23]: b[0]\r\n    Out[23]: array([1], dtype=object)\r\n\r\n'"
materialsinnovation/pymks#132,52385179,wd15,davidbrough1,2014-12-18 16:26:28,2015-01-12 17:45:39,2015-01-12 17:45:39,closed,,version 0.2,3,bug,https://api.github.com/repos/materialsinnovation/pymks/issues/132,b'GridSearchCV no longer accepting multidimensional features',"b""See GridSearchCV in http://openmaterials.github.io/pymks/rst/cahn_hilliard_Legendre.html as an example of where this is broken. The solution is to subclass GridSearchCV and flatten the array and then wrap the model class dynamically in a new class that unwrap the flatten data passed through GridSearchCV's fit method."""
sphinx-gallery/sphinx-gallery#18,57827251,Titan-C,lesteve,2015-02-16 16:49:58,2016-03-17 15:31:09,2016-03-17 15:31:09,closed,,,0,,https://api.github.com/repos/sphinx-gallery/sphinx-gallery/issues/18,b'Unicode support',"b""I came to notice that if the examples docstring have an unicode character, sphinx-gallery fails in python2. It is not the case if the unicode characters are outside the main docstring, because the encoding header of the file makes the scripts work. The reason the files having my foreign accented name still work.\r\n\r\nthis PR https://github.com/scikit-learn/scikit-learn/pull/3777 didn't help with my issue."""
scipy/scipy#2481,14215555,WarrenWeckesser,rgommers,2013-05-11 01:07:38,2014-04-16 20:50:19,2014-04-16 20:50:19,closed,,0.14.0,2,defect;prio-normal;scipy.sparse,https://api.github.com/repos/scipy/scipy/issues/2481,b'BUG: sparse: todense() fails for coo matrix with dtype np.float16',"b'Example:\r\n\r\n```\r\n>>> coo_matrix([[0]], dtype=np.float16).todense()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/warren/local_scipy/lib/python2.7/site-packages/scipy/sparse/base.py"", line 515, in todense\r\n    return np.asmatrix(self.toarray(order=order, out=out))\r\n  File ""/home/warren/local_scipy/lib/python2.7/site-packages/scipy/sparse/coo.py"", line 244, in toarray\r\n    B.ravel(\'A\'), fortran)\r\n  File ""/home/warren/local_scipy/lib/python2.7/site-packages/scipy/sparse/sparsetools/coo.py"", line 175, in coo_todense\r\n    return _coo.coo_todense(*args)\r\nTypeError: Array of type \'float\' required.  Array of type \'char\' given\r\n```\r\n\r\nThis was reported on stackoverflow: http://stackoverflow.com/questions/16492224/python-typeerror-using-scipys-coo-matrix-todense/'"
joblib/joblib#192,60477162,lesteve,GaelVaroquaux,2015-03-10 10:16:09,2015-03-11 07:45:30,2015-03-10 15:13:37,closed,,,10,,https://api.github.com/repos/joblib/joblib/issues/192,b'Fix python 2 and 3 compatibility problems with compressed pickles',b'From https://github.com/scikit-learn/scikit-learn/issues/3596.'
mne-tools/mne-python#1401,37278399,dengemann,dengemann,2014-07-07 15:17:01,2014-08-05 11:00:00,2014-07-07 20:25:13,closed,,,4,,https://api.github.com/repos/mne-tools/mne-python/issues/1401,b'BUG: errors in sklearn CSP',"b'```Python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/Users/dengemann/github/mne-python/examples/decoding/plot_decoding_csp_eeg.py in <module>()\r\n     93 from sklearn.cross_validation import cross_val_score\r\n     94 clf = Pipeline([(\'CSP\', csp), (\'SVC\', svc)])\r\n---> 95 scores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\r\n     96\r\n     97 # Printing the results\r\n\r\n/Users/dengemann/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, score_func, pre_dispatch)\r\n   1098         Array of scores of the estimator for each run of the cross validation.\r\n   1099     """"""\r\n-> 1100     X, y = check_arrays(X, y, sparse_format=\'csr\', allow_lists=True)\r\n   1101     if y is not None:\r\n   1102         y = np.asarray(y)\r\n\r\n/Users/dengemann/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_arrays(*arrays, **options)\r\n    253             if array.ndim >= 3:\r\n    254                 raise ValueError(""Found array with dim %d. Expected <= 2"" %\r\n--> 255                                  array.ndim)\r\n    256\r\n    257         if copy and array is array_orig:\r\n\r\nValueError: Found array with dim 3. Expected <= 2\r\n```\r\n@kazemakase @agramfort '"
,,,,,,,,,,,,,,
scipy/scipy#4142,48631914,ogrisel,stefanv,2014-11-13 12:45:14,2014-11-14 09:44:21,2014-11-13 22:12:31,closed,,,4,,https://api.github.com/repos/scipy/scipy/issues/4142,b'BUG: ZeroDivisionError in scipy.sparse.linalg.lsqr',"b""This problem was found as an unstable test in scikit-learn: scikit-learn/scikit-learn#2986.\r\n\r\nThis PR includes a non regression test and a potential fix (adding eps's in the denominators of fractions computed for convergence checks). I don't understand the overall algorithm but I think that those eps should be pretty harmless as they only impact quantities computed for breaking out of the loop, not the solution itself."""
numpy/numpy#4422,28588916,jnothman,,2014-03-02 21:30:50,2015-10-30 02:46:51,None,open,,,3,11 - Bug;component: numpy.ma;priority: normal,https://api.github.com/repos/numpy/numpy/issues/4422,"b'ma.sort(..., endwith=True) broken with nans'","b""A masked array containing `nan` currently sorts such that `nan`s appear at the end of the array, violating `endwith=True`:\r\n```python\r\n>>> m = np.ma.masked_array([np.nan, -1], mask=[0, 1])\r\n>>> m\r\nmasked_array(data = [nan --],\r\n             mask = [False  True],\r\n       fill_value = 1e+20)\r\n>>> m.sort()\r\n>>> m\r\nmasked_array(data = [-- nan],\r\n             mask = [ True False],\r\n       fill_value = 1e+20)\r\n```\r\n\r\nApart from its own contract, this breaks consistency within the current implementation of `np.ma.median` (which should probably not rely on `sort` anyway), returning:\r\n```python\r\n>>> np.ma.median(np.ma.masked_array([np.nan, 5, -1], mask=[0, 0, 1]))\r\n5.0\r\n>>> np.ma.median(np.ma.masked_array([np.nan, 5], mask=[0, 0]))\r\nnan\r\n```\r\n(not that I'm sure what it means to take a median when there are `nan`s present)\r\n\r\nPerhaps `minimum_fill_value` needs an alternative for the `sort` case because of the convention that `nan` comes last."""
joblib/joblib#103,25035638,venuktan,,2014-01-03 21:02:23,2014-04-25 20:42:00,None,open,,,1,,https://api.github.com/repos/joblib/joblib/issues/103,b'sklearn model write to disk and read back from disk using joblib',"b'I am building my model and storing it at a location using \r\n`joblib.dump(model, modelStoreLocation, compress=9)`\r\n\r\nThen in a separate execution I am trying to read the model from the location as show below : \r\n`# loading the model stored on disk\r\nmodel = joblib.load(modelStoreLocation)`\r\n\r\nI get the following error : \r\n```\r\nTraceback (most recent call last):\r\nFile ""/Users/venuktangirala/PycharmProjects/nFlate/compute/prediction/categories/logisticRegression.py"", line 84, in <module>x\r\nprediction = predictDataFrame(dataFrame.ix[600:623, :-1].values, modelStoreLocation)\r\nFile ""/Users/venuktangirala/PycharmProjects/nFlate/compute/prediction/categories/logisticRegression.py"", line 58, in predictDataFrame\r\nprediction = model.predict(dataToPredict)\r\nFile ""/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.py"", line 223, in predict\r\nscores = self.decision_function(X)\r\nFile ""/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.py"", line 207, in decision_function\r\ndense_output=True) + self.intercept_\r\nFile ""/anaconda/lib/python2.7/site-packages/sklearn/utils/extmath.py"", line 83, in safe_sparse_dot\r\nreturn np.dot(a, b)\r\nTypeError: can\'t multiply sequence by non-int of type \'float\'\r\n```\r\n\r\nThe same works with out any problem when its in the same execution i.e. build the model , write to dsik and read from disk in the same execution.\r\n\r\nWhat is the fix for this ?'"
mne-tools/mne-python#985,24265958,dengemann,dengemann,2013-12-13 19:09:15,2013-12-20 14:27:54,2013-12-20 14:27:54,closed,,,2,,https://api.github.com/repos/mne-tools/mne-python/issues/985,b'BUG: deal with RandomizedPCA sklearn issue in ICA',b'cf. \r\n\r\nhttps://github.com/scikit-learn/scikit-learn/issues/2663#issuecomment-30507653\r\n\r\nas a consequence our explained variance API is wrong :-(\r\n'
iancze/Starfish#26,126809226,gully,gully,2016-01-15 05:16:03,2016-02-18 05:56:48,2016-02-18 05:56:48,closed,,v0.2.0,6,bug,https://api.github.com/repos/iancze/Starfish/issues/26,"b'""leading minor not positive definite"" in Cholesky decomposition for lnprob calculation'","b'Lately I\'ve been hitting a problem in the Cholesky decomposition step during `star.py --sample=ThetaPhi`\r\n\r\n```python\r\n  File ""/anaconda/lib/python3.4/site-packages/emcee/sampler.py"", line 116, in get_lnprob\r\n    return self.lnprobfn(p, *self.args, **self.kwargs)\r\n  File ""/Users/gully/GitHub/Starfish/Starfish/parallel.py"", line 696, in lnfunc\r\n    lnp = self.evaluate()\r\n  File ""/Users/gully/GitHub/Starfish/Starfish/parallel.py"", line 297, in evaluate\r\n    factor, flag = cho_factor(CC)\r\n  File ""/anaconda/lib/python3.4/site-packages/scipy/linalg/decomp_cholesky.py"", line 132, in cho_factor\r\n    check_finite=check_finite)\r\n  File ""/anaconda/lib/python3.4/site-packages/scipy/linalg/decomp_cholesky.py"", line 30, in _cholesky\r\n    raise LinAlgError(""%d-th leading minor not positive definite"" % info)\r\nnumpy.linalg.linalg.LinAlgError: 2-th leading minor not positive definite\r\n```\r\n\r\nAt first I thought I had too large of a covariance kernel and the fits were going crazy.  I looked into it and I\'m guessing it\'s a subtle type of roundoff error, based on [this issue from scikit-learn](https://github.com/scikit-learn/scikit-learn/issues/2640).  The issue and affiliated pull request shows how to work around it, I think.\r\n\r\nThere could be other problems at play, so I am still experimenting.  I\'ll post what I find out.'"
mne-tools/mne-python#3425,165893832,kingjr,,2016-07-15 23:15:08,2016-07-25 22:59:53,None,open,,0.13,44,,https://api.github.com/repos/mne-tools/mne-python/issues/3425,b'MRG: Refactor Xdawn',"b'* Refactor: Xdawn and XdawnTransformer classes, now both in `mne.preprocessing.xdawn`\r\n* FIX: bug weird channels picking in tests and Xdawn class\r\n* FIX: unstable unit test\r\n* FIX: remove undocumented attributes (e.g. `Xdawn.exclude`), add and document necessary ones (`Xdawn.overlap_`, `evokeds_`, `.event_id_`)\r\n* ENH: `_xdawn_fit` private function for more explicit args.\r\n* FIX: `XdawnTransformer` devoid of any fancy objects (Evoked / dictionary etc), and only uses numpy\r\n\r\ncc @kaichogami @alexandrebarachant @dengemann '"
MichelleLochner/supernova-machine#6,58090652,mkerrwinter,,2015-02-18 16:27:51,2015-02-18 16:27:51,None,open,,,0,,https://api.github.com/repos/MichelleLochner/supernova-machine/issues/6,b'Using GridSearch with KNN',"b""Hey, I'm trying to optimise KNN with grid_search, but for some obscure reason grid_search can't handle KNN with the Mahalanobis distance metric. This problem is described here:\r\nhttps://github.com/scikit-learn/scikit-learn/issues/2609\r\nand they say they've fixed it. So how do I get their new code where the problem's fixed? \r\nThanks!"""
numpy/numpy#4012,22110190,ankit-maverick,charris,2013-11-05 10:09:29,2014-02-24 14:48:56,2014-02-24 14:48:56,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/4012,b'Inconsistent behaviour of np.ma.median',"b'The correct mask of the masked_array returned by np.ma.median in the following case should be `[False False]`. Can someone confirm this? This blocks https://github.com/scikit-learn/scikit-learn/issues/2560\r\n\r\n```py\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: a = np.arange(20).reshape(10,2).astype(np.float)\r\n\r\nIn [3]: a[3,1] = np.nan\r\n\r\nIn [4]: a[5,0] = np.nan\r\n\r\nIn [5]: mask = a == np.nan\r\n\r\nIn [6]: masked_a = np.ma.masked_array(a, mask=mask)\r\n\r\nIn [7]: median_masked = np.ma.median(masked_a, axis=0)\r\n\r\nIn [8]: median_masked\r\nOut[8]: \r\nmasked_array(data = [ 10.  12.],\r\n             mask = False,\r\n       fill_value = 1e+20)\r\n\r\nIn [9]: mean_masked = np.ma.mean(masked_a, axis=0)\r\n\r\nIn [10]: mean_masked\r\nOut[10]: \r\nmasked_array(data = [-- --],\r\n             mask = [ True  True],\r\n       fill_value = 1e+20)\r\n```'"
scipy/scipy#2547,15296845,joernhees,rgommers,2013-06-07 23:56:18,2013-09-25 09:43:37,2013-06-10 17:35:37,closed,cournape,0.13.0,34,defect;prio-normal;scipy.sparse.linalg,https://api.github.com/repos/scipy/scipy/issues/2547,b'arpack tests fail when installed on mac os x',"b'I ran into this problem several times already with different installation methods. This time i used brew to install scipy:\r\n```\r\nbrew tap homebrew/science\r\nbrew tap samueljohn/python\r\nbrew update\r\nbrew install python\r\npip install nose\r\nbrew install scipy\r\n```\r\n\r\nIf you then run the tests the following errors occur: https://gist.github.com/joernhees/5733199\r\n\r\nA shortened version here:\r\n\r\n```\r\n$ ipython\r\nPython 2.7.5 (default, Jun 8 2013, 00:27:06)\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n \r\nIPython 0.13.2 -- An enhanced Interactive Python.\r\n? -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp -> Python\'s own help system.\r\nobject? -> Details about \'object\', use \'object??\' for extra details.\r\n \r\nIn [1]: import scipy\r\n \r\nIn [2]: scipy.test()\r\nRunning unit tests for scipy\r\nNumPy version 1.7.1\r\nNumPy is installed in /usr/local/lib/python2.7/site-packages/numpy\r\nSciPy version 0.12.0\r\nSciPy is installed in /usr/local/lib/python2.7/site-packages/scipy\r\nPython version 2.7.5 (default, Jun 8 2013, 00:27:06) [GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\r\nnose version 1.3.0\r\n/usr/local/lib/python2.7/site-packages/numpy/lib/utils.py:139: DeprecationWarning: `scipy.lib.blas` is deprecated, use `scipy.linalg.blas` instead!\r\nwarnings.warn(depdoc, DeprecationWarning)\r\n/usr/local/lib/python2.7/site-packages/numpy/lib/utils.py:139: DeprecationWarning: `scipy.lib.lapack` is deprecated, use `scipy.linalg.lapack` instead!\r\nwarnings.warn(depdoc, DeprecationWarning)\r\n...............................................K...............................................K..K........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................SSSSSS......SSSSSS......S..........K..............................................................................................................................................................................................................................................................................................K...................................................................................................................................................................................................../usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n./usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n./usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................FFF.....FFF..FF......FF..F..........FFF.....FFF..............FF..FF.FFF..FF..FF..FF..F..........FF..FFF.FFF.............F.......F....F.......F..............FFF......FF.............F...F...F....F...F...F..............FFF.FFF.FFF..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................K....................................................................K......................................................................................................................................................................KK.................................................................................................................................................................................................................................................................................................................................................................................................................................K........K.......................................................................................................................................................................................................................S...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <std-symmetric>, \'f\', 2, \'LM\', None, 0.5, <class \'scipy.sparse.csr.csr_matrix\'>, None, \'normal\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.00178814, atol=0.000357628\r\nerror for eigsh:standard, typ=f, which=LM, sigma=0.5, mattype=csr_matrix, OPpart=None, mode=normal\r\n(mismatch 100.0%)\r\nx: array([[ 2.38156418e-01, -6.75444982e+09],\r\n[ -1.07853470e-01, -8.01245676e+09],\r\n[ 1.24683023e-01, -5.19757686e+09],...\r\ny: array([[ 2.38156418e-01, -5.70949789e+08],\r\n[ -1.07853470e-01, -4.05829392e+08],\r\n[ 1.24683023e-01, 6.25800146e+07],...\r\n \r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <std-symmetric>, \'f\', 2, \'LM\', None, 0.5, <class \'scipy.sparse.csr.csr_matrix\'>, None, \'buckling\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.00178814, atol=0.000357628\r\nerror for eigsh:standard, typ=f, which=LM, sigma=0.5, mattype=csr_matrix, OPpart=None, mode=buckling\r\n(mismatch 100.0%)\r\nx: array([[ 3.53755447e-01, -2.29114355e+04],\r\n[ -1.60204595e-01, -6.65625445e+04],\r\n[ 1.85203065e-01, -2.69012500e+04],...\r\ny: array([[ 3.53755447e-01, -8.88255444e+05],\r\n[ -1.60204595e-01, -2.39343354e+06],\r\n[ 1.85203065e-01, -3.96842525e+04],...\r\n \r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <std-symmetric>, \'f\', 2, \'LM\', None, 0.5, <class \'scipy.sparse.csr.csr_matrix\'>, None, \'cayley\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.00178814, atol=0.000357628\r\nerror for eigsh:standard, typ=f, which=LM, sigma=0.5, mattype=csr_matrix, OPpart=None, mode=cayley\r\n(mismatch 100.0%)\r\nx: array([[ -2.38156418e-01, 1.04661597e+09],\r\n[ 1.07853470e-01, 1.39930271e+09],\r\n[ -1.24683023e-01, 9.56906461e+08],...\r\ny: array([[ -2.38156418e-01, 7.63721281e+07],\r\n[ 1.07853470e-01, 1.25169905e+08],\r\n[ -1.24683023e-01, 2.91283130e+07],...\r\n \r\n======================================================================\r\n...\r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <gen-symmetric>, \'d\', 2, \'SA\', None, 0.5, <function asarray at 0x10ad03e60>, None, \'cayley\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=4.44089e-13, atol=4.44089e-13\r\nerror for eigsh:general, typ=d, which=SA, sigma=0.5, mattype=asarray, OPpart=None, mode=cayley\r\n(mismatch 100.0%)\r\nx: array([[-0.36892684, -0.01935691],\r\n[-0.26850996, -0.11053158],\r\n[-0.40976156, -0.13223572],...\r\ny: array([[-0.43633077, -0.01935691],\r\n[-0.25161386, -0.11053158],\r\n[-0.36756684, -0.13223572],...\r\n \r\n----------------------------------------------------------------------\r\nRan 4831 tests in 54.169s\r\n \r\nFAILED (KNOWNFAIL=11, SKIP=14, failures=63)\r\nOut[2]: <nose.result.TextTestResult run=4831 errors=0 failures=63>\r\n \r\nIn [3]:\r\n```'"
scipy/scipy#2547,15296845,joernhees,rgommers,2013-06-07 23:56:18,2013-09-25 09:43:37,2013-06-10 17:35:37,closed,cournape,0.13.0,34,defect;prio-normal;scipy.sparse.linalg,https://api.github.com/repos/scipy/scipy/issues/2547,b'arpack tests fail when installed on mac os x',"b'I ran into this problem several times already with different installation methods. This time i used brew to install scipy:\r\n```\r\nbrew tap homebrew/science\r\nbrew tap samueljohn/python\r\nbrew update\r\nbrew install python\r\npip install nose\r\nbrew install scipy\r\n```\r\n\r\nIf you then run the tests the following errors occur: https://gist.github.com/joernhees/5733199\r\n\r\nA shortened version here:\r\n\r\n```\r\n$ ipython\r\nPython 2.7.5 (default, Jun 8 2013, 00:27:06)\r\nType ""copyright"", ""credits"" or ""license"" for more information.\r\n \r\nIPython 0.13.2 -- An enhanced Interactive Python.\r\n? -> Introduction and overview of IPython\'s features.\r\n%quickref -> Quick reference.\r\nhelp -> Python\'s own help system.\r\nobject? -> Details about \'object\', use \'object??\' for extra details.\r\n \r\nIn [1]: import scipy\r\n \r\nIn [2]: scipy.test()\r\nRunning unit tests for scipy\r\nNumPy version 1.7.1\r\nNumPy is installed in /usr/local/lib/python2.7/site-packages/numpy\r\nSciPy version 0.12.0\r\nSciPy is installed in /usr/local/lib/python2.7/site-packages/scipy\r\nPython version 2.7.5 (default, Jun 8 2013, 00:27:06) [GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\r\nnose version 1.3.0\r\n/usr/local/lib/python2.7/site-packages/numpy/lib/utils.py:139: DeprecationWarning: `scipy.lib.blas` is deprecated, use `scipy.linalg.blas` instead!\r\nwarnings.warn(depdoc, DeprecationWarning)\r\n/usr/local/lib/python2.7/site-packages/numpy/lib/utils.py:139: DeprecationWarning: `scipy.lib.lapack` is deprecated, use `scipy.linalg.lapack` instead!\r\nwarnings.warn(depdoc, DeprecationWarning)\r\n...............................................K...............................................K..K........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................SSSSSS......SSSSSS......S..........K..............................................................................................................................................................................................................................................................................................K...................................................................................................................................................................................................../usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n/usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n./usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n./usr/local/lib/python2.7/site-packages/scipy/misc/pilutil.py:230: DeprecationWarning: fromstring() is deprecated. Please call frombytes() instead.\r\nimage = Image.fromstring(\'L\',shape,bytedata.tostring())\r\n.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................FFF.....FFF..FF......FF..F..........FFF.....FFF..............FF..FF.FFF..FF..FF..FF..F..........FF..FFF.FFF.............F.......F....F.......F..............FFF......FF.............F...F...F....F...F...F..............FFF.FFF.FFF..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................K....................................................................K......................................................................................................................................................................KK.................................................................................................................................................................................................................................................................................................................................................................................................................................K........K.......................................................................................................................................................................................................................S...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <std-symmetric>, \'f\', 2, \'LM\', None, 0.5, <class \'scipy.sparse.csr.csr_matrix\'>, None, \'normal\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.00178814, atol=0.000357628\r\nerror for eigsh:standard, typ=f, which=LM, sigma=0.5, mattype=csr_matrix, OPpart=None, mode=normal\r\n(mismatch 100.0%)\r\nx: array([[ 2.38156418e-01, -6.75444982e+09],\r\n[ -1.07853470e-01, -8.01245676e+09],\r\n[ 1.24683023e-01, -5.19757686e+09],...\r\ny: array([[ 2.38156418e-01, -5.70949789e+08],\r\n[ -1.07853470e-01, -4.05829392e+08],\r\n[ 1.24683023e-01, 6.25800146e+07],...\r\n \r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <std-symmetric>, \'f\', 2, \'LM\', None, 0.5, <class \'scipy.sparse.csr.csr_matrix\'>, None, \'buckling\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.00178814, atol=0.000357628\r\nerror for eigsh:standard, typ=f, which=LM, sigma=0.5, mattype=csr_matrix, OPpart=None, mode=buckling\r\n(mismatch 100.0%)\r\nx: array([[ 3.53755447e-01, -2.29114355e+04],\r\n[ -1.60204595e-01, -6.65625445e+04],\r\n[ 1.85203065e-01, -2.69012500e+04],...\r\ny: array([[ 3.53755447e-01, -8.88255444e+05],\r\n[ -1.60204595e-01, -2.39343354e+06],\r\n[ 1.85203065e-01, -3.96842525e+04],...\r\n \r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <std-symmetric>, \'f\', 2, \'LM\', None, 0.5, <class \'scipy.sparse.csr.csr_matrix\'>, None, \'cayley\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.00178814, atol=0.000357628\r\nerror for eigsh:standard, typ=f, which=LM, sigma=0.5, mattype=csr_matrix, OPpart=None, mode=cayley\r\n(mismatch 100.0%)\r\nx: array([[ -2.38156418e-01, 1.04661597e+09],\r\n[ 1.07853470e-01, 1.39930271e+09],\r\n[ -1.24683023e-01, 9.56906461e+08],...\r\ny: array([[ -2.38156418e-01, 7.63721281e+07],\r\n[ 1.07853470e-01, 1.25169905e+08],\r\n[ -1.24683023e-01, 2.91283130e+07],...\r\n \r\n======================================================================\r\n...\r\n======================================================================\r\nFAIL: test_arpack.test_symmetric_modes(True, <gen-symmetric>, \'d\', 2, \'SA\', None, 0.5, <function asarray at 0x10ad03e60>, None, \'cayley\')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\nFile ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 197, in runTest\r\nself.test(*self.arg)\r\nFile ""/usr/local/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/tests/test_arpack.py"", line 259, in eval_evec\r\nassert_allclose(LHS, RHS, rtol=rtol, atol=atol, err_msg=err)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 1179, in assert_allclose\r\nverbose=verbose, header=header)\r\nFile ""/usr/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 645, in assert_array_compare\r\nraise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=4.44089e-13, atol=4.44089e-13\r\nerror for eigsh:general, typ=d, which=SA, sigma=0.5, mattype=asarray, OPpart=None, mode=cayley\r\n(mismatch 100.0%)\r\nx: array([[-0.36892684, -0.01935691],\r\n[-0.26850996, -0.11053158],\r\n[-0.40976156, -0.13223572],...\r\ny: array([[-0.43633077, -0.01935691],\r\n[-0.25161386, -0.11053158],\r\n[-0.36756684, -0.13223572],...\r\n \r\n----------------------------------------------------------------------\r\nRan 4831 tests in 54.169s\r\n \r\nFAILED (KNOWNFAIL=11, SKIP=14, failures=63)\r\nOut[2]: <nose.result.TextTestResult run=4831 errors=0 failures=63>\r\n \r\nIn [3]:\r\n```'"
,,,,,,,,,,,,,,
paulgb/sklearn-pandas#18,55820324,calpaterson,paulgb,2015-01-28 21:44:59,2015-05-20 15:46:19,2015-03-01 16:04:00,closed,,,10,,https://api.github.com/repos/paulgb/sklearn-pandas/issues/18,b'Remove workaround for sklearn bug #2374',b'The bug has since been fixed:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/issues/2374'
lisitsyn/tapkee_benchmarks#1,17034060,har777,,2013-07-22 07:38:24,2013-07-22 07:38:25,None,open,,,0,,https://api.github.com/repos/lisitsyn/tapkee_benchmarks/issues/1,b'Update twenty_newsgroups.rst',b'Vecotrizer() changed to TfidfVectorizer()\r\nIssue #2173\r\nhttps://github.com/scikit-learn/scikit-learn/issues/2173'
paulgb/sklearn-pandas#9,26194783,ghost,paulgb,2014-01-23 20:14:57,2014-01-27 23:13:09,2014-01-27 23:09:24,closed,,,1,,https://api.github.com/repos/paulgb/sklearn-pandas/issues/9,"b""MinMaxScaler():  TypeError: 'numpy.int64' object does not support item assignment""","b""The MinMaxScaler() throws an error when put through the DataFrameMapper.  There's a related issue on scikit-learn that can be fixed by the addition of braces, but I can't figure out a syntax in the mapper that will do the equivalent.  \r\n\r\nSee here:\r\nhttps://github.com/scikit-learn/scikit-learn/issues/1973\r\n\r\ncode follows\r\n\r\n    mapper = DataFrameMapper([\r\n        ('NODE', sklearn.preprocessing.LabelEncoder()),\r\n        ('MaxLength', sklearn.preprocessing.MinMaxScaler()),\r\n        ('AgeYearsDecimal', sklearn.preprocessing.MinMaxScaler()),\r\n    ])\r\n    \r\n    x,y=mapper.fit_transform(Xconv)[:,-2],mapper.fit_transform(Xconv)[:,-1]\r\n\r\n\r\n    c:\\Python27\\lib\\site-packages\\sklearn\\base.pyc in fit_transform(self, X, y, **fit_params)\r\n        406         if y is None:\r\n        407             # fit method of arity 1 (unsupervised transformation)\r\n    --> 408             return self.fit(X, **fit_params).transform(X)\r\n        409         else:\r\n        410             # fit method of arity 2 (supervised transformation)\r\n    \r\n    c:\\Python27\\lib\\site-packages\\sklearn_pandas\\__init__.pyc in fit(self, X, y)\r\n        104         for columns, transformer in self.features:\r\n        105             if transformer is not None:\r\n    --> 106                 transformer.fit(self._get_col_subset(X, columns))\r\n        107         return self\r\n        108 \r\n    \r\n    c:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.pyc in fit(self, X, y)\r\n        203         data_range = np.max(X, axis=0) - data_min\r\n        204         # Do not scale constant features\r\n    --> 205         data_range[data_range == 0.0] = 1.0\r\n        206         self.scale_ = (feature_range[1] - feature_range[0]) / data_range\r\n        207         self.min_ = feature_range[0] - data_min * self.scale_\r\n    \r\n    TypeError: 'numpy.int64' object does not support item assignment\r\n"""
statsmodels/statsmodels#1915,40657685,kianho,,2014-08-20 02:17:53,2014-08-20 02:58:37,None,open,,,1,comp-nonparametric;type-bug,https://api.github.com/repos/statsmodels/statsmodels/issues/1915,b'Error fitting kde.KDEUnivariate using integer arrays',"b'Version info:\r\n```\r\npython version:\r\n2.7.6 (default, Mar 22 2014, 22:59:56) \r\n[GCC 4.8.2]\r\nstatsmodels version:\r\n0.5.0\r\ngit revision:\r\n82e027e91cac47cfcfbcab754a244731cd726f06\r\ninstallation method:\r\npip install --user statsmodels\r\n```\r\n\r\nRunning the following snippet:\r\n```python\r\nimport numpy\r\nfrom statsmodels.nonparametric import kde\r\n\r\nrand_ints = numpy.random.random_integers(0, 100, size=100)\r\nrand_floats = numpy.random.random_sample(size=100)\r\n\r\n# OK\r\nkde_ = kde.KDEUnivariate(rand_floats)\r\nkde_.fit()\r\n\r\n# Crash\r\nkde_ = kde.KDEUnivariate(rand_ints)\r\nkde_.fit()\r\n```\r\nproduces the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""./statsmodels_01.py"", line 40, in <module>\r\n    kde_.fit()\r\n  File ""/home/kian/.local/lib/python2.7/site-packages/statsmodels/nonparametric/kde.py"", line 142, in fit\r\n    clip=clip, cut=cut)\r\n  File ""/home/kian/.local/lib/python2.7/site-packages/statsmodels/nonparametric/kde.py"", line 484, in kdensityfft\r\n    binned = fast_linbin(X,a,b,gridsize)/(delta*nobs)\r\n  File ""linbin.pyx"", line 17, in statsmodels.nonparametric.linbin.fast_linbin (statsmodels/nonparametric/linbin.c:1246)\r\nValueError: Buffer dtype mismatch, expected \'DOUBLE\' but got \'long\'\r\n```\r\nI can avoid this error by casting integer input values (to `kde.KDEUnivariate`) to numpy floats:\r\n```python\r\n# FIX: cast integer values to floats.\r\n# OK\r\nkde_ = kde.KDEUnivariate(rand_ints.astype(numpy.float))\r\nkde_.fit()\r\n```\r\n\r\nSimilar issues were raised:\r\n- #1586\r\n- https://github.com/scikit-learn/scikit-learn/issues/1709\r\n\r\nThanks!'"
dirko/pyhacrf#22,105895712,Shotgunosine,dirko,2015-09-10 20:54:55,2015-09-19 14:48:35,2015-09-19 14:48:35,closed,,,24,,https://api.github.com/repos/dirko/pyhacrf/issues/22,b'build error under conda on Windows 7',"b'I am trying to use pip to install pyhacrf under conda on Windows 7 and it is giving me the following error. Any help with a work around would be greatly apprecieated.\r\n\r\n```\r\nC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda>pip install pyhacrf\r\nCollecting pyhacrf\r\n  Using cached pyhacrf-0.0.12.tar.gz\r\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.9 in c:\\users\\User\\appdata\\local\\continuum\\anaconda\\lib\\site-p\r\nackages (from pyhacrf)\r\nRequirement already satisfied (use --upgrade to upgrade): PyLBFGS>=0.1.3 in c:\\users\\User\\appdata\\local\\continuum\\anaconda\\lib\\si\r\nte-packages (from pyhacrf)\r\nBuilding wheels for collected packages: pyhacrf\r\n  Running setup.py bdist_wheel for pyhacrf\r\n  Complete output from command C:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\python.exe -c ""import setuptools;__file__=\'c:\\\\user\r\ns\\\\User\\\\appdata\\\\local\\\\temp\\\\pip-build-rojt7c\\\\pyhacrf\\\\setup.py\';exec(compile(open(__file__).read().replace(\'\\r\\n\', \'\\n\'), __f\r\nile__, \'exec\'))"" bdist_wheel -d c:\\users\\User\\appdata\\local\\temp\\tmpqnoh1dpip-wheel-:\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build\\lib.win-amd64-2.7\r\n  creating build\\lib.win-amd64-2.7\\pyhacrf\r\n  copying pyhacrf\\feature_extraction.py -> build\\lib.win-amd64-2.7\\pyhacrf\r\n  copying pyhacrf\\pyhacrf.py -> build\\lib.win-amd64-2.7\\pyhacrf\r\n  copying pyhacrf\\state_machine.py -> build\\lib.win-amd64-2.7\\pyhacrf\r\n  copying pyhacrf\\__init__.py -> build\\lib.win-amd64-2.7\\pyhacrf\r\n  running build_ext\r\n  Looking for python27.dll\r\n  building \'pyhacrf.algorithms\' extension\r\n  C compiler: gcc -m64 -g -DNDEBUG -DMS_WIN64 -O2 -Wall -Wstrict-prototypes\r\n\r\n  creating build\\temp.win-amd64-2.7\r\n  creating build\\temp.win-amd64-2.7\\Release\r\n  creating build\\temp.win-amd64-2.7\\Release\\pyhacrf\r\n  compile options: \'-D__MSVCRT_VERSION__=0x0900 -IC:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\r\n\\\\core\\\\include -IC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\numpy\\core\\include -IC:\\Users\\User\\AppData\\\r\nLocal\\Continuum\\Anaconda\\include -IC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\PC -c\'\r\n  gcc -m64 -g -DNDEBUG -DMS_WIN64 -O2 -Wall -Wstrict-prototypes -D__MSVCRT_VERSION__=0x0900 -IC:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Co\r\nntinuum\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include -IC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\nu\r\nmpy\\core\\include -IC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\include -IC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\PC\r\n-c pyhacrf/algorithms.c -o build\\temp.win-amd64-2.7\\Release\\pyhacrf\\algorithms.o\r\n  Found executable C:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\Scripts\\gcc.bat\r\n  gcc -m64 -g -shared build\\temp.win-amd64-2.7\\Release\\pyhacrf\\algorithms.o -LC:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Continuum\\\\Anacond\r\na\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\lib -LC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\libs -LC:\\Users\\User\\AppData\\Local\\Co\r\nntinuum\\Anaconda\\PCbuild\\amd64 -lnpymath -lpython27 -lmsvcr90 -o build\\lib.win-amd64-2.7\\pyhacrf\\algorithms.pyd\r\n  Warning: .drectve `/manifestdependency:""type=\'win32\' name=\'Microsoft.VC90.CRT\' version=\'9.0.21022.8\' processorArchitecture=\'amd64\'\r\n publicKeyToken=\'1fc8b3b9a1e18e3b\'"" /DEFAULTLIB:""python27.lib"" /DEFAULTLIB:""MSVCRT"" /DEFAULTLIB:""OLDNAMES"" \' unrecognized\r\n  Warning: .drectve `/manifestdependency:""type=\'win32\' name=\'Microsoft.VC90.CRT\' version=\'9.0.21022.8\' processorArchitecture=\'amd64\'\r\n publicKeyToken=\'1fc8b3b9a1e18e3b\'"" /DEFAULTLIB:""python27.lib"" /DEFAULTLIB:""MSVCRT"" /DEFAULTLIB:""OLDNAMES"" \' unrecognized\r\n  C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\lib/npymath.lib(build/temp.win-amd64-2.7\r\n/build/src.win-amd64-2.7/numpy/core/src/npymath/npy_math.obj):(.text+0x2e3): undefined reference to `__imp_modff\'\r\n  collect2.exe: error: ld returned 1 exit status\r\n  error: Command ""gcc -m64 -g -shared build\\temp.win-amd64-2.7\\Release\\pyhacrf\\algorithms.o -LC:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Co\r\nntinuum\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\lib -LC:\\Users\\User\\AppData\\Local\\Continuum\\Anaconda\\libs -LC:\\Users\\User\\\r\nAppData\\Local\\Continuum\\Anaconda\\PCbuild\\amd64 -lnpymath -lpython27 -lmsvcr90 -o build\\lib.win-amd64-2.7\\pyhacrf\\algorithms.pyd"" fai\r\nled with exit status 1\r\n\r\n  ----------------------------------------\r\n  Failed building wheel for pyhacrf\r\n```'"
EducationalTestingService/skll#64,20262840,dan-blanchard,dan-blanchard,2013-09-30 13:39:15,2014-01-13 13:46:03,2013-11-01 14:30:03,closed,,1.0,5,wontfix,https://api.github.com/repos/EducationalTestingService/skll/issues/64,"b'""Generator already executing"" errors with some datasets.'","b""I believe this is strongly related to scikit-learn/scikit-learn#1565, but I can't establish much of a pattern for why it only happens sometimes. I've gotten it in local mode and gridmap mode, for regression and for classification, but not with every dataset.\r\n\r\nI will say that the dataset I'm seeing it with the most reliably is one that uses our new `FilteredLeaveOneLabelOut` fold generator, and the unit tests seem to pass, so I'm confused."""
joblib/joblib#75,17835764,dohmatob,ogrisel,2013-08-08 22:04:35,2014-07-12 01:24:50,2014-01-13 12:10:48,closed,,,2,,https://api.github.com/repos/joblib/joblib/issues/75,b'NEWTEST: closing issue #72',"b""issue #72 has been silently fixed by an unknown commit.\r\n* I've added a test in test_memory.py  to track the problem, should it pop-up  again in future\r\n* I've made no other modifications to the code-base"""
joblib/joblib#105,25303442,bthirion,GaelVaroquaux,2014-01-09 08:50:32,2014-01-13 13:45:45,2014-01-10 11:04:16,closed,ogrisel,0.8.0,12,,https://api.github.com/repos/joblib/joblib/issues/105,"b""Random error when lanching  sklearn's cross_val_score loops with n_jobs=-1""","b""Hi,\r\n\r\nI'm using joblib from the sklearn master. I randomly get the following weird error.\r\nThx for your help.\r\n\r\nBest,\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.cross_validation import ShuffleSplit\r\nfrom sklearn.cross_validation import cross_val_score\r\n\r\nn, p = 78, 19\r\n\r\ncv = ShuffleSplit(n, n_iter=20, train_size=.9,\r\n                  test_size=.1, random_state=2)\r\n\r\nlr = LinearRegression(fit_intercept=True)\r\nx = np.random.randn(n, p)\r\ny = np.random.randn(n)\r\nfor i in range(10):\r\n    cross_val_score(lr, x, y, cv=cv, n_jobs=-1,\r\n                scoring='mean_squared_error').sum()\r\n```\r\nYields sometimes the following error:\r\n\r\n```\r\nIn [244]: %run debug.py\r\n[Parallel] Pool seems closed\r\n[Parallel] Pool seems closed\r\n[Parallel] Pool seems closed\r\n[Parallel] Pool seems closed\r\n[Parallel] Pool seems closed\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/lib/python2.7/dist-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)\r\n    173             else:\r\n    174                 filename = fname\r\n--> 175             __builtin__.execfile(filename, *where)\r\n\r\n/volatile/thirion/mygit/bthirion/cohort/archi/debug.py in <module>()\r\n     14 for i in range(10):\r\n     15     cross_val_score(lr, x, y, cv=cv, n_jobs=-1,\r\n---> 16                 scoring='mean_squared_error').sum()\r\n     17 \r\n\r\n/volatile/thirion/mygit/scikit-learn/sklearn/cross_validation.pyc in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, score_func, pre_dispatch)\r\n   1146         delayed(_cross_val_score)(clone(estimator), X, y, scorer, train, test,\r\n   1147                                   verbose, fit_params)\r\n-> 1148         for train, test in cv)\r\n   1149     return np.array(scores)\r\n   1150 \r\n\r\n/volatile/thirion/mygit/scikit-learn/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\r\n    514             n_jobs = max(mp.cpu_count() + 1 + n_jobs, 1)\r\n    515 \r\n--> 516         # The list of exceptions that we will capture\r\n\r\n    517         self.exceptions = [TransportableException]\r\n    518         self._lock = threading.Lock()\r\n\r\nValueError: generator already executing\r\n```\r\n\r\n"""
joblib/joblib#51,8965383,abhirk,ogrisel,2012-12-03 21:54:43,2013-10-04 14:24:07,2013-10-03 09:07:30,closed,,,2,,https://api.github.com/repos/joblib/joblib/issues/51,b'Joblib.dump Memory error',"b'Memory usage by joblib.dump jumps to more than 3 times the size of the object.\nWhile  training the classifier takes about 11g of memory, but when dumping the classifier\nobject using joblib.dump(compress=9), the usage jumps up to 38.4g\n[Causing issues with limited RAM]. [I tried values compress=3, 5, 7, 9, always get memory\n error]. If ""compress"" is not used for joblib.dump, the classifier object  is about 11g.\nFollowing is the minimalistic script that demonstrates the steps in my classifier script.\n\n\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.externals import joblib\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.linear_model import SGDClassifier\n    import time\n\n    # Here ""data"" is the list of plaintext paragraphs and target array has the \n    # category . Each paragraph belongs to one of the n categories. \n\n    def train(data, target):\n        vectorizer = TfidfVectorizer(stop_words=\'english\', ngram_range=(1,2),\n                        smooth_idf=True, sublinear_tf=True, max_df=0.5,\n                        token_pattern=ur\'\\b(?!\\d)\\w\\w+\\b\', use_idf=False)\n\n        # Vectorize the input data\n        print ""Extracting features from dataset using %s."" % (self.vectorizer)\n        start_time=time()\n        data_vectors = vectorizer.fit_transform(data)\n        extract_features_time = time() - start_time\n        print ""Feature extraction of training data done in %s seconds"" % extract_features_time\n        print ""Number of samples in training data: %d\\n Number of features: %d"" % data_vectors.shape\n        print """"\n        \n        # Dump the vectorizer, dataset and target array objects for later use. This seems to work correctly\n        # with any value of compress.\n        print ""Dumping vectorizer..."",\n        joblib.dump(self.vectorizer, ""vectorizer.joblib"", compress=9)\n        print  ""done""\n\n        print ""Dumping data vectors..."",\n        joblib.dump(data_vectors,  ""datavectors.joblib"",compress=9)\n        print ""done."" \n\n        print ""Dumping target array..."",\n         joblib.dump(target, ""targetarray.joblib"", compress=9)\n         print ""done."" \n        \n\n        # Train the classifer with OvR. The maximum memory used during training is\n        # 11g for a dataset of size 655M.\n        clf = OneVsRestClassifier(SGDClassifier(loss=\'log\', n_iter=35,\n                                            alpha=0.00001, n_jobs=-1))\n        start_time=time()\n        print ""Training %s"" % clf\n        clf.fit(data_vectors, target)\n        print ""done [%.3fs]"" % (start_time-time())\n     \n        # Dump the classifier for later use. Joblib dumps the classifier correctly\n        # without any compression. However the size of the vector dumped is about 10-11g.\n        # This seems to be too large for our purpose, and hence trying to compress\n        # the dumped object.  For compress=3,5,7,9 the  memory usage jumps to 38.4g\n        # Since the available memory is only 32g, the process ends up using swap space\n        # where the process is stalled for a long time and eventually killed.\n        print ""Dumping classifier....."",\n        joblib.dump(clf, ""classifier.joblib"", compress=9)\n        print ""done""\n\nAnd following is the traceback from the exception:\n```\nDumping classifier... Unexpected error:Traceback (most recent call last):\n  File ""trainclf.py"", line 324, in train_model\n    joblib.dump(self.clf, self.classifierfilename, compress=9)\n  File ""/home/n7/env/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 362, in dump\n    pickler.close()\n  File ""/home/n7/env/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 252, in close\n    self.file.getvalue(), self.compress)\n  File ""/home/n7/env/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py"", line 89, in write_zfile\n    file_handle.write(zlib.compress(data, compress))\nMemoryError: Can\'t allocate memory to compress data\n```\n\nMoving issue from scikit to Joblib [Ref: https://github.com/scikit-learn/scikit-learn/issues/1414]'"
joblib/joblib#66,14038153,mblondel,,2013-05-07 08:20:35,2014-01-28 13:55:15,None,open,,,4,,https://api.github.com/repos/joblib/joblib/issues/66,b'memory error when using numpy pickle with compression',"b'I wrote a script which loads a large sparse matrix (250 million non-zero values) from a text file then pickle it with joblib. This works fine without compression but I get a MemoryError exception when using compress=9. \r\n\r\nAs far as I can see, this comes from the fact that joblib converts the data to a big compressed string as can be seen below:\r\n\r\n```python\r\ndef write_zfile(file_handle, data, compress=1):\r\n    """"""Write the data in the given file as a Z-file.\r\n\r\nZ-files are raw data compressed with zlib used internally by joblib\r\nfor persistence. Backward compatibility is not guarantied. Do not\r\nuse for external purposes.\r\n""""""\r\n    file_handle.write(_ZFILE_PREFIX)\r\n    length = hex(len(data))\r\n    if sys.version_info[0] < 3 and type(length) is long:\r\n        # We need to remove the trailing \'L\' in the hex representation\r\n        length = length[:-1]\r\n    # Store the length of the data\r\n    file_handle.write(asbytes(length.ljust(_MAX_LEN)))\r\n    file_handle.write(zlib.compress(asbytes(data), compress))\r\n```\r\n\r\nSo, at one point, the data exist twice in memory (once in compressed and oncee in uncompressed form).\r\n\r\nI don\'t know the joblib internals so I could well be wrong but it seems to me that a potential solution would be to use gzip.GzipFile (http://docs.python.org/2/library/gzip.html#module-gzip) instead of zlib.compress. The goal of using GzipFile would be to allow Python to write the compressed file by chunks.\r\n\r\nMathieu'"
numpy/numpy#2694,7763514,seberg,njsmith,2012-10-22 11:34:10,2014-06-13 19:09:47,2012-10-25 22:16:11,closed,,,18,,https://api.github.com/repos/numpy/numpy/issues/2694,b'ENH: Make 1-dimensional axes not matter for contiguous flags',"b""This PR implements that ie. a 1x3x1 array will be both C- and F-Contiguous (or neither). I have mentioned this before and cleaned up the branch a little against current master and I though I am not completly certain I think it should cover the change fully.  I am not sure if the tests should be done differently or more extensive though.\r\n\r\nPersonally I see only one possible problem with it, and that is external libraries relying on `strides[-1] == itemsize` (for C-contiguous arrays) or `strides[0] == itemsize` (for F-Contiguous arrays). They could break in very bad ways then. I doubt this should be common or even occur, but there was code in numpy itself that relied on it.\r\n\r\nI have run `scipy.test()` (admittingly old ubuntu) with this as well and no (new) errors occured even when I filled bogus values for the 1-dim strides in array creation. I have checked numpy test for that as well.\r\n\r\nI have also removed some strides cleanup code, I don't really see much of a point to make strides look nice when it doesn't do anything else.\r\n\r\nBecause I am not sure what the special casing in `CreateSortedStridePerm` was for and I thought it might be to ensure contiguouity of the new array in some cases, I have reverted that change to it to fix Issue #434 with wrong sorting and included it here since the rest of this commit makes that kind of special casing unnecessary (so I am not sure the fix is quite correct outside of this context)."""
numpy/numpy#2770,8681996,GaelVaroquaux,GaelVaroquaux,2012-11-26 16:09:06,2012-11-27 14:35:07,2012-11-26 16:27:38,closed,,,3,,https://api.github.com/repos/numpy/numpy/issues/2770,b'A C-contiguous array can now have strides[1] == 0',"b'Since: https://github.com/numpy/numpy/commit/c48156dfdc408f0a1e59ef54ac490cccbd6b8d73\n\na C-contiguous array can now have strides[1] == 0.\n\nThis is a change in behavior since numpy 1.7 and it induces a bunch of bugs in scikit-learn.\n\nThe question is: should this be considered as a bug, or should the scikit-learn adapt to this change in behavior.'"
joblib/joblib#44,6606635,ogrisel,GaelVaroquaux,2012-09-02 23:50:30,2014-06-13 22:59:03,2013-07-31 07:20:41,closed,,,126,,https://api.github.com/repos/joblib/joblib/issues/44,b'[MRG] Custom pickling pool for no-copy memmap handling with multiprocessing',"b""This is a new `multiprocessing.Pool` subclass to better deal with the shared memory situation as discussed at the end of the comment thread of #43.\r\n\r\nFeedback welcome, I plan to do more testing, benchmarking, documentation and `joblib.parallel` integration + validate on the sklearn's `RandomForestClassifier` use case in the coming week.\r\n\r\nTODO before merge:\r\n- [x] doctest fixture to skip memmap doctest when numpy is not installed\r\n- [x] support for memmap instance in the `.base` attribute of an array\r\n- [x] add a `joblib.has_shareable_memory` utility function to detect datastructures with shared memory.\r\n\r\nTasks left as future work for another pull request:\r\n\r\n- add support for multiprocessing Lock if possible\r\n- demonstrate concurrent read / write access with Lock + doc\r\n"""
RaRe-Technologies/gensim#92,6136787,buma,buma,2012-08-09 20:31:54,2012-08-24 20:01:19,2012-08-24 20:01:19,closed,,,12,,https://api.github.com/repos/RaRe-Technologies/gensim/issues/92,b'LDA model gives Segmentation Fault',"b'I would like to use LDA model on mine corpus, but I get segmentation fault when trying to transform corpus.\n\nVersions for running valgrind:\ngensim 0.8.5\nnumpy 1.8.0.dev-fcdbcac\nscipy 0.12.0.dev-e75a945\n\nOn stable version the problem is the same only file names in valgrind are missing, because there are no debug symbols:\ngensim 0.8.5\nnumpy 1.6.2\nscipy 0.10.1\n\nI am running Arch Linux kernel 3.4.7-1 PAE kernel i686 and 8 GB of memory with python2.7.3-2 .\n\nAll files needed to run these are [here](http://dl.dropbox.com/u/25828692/example2/files.zip)\n\nIntrestingly in scikit-learn I have the [same problem](https://github.com/scikit-learn/scikit-learn/issues/998) with similar stacktrace.\n\nI would be happy for any help.\n\n```python\nfrom gensim import corpora, models\nfrom gensim.corpora.dictionary import Dictionary\nimport logging\n\nlogging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n\n\nmoj_korpus = corpora.mmcorpus.MmCorpus(""podatki_del.mm"")\nprint moj_korpus\n\ndictionary = Dictionary.load_from_text(""dictionary.txt"")\nprint dictionary\nrpmodel = models.ldamodel.LdaModel(moj_korpus, id2word=dictionary, num_topics=97, update_every=1, passes=20, chunksize=10)\n\nprint rpmodel\n\ncorpus_rpmodel = rpmodel[moj_korpus]\n\ncorpora.MmCorpus.serialize(\'lda_corpus.mm\', corpus_rpmodel)\n```\n\nSegmentation fault happens when merging changes:\n2012-08-09 22:17:47,234 : INFO : PROGRESS: iteration 0, at document #10/100\n2012-08-09 22:17:47,354 : INFO : 2/10 documents converged within 50 iterations\n2012-08-09 22:17:48,899 : INFO : merging changes from 10 documents into a model of 100 documents\nTraceback (most recent call last):\n  File ""example.py"", line 13, in <module>\n    rpmodel = models.ldamodel.LdaModel(moj_korpus, id2word=dictionary, num_topics=97, update_every=1, passes=20, chunksize=10)\n  File ""/home/mabu/kaggle/zascikit/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 265, in __init__\n    self.update(corpus)\n  File ""/home/mabu/kaggle/zascikit/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 455, in update\n    self.do_mstep(rho(), other)\n  File ""/home/mabu/kaggle/zascikit/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 489, in do_mstep\n    diff -= self.state.get_Elogbeta()\n  File ""/home/mabu/kaggle/zascikit/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 153, in get_Elogbeta\n    return dirichlet_expectation(self.get_lambda())\n  File ""/home/mabu/kaggle/zascikit/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 60, in dirichlet_expectation\n    result = psi(alpha) - psi(numpy.sum(alpha, 1))[:, numpy.newaxis]\nMemoryError\n\nValgrind gives this:\n2012-08-09 21:51:36,556 : INFO : loaded corpus index from podatki_del.mm.index\n2012-08-09 21:51:36,576 : INFO : initializing corpus reader from podatki_del.mm\n2012-08-09 21:51:36,579 : INFO : accepted corpus with 100 documents, 591333 features, 11109 non-zero entries\nMmCorpus(100 documents, 591333 features, 11109 non-zero entries)\nDictionary(592158 unique tokens)\n2012-08-09 21:52:38,601 : INFO : using serial LDA version on this node\n==25177== Warning: set address range perms: large range [0xc84f028, 0x27e89318) (undefined)\n==25177== Warning: set address range perms: large range [0x38d41028, 0x5437b318) (undefined)\n==25177== Warning: set address range perms: large range [0xc84f018, 0x27e89328) (noaccess)\n==25177== Warning: set address range perms: large range [0xc84f028, 0x27e89318) (undefined)\n==25177== Warning: set address range perms: large range [0x75710028, 0x90d4a318) (undefined)\n==25177== Warning: set address range perms: large range [0x97adf028, 0xb3119318) (undefined)\n==25177== Warning: set address range perms: large range [0x75710018, 0x90d4a328) (noaccess)\n==25177== Warning: set address range perms: large range [0x75710028, 0x90d4a318) (undefined)\n==25177== Warning: set address range perms: large range [0x97adf018, 0xb3119328) (noaccess)\n==25177== Warning: set address range perms: large range [0xc84f018, 0x27e89328) (noaccess)\n==25177== Warning: set address range perms: large range [0xc84f028, 0x27e89318) (undefined)\n==25177== Warning: set address range perms: large range [0x75710018, 0x90d4a328) (noaccess)\n2012-08-09 21:56:07,067 : INFO : running online LDA training, 97 topics, 20 passes over the supplied corpus of 100 documents, updating model once every 10 documents\n==25177== Warning: set address range perms: large range [0x75710028, 0x90d4a318) (undefined)\n2012-08-09 21:56:07,978 : INFO : PROGRESS: iteration 0, at document #10/100\n==25177== Warning: set address range perms: large range [0x97adf028, 0xb3119318) (undefined)\n==25177== Conditional jump or move depends on uninitialised value(s)\n==25177==    at 0x4A2B7F3: PyArray_MapIterReset (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x4AA5E97: array_subscript (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x4AA691A: array_subscript_nice (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x408F4FF: PyObject_GetItem (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4124478: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177== Conditional jump or move depends on uninitialised value(s)\n==25177==    at 0x4A2B7F3: PyArray_MapIterReset (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x4A9F10C: array_ass_sub (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x408F823: PyObject_SetItem (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412439F: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x40B6B20: function_call (in /usr/lib/libpython2.7.so.1.0)\n==25177==\n2012-08-09 21:56:09,730 : INFO : 2/10 documents converged within 50 iterations\n==25177== Warning: set address range perms: large range [0x97adf018, 0xb3119328) (noaccess)\n==25177== Warning: set address range perms: large range [0x97adf028, 0xb3119318) (undefined)\n2012-08-09 21:56:34,273 : INFO : merging changes from 10 documents into a model of 100 documents\n==25177== Invalid read of size 4\n==25177==    at 0x4B3960B: trivial_three_operand_loop (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==25177==    by 0x4B4EEF7: PyUFunc_GenericFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==25177==    by 0x4B4F1BA: ufunc_generic_call (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==25177==    by 0x409025F: PyObject_Call (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4090347: call_function_tail (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x409045F: _PyObject_CallFunction_SizeT (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4A67B80: PyArray_GenericBinaryFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x408BE96: binary_op1 (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x408E685: PyNumber_Multiply (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412587C: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==  Address 0x1c is not stack\'d, malloc\'d or (recently) free\'d\n==25177==\n==25177==\n==25177== Process terminating with default action of signal 11 (SIGSEGV)\n==25177==  Access not within mapped region at address 0x1C\n==25177==    at 0x4B3960B: trivial_three_operand_loop (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==25177==    by 0x4B4EEF7: PyUFunc_GenericFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==25177==    by 0x4B4F1BA: ufunc_generic_call (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/umath.so)\n==25177==    by 0x409025F: PyObject_Call (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4090347: call_function_tail (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x409045F: _PyObject_CallFunction_SizeT (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4A67B80: PyArray_GenericBinaryFunction (in /home/mabu/kaggle/zascikit/lib/python2.7/site-packages/numpy/core/multiarray.so)\n==25177==    by 0x408BE96: binary_op1 (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x408E685: PyNumber_Multiply (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412587C: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x412A03C: PyEval_EvalCodeEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==    by 0x4128233: PyEval_EvalFrameEx (in /usr/lib/libpython2.7.so.1.0)\n==25177==  If you believe this happened as a result of a stack\n==25177==  overflow in your program\'s main thread (unlikely but\n==25177==  possible), you can try to increase the size of the\n==25177==  main thread stack using the --main-stacksize= flag.\n==25177==  The main thread stack size used in this run was 8388608.\n==25177==\n==25177== HEAP SUMMARY:\n==25177==     in use at exit: 1,899,201,128 bytes in 13,757 blocks\n==25177==   total heap usage: 719,362 allocs, 705,604 frees, 4,800,996,361 bytes allocated\n==25177==\n==25177== LEAK SUMMARY:\n==25177==    definitely lost: 24 bytes in 1 blocks\n==25177==    indirectly lost: 0 bytes in 0 blocks\n==25177==      possibly lost: 308,652 bytes in 541 blocks\n==25177==    still reachable: 1,898,892,436 bytes in 13,214 blocks\n==25177==         suppressed: 16 bytes in 1 blocks\n==25177== Rerun with --leak-check=full to see details of leaked memory\n==25177==\n==25177== For counts of detected and suppressed errors, rerun with: -v\n==25177== Use --track-origins=yes to see where uninitialised values come from\n==25177== ERROR SUMMARY: 4 errors from 3 contexts (suppressed: 5120 from 514)\nSegmentation fault\n'"
joblib/joblib#44,6606635,ogrisel,GaelVaroquaux,2012-09-02 23:50:30,2014-06-13 22:59:03,2013-07-31 07:20:41,closed,,,126,,https://api.github.com/repos/joblib/joblib/issues/44,b'[MRG] Custom pickling pool for no-copy memmap handling with multiprocessing',"b""This is a new `multiprocessing.Pool` subclass to better deal with the shared memory situation as discussed at the end of the comment thread of #43.\r\n\r\nFeedback welcome, I plan to do more testing, benchmarking, documentation and `joblib.parallel` integration + validate on the sklearn's `RandomForestClassifier` use case in the coming week.\r\n\r\nTODO before merge:\r\n- [x] doctest fixture to skip memmap doctest when numpy is not installed\r\n- [x] support for memmap instance in the `.base` attribute of an array\r\n- [x] add a `joblib.has_shareable_memory` utility function to detect datastructures with shared memory.\r\n\r\nTasks left as future work for another pull request:\r\n\r\n- add support for multiprocessing Lock if possible\r\n- demonstrate concurrent read / write access with Lock + doc\r\n"""
joblib/joblib#232,103324716,ogrisel,lesteve,2015-08-26 17:25:36,2015-08-27 11:03:46,2015-08-27 11:03:46,closed,,,1,,https://api.github.com/repos/joblib/joblib/issues/232,b'[MRG] FIX make joblib not break BLAS / OpenMP under Python 3.4+',"b""Let's enable the forkserver start method by default under Python 3.4+"""
xianyi/OpenBLAS#294,18987526,ogrisel,xianyi,2013-09-04 18:10:49,2014-09-30 03:50:59,2014-02-19 22:58:28,closed,,,88,,https://api.github.com/repos/xianyi/OpenBLAS/issues/294,b'OpenBLAS hangs when calling DGEMM after a Unix fork',"b'Here is a sample program that illustrate a problem when I want to use OpenBLAS with Unix fork (without exec):\r\n\r\nhttps://gist.github.com/ogrisel/6440446\r\n\r\n```\r\n$ wget https://gist.github.com/ogrisel/6440446/raw/openblas_fork.c\r\n$ gcc -lopenblas -L/opt/OpenBLAS/lib -I/opt/OpenBLAS/include \\\r\n           openblas_fork.c -o openblas_fork\r\n```\r\n\r\nThen calling GEMM with a 7x7 squared matrix in parent and child process works without issue:\r\n\r\n```\r\n$ ./openblas_fork 7\r\ncomputing for size 7\r\nc[0][0] is 7.000000\r\nparent d[0][0] is 7.000000\r\nchild d[0][0] is 7.000000\r\n```\r\n\r\nDoing the same for a 8x8 matrix will cause the child process to never completes (while burning 100% of a CPU) until I kill it.\r\n\r\nMight this be caused by some static initialization step in the OpenBLAS runtime? If so would it be possible during init to store the pid of the process at init time and then later on check that the current process pid still the same and if not reinit the runtime? \r\n\r\nAtlas, Apple vecLib and Intel MKL have no trouble with Unix forks. Hence this limitation (bug?) prevents OpenBLAS to be used as a drop-in replacement for those guys. This is especially annoying when used with Python that uses Unix forks in its standard library via the multiprocessing module. As Python threads suffer from a locking issue, many project use multiprocessing as a work around.\r\n\r\n\r\n\r\n'"
numpy/numpy#4007,22049781,piskvorky,juliantaylor,2013-11-04 13:05:02,2014-10-29 00:02:19,2014-10-26 16:20:24,closed,,,155,component: numpy.linalg,https://api.github.com/repos/numpy/numpy/issues/4007,b'numpy.dot crash with numpy.float32 input',"b""A user of gensim @fbkarsdorp reported crash (segfault) with NumPy: piskvorky/gensim#131\r\n\r\nThe crash seems to have nothing to do with gensim, so I'm transferring the issue here. It happens in `dot` of matrix*vector in single precision, on his OS X Maverick.\r\n"""
